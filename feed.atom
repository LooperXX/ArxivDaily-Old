<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-07-27T02:03:40.016Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-07-27T02:03:39.955Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08751</id>
        <link href="http://arxiv.org/abs/2107.08751"/>
        <updated>2021-07-27T02:03:39.753Z</updated>
        <summary type="html"><![CDATA[Deep learning for medical imaging suffers from temporal and privacy-related
restrictions on data availability. To still obtain viable models, continual
learning aims to train in sequential order, as and when data is available. The
main challenge that continual learning methods face is to prevent catastrophic
forgetting, i.e., a decrease in performance on the data encountered earlier.
This issue makes continuous training of segmentation models for medical
applications extremely difficult. Yet, often, data from at least two different
domains is available which we can exploit to train the model in a way that it
disregards domain-specific information. We propose an architecture that
leverages the simultaneous availability of two or more datasets to learn a
disentanglement between the content and domain in an adversarial fashion. The
domain-invariant content representation then lays the base for continual
semantic segmentation. Our approach takes inspiration from domain adaptation
and combines it with continual learning for hippocampal segmentation in brain
MRI. We showcase that our method reduces catastrophic forgetting and
outperforms state-of-the-art continual learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1"&gt;Marius Memmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1"&gt;Camila Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Anirban Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits. (arXiv:2107.11419v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.11419</id>
        <link href="http://arxiv.org/abs/2107.11419"/>
        <updated>2021-07-27T02:03:39.735Z</updated>
        <summary type="html"><![CDATA[We consider nonstationary multi-armed bandit problems where the model
parameters of the arms change over time. We introduce the adaptive resetting
bandit (ADR-bandit), which is a class of bandit algorithms that leverages
adaptive windowing techniques from the data stream community. We first provide
new guarantees on the quality of estimators resulting from adaptive windowing
techniques, which are of independent interest in the data mining community.
Furthermore, we conduct a finite-time analysis of ADR-bandit in two typical
environments: an abrupt environment where changes occur instantaneously and a
gradual environment where changes occur progressively. We demonstrate that
ADR-bandit has nearly optimal performance when the abrupt or global changes
occur in a coordinated manner that we call global changes. We demonstrate that
forced exploration is unnecessary when we restrict the interest to the global
changes. Unlike the existing nonstationary bandit algorithms, ADR-bandit has
optimal performance in stationary environments as well as nonstationary
environments with global changes. Our experiments show that the proposed
algorithms outperform the existing approaches in synthetic and real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Komiyama_J/0/1/0/all/0/1"&gt;Junpei Komiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fouche_E/0/1/0/all/0/1"&gt;Edouard Fouch&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1"&gt;Junya Honda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08364</id>
        <link href="http://arxiv.org/abs/2107.08364"/>
        <updated>2021-07-27T02:03:39.406Z</updated>
        <summary type="html"><![CDATA[Software Vulnerabilities (SVs) are increasing in complexity and scale, posing
great security risks to many software systems. Given the limited resources in
practice, SV assessment and prioritization help practitioners devise optimal SV
mitigation plans based on various SV characteristics. The surge in SV data
sources and data-driven techniques such as Machine Learning and Deep Learning
have taken SV assessment and prioritization to the next level. Our survey
provides a taxonomy of the past research efforts and highlights the best
practices for data-driven SV assessment and prioritization. We also discuss the
current limitations and propose potential solutions to address such issues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Triet H. M. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huaming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1"&gt;M. Ali Babar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03844</id>
        <link href="http://arxiv.org/abs/2107.03844"/>
        <updated>2021-07-27T02:03:39.382Z</updated>
        <summary type="html"><![CDATA[Bangla -- ranked as the 6th most widely spoken language across the world
(https://www.ethnologue.com/guides/ethnologue200), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Arid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvirul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1"&gt;Janntatul Tajrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naira Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08251</id>
        <link href="http://arxiv.org/abs/2107.08251"/>
        <updated>2021-07-27T02:03:39.376Z</updated>
        <summary type="html"><![CDATA[We introduce ParaBLEU, a paraphrase representation learning model and
evaluation metric for text generation. Unlike previous approaches, ParaBLEU
learns to understand paraphrasis using generative conditioning as a pretraining
objective. ParaBLEU correlates more strongly with human judgements than
existing metrics, obtaining new state-of-the-art results on the 2017 WMT
Metrics Shared Task. We show that our model is robust to data scarcity,
exceeding previous state-of-the-art performance using only $50\%$ of the
available training data and surpassing BLEU, ROUGE and METEOR with only $40$
labelled examples. Finally, we demonstrate that ParaBLEU can be used to
conditionally generate novel paraphrases from a single demonstration, which we
use to confirm our hypothesis that it learns abstract, generalized paraphrase
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jack Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1"&gt;Raphael Lenain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1"&gt;Udeepa Meepegama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1"&gt;Emil Fristed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07647</id>
        <link href="http://arxiv.org/abs/2107.07647"/>
        <updated>2021-07-27T02:03:39.375Z</updated>
        <summary type="html"><![CDATA[A novel energy-efficient edge computing paradigm is proposed for real-time
deep learning-based image upsampling applications. State-of-the-art deep
learning solutions for image upsampling are currently trained using either
resize or sub-pixel convolution to learn kernels that generate high fidelity
images with minimal artifacts. However, performing inference with these learned
convolution kernels requires memory-intensive feature map transformations that
dominate time and energy costs in real-time applications. To alleviate this
pressure on memory bandwidth, we confine the use of resize or sub-pixel
convolution to training in the cloud by transforming learned convolution
kernels to deconvolution kernels before deploying them for inference as a
functionally equivalent deconvolution. These kernel transformations, intended
as a one-time cost when shifting from training to inference, enable a systems
designer to use each algorithm in their optimal context by preserving the image
fidelity learned when training in the cloud while minimizing data transfer
penalties during inference at the edge. We also explore existing variants of
deconvolution inference algorithms and introduce a novel variant for
consideration. We analyze and compare the inference properties of
convolution-based upsampling algorithms using a quantitative model of incurred
time and energy costs and show that using deconvolution for inference at the
edge improves both system latency and energy efficiency when compared to their
sub-pixel or resize convolution counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1"&gt;Ian Colbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1"&gt;Ken Kreutz-Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Srinjoy Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08209</id>
        <link href="http://arxiv.org/abs/2107.08209"/>
        <updated>2021-07-27T02:03:39.373Z</updated>
        <summary type="html"><![CDATA[For the binary prevalence quantification problem under prior probability
shift, we determine the asymptotic variance of the maximum likelihood
estimator. We find that it is a function of the Brier score for the regression
of the class label against the features under the test data set distribution.
This observation suggests that optimising the accuracy of a base classifier on
the training data set helps to reduce the variance of the related quantifier on
the test data set. Therefore, we also point out training criteria for the base
classifier that imply optimisation of both of the Brier scores on the training
and the test data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1"&gt;Dirk Tasche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08293</id>
        <link href="http://arxiv.org/abs/2107.08293"/>
        <updated>2021-07-27T02:03:39.372Z</updated>
        <summary type="html"><![CDATA[We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input
single-output (MISO) system for downlink transmission. We compare the
performance of Deep Reinforcement Learning (DRL) and conventional optimization
methods in finding optimal phase shifts of the IRS elements to maximize the
user signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of
these methods to channel impairments and changes in the system. We demonstrate
numerically that DRL solutions show more robustness to noisy channels and user
mobility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1"&gt;Amal Feriani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1"&gt;Amine Mezghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1"&gt;Ekram Hossain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Graph Auto-Encoder Models for Attributed Graph Clustering. (arXiv:2107.08562v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08562</id>
        <link href="http://arxiv.org/abs/2107.08562"/>
        <updated>2021-07-27T02:03:39.370Z</updated>
        <summary type="html"><![CDATA[Most recent graph clustering methods have resorted to Graph Auto-Encoders
(GAEs) to perform joint clustering and embedding learning. However, two
critical issues have been overlooked. First, the accumulative error, inflicted
by learning with noisy clustering assignments, degrades the effectiveness and
robustness of the clustering model. This problem is called Feature Randomness.
Second, reconstructing the adjacency matrix sets the model to learn irrelevant
similarities for the clustering task. This problem is called Feature Drift.
Interestingly, the theoretical relation between the aforementioned problems has
not yet been investigated. We study these issues from two aspects: (1) there is
a trade-off between Feature Randomness and Feature Drift when clustering and
reconstruction are performed at the same level, and (2) the problem of Feature
Drift is more pronounced for GAE models, compared with vanilla auto-encoder
models, due to the graph convolutional operation and the graph decoding design.
Motivated by these findings, we reformulate the GAE-based clustering
methodology. Our solution is two-fold. First, we propose a sampling operator
$\Xi$ that triggers a protection mechanism against the noisy clustering
assignments. Second, we propose an operator $\Upsilon$ that triggers a
correction mechanism against Feature Drift by gradually transforming the
reconstructed graph into a clustering-oriented one. As principal advantages,
our solution grants a considerable improvement in clustering effectiveness and
robustness and can be easily tailored to existing GAE models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1"&gt;Nairouz Mrabah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1"&gt;Mohamed Bouguessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1"&gt;Mohamed Fawzi Touati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1"&gt;Riadh Ksantini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06232</id>
        <link href="http://arxiv.org/abs/2106.06232"/>
        <updated>2021-07-27T02:03:39.369Z</updated>
        <summary type="html"><![CDATA[Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning
(DRL) via combining deep learning (DL) with reinforcement learning (RL), which
has noticed that the distribution of the acquired data would change during the
training process. DQN found this property might cause instability for training,
so it proposed effective methods to handle the downside of the property.
Instead of focusing on the unfavourable aspects, we find it critical for RL to
ease the gap between the estimated data distribution and the ground truth data
distribution while supervised learning (SL) fails to do so. From this new
perspective, we extend the basic paradigm of RL called the Generalized Policy
Iteration (GPI) into a more generalized version, which is called the
Generalized Data Distribution Iteration (GDI). We see massive RL algorithms and
techniques can be unified into the GDI paradigm, which can be considered as one
of the special cases of GDI. We provide theoretical proof of why GDI is better
than GPI and how it works. Several practical algorithms based on GDI have been
proposed to verify the effectiveness and extensiveness of it. Empirical
experiments prove our state-of-the-art (SOTA) performance on Arcade Learning
Environment (ALE), wherein our algorithm has achieved 9620.98% mean human
normalized score (HNS), 1146.39% median HNS and 22 human world record
breakthroughs (HWRB) using only 200M training frames. Our work aims to lead the
RL research to step into the journey of conquering the human world records and
seek real superhuman agents on both performance and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiajun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Changnan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions. (arXiv:2107.03383v2 [q-bio.GN] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03383</id>
        <link href="http://arxiv.org/abs/2107.03383"/>
        <updated>2021-07-27T02:03:39.361Z</updated>
        <summary type="html"><![CDATA[Whole genome sequencing (WGS) is quickly becoming the customary means for
identification of antimicrobial resistance (AMR) due to its ability to obtain
high resolution information about the genes and mechanisms that are causing
resistance and driving pathogen mobility. By contrast, traditional phenotypic
(antibiogram) testing cannot easily elucidate such information. Yet development
of AMR prediction tools from genotype-phenotype data can be biased, since
sampling is non-randomized. Sample provenience, period of collection, and
species representation can confound the association of genetic traits with AMR.
Thus, prediction models can perform poorly on new data with sampling
distribution shifts. In this work -- under an explicit set of causal
assumptions -- we evaluate the effectiveness of propensity-based rebalancing
and confounding adjustment on AMR prediction using genotype-phenotype AMR data
from the Pathosystems Resource Integration Center (PATRIC). We select bacterial
genotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),
country, year, species, and AMR phenotypes for the tetracycline drug class,
preparing test data with recent genomes coming from a single country. We test
boosted logistic regression (BLR) and random forests (RF) with/without
bias-handling. On 10,936 instances, we find evidence of species, location and
year imbalance with respect to the AMR phenotype. The crude versus
bias-adjusted change in effect of genetic signatures on AMR varies but only
moderately (selecting the top 20,000 out of 40+ million k-mers). The area under
the receiver operating characteristic (AUROC) of the RF (0.95) is comparable to
that of BLR (0.94) on both out-of-bag samples from bootstrap and the external
test (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC
with bias-handling compared to the sole use of genetic signatures. ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Prosperi_M/0/1/0/all/0/1"&gt;Mattia Prosperi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Marini_S/0/1/0/all/0/1"&gt;Simone Marini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Boucher_C/0/1/0/all/0/1"&gt;Christina Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07933</id>
        <link href="http://arxiv.org/abs/2107.07933"/>
        <updated>2021-07-27T02:03:39.359Z</updated>
        <summary type="html"><![CDATA[Unprecedented access to multi-temporal satellite imagery has opened new
perspectives for a variety of Earth observation tasks. Among them,
pixel-precise panoptic segmentation of agricultural parcels has major economic
and environmental implications. While researchers have explored this problem
for single images, we argue that the complex temporal patterns of crop
phenology are better addressed with temporal sequences of images. In this
paper, we present the first end-to-end, single-stage method for panoptic
segmentation of Satellite Image Time Series (SITS). This module can be combined
with our novel image sequence encoding network which relies on temporal
self-attention to extract rich and adaptive multi-scale spatio-temporal
features. We also introduce PASTIS, the first open-access SITS dataset with
panoptic annotations. We demonstrate the superiority of our encoder for
semantic segmentation against multiple competing architectures, and set up the
first state-of-the-art of panoptic segmentation of SITS. Our implementation and
PASTIS are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1"&gt;Vivien Sainte Fare Garnot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1"&gt;Loic Landrieu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09604</id>
        <link href="http://arxiv.org/abs/2102.09604"/>
        <updated>2021-07-27T02:03:39.357Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) are a powerful architecture for
representation learning on documents that naturally occur as graphs, e.g.,
citation or social networks. However, sensitive personal information, such as
documents with people's profiles or relationships as edges, are prone to
privacy leaks, as the trained model might reveal the original input. Although
differential privacy (DP) offers a well-founded privacy-preserving framework,
GCNs pose theoretical and practical challenges due to their training specifics.
We address these challenges by adapting differentially-private gradient-based
training to GCNs and conduct experiments using two optimizers on five NLP
datasets in two languages. We propose a simple yet efficient method based on
random graph splits that not only improves the baseline privacy bounds by a
factor of 2.7 while retaining competitive F1 scores, but also provides strong
privacy guarantees of epsilon = 1.0. We show that, under certain modeling
choices, privacy-preserving GCNs perform up to 90% of their non-private
variants, while formally guaranteeing strong privacy measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1"&gt;Timour Igamberdiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1"&gt;Ivan Habernal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric convergence of elliptical slice sampling. (arXiv:2105.03308v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03308</id>
        <link href="http://arxiv.org/abs/2105.03308"/>
        <updated>2021-07-27T02:03:39.356Z</updated>
        <summary type="html"><![CDATA[For Bayesian learning, given likelihood function and Gaussian prior, the
elliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides
a tool for the construction of a Markov chain for approximate sampling of the
underlying posterior distribution. Besides of its wide applicability and
simplicity its main feature is that no tuning is necessary. Under weak
regularity assumptions on the posterior density we show that the corresponding
Markov chain is geometrically ergodic and therefore yield qualitative
convergence guarantees. We illustrate our result for Gaussian posteriors as
they appear in Gaussian process regression, as well as in a setting of a
multi-modal distribution. Remarkably, our numerical experiments indicate a
dimension-independent performance of elliptical slice sampling even in
situations where our ergodicity result does not apply.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Natarovskii_V/0/1/0/all/0/1"&gt;Viacheslav Natarovskii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudolf_D/0/1/0/all/0/1"&gt;Daniel Rudolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sprungk_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Sprungk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties (Technical Report). (arXiv:2107.08588v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08588</id>
        <link href="http://arxiv.org/abs/2107.08588"/>
        <updated>2021-07-27T02:03:39.355Z</updated>
        <summary type="html"><![CDATA[High-quality labels are expensive to obtain for many machine learning tasks,
such as medical image classification tasks. Therefore, probabilistic (weak)
labels produced by weak supervision tools are used to seed a process in which
influential samples with weak labels are identified and cleaned by several
human annotators to improve the model performance. To lower the overall cost
and computational overhead of this process, we propose a solution called CHEF
(CHEap and Fast label cleaning), which consists of the following three
components. First, to reduce the cost of human annotators, we use Infl, which
prioritizes the most influential training samples for cleaning and provides
cleaned labels to save the cost of one human annotator. Second, to accelerate
the sample selector phase and the model constructor phase, we use Increm-Infl
to incrementally produce influential samples, and DeltaGrad-L to incrementally
update the model. Third, we redesign the typical label cleaning pipeline so
that human annotators iteratively clean smaller batch of samples rather than
one big batch of samples. This yields better over all model performance and
enables possible early termination when the expected model performance has been
achieved. Extensive experiments show that our approach gives good model
prediction performance while achieving significant speed-ups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yinjun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1"&gt;Susan B. Davidson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demonstration of Panda: A Weakly Supervised Entity Matching System. (arXiv:2106.10821v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10821</id>
        <link href="http://arxiv.org/abs/2106.10821"/>
        <updated>2021-07-27T02:03:39.354Z</updated>
        <summary type="html"><![CDATA[Entity matching (EM) refers to the problem of identifying tuple pairs in one
or more relations that refer to the same real world entities. Supervised
machine learning (ML) approaches, and deep learning based approaches in
particular, typically achieve state-of-the-art matching results. However, these
approaches require many labeled examples, in the form of matching and
non-matching pairs, which are expensive and time-consuming to label. In this
paper, we introduce Panda, a weakly supervised system specifically designed for
EM. Panda uses the same labeling function abstraction as Snorkel, where
labeling functions (LF) are user-provided programs that can generate large
amounts of (somewhat noisy) labels quickly and cheaply, which can then be
combined via a labeling model to generate accurate final predictions. To
support users developing LFs for EM, Panda provides an integrated development
environment (IDE) that lives in a modern browser architecture. Panda's IDE
facilitates the development, debugging, and life-cycle management of LFs in the
context of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel
in general-purpose programming. Panda's IDE includes many novel features
purpose-built for EM, such as smart data sampling, a builtin library of EM
utility functions, automatically generated LFs, visual debugging of LFs, and
finally, an EM-specific labeling model. We show in this demo that Panda IDE can
greatly accelerate the development of high-quality EM solutions using weak
supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Renzhi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakala_P/0/1/0/all/0/1"&gt;Prem Sakala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xu Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yeye He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v3 [gr-qc] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01863</id>
        <link href="http://arxiv.org/abs/2107.01863"/>
        <updated>2021-07-27T02:03:39.352Z</updated>
        <summary type="html"><![CDATA[LIGO is considered the most sensitive and complicated gravitational
experiment ever built. Its main objective is to detect the gravitational wave
from the strongest events in the universe by observing if the length of its
4-kilometer arms change by a distance 10,000 times smaller than the diameter of
a proton. Due to its sensitivity, LIGO is prone to the disturbance of external
noises which affects the data being collected to detect the gravitational wave.
These noises are commonly called by the LIGO community as glitches. The
objective of this study is to evaluate the effeciency of various deep trasnfer
learning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch
waveform in gravitational wave data. The accuracy achieved by the said models
are 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models
achieved fairly high accuracy, it is observed that all of the model suffered
from the lack of data for certain classes which is the main concern found in
the experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1"&gt;Reymond Mesuga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1"&gt;Brian James Bayanay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrating sufficiently. (arXiv:2105.07283v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07283</id>
        <link href="http://arxiv.org/abs/2105.07283"/>
        <updated>2021-07-27T02:03:39.349Z</updated>
        <summary type="html"><![CDATA[When probabilistic classifiers are trained and calibrated, the so-called
grouping loss component of the calibration loss can easily be overlooked.
Grouping loss refers to the gap between observable information and information
actually exploited in the calibration exercise. We investigate the relation
between grouping loss and the concept of sufficiency, identifying
comonotonicity as a useful criterion for sufficiency. We revisit the probing
reduction approach of Langford & Zadrozny (2005) and find that it produces an
estimator of probabilistic classifiers that reduces grouping loss. Finally, we
discuss Brier curves as tools to support training and 'sufficient' calibration
of probabilistic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1"&gt;Dirk Tasche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02961</id>
        <link href="http://arxiv.org/abs/2105.02961"/>
        <updated>2021-07-27T02:03:39.347Z</updated>
        <summary type="html"><![CDATA[Boundary Representations (B-Reps) are the industry standard in 3D Computer
Aided Design/Manufacturing (CAD/CAM) and industrial design due to their
fidelity in representing stylistic details. However, they have been ignored in
the 3D style research. Existing 3D style metrics typically operate on meshes or
pointclouds, and fail to account for end-user subjectivity by adopting fixed
definitions of style, either through crowd-sourcing for style labels or
hand-crafted features. We propose UVStyle-Net, a style similarity measure for
B-Reps that leverages the style signals in the second order statistics of the
activations in a pre-trained (unsupervised) 3D encoder, and learns their
relative importance to a subjective end-user through few-shot learning. Our
approach differs from all existing data-driven 3D style methods since it may be
used in completely unsupervised settings, which is desirable given the lack of
publicly available labelled B-Rep datasets. More importantly, the few-shot
learning accounts for the inherent subjectivity associated with style. We show
quantitatively that our proposed method with B-Reps is able to capture stronger
style signals than alternative methods on meshes and pointclouds despite its
significantly greater computational efficiency. We also show it is able to
generate meaningful style gradients with respect to the input shape, and that
few-shot learning with as few as two positive examples selected by an end-user
is sufficient to significantly improve the style measure. Finally, we
demonstrate its efficacy on a large unlabeled public dataset of CAD models.
Source code and data will be released in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1"&gt;Peter Meltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1"&gt;Hooman Shayani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1"&gt;Amir Khasahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1"&gt;Pradeep Kumar Jayaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1"&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1"&gt;Joseph Lambourne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v2 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14659</id>
        <link href="http://arxiv.org/abs/2104.14659"/>
        <updated>2021-07-27T02:03:39.346Z</updated>
        <summary type="html"><![CDATA[We describe a novel application of the end-to-end deep learning technique to
the task of discriminating top quark-initiated jets from those originating from
the hadronization of a light quark or a gluon. The end-to-end deep learning
technique combines deep learning algorithms and low-level detector
representation of the high-energy collision event. In this study, we use
low-level detector information from the simulated CMS Open Data samples to
construct the top jet classifiers. To optimize classifier performance we
progressively add low-level information from the CMS tracking detector,
including pixel detector reconstructed hits and impact parameters, and
demonstrate the value of additional tracking information even when no new
spatial structures are added. Relying only on calorimeter energy deposits and
reconstructed pixel detector hits, the end-to-end classifier achieves an AUC
score of 0.975$\pm$0.002 for the task of classifying boosted top quark jets.
After adding derived track quantities, the classifier AUC score increases to
0.9824$\pm$0.0013, serving as the first performance benchmark for these CMS
Open Data samples. We additionally provide a timing performance comparison of
different processor unit architectures for training the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1"&gt;Michael Andrews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1"&gt;Bjorn Burkle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-fan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1"&gt;Davide DiCroce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1"&gt;Sergei Gleyzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1"&gt;Ulrich Heintz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1"&gt;Meenakshi Narain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1"&gt;Manfred Paulini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1"&gt;Nikolas Pervan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1"&gt;Yusef Shafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1"&gt;Emanuele Usai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Manipulation Detection by Multi-View Multi-Scale Supervision. (arXiv:2104.06832v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06832</id>
        <link href="http://arxiv.org/abs/2104.06832"/>
        <updated>2021-07-27T02:03:39.344Z</updated>
        <summary type="html"><![CDATA[The key challenge of image manipulation detection is how to learn
generalizable features that are sensitive to manipulations in novel data,
whilst specific to prevent false alarms on authentic images. Current research
emphasizes the sensitivity, with the specificity overlooked. In this paper we
address both aspects by multi-view feature learning and multi-scale
supervision. By exploiting noise distribution and boundary artifact surrounding
tampered regions, the former aims to learn semantic-agnostic and thus more
generalizable features. The latter allows us to learn from authentic images
which are nontrivial to be taken into account by current semantic segmentation
network based methods. Our thoughts are realized by a new network which we term
MVSS-Net. Extensive experiments on five benchmark sets justify the viability of
MVSS-Net for both pixel-level and image-level manipulation detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinru Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chengbo Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jiaqi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Juan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xirong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction. (arXiv:2106.04362v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04362</id>
        <link href="http://arxiv.org/abs/2106.04362"/>
        <updated>2021-07-27T02:03:39.343Z</updated>
        <summary type="html"><![CDATA[How and where proteins interface with one another can ultimately impact the
proteins' functions along with a range of other biological processes. As such,
precise computational methods for protein interface prediction (PIP) come
highly sought after as they could yield significant advances in drug discovery
and design as well as protein function analysis. However, the traditional
benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a
modest 230 complexes for training, validating, and testing different machine
learning algorithms. In this work, we expand on a dataset recently introduced
for this task, the Database of Interacting Protein Structures (DIPS), to
present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for
geometric deep learning of protein interfaces. The previous version of DIPS
contains only the Cartesian coordinates and types of the atoms comprising a
given protein complex, whereas DIPS-Plus now includes a plethora of new
residue-level features including protrusion indices, half-sphere amino acid
compositions, and new profile hidden Markov model (HMM)-based sequence features
for each amino acid, giving researchers a large, well-curated feature bank for
training protein interface prediction methods. We demonstrate through rigorous
benchmarks that training an existing state-of-the-art (SOTA) model for PIP on
DIPS-Plus yields SOTA results, surpassing the performance of all other models
trained on residue-level and atom-level encodings of protein complexes to date.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Morehead_A/0/1/0/all/0/1"&gt;Alex Morehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sedova_A/0/1/0/all/0/1"&gt;Ada Sedova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jianlin Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-07-27T02:03:39.342Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convolutional Neural Network based Cascade Reconstruction for the IceCube Neutrino Observatory. (arXiv:2101.11589v2 [hep-ex] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11589</id>
        <link href="http://arxiv.org/abs/2101.11589"/>
        <updated>2021-07-27T02:03:37.189Z</updated>
        <summary type="html"><![CDATA[Continued improvements on existing reconstruction methods are vital to the
success of high-energy physics experiments, such as the IceCube Neutrino
Observatory. In IceCube, further challenges arise as the detector is situated
at the geographic South Pole where computational resources are limited.
However, to perform real-time analyses and to issue alerts to telescopes around
the world, powerful and fast reconstruction methods are desired. Deep neural
networks can be extremely powerful, and their usage is computationally
inexpensive once the networks are trained. These characteristics make a deep
learning-based approach an excellent candidate for the application in IceCube.
A reconstruction method based on convolutional architectures and hexagonally
shaped kernels is presented. The presented method is robust towards systematic
uncertainties in the simulation and has been tested on experimental data. In
comparison to standard reconstruction methods in IceCube, it can improve upon
the reconstruction accuracy, while reducing the time necessary to run the
reconstruction by two to three orders of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ex/1/au:+Abbasi_R/0/1/0/all/0/1"&gt;R. Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ackermann_M/0/1/0/all/0/1"&gt;M. Ackermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Adams_J/0/1/0/all/0/1"&gt;J. Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Aguilar_J/0/1/0/all/0/1"&gt;J. A. Aguilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ahlers_M/0/1/0/all/0/1"&gt;M. Ahlers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ahrens_M/0/1/0/all/0/1"&gt;M. Ahrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Alispach_C/0/1/0/all/0/1"&gt;C. Alispach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Alves_A/0/1/0/all/0/1"&gt;A. A. Alves Jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Amin_N/0/1/0/all/0/1"&gt;N. M. Amin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+An_R/0/1/0/all/0/1"&gt;R. An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Andeen_K/0/1/0/all/0/1"&gt;K. Andeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Anderson_T/0/1/0/all/0/1"&gt;T. Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ansseau_I/0/1/0/all/0/1"&gt;I. Ansseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Anton_G/0/1/0/all/0/1"&gt;G. Anton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Arguelles_C/0/1/0/all/0/1"&gt;C. Arg&amp;#xfc;elles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Axani_S/0/1/0/all/0/1"&gt;S. Axani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bai_X/0/1/0/all/0/1"&gt;X. Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+V%2E_A/0/1/0/all/0/1"&gt;A. Balagopal V.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Barbano_A/0/1/0/all/0/1"&gt;A. Barbano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Barwick_S/0/1/0/all/0/1"&gt;S. W. Barwick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bastian_B/0/1/0/all/0/1"&gt;B. Bastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Basu_V/0/1/0/all/0/1"&gt;V. Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Baum_V/0/1/0/all/0/1"&gt;V. Baum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Baur_S/0/1/0/all/0/1"&gt;S. Baur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bay_R/0/1/0/all/0/1"&gt;R. Bay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Beatty_J/0/1/0/all/0/1"&gt;J. J. Beatty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Becker_K/0/1/0/all/0/1"&gt;K.-H. Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Tjus_J/0/1/0/all/0/1"&gt;J. Becker Tjus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bellenghi_C/0/1/0/all/0/1"&gt;C. Bellenghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+BenZvi_S/0/1/0/all/0/1"&gt;S. BenZvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Berley_D/0/1/0/all/0/1"&gt;D. Berley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bernardini_E/0/1/0/all/0/1"&gt;E. Bernardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Besson_D/0/1/0/all/0/1"&gt;D. Z. Besson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Binder_G/0/1/0/all/0/1"&gt;G. Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bindig_D/0/1/0/all/0/1"&gt;D. Bindig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Blaufuss_E/0/1/0/all/0/1"&gt;E. Blaufuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Blot_S/0/1/0/all/0/1"&gt;S. Blot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Boser_S/0/1/0/all/0/1"&gt;S. B&amp;#xf6;ser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Botner_O/0/1/0/all/0/1"&gt;O. Botner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bottcher_J/0/1/0/all/0/1"&gt;J. B&amp;#xf6;ttcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bourbeau_E/0/1/0/all/0/1"&gt;E. Bourbeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bourbeau_J/0/1/0/all/0/1"&gt;J. Bourbeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bradascio_F/0/1/0/all/0/1"&gt;F. Bradascio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Braun_J/0/1/0/all/0/1"&gt;J. Braun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Bron_S/0/1/0/all/0/1"&gt;S. Bron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Brostean_Kaiser_J/0/1/0/all/0/1"&gt;J. Brostean-Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Burgman_A/0/1/0/all/0/1"&gt;A. Burgman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Busse_R/0/1/0/all/0/1"&gt;R. S. Busse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Campana_M/0/1/0/all/0/1"&gt;M. A. Campana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Chen_C/0/1/0/all/0/1"&gt;C. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Chirkin_D/0/1/0/all/0/1"&gt;D. Chirkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Choi_S/0/1/0/all/0/1"&gt;S. Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Clark_B/0/1/0/all/0/1"&gt;B. A. Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Clark_K/0/1/0/all/0/1"&gt;K. Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Classen_L/0/1/0/all/0/1"&gt;L. Classen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Coleman_A/0/1/0/all/0/1"&gt;A. Coleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Collin_G/0/1/0/all/0/1"&gt;G. H. Collin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Conrad_J/0/1/0/all/0/1"&gt;J. M. Conrad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Coppin_P/0/1/0/all/0/1"&gt;P. Coppin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Correa_P/0/1/0/all/0/1"&gt;P. Correa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Cowen_D/0/1/0/all/0/1"&gt;D. F. Cowen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Cross_R/0/1/0/all/0/1"&gt;R. Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Dave_P/0/1/0/all/0/1"&gt;P. Dave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Clercq_C/0/1/0/all/0/1"&gt;C. De Clercq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+DeLaunay_J/0/1/0/all/0/1"&gt;J. J. DeLaunay&lt;/a&gt;, et al. (303 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering nonlinear resonances through physics-informed machine learning. (arXiv:2104.13471v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13471</id>
        <link href="http://arxiv.org/abs/2104.13471"/>
        <updated>2021-07-27T02:03:37.141Z</updated>
        <summary type="html"><![CDATA[For an ensemble of nonlinear systems that model, for instance, molecules or
photonic systems, we propose a method that finds efficiently the configuration
that has prescribed transfer properties. Specifically, we use physics-informed
machine-learning (PIML) techniques to find the parameters for the efficient
transfer of an electron (or photon) to a targeted state in a non-linear dimer.
We create a machine learning model containing two variables, $\chi_D$, and
$\chi_A$, representing the non-linear terms in the donor and acceptor target
system states. We then introduce a data-free physics-informed loss function as
$1.0 - P_j$, where $P_j$ is the probability, the electron being in the targeted
state, $j$. By minimizing the loss function, we maximize the occupation
probability to the targeted state. The method recovers known results in the
Targeted Energy Transfer (TET) model, and it is then applied to a more complex
system with an additional intermediate state. In this trimer configuration, the
PIML approach discovers desired resonant paths from the donor to acceptor
units. The proposed PIML method is general and may be used in the chemical
design of molecular complexes or engineering design of quantum or photonic
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Barmparis_G/0/1/0/all/0/1"&gt;G. D. Barmparis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tsironis_G/0/1/0/all/0/1"&gt;G. P. Tsironis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control. (arXiv:2103.02142v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02142</id>
        <link href="http://arxiv.org/abs/2103.02142"/>
        <updated>2021-07-27T02:03:37.081Z</updated>
        <summary type="html"><![CDATA[Robotic simulators are crucial for academic research and education as well as
the development of safety-critical applications. Reinforcement learning
environments -- simple simulations coupled with a problem specification in the
form of a reward function -- are also important to standardize the development
(and benchmarking) of learning algorithms. Yet, full-scale simulators typically
lack portability and parallelizability. Vice versa, many reinforcement learning
environments trade-off realism for high sample throughputs in toy-like
problems. While public data sets have greatly benefited deep learning and
computer vision, we still lack the software tools to simultaneously develop --
and fairly compare -- control theory and reinforcement learning approaches. In
this paper, we propose an open-source OpenAI Gym-like environment for multiple
quadcopters based on the Bullet physics engine. Its multi-agent and vision
based reinforcement learning interfaces, as well as the support of realistic
collisions and aerodynamic effects, make it, to the best of our knowledge, a
first of its kind. We demonstrate its use through several examples, either for
control (trajectory tracking with PID control, multi-robot flight with
downwash, etc.) or reinforcement learning (single and multi-agent stabilization
tasks), hoping to inspire future research that combines control theory and
machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panerati_J/0/1/0/all/0/1"&gt;Jacopo Panerati&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hehui Zheng&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;SiQi Zhou&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;James Xu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1"&gt;Amanda Prorok&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Schoellig_A/0/1/0/all/0/1"&gt;Angela P. Schoellig&lt;/a&gt; (1 and 2) ((1) University of Toronto Institute for Aerospace Studies, (2) Vector Institute for Artificial Intelligence, (3) University of Cambridge)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning low-rank latent mesoscale structures in networks. (arXiv:2102.06984v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06984</id>
        <link href="http://arxiv.org/abs/2102.06984"/>
        <updated>2021-07-27T02:03:37.074Z</updated>
        <summary type="html"><![CDATA[It is common to use networks to encode the architecture of interactions
between entities in complex systems in the physical, biological, social, and
information sciences. Moreover, to study the large-scale behavior of complex
systems, it is important to study mesoscale structures in networks as building
blocks that influence such behavior. In this paper, we present a new approach
for describing low-rank mesoscale structure in networks, and we illustrate our
approach using several synthetic network models and empirical friendship,
collaboration, and protein--protein interaction (PPI) networks. We find that
these networks possess a relatively small number of `latent motifs' that
together can successfully approximate most subnetworks at a fixed mesoscale. We
use an algorithm that we call "network dictionary learning" (NDL), which
combines a network sampling method and nonnegative matrix factorization, to
learn the latent motifs of a given network. The ability to encode a network
using a set of latent motifs has a wide range of applications to
network-analysis tasks, such as comparison, denoising, and edge inference.
Additionally, using our new network denoising and reconstruction (NDR)
algorithm, we demonstrate how to denoise a corrupted network by using only the
latent motifs that one learns directly from the corrupted networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1"&gt;Hanbaek Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kureh_Y/0/1/0/all/0/1"&gt;Yacoub H. Kureh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vendrow_J/0/1/0/all/0/1"&gt;Joshua Vendrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porter_M/0/1/0/all/0/1"&gt;Mason A. Porter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning. (arXiv:2104.14504v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14504</id>
        <link href="http://arxiv.org/abs/2104.14504"/>
        <updated>2021-07-27T02:03:37.068Z</updated>
        <summary type="html"><![CDATA[We address an inherent difficulty in welfare-theoretic fair machine learning
by proposing an equivalently axiomatically-justified alternative and studying
the resulting computational and statistical learning questions. Welfare metrics
quantify overall wellbeing across a population of one or more groups, and
welfare-based objectives and constraints have recently been proposed to
incentivize fair machine learning methods to produce satisfactory solutions
that consider the diverse needs of multiple groups. Unfortunately, many
machine-learning problems are more naturally cast as loss minimization tasks,
rather than utility maximization, which complicates direct application of
welfare-centric methods to fair machine learning. In this work, we define a
complementary measure, termed malfare, measuring overall societal harm (rather
than wellbeing), with axiomatic justification via the standard axioms of
cardinal welfare. We then cast fair machine learning as malfare minimization
over the risk values (expected losses) of each group. Surprisingly, the axioms
of cardinal welfare (malfare) dictate that this is not equivalent to simply
defining utility as negative loss. Building upon these concepts, we define
fair-PAC (FPAC) learning, where an FPAC learner is an algorithm that learns an
$\varepsilon$-$\delta$ malfare-optimal model with bounded sample complexity,
for any data distribution, and for any (axiomatically justified) malfare
concept. Finally, we show broad conditions under which, with appropriate
modifications, standard PAC-learners may be converted to FPAC learners. This
places FPAC learning on firm theoretical ground, as it yields statistical and
computational efficiency guarantees for many well-studied machine-learning
models, and is also practically relevant, as it democratizes fair ML by
providing concrete training algorithms and rigorous generalization guarantees
for these models]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cousins_C/0/1/0/all/0/1"&gt;Cyrus Cousins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edgeless-GNN: Unsupervised Inductive Edgeless Network Embedding. (arXiv:2104.05225v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05225</id>
        <link href="http://arxiv.org/abs/2104.05225"/>
        <updated>2021-07-27T02:03:37.061Z</updated>
        <summary type="html"><![CDATA[We study the problem of embedding edgeless nodes such as users who newly
enter the underlying network, while using graph neural networks (GNNs) widely
studied for effective representation learning of graphs thanks to its highly
expressive capability via message passing. Our study is motivated by the fact
that existing GNNs cannot be adopted for our problem since message passing to
such edgeless nodes having no connections is impossible. To tackle this
challenge, we propose Edgeless-GNN, a new framework that enables GNNs to
generate node embeddings even for edgeless nodes through unsupervised inductive
learning. Specifically, we start by constructing a $k$-nearest neighbor graph
($k$NNG) based on the similarity of node attributes to replace the GNN's
computation graph defined by the neighborhood-based aggregation of each node.
As our main contributions, the known network structure is used to train model
parameters, while a new loss function is established using energy-based
learning in such a way that our model learns the network structure. For the
edgeless nodes, we inductively infer embeddings for the edgeless nodes by using
edges via $k$NNG construction as a computation graph. By evaluating the
performance of various downstream machine learning (ML) tasks, we empirically
demonstrate that Edgeless-GNN consistently outperforms state-of-the-art methods
of inductive network embedding. Moreover, our findings corroborate the
effectiveness of Edgeless-GNN in judiciously combining the replaced computation
graph with our newly designed loss. Our framework is GNN-model-agnostic; thus,
GNN models can be appropriately chosen according to ones' needs and ML tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yong-Min Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won-Yong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xin Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04156</id>
        <link href="http://arxiv.org/abs/2106.04156"/>
        <updated>2021-07-27T02:03:37.054Z</updated>
        <summary type="html"><![CDATA[Recent works in self-supervised learning have advanced the state-of-the-art
by relying on the contrastive learning paradigm, which learns representations
by pushing positive pairs, or similar examples from the same class, closer
together while keeping negative pairs far apart. Despite the empirical
successes, theoretical foundations are limited -- prior analyses assume
conditional independence of the positive pairs given the same class label, but
recent empirical applications use heavily correlated positive pairs (i.e., data
augmentations of the same image). Our work analyzes contrastive learning
without assuming conditional independence of positive pairs using a novel
concept of the augmentation graph on data. Edges in this graph connect
augmentations of the same data, and ground-truth classes naturally form
connected sub-graphs. We propose a loss that performs spectral decomposition on
the population augmentation graph and can be succinctly written as a
contrastive learning objective on neural net representations. Minimizing this
objective leads to features with provable accuracy guarantees under linear
probe evaluation. By standard generalization bounds, these accuracy guarantees
also hold when minimizing the training contrastive loss. Empirically, the
features learned by our objective can match or outperform several strong
baselines on benchmark vision datasets. In all, this work provides the first
provable analysis for contrastive learning where guarantees for linear probe
evaluation can apply to realistic empirical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1"&gt;Jeff Z. HaoChen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Dynamic Personalized Pricing with Demand Learning. (arXiv:2009.12920v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12920</id>
        <link href="http://arxiv.org/abs/2009.12920"/>
        <updated>2021-07-27T02:03:37.028Z</updated>
        <summary type="html"><![CDATA[The prevalence of e-commerce has made detailed customers' personal
information readily accessible to retailers, and this information has been
widely used in pricing decisions. When involving personalized information, how
to protect the privacy of such information becomes a critical issue in
practice. In this paper, we consider a dynamic pricing problem over $T$ time
periods with an \emph{unknown} demand function of posted price and personalized
information. At each time $t$, the retailer observes an arriving customer's
personal information and offers a price. The customer then makes the purchase
decision, which will be utilized by the retailer to learn the underlying demand
function. There is potentially a serious privacy concern during this process: a
third party agent might infer the personalized information and purchase
decisions from price changes from the pricing system. Using the fundamental
framework of differential privacy from computer science, we develop a
privacy-preserving dynamic pricing policy, which tries to maximize the retailer
revenue while avoiding information leakage of individual customer's information
and purchasing decisions. To this end, we first introduce a notion of
\emph{anticipating} $(\varepsilon, \delta)$-differential privacy that is
tailored to dynamic pricing problem. Our policy achieves both the privacy
guarantee and the performance guarantee in terms of regret. Roughly speaking,
for $d$-dimensional personalized information, our algorithm achieves the
expected regret at the order of $\tilde{O}(\varepsilon^{-1} \sqrt{d^3 T})$,
when the customers' information is adversarially chosen. For stochastic
personalized information, the regret bound can be further improved to
$\tilde{O}(\sqrt{d^2T} + \varepsilon^{-2} d^2)$]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yining Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural message passing for joint paratope-epitope prediction. (arXiv:2106.00757v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00757</id>
        <link href="http://arxiv.org/abs/2106.00757"/>
        <updated>2021-07-27T02:03:37.020Z</updated>
        <summary type="html"><![CDATA[Antibodies are proteins in the immune system which bind to antigens to detect
and neutralise them. The binding sites in an antibody-antigen interaction are
known as the paratope and epitope, respectively, and the prediction of these
regions is key to vaccine and synthetic antibody development. Contrary to prior
art, we argue that paratope and epitope predictors require asymmetric
treatment, and propose distinct neural message passing architectures that are
geared towards the specific aspects of paratope and epitope prediction,
respectively. We obtain significant improvements on both tasks, setting the new
state-of-the-art and recovering favourable qualitative predictions on antigens
of relevance to COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Vecchio_A/0/1/0/all/0/1"&gt;Alice Del Vecchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deac_A/0/1/0/all/0/1"&gt;Andreea Deac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Sampling Density for Nonparametric Regression. (arXiv:2105.11990v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11990</id>
        <link href="http://arxiv.org/abs/2105.11990"/>
        <updated>2021-07-27T02:03:36.950Z</updated>
        <summary type="html"><![CDATA[We propose a novel active learning strategy for regression, which is
model-agnostic, robust against model mismatch, and interpretable. Assuming that
a small number of initial samples are available, we derive the optimal training
density that minimizes the generalization error of local polynomial smoothing
(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated
squared error (MISE) as a generalization criterion, and use the asymptotic
behavior of the MISE as well as the locally optimal bandwidths (LOB) - the
bandwidth function that minimizes MISE in the asymptotic limit. The asymptotic
expression of our objective then reveals the dependence of the MISE on the
training density, enabling analytic minimization. As a result,we obtain the
optimal training density in a closed-form. The almost model-free nature of our
approach thus helps to encode the essential properties of the target problem,
providing a robust and model-agnostic active learning strategy. Furthermore,
the obtained training density factorizes the influence of local function
complexity, noise level and test density in a transparent and interpretable
way. We validate our theory in numerical simulations, and show that the
proposed active learning method outperforms the existing state-of-the-art
model-agnostic approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panknin_D/0/1/0/all/0/1"&gt;Danny Panknin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shinichi Nakajima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02961</id>
        <link href="http://arxiv.org/abs/2105.02961"/>
        <updated>2021-07-27T02:03:36.928Z</updated>
        <summary type="html"><![CDATA[Boundary Representations (B-Reps) are the industry standard in 3D Computer
Aided Design/Manufacturing (CAD/CAM) and industrial design due to their
fidelity in representing stylistic details. However, they have been ignored in
the 3D style research. Existing 3D style metrics typically operate on meshes or
pointclouds, and fail to account for end-user subjectivity by adopting fixed
definitions of style, either through crowd-sourcing for style labels or
hand-crafted features. We propose UVStyle-Net, a style similarity measure for
B-Reps that leverages the style signals in the second order statistics of the
activations in a pre-trained (unsupervised) 3D encoder, and learns their
relative importance to a subjective end-user through few-shot learning. Our
approach differs from all existing data-driven 3D style methods since it may be
used in completely unsupervised settings, which is desirable given the lack of
publicly available labelled B-Rep datasets. More importantly, the few-shot
learning accounts for the inherent subjectivity associated with style. We show
quantitatively that our proposed method with B-Reps is able to capture stronger
style signals than alternative methods on meshes and pointclouds despite its
significantly greater computational efficiency. We also show it is able to
generate meaningful style gradients with respect to the input shape, and that
few-shot learning with as few as two positive examples selected by an end-user
is sufficient to significantly improve the style measure. Finally, we
demonstrate its efficacy on a large unlabeled public dataset of CAD models.
Source code and data will be released in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1"&gt;Peter Meltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1"&gt;Hooman Shayani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1"&gt;Amir Khasahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1"&gt;Pradeep Kumar Jayaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1"&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1"&gt;Joseph Lambourne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12321</id>
        <link href="http://arxiv.org/abs/2102.12321"/>
        <updated>2021-07-27T02:03:36.917Z</updated>
        <summary type="html"><![CDATA[For machine agents to successfully interact with humans in real-world
settings, they will need to develop an understanding of human mental life.
Intuitive psychology, the ability to reason about hidden mental variables that
drive observable actions, comes naturally to people: even pre-verbal infants
can tell agents from objects, expecting agents to act efficiently to achieve
goals given constraints. Despite recent interest in machine agents that reason
about other agents, it is not clear if such agents learn or hold the core
psychology principles that drive human reasoning. Inspired by cognitive
development studies on intuitive psychology, we present a benchmark consisting
of a large dataset of procedurally generated 3D animations, AGENT (Action,
Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal
preferences, action efficiency, unobserved constraints, and cost-reward
trade-offs) that probe key concepts of core intuitive psychology. We validate
AGENT with human-ratings, propose an evaluation protocol emphasizing
generalization, and compare two strong baselines built on Bayesian inverse
planning and a Theory of Mind neural network. Our results suggest that to pass
the designed tests of core intuitive psychology at human levels, a model must
acquire or have built-in representations of how agents plan, combining utility
computations and core knowledge of objects and physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1"&gt;Tianmin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1"&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shari Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1"&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1"&gt;Elizabeth Spelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1"&gt;Tomer D. Ullman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual analytics of set data for knowledge discovery and member selection support. (arXiv:2104.09231v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09231</id>
        <link href="http://arxiv.org/abs/2104.09231"/>
        <updated>2021-07-27T02:03:36.876Z</updated>
        <summary type="html"><![CDATA[Visual analytics (VA) is a visually assisted exploratory analysis approach in
which knowledge discovery is executed interactively between the user and system
in a human-centered manner. The purpose of this study is to develop a method
for the VA of set data aimed at supporting knowledge discovery and member
selection. A typical target application is a visual support system for team
analysis and member selection, by which users can analyze past teams and
examine candidate lineups for new teams. Because there are several
difficulties, such as the combinatorial explosion problem, developing a VA
system of set data is challenging. In this study, we first define the
requirements that the target system should satisfy and clarify the accompanying
challenges. Then we propose a method for the VA of set data, which satisfies
the requirements. The key idea is to model the generation process of sets and
their outputs using a manifold network model. The proposed method visualizes
the relevant factors as a set of topographic maps on which various information
is visualized. Furthermore, using the topographic maps as a bidirectional
interface, users can indicate their targets of interest in the system on these
maps. We demonstrate the proposed method by applying it to basketball teams,
and compare with a benchmark system for outcome prediction and lineup
reconstruction tasks. Because the method can be adapted to individual
application cases by extending the network structure, it can be a general
method by which practical systems can be built.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_R/0/1/0/all/0/1"&gt;Ryuji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishibashi_H/0/1/0/all/0/1"&gt;Hideaki Ishibashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furukawa_T/0/1/0/all/0/1"&gt;Tetsuo Furukawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-07-27T02:03:36.838Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via the moral philosophy definition of social good, propose a
framework to evaluate the direct and indirect real-world impact of NLP tasks,
and adopt the methodology of global priorities research to identify priority
causes for NLP research. Finally, we use our theoretical framework to provide
some practical guidelines for future NLP research for social good. Our data and
code are available at this http URL In
addition, we curate a list of papers and resources on NLP for social good at
https://github.com/zhijing-jin/NLP4SocialGood_Papers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FERMI: Fair Empirical Risk Minimization via Exponential R\'enyi Mutual Information. (arXiv:2102.12586v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12586</id>
        <link href="http://arxiv.org/abs/2102.12586"/>
        <updated>2021-07-27T02:03:36.819Z</updated>
        <summary type="html"><![CDATA[Despite the success of large-scale empirical risk minimization (ERM) at
achieving high accuracy across a variety of machine learning tasks, fair ERM is
hindered by the incompatibility of fairness constraints with stochastic
optimization. In this paper, we propose the fair empirical risk minimization
via exponential R\'enyi mutual information (FERMI) framework. FERMI is built on
a stochastic estimator for exponential R\'enyi mutual information (ERMI), an
information divergence measuring the degree of the dependence of predictions on
sensitive attributes. Theoretically, we show that ERMI upper bounds existing
popular fairness violation metrics, thus controlling ERMI provides guarantees
on other commonly used violations, such as $L_\infty$. We derive an unbiased
estimator for ERMI, which we use to derive the FERMI algorithm. We prove that
FERMI converges for demographic parity, equalized odds, and equal opportunity
notions of fairness in stochastic optimization. Empirically, we show that FERMI
is amenable to large-scale problems with multiple (non-binary) sensitive
attributes and non-binary targets. Extensive experiments show that FERMI
achieves the most favorable tradeoffs between fairness violation and test
accuracy across all tested setups compared with state-of-the-art baselines for
demographic parity, equalized odds, equal opportunity. These benefits are
especially significant for non-binary classification with large sensitive sets
and small batch sizes, showcasing the effectiveness of the FERMI objective and
the developed stochastic algorithm for solving it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1"&gt;Andrew Lowy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavan_R/0/1/0/all/0/1"&gt;Rakesh Pavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1"&gt;Sina Baharlouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1"&gt;Meisam Razaviyayn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilayer Network Analysis for Improved Credit Risk Prediction. (arXiv:2010.09559v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09559</id>
        <link href="http://arxiv.org/abs/2010.09559"/>
        <updated>2021-07-27T02:03:36.812Z</updated>
        <summary type="html"><![CDATA[We present a multilayer network model for credit risk assessment. Our model
accounts for multiple connections between borrowers (such as their geographic
location and their economic activity) and allows for explicitly modelling the
interaction between connected borrowers. We develop a multilayer personalized
PageRank algorithm that allows quantifying the strength of the default exposure
of any borrower in the network. We test our methodology in an agricultural
lending framework, where it has been suspected for a long time default
correlates between borrowers when they are subject to the same structural
risks. Our results show there are significant predictive gains just by
including centrality multilayer network information in the model, and these
gains are increased by more complex information such as the multilayer PageRank
variables. The results suggest default risk is highest when an individual is
connected to many defaulters, but this risk is mitigated by the size of the
neighbourhood of the individual, showing both default risk and financial
stability propagate throughout the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oskarsdottir_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a &amp;#xd3;skarsd&amp;#xf3;ttir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1"&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09540</id>
        <link href="http://arxiv.org/abs/2105.09540"/>
        <updated>2021-07-27T02:03:36.805Z</updated>
        <summary type="html"><![CDATA[The increasing concerns about data privacy and security drive an emerging
field of studying privacy-preserving machine learning from isolated data
sources, i.e., federated learning. A class of federated learning, vertical
federated learning, where different parties hold different features for common
users, has a great potential of driving a more variety of business cooperation
among enterprises in many fields. In machine learning, decision tree ensembles
such as gradient boosting decision tree (GBDT) and random forest are widely
applied powerful models with high interpretability and modeling efficiency.
However, the interpretability is compromised in state-of-the-art vertical
federated learning frameworks such as SecureBoost with anonymous features to
avoid possible data breaches. To address this issue in the inference process,
in this paper, we propose Fed-EINI to protect data privacy and allow the
disclosure of feature meaning by concealing decision paths with a
communication-efficient secure computation method for inference outputs. The
advantages of Fed-EINI will be demonstrated through both theoretical analysis
and extensive numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zejin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quantum Classifiers Through the Lens of the Hessian. (arXiv:2105.10162v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10162</id>
        <link href="http://arxiv.org/abs/2105.10162"/>
        <updated>2021-07-27T02:03:36.797Z</updated>
        <summary type="html"><![CDATA[In quantum computing, the variational quantum algorithms (VQAs) are well
suited for finding optimal combinations of things in specific applications
ranging from chemistry all the way to finance. The training of VQAs with
gradient descent optimization algorithm has shown a good convergence. At an
early stage, the simulation of variational quantum circuits on noisy
intermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like
classical deep learning, it also suffers from vanishing gradient problems. It
is a realistic goal to study the topology of loss landscape, to visualize the
curvature information and trainability of these circuits in the existence of
vanishing gradients. In this paper, we calculated the Hessian and visualized
the loss landscape of variational quantum classifiers at different points in
parameter space. The curvature information of variational quantum classifiers
(VQC) is interpreted and the loss function's convergence is shown. It helps us
better understand the behavior of variational quantum circuits to tackle
optimization problems efficiently. We investigated the variational quantum
classifiers via Hessian on quantum computers, started with a simple 4-bit
parity problem to gain insight into the practical behavior of Hessian, then
thoroughly analyzed the behavior of Hessian's eigenvalues on training the
variational quantum classifier for the Diabetes dataset. Finally, we show that
how the adaptive Hessian learning rate can influence the convergence while
training the variational circuits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sen_P/0/1/0/all/0/1"&gt;Pinaki Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bhatia_A/0/1/0/all/0/1"&gt;Amandeep Singh Bhatia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Individual Heterogeneity: An Automatic Inference Framework. (arXiv:2010.14694v2 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14694</id>
        <link href="http://arxiv.org/abs/2010.14694"/>
        <updated>2021-07-27T02:03:36.768Z</updated>
        <summary type="html"><![CDATA[We develop methodology for estimation and inference using machine learning to
enrich economic models. Our framework takes a standard economic model and
recasts the parameters as fully flexible nonparametric functions, to capture
the rich heterogeneity based on potentially high dimensional or complex
observable characteristics. These "parameter functions" retain the
interpretability, economic meaning, and discipline of classical parameters.
Deep learning is particularly well-suited to structured modeling of
heterogeneity in economics. We show how to design the network architecture to
match the structure of the economic model, delivering novel methodology that
moves deep learning beyond prediction. We prove convergence rates for the
estimated parameter functions. These functions are the key inputs into the
finite-dimensional parameter of inferential interest. We obtain inference based
on a novel influence function calculation that covers any second-stage
parameter and any machine-learning-enriched model that uses a smooth
per-observation loss function. No additional derivations are required. The
score can be taken directly to data, using automatic differentiation if needed.
The researcher need only define the original model and define the parameter of
interest. A key insight is that we need not write down the influence function
in order to evaluate it on the data. Our framework gives new results for a host
of contexts, covering such diverse examples as price elasticities,
willingness-to-pay, and surplus measures in binary or multinomial choice
models, effects of continuous treatment variables, fractional outcome models,
count data, heterogeneous production functions, and more. We apply our
methodology to a large scale advertising experiment for short-term loans. We
show how economically meaningful estimates and inferences can be made that
would be unavailable without our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Farrell_M/0/1/0/all/0/1"&gt;Max H. Farrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tengyuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Misra_S/0/1/0/all/0/1"&gt;Sanjog Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?. (arXiv:2012.15180v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15180</id>
        <link href="http://arxiv.org/abs/2012.15180"/>
        <updated>2021-07-27T02:03:36.756Z</updated>
        <summary type="html"><![CDATA[Do state-of-the-art natural language understanding models care about word
order - one of the most important characteristics of a sequence? Not always! We
found 75% to 90% of the correct predictions of BERT-based classifiers, trained
on many GLUE tasks, remain constant after input words are randomly shuffled.
Despite BERT embeddings are famously contextual, the contribution of each
individual word to downstream tasks is almost unchanged even after the word's
context is shuffled. BERT-based models are able to exploit superficial cues
(e.g. the sentiment of keywords in sentiment analysis; or the word-wise
similarity between sequence-pair inputs in natural language inference) to make
correct decisions when tokens are arranged in random orders. Encouraging
classifiers to capture word order information improves the performance on most
GLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE
tasks are not challenging machines to understand the meaning of a sentence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1"&gt;Thang M. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1"&gt;Long Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does LIME really see in images?. (arXiv:2102.06307v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06307</id>
        <link href="http://arxiv.org/abs/2102.06307"/>
        <updated>2021-07-27T02:03:36.746Z</updated>
        <summary type="html"><![CDATA[The performance of modern algorithms on certain computer vision tasks such as
object recognition is now close to that of humans. This success was achieved at
the price of complicated architectures depending on millions of parameters and
it has become quite challenging to understand how particular predictions are
made. Interpretability methods propose to give us this understanding. In this
paper, we study LIME, perhaps one of the most popular. On the theoretical side,
we show that when the number of generated examples is large, LIME explanations
are concentrated around a limit explanation for which we give an explicit
expression. We further this study for elementary shape detectors and linear
models. As a consequence of this analysis, we uncover a connection between LIME
and integrated gradients, another explanation method. More precisely, the LIME
explanations are similar to the sum of integrated gradients over the
superpixels used in the preprocessing step of LIME.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1"&gt;Damien Garreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mardaoui_D/0/1/0/all/0/1"&gt;Dina Mardaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology. (arXiv:2104.07365v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07365</id>
        <link href="http://arxiv.org/abs/2104.07365"/>
        <updated>2021-07-27T02:03:36.736Z</updated>
        <summary type="html"><![CDATA[The convergence speed of machine learning models trained with Federated
Learning is significantly affected by non-independent and identically
distributed (non-IID) data partitions, even more so in a fully decentralized
setting without a central server. In this paper, we show that the impact of
local class bias, an important type of data non-IIDness, can be significantly
reduced by carefully designing the underlying communication topology. We
present D-Cliques, a novel topology that reduces gradient bias by grouping
nodes in interconnected cliques such that the local joint distribution in a
clique is representative of the global class distribution. We also show how to
adapt the updates of decentralized SGD to obtain unbiased gradients and
implement an effective momentum with D-Cliques. Our empirical evaluation on
MNIST and CIFAR10 demonstrates that our approach provides similar convergence
speed as a fully-connected topology with a significant reduction in the number
of edges and messages. In a 1000-node topology, D-Cliques requires 98% less
edges and 96% less total messages, with further possible gains using a
small-world topology across cliques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1"&gt;Anne-Marie Kermarrec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavoie_E/0/1/0/all/0/1"&gt;Erick Lavoie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Multisensor Change Detection. (arXiv:2103.05102v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05102</id>
        <link href="http://arxiv.org/abs/2103.05102"/>
        <updated>2021-07-27T02:03:36.711Z</updated>
        <summary type="html"><![CDATA[Most change detection methods assume that pre-change and post-change images
are acquired by the same sensor. However, in many real-life scenarios, e.g.,
natural disaster, it is more practical to use the latest available images
before and after the occurrence of incidence, which may be acquired using
different sensors. In particular, we are interested in the combination of the
images acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR
images appear vastly different from the optical images even when capturing the
same scene. Adding to this, change detection methods are often constrained to
use only target image-pair, no labeled data, and no additional unlabeled data.
Such constraints limit the scope of traditional supervised machine learning and
unsupervised generative approaches for multi-sensor change detection. Recent
rapid development of self-supervised learning methods has shown that some of
them can even work with only few images. Motivated by this, in this work we
propose a method for multi-sensor change detection using only the unlabeled
target bi-temporal images that are used for training a network in
self-supervised fashion by using deep clustering and contrastive learning. The
proposed method is evaluated on four multi-modal bi-temporal scenes showing
change and the benefits of our self-supervised approach are demonstrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sudipan Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1"&gt;Patrick Ebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active multi-fidelity Bayesian online changepoint detection. (arXiv:2103.14224v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14224</id>
        <link href="http://arxiv.org/abs/2103.14224"/>
        <updated>2021-07-27T02:03:36.697Z</updated>
        <summary type="html"><![CDATA[Online algorithms for detecting changepoints, or abrupt shifts in the
behavior of a time series, are often deployed with limited resources, e.g., to
edge computing settings such as mobile phones or industrial sensors. In these
scenarios it may be beneficial to trade the cost of collecting an environmental
measurement against the quality or "fidelity" of this measurement and how the
measurement affects changepoint estimation. For instance, one might decide
between inertial measurements or GPS to determine changepoints for motion. A
Bayesian approach to changepoint detection is particularly appealing because we
can represent our posterior uncertainty about changepoints and make active,
cost-sensitive decisions about data fidelity to reduce this posterior
uncertainty. Moreover, the total cost could be dramatically lowered through
active fidelity switching, while remaining robust to changes in data
distribution. We propose a multi-fidelity approach that makes cost-sensitive
decisions about which data fidelity to collect based on maximizing information
gain with respect to changepoints. We evaluate this framework on synthetic,
video, and audio data and show that this information-based approach results in
accurate predictions while reducing total cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gundersen_G/0/1/0/all/0/1"&gt;Gregory W. Gundersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_D/0/1/0/all/0/1"&gt;Diana Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuteng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Engelhardt_B/0/1/0/all/0/1"&gt;Barbara E. Engelhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Adams_R/0/1/0/all/0/1"&gt;Ryan P. Adams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the Adaptive Generalisation Gap. (arXiv:2011.08181v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08181</id>
        <link href="http://arxiv.org/abs/2011.08181"/>
        <updated>2021-07-27T02:03:36.679Z</updated>
        <summary type="html"><![CDATA[We conjecture that the inherent difference in generalisation between adaptive
and non-adaptive gradient methods stems from the increased estimation noise in
the flattest directions of the true loss surface. We demonstrate that typical
schedules used for adaptive methods (with low numerical stability or damping
constants) serve to bias relative movement towards flat directions relative to
sharp directions, effectively amplifying the noise-to-signal ratio and harming
generalisation. We further demonstrate that the numerical stability/damping
constant used in these methods can be decomposed into a learning rate reduction
and linear shrinkage of the estimated curvature matrix. We then demonstrate
significant generalisation improvements by increasing the shrinkage
coefficient, closing the generalisation gap entirely in both Logistic
Regression and Deep Neural Network experiments. Finally, we show that other
popular modifications to adaptive methods, such as decoupled weight decay and
partial adaptivity can be shown to calibrate parameter updates to make better
use of sharper, more reliable directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1"&gt;Diego Granziol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xingchen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Albanie_S/0/1/0/all/0/1"&gt;Samuel Albanie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Redundancy in Reinforcement Learning. (arXiv:2102.11329v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11329</id>
        <link href="http://arxiv.org/abs/2102.11329"/>
        <updated>2021-07-27T02:03:36.672Z</updated>
        <summary type="html"><![CDATA[Maximum Entropy (MaxEnt) reinforcement learning is a powerful learning
paradigm which seeks to maximize return under entropy regularization. However,
action entropy does not necessarily coincide with state entropy, e.g., when
multiple actions produce the same transition. Instead, we propose to maximize
the transition entropy, i.e., the entropy of next states. We show that
transition entropy can be described by two terms; namely, model-dependent
transition entropy and action redundancy. Particularly, we explore the latter
in both deterministic and stochastic settings and develop tractable
approximation methods in a near model-free setup. We construct algorithms to
minimize action redundancy and demonstrate their effectiveness on a synthetic
environment with multiple redundant actions as well as contemporary benchmarks
in Atari and Mujoco. Our results suggest that action redundancy is a
fundamental problem in reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baram_N/0/1/0/all/0/1"&gt;Nir Baram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1"&gt;Guy Tennenholtz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design Space for Graph Neural Networks. (arXiv:2011.08843v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08843</id>
        <link href="http://arxiv.org/abs/2011.08843"/>
        <updated>2021-07-27T02:03:36.665Z</updated>
        <summary type="html"><![CDATA[The rapid evolution of Graph Neural Networks (GNNs) has led to a growing
number of new architectures as well as novel applications. However, current
research focuses on proposing and evaluating specific architectural designs of
GNNs, as opposed to studying the more general design space of GNNs that
consists of a Cartesian product of different design dimensions, such as the
number of layers or the type of the aggregation function. Additionally, GNN
designs are often specialized to a single task, yet few efforts have been made
to understand how to quickly find the best GNN design for a novel task or a
novel dataset. Here we define and systematically study the architectural design
space for GNNs which consists of 315,000 different designs over 32 different
predictive tasks. Our approach features three key innovations: (1) A general
GNN design space; (2) a GNN task space with a similarity metric, so that for a
given novel task/dataset, we can quickly identify/transfer the best performing
architecture; (3) an efficient and effective design space evaluation method
which allows insights to be distilled from a huge number of model-task
combinations. Our key results include: (1) A comprehensive set of guidelines
for designing well-performing GNNs; (2) while best GNN designs for different
tasks vary significantly, the GNN task space allows for transferring the best
designs across different tasks; (3) models discovered using our design space
achieve state-of-the-art performance. Overall, our work offers a principled and
scalable approach to transition from studying individual GNN designs for
specific tasks, to systematically studying the GNN design space and the task
space. Finally, we release GraphGym, a powerful platform for exploring
different GNN designs and tasks. GraphGym features modularized GNN
implementation, standardized GNN evaluation, and reproducible and scalable
experiment management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jiaxuan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rex Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04529</id>
        <link href="http://arxiv.org/abs/2103.04529"/>
        <updated>2021-07-27T02:03:36.643Z</updated>
        <summary type="html"><![CDATA[We introduce Self-supervised Online Reward Shaping (SORS), which aims to
improve the sample efficiency of any RL algorithm in sparse-reward environments
by automatically densifying rewards. The proposed framework alternates between
classification-based reward inference and policy update steps -- the original
sparse reward provides a self-supervisory signal for reward inference by
ranking trajectories that the agent observes, while the policy update is
performed with the newly inferred, typically dense reward function. We
introduce theory that shows that, under certain conditions, this alteration of
the reward function will not change the optimal policy of the original MDP,
while potentially increasing learning speed significantly. Experimental results
on several sparse-reward environments demonstrate that, across multiple
domains, the proposed algorithm is not only significantly more sample efficient
than a standard RL baseline using sparse rewards, but, at times, also achieves
similar sample efficiency compared to when hand-designed dense reward functions
are used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1"&gt;Farzan Memarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1"&gt;Wonjoon Goo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1"&gt;Rudolf Lioutikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive Monitoring with Logic-Calibrated Uncertainty for Cyber-Physical Systems. (arXiv:2011.00384v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00384</id>
        <link href="http://arxiv.org/abs/2011.00384"/>
        <updated>2021-07-27T02:03:36.635Z</updated>
        <summary type="html"><![CDATA[Predictive monitoring -- making predictions about future states and
monitoring if the predicted states satisfy requirements -- offers a promising
paradigm in supporting the decision making of Cyber-Physical Systems (CPS).
Existing works of predictive monitoring mostly focus on monitoring individual
predictions rather than sequential predictions. We develop a novel approach for
monitoring sequential predictions generated from Bayesian Recurrent Neural
Networks (RNNs) that can capture the inherent uncertainty in CPS, drawing on
insights from our study of real-world CPS datasets. We propose a new logic
named \emph{Signal Temporal Logic with Uncertainty} (STL-U) to monitor a
flowpipe containing an infinite set of uncertain sequences predicted by
Bayesian RNNs. We define STL-U strong and weak satisfaction semantics based on
if all or some sequences contained in a flowpipe satisfy the requirement. We
also develop methods to compute the range of confidence levels under which a
flowpipe is guaranteed to strongly (weakly) satisfy an STL-U formula.
Furthermore, we develop novel criteria that leverage STL-U monitoring results
to calibrate the uncertainty estimation in Bayesian RNNs. Finally, we evaluate
the proposed approach via experiments with real-world datasets and a simulated
smart city case study, which show very encouraging results of STL-U based
predictive monitoring approach outperforming baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Meiyi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1"&gt;John Stankovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1"&gt;Ezio Bartocci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lu Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Theory Based Methods for RKHS Bandits. (arXiv:2010.12167v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12167</id>
        <link href="http://arxiv.org/abs/2010.12167"/>
        <updated>2021-07-27T02:03:36.628Z</updated>
        <summary type="html"><![CDATA[The RKHS bandit problem (also called kernelized multi-armed bandit problem)
is an online optimization problem of non-linear functions with noisy feedback.
Although the problem has been extensively studied, there are unsatisfactory
results for some problems compared to the well-studied linear bandit case.
Specifically, there is no general algorithm for the adversarial RKHS bandit
problem. In addition, high computational complexity of existing algorithms
hinders practical application. We address these issues by considering a novel
amalgamation of approximation theory and the misspecified linear bandit
problem. Using an approximation method, we propose efficient algorithms for the
stochastic RKHS bandit problem and the first general algorithm for the
adversarial RKHS bandit problem. Furthermore, we empirically show that one of
our proposed methods has comparable cumulative regret to IGP-UCB and its
running time is much shorter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takemori_S/0/1/0/all/0/1"&gt;Sho Takemori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1"&gt;Masahiro Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at Codistillation for Distributed Training. (arXiv:2010.02838v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02838</id>
        <link href="http://arxiv.org/abs/2010.02838"/>
        <updated>2021-07-27T02:03:36.618Z</updated>
        <summary type="html"><![CDATA[Codistillation has been proposed as a mechanism to share knowledge among
concurrently trained models by encouraging them to represent the same function
through an auxiliary loss. This contrasts with the more commonly used
fully-synchronous data-parallel stochastic gradient descent methods, where
different model replicas average their gradients (or parameters) at every
iteration and thus maintain identical parameters. We investigate codistillation
in a distributed training setup, complementing previous work which focused on
extremely large batch sizes. Surprisingly, we find that even at moderate batch
sizes, models trained with codistillation can perform as well as models trained
with synchronous data-parallel methods, despite using a much weaker
synchronization mechanism. These findings hold across a range of batch sizes
and learning rate schedules, as well as different kinds of models and datasets.
Obtaining this level of accuracy, however, requires properly accounting for the
regularization effect of codistillation, which we highlight through several
empirical observations. Overall, this work contributes to a better
understanding of codistillation and how to best take advantage of it in a
distributed computing environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1"&gt;Shagun Sodhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1"&gt;Olivier Delalleau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1"&gt;Mahmoud Assran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1"&gt;Koustuv Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Multiple-Instance Data Classification with Costly Features. (arXiv:1911.08756v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08756</id>
        <link href="http://arxiv.org/abs/1911.08756"/>
        <updated>2021-07-27T02:03:36.595Z</updated>
        <summary type="html"><![CDATA[We extend the framework of Classification with Costly Features (CwCF) that
works with samples of fixed dimensions to trees of varying depth and breadth
(similar to a JSON/XML file). In this setting, the sample is a tree - sets of
sets of features. Individually for each sample, the task is to sequentially
select informative features that help the classification. Each feature has a
real-valued cost, and the objective is to maximize accuracy while minimizing
the total cost. The process is modeled as an MDP where the states represent the
acquired features, and the actions select unknown features. We present a
specialized neural network architecture trained through deep reinforcement
learning that naturally fits the data and directly selects features in the
tree. We demonstrate our method in seven datasets and compare it to two
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Janisch_J/0/1/0/all/0/1"&gt;Jarom&amp;#xed;r Janisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1"&gt;Tom&amp;#xe1;&amp;#x161; Pevn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1"&gt;Viliam Lis&amp;#xfd;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12573</id>
        <link href="http://arxiv.org/abs/2012.12573"/>
        <updated>2021-07-27T02:03:36.587Z</updated>
        <summary type="html"><![CDATA[Health literacy has emerged as a crucial factor in making appropriate health
decisions and ensuring treatment outcomes. However, medical jargon and the
complex structure of professional language in this domain make health
information especially hard to interpret. Thus, there is an urgent unmet need
for automated methods to enhance the accessibility of the biomedical literature
to the general population. This problem can be framed as a type of translation
problem between the language of healthcare professionals, and that of the
general public. In this paper, we introduce the novel task of automated
generation of lay language summaries of biomedical scientific reviews, and
construct a dataset to support the development and evaluation of automated
methods through which to enhance the accessibility of the biomedical
literature. We conduct analyses of the various challenges in solving this task,
including not only summarization of the key points but also explanation of
background knowledge and simplification of professional language. We experiment
with state-of-the-art summarization models as well as several data augmentation
techniques, and evaluate their performance using both automated metrics and
human assessment. Results indicate that automatically generated summaries
produced using contemporary neural architectures can achieve promising quality
and readability as compared with reference summaries developed for the lay
public by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score
of 13.30). We also discuss the limitations of the current attempt, providing
insights and directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yue Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Wei Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Trevor Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Sparse Experience Replay for Lifelong Language Learning. (arXiv:2009.04891v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04891</id>
        <link href="http://arxiv.org/abs/2009.04891"/>
        <updated>2021-07-27T02:03:36.580Z</updated>
        <summary type="html"><![CDATA[Lifelong learning requires models that can continuously learn from sequential
streams of data without suffering catastrophic forgetting due to shifts in data
distributions. Deep learning models have thrived in the non-sequential learning
paradigm; however, when used to learn a sequence of tasks, they fail to retain
past knowledge and learn incrementally. We propose a novel approach to lifelong
learning of language tasks based on meta-learning with sparse experience replay
that directly optimizes to prevent forgetting. We show that under the realistic
setting of performing a single pass on a stream of tasks and without any task
identifiers, our method obtains state-of-the-art results on lifelong text
classification and relation extraction. We analyze the effectiveness of our
approach and further demonstrate its low computational and space complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1"&gt;Nithin Holla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1"&gt;Pushkar Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1"&gt;Helen Yannakoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1"&gt;Ekaterina Shutova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image compression optimized for 3D reconstruction by utilizing deep neural networks. (arXiv:2003.12618v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12618</id>
        <link href="http://arxiv.org/abs/2003.12618"/>
        <updated>2021-07-27T02:03:36.574Z</updated>
        <summary type="html"><![CDATA[Computer vision tasks are often expected to be executed on compressed images.
Classical image compression standards like JPEG 2000 are widely used. However,
they do not account for the specific end-task at hand. Motivated by works on
recurrent neural network (RNN)-based image compression and three-dimensional
(3D) reconstruction, we propose unified network architectures to solve both
tasks jointly. These joint models provide image compression tailored for the
specific task of 3D reconstruction. Images compressed by our proposed models,
yield 3D reconstruction performance superior as compared to using JPEG 2000
compression. Our models significantly extend the range of compression rates for
which 3D reconstruction is possible. We also show that this can be done highly
efficiently at almost no additional cost to obtain compression on top of the
computation already required for performing the 3D reconstruction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Golts_A/0/1/0/all/0/1"&gt;Alex Golts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schechner_Y/0/1/0/all/0/1"&gt;Yoav Y. Schechner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Settling the Robust Learnability of Mixtures of Gaussians. (arXiv:2011.03622v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03622</id>
        <link href="http://arxiv.org/abs/2011.03622"/>
        <updated>2021-07-27T02:03:36.567Z</updated>
        <summary type="html"><![CDATA[This work represents a natural coalescence of two important lines of work:
learning mixtures of Gaussians and algorithmic robust statistics. In particular
we give the first provably robust algorithm for learning mixtures of any
constant number of Gaussians. We require only mild assumptions on the mixing
weights (bounded fractionality) and that the total variation distance between
components is bounded away from zero. At the heart of our algorithm is a new
method for proving dimension-independent polynomial identifiability through
applying a carefully chosen sequence of differential operations to certain
generating functions that not only encode the parameters we would like to learn
but also the system of polynomial equations we would like to solve. We show how
the symbolic identities we derive can be directly used to analyze a natural
sum-of-squares relaxation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Trust Intervals for Out of Distribution Detection. (arXiv:2102.01336v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01336</id>
        <link href="http://arxiv.org/abs/2102.01336"/>
        <updated>2021-07-27T02:03:36.544Z</updated>
        <summary type="html"><![CDATA[Building neural network classifiers with an ability to distinguish between in
and out-of distribution inputs is an important step towards faithful deep
learning systems. Some of the successful approaches for this, resort to
architectural novelties, such as ensembles, with increased complexities in
terms of the number of parameters and training procedures. Whereas some other
approaches make use of surrogate samples, which are easy to create and work as
proxies for actual out-of-distribution (OOD) samples, to train the networks for
OOD detection. In this paper, we propose a very simple approach for enhancing
the ability of a pretrained network to detect OOD inputs without even altering
the original parameter values. We define a probabilistic trust interval for
each weight parameter of the network and optimize its size according to the
in-distribution (ID) inputs. It allows the network to sample additional weight
values along with the original values at the time of inference and use the
observed disagreement among the corresponding outputs for OOD detection. In
order to capture the disagreement effectively, we also propose a measure and
establish its suitability using empirical evidence. Our approach outperforms
the existing state-of-the-art methods on various OOD datasets by considerable
margins without using any real or surrogate OOD samples. We also analyze the
performance of our approach on adversarial and corrupted inputs such as
CIFAR-10-C and demonstrate its ability to clearly distinguish such inputs as
well. By using fundamental theorem of calculus on neural networks, we explain
why our technique doesn't need to observe OOD samples during training to
achieve results better than the previous works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1"&gt;Deepak Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis of LIME for Text Data. (arXiv:2010.12487v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12487</id>
        <link href="http://arxiv.org/abs/2010.12487"/>
        <updated>2021-07-27T02:03:36.538Z</updated>
        <summary type="html"><![CDATA[Text data are increasingly handled in an automated fashion by machine
learning algorithms. But the models handling these data are not always
well-understood due to their complexity and are more and more often referred to
as "black-boxes." Interpretability methods aim to explain how these models
operate. Among them, LIME has become one of the most popular in recent years.
However, it comes without theoretical guarantees: even for simple models, we
are not sure that LIME behaves accurately. In this paper, we provide a first
theoretical analysis of LIME for text data. As a consequence of our theoretical
findings, we show that LIME indeed provides meaningful explanations for simple
models, namely decision trees and linear models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mardaoui_D/0/1/0/all/0/1"&gt;Dina Mardaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1"&gt;Damien Garreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Automated Diagnosis of COVID-19 Based on Scanning Images. (arXiv:2006.05245v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05245</id>
        <link href="http://arxiv.org/abs/2006.05245"/>
        <updated>2021-07-27T02:03:36.531Z</updated>
        <summary type="html"><![CDATA[The pandemic of COVID-19 has caused millions of infections, which has led to
a great loss all over the world, socially and economically. Due to the
false-negative rate and the time-consuming of the conventional Reverse
Transcription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on
X-ray images and Computed Tomography (CT) images has been widely adopted.
Therefore, researchers of the computer vision area have developed many
automatic diagnosing models based on machine learning or deep learning to
assist the radiologists and improve the diagnosing accuracy. In this paper, we
present a review of these recently emerging automatic diagnosing models. 70
models proposed from February 14, 2020, to July 21, 2020, are involved. We
analyzed the models from the perspective of preprocessing, feature extraction,
classification, and evaluation. Based on the limitation of existing models, we
pointed out that domain adaption in transfer learning and interpretability
promotion would be the possible future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1"&gt;Delong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shunhui Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xinyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dark (and Bright) Side of IoT: Attacks and Countermeasures for Identifying Smart Home Devices and Services. (arXiv:2009.07672v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07672</id>
        <link href="http://arxiv.org/abs/2009.07672"/>
        <updated>2021-07-27T02:03:36.525Z</updated>
        <summary type="html"><![CDATA[We present a new machine learning-based attack that exploits network patterns
to detect the presence of smart IoT devices and running services in the WiFi
radio spectrum. We perform an extensive measurement campaign of data
collection, and we build up a model describing the traffic patterns
characterizing three popular IoT smart home devices, i.e., Google Nest Mini,
Amazon Echo, and Amazon Echo Dot. We prove that it is possible to detect and
identify with overwhelming probability their presence and the services running
by the aforementioned devices in a crowded WiFi scenario. This work proves that
standard encryption techniques alone are not sufficient to protect the privacy
of the end-user, since the network traffic itself exposes the presence of both
the device and the associated service. While more work is required to prevent
non-trusted third parties to detect and identify the user's devices, we
introduce Eclipse, a technique to mitigate these types of attacks, which
reshapes the traffic making the identification of the devices and the
associated services similar to the random classification baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Ahmed Mohamed Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oligeri_G/0/1/0/all/0/1"&gt;Gabriele Oligeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voigt_T/0/1/0/all/0/1"&gt;Thiemo Voigt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics Engine for Tensegrity Robots. (arXiv:2011.04929v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04929</id>
        <link href="http://arxiv.org/abs/2011.04929"/>
        <updated>2021-07-27T02:03:36.518Z</updated>
        <summary type="html"><![CDATA[Learning policies in simulation is promising for reducing human effort when
training robot controllers. This is especially true for soft robots that are
more adaptive and safe but also more difficult to accurately model and control.
The sim2real gap is the main barrier to successfully transfer policies from
simulation to a real robot. System identification can be applied to reduce this
gap but traditional identification methods require a lot of manual tuning.
Data-driven alternatives can tune dynamical models directly from data but are
often data hungry, which also incorporates human effort in collecting data.
This work proposes a data-driven, end-to-end differentiable simulator focused
on the exciting but challenging domain of tensegrity robots. To the best of the
authors' knowledge, this is the first differentiable physics engine for
tensegrity robots that supports cable, contact, and actuation modeling. The aim
is to develop a reasonably simplified, data-driven simulation, which can learn
approximate dynamics with limited ground truth data. The dynamics must be
accurate enough to generate policies that can be transferred back to the
ground-truth system. As a first step in this direction, the current work
demonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts
as a ground truth system. Two different tensegrity robots are used for
evaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity.
The results indicate that only 0.25\% of ground truth data are needed to train
a policy that works on the ground truth system when the differentiable engine
is used for training against training the policy directly on the ground truth
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aanjaneya_M/0/1/0/all/0/1"&gt;Mridul Aanjaneya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1"&gt;Kostas Bekris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[INSTA-YOLO: Real-Time Instance Segmentation. (arXiv:2102.06777v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06777</id>
        <link href="http://arxiv.org/abs/2102.06777"/>
        <updated>2021-07-27T02:03:36.494Z</updated>
        <summary type="html"><![CDATA[Instance segmentation has gained recently huge attention in various computer
vision applications. It aims at providing different IDs to different objects of
the scene, even if they belong to the same class. Instance segmentation is
usually performed as a two-stage pipeline. First, an object is detected, then
semantic segmentation within the detected box area is performed which involves
costly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage
end-to-end deep learning model for real-time instance segmentation. Instead of
pixel-wise prediction, our model predicts instances as object contours
represented by 2D points in Cartesian space. We evaluate our model on three
datasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the
state-of-the-art models for instance segmentation. The results show our model
achieves competitive accuracy in terms of mAP at twice the speed on GTX-1080
GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1"&gt;Abdelrahman Shaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadhoud_M/0/1/0/all/0/1"&gt;Mayada Hadhoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09046</id>
        <link href="http://arxiv.org/abs/2001.09046"/>
        <updated>2021-07-27T02:03:36.473Z</updated>
        <summary type="html"><![CDATA[We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.

Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.

We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.

We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1"&gt;Bart Smets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1"&gt;Jim Portegies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1"&gt;Remco Duits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11697</id>
        <link href="http://arxiv.org/abs/2010.11697"/>
        <updated>2021-07-27T02:03:36.466Z</updated>
        <summary type="html"><![CDATA[Iconography in art is the discipline that studies the visual content of
artworks to determine their motifs and themes andto characterize the way these
are represented. It is a subject of active research for a variety of purposes,
including the interpretation of meaning, the investigation of the origin and
diffusion in time and space of representations, and the study of influences
across artists and art works. With the proliferation of digital archives of art
images, the possibility arises of applying Computer Vision techniques to the
analysis of art images at an unprecedented scale, which may support iconography
research and education. In this paper we introduce a novel paintings data set
for iconography classification and present the quantitativeand qualitative
results of applying a Convolutional Neural Network (CNN) classifier to the
recognition of the iconography of artworks. The proposed classifier achieves
good performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%
Average Precision) in the task of identifying saints in Christian religious
paintings, a task made difficult by the presence of classes with very similar
visual features. Qualitative analysis of the results shows that the CNN focuses
on the traditional iconic motifs that characterize the representation of each
saint and exploits such hints to attain correct identification. The ultimate
goal of our work is to enable the automatic extraction, decomposition, and
comparison of iconography elements to support iconographic studies and
automatic art work annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1"&gt;Federico Milani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraternali_P/0/1/0/all/0/1"&gt;Piero Fraternali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit bias of gradient descent for mean squared error regression with wide neural networks. (arXiv:2006.07356v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07356</id>
        <link href="http://arxiv.org/abs/2006.07356"/>
        <updated>2021-07-27T02:03:36.459Z</updated>
        <summary type="html"><![CDATA[We investigate gradient descent training of wide neural networks and the
corresponding implicit bias in function space. For univariate regression, we
show that the solution of training a width-$n$ shallow ReLU network is within
$n^{- 1/2}$ of the function which fits the training data and whose difference
from the initial function has the smallest 2-norm of the second derivative
weighted by a curvature penalty that depends on the probability distribution
that is used to initialize the network parameters. We compute the curvature
penalty function explicitly for various common initialization procedures. For
instance, asymmetric initialization with a uniform distribution yields a
constant curvature penalty, and thence the solution function is the natural
cubic spline interpolation of the training data. We obtain a similar result for
different activation functions. For multivariate regression we show an
analogous result, whereby the second derivative is replaced by the Radon
transform of a fractional Laplacian. For initialization schemes that yield a
constant penalty function, the solutions are polyharmonic splines. Moreover, we
show that the training trajectories are captured by trajectories of smoothing
splines with decreasing regularization strength.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hui Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Mont&amp;#xfa;far&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12137</id>
        <link href="http://arxiv.org/abs/2107.12137"/>
        <updated>2021-07-27T02:03:36.453Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects using
point cloud data. We present anchor design along with custom loss functions
used in this work. A combination of spatial and channel wise attention module
is used in this work. We use the Kitti 3D Birds Eye View dataset for
benchmarking and validating our results. Our method surpasses previous state of
the art in this domain both in terms of average precision and speed running at
> 30 FPS. Finally, we present the ablation study to demonstrate that the
performance of our network is generalizable. This makes it a feasible option to
be deployed in real time applications like self driving cars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Adaptive Refined Labeling. (arXiv:2107.11972v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11972</id>
        <link href="http://arxiv.org/abs/2107.11972"/>
        <updated>2021-07-27T02:03:36.447Z</updated>
        <summary type="html"><![CDATA[Price movement forecasting aims at predicting the future trends of financial
assets based on the current market conditions and other relevant information.
Recently, machine learning(ML) methods have become increasingly popular and
achieved promising results for price movement forecasting in both academia and
industry. Most existing ML solutions formulate the forecasting problem as a
classification(to predict the direction) or a regression(to predict the return)
problem in the entire set of training data. However, due to the extremely low
signal-to-noise ratio and stochastic nature of financial data, good trading
opportunities are extremely scarce. As a result, without careful selection of
potentially profitable samples, such ML methods are prone to capture the
patterns of noises instead of real signals. To address the above issues, we
propose a novel framework-LARA(Locality-Aware Attention and Adaptive Refined
Labeling), which contains the following three components: 1)Locality-aware
attention automatically extracts the potentially profitable samples by
attending to their label information in order to construct a more accurate
classifier on these selected samples. 2)Adaptive refined labeling further
iteratively refines the labels, alleviating the noise of samples. 3)Equipped
with metric learning techniques, Locality-aware attention enjoys task-specific
distance metrics and distributes attention on potentially profitable samples in
a more effective way. To validate our method, we conduct comprehensive
experiments on three real-world financial markets: ETFs, the China's A-share
stock market, and the cryptocurrency market. LARA achieves superior performance
compared with the time-series analysis methods and a set of machine learning
based competitors on the Qlib platform. Extensive ablation studies and
experiments demonstrate that LARA indeed captures more reliable trading
opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1"&gt;Liang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1"&gt;Hui Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruchen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zhonghao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1"&gt;Dewei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Ling Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Bounds in the Predict-then-Optimize Framework. (arXiv:1905.11488v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11488</id>
        <link href="http://arxiv.org/abs/1905.11488"/>
        <updated>2021-07-27T02:03:36.422Z</updated>
        <summary type="html"><![CDATA[The predict-then-optimize framework is fundamental in many practical
settings: predict the unknown parameters of an optimization problem, and then
solve the problem using the predicted values of the parameters. A natural loss
function in this environment is to consider the cost of the decisions induced
by the predicted parameters, in contrast to the prediction error of the
parameters. This loss function was recently introduced in Elmachtoub and Grigas
(2017) and referred to as the Smart Predict-then-Optimize (SPO) loss. In this
work, we seek to provide bounds on how well the performance of a prediction
model fit on training data generalizes out-of-sample, in the context of the SPO
loss. Since the SPO loss is non-convex and non-Lipschitz, standard results for
deriving generalization bounds do not apply.

We first derive bounds based on the Natarajan dimension that, in the case of
a polyhedral feasible region, scale at most logarithmically in the number of
extreme points, but, in the case of a general convex feasible region, have
linear dependence on the decision dimension. By exploiting the structure of the
SPO loss function and a key property of the feasible region, which we denote as
the strength property, we can dramatically improve the dependence on the
decision and feature dimensions. Our approach and analysis rely on placing a
margin around problematic predictions that do not yield unique optimal
solutions, and then providing generalization bounds in the context of a
modified margin SPO loss function that is Lipschitz continuous. Finally, we
characterize the strength property and show that the modified SPO loss can be
computed efficiently for both strongly convex bodies and polytopes with an
explicit extreme point representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balghiti_O/0/1/0/all/0/1"&gt;Othman El Balghiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elmachtoub_A/0/1/0/all/0/1"&gt;Adam N. Elmachtoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigas_P/0/1/0/all/0/1"&gt;Paul Grigas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ambuj Tewari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study on Speech Enhancement Based on Diffusion Probabilistic Model. (arXiv:2107.11876v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.11876</id>
        <link href="http://arxiv.org/abs/2107.11876"/>
        <updated>2021-07-27T02:03:36.411Z</updated>
        <summary type="html"><![CDATA[Diffusion probabilistic models have demonstrated an outstanding capability to
model natural images and raw audio waveforms through a paired diffusion and
reverse processes. The unique property of the reverse process (namely,
eliminating non-target signals from the Gaussian noise and noisy signals) could
be utilized to restore clean signals. Based on this property, we propose a
diffusion probabilistic model-based speech enhancement (DiffuSE) model that
aims to recover clean speech signals from noisy signals. The fundamental
architecture of the proposed DiffuSE model is similar to that of DiffWave--a
high-quality audio waveform generation model that has a relatively low
computational cost and footprint. To attain better enhancement performance, we
designed an advanced reverse process, termed the supportive reverse process,
which adds noisy speech in each time-step to the predicted speech. The
experimental results show that DiffuSE yields performance that is comparable to
related audio generative models on the standardized Voice Bank corpus SE task.
Moreover, relative to the generally suggested full sampling schedule, the
proposed supportive reverse process especially improved the fast sampling,
taking few steps to yield better enhancement results over the conventional full
step inference process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yen-Ju Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12048</id>
        <link href="http://arxiv.org/abs/2107.12048"/>
        <updated>2021-07-27T02:03:36.404Z</updated>
        <summary type="html"><![CDATA[Decentralized federated learning (DFL) is a powerful framework of distributed
machine learning and decentralized stochastic gradient descent (SGD) is a
driving engine for DFL. The performance of decentralized SGD is jointly
influenced by communication-efficiency and convergence rate. In this paper, we
propose a general decentralized federated learning framework to strike a
balance between communication-efficiency and convergence performance. The
proposed framework performs both multiple local updates and multiple inter-node
communications periodically, unifying traditional decentralized SGD methods. We
establish strong convergence guarantees for the proposed DFL algorithm without
the assumption of convex objective function. The balance of communication and
computation rounds is essential to optimize decentralized federated learning
under constrained communication and computation resources. For further
improving communication-efficiency of DFL, compressed communication is applied
to DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL
exhibits linear convergence for strongly convex objectives. Experiment results
based on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over
traditional decentralized SGD methods and show that C-DFL further enhances
communication-efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenyi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedGroup: Efficient Clustered Federated Learning via Decomposed Data-Driven Measure. (arXiv:2010.06870v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06870</id>
        <link href="http://arxiv.org/abs/2010.06870"/>
        <updated>2021-07-27T02:03:36.390Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) enables the multiple participating devices to
collaboratively contribute to a global neural network model while keeping the
training data locally. Unlike the centralized training setting, the non-IID and
imbalanced (statistical heterogeneity) training data of FL is distributed in
the federated network, which will increase the divergences between the local
models and global model, further degrading performance. In this paper, we
propose a novel clustered federated learning (CFL) framework FedGroup, in which
we 1) group the training of clients based on the similarities between the
clients' optimization directions for high training performance; 2) construct a
new data-driven distance measure to improve the efficiency of the client
clustering procedure. 3) implement a newcomer device cold start mechanism based
on the auxiliary global model for framework scalability and practicality.

FedGroup can achieve improvements by dividing joint optimization into groups
of sub-optimization and can be combined with FL optimizer FedProx. The
convergence and complexity are analyzed to demonstrate the efficiency of our
proposed framework. We also evaluate FedGroup and FedGrouProx (combined with
FedProx) on several open datasets and made comparisons with related CFL
frameworks. The results show that FedGroup can significantly improve absolute
test accuracy by +14.1% on FEMNIST compared to FedAvg. +3.4% on Sentiment140
compared to FedProx, +6.9% on MNIST compared to FeSEM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1"&gt;Moming Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Duo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xinyuan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Renping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Liang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xianzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yujuan Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12079</id>
        <link href="http://arxiv.org/abs/2107.12079"/>
        <updated>2021-07-27T02:03:36.382Z</updated>
        <summary type="html"><![CDATA[Dialogue systems are widely used in AI to support timely and interactive
communication with users. We propose a general-purpose dialogue system
architecture that leverages computational argumentation and state-of-the-art
language technologies. We illustrate and evaluate the system using a COVID-19
vaccine information case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1"&gt;Bettina Fazzinga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1"&gt;Andrea Galassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1"&gt;Paolo Torroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks. (arXiv:2107.11889v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11889</id>
        <link href="http://arxiv.org/abs/2107.11889"/>
        <updated>2021-07-27T02:03:36.357Z</updated>
        <summary type="html"><![CDATA[While graph neural networks (GNNs) have been shown to perform well on
graph-based data from a variety of fields, they suffer from a lack of
transparency and accountability, which hinders trust and consequently the
deployment of such models in high-stake and safety-critical scenarios. Even
though recent research has investigated methods for explaining GNNs, these
methods are limited to single-instance explanations, also known as local
explanations. Motivated by the aim of providing global explanations, we adapt
the well-known Automated Concept-based Explanation approach (Ghorbani et al.,
2019) to GNN node and graph classification, and propose GCExplainer.
GCExplainer is an unsupervised approach for post-hoc discovery and extraction
of global concept-based explanations for GNNs, which puts the human in the
loop. We demonstrate the success of our technique on five node classification
datasets and two graph classification datasets, showing that we are able to
discover and extract high-quality concept representations by putting the human
in the loop. We achieve a maximum completeness score of 1 and an average
completeness score of 0.753 across the datasets. Finally, we show that the
concept-based explanations provide an improved insight into the datasets and
GNN models compared to the state-of-the-art explanations produced by
GNNExplainer (Ying et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1"&gt;Lucie Charlotte Magister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vikash Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of Outside-Vehicle Objects. (arXiv:2107.12167v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.12167</id>
        <link href="http://arxiv.org/abs/2107.12167"/>
        <updated>2021-07-27T02:03:36.350Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in more intelligent natural user interaction with
the car. Hand gestures and speech are already being applied for driver-car
interaction. Moreover, multimodal approaches are also showing promise in the
automotive industry. In this paper, we utilize deep learning for a multimodal
fusion network for referencing objects outside the vehicle. We use features
from gaze, head pose and finger pointing simultaneously to precisely predict
the referenced objects in different car poses. We demonstrate the practical
limitations of each modality when used for a natural form of referencing,
specifically inside the car. As evident from our results, we overcome the
modality specific limitations, to a large extent, by the addition of other
modalities. This work highlights the importance of multimodal sensing,
especially when moving towards natural user interaction. Furthermore, our user
based analysis shows noteworthy differences in recognition of user behavior
depending upon the vehicle pose.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1"&gt;Abdul Rafey Aftab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1"&gt;Michael von der Beeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrhirsch_S/0/1/0/all/0/1"&gt;Steven Rohrhirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diotte_B/0/1/0/all/0/1"&gt;Benoit Diotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1"&gt;Michael Feld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. (arXiv:2107.12045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12045</id>
        <link href="http://arxiv.org/abs/2107.12045"/>
        <updated>2021-07-27T02:03:36.344Z</updated>
        <summary type="html"><![CDATA[Context: Machine Learning (ML) has been at the heart of many innovations over
the past years. However, including it in so-called 'safety-critical' systems
such as automotive or aeronautic has proven to be very challenging, since the
shift in paradigm that ML brings completely changes traditional certification
approaches.

Objective: This paper aims to elucidate challenges related to the
certification of ML-based safety-critical systems, as well as the solutions
that are proposed in the literature to tackle them, answering the question 'How
to Certify Machine Learning Based Safety-critical Systems?'.

Method: We conduct a Systematic Literature Review (SLR) of research papers
published between 2015 to 2020, covering topics related to the certification of
ML systems. In total, we identified 229 papers covering topics considered to be
the main pillars of ML certification: Robustness, Uncertainty, Explainability,
Verification, Safe Reinforcement Learning, and Direct Certification. We
analyzed the main trends and problems of each sub-field and provided summaries
of the papers extracted.

Results: The SLR results highlighted the enthusiasm of the community for this
subject, as well as the lack of diversity in terms of datasets and type of
models. It also emphasized the need to further develop connections between
academia and industries to deepen the domain study. Finally, it also
illustrated the necessity to build connections between the above mention main
pillars that are for now mainly studied separately.

Conclusion: We highlighted current efforts deployed to enable the
certification of ML based software systems, and discuss some future research
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1"&gt;Florian Tambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1"&gt;Gabriel Laberge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1"&gt;Le An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mindom_P/0/1/0/all/0/1"&gt;Paulina Stevia Nouwou Mindom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1"&gt;Yann Pequignot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1"&gt;Giulio Antoniol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1"&gt;Ettore Merlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laviolette_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Laviolette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14353</id>
        <link href="http://arxiv.org/abs/2012.14353"/>
        <updated>2021-07-27T02:03:36.331Z</updated>
        <summary type="html"><![CDATA[The exponential growths of social media and micro-blogging sites not only
provide platforms for empowering freedom of expressions and individual voices,
but also enables people to express anti-social behavior like online harassment,
cyberbullying, and hate speech. Numerous works have been proposed to utilize
textual data for social and anti-social behavior analysis, by predicting the
contexts mostly for highly-resourced languages like English. However, some
languages are under-resourced, e.g., South Asian languages like Bengali, that
lack computational resources for accurate natural language processing (NLP). In
this paper, we propose an explainable approach for hate speech detection from
the under-resourced Bengali language, which we called DeepHateExplainer.
Bengali texts are first comprehensively preprocessed, before classifying them
into political, personal, geopolitical, and religious hates using a neural
ensemble method of transformer-based neural architectures (i.e., monolingual
Bangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa).
Important~(most and least) terms are then identified using sensitivity analysis
and layer-wise relevance propagation~(LRP), before providing
human-interpretable explanations. Finally, we compute comprehensiveness and
sufficiency scores to measure the quality of explanations w.r.t faithfulness.
Evaluations against machine learning~(linear and tree-based models) and neural
networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines
yield F1-scores of 78%, 91%, 89%, and 84%, for political, personal,
geopolitical, and religious hates, respectively, outperforming both ML and DNN
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Md. Rezaul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1"&gt;Sumon Kanti Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tanhim Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1"&gt;Sagor Sarker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1"&gt;Mehadi Hasan Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1"&gt;Kabir Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md. Azam Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1"&gt;Stefan Decker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning. (arXiv:2107.12173v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.12173</id>
        <link href="http://arxiv.org/abs/2107.12173"/>
        <updated>2021-07-27T02:03:36.323Z</updated>
        <summary type="html"><![CDATA[An over-the-air membership inference attack (MIA) is presented to leak
private information from a wireless signal classifier. Machine learning (ML)
provides powerful means to classify wireless signals, e.g., for PHY-layer
authentication. As an adversarial machine learning attack, the MIA infers
whether a signal of interest has been used in the training data of a target
classifier. This private information incorporates waveform, channel, and device
characteristics, and if leaked, can be exploited by an adversary to identify
vulnerabilities of the underlying ML model (e.g., to infiltrate the PHY-layer
authentication). One challenge for the over-the-air MIA is that the received
signals and consequently the RF fingerprints at the adversary and the intended
receiver differ due to the discrepancy in channel conditions. Therefore, the
adversary first builds a surrogate classifier by observing the spectrum and
then launches the black-box MIA on this classifier. The MIA results show that
the adversary can reliably infer signals (and potentially the radio and channel
information) used to build the target classifier. Therefore, a proactive
defense is developed against the MIA by building a shadow MIA model and fooling
the adversary. This defense can successfully reduce the MIA accuracy and
prevent information leakage from the wireless signal classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yi Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1"&gt;Yalin E. Sagduyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining Local, Global, And Higher-Order Interactions In Deep Learning. (arXiv:2006.08601v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08601</id>
        <link href="http://arxiv.org/abs/2006.08601"/>
        <updated>2021-07-27T02:03:36.297Z</updated>
        <summary type="html"><![CDATA[We present a simple yet highly generalizable method for explaining
interacting parts within a neural network's reasoning process. First, we design
an algorithm based on cross derivatives for computing statistical interaction
effects between individual features, which is generalized to both 2-way and
higher-order (3-way or more) interactions. We present results side by side with
a weight-based attribution technique, corroborating that cross derivatives are
a superior metric for both 2-way and higher-order interaction detection.
Moreover, we extend the use of cross derivatives as an explanatory device in
neural networks to the computer vision setting by expanding Grad-CAM, a popular
gradient-based explanatory tool for CNNs, to the higher order. While Grad-CAM
can only explain the importance of individual objects in images, our method,
which we call Taylor-CAM, can explain a neural network's relational reasoning
across multiple objects. We show the success of our explanations both
qualitatively and quantitatively, including with a user study. We will release
all code as a tool package to facilitate explainable deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lerman_S/0/1/0/all/0/1"&gt;Samuel Lerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venuto_C/0/1/0/all/0/1"&gt;Charles Venuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_H/0/1/0/all/0/1"&gt;Henry Kautz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Maximum-Likelihood with Deep Learning for Event Reconstruction in IceCube. (arXiv:2107.12110v1 [astro-ph.HE])]]></title>
        <id>http://arxiv.org/abs/2107.12110</id>
        <link href="http://arxiv.org/abs/2107.12110"/>
        <updated>2021-07-27T02:03:36.290Z</updated>
        <summary type="html"><![CDATA[The field of deep learning has become increasingly important for particle
physics experiments, yielding a multitude of advances, predominantly in event
classification and reconstruction tasks. Many of these applications have been
adopted from other domains. However, data in the field of physics are unique in
the context of machine learning, insofar as their generation process and the
laws and symmetries they abide by are usually well understood. Most commonly
used deep learning architectures fail at utilizing this available information.
In contrast, more traditional likelihood-based methods are capable of
exploiting domain knowledge, but they are often limited by computational
complexity. In this contribution, a hybrid approach is presented that utilizes
generative neural networks to approximate the likelihood, which may then be
used in a traditional maximum-likelihood setting. Domain knowledge, such as
invariances and detector characteristics, can easily be incorporated in this
approach. The hybrid approach is illustrated by the example of event
reconstruction in IceCube.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hunnefeld_M/0/1/0/all/0/1"&gt;Mirco H&amp;#xfc;nnefeld&lt;/a&gt; (for the IceCube Collaboration)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09451</id>
        <link href="http://arxiv.org/abs/2101.09451"/>
        <updated>2021-07-27T02:03:36.283Z</updated>
        <summary type="html"><![CDATA[Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Picket: Guarding Against Corrupted Data in Tabular Data during Learning and Inference. (arXiv:2006.04730v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04730</id>
        <link href="http://arxiv.org/abs/2006.04730"/>
        <updated>2021-07-27T02:03:36.269Z</updated>
        <summary type="html"><![CDATA[Data corruption is an impediment to modern machine learning deployments.
Corrupted data can severely bias the learned model and can also lead to invalid
inferences. We present, Picket, a simple framework to safeguard against data
corruptions during both training and deployment of machine learning models over
tabular data. For the training stage, Picket identifies and removes corrupted
data points from the training data to avoid obtaining a biased model. For the
deployment stage, Picket flags, in an online manner, corrupted query points to
a trained machine learning model that due to noise will result in incorrect
predictions. To detect corrupted data, Picket uses a self-supervised deep
learning model for mixed-type tabular data, which we call PicketNet. To
minimize the burden of deployment, learning a PicketNet model does not require
any human-labeled data. Picket is designed as a plugin that can increase the
robustness of any machine learning pipeline. We evaluate Picket on a diverse
array of real-world data considering different corruption models that include
systematic and adversarial noise during both training and testing. We show that
Picket consistently safeguards against corrupted data during both training and
deployment of various models ranging from SVMs to neural networks, beating a
diverse array of competing methods that span from data quality validation
models to robust outlier-detection models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhechun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1"&gt;Theodoros Rekatsinas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework. (arXiv:2107.12046v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.12046</id>
        <link href="http://arxiv.org/abs/2107.12046"/>
        <updated>2021-07-27T02:03:36.259Z</updated>
        <summary type="html"><![CDATA[Background: Glioma is the most common brain malignant tumor, with a high
morbidity rate and a mortality rate of more than three percent, which seriously
endangers human health. The main method of acquiring brain tumors in the clinic
is MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is
helpful for treatment inspection, post-diagnosis monitoring, and effect
evaluation of patients. However, the common operation in clinical brain tumor
segmentation is still manual segmentation, lead to its time-consuming and large
performance difference between different operators, a consistent and accurate
automatic segmentation method is urgently needed. Methods: To meet the above
challenges, we propose an automatic brain tumor MRI data segmentation framework
which is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is
added to each encoder, the Attention Guide Filter (AG) module is added to each
decoder, using the channel relationship to automatically enhance the useful
information in the channel to suppress the useless information, and use the
attention mechanism to guide the edge information and remove the influence of
irrelevant information such as noise. Results: We used the BraTS2020 challenge
online verification tool to evaluate our approach. The focus of verification is
that the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced
tumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI
images have different intensities, AGSE-VNet is not affected by the size of the
tumor, and can more accurately extract the features of the three regions, it
has achieved impressive results and made outstanding contributions to the
clinical diagnosis and treatment of brain tumor patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1"&gt;Xi Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jianming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiji Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaomei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1"&gt;Xiaobo Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning AI With Shared Human Values. (arXiv:2008.02275v5 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02275</id>
        <link href="http://arxiv.org/abs/2008.02275"/>
        <updated>2021-07-27T02:03:36.231Z</updated>
        <summary type="html"><![CDATA[We show how to assess a language model's knowledge of basic concepts of
morality. We introduce the ETHICS dataset, a new benchmark that spans concepts
in justice, well-being, duties, virtues, and commonsense morality. Models
predict widespread moral judgments about diverse text scenarios. This requires
connecting physical and social world knowledge to value judgements, a
capability that may enable us to steer chatbot outputs or eventually regularize
open-ended reinforcement learning agents. With the ETHICS dataset, we find that
current language models have a promising but incomplete ability to predict
basic human ethical judgements. Our work shows that progress can be made on
machine ethics today, and it provides a steppingstone toward AI that is aligned
with human values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1"&gt;Collin Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1"&gt;Andrew Critch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A vector-contraction inequality for Rademacher complexities using $p$-stable variables. (arXiv:1912.10136v2 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.10136</id>
        <link href="http://arxiv.org/abs/1912.10136"/>
        <updated>2021-07-27T02:03:36.225Z</updated>
        <summary type="html"><![CDATA[Andreas Maurer in the paper "A vector-contraction inequality for Rademacher
complexities" extended the contraction inequality for Rademacher averages to
Lipschitz functions with vector-valued domains; He did it replacing the
Rademacher variables in the bounding expression by arbitrary idd symmetric and
sub-gaussian variables. We will see how to extend this work when we replace
sub-gaussian variables by $p$-stable variables for $1<p<2$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zatarain_Vera_O/0/1/0/all/0/1"&gt;Oscar Zatarain-Vera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Regularized Locality Preserving Indexing for Fiedler Vector Estimation. (arXiv:2107.12070v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.12070</id>
        <link href="http://arxiv.org/abs/2107.12070"/>
        <updated>2021-07-27T02:03:36.218Z</updated>
        <summary type="html"><![CDATA[The Fiedler vector of a connected graph is the eigenvector associated with
the algebraic connectivity of the graph Laplacian and it provides substantial
information to learn the latent structure of a graph. In real-world
applications, however, the data may be subject to heavy-tailed noise and
outliers which results in deteriorations in the structure of the Fiedler vector
estimate. We design a Robust Regularized Locality Preserving Indexing (RRLPI)
method for Fiedler vector estimation that aims to approximate the nonlinear
manifold structure of the Laplace Beltrami operator while minimizing the
negative impact of outliers. First, an analysis of the effects of two
fundamental outlier types on the eigen-decomposition for block affinity
matrices which are essential in cluster analysis is conducted. Then, an error
model is formulated and a robust Fiedler vector estimation algorithm is
developed. An unsupervised penalty parameter selection algorithm is proposed
that leverages the geometric structure of the projection space to perform
robust regularized Fiedler estimation. The performance of RRLPI is benchmarked
against existing competitors in terms of detection probability, partitioning
quality, image segmentation capability, robustness and computation time using a
large variety of synthetic and real data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tastan_A/0/1/0/all/0/1"&gt;Aylin Tastan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muma_M/0/1/0/all/0/1"&gt;Michael Muma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zoubir_A/0/1/0/all/0/1"&gt;Abdelhak M. Zoubir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks. (arXiv:2009.12462v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12462</id>
        <link href="http://arxiv.org/abs/2009.12462"/>
        <updated>2021-07-27T02:03:36.212Z</updated>
        <summary type="html"><![CDATA[We focus on reinforcement learning (RL) in relational problems that are
naturally defined in terms of objects, their relations, and manipulations.
These problems are characterized by variable state and action spaces, and
finding a fixed-length representation, required by most existing RL methods, is
difficult, if not impossible. We present a deep RL framework based on graph
neural networks and auto-regressive policy decomposition that naturally works
with these problems and is completely domain-independent. We demonstrate the
framework in three very distinct domains and we report the method's competitive
performance and impressive zero-shot generalization over different problem
sizes. In goal-oriented BlockWorld, we demonstrate multi-parameter actions with
pre-conditions. In SysAdmin, we show how to select multiple objects
simultaneously. In the classical planning domain of Sokoban, the method trained
exclusively on 10x10 problems with three boxes solves 89% of 15x15 problems
with five boxes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Janisch_J/0/1/0/all/0/1"&gt;Jarom&amp;#xed;r Janisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1"&gt;Tom&amp;#xe1;&amp;#x161; Pevn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1"&gt;Viliam Lis&amp;#xfd;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. (arXiv:2006.16241v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16241</id>
        <link href="http://arxiv.org/abs/2006.16241"/>
        <updated>2021-07-27T02:03:36.205Z</updated>
        <summary type="html"><![CDATA[We introduce four new real-world distribution shift datasets consisting of
changes in image style, image blurriness, geographic location, camera
operation, and more. With our new datasets, we take stock of previously
proposed methods for improving out-of-distribution robustness and put them to
the test. We find that using larger models and artificial data augmentations
can improve robustness on real-world distribution shifts, contrary to claims in
prior work. We find improvements in artificial robustness benchmarks can
transfer to real-world distribution shifts, contrary to claims in prior work.
Motivated by our observation that data augmentations can help with real-world
distribution shifts, we also introduce a new data augmentation method which
advances the state-of-the-art and outperforms models pretrained with 1000 times
more labeled data. Overall we find that some methods consistently help with
distribution shifts in texture and local image statistics, but these methods do
not help with some other distribution shifts like geographic changes. Our
results show that future research must study multiple distribution shifts
simultaneously, as we demonstrate that no evaluated method consistently
improves robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1"&gt;Norman Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1"&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Frank Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorundo_E/0/1/0/all/0/1"&gt;Evan Dorundo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1"&gt;Rahul Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tyler Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1"&gt;Samyak Parajuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mike Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1"&gt;Justin Gilmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic social learning under graph constraints. (arXiv:2007.03983v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03983</id>
        <link href="http://arxiv.org/abs/2007.03983"/>
        <updated>2021-07-27T02:03:36.198Z</updated>
        <summary type="html"><![CDATA[We introduce a model of graph-constrained dynamic choice with reinforcement
modeled by positively $\alpha$-homogeneous rewards. We show that its empirical
process, which can be written as a stochastic approximation recursion with
Markov noise, has the same probability law as a certain vertex reinforced
random walk. We use this equivalence to show that for $\alpha > 0$, the
asymptotic outcome concentrates around the optimum in a certain limiting sense
when `annealed' by letting $\alpha\uparrow\infty$ slowly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Avrachenkov_K/0/1/0/all/0/1"&gt;Konstantin Avrachenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek S. Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Moharir_S/0/1/0/all/0/1"&gt;Sharayu Moharir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shah_S/0/1/0/all/0/1"&gt;Suhail M. Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[6DCNN with roto-translational convolution filters for volumetric data processing. (arXiv:2107.12078v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.12078</id>
        <link href="http://arxiv.org/abs/2107.12078"/>
        <updated>2021-07-27T02:03:36.173Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce 6D Convolutional Neural Network (6DCNN) designed
to tackle the problem of detecting relative positions and orientations of local
patterns when processing three-dimensional volumetric data. 6DCNN also includes
SE(3)-equivariant message-passing and nonlinear activation operations
constructed in the Fourier space. Working in the Fourier space allows
significantly reducing the computational complexity of our operations. We
demonstrate the properties of the 6D convolution and its efficiency in the
recognition of spatial patterns. We also assess the 6DCNN model on several
datasets from the recent CASP protein structure prediction challenges. Here,
6DCNN improves over the baseline architecture and also outperforms the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhemchuzhnikov_D/0/1/0/all/0/1"&gt;Dmitrii Zhemchuzhnikov&lt;/a&gt; (DAO), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Igashov_I/0/1/0/all/0/1"&gt;Ilia Igashov&lt;/a&gt; (DAO), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Grudinin_S/0/1/0/all/0/1"&gt;Sergei Grudinin&lt;/a&gt; (DAO)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12216</id>
        <link href="http://arxiv.org/abs/2107.12216"/>
        <updated>2021-07-27T02:03:36.167Z</updated>
        <summary type="html"><![CDATA[Policy gradient methods are appealing in deep reinforcement learning but
suffer from high variance of gradient estimate. To reduce the variance, the
state value function is applied commonly. However, the effect of the state
value function becomes limited in stochastic dynamic environments, where the
unexpected state dynamics and rewards will increase the variance. In this
paper, we propose to replace the state value function with a novel hindsight
value function, which leverages the information from the future to reduce the
variance of the gradient estimate for stochastic dynamic environments.

Particularly, to obtain an ideally unbiased gradient estimate, we propose an
information-theoretic approach, which optimizes the embeddings of the future to
be independent of previous actions. In our experiments, we apply the proposed
hindsight value function in stochastic dynamic environments, including
discrete-action environments and continuous-action environments. Compared with
the standard state value function, the proposed hindsight value function
consistently reduces the variance, stabilizes the training, and improves the
eventual policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiaming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xishan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaohui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1"&gt;Qi Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zidong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunji Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On The Impact of Client Sampling on Federated Learning Convergence. (arXiv:2107.12211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12211</id>
        <link href="http://arxiv.org/abs/2107.12211"/>
        <updated>2021-07-27T02:03:36.161Z</updated>
        <summary type="html"><![CDATA[While clients' sampling is a central operation of current state-of-the-art
federated learning (FL) approaches, the impact of this procedure on the
convergence and speed of FL remains to date under-investigated. In this work we
introduce a novel decomposition theorem for the convergence of FL, allowing to
clearly quantify the impact of client sampling on the global model update.
Contrarily to previous convergence analyses, our theorem provides the exact
decomposition of a given convergence step, thus enabling accurate
considerations about the role of client sampling and heterogeneity. First, we
provide a theoretical ground for previously reported results on the
relationship between FL convergence and the variance of the aggregation
weights. Second, we prove for the first time that the quality of FL convergence
is also impacted by the resulting covariance between aggregation weights.
Third, we establish that the sum of the aggregation weights is another source
of slow-down and should be equal to 1 to improve FL convergence speed. Our
theory is general, and is here applied to Multinomial Distribution (MD) and
Uniform sampling, the two default client sampling in FL, and demonstrated
through a series of experiments in non-iid and unbalanced scenarios. Our
results suggest that MD sampling should be used as default sampling scheme, due
to the resilience to the changes in data ratio during the learning process,
while Uniform sampling is superior only in the special case when clients have
the same amount of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1"&gt;Yann Fraboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Richard Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1"&gt;Laetitia Kameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parametric Contrastive Learning. (arXiv:2107.12028v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12028</id>
        <link href="http://arxiv.org/abs/2107.12028"/>
        <updated>2021-07-27T02:03:36.138Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle
long-tailed recognition. Based on theoretical analysis, we observe supervised
contrastive loss tends to bias on high-frequency classes and thus increases the
difficulty of imbalance learning. We introduce a set of parametric class-wise
learnable centers to rebalance from an optimization perspective. Further, we
analyze our PaCo loss under a balanced setting. Our analysis demonstrates that
PaCo can adaptively enhance the intensity of pushing samples of the same class
close as more samples are pulled together with their corresponding centers and
benefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,
Places, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed
recognition. On full ImageNet, models trained with PaCo loss surpass supervised
contrastive learning across various ResNet backbones. Our code is available at
\url{https://github.com/jiequancui/Parametric-Contrastive-Learning}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jiequan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhisheng Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision-forest voting scheme for classification of rare classes in network intrusion detection. (arXiv:2107.11862v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11862</id>
        <link href="http://arxiv.org/abs/2107.11862"/>
        <updated>2021-07-27T02:03:36.131Z</updated>
        <summary type="html"><![CDATA[In this paper, Bayesian based aggregation of decision trees in an ensemble
(decision forest) is investigated. The focus is laid on multi-class
classification with number of samples significantly skewed toward one of the
classes. The algorithm leverages out-of-bag datasets to estimate prediction
errors of individual trees, which are then used in accordance with the Bayes
rule to refine the decision of the ensemble. The algorithm takes prevalence of
individual classes into account and does not require setting of any additional
parameters related to class weights or decision-score thresholds. Evaluation is
based on publicly available datasets as well as on an proprietary dataset
comprising network traffic telemetry from hundreds of enterprise networks with
over a million of users overall. The aim is to increase the detection
capabilities of an operating malware detection system. While we were able to
keep precision of the system higher than 94\%, that is only 6 out of 100
detections shown to the network administrator are false alarms, we were able to
achieve increase of approximately 7\% in the number of detections. The
algorithm effectively handles large amounts of data, and can be used in
conjunction with most of the state-of-the-art algorithms used to train decision
forests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1"&gt;Jan Brabec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machlica_L/0/1/0/all/0/1"&gt;Lukas Machlica&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain Inspired Computing Approach for the Optimization of the Thin Film Thickness of Polystyrene on the Glass Substrates. (arXiv:2107.12156v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.12156</id>
        <link href="http://arxiv.org/abs/2107.12156"/>
        <updated>2021-07-27T02:03:36.125Z</updated>
        <summary type="html"><![CDATA[Advent in machine learning is leaving a deep impact on various sectors
including the material science domain. The present paper highlights the
application of various supervised machine learning regression algorithms such
as polynomial regression, decision tree regression algorithm, random forest
algorithm, support vector regression algorithm, and artificial neural network
algorithm to determine the thin film thickness of Polystyrene on the glass
substrates. The results showed that the polynomial regression machine learning
algorithm outperforms all other machine learning models by yielding the
coefficient of determination of 0.96 approximately and mean square error of
0.04 respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Akshansh Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Dixit_D/0/1/0/all/0/1"&gt;Devarrishi Dixit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable Dynamic Mode Decomposition Algorithm for Noisy Pressure-Sensitive Paint Measurement Data. (arXiv:2107.11999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11999</id>
        <link href="http://arxiv.org/abs/2107.11999"/>
        <updated>2021-07-27T02:03:36.118Z</updated>
        <summary type="html"><![CDATA[In this study, we proposed the truncated total least squares dynamic mode
decomposition (T-TLS DMD) algorithm, which can perform DMD analysis of noisy
data. By adding truncation regularization to the conventional TLS DMD
algorithm, T-TLS DMD improves the stability of the computation while
maintaining the accuracy of TLS DMD. The effectiveness of the proposed method
was evaluated by the analysis of the wake behind a cylinder and
pressure-sensitive paint (PSP) data for the buffet cell phenomenon. The results
showed the importance of regularization in the DMD algorithm. With respect to
the eigenvalues, T-TLS DMD was less affected by noise, and accurate eigenvalues
could be obtained stably, whereas the eigenvalues of TLS and subspace DMD
varied greatly due to noise. It was also observed that the eigenvalues of the
standard and exact DMD had the problem of shifting to the damping side, as
reported in previous studies. With respect to eigenvectors, T-TLS and exact DMD
captured the characteristic flow patterns clearly even in the presence of
noise, whereas TLS and subspace DMD were not able to capture them clearly due
to noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ohmichi_Y/0/1/0/all/0/1"&gt;Yuya Ohmichi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugioka_Y/0/1/0/all/0/1"&gt;Yosuke Sugioka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakakita_K/0/1/0/all/0/1"&gt;Kazuyuki Nakakita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EGGS: Eigen-Gap Guided Search\\ Making Subspace Clustering Easy. (arXiv:2107.12183v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12183</id>
        <link href="http://arxiv.org/abs/2107.12183"/>
        <updated>2021-07-27T02:03:36.111Z</updated>
        <summary type="html"><![CDATA[The performance of spectral clustering heavily relies on the quality of
affinity matrix. A variety of affinity-matrix-construction methods have been
proposed but they have hyper-parameters to determine beforehand, which requires
strong experience and lead to difficulty in real applications especially when
the inter-cluster similarity is high or/and the dataset is large. On the other
hand, we often have to determine to use a linear model or a nonlinear model,
which still depends on experience. To solve these two problems, in this paper,
we present an eigen-gap guided search method for subspace clustering. The main
idea is to find the most reliable affinity matrix among a set of candidates
constructed by linear and kernel regressions, where the reliability is
quantified by the \textit{relative-eigen-gap} of graph Laplacian defined in
this paper. We show, theoretically and numerically, that the Laplacian matrix
with a larger relative-eigen-gap often yields a higher clustering accuracy and
stability. Our method is able to automatically search the best model and
hyper-parameters in a pre-defined space. The search space is very easy to
determine and can be arbitrarily large, though a relatively compact search
space can reduce the highly unnecessary computation. Our method has high
flexibility and convenience in real applications, and also has low
computational cost because the affinity matrix is not computed by iterative
optimization. We extend the method to large-scale datasets such as MNIST, on
which the time cost is less than 90s and the clustering accuracy is
state-of-the-art. Extensive experiments of natural image clustering show that
our method is more stable, accurate, and efficient than baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jicong Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1"&gt;Yiheng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingbo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Workpiece Image-based Tool Wear Classification in Blanking Processes Using Deep Convolutional Neural Networks. (arXiv:2107.12034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12034</id>
        <link href="http://arxiv.org/abs/2107.12034"/>
        <updated>2021-07-27T02:03:36.096Z</updated>
        <summary type="html"><![CDATA[Blanking processes belong to the most widely used manufacturing techniques
due to their economic efficiency. Their economic viability depends to a large
extent on the resulting product quality and the associated customer
satisfaction as well as on possible downtimes. In particular, the occurrence of
increased tool wear reduces the product quality and leads to downtimes, which
is why considerable research has been carried out in recent years with regard
to wear detection. While processes have widely been monitored based on force
and acceleration signals, a new approach is pursued in this paper. Blanked
workpieces manufactured by punches with 16 different wear states are
photographed and then used as inputs for Deep Convolutional Neural Networks to
classify wear states. The results show that wear states can be predicted with
surprisingly high accuracy, opening up new possibilities and research
opportunities for tool wear monitoring of blanking processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1"&gt;Dirk Alexander Molitor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubik_C/0/1/0/all/0/1"&gt;Christian Kubik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hetfleisch_R/0/1/0/all/0/1"&gt;Ruben Helmut Hetfleisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groche_P/0/1/0/all/0/1"&gt;Peter Groche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Influential Higher-Order Patterns in Temporal Network Data. (arXiv:2107.12100v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.12100</id>
        <link href="http://arxiv.org/abs/2107.12100"/>
        <updated>2021-07-27T02:03:36.090Z</updated>
        <summary type="html"><![CDATA[Networks are frequently used to model complex systems comprised of
interacting elements. While links capture the topology of direct interactions,
the true complexity of many systems originates from higher-order patterns in
paths by which nodes can indirectly influence each other. Path data,
representing ordered sequences of consecutive direct interactions, can be used
to model these patterns. However, to avoid overfitting, such models should only
consider those higher-order patterns for which the data provide sufficient
statistical evidence. On the other hand, we hypothesise that network models,
which capture only direct interactions, underfit higher-order patterns present
in data. Consequently, both approaches are likely to misidentify influential
nodes in complex networks. We contribute to this issue by proposing eight
centrality measures based on MOGen, a multi-order generative model that
accounts for all paths up to a maximum distance but disregards paths at higher
distances. We compare MOGen-based centralities to equivalent measures for
network models and path data in a prediction experiment where we aim to
identify influential nodes in out-of-sample data. Our results show strong
evidence supporting our hypothesis. MOGen consistently outperforms both the
network model and path-based prediction. We further show that the performance
difference between MOGen and the path-based approach disappears if we have
sufficient observations, confirming that the error is due to overfitting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gote_C/0/1/0/all/0/1"&gt;Christoph Gote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perri_V/0/1/0/all/0/1"&gt;Vincenzo Perri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholtes_I/0/1/0/all/0/1"&gt;Ingo Scholtes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compensation Learning. (arXiv:2107.11921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11921</id>
        <link href="http://arxiv.org/abs/2107.11921"/>
        <updated>2021-07-27T02:03:36.083Z</updated>
        <summary type="html"><![CDATA[Weighting strategy prevails in machine learning. For example, a common
approach in robust machine learning is to exert lower weights on samples which
are likely to be noisy or hard. This study reveals another undiscovered
strategy, namely, compensating, that has also been widely used in machine
learning. Learning with compensating is called compensation learning and a
systematic taxonomy is constructed for it in this study. In our taxonomy,
compensation learning is divided on the basis of the compensation targets,
inference manners, and granularity levels. Many existing learning algorithms
including some classical ones can be seen as a special case of compensation
learning or partially leveraging compensating. Furthermore, a family of new
learning algorithms can be obtained by plugging the compensation learning into
existing learning algorithms. Specifically, three concrete new learning
algorithms are proposed for robust machine learning. Extensive experiments on
text sentiment analysis, image classification, and graph classification verify
the effectiveness of the three new algorithms. Compensation learning can also
be used in various learning scenarios, such as imbalance learning, clustering,
regression, and so on.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1"&gt;Rujing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1"&gt;Ou Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. (arXiv:2107.11906v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11906</id>
        <link href="http://arxiv.org/abs/2107.11906"/>
        <updated>2021-07-27T02:03:35.956Z</updated>
        <summary type="html"><![CDATA[We describe an efficient hierarchical method to compute attention in the
Transformer architecture. The proposed attention mechanism exploits a matrix
structure similar to the Hierarchical Matrix (H-Matrix) developed by the
numerical analysis community, and has linear run time and memory complexity. We
perform extensive experiments to show that the inductive bias embodied by our
hierarchical attention is effective in capturing the hierarchical structure in
the sequences typical for natural language and vision tasks. Our method is
superior to alternative sub-quadratic proposals by over +6 points on average on
the Long Range Arena benchmark. It also sets a new SOTA test perplexity on
One-Billion Word dataset with 5x fewer model parameters than that of the
previous-best Transformer-based models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenhai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1"&gt;Radu Soricut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Shallow Ritz Method for elliptic problems with Singular Sources. (arXiv:2107.12013v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.12013</id>
        <link href="http://arxiv.org/abs/2107.12013"/>
        <updated>2021-07-27T02:03:35.948Z</updated>
        <summary type="html"><![CDATA[In this paper, a shallow Ritz-type neural network for solving elliptic
problems with delta function singular sources on an interface is developed.
There are three novel features in the present work; namely, (i) the delta
function singularity is naturally removed, (ii) level set function is
introduced as a feather input, (iii) it is completely shallow consisting of
only one hidden layer. We first introduce the energy functional of the problem
and then transform the contribution of singular sources to a regular surface
integral along the interface. In such a way the delta function singularity can
be naturally removed without the introduction of discrete delta function that
is commonly used in traditional regularization methods such as the well-known
immersed boundary method. The original problem is then reformulated as a
minimization problem. We propose a shallow Ritz-type neural network with one
hidden layer to approximate the global minimizer of the energy functional. As a
result, the network is trained by minimizing the loss function that is a
discrete version of the energy. In addition, we include the level set function
of the interface as a feature input and find that it significantly improves the
training efficiency and accuracy. We perform a series of numerical tests to
demonstrate the accuracy of the present network as well as its capability for
problems in irregular domains and in higher dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lai_M/0/1/0/all/0/1"&gt;Ming-Chih Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chang_C/0/1/0/all/0/1"&gt;Che-Chia Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei-Syuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei-Fan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1"&gt;Te-Sheng Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logspace Reducibility From Secret Leakage Planted Clique. (arXiv:2107.11886v1 [cs.CC])]]></title>
        <id>http://arxiv.org/abs/2107.11886</id>
        <link href="http://arxiv.org/abs/2107.11886"/>
        <updated>2021-07-27T02:03:35.926Z</updated>
        <summary type="html"><![CDATA[The planted clique problem is well-studied in the context of observing,
explaining, and predicting interesting computational phenomena associated with
statistical problems. When equating computational efficiency with the existence
of polynomial time algorithms, the computational hardness of (some variant of)
the planted clique problem can be used to infer the computational hardness of a
host of other statistical problems.

Is this ability to transfer computational hardness from (some variant of) the
planted clique problem to other statistical problems robust to changing our
notion of computational efficiency to space efficiency?

We answer this question affirmatively for three different statistical
problems, namely Sparse PCA, submatrix detection, and testing almost k-wise
independence. The key challenge is that space efficient randomized reductions
need to repeatedly access the randomness they use. Known reductions to these
problems are all randomized and need polynomially many random bits to
implement. Since we can not store polynomially many random bits in memory, it
is unclear how to implement these existing reductions space efficiently. There
are two ideas involved in circumventing this issue and implementing known
reductions to these problems space efficiently.

1. When solving statistical problems, we can use parts of the input itself as
randomness.

2. Secret leakage variants of the planted clique problem with appropriate
secret leakage can be more useful than the standard planted clique problem when
we want to use parts of the input as randomness.

(abstract shortened due to arxiv constraints)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mardia_J/0/1/0/all/0/1"&gt;Jay Mardia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans. (arXiv:2107.12009v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12009</id>
        <link href="http://arxiv.org/abs/2107.12009"/>
        <updated>2021-07-27T02:03:35.918Z</updated>
        <summary type="html"><![CDATA[Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood
clots. PE accounts for approximately 100,000 deaths per year in the United
States alone. The clinical presentation of PE is often nonspecific, making the
diagnosis challenging. Thus, rapid and accurate risk stratification is of
paramount importance. High-risk PE is caused by right ventricular (RV)
dysfunction from acute pressure overload, which in return can help identify
which patients require more aggressive therapy. Reconstructed four-chamber
views of the heart on chest CT can detect right ventricular enlargement. CT
pulmonary angiography (CTPA) is the golden standard in the diagnostic workup of
suspected PE. Therefore, it can link between diagnosis and risk stratification
strategies. We developed a weakly supervised deep learning algorithm, with an
emphasis on a novel attention mechanism, to automatically classify RV strain on
CTPA. Our method is a 3D DenseNet model with integrated 3D residual attention
blocks. We evaluated our model on a dataset of CTPAs of emergency department
(ED) PE patients. This model achieved an area under the receiver operating
characteristic curve (AUC) of 0.88 for classifying RV strain. The model showed
a sensitivity of 87% and specificity of 83.7%. Our solution outperforms
state-of-the-art 3D CNN networks. The proposed design allows for a fully
automated network that can be trained easily in an end-to-end manner without
requiring computationally intensive and time-consuming preprocessing or
strenuous labeling of the data.We infer that unmarked CTPAs can be used for
effective RV strain classification. This could be used as a second reader,
alerting for high-risk PE patients. To the best of our knowledge, there are no
previous deep learning-based studies that attempted to solve this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cahan_N/0/1/0/all/0/1"&gt;Noa Cahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marom_E/0/1/0/all/0/1"&gt;Edith M. Marom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soffer_S/0/1/0/all/0/1"&gt;Shelly Soffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barash_Y/0/1/0/all/0/1"&gt;Yiftach Barash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Konen_E/0/1/0/all/0/1"&gt;Eli Konen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klang_E/0/1/0/all/0/1"&gt;Eyal Klang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greenspan_H/0/1/0/all/0/1"&gt;Hayit Greenspan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation Learning on Tissue-Specific Multi-Omics. (arXiv:2107.11856v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2107.11856</id>
        <link href="http://arxiv.org/abs/2107.11856"/>
        <updated>2021-07-27T02:03:35.911Z</updated>
        <summary type="html"><![CDATA[Combining different modalities of data from human tissues has been critical
in advancing biomedical research and personalised medical care. In this study,
we leverage a graph embedding model (i.e VGAE) to perform link prediction on
tissue-specific Gene-Gene Interaction (GGI) networks. Through ablation
experiments, we prove that the combination of multiple biological modalities
(i.e multi-omics) leads to powerful embeddings and better link prediction
performances. Our evaluation shows that the integration of gene methylation
profiles and RNA-sequencing data significantly improves the link prediction
performance. Overall, the combination of RNA-sequencing and gene methylation
data leads to a link prediction accuracy of 71% on GGI networks. By harnessing
graph representation learning on multi-omics data, our work brings novel
insights to the current literature on multi-omics integration in
bioinformatics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Amor_A/0/1/0/all/0/1"&gt;Amine Amor&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Lio&amp;#x27;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vikash Singh&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Torne_R/0/1/0/all/0/1"&gt;Ramon Vi&amp;#xf1;as Torn&amp;#xe9;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Terre_H/0/1/0/all/0/1"&gt;Helena Andres Terre&lt;/a&gt; (1)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Risk-aware Costmaps for Traversability in Challenging Environments. (arXiv:2107.11722v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.11722</id>
        <link href="http://arxiv.org/abs/2107.11722"/>
        <updated>2021-07-27T02:03:35.905Z</updated>
        <summary type="html"><![CDATA[One of the main challenges in autonomous robotic exploration and navigation
in unknown and unstructured environments is determining where the robot can or
cannot safely move. A significant source of difficulty in this determination
arises from stochasticity and uncertainty, coming from localization error,
sensor sparsity and noise, difficult-to-model robot-ground interactions, and
disturbances to the motion of the vehicle. Classical approaches to this problem
rely on geometric analysis of the surrounding terrain, which can be prone to
modeling errors and can be computationally expensive. Moreover, modeling the
distribution of uncertain traversability costs is a difficult task, compounded
by the various error sources mentioned above. In this work, we take a
principled learning approach to this problem. We introduce a neural network
architecture for robustly learning the distribution of traversability costs.
Because we are motivated by preserving the life of the robot, we tackle this
learning problem from the perspective of learning tail-risks, i.e. the
Conditional Value-at-Risk (CVaR). We show that this approach reliably learns
the expected tail risk given a desired probability risk threshold between 0 and
1, producing a traversability costmap which is more robust to outliers, more
accurately captures tail risks, and is more computationally efficient, when
compared against baselines. We validate our method on data collected a legged
robot navigating challenging, unstructured environments including an abandoned
subway, limestone caves, and lava tube caves.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;David D. Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1"&gt;Ali-akbar Agha-mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos A. Theodorou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Models as Zero-shot Visual Semantic Learners. (arXiv:2107.12021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12021</id>
        <link href="http://arxiv.org/abs/2107.12021"/>
        <updated>2021-07-27T02:03:35.877Z</updated>
        <summary type="html"><![CDATA[Visual Semantic Embedding (VSE) models, which map images into a rich semantic
embedding space, have been a milestone in object recognition and zero-shot
learning. Current approaches to VSE heavily rely on static word em-bedding
techniques. In this work, we propose a Visual Se-mantic Embedding Probe (VSEP)
designed to probe the semantic information of contextualized word embeddings in
visual semantic understanding tasks. We show that the knowledge encoded in
transformer language models can be exploited for tasks requiring visual
semantic understanding.The VSEP with contextual representations can distinguish
word-level object representations in complicated scenes as a compositional
zero-shot learner. We further introduce a zero-shot setting with VSEPs to
evaluate a model's ability to associate a novel word with a novel visual
category. We find that contextual representations in language mod-els
outperform static word embeddings, when the compositional chain of object is
short. We notice that current visual semantic embedding models lack a mutual
exclusivity bias which limits their performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yue Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1"&gt;Adam Pr&amp;#xfc;gel-Bennett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Circuit Synthesis from Specification Patterns. (arXiv:2107.11864v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11864</id>
        <link href="http://arxiv.org/abs/2107.11864"/>
        <updated>2021-07-27T02:03:35.870Z</updated>
        <summary type="html"><![CDATA[We train hierarchical Transformers on the task of synthesizing hardware
circuits directly out of high-level logical specifications in linear-time
temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic
challenge with a long history and an annual competition is organized to track
the improvement of algorithms and tooling over time. New approaches using
machine learning might open a lot of possibilities in this area, but suffer
from the lack of sufficient amounts of training data. In this paper, we
consider a method to generate large amounts of additional training data, i.e.,
pairs of specifications and circuits implementing them. We ensure that this
synthetic data is sufficiently close to human-written specifications by mining
common patterns from the specifications used in the synthesis competitions. We
show that hierarchical Transformers trained on this synthetic data solve a
significant portion of problems from the synthesis competitions, and even
out-of-distribution examples from a recent case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_F/0/1/0/all/0/1"&gt;Frederik Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1"&gt;Christopher Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabe_M/0/1/0/all/0/1"&gt;Markus N. Rabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finkbeiner_B/0/1/0/all/0/1"&gt;Bernd Finkbeiner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Power of human-algorithm collaboration in solving combinatorial optimization problems. (arXiv:2107.11784v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.11784</id>
        <link href="http://arxiv.org/abs/2107.11784"/>
        <updated>2021-07-27T02:03:35.863Z</updated>
        <summary type="html"><![CDATA[Many combinatorial optimization problems are often considered intractable to
solve exactly or by approximation. An example of such problem is maximum clique
which -- under standard assumptions in complexity theory -- cannot be solved in
sub-exponential time or be approximated within polynomial factor efficiently.
We show that if a polynomial time algorithm can query informative Gaussian
priors from an expert $poly(n)$ times, then a class of combinatorial
optimization problems can be solved efficiently in expectation up to a
multiplicative factor $\epsilon$ where $\epsilon$ is arbitrary constant. While
our proposed methods are merely theoretical, they cast new light on how to
approach solving these problems that have been usually considered intractable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toivonen_T/0/1/0/all/0/1"&gt;Tapani Toivonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient inference of interventional distributions. (arXiv:2107.11712v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.11712</id>
        <link href="http://arxiv.org/abs/2107.11712"/>
        <updated>2021-07-27T02:03:35.856Z</updated>
        <summary type="html"><![CDATA[We consider the problem of efficiently inferring interventional distributions
in a causal Bayesian network from a finite number of observations. Let
$\mathcal{P}$ be a causal model on a set $\mathbf{V}$ of observable variables
on a given causal graph $G$. For sets $\mathbf{X},\mathbf{Y}\subseteq
\mathbf{V}$, and setting ${\bf x}$ to $\mathbf{X}$, let $P_{\bf x}(\mathbf{Y})$
denote the interventional distribution on $\mathbf{Y}$ with respect to an
intervention ${\bf x}$ to variables ${\bf x}$. Shpitser and Pearl (AAAI 2006),
building on the work of Tian and Pearl (AAAI 2001), gave an exact
characterization of the class of causal graphs for which the interventional
distribution $P_{\bf x}({\mathbf{Y}})$ can be uniquely determined. We give the
first efficient version of the Shpitser-Pearl algorithm. In particular, under
natural assumptions, we give a polynomial-time algorithm that on input a causal
graph $G$ on observable variables $\mathbf{V}$, a setting ${\bf x}$ of a set
$\mathbf{X} \subseteq \mathbf{V}$ of bounded size, outputs succinct
descriptions of both an evaluator and a generator for a distribution $\hat{P}$
that is $\varepsilon$-close (in total variation distance) to $P_{\bf
x}({\mathbf{Y}})$ where $Y=\mathbf{V}\setminus \mathbf{X}$, if $P_{\bf
x}(\mathbf{Y})$ is identifiable. We also show that when $\mathbf{Y}$ is an
arbitrary set, there is no efficient algorithm that outputs an evaluator of a
distribution that is $\varepsilon$-close to $P_{\bf x}({\mathbf{Y}})$ unless
all problems that have statistical zero-knowledge proofs, including the Graph
Isomorphism problem, have efficient randomized algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1"&gt;Sutanu Gayen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_S/0/1/0/all/0/1"&gt;Saravanan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raval_V/0/1/0/all/0/1"&gt;Vedant Raval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1"&gt;N. V. Vinodchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Character Spotting Using Machine Learning Techniques. (arXiv:2107.11795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11795</id>
        <link href="http://arxiv.org/abs/2107.11795"/>
        <updated>2021-07-27T02:03:35.850Z</updated>
        <summary type="html"><![CDATA[This work presents a comparison of machine learning algorithms that are
implemented to segment the characters of text presented as an image. The
algorithms are designed to work on degraded documents with text that is not
aligned in an organized fashion. The paper investigates the use of Support
Vector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform
the operation of character spotting. Character Spotting involves extracting
potential characters from a stream of text by selecting regions bound by white
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1"&gt;P Preethi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1"&gt;Hrishikesh Viswanath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Restless Bandits with Many Arms: Beating the Central Limit Theorem. (arXiv:2107.11911v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.11911</id>
        <link href="http://arxiv.org/abs/2107.11911"/>
        <updated>2021-07-27T02:03:35.843Z</updated>
        <summary type="html"><![CDATA[We consider finite-horizon restless bandits with multiple pulls per period,
which play an important role in recommender systems, active learning, revenue
management, and many other areas. While an optimal policy can be computed, in
principle, using dynamic programming, the computation required scales
exponentially in the number of arms $N$. Thus, there is substantial value in
understanding the performance of index policies and other policies that can be
computed efficiently for large $N$. We study the growth of the optimality gap,
i.e., the loss in expected performance compared to an optimal policy, for such
policies in a classical asymptotic regime proposed by Whittle in which $N$
grows while holding constant the fraction of arms that can be pulled per
period. Intuition from the Central Limit Theorem and the tightest previous
theoretical bounds suggest that this optimality gap should grow like
$O(\sqrt{N})$. Surprisingly, we show that it is possible to outperform this
bound. We characterize a non-degeneracy condition and a wide class of novel
practically-computable policies, called fluid-priority policies, in which the
optimality gap is $O(1)$. These include most widely-used index policies. When
this non-degeneracy condition does not hold, we show that fluid-priority
policies nevertheless have an optimality gap that is $O(\sqrt{N})$,
significantly generalizing the class of policies for which convergence rates
are known. We demonstrate that fluid-priority policies offer state-of-the-art
performance on a collection of restless bandit problems in numerical
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Frazier_P/0/1/0/all/0/1"&gt;Peter I. Frazier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced Imitation Learning by Free Energy Principle. (arXiv:2107.11811v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11811</id>
        <link href="http://arxiv.org/abs/2107.11811"/>
        <updated>2021-07-27T02:03:35.815Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning (RL) requires a large amount of exploration especially
in sparse-reward settings. Imitation Learning (IL) can learn from expert
demonstrations without exploration, but it never exceeds the expert's
performance and is also vulnerable to distributional shift between
demonstration and execution. In this paper, we radically unify RL and IL based
on Free Energy Principle (FEP). FEP is a unified Bayesian theory of the brain
that explains perception, action and model learning by a common fundamental
principle. We present a theoretical extension of FEP and derive an algorithm in
which an agent learns the world model that internalizes expert demonstrations
and at the same time uses the model to infer the current and future states and
actions that maximize rewards. The algorithm thus reduces exploration costs by
partially imitating experts as well as maximizing its return in a seamless way,
resulting in a higher performance than the suboptimal expert. Our experimental
results show that this approach is promising in visual control tasks especially
in sparse-reward environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ogishima_R/0/1/0/all/0/1"&gt;Ryoya Ogishima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karino_I/0/1/0/all/0/1"&gt;Izumi Karino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1"&gt;Yasuo Kuniyoshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective. (arXiv:2107.11882v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11882</id>
        <link href="http://arxiv.org/abs/2107.11882"/>
        <updated>2021-07-27T02:03:35.807Z</updated>
        <summary type="html"><![CDATA[Data from multi-modality provide complementary information in clinical
prediction, but missing data in clinical cohorts limits the number of subjects
in multi-modal learning context. Multi-modal missing imputation is challenging
with existing methods when 1) the missing data span across heterogeneous
modalities (e.g., image vs. non-image); or 2) one modality is largely missing.
In this paper, we address imputation of missing data by modeling the joint
distribution of multi-modal data. Motivated by partial bidirectional generative
adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method
that imputes one modality combining the conditional knowledge from another
modality. Specifically, C-PBiGAN introduces a conditional latent space in a
missing imputation framework that jointly encodes the available multi-modal
data, along with a class regularization loss on imputed data to recover
discriminative information. To our knowledge, it is the first generative
adversarial model that addresses multi-modal missing imputation by modeling the
joint distribution of image and non-image data. We validate our model with both
the national lung screening trial (NLST) dataset and an external clinical
validation cohort. The proposed C-PBiGAN achieves significant improvements in
lung cancer risk estimation compared with representative imputation methods
(e.g., AUC values increase in both NLST (+2.9\%) and in-house dataset (+4.3\%)
compared with PBiGAN, p$<$0.05).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1"&gt;Riqiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaiwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deppen_S/0/1/0/all/0/1"&gt;Steve Deppen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1"&gt;Kim Sandler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Massion_P/0/1/0/all/0/1"&gt;Pierre Massion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1"&gt;Thomas A. Lasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11769</id>
        <link href="http://arxiv.org/abs/2107.11769"/>
        <updated>2021-07-27T02:03:35.792Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep learning on supervised point cloud semantic
segmentation, obtaining large-scale point-by-point manual annotations is still
a significant challenge. To reduce the huge annotation burden, we propose a
Region-based and Diversity-aware Active Learning (ReDAL), a general framework
for many deep learning approaches, aiming to automatically select only
informative and diverse sub-scene regions for label acquisition. Observing that
only a small portion of annotated regions are sufficient for 3D scene
understanding with deep learning, we use softmax entropy, color discontinuity,
and structural complexity to measure the information of sub-scene regions. A
diversity-aware selection algorithm is also developed to avoid redundant
annotations resulting from selecting informative but similar regions in a
querying batch. Extensive experiments show that our method highly outperforms
previous active learning strategies, and we achieve the performance of 90%
fully supervised learning, while less than 15% and 5% annotations are required
on S3DIS and SemanticKITTI datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tsung-Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Ping-Chia Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A brief note on understanding neural networks as Gaussian processes. (arXiv:2107.11892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11892</id>
        <link href="http://arxiv.org/abs/2107.11892"/>
        <updated>2021-07-27T02:03:35.785Z</updated>
        <summary type="html"><![CDATA[As a generalization of the work in [Lee et al., 2017], this note briefly
discusses when the prior of a neural network output follows a Gaussian process,
and how a neural-network-induced Gaussian process is formulated. The posterior
mean functions of such a Gaussian process regression lie in the reproducing
kernel Hilbert space defined by the neural-network-induced kernel. In the case
of two-layer neural networks, the induced Gaussian processes provide an
interpretation of the reproducing kernel Hilbert spaces whose union forms a
Barron space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mengwu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting FLOPs along input dimensions for GreenAI cost estimations. (arXiv:2107.11949v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11949</id>
        <link href="http://arxiv.org/abs/2107.11949"/>
        <updated>2021-07-27T02:03:35.778Z</updated>
        <summary type="html"><![CDATA[The term GreenAI refers to a novel approach to Deep Learning, that is more
aware of the ecological impact and the computational efficiency of its methods.
The promoters of GreenAI suggested the use of Floating Point Operations (FLOPs)
as a measure of the computational cost of Neural Networks; however, that
measure does not correlate well with the energy consumption of hardware
equipped with massively parallel processing units like GPUs or TPUs. In this
article, we propose a simple refinement of the formula used to compute floating
point operations for convolutional layers, called {\alpha}-FLOPs, explaining
and correcting the traditional discrepancy with respect to different layers,
and closer to reality. The notion of {\alpha}-FLOPs relies on the crucial
insight that, in case of inputs with multiple dimensions, there is no reason to
believe that the speedup offered by parallelism will be uniform along all
different axes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asperti_A/0/1/0/all/0/1"&gt;Andrea Asperti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1"&gt;Davide Evangelista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzolla_M/0/1/0/all/0/1"&gt;Moreno Marzolla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications. (arXiv:2107.11750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11750</id>
        <link href="http://arxiv.org/abs/2107.11750"/>
        <updated>2021-07-27T02:03:35.751Z</updated>
        <summary type="html"><![CDATA[Uncertainties in machine learning are a significant roadblock for its
application in safety-critical cyber-physical systems (CPS). One source of
uncertainty arises from distribution shifts in the input data between training
and test scenarios. Detecting such distribution shifts in real-time is an
emerging approach to address the challenge. The high dimensional input space in
CPS applications involving imaging adds extra difficulty to the task.
Generative learning models are widely adopted for the task, namely
out-of-distribution (OoD) detection. To improve the state-of-the-art, we
studied existing proposals from both machine learning and CPS fields. In the
latter, safety monitoring in real-time for autonomous driving agents has been a
focus. Exploiting the spatiotemporal correlation of motion in videos, we can
robustly detect hazardous motion around autonomous driving agents. Inspired by
the latest advances in the Variational Autoencoder (VAE) theory and practice,
we tapped into the prior knowledge in data to further boost OoD detection's
robustness. Comparison studies over nuScenes and Synthia data sets show our
methods significantly improve detection capabilities of OoD factors unique to
driving scenarios, 42% better than state-of-the-art approaches. Our model also
generalized near-perfectly, 97% better than the state-of-the-art across the
real-world and simulation driving data sets experimented. Finally, we
customized one proposed method into a twin-encoder model that can be deployed
to resource limited embedded devices for real-time OoD detection. Its execution
time was reduced over four times in low-precision 8-bit integer inference,
while detection capability is comparable to its corresponding floating-point
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yeli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1"&gt;Daniel Jun Xian Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1"&gt;Arvind Easwaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Explicit Differentiable Predictive Control Laws for Buildings. (arXiv:2107.11843v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.11843</id>
        <link href="http://arxiv.org/abs/2107.11843"/>
        <updated>2021-07-27T02:03:35.744Z</updated>
        <summary type="html"><![CDATA[We present a differentiable predictive control (DPC) methodology for learning
constrained control laws for unknown nonlinear systems. DPC poses an
approximate solution to multiparametric programming problems emerging from
explicit nonlinear model predictive control (MPC). Contrary to approximate MPC,
DPC does not require supervision by an expert controller. Instead, a system
dynamics model is learned from the observed system's dynamics, and the neural
control law is optimized offline by leveraging the differentiable closed-loop
system model. The combination of a differentiable closed-loop system and
penalty methods for constraint handling of system outputs and inputs allows us
to optimize the control law's parameters directly by backpropagating economic
MPC loss through the learned system model. The control performance of the
proposed DPC method is demonstrated in simulation using learned model of
multi-zone building thermal dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Drgona_J/0/1/0/all/0/1"&gt;Jan Drgona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tuor_A/0/1/0/all/0/1"&gt;Aaron Tuor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vasisht_S/0/1/0/all/0/1"&gt;Soumya Vasisht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Skomski_E/0/1/0/all/0/1"&gt;Elliott Skomski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vrabie_D/0/1/0/all/0/1"&gt;Draguna Vrabie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11769</id>
        <link href="http://arxiv.org/abs/2107.11769"/>
        <updated>2021-07-27T02:03:35.737Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep learning on supervised point cloud semantic
segmentation, obtaining large-scale point-by-point manual annotations is still
a significant challenge. To reduce the huge annotation burden, we propose a
Region-based and Diversity-aware Active Learning (ReDAL), a general framework
for many deep learning approaches, aiming to automatically select only
informative and diverse sub-scene regions for label acquisition. Observing that
only a small portion of annotated regions are sufficient for 3D scene
understanding with deep learning, we use softmax entropy, color discontinuity,
and structural complexity to measure the information of sub-scene regions. A
diversity-aware selection algorithm is also developed to avoid redundant
annotations resulting from selecting informative but similar regions in a
querying batch. Extensive experiments show that our method highly outperforms
previous active learning strategies, and we achieve the performance of 90%
fully supervised learning, while less than 15% and 5% annotations are required
on S3DIS and SemanticKITTI datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tsung-Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Ping-Chia Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Ethics in AI with AI: A Methodology and Dataset Construction. (arXiv:2107.11913v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.11913</id>
        <link href="http://arxiv.org/abs/2107.11913"/>
        <updated>2021-07-27T02:03:35.731Z</updated>
        <summary type="html"><![CDATA[Recently, the use of sound measures and metrics in Artificial Intelligence
has become the subject of interest of academia, government, and industry.
Efforts towards measuring different phenomena have gained traction in the AI
community, as illustrated by the publication of several influential field
reports and policy documents. These metrics are designed to help decision
takers to inform themselves about the fast-moving and impacting influences of
key advances in Artificial Intelligence in general and Machine Learning in
particular. In this paper we propose to use such newfound capabilities of AI
technologies to augment our AI measuring capabilities. We do so by training a
model to classify publications related to ethical issues and concerns. In our
methodology we use an expert, manually curated dataset as the training set and
then evaluate a large set of research papers. Finally, we highlight the
implications of AI metrics, in particular their contribution towards developing
trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI
Fairness; AI Measurement. Ethics in Computer Science.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1"&gt;Pedro H.C. Avelar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Audibert_R/0/1/0/all/0/1"&gt;Rafael B. Audibert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavares_A/0/1/0/all/0/1"&gt;Anderson R. Tavares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1"&gt;Lu&amp;#xed;s C. Lamb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11707</id>
        <link href="http://arxiv.org/abs/2107.11707"/>
        <updated>2021-07-27T02:03:35.716Z</updated>
        <summary type="html"><![CDATA[Video captioning is one of the challenging problems at the intersection of
vision and language, having many real-life applications in video retrieval,
video surveillance, assisting visually challenged people, Human-machine
interface, and many more. Recent deep learning-based methods have shown
promising results but are still on the lower side than other vision tasks (such
as image classification, object detection). A significant drawback with
existing video captioning methods is that they are optimized over cross-entropy
loss function, which is uncorrelated to the de facto evaluation metrics (BLEU,
METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate
of the true loss function for video captioning. This paper addresses the
drawback by introducing a dynamic loss network (DLN), which provides an
additional feedback signal that directly reflects the evaluation metrics. Our
results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to
Text (MSRVTT) datasets outperform previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1"&gt;Nasibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1"&gt;Partha Pratim Mohanta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go Wider Instead of Deeper. (arXiv:2107.11817v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11817</id>
        <link href="http://arxiv.org/abs/2107.11817"/>
        <updated>2021-07-27T02:03:35.681Z</updated>
        <summary type="html"><![CDATA[The transformer has recently achieved impressive results on various tasks. To
further improve the effectiveness and efficiency of the transformer, there are
two trains of thought among existing works: (1) going wider by scaling to more
trainable parameters; (2) going shallower by parameter sharing or model
compressing along with the depth. However, larger models usually do not scale
well when fewer tokens are available to train, and advanced parallelisms are
required when the model is extremely large. Smaller models usually achieve
inferior performance compared to the original transformer model due to the loss
of representation power. In this paper, to achieve better performance with
fewer trainable parameters, we propose a framework to deploy trainable
parameters efficiently, by going wider instead of deeper. Specially, we scale
along model width by replacing feed-forward network (FFN) with
mixture-of-experts (MoE). We then share the MoE layers across transformer
blocks using individual layer normalization. Such deployment plays the role to
transform various semantic representations, which makes the model more
parameter-efficient and effective. To evaluate our framework, we design WideNet
and evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer
(ViT) by $1.46\%$ with $0.72 \times$ trainable parameters. Using $0.46 \times$
and $0.13 \times$ parameters, our WideNet can still surpass ViT and ViT-MoE by
$0.83\%$ and $2.08\%$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Ziji Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yuxuan Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imbalanced Big Data Oversampling: Taxonomy, Algorithms, Software, Guidelines and Future Directions. (arXiv:2107.11508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11508</id>
        <link href="http://arxiv.org/abs/2107.11508"/>
        <updated>2021-07-27T02:03:35.666Z</updated>
        <summary type="html"><![CDATA[Learning from imbalanced data is among the most challenging areas in
contemporary machine learning. This becomes even more difficult when considered
the context of big data that calls for dedicated architectures capable of
high-performance processing. Apache Spark is a highly efficient and popular
architecture, but it poses specific challenges for algorithms to be implemented
for it. While oversampling algorithms are an effective way for handling class
imbalance, they have not been designed for distributed environments. In this
paper, we propose a holistic look on oversampling algorithms for imbalanced big
data. We discuss the taxonomy of oversampling algorithms and their mechanisms
used to handle skewed class distributions. We introduce a Spark library with 14
state-of-the-art oversampling algorithms implemented and evaluate their
efficacy via extensive experimental study. Using binary and multi-class massive
data sets, we analyze the effectiveness of oversampling algorithms and their
relationships with different types of classifiers. We evaluate the trade-off
between accuracy and time complexity of oversampling algorithms, as well as
their scalability when increasing the size of data. This allows us to gain
insight into the usefulness of specific components of oversampling algorithms
for big data, as well as formulate guidelines and recommendations for designing
future resampling approaches for massive imbalanced data. Our library can be
downloaded from https://github.com/fsleeman/spark-class-balancing.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sleeman_W/0/1/0/all/0/1"&gt;William C. Sleeman IV&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1"&gt;Bartosz Krawczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial training may be a double-edged sword. (arXiv:2107.11671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11671</id>
        <link href="http://arxiv.org/abs/2107.11671"/>
        <updated>2021-07-27T02:03:35.576Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been shown as an effective approach to improve the
robustness of image classifiers against white-box attacks. However, its
effectiveness against black-box attacks is more nuanced. In this work, we
demonstrate that some geometric consequences of adversarial training on the
decision boundary of deep networks give an edge to certain types of black-box
attacks. In particular, we define a metric called robustness gain to show that
while adversarial training is an effective method to dramatically improve the
robustness in white-box scenarios, it may not provide such a good robustness
gain against the more realistic decision-based black-box attacks. Moreover, we
show that even the minimal perturbation white-box attacks can converge faster
against adversarially-trained neural networks compared to the regular ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1"&gt;Ali Rahmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1"&gt;Seyed-Mohsen Moosavi-Dezfooli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Huaiyu Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11576</id>
        <link href="http://arxiv.org/abs/2107.11576"/>
        <updated>2021-07-27T02:03:35.568Z</updated>
        <summary type="html"><![CDATA[Encouraging progress has been made towards Visual Question Answering (VQA) in
recent years, but it is still challenging to enable VQA models to adaptively
generalize to out-of-distribution (OOD) samples. Intuitively, recompositions of
existing visual concepts (i.e., attributes and objects) can generate unseen
compositions in the training set, which will promote VQA models to generalize
to OOD samples. In this paper, we formulate OOD generalization in VQA as a
compositional generalization problem and propose a graph generative
modeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM
leverages graph generative modeling to iteratively generate a relation matrix
and node representations for the predefined graph that utilizes
attribute-object pairs as nodes. Furthermore, to alleviate the unstable
training issue in graph generative modeling, we propose a gradient distribution
consistency loss to constrain the data distribution with adversarial
perturbations and the generated distribution. The baseline VQA model (LXMERT)
trained with the X-GGM scheme achieves state-of-the-art OOD performance on two
standard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation
studies demonstrate the effectiveness of X-GGM components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jingjing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1"&gt;Zhixiong Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Online Learning and Offline Learning for Contextual Bandits with Deficient Support. (arXiv:2107.11533v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.11533</id>
        <link href="http://arxiv.org/abs/2107.11533"/>
        <updated>2021-07-27T02:03:35.542Z</updated>
        <summary type="html"><![CDATA[We address policy learning with logged data in contextual bandits. Current
offline-policy learning algorithms are mostly based on inverse propensity score
(IPS) weighting requiring the logging policy to have \emph{full support} i.e. a
non-zero probability for any context/action of the evaluation policy. However,
many real-world systems do not guarantee such logging policies, especially when
the action space is large and many actions have poor or missing rewards. With
such \emph{support deficiency}, the offline learning fails to find optimal
policies. We propose a novel approach that uses a hybrid of offline learning
with online exploration. The online exploration is used to explore unsupported
actions in the logged data whilst offline learning is used to exploit supported
actions from the logged data avoiding unnecessary explorations. Our approach
determines an optimal policy with theoretical guarantees using the minimal
number of online explorations. We demonstrate our algorithms' effectiveness
empirically on a diverse collection of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1"&gt;Hung Tran-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1"&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rana_S/0/1/0/all/0/1"&gt;Santu Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions. (arXiv:2107.11585v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11585</id>
        <link href="http://arxiv.org/abs/2107.11585"/>
        <updated>2021-07-27T02:03:35.534Z</updated>
        <summary type="html"><![CDATA[As the field of remote sensing is evolving, we witness the accumulation of
information from several modalities, such as multispectral (MS), hyperspectral
(HSI), LiDAR etc. Each of these modalities possess its own distinct
characteristics and when combined synergistically, perform very well in the
recognition and classification tasks. However, fusing multiple modalities in
remote sensing is cumbersome due to highly disparate domains. Furthermore, the
existing methods do not facilitate cross-modal interactions. To this end, we
propose a novel transformer based fusion method for HSI and LiDAR modalities.
The model is composed of stacked auto encoders that harness the cross key-value
pairs for HSI and LiDAR, thus establishing a communication between the two
modalities, while simultaneously using the CNNs to extract the spectral and
spatial information from HSI and LiDAR. We test our model on Houston (Data
Fusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1"&gt;Rupak Bose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1"&gt;Shivam Pande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1"&gt;Biplab Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Graph Neural Networks with Expert Knowledge for Smart Contract Vulnerability Detection. (arXiv:2107.11598v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.11598</id>
        <link href="http://arxiv.org/abs/2107.11598"/>
        <updated>2021-07-27T02:03:35.527Z</updated>
        <summary type="html"><![CDATA[Smart contract vulnerability detection draws extensive attention in recent
years due to the substantial losses caused by hacker attacks. Existing efforts
for contract security analysis heavily rely on rigid rules defined by experts,
which are labor-intensive and non-scalable. More importantly, expert-defined
rules tend to be error-prone and suffer the inherent risk of being cheated by
crafty attackers. Recent researches focus on the symbolic execution and formal
analysis of smart contracts for vulnerability detection, yet to achieve a
precise and scalable solution. Although several methods have been proposed to
detect vulnerabilities in smart contracts, there is still a lack of effort that
considers combining expert-defined security patterns with deep neural networks.
In this paper, we explore using graph neural networks and expert knowledge for
smart contract vulnerability detection. Specifically, we cast the rich control-
and data- flow semantics of the source code into a contract graph. To highlight
the critical nodes in the graph, we further design a node elimination phase to
normalize the graph. Then, we propose a novel temporal message propagation
network to extract the graph feature from the normalized graph, and combine the
graph feature with designed expert patterns to yield a final detection system.
Extensive experiments are conducted on all the smart contracts that have source
code in Ethereum and VNT Chain platforms. Empirical results show significant
accuracy improvements over the state-of-the-art methods on three types of
vulnerabilities, where the detection accuracy of our method reaches 89.15%,
89.02%, and 83.21% for reentrancy, timestamp dependence, and infinite loop
vulnerabilities, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1"&gt;Peng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yuan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Lin Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Repairing Neural Networks: Provable Safety for Deep Networks via Dynamic Repair. (arXiv:2107.11445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11445</id>
        <link href="http://arxiv.org/abs/2107.11445"/>
        <updated>2021-07-27T02:03:35.520Z</updated>
        <summary type="html"><![CDATA[Neural networks are increasingly being deployed in contexts where safety is a
critical concern. In this work, we propose a way to construct neural network
classifiers that dynamically repair violations of non-relational safety
constraints called safe ordering properties. Safe ordering properties relate
requirements on the ordering of a network's output indices to conditions on
their input, and are sufficient to express most useful notions of
non-relational safety for classifiers. Our approach is based on a novel
self-repairing layer, which provably yields safe outputs regardless of the
characteristics of its input. We compose this layer with an existing network to
construct a self-repairing network (SR-Net), and show that in addition to
providing safe outputs, the SR-Net is guaranteed to preserve the accuracy of
the original network. Notably, our approach is independent of the size and
architecture of the network being repaired, depending only on the specified
property and the dimension of the network's output; thus it is scalable to
large state-of-the-art networks. We show that our approach can be implemented
using vectorized computations that execute efficiently on a GPU, introducing
run-time overhead of less than one millisecond on current hardware -- even on
large, widely-used networks containing hundreds of thousands of neurons and
millions of parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1"&gt;Klas Leino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fromherz_A/0/1/0/all/0/1"&gt;Aymeric Fromherz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangal_R/0/1/0/all/0/1"&gt;Ravi Mangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1"&gt;Matt Fredrikson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parno_B/0/1/0/all/0/1"&gt;Bryan Parno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1"&gt;Corina P&amp;#x103;s&amp;#x103;reanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08251</id>
        <link href="http://arxiv.org/abs/2107.08251"/>
        <updated>2021-07-27T02:03:35.510Z</updated>
        <summary type="html"><![CDATA[We introduce ParaBLEU, a paraphrase representation learning model and
evaluation metric for text generation. Unlike previous approaches, ParaBLEU
learns to understand paraphrasis using generative conditioning as a pretraining
objective. ParaBLEU correlates more strongly with human judgements than
existing metrics, obtaining new state-of-the-art results on the 2017 WMT
Metrics Shared Task. We show that our model is robust to data scarcity,
exceeding previous state-of-the-art performance using only $50\%$ of the
available training data and surpassing BLEU, ROUGE and METEOR with only $40$
labelled examples. Finally, we demonstrate that ParaBLEU can be used to
conditionally generate novel paraphrases from a single demonstration, which we
use to confirm our hypothesis that it learns abstract, generalized paraphrase
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jack Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1"&gt;Raphael Lenain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1"&gt;Udeepa Meepegama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1"&gt;Emil Fristed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11621</id>
        <link href="http://arxiv.org/abs/2107.11621"/>
        <updated>2021-07-27T02:03:35.485Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a solution for privacy challenge, which allows
multiparty to train a shared model without violating privacy protection
regulations. Many excellent works of FL have been proposed in recent years. To
help researchers verify their ideas in FL, we designed and developed FedLab, a
flexible and modular FL framework based on PyTorch. In this paper, we will
introduce architecture and features of FedLab. For current popular research
points: optimization and communication compression, FedLab provides functional
interfaces and a series of baseline implementation are available, making
researchers quickly implement ideas. In addition, FedLab is scale-able in both
client simulation and distributed communication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1"&gt;Dun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Siqi Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiangjing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?. (arXiv:2107.11587v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11587</id>
        <link href="http://arxiv.org/abs/2107.11587"/>
        <updated>2021-07-27T02:03:35.479Z</updated>
        <summary type="html"><![CDATA[We contribute to micro-data model-based reinforcement learning (MBRL) by
rigorously comparing popular generative models using a fixed (random shooting)
control agent. We find that on an environment that requires multimodal
posterior predictives, mixture density nets outperform all other models by a
large margin. When multimodality is not required, our surprising finding is
that we do not need probabilistic posterior predictives: deterministic models
are on par, in fact they consistently (although non-significantly) outperform
their probabilistic counterparts. We also found that heteroscedasticity at
training time, perhaps acting as a regularizer, improves predictions at longer
horizons. At the methodological side, we design metrics and an experimental
protocol which can be used to evaluate the various models, predicting their
asymptotic performance when using them on the control problem. Using this
framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot
by two to four folds, using an aggressive training schedule which is outside of
the hyperparameter interval usually considered]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kegl_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs K&amp;#xe9;gl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hurtado_G/0/1/0/all/0/1"&gt;Gabriel Hurtado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1"&gt;Albert Thomas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Denoising Flows. (arXiv:2107.11625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11625</id>
        <link href="http://arxiv.org/abs/2107.11625"/>
        <updated>2021-07-27T02:03:35.472Z</updated>
        <summary type="html"><![CDATA[Discrete flow-based models are a recently proposed class of generative models
that learn invertible transformations for discrete random variables. Since they
do not require data dequantization and maximize an exact likelihood objective,
they can be used in a straight-forward manner for lossless compression. In this
paper, we introduce a new discrete flow-based model for categorical random
variables: Discrete Denoising Flows (DDFs). In contrast with other discrete
flow-based models, our model can be locally trained without introducing
gradient bias. We show that DDFs outperform Discrete Flows on modeling a toy
example, binary MNIST and Cityscapes segmentation maps, measured in
log-likelihood.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindt_A/0/1/0/all/0/1"&gt;Alexandra Lindt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1"&gt;Emiel Hoogeboom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HierMUD: Hierarchical Multi-task Unsupervised Domain Adaptation between Bridges for Drive-by Damage Diagnosis. (arXiv:2107.11435v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.11435</id>
        <link href="http://arxiv.org/abs/2107.11435"/>
        <updated>2021-07-27T02:03:35.466Z</updated>
        <summary type="html"><![CDATA[Monitoring bridge health using vibrations of drive-by vehicles has various
benefits, such as no need for directly installing and maintaining sensors on
the bridge. However, many of the existing drive-by monitoring approaches are
based on supervised learning models that require labeled data from every bridge
of interest, which is expensive and time-consuming, if not impossible, to
obtain. To this end, we introduce a new framework that transfers the model
learned from one bridge to diagnose damage in another bridge without any labels
from the target bridge. Our framework trains a hierarchical neural network
model in an adversarial way to extract task-shared and task-specific features
that are informative to multiple diagnostic tasks and invariant across multiple
bridges. We evaluate our framework on experimental data collected from 2
bridges and 3 vehicles. We achieve accuracies of 95% for damage detection, 93%
for localization, and up to 72% for quantification, which are ~2 times
improvements from baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Susu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berges_M/0/1/0/all/0/1"&gt;Mario Berg&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1"&gt;Hae Young Noh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Hyperbolic Neural Networks with Limited Radii. (arXiv:2107.11472v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11472</id>
        <link href="http://arxiv.org/abs/2107.11472"/>
        <updated>2021-07-27T02:03:35.460Z</updated>
        <summary type="html"><![CDATA[Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic
space, has attracted sustained attention in the community of machine learning.
Hyperbolic space, owing to its ability to embed hierarchical structures
continuously with low distortion, has been applied for learning data with
tree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly
in hyperbolic space have also been proposed recently to further exploit the
potential of hyperbolic representations. While HNNs have achieved better
performance than Euclidean neural networks (ENNs) on datasets with implicit
hierarchical structure, they still perform poorly on standard classification
benchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is
critical for the data to respect the hyperbolic geometry when applying HNNs. In
this paper, we first conduct an empirical study showing that the inferior
performance of HNNs on standard recognition datasets can be attributed to the
notorious vanishing gradient problem. We further discovered that this problem
stems from the hybrid architecture of HNNs. Our analysis leads to a simple yet
effective solution called Feature Clipping, which regularizes the hyperbolic
embedding whenever its norm exceeding a given threshold. Our thorough
experiments show that the proposed method can successfully avoid the vanishing
gradient problem when training HNNs with backpropagation. The improved HNNs are
able to achieve comparable performance with ENNs on standard image recognition
datasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating
more adversarial robustness and stronger out-of-distribution detection
capability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yunhui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xudong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition. (arXiv:2107.11442v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11442</id>
        <link href="http://arxiv.org/abs/2107.11442"/>
        <updated>2021-07-27T02:03:35.449Z</updated>
        <summary type="html"><![CDATA[We present a novel global compression framework for deep neural networks that
automatically analyzes each layer to identify the optimal per-layer compression
ratio, while simultaneously achieving the desired overall compression. Our
algorithm hinges on the idea of compressing each convolutional (or
fully-connected) layer by slicing its channels into multiple groups and
decomposing each group via low-rank decomposition. At the core of our algorithm
is the derivation of layer-wise error bounds from the Eckart Young Mirsky
theorem. We then leverage these bounds to frame the compression problem as an
optimization problem where we wish to minimize the maximum compression error
across layers and propose an efficient algorithm towards a solution. Our
experiments indicate that our method outperforms existing low-rank compression
approaches across a wide range of networks and data sets. We believe that our
results open up new avenues for future research into the global
performance-size trade-offs of modern neural networks. Our code is available at
https://github.com/lucaslie/torchprune.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1"&gt;Alaa Maalouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_O/0/1/0/all/0/1"&gt;Oren Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1"&gt;Dan Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASOD60K: Audio-Induced Salient Object Detection in Panoramic Videos. (arXiv:2107.11629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11629</id>
        <link href="http://arxiv.org/abs/2107.11629"/>
        <updated>2021-07-27T02:03:35.422Z</updated>
        <summary type="html"><![CDATA[Exploring to what humans pay attention in dynamic panoramic scenes is useful
for many fundamental applications, including augmented reality (AR) in retail,
AR-powered recruitment, and visual language navigation. With this goal in mind,
we propose PV-SOD, a new task that aims to segment salient objects from
panoramic videos. In contrast to existing fixation-level or object-level
saliency detection tasks, we focus on multi-modal salient object detection
(SOD), which mimics human attention mechanism by segmenting salient objects
with the guidance of audio-visual cues. To support this task, we collect the
first large-scale dataset, named ASOD60K, which contains 4K-resolution video
frames annotated with a six-level hierarchy, thus distinguishing itself with
richness, diversity and quality. Specifically, each sequence is marked with
both its super-/sub-class, with objects of each sub-class being further
annotated with human eye fixations, bounding boxes, object-/instance-level
masks, and associated attributes (e.g., geometrical distortion). These
coarse-to-fine annotations enable detailed analysis for PV-SOD modeling, e.g.,
determining the major challenges for existing SOD models, and predicting
scanpaths to study the long-term eye fixation behaviors of humans. We
systematically benchmark 11 representative approaches on ASOD60K and derive
several interesting findings. We hope this study could serve as a good starting
point for advancing SOD research towards panoramic videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fang-Yi Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them. (arXiv:2107.11630v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11630</id>
        <link href="http://arxiv.org/abs/2107.11630"/>
        <updated>2021-07-27T02:03:35.415Z</updated>
        <summary type="html"><![CDATA[Making classifiers robust to adversarial examples is hard. Thus, many
defenses tackle the seemingly easier task of detecting perturbed inputs. We
show a barrier towards this goal. We prove a general hardness reduction between
detection and classification of adversarial examples: given a robust detector
for attacks at distance {\epsilon} (in some metric), we can build a similarly
robust (but inefficient) classifier for attacks at distance {\epsilon}/2. Our
reduction is computationally inefficient, and thus cannot be used to build
practical classifiers. Instead, it is a useful sanity check to test whether
empirical detection results imply something much stronger than the authors
presumably anticipated. To illustrate, we revisit 13 detector defenses. For
11/13 cases, we show that the claimed detection results would imply an
inefficient classifier with robustness far beyond the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1"&gt;Florian Tram&amp;#xe8;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Content Delivery Networks Security Framework Using Optimized Unsupervised Anomaly Detection. (arXiv:2107.11514v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.11514</id>
        <link href="http://arxiv.org/abs/2107.11514"/>
        <updated>2021-07-27T02:03:35.407Z</updated>
        <summary type="html"><![CDATA[Content delivery networks (CDNs) provide efficient content distribution over
the Internet. CDNs improve the connectivity and efficiency of global
communications, but their caching mechanisms may be breached by
cyber-attackers. Among the security mechanisms, effective anomaly detection
forms an important part of CDN security enhancement. In this work, we propose a
multi-perspective unsupervised learning framework for anomaly detection in
CDNs. In the proposed framework, a multi-perspective feature engineering
approach, an optimized unsupervised anomaly detection model that utilizes an
isolation forest and a Gaussian mixture model, and a multi-perspective
validation method, are developed to detect abnormal behaviors in CDNs mainly
from the client Internet Protocol (IP) and node perspectives, therefore to
identify the denial of service (DoS) and cache pollution attack (CPA) patterns.
Experimental results are presented based on the analytics of eight days of
real-world CDN log data provided by a major CDN operator. Through experiments,
the abnormal contents, compromised nodes, malicious IPs, as well as their
corresponding attack types, are identified effectively by the proposed
framework and validated by multiple cybersecurity experts. This shows the
effectiveness of the proposed method when applied to real-world CDN data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Li Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_A/0/1/0/all/0/1"&gt;Abdallah Moubayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shami_A/0/1/0/all/0/1"&gt;Abdallah Shami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_P/0/1/0/all/0/1"&gt;Parisa Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boukhtouta_A/0/1/0/all/0/1"&gt;Amine Boukhtouta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larabi_A/0/1/0/all/0/1"&gt;Adel Larabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunner_R/0/1/0/all/0/1"&gt;Richard Brunner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preda_S/0/1/0/all/0/1"&gt;Stere Preda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Migault_D/0/1/0/all/0/1"&gt;Daniel Migault&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity of Privately Learning Axis-Aligned Rectangles. (arXiv:2107.11526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11526</id>
        <link href="http://arxiv.org/abs/2107.11526"/>
        <updated>2021-07-27T02:03:35.400Z</updated>
        <summary type="html"><![CDATA[We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a
finite grid $X^d\subseteq{\mathbb{R}}^d$ with differential privacy. Existing
results show that the sample complexity of this problem is at most $\min\left\{
d{\cdot}\log|X| \;,\; d^{1.5}{\cdot}\left(\log^*|X| \right)^{1.5}\right\}$.
That is, existing constructions either require sample complexity that grows
linearly with $\log|X|$, or else it grows super linearly with the dimension
$d$. We present a novel algorithm that reduces the sample complexity to only
$\tilde{O}\left\{d{\cdot}\left(\log^*|X|\right)^{1.5}\right\}$, attaining a
dimensionality optimal dependency without requiring the sample complexity to
grow with $\log|X|$.The technique used in order to attain this improvement
involves the deletion of "exposed" data-points on the go, in a fashion designed
to avoid the cost of the adaptive composition theorems. The core of this
technique may be of individual interest, introducing a new method for
constructing statistically-efficient private algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadigurschi_M/0/1/0/all/0/1"&gt;Menachem Sadigurschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1"&gt;Uri Stemmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAConv: Local Adaptive Convolution for Image Fusion. (arXiv:2107.11617v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11617</id>
        <link href="http://arxiv.org/abs/2107.11617"/>
        <updated>2021-07-27T02:03:35.375Z</updated>
        <summary type="html"><![CDATA[The convolution operation is a powerful tool for feature extraction and plays
a prominent role in the field of computer vision. However, when targeting the
pixel-wise tasks like image fusion, it would not fully perceive the
particularity of each pixel in the image if the uniform convolution kernel is
used on different patches. In this paper, we propose a local adaptive
convolution (LAConv), which is dynamically adjusted to different spatial
locations. LAConv enables the network to pay attention to every specific local
area in the learning process. Besides, the dynamic bias (DYB) is introduced to
provide more possibilities for the depiction of features and make the network
more flexible. We further design a residual structure network equipped with the
proposed LAConv and DYB modules, and apply it to two image fusion tasks.
Experiments for pansharpening and hyperspectral image super-resolution (HISR)
demonstrate the superiority of our method over other state-of-the-art methods.
It is worth mentioning that LAConv can also be competent for other
super-resolution tasks with less computation effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zi-Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Liang-Jian Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tai-Xiang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tian-Jing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\mu$DARTS: Model Uncertainty-Aware Differentiable Architecture Search. (arXiv:2107.11500v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11500</id>
        <link href="http://arxiv.org/abs/2107.11500"/>
        <updated>2021-07-27T02:03:35.280Z</updated>
        <summary type="html"><![CDATA[We present a Model Uncertainty-aware Differentiable ARchiTecture Search
($\mu$DARTS) that optimizes neural networks to simultaneously achieve high
accuracy and low uncertainty. We introduce concrete dropout within DARTS cells
and include a Monte-Carlo regularizer within the training loss to optimize the
concrete dropout probabilities. A predictive variance term is introduced in the
validation loss to enable searching for architecture with minimal model
uncertainty. The experiments on CIFAR10, CIFAR100, SVHN, and ImageNet verify
the effectiveness of $\mu$DARTS in improving accuracy and reducing uncertainty
compared to existing DARTS methods. Moreover, the final architecture obtained
from $\mu$DARTS shows higher robustness to noise at the input image and model
parameters compared to the architecture obtained from existing DARTS methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1"&gt;Biswadeep Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1"&gt;Saibal Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training multi-objective/multi-task collocation physics-informed neural network with student/teachers transfer learnings. (arXiv:2107.11496v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11496</id>
        <link href="http://arxiv.org/abs/2107.11496"/>
        <updated>2021-07-27T02:03:35.261Z</updated>
        <summary type="html"><![CDATA[This paper presents a PINN training framework that employs (1) pre-training
steps that accelerates and improve the robustness of the training of
physics-informed neural network with auxiliary data stored in point clouds, (2)
a net-to-net knowledge transfer algorithm that improves the weight
initialization of the neural network and (3) a multi-objective optimization
algorithm that may improve the performance of a physical-informed neural
network with competing constraints. We consider the training and transfer and
multi-task learning of physics-informed neural network (PINN) as
multi-objective problems where the physics constraints such as the governing
equation, boundary conditions, thermodynamic inequality, symmetry, and
invariant properties, as well as point cloud used for pre-training can
sometimes lead to conflicts and necessitating the seek of the Pareto optimal
solution. In these situations, weighted norms commonly used to handle multiple
constraints may lead to poor performance, while other multi-objective
algorithms may scale poorly with increasing dimensionality. To overcome this
technical barrier, we adopt the concept of vectorized objective function and
modify a gradient descent approach to handle the issue of conflicting
gradients. Numerical experiments are compared the benchmark boundary value
problems solved via PINN. The performance of the proposed paradigm is compared
against the classical equal-weighted norm approach. Our numerical experiments
indicate that the brittleness and lack of robustness demonstrated in some PINN
implementations can be overcome with the proposed strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahmani_B/0/1/0/all/0/1"&gt;Bahador Bahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;WaiChing Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A general sample complexity analysis of vanilla policy gradient. (arXiv:2107.11433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11433</id>
        <link href="http://arxiv.org/abs/2107.11433"/>
        <updated>2021-07-27T02:03:35.252Z</updated>
        <summary type="html"><![CDATA[The policy gradient (PG) is one of the most popular methods for solving
reinforcement learning (RL) problems. However, a solid theoretical
understanding of even the "vanilla" PG has remained elusive for long time. In
this paper, we apply recent tools developed for the analysis of SGD in
non-convex optimization to obtain convergence guarantees for both REINFORCE and
GPOMDP under smoothness assumption on the objective function and weak
conditions on the second moment of the norm of the estimated gradient. When
instantiated under common assumptions on the policy space, our general result
immediately recovers existing $\widetilde{\mathcal{O}}(\epsilon^{-4})$ sample
complexity guarantees, but for wider ranges of parameters (e.g., step size and
batch size $m$) with respect to previous literature. Notably, our result
includes the single trajectory case (i.e., $m=1$) and it provides a more
accurate analysis of the dependency on problem-specific parameters by fixing
previous results available in the literature. We believe that the integration
of state-of-the-art tools from non-convex optimization may lead to identify a
much broader range of problems where PG methods enjoy strong theoretical
guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1"&gt;Rui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1"&gt;Robert M. Gower&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2105.10843v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10843</id>
        <link href="http://arxiv.org/abs/2105.10843"/>
        <updated>2021-07-27T02:03:35.246Z</updated>
        <summary type="html"><![CDATA[Recent studies imply that deep neural networks are vulnerable to adversarial
examples -- inputs with a slight but intentional perturbation are incorrectly
classified by the network. Such vulnerability makes it risky for some
security-related applications (e.g., semantic segmentation in autonomous cars)
and triggers tremendous concerns on the model reliability. For the first time,
we comprehensively evaluate the robustness of existing UDA methods and propose
a robust UDA approach. It is rooted in two observations: (i) the robustness of
UDA methods in semantic segmentation remains unexplored, which pose a security
concern in this field; and (ii) although commonly used self-supervision (e.g.,
rotation and jigsaw) benefits image tasks such as classification and
recognition, they fail to provide the critical supervision signals that could
learn discriminative representation for segmentation tasks. These observations
motivate us to propose adversarial self-supervision UDA (or ASSUDA) that
maximizes the agreement between clean images and their adversarial examples by
a contrastive loss in the output space. Extensive empirical studies on commonly
used benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1"&gt;Weizhi An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hehuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuzhi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images. (arXiv:2107.11468v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11468</id>
        <link href="http://arxiv.org/abs/2107.11468"/>
        <updated>2021-07-27T02:03:35.239Z</updated>
        <summary type="html"><![CDATA[We analyze a dataset of retinal images using linear probes: linear regression
models trained on some "target" task, using embeddings from a deep
convolutional (CNN) model trained on some "source" task as input. We use this
method across all possible pairings of 93 tasks in the UK Biobank dataset of
retinal images, leading to ~164k different models. We analyze the performance
of these linear probes by source and target task and by layer depth. We observe
that representations from the middle layers of the network are more
generalizable. We find that some target tasks are easily predicted irrespective
of the source task, and that some other target tasks are more accurately
predicted from correlated source tasks than from embeddings trained on the same
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1"&gt;Katy Blumer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1"&gt;Michael P. Brenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1"&gt;Jon Kleinberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06825</id>
        <link href="http://arxiv.org/abs/2107.06825"/>
        <updated>2021-07-27T02:03:35.213Z</updated>
        <summary type="html"><![CDATA[We introduce a generalization to the lottery ticket hypothesis in which the
notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this "generalized" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1"&gt;Ilya Tolstikhin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08751</id>
        <link href="http://arxiv.org/abs/2107.08751"/>
        <updated>2021-07-27T02:03:35.207Z</updated>
        <summary type="html"><![CDATA[Deep learning for medical imaging suffers from temporal and privacy-related
restrictions on data availability. To still obtain viable models, continual
learning aims to train in sequential order, as and when data is available. The
main challenge that continual learning methods face is to prevent catastrophic
forgetting, i.e., a decrease in performance on the data encountered earlier.
This issue makes continuous training of segmentation models for medical
applications extremely difficult. Yet, often, data from at least two different
domains is available which we can exploit to train the model in a way that it
disregards domain-specific information. We propose an architecture that
leverages the simultaneous availability of two or more datasets to learn a
disentanglement between the content and domain in an adversarial fashion. The
domain-invariant content representation then lays the base for continual
semantic segmentation. Our approach takes inspiration from domain adaptation
and combines it with continual learning for hippocampal segmentation in brain
MRI. We showcase that our method reduces catastrophic forgetting and
outperforms state-of-the-art continual learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1"&gt;Marius Memmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1"&gt;Camila Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Anirban Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saliency-Guided Deep Learning Network for Automatic Tumor Bed Volume Delineation in Post-operative Breast Irradiation. (arXiv:2105.02771v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02771</id>
        <link href="http://arxiv.org/abs/2105.02771"/>
        <updated>2021-07-27T02:03:35.200Z</updated>
        <summary type="html"><![CDATA[Efficient, reliable and reproducible target volume delineation is a key step
in the effective planning of breast radiotherapy. However, post-operative
breast target delineation is challenging as the contrast between the tumor bed
volume (TBV) and normal breast tissue is relatively low in CT images. In this
study, we propose to mimic the marker-guidance procedure in manual target
delineation. We developed a saliency-based deep learning segmentation (SDL-Seg)
algorithm for accurate TBV segmentation in post-operative breast irradiation.
The SDL-Seg algorithm incorporates saliency information in the form of markers'
location cues into a U-Net model. The design forces the model to encode the
location-related features, which underscores regions with high saliency levels
and suppresses low saliency regions. The saliency maps were generated by
identifying markers on CT images. Markers' locations were then converted to
probability maps using a distance-transformation coupled with a Gaussian
filter. Subsequently, the CT images and the corresponding saliency maps formed
a multi-channel input for the SDL-Seg network. Our in-house dataset was
comprised of 145 prone CT images from 29 post-operative breast cancer patients,
who received 5-fraction partial breast irradiation (PBI) regimen on GammaPod.
The performance of the proposed method was compared against basic U-Net. Our
model achieved mean (standard deviation) of 76.4 %, 6.76 mm, and 1.9 mm for
DSC, HD95, and ASD respectively on the test set with computation time of below
11 seconds per one CT volume. SDL-Seg showed superior performance relative to
basic U-Net for all the evaluation metrics while preserving low computation
cost. The findings demonstrate that SDL-Seg is a promising approach for
improving the efficiency and accuracy of the on-line treatment planning
procedure of PBI, such as GammaPod based PBI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kazemimoghadam_M/0/1/0/all/0/1"&gt;Mahdieh Kazemimoghadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chi_W/0/1/0/all/0/1"&gt;Weicheng Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahimi_A/0/1/0/all/0/1"&gt;Asal Rahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_N/0/1/0/all/0/1"&gt;Nathan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alluri_P/0/1/0/all/0/1"&gt;Prasanna Alluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nwachukwu_C/0/1/0/all/0/1"&gt;Chika Nwachukwu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weiguo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xuejun Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10777</id>
        <link href="http://arxiv.org/abs/2106.10777"/>
        <updated>2021-07-27T02:03:35.176Z</updated>
        <summary type="html"><![CDATA[We propose a manifold matching approach to generative models which includes a
distribution generator (or data generator) and a metric generator. In our
framework, we view the real data set as some manifold embedded in a
high-dimensional Euclidean space. The distribution generator aims at generating
samples that follow some distribution condensed around the real data manifold.
It is achieved by matching two sets of points using their geometric shape
descriptors, such as centroid and $p$-diameter, with learned distance metric;
the metric generator utilizes both real data and generated samples to learn a
distance metric which is close to some intrinsic geodesic distance on the real
data manifold. The produced distance metric is further used for manifold
matching. The two networks are learned simultaneously during the training
process. We apply the approach on both unsupervised and supervised learning
tasks: in unconditional image generation task, the proposed method obtains
competitive results compared with existing generative models; in
super-resolution task, we incorporate the framework in perception-based models
and improve visual qualities by producing samples with more natural textures.
Both theoretical analysis and real data experiments demonstrate the feasibility
and effectiveness of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1"&gt;Mengyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1"&gt;Haibin Hang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07647</id>
        <link href="http://arxiv.org/abs/2107.07647"/>
        <updated>2021-07-27T02:03:35.169Z</updated>
        <summary type="html"><![CDATA[A novel energy-efficient edge computing paradigm is proposed for real-time
deep learning-based image upsampling applications. State-of-the-art deep
learning solutions for image upsampling are currently trained using either
resize or sub-pixel convolution to learn kernels that generate high fidelity
images with minimal artifacts. However, performing inference with these learned
convolution kernels requires memory-intensive feature map transformations that
dominate time and energy costs in real-time applications. To alleviate this
pressure on memory bandwidth, we confine the use of resize or sub-pixel
convolution to training in the cloud by transforming learned convolution
kernels to deconvolution kernels before deploying them for inference as a
functionally equivalent deconvolution. These kernel transformations, intended
as a one-time cost when shifting from training to inference, enable a systems
designer to use each algorithm in their optimal context by preserving the image
fidelity learned when training in the cloud while minimizing data transfer
penalties during inference at the edge. We also explore existing variants of
deconvolution inference algorithms and introduce a novel variant for
consideration. We analyze and compare the inference properties of
convolution-based upsampling algorithms using a quantitative model of incurred
time and energy costs and show that using deconvolution for inference at the
edge improves both system latency and energy efficiency when compared to their
sub-pixel or resize convolution counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1"&gt;Ian Colbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1"&gt;Ken Kreutz-Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Srinjoy Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepPlastic: A Novel Approach to Detecting Epipelagic Bound Plastic Using Deep Visual Models. (arXiv:2105.01882v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01882</id>
        <link href="http://arxiv.org/abs/2105.01882"/>
        <updated>2021-07-27T02:03:35.162Z</updated>
        <summary type="html"><![CDATA[The quantification of positively buoyant marine plastic debris is critical to
understanding how concentrations of trash from across the world's ocean and
identifying high concentration garbage hotspots in dire need of trash removal.
Currently, the most common monitoring method to quantify floating plastic
requires the use of a manta trawl. Techniques requiring manta trawls (or
similar surface collection devices) utilize physical removal of marine plastic
debris as the first step and then analyze collected samples as a second step.
The need for physical removal before analysis incurs high costs and requires
intensive labor preventing scalable deployment of a real-time marine plastic
monitoring service across the entirety of Earth's ocean bodies. Without better
monitoring and sampling methods, the total impact of plastic pollution on the
environment as a whole, and details of impact within specific oceanic regions,
will remain unknown. This study presents a highly scalable workflow that
utilizes images captured within the epipelagic layer of the ocean as an input.
It produces real-time quantification of marine plastic debris for accurate
quantification and physical removal. The workflow includes creating and
preprocessing a domain-specific dataset, building an object detection model
utilizing a deep neural network, and evaluating the model's performance.
YOLOv5-S was the best performing model, which operates at a Mean Average
Precision (mAP) of 0.851 and an F1-Score of 0.89 while maintaining
near-real-time speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tata_G/0/1/0/all/0/1"&gt;Gautam Tata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Royer_S/0/1/0/all/0/1"&gt;Sarah-Jeanne Royer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poirion_O/0/1/0/all/0/1"&gt;Olivier Poirion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_J/0/1/0/all/0/1"&gt;Jay Lowe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02874</id>
        <link href="http://arxiv.org/abs/2106.02874"/>
        <updated>2021-07-27T02:03:35.155Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled
source domain and an unsupervised loss in an unlabeled target domain, which
often faces more severe overfitting (than classical supervised learning) as the
supervised source loss has clear domain gap and the unsupervised target loss is
often noisy due to the lack of annotations. This paper presents RDA, a robust
domain adaptation technique that introduces adversarial attacking to mitigate
overfitting in UDA. We achieve robust domain adaptation by a novel Fourier
adversarial attacking (FAA) method that allows large magnitude of perturbation
noises but has minimal modification of image semantics, the former is critical
to the effectiveness of its generated adversarial samples due to the existence
of 'domain gaps'. Specifically, FAA decomposes images into multiple frequency
components (FCs) and generates adversarial samples by just perturbating certain
FCs that capture little semantic information. With FAA-generated samples, the
training can continue the 'random walk' and drift into an area with a flat loss
landscape, leading to more robust domain adaptation. Extensive experiments over
multiple domain adaptation tasks show that RDA can work with different computer
vision tasks with superior performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05528</id>
        <link href="http://arxiv.org/abs/2105.05528"/>
        <updated>2021-07-27T02:03:35.148Z</updated>
        <summary type="html"><![CDATA[The use of gait for person identification has important advantages such as
being non-invasive, unobtrusive, not requiring cooperation and being less
likely to be obscured compared to other biometrics. Existing methods for gait
recognition require cooperative gait scenarios, in which a single person is
walking multiple times in a straight line in front of a camera. We aim to
address the hard challenges of real-world scenarios in which camera feeds
capture multiple people, who in most cases pass in front of the camera only
once. We address privacy concerns by using only the motion information of
walking individuals, with no identifiable appearance-based information. As
such, we propose a novel weakly supervised learning framework, WildGait, which
consists of training a Spatio-Temporal Graph Convolutional Network on a large
number of automatically annotated skeleton sequences obtained from raw,
real-world, surveillance streams to learn useful gait signatures. Our results
show that, with fine-tuning, we surpass in terms of recognition accuracy the
current state-of-the-art pose-based gait recognition solutions. Our proposed
method is reliable in training gait recognition methods in unconstrained
environments, especially in settings with scarce amounts of annotated data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1"&gt;Adrian Cosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1"&gt;Emilian Radoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial expression and attributes recognition based on multi-task learning of lightweight neural networks. (arXiv:2103.17107v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17107</id>
        <link href="http://arxiv.org/abs/2103.17107"/>
        <updated>2021-07-27T02:03:35.140Z</updated>
        <summary type="html"><![CDATA[In this paper, the multi-task learning of lightweight convolutional neural
networks is studied for face identification and classification of facial
attributes (age, gender, ethnicity) trained on cropped faces without margins.
The necessity to fine-tune these networks to predict facial expressions is
highlighted. Several models are presented based on MobileNet, EfficientNet and
RexNet architectures. It was experimentally demonstrated that they lead to near
state-of-the-art results in age, gender and race recognition on the UTKFace
dataset and emotion classification on the AffectNet dataset. Moreover, it is
shown that the usage of the trained models as feature extractors of facial
regions in video frames leads to 4.5% higher accuracy than the previously known
state-of-the-art single models for the AFEW and the VGAF datasets from the
EmotiW challenges. The models and source code are publicly available at
https://github.com/HSE-asavchenko/face-emotion-recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1"&gt;Andrey V. Savchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10937</id>
        <link href="http://arxiv.org/abs/2105.10937"/>
        <updated>2021-07-27T02:03:35.133Z</updated>
        <summary type="html"><![CDATA[Terrain traversability analysis plays a major role in ensuring safe robotic
navigation in unstructured environments. However, real-time constraints
frequently limit the accuracy of online tests especially in scenarios where
realistic robot-terrain interactions are complex to model. In this context, we
propose a deep learning framework trained in an end-to-end fashion from
elevation maps and trajectories to estimate the occurrence of failure events.
The network is first trained and tested in simulation over synthetic maps
generated by the OpenSimplex algorithm. The prediction performance of the Deep
Learning framework is illustrated by being able to retain over 94% recall of
the original simulator at 30% of the computational time. Finally, the network
is transferred and tested on real elevation maps collected by the SEEKER
consortium during the Martian rover test trial in the Atacama desert in Chile.
We show that transferring and fine-tuning of an application-independent
pre-trained model retains better performance than training uniquely on scarcely
available real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1"&gt;Marco Visca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1"&gt;Roger Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training. (arXiv:2107.05736v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05736</id>
        <link href="http://arxiv.org/abs/2107.05736"/>
        <updated>2021-07-27T02:03:35.121Z</updated>
        <summary type="html"><![CDATA[Facial expression recognition (FER) in the wild is crucial for building
reliable human-computer interactive systems. However, annotations of large
scale datasets in FER has been a key challenge as these datasets suffer from
noise due to various factors like crowd sourcing, subjectivity of annotators,
poor quality of images, automatic labelling based on key word search etc. Such
noisy annotations impede the performance of FER due to the memorization ability
of deep networks. During early learning stage, deep networks fit on clean data.
Then, eventually, they start overfitting on noisy labels due to their
memorization ability, which limits FER performance. This report presents
Consensual Collaborative Training (CCT) framework used in our submission to
expression recognition track of the Affective Behaviour Analysis in-the-wild
(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex
combination of supervision loss and consistency loss, without making any
assumption about the noise distribution. A dynamic transition mechanism is used
to move from supervision loss in early learning to consistency loss for
consensus of predictions among networks in the later stage. Co-training reduces
overall error, and consistency loss prevents overfitting to noisy samples. The
performance of the model is validated on challenging Aff-Wild2 dataset for
categorical expression classification. Our code is made publicly available at
https://github.com/1980x/ABAW2021DMACS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1"&gt;Darshan Gera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1"&gt;S Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04281</id>
        <link href="http://arxiv.org/abs/2107.04281"/>
        <updated>2021-07-27T02:03:35.098Z</updated>
        <summary type="html"><![CDATA[Image inpainting aims to restore the missing regions and make the recovery
results identical to the originally complete image, which is different from the
common generative task emphasizing the naturalness of generated images.
Nevertheless, existing works usually regard it as a pure generation problem and
employ cutting-edge generative techniques to address it. The generative
networks fill the main missing parts with realistic contents but usually
distort the local structures. In this paper, we formulate image inpainting as a
mix of two problems, i.e., predictive filtering and deep generation. Predictive
filtering is good at preserving local structures and removing artifacts but
falls short to complete the large missing regions. The deep generative network
can fill the numerous missing pixels based on the understanding of the whole
scene but hardly restores the details identical to the original ones. To make
use of their respective advantages, we propose the joint predictive filtering
and generative network (JPGNet) that contains three branches: predictive
filtering & uncertainty network (PFUNet), deep generative network, and
uncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict
pixel-wise kernels for filtering-based inpainting according to the input image
and output an uncertainty map. This map indicates the pixels should be
processed by filtering or generative networks, which is further fed to the
UAFNet for a smart combination between filtering and generative results. Note
that, our method as a novel framework for the image inpainting problem can
benefit any existing generation-based methods. We validate our method on three
public datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our
method can enhance three state-of-the-art generative methods (i.e., StructFlow,
EdgeConnect, and RFRNet) significantly with the slightly extra time cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1"&gt;Song wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-07-27T02:03:35.070Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[G2DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person Re-Identification. (arXiv:2106.07853v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07853</id>
        <link href="http://arxiv.org/abs/2106.07853"/>
        <updated>2021-07-27T02:03:35.063Z</updated>
        <summary type="html"><![CDATA[RGB-Infrared (IR) person re-identification aims to retrieve
person-of-interest from heterogeneous cameras, easily suffering from large
image modality discrepancy caused by different sensing wavelength ranges.
Existing work usually minimizes such discrepancy by aligning domain
distribution of global features, while neglecting the intra-modality structural
relations between semantic parts. This could result in the network overly
focusing on local cues, without considering long-range body part dependencies,
leading to meaningless region representations. In this paper, we propose a
graph-enabled distribution matching solution, dubbed Geometry-Guided
Dual-Alignment (G2DA) learning, for RGB-IR ReID. It can jointly encourage the
cross-modal consistency between part semantics and structural relations for
fine-grained modality alignment by solving a graph matching task within a
multi-scale skeleton graph that embeds human topology information.
Specifically, we propose to build a semantic-aligned complete graph into which
all cross-modality images can be mapped via a pose-adaptive graph construction
mechanism. This graph represents extracted whole-part features by nodes and
expresses the node-wise similarities with associated edges. To achieve the
graph-based dual-alignment learning, an Optimal Transport (OT) based structured
metric is further introduced to simultaneously measure point-wise relations and
group-wise structural similarities across modalities. By minimizing the cost of
an inter-modality transport plan, G2DA can learn a consistent and
discriminative feature subspace for cross-modality image retrieval.
Furthermore, we advance a Message Fusion Attention (MFA) mechanism to
adaptively reweight the information flow of semantic propagation, effectively
strengthening the discriminability of extracted semantic features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1"&gt;Lin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zongyuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1"&gt;Qianyan Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yehansen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Lijing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhihang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-guided Temporally Coherent Video Object Matting. (arXiv:2105.11427v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11427</id>
        <link href="http://arxiv.org/abs/2105.11427"/>
        <updated>2021-07-27T02:03:35.056Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel deep learning-based video object matting method
that can achieve temporally coherent matting results. Its key component is an
attention-based temporal aggregation module that maximizes image matting
networks' strength for video matting networks. This module computes temporal
correlations for pixels adjacent to each other along the time axis in feature
space, which is robust against motion noises. We also design a novel loss term
to train the attention weights, which drastically boosts the video matting
performance. Besides, we show how to effectively solve the trimap generation
problem by fine-tuning a state-of-the-art video object segmentation network
with a sparse set of user-annotated keyframes. To facilitate video matting and
trimap generation networks' training, we construct a large-scale video matting
dataset with 80 training and 28 validation foreground video clips with
ground-truth alpha mattes. Experimental results show that our method can
generate high-quality alpha mattes for various videos featuring appearance
change, occlusion, and fast motion. Our code and dataset can be found at:
https://github.com/yunkezhang/TCVOM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1"&gt;Miaomiao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Peiran Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-sheng Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weiwei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer. (arXiv:2107.05887v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05887</id>
        <link href="http://arxiv.org/abs/2107.05887"/>
        <updated>2021-07-27T02:03:35.027Z</updated>
        <summary type="html"><![CDATA[We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for
object detection from a sequence of temporal frames. We treat the temporal
frames as sequences in both space and time and employ the full attention
mechanisms to take advantage of the features correlations over both dimensions.
This treatment enables us to deal with frames sequence as temporal object
features traces over every location in the space. We explore two possible
approaches; the early spatial features aggregation over the temporal dimension,
and the late temporal aggregation of object query spatial features. Moreover,
we propose a novel Temporal Positional Embedding technique to encode the time
sequence information. To evaluate our approach, we choose the Moving Object
Detection (MOD)task, since it is a perfect candidate to showcase the importance
of the temporal dimension. Results show a significant 5% mAP improvement on the
KITTI MOD dataset over the 1-step spatial baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multi-Scene Absolute Pose Regression with Transformers. (arXiv:2103.11468v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11468</id>
        <link href="http://arxiv.org/abs/2103.11468"/>
        <updated>2021-07-27T02:03:35.021Z</updated>
        <summary type="html"><![CDATA[Absolute camera pose regressors estimate the position and orientation of a
camera from the captured image alone. Typically, a convolutional backbone with
a multi-layer perceptron head is trained with images and pose labels to embed a
single reference scene at a time. Recently, this scheme was extended for
learning multiple scenes by replacing the MLP head with a set of fully
connected layers. In this work, we propose to learn multi-scene absolute camera
pose regression with Transformers, where encoders are used to aggregate
activation maps with self-attention and decoders transform latent features and
scenes encoding into candidate pose predictions. This mechanism allows our
model to focus on general features that are informative for localization while
embedding multiple scenes in parallel. We evaluate our method on commonly
benchmarked indoor and outdoor datasets and show that it surpasses both
multi-scene and state-of-the-art single-scene absolute pose regressors. We make
our code publicly available from
https://github.com/yolish/multi-scene-pose-transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1"&gt;Yoli Shavit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferens_R/0/1/0/all/0/1"&gt;Ron Ferens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yosi Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07201</id>
        <link href="http://arxiv.org/abs/2107.07201"/>
        <updated>2021-07-27T02:03:35.002Z</updated>
        <summary type="html"><![CDATA[Vision and Language Navigation (VLN) requires an agent to navigate to a
target location by following natural language instructions. Most of existing
works represent a navigation candidate by the feature of the corresponding
single view where the candidate lies in. However, an instruction may mention
landmarks out of the single view as references, which might lead to failures of
textual-visual matching of existing methods. In this work, we propose a
multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate
visual contexts from neighbor views for better textual-visual matching.
Specifically, our NvEM utilizes a subject module and a reference module to
collect contexts from neighbor views. The subject module fuses neighbor views
at a global level, and the reference module fuses neighbor objects at a local
level. Subjects and references are adaptively determined via attention
me'chanisms. Our model also includes an action module to utilize the strong
orientation guidance (e.g., "turn left") in instructions. Each module predicts
navigation action separately and their weighted sum is used for predicting the
final action. Extensive experimental results demonstrate the effectiveness of
the proposed method on the R2R and R4R benchmarks against several
state-of-the-art navigators, and NvEM even beats some pre-training ones. Our
code is available at https://github.com/MarSaKi/NvEM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1"&gt;Dong An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yuankai Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00420</id>
        <link href="http://arxiv.org/abs/2107.00420"/>
        <updated>2021-07-27T02:03:34.995Z</updated>
        <summary type="html"><![CDATA[Modern top-performing object detectors depend heavily on backbone networks,
whose advances bring consistent performance gains through exploring more
effective network structures. In this paper, we propose a novel and flexible
backbone framework, namely CBNetV2, to construct high-performance detectors
using existing open-sourced pre-trained backbones under the pre-training
fine-tuning paradigm. In particular, CBNetV2 architecture groups multiple
identical backbones, which are connected through composite connections.
Specifically, it integrates the high- and low-level features of multiple
backbone networks and gradually expands the receptive field to more efficiently
perform object detection. We also propose a better training strategy with
assistant supervision for CBNet-based detectors. Without additional
pre-training of the composite backbone, CBNetV2 can be adapted to various
backbones (CNN-based vs. Transformer-based) and head designs of most mainstream
detectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based).
Experiments provide strong evidence that, compared with simply increasing the
depth and width of the network, CBNetV2 introduces a more efficient, effective,
and resource-friendly way to build high-performance backbone networks.
Particularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO
test-dev under the single-model and single-scale testing protocol, which is
significantly better than the state-of-the-art result (57.7% box AP and 50.2%
mask AP) achieved by Swin-L, while the training schedule is reduced by
6$\times$. With multi-scale testing, we push the current best single model
result to a new record of 60.1% box AP and 52.3% mask AP without using extra
training data. Code is available at https://github.com/VDIGPKU/CBNetV2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tingting Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiaojie Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yudong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fit4CAD: A point cloud benchmark for fitting simple geometric primitives in CAD models. (arXiv:2105.06858v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06858</id>
        <link href="http://arxiv.org/abs/2105.06858"/>
        <updated>2021-07-27T02:03:34.987Z</updated>
        <summary type="html"><![CDATA[We propose Fit4CAD, a benchmark for the evaluation and comparison of methods
for fitting simple geometric primitives in point clouds representing CAD
models. This benchmark is meant to help both method developers and those who
want to identify the best performing tools. The Fit4CAD dataset is composed by
225 high quality point clouds, each of which has been obtained by sampling a
CAD model. The way these elements were created by using existing platforms and
datasets makes the benchmark easily expandable. The dataset is already split
into a training set and a test set. To assess performance and accuracy of the
different primitive fitting methods, various measures are defined. To
demonstrate the effective use of Fit4CAD, we have tested it on two methods
belonging to two different categories of approaches to the primitive fitting
problem: a clustering method based on a primitive growing framework and a
parametric method based on the Hough transform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romanengo_C/0/1/0/all/0/1"&gt;Chiara Romanengo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffo_A/0/1/0/all/0/1"&gt;Andrea Raffo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qie_Y/0/1/0/all/0/1"&gt;Yifan Qie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwer_N/0/1/0/all/0/1"&gt;Nabil Anwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcidieno_B/0/1/0/all/0/1"&gt;Bianca Falcidieno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14758</id>
        <link href="http://arxiv.org/abs/2105.14758"/>
        <updated>2021-07-27T02:03:34.961Z</updated>
        <summary type="html"><![CDATA[Low-dose CT has been a key diagnostic imaging modality to reduce the
potential risk of radiation overdose to patient health. Despite recent
advances, CNN-based approaches typically apply filters in a spatially invariant
way and adopt similar pixel-level losses, which treat all regions of the CT
image equally and can be inefficient when fine-grained structures coexist with
non-uniformly distributed noises. To address this issue, we propose a
Structure-preserving Kernel Prediction Network (StructKPN) that combines the
kernel prediction network with a structure-aware loss function that utilizes
the pixel gradient statistics and guides the model towards spatially-variant
filters that enhance noise removal, prevent over-smoothing and preserve
detailed structures for different regions in CT imaging. Extensive experiments
demonstrated that our approach achieved superior performance on both synthetic
and non-synthetic datasets, and better preserves structures that are highly
desired in clinical screening and low-dose protocol optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daoye Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jimmy Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zhaoxiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Sparsity for Smoothing Filters. (arXiv:2107.00627v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00627</id>
        <link href="http://arxiv.org/abs/2107.00627"/>
        <updated>2021-07-27T02:03:34.955Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an interesting semi-sparsity smoothing algorithm
based on a novel sparsity-inducing optimization framework. This method is
derived from the multiple observations, that is, semi-sparsity prior knowledge
is more universally applicable, especially in areas where sparsity is not fully
admitted, such as polynomial-smoothing surfaces. We illustrate that this
semi-sparsity can be identified into a generalized $L_0$-norm minimization in
higher-order gradient domains, thereby giving rise to a new "feature-aware"
filtering method with a powerful simultaneous-fitting ability in both sparse
features (singularities and sharpening edges) and non-sparse regions
(polynomial-smoothing surfaces). Notice that a direct solver is always
unavailable due to the non-convexity and combinatorial nature of $L_0$-norm
minimization. Instead, we solve the model based on an efficient half-quadratic
splitting minimization with fast Fourier transforms (FFTs) for acceleration. We
finally demonstrate its versatility and many benefits to a series of
signal/image processing and computer vision applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junqing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuechao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1"&gt;Michael Ruzhansky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07761</id>
        <link href="http://arxiv.org/abs/2107.07761"/>
        <updated>2021-07-27T02:03:34.948Z</updated>
        <summary type="html"><![CDATA[Computer-aided analysis of biological images typically requires extensive
training on large-scale annotated datasets, which is not viable in many
situations. In this paper we present GAN-DL, a Discriminator Learner based on
the StyleGAN2 architecture, which we employ for self-supervised image
representation learning in the case of fluorescent biological images. We show
that Wasserstein Generative Adversarial Networks combined with linear Support
Vector Machines enable high-throughput compound screening based on raw images.
We demonstrate this by classifying active and inactive compounds tested for the
inhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to
previous methods, our deep learning based approach does not require any
annotation besides the one that is normally collected during the sample
preparation process. We test our technique on the RxRx19a Sars-CoV-2 image
collection. The dataset consists of fluorescent images that were generated to
assess the ability of regulatory-approved or in late-stage clinical trials
compound to modulate the in vitro infection from SARS-CoV-2 in both VERO and
HRCE cell lines. We show that our technique can be exploited not only for
classification tasks, but also to effectively derive a dose response curve for
the tested treatments, in a self-supervised manner. Lastly, we demonstrate its
generalization capabilities by successfully addressing a zero-shot learning
task, consisting in the categorization of four different cell types of the
RxRx1 fluorescent images collection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1"&gt;Alessio Mascolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1"&gt;Dario Cardamone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1"&gt;Francesco Ponzio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1"&gt;Santa Di Cataldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1"&gt;Elisa Ficarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Free Non-Stationary RL: Near-Optimal Regret and Applications in Multi-Agent RL and Inventory Control. (arXiv:2010.03161v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03161</id>
        <link href="http://arxiv.org/abs/2010.03161"/>
        <updated>2021-07-27T02:03:34.879Z</updated>
        <summary type="html"><![CDATA[We consider model-free reinforcement learning (RL) in non-stationary Markov
decision processes. Both the reward functions and the state transition
functions are allowed to vary arbitrarily over time as long as their cumulative
variations do not exceed certain variation budgets. We propose Restarted
Q-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free
algorithm for non-stationary RL, and show that it outperforms existing
solutions in terms of dynamic regret. Specifically, RestartQ-UCB with
Freedman-type bonus terms achieves a dynamic regret bound of
$\widetilde{O}(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H
T^{\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions,
respectively, $\Delta>0$ is the variation budget, $H$ is the number of time
steps per episode, and $T$ is the total number of time steps. We further
present a parameter-free algorithm named Double-Restart Q-UCB that does not
require prior knowledge of the variation budget. We show that our algorithms
are \emph{nearly optimal} by establishing an information-theoretical lower
bound of $\Omega(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}}
H^{\frac{2}{3}} T^{\frac{2}{3}})$, the first lower bound in non-stationary RL.
Numerical experiments validate the advantages of RestartQ-UCB in terms of both
cumulative rewards and computational efficiency. We demonstrate the power of
our results in examples of multi-agent RL and inventory control across related
products.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1"&gt;Weichao Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ruihao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basar_T/0/1/0/all/0/1"&gt;Tamer Ba&amp;#x15f;ar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic selection of inducing points in sparse Gaussian processes. (arXiv:2010.09370v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09370</id>
        <link href="http://arxiv.org/abs/2010.09370"/>
        <updated>2021-07-27T02:03:34.843Z</updated>
        <summary type="html"><![CDATA[Sparse Gaussian processes and various extensions thereof are enabled through
inducing points, that simultaneously bottleneck the predictive capacity and act
as the main contributor towards model complexity. However, the number of
inducing points is generally not associated with uncertainty which prevents us
from applying the apparatus of Bayesian reasoning for identifying an
appropriate trade-off. In this work we place a point process prior on the
inducing points and approximate the associated posterior through stochastic
variational inference. By letting the prior encourage a moderate number of
inducing points, we enable the model to learn which and how many points to
utilise. We experimentally show that fewer inducing points are preferred by the
model as the points become less informative, and further demonstrate how the
method can be employed in deep Gaussian processes and latent variable
modelling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uhrenholt_A/0/1/0/all/0/1"&gt;Anders Kirk Uhrenholt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charvet_V/0/1/0/all/0/1"&gt;Valentin Charvet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rn Sand Jensen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Algorithms for Minimax Optimization. (arXiv:2002.02417v6 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02417</id>
        <link href="http://arxiv.org/abs/2002.02417"/>
        <updated>2021-07-27T02:03:34.817Z</updated>
        <summary type="html"><![CDATA[This paper resolves a longstanding open question pertaining to the design of
near-optimal first-order algorithms for smooth and
strongly-convex-strongly-concave minimax problems. Current state-of-the-art
first-order algorithms find an approximate Nash equilibrium using
$\tilde{O}(\kappa_{\mathbf x}+\kappa_{\mathbf y})$ or
$\tilde{O}(\min\{\kappa_{\mathbf x}\sqrt{\kappa_{\mathbf y}},
\sqrt{\kappa_{\mathbf x}}\kappa_{\mathbf y}\})$ gradient evaluations, where
$\kappa_{\mathbf x}$ and $\kappa_{\mathbf y}$ are the condition numbers for the
strong-convexity and strong-concavity assumptions. A gap still remains between
these results and the best existing lower bound
$\tilde{\Omega}(\sqrt{\kappa_{\mathbf x}\kappa_{\mathbf y}})$. This paper
presents the first algorithm with $\tilde{O}(\sqrt{\kappa_{\mathbf
x}\kappa_{\mathbf y}})$ gradient complexity, matching the lower bound up to
logarithmic factors. Our algorithm is designed based on an accelerated proximal
point method and an accelerated solver for minimax proximal steps. It can be
easily extended to the settings of strongly-convex-concave, convex-concave,
nonconvex-strongly-concave, and nonconvex-concave functions. This paper also
presents algorithms that match or outperform all existing methods in these
settings in terms of gradient complexity, up to logarithmic factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael. I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis. (arXiv:1806.03884v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.03884</id>
        <link href="http://arxiv.org/abs/1806.03884"/>
        <updated>2021-07-27T02:03:34.810Z</updated>
        <summary type="html"><![CDATA[Optimization algorithms that leverage gradient covariance information, such
as variants of natural gradient descent (Amari, 1998), offer the prospect of
yielding more effective descent directions. For models with many parameters,
the covariance matrix they are based on becomes gigantic, making them
inapplicable in their original form. This has motivated research into both
simple diagonal approximations and more sophisticated factored approximations
such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In
the present work we draw inspiration from both to propose a novel approximation
that is provably better than KFAC and amendable to cheap partial updates. It
consists in tracking a diagonal variance, not in parameter coordinates, but in
a Kronecker-factored eigenbasis, in which the diagonal approximation is likely
to be more effective. Experiments show improvements over KFAC in optimization
speed for several deep network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1"&gt;Thomas George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laurent_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Laurent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouthillier_X/0/1/0/all/0/1"&gt;Xavier Bouthillier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1"&gt;Pascal Vincent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local2Global: Scaling global representation learning on graphs via local training. (arXiv:2107.12224v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12224</id>
        <link href="http://arxiv.org/abs/2107.12224"/>
        <updated>2021-07-27T02:03:34.804Z</updated>
        <summary type="html"><![CDATA[We propose a decentralised "local2global" approach to graph representation
learning, that one can a-priori use to scale any embedding technique. Our
local2global approach proceeds by first dividing the input graph into
overlapping subgraphs (or "patches") and training local representations for
each patch independently. In a second step, we combine the local
representations into a globally consistent representation by estimating the set
of rigid motions that best align the local representations using information
from the patch overlaps, via group synchronization. A key distinguishing
feature of local2global relative to existing work is that patches are trained
independently without the need for the often costly parameter synchronisation
during distributed training. This allows local2global to scale to large-scale
industrial applications, where the input graph may not even fit into memory and
may be stored in a distributed manner. Preliminary results on medium-scale data
sets (up to $\sim$7K nodes and $\sim$200K edges) are promising, with a graph
reconstruction performance for local2global that is comparable to that of
globally trained embeddings. A thorough evaluation of local2global on large
scale data and applications to downstream tasks, such as node classification
and link prediction, constitutes ongoing work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeub_L/0/1/0/all/0/1"&gt;Lucas G. S. Jeub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1"&gt;Giovanni Colavizza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaowen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1"&gt;Marya Bazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1"&gt;Mihai Cucuringu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction-Centric Learning of Independent Cascade Dynamics from Partial Observations. (arXiv:2007.06557v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06557</id>
        <link href="http://arxiv.org/abs/2007.06557"/>
        <updated>2021-07-27T02:03:34.797Z</updated>
        <summary type="html"><![CDATA[Spreading processes play an increasingly important role in modeling for
diffusion networks, information propagation, marketing and opinion setting. We
address the problem of learning of a spreading model such that the predictions
generated from this model are accurate and could be subsequently used for the
optimization, and control of diffusion dynamics. We focus on a challenging
setting where full observations of the dynamics are not available, and standard
approaches such as maximum likelihood quickly become intractable for large
network instances. We introduce a computationally efficient algorithm, based on
a scalable dynamic message-passing approach, which is able to learn parameters
of the effective spreading model given only limited information on the
activation times of nodes in the network. The popular Independent Cascade model
is used to illustrate our approach. We show that tractable inference from the
learned model generates a better prediction of marginal probabilities compared
to the original model. We develop a systematic procedure for learning a mixture
of models which further improves the prediction quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilinski_M/0/1/0/all/0/1"&gt;Mateusz Wilinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lokhov_A/0/1/0/all/0/1"&gt;Andrey Y. Lokhov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protein-RNA interaction prediction with deep learning: Structure matters. (arXiv:2107.12243v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2107.12243</id>
        <link href="http://arxiv.org/abs/2107.12243"/>
        <updated>2021-07-27T02:03:34.772Z</updated>
        <summary type="html"><![CDATA[Protein-RNA interactions are of vital importance to a variety of cellular
activities. Both experimental and computational techniques have been developed
to study the interactions. Due to the limitation of the previous database,
especially the lack of protein structure data, most of the existing
computational methods rely heavily on the sequence data, with only a small
portion of the methods utilizing the structural information. Recently,
AlphaFold has revolutionized the entire protein and biology field. Foreseeably,
the protein-RNA interaction prediction will also be promoted significantly in
the upcoming years. In this work, we give a thorough review of this field,
surveying both the binding site and binding preference prediction problems and
covering the commonly used datasets, features, and models. We also point out
the potential challenges and opportunities in this field. This survey
summarizes the development of the RBP-RNA interaction field in the past and
foresees its future development in the post-AlphaFold era.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wei_J/0/1/0/all/0/1"&gt;Junkang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zong_L/0/1/0/all/0/1"&gt;Licheng Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising and Segmentation of Epigraphical Scripts. (arXiv:2107.11801v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11801</id>
        <link href="http://arxiv.org/abs/2107.11801"/>
        <updated>2021-07-27T02:03:34.764Z</updated>
        <summary type="html"><![CDATA[This paper is a presentation of a new method for denoising images using
Haralick features and further segmenting the characters using artificial neural
networks. The image is divided into kernels, each of which is converted to a
GLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation
function is called, the result of which is an array with fourteen elements
corresponding to fourteen features The Haralick values and the corresponding
noise/text classification form a dictionary, which is then used to de-noise the
image through kernel comparison. Segmentation is the process of extracting
characters from a document and can be used when letters are separated by white
space, which is an explicit boundary marker. Segmentation is the first step in
many Natural Language Processing problems. This paper explores the process of
segmentation using Neural Networks. While there have been numerous methods to
segment characters of a document, this paper is only concerned with the
accuracy of doing so using neural networks. It is imperative that the
characters be segmented correctly, for failing to do so will lead to incorrect
recognition by Natural language processing tools. Artificial Neural Networks
was used to attain accuracy of upto 89%. This method is suitable for languages
where the characters are delimited by white space. However, this method will
fail to provide acceptable results when the language heavily uses connected
letters. An example would be the Devanagari script, which is predominantly used
in northern India.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1"&gt;P Preethi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1"&gt;Hrishikesh Viswanath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A binary variant of gravitational search algorithm and its application to windfarm layout optimization problem. (arXiv:2107.11844v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.11844</id>
        <link href="http://arxiv.org/abs/2107.11844"/>
        <updated>2021-07-27T02:03:34.758Z</updated>
        <summary type="html"><![CDATA[In the binary search space, GSA framework encounters the shortcomings of
stagnation, diversity loss, premature convergence and high time complexity. To
address these issues, a novel binary variant of GSA called `A novel
neighbourhood archives embedded gravitational constant in GSA for binary search
space (BNAGGSA)' is proposed in this paper. In BNAGGSA, the novel
fitness-distance based social interaction strategy produces a self-adaptive
step size mechanism through which the agent moves towards the optimal direction
with the optimal step size, as per its current search requirement. The
performance of the proposed algorithm is compared with the two binary variants
of GSA over 23 well-known benchmark test problems. The experimental results and
statistical analyses prove the supremacy of BNAGGSA over the compared
algorithms. Furthermore, to check the applicability of the proposed algorithm
in solving real-world applications, a windfarm layout optimization problem is
considered. Two case studies with two different wind data sets of two different
wind sites is considered for experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Susheel Kumar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_J/0/1/0/all/0/1"&gt;Jagdish Chand Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Shifts in Automated Diabetic Retinopathy Screening. (arXiv:2107.11822v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11822</id>
        <link href="http://arxiv.org/abs/2107.11822"/>
        <updated>2021-07-27T02:03:34.751Z</updated>
        <summary type="html"><![CDATA[Deep learning-based models are developed to automatically detect if a retina
image is `referable' in diabetic retinopathy (DR) screening. However, their
classification accuracy degrades as the input images distributionally shift
from their training distribution. Further, even if the input is not a retina
image, a standard DR classifier produces a high confident prediction that the
image is `referable'. Our paper presents a Dirichlet Prior Network-based
framework to address this issue. It utilizes an out-of-distribution (OOD)
detector model and a DR classification model to improve generalizability by
identifying OOD images. Experiments on real-world datasets indicate that the
proposed framework can eliminate the unknown non-retina images and identify the
distributionally shifted retina images for human intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1"&gt;Jay Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Detection Of Noise Events at Shooting Range Using Machine Learning. (arXiv:2107.11453v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.11453</id>
        <link href="http://arxiv.org/abs/2107.11453"/>
        <updated>2021-07-27T02:03:34.745Z</updated>
        <summary type="html"><![CDATA[Outdoor shooting ranges are subject to noise regulations from local and
national authorities. Restrictions found in these regulations may include
limits on times of activities, the overall number of noise events, as well as
limits on number of events depending on the class of noise or activity. A noise
monitoring system may be used to track overall sound levels, but rarely provide
the ability to detect activity or count the number of events, required to
compare directly with such regulations. This work investigates the feasibility
and performance of an automatic detection system to count noise events. An
empirical evaluation was done by collecting data at a newly constructed
shooting range and training facility. The data includes tests of multiple
weapon configurations from small firearms to high caliber rifles and
explosives, at multiple source positions, and collected on multiple different
days. Several alternative machine learning models are tested, using as inputs
time-series of standard acoustic indicators such as A-weighted sound levels and
1/3 octave spectrogram, and classifiers such as Logistic Regression and
Convolutional Neural Networks. Performance for the various alternatives are
reported in terms of the False Positive Rate and False Negative Rate. The
detection performance was found to be satisfactory for use in automatic logging
of time-periods with training activity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nordby_J/0/1/0/all/0/1"&gt;Jon Nordby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemazi_F/0/1/0/all/0/1"&gt;Fabian Nemazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieber_D/0/1/0/all/0/1"&gt;Dag Rieber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregate or Not? Exploring Where to Privatize in DNN Based Federated Learning Under Different Non-IID Scenes. (arXiv:2107.11954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11954</id>
        <link href="http://arxiv.org/abs/2107.11954"/>
        <updated>2021-07-27T02:03:34.722Z</updated>
        <summary type="html"><![CDATA[Although federated learning (FL) has recently been proposed for efficient
distributed training and data privacy protection, it still encounters many
obstacles. One of these is the naturally existing statistical heterogeneity
among clients, making local data distributions non independently and
identically distributed (i.e., non-iid), which poses challenges for model
aggregation and personalization. For FL with a deep neural network (DNN),
privatizing some layers is a simple yet effective solution for non-iid
problems. However, which layers should we privatize to facilitate the learning
process? Do different categories of non-iid scenes have preferred privatization
ways? Can we automatically learn the most appropriate privatization way during
FL? In this paper, we answer these questions via abundant experimental studies
on several FL benchmarks. First, we present the detailed statistics of these
benchmarks and categorize them into covariate and label shift non-iid scenes.
Then, we investigate both coarse-grained and fine-grained network splits and
explore whether the preferred privatization ways have any potential relations
to the specific category of a non-iid scene. Our findings are exciting, e.g.,
privatizing the base layers could boost the performances even in label shift
non-iid scenes, which are inconsistent with some natural conjectures. We also
find that none of these privatization ways could improve the performances on
the Shakespeare benchmark, and we guess that Shakespeare may not be a seriously
non-iid scene. Finally, we propose several approaches to automatically learn
where to aggregate via cross-stitch, soft attention, and hard selection. We
advocate the proposed methods could serve as a preliminary try to explore where
to privatize for a novel non-iid scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin-Chun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shaoming Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12003</id>
        <link href="http://arxiv.org/abs/2107.12003"/>
        <updated>2021-07-27T02:03:34.706Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an effective method to synthesize speaker-specific
speech waveforms by conditioning on videos of an individual's face. Using a
generative adversarial network (GAN) with linguistic and speaker characteristic
features as auxiliary conditions, our method directly converts face images into
speech waveforms under an end-to-end training framework. The linguistic
features are extracted from lip movements using a lip-reading model, and the
speaker characteristic features are predicted from face images using
cross-modal learning with a pre-trained acoustic model. Since these two
features are uncorrelated and controlled independently, we can flexibly
synthesize speech waveforms whose speaker characteristics vary depending on the
input face images. Therefore, our method can be regarded as a multi-speaker
face-to-speech waveform model. We show the superiority of our proposed model
over conventional methods in terms of both objective and subjective evaluation
results. Specifically, we evaluate the performances of the linguistic feature
and the speaker characteristic generation modules by measuring the accuracy of
automatic speech recognition and automatic speaker/gender recognition tasks,
respectively. We also evaluate the naturalness of the synthesized speech
waveforms using a mean opinion score (MOS) test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Um_S/0/1/0/all/0/1"&gt;Se-Yun Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jihyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sangshin Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1"&gt;Kyungguen Byun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hong-Goo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-learning-driven Reliable Single-pixel Imaging with Uncertainty Approximation. (arXiv:2107.11678v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11678</id>
        <link href="http://arxiv.org/abs/2107.11678"/>
        <updated>2021-07-27T02:03:34.669Z</updated>
        <summary type="html"><![CDATA[Single-pixel imaging (SPI) has the advantages of high-speed acquisition over
a broad wavelength range and system compactness, which are difficult to achieve
by conventional imaging sensors. However, a common challenge is low image
quality arising from undersampling. Deep learning (DL) is an emerging and
powerful tool in computational imaging for many applications and researchers
have applied DL in SPI to achieve higher image quality than conventional
reconstruction approaches. One outstanding challenge, however, is that the
accuracy of DL predictions in SPI cannot be assessed in practical applications
where the ground truths are unknown. Here, we propose the use of the Bayesian
convolutional neural network (BCNN) to approximate the uncertainty (coming from
finite training data and network model) of the DL predictions in SPI. Each
pixel in the predicted result from BCNN represents the parameter of a
probability distribution rather than the image intensity value. Then, the
uncertainty can be approximated with BCNN by minimizing a negative
log-likelihood loss function in the training stage and Monte Carlo dropout in
the prediction stage. The results show that the BCNN can reliably approximate
the uncertainty of the DL predictions in SPI with varying compression ratios
and noise levels. The predicted uncertainty from BCNN in SPI reveals that most
of the reconstruction errors in deep-learning-based SPI come from the edges of
the image features. The results show that the proposed BCNN can provide a
reliable tool to approximate the uncertainty of DL predictions in SPI and can
be widely used in many applications of SPI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shang_R/0/1/0/all/0/1"&gt;Ruibo Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OBrien_M/0/1/0/all/0/1"&gt;Mikaela A. O&amp;#x27;Brien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luke_G/0/1/0/all/0/1"&gt;Geoffrey P. Luke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection. (arXiv:2007.06963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06963</id>
        <link href="http://arxiv.org/abs/2007.06963"/>
        <updated>2021-07-27T02:03:34.662Z</updated>
        <summary type="html"><![CDATA[One-class novelty detection is to identify anomalous instances that do not
conform to the expected normal instances. In this paper, the Generative
Adversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used
for detection and achieve state-of-the-art performance. However, deep neural
networks are too over-parameterized to deploy on resource-limited devices.
Therefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to
learn compact and fast novelty detection networks. The P-KDGAN is a novel
attempt to connect two standard GANs by the designed distillation loss for
transferring knowledge from the teacher to the student. The progressive
learning of knowledge distillation is a two-step approach that continuously
improves the performance of the student GAN and achieves better performance
than single step methods. In the first step, the student GAN learns the basic
knowledge totally from the teacher via guiding of the pretrained teacher GAN
with fixed weights. In the second step, joint fine-training is adopted for the
knowledgeable teacher and student GANs to further improve the performance and
stability. The experimental results on CIFAR-10, MNIST, and FMNIST show that
our method improves the performance of the student GAN by 2.44%, 1.77%, and
1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and
700:1, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Federated Edge Learning via Optimized Probabilistic Device Scheduling. (arXiv:2107.11588v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.11588</id>
        <link href="http://arxiv.org/abs/2107.11588"/>
        <updated>2021-07-27T02:03:34.640Z</updated>
        <summary type="html"><![CDATA[The popular federated edge learning (FEEL) framework allows
privacy-preserving collaborative model training via frequent learning-updates
exchange between edge devices and server. Due to the constrained bandwidth,
only a subset of devices can upload their updates at each communication round.
This has led to an active research area in FEEL studying the optimal device
scheduling policy for minimizing communication time. However, owing to the
difficulty in quantifying the exact communication time, prior work in this area
can only tackle the problem partially by considering either the communication
rounds or per-round latency, while the total communication time is determined
by both metrics. To close this gap, we make the first attempt in this paper to
formulate and solve the communication time minimization problem. We first
derive a tight bound to approximate the communication time through
cross-disciplinary effort involving both learning theory for convergence
analysis and communication theory for per-round latency analysis. Building on
the analytical result, an optimized probabilistic scheduling policy is derived
in closed-form by solving the approximate communication time minimization
problem. It is found that the optimized policy gradually turns its priority
from suppressing the remaining communication rounds to reducing per-round
latency as the training process evolves. The effectiveness of the proposed
scheme is demonstrated via a use case on collaborative 3D objective detection
in autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Maojun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangxu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiamo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Caijun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ROD: Reception-aware Online Distillation for Sparse Graphs. (arXiv:2107.11789v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11789</id>
        <link href="http://arxiv.org/abs/2107.11789"/>
        <updated>2021-07-27T02:03:34.633Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have been widely used in many graph-based tasks
such as node classification, link prediction, and node clustering. However,
GNNs gain their performance benefits mainly from performing the feature
propagation and smoothing across the edges of the graph, thus requiring
sufficient connectivity and label information for effective propagation.
Unfortunately, many real-world networks are sparse in terms of both edges and
labels, leading to sub-optimal performance of GNNs. Recent interest in this
sparse problem has focused on the self-training approach, which expands
supervised signals with pseudo labels. Nevertheless, the self-training approach
inherently cannot realize the full potential of refining the learning
performance on sparse graphs due to the unsatisfactory quality and quantity of
pseudo labels.

In this paper, we propose ROD, a novel reception-aware online knowledge
distillation approach for sparse graph learning. We design three supervision
signals for ROD: multi-scale reception-aware graph knowledge, task-based
supervision, and rich distilled knowledge, allowing online knowledge transfer
in a peer-teaching manner. To extract knowledge concealed in the multi-scale
reception fields, ROD explicitly requires individual student models to preserve
different levels of locality information. For a given task, each student would
predict based on its reception-scale knowledge, while simultaneously a strong
teacher is established on-the-fly by combining multi-scale knowledge. Our
approach has been extensively evaluated on 9 datasets and a variety of
graph-based tasks, including node classification, link prediction, and node
clustering. The result demonstrates that ROD achieves state-of-art performance
and is more robust for the graph sparsity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuezihan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1"&gt;Zeang Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1"&gt;Xupeng Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Direction and Proximity Classification of Overlapping Sound Events from Binaural Audio. (arXiv:2107.12033v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.12033</id>
        <link href="http://arxiv.org/abs/2107.12033"/>
        <updated>2021-07-27T02:03:34.622Z</updated>
        <summary type="html"><![CDATA[Sound source proximity and distance estimation are of great interest in many
practical applications, since they provide significant information for acoustic
scene analysis. As both tasks share complementary qualities, ensuring efficient
interaction between these two is crucial for a complete picture of an aural
environment. In this paper, we aim to investigate several ways of performing
joint proximity and direction estimation from binaural recordings, both defined
as coarse classification problems based on Deep Neural Networks (DNNs).
Considering the limitations of binaural audio, we propose two methods of
splitting the sphere into angular areas in order to obtain a set of directional
classes. For each method we study different model types to acquire information
about the direction-of-arrival (DoA). Finally, we propose various ways of
combining the proximity and direction estimation problems into a joint task
providing temporal information about the onsets and offsets of the appearing
sources. Experiments are performed for a synthetic reverberant binaural dataset
consisting of up to two overlapping sound events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krause_D/0/1/0/all/0/1"&gt;Daniel Aleksander Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Politis_A/0/1/0/all/0/1"&gt;Archontis Politis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mesaros_A/0/1/0/all/0/1"&gt;Annamaria Mesaros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tail of Distribution GAN (TailGAN): Generative- Adversarial-Network-Based Boundary Formation. (arXiv:2107.11658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11658</id>
        <link href="http://arxiv.org/abs/2107.11658"/>
        <updated>2021-07-27T02:03:34.616Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GAN) are a powerful methodology and can be
used for unsupervised anomaly detection, where current techniques have
limitations such as the accurate detection of anomalies near the tail of a
distribution. GANs generally do not guarantee the existence of a probability
density and are susceptible to mode collapse, while few GANs use likelihood to
reduce mode collapse. In this paper, we create a GAN-based tail formation model
for anomaly detection, the Tail of distribution GAN (TailGAN), to generate
samples on the tail of the data distribution and detect anomalies near the
support boundary. Using TailGAN, we leverage GANs for anomaly detection and use
maximum entropy regularization. Using GANs that learn the probability of the
underlying distribution has advantages in improving the anomaly detection
methodology by allowing us to devise a generator for boundary samples, and use
this model to characterize anomalies. TailGAN addresses supports with disjoint
components and achieves competitive performance on images. We evaluate TailGAN
for identifying Out-of-Distribution (OoD) data and its performance evaluated on
MNIST, CIFAR-10, Baggage X-Ray, and OoD data shows competitiveness compared to
methods from the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1"&gt;Nikolaos Dionelis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Accelerated Decentralized Gradient Method Over Unbalanced Directed Graphs. (arXiv:2107.12065v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.12065</id>
        <link href="http://arxiv.org/abs/2107.12065"/>
        <updated>2021-07-27T02:03:34.607Z</updated>
        <summary type="html"><![CDATA[In this work, we consider the decentralized optimization problem in which a
network of $n$ agents, each possessing a smooth and convex objective function,
wish to collaboratively minimize the average of all the objective functions
through peer-to-peer communication in a directed graph. To solve the problem,
we propose two accelerated Push-DIGing methods termed APD and APD-SC for
minimizing non-strongly convex objective functions and strongly convex ones,
respectively. We show that APD and APD-SC respectively converge at the rates
$O\left(\frac{1}{k^2}\right)$ and $O\left(\left(1 -
C\sqrt{\frac{\mu}{L}}\right)^k\right)$ up to constant factors depending only on
the mixing matrix. To the best of our knowledge, APD and APD-SC are the first
decentralized methods to achieve provable acceleration over unbalanced directed
graphs. Numerical experiments demonstrate the effectiveness of both methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhuoqing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Lei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shi Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Model-Agnostic Algorithm for Bayes Error Determination in Binary Classification. (arXiv:2107.11609v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11609</id>
        <link href="http://arxiv.org/abs/2107.11609"/>
        <updated>2021-07-27T02:03:34.590Z</updated>
        <summary type="html"><![CDATA[This paper presents the intrinsic limit determination algorithm (ILD
Algorithm), a novel technique to determine the best possible performance,
measured in terms of the AUC (area under the ROC curve) and accuracy, that can
be obtained from a specific dataset in a binary classification problem with
categorical features {\sl regardless} of the model used. This limit, namely the
Bayes error, is completely independent of any model used and describes an
intrinsic property of the dataset. The ILD algorithm thus provides important
information regarding the prediction limits of any binary classification
algorithm when applied to the considered dataset. In this paper the algorithm
is described in detail, its entire mathematical framework is presented and the
pseudocode is given to facilitate its implementation. Finally, an example with
a real dataset is given.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michelucci_U/0/1/0/all/0/1"&gt;Umberto Michelucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sperti_M/0/1/0/all/0/1"&gt;Michela Sperti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piga_D/0/1/0/all/0/1"&gt;Dario Piga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venturini_F/0/1/0/all/0/1"&gt;Francesca Venturini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deriu_M/0/1/0/all/0/1"&gt;Marco A. Deriu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-intrusive reduced order modeling of natural convection in porous media using convolutional autoencoders: comparison with linear subspace techniques. (arXiv:2107.11460v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2107.11460</id>
        <link href="http://arxiv.org/abs/2107.11460"/>
        <updated>2021-07-27T02:03:34.584Z</updated>
        <summary type="html"><![CDATA[Natural convection in porous media is a highly nonlinear multiphysical
problem relevant to many engineering applications (e.g., the process of
$\mathrm{CO_2}$ sequestration). Here, we present a non-intrusive reduced order
model of natural convection in porous media employing deep convolutional
autoencoders for the compression and reconstruction and either radial basis
function (RBF) interpolation or artificial neural networks (ANNs) for mapping
parameters of partial differential equations (PDEs) on the corresponding
nonlinear manifolds. To benchmark our approach, we also describe linear
compression and reconstruction processes relying on proper orthogonal
decomposition (POD) and ANNs. We present comprehensive comparisons among
different models through three benchmark problems. The reduced order models,
linear and nonlinear approaches, are much faster than the finite element model,
obtaining a maximum speed-up of $7 \times 10^{6}$ because our framework is not
bound by the Courant-Friedrichs-Lewy condition; hence, it could deliver
quantities of interest at any given time contrary to the finite element model.
Our model's accuracy still lies within a mean squared error of 0.07 (two-order
of magnitude lower than the maximum value of the finite element results) in the
worst-case scenario. We illustrate that, in specific settings, the nonlinear
approach outperforms its linear counterpart and vice versa. We hypothesize that
a visual comparison between principal component analysis (PCA) or t-Distributed
Stochastic Neighbor Embedding (t-SNE) could indicate which method will perform
better prior to employing any specific compression strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kadeethum_T/0/1/0/all/0/1"&gt;T. Kadeethum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballarin_F/0/1/0/all/0/1"&gt;F. Ballarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Y. Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OMalley_D/0/1/0/all/0/1"&gt;D. O&amp;#x27;Malley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1"&gt;H. Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouklas_N/0/1/0/all/0/1"&gt;N. Bouklas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning to Emulate Agent-Based Simulations. (arXiv:2005.02077v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02077</id>
        <link href="http://arxiv.org/abs/2005.02077"/>
        <updated>2021-07-27T02:03:34.563Z</updated>
        <summary type="html"><![CDATA[In this proof-of-concept work, we evaluate the performance of multiple
machine-learning methods as statistical emulators for use in the analysis of
agent-based models (ABMs). Analysing ABM outputs can be challenging, as the
relationships between input parameters can be non-linear or even chaotic even
in relatively simple models, and each model run can require significant CPU
time. Statistical emulation, in which a statistical model of the ABM is
constructed to facilitate detailed model analyses, has been proposed as an
alternative to computationally costly Monte Carlo methods. Here we compare
multiple machine-learning methods for ABM emulation in order to determine the
approaches best suited to emulating the complex behaviour of ABMs. Our results
suggest that, in most scenarios, artificial neural networks (ANNs) and
gradient-boosted trees outperform Gaussian process emulators, currently the
most commonly used method for the emulation of complex computational models.
ANNs produced the most accurate model replications in scenarios with high
numbers of model runs, although training times were longer than the other
methods. We propose that agent-based modelling would benefit from using
machine-learning methods for emulation, as this can facilitate more robust
sensitivity analyses for the models while also reducing CPU time consumption
when calibrating and analysing the simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angione_C/0/1/0/all/0/1"&gt;Claudio Angione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silverman_E/0/1/0/all/0/1"&gt;Eric Silverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaneske_E/0/1/0/all/0/1"&gt;Elisabeth Yaneske&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12003</id>
        <link href="http://arxiv.org/abs/2107.12003"/>
        <updated>2021-07-27T02:03:34.509Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an effective method to synthesize speaker-specific
speech waveforms by conditioning on videos of an individual's face. Using a
generative adversarial network (GAN) with linguistic and speaker characteristic
features as auxiliary conditions, our method directly converts face images into
speech waveforms under an end-to-end training framework. The linguistic
features are extracted from lip movements using a lip-reading model, and the
speaker characteristic features are predicted from face images using
cross-modal learning with a pre-trained acoustic model. Since these two
features are uncorrelated and controlled independently, we can flexibly
synthesize speech waveforms whose speaker characteristics vary depending on the
input face images. Therefore, our method can be regarded as a multi-speaker
face-to-speech waveform model. We show the superiority of our proposed model
over conventional methods in terms of both objective and subjective evaluation
results. Specifically, we evaluate the performances of the linguistic feature
and the speaker characteristic generation modules by measuring the accuracy of
automatic speech recognition and automatic speaker/gender recognition tasks,
respectively. We also evaluate the naturalness of the synthesized speech
waveforms using a mean opinion score (MOS) test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Um_S/0/1/0/all/0/1"&gt;Se-Yun Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jihyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sangshin Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1"&gt;Kyungguen Byun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hong-Goo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying the fragment structure of the organic compounds by deeply learning the original NMR data. (arXiv:2107.11740v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.11740</id>
        <link href="http://arxiv.org/abs/2107.11740"/>
        <updated>2021-07-27T02:03:34.491Z</updated>
        <summary type="html"><![CDATA[We preprocess the raw NMR spectrum and extract key characteristic features by
using two different methodologies, called equidistant sampling and peak
sampling for subsequent substructure pattern recognition; meanwhile may provide
the alternative strategy to address the imbalance issue of the NMR dataset
frequently encountered in dataset collection of statistical modeling and
establish two conventional SVM and KNN models to assess the capability of two
feature selection, respectively. Our results in this study show that the models
using the selected features of peak sampling outperform the ones using the
other. Then we build the Recurrent Neural Network (RNN) model trained by Data B
collected from peak sampling. Furthermore, we illustrate the easier
optimization of hyper parameters and the better generalization ability of the
RNN deep learning model by comparison with traditional machine learning SVM and
KNN models in detail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongcan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cong_Y/0/1/0/all/0/1"&gt;Yong Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deng_W/0/1/0/all/0/1"&gt;Weihua Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Fair Worker Selection: A Multi-Round Submodular Maximization Approach. (arXiv:2107.11728v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.11728</id>
        <link href="http://arxiv.org/abs/2107.11728"/>
        <updated>2021-07-27T02:03:34.483Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of fair worker selection in Federated
Learning systems, where fairness serves as an incentive mechanism that
encourages more workers to participate in the federation. Considering the
achieved training accuracy of the global model as the utility of the selected
workers, which is typically a monotone submodular function, we formulate the
worker selection problem as a new multi-round monotone submodular maximization
problem with cardinality and fairness constraints. The objective is to maximize
the time-average utility over multiple rounds subject to an additional fairness
requirement that each worker must be selected for a certain fraction of time.
While the traditional submodular maximization with a cardinality constraint is
already a well-known NP-Hard problem, the fairness constraint in the
multi-round setting adds an extra layer of difficulty. To address this novel
challenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and
FairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness
requirement whenever feasible. Moreover, we prove nontrivial lower bounds on
the achieved time-average utility under FairCG1 and FairCG2. In addition, by
giving a higher priority to fairness, FairDG ensures a stronger short-term
fairness guarantee, which holds in every round. Finally, we perform extensive
simulations to verify the effectiveness of the proposed algorithms in terms of
the time-average utility and fairness satisfaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fengjiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1"&gt;Bo Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference of collective Gaussian hidden Markov models. (arXiv:2107.11662v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.11662</id>
        <link href="http://arxiv.org/abs/2107.11662"/>
        <updated>2021-07-27T02:03:34.477Z</updated>
        <summary type="html"><![CDATA[We consider inference problems for a class of continuous state collective
hidden Markov models, where the data is recorded in aggregate (collective) form
generated by a large population of individuals following the same dynamics. We
propose an aggregate inference algorithm called collective Gaussian
forward-backward algorithm, extending recently proposed Sinkhorn belief
propagation algorithm to models characterized by Gaussian densities. Our
algorithm enjoys convergence guarantee. In addition, it reduces to the standard
Kalman filter when the observations are generated by a single individual. The
efficacy of the proposed algorithm is demonstrated through multiple
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongxin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariance-based Multi-Clustering of Latent Space Embeddings for Equivariant Learning. (arXiv:2107.11717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11717</id>
        <link href="http://arxiv.org/abs/2107.11717"/>
        <updated>2021-07-27T02:03:34.470Z</updated>
        <summary type="html"><![CDATA[Variational Autoencoders (VAEs) have been shown to be remarkably effective in
recovering model latent spaces for several computer vision tasks. However,
currently trained VAEs, for a number of reasons, seem to fall short in learning
invariant and equivariant clusters in latent space. Our work focuses on
providing solutions to this problem and presents an approach to disentangle
equivariance feature maps in a Lie group manifold by enforcing deep,
group-invariant learning. Simultaneously implementing a novel separation of
semantic and equivariant variables of the latent space representation, we
formulate a modified Evidence Lower BOund (ELBO) by using a mixture model pdf
like Gaussian mixtures for invariant cluster embeddings that allows superior
unsupervised variational clustering. Our experiments show that this model
effectively learns to disentangle the invariant and equivariant representations
with significant improvements in the learning rate and an observably superior
image recognition and canonical state reconstruction compared to the currently
best deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1"&gt;Chandrajit Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Avik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoran Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WiP Abstract : Robust Out-of-distribution Motion Detection and Localization in Autonomous CPS. (arXiv:2107.11736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11736</id>
        <link href="http://arxiv.org/abs/2107.11736"/>
        <updated>2021-07-27T02:03:34.463Z</updated>
        <summary type="html"><![CDATA[Highly complex deep learning models are increasingly integrated into modern
cyber-physical systems (CPS), many of which have strict safety requirements.
One problem arising from this is that deep learning lacks interpretability,
operating as a black box. The reliability of deep learning is heavily impacted
by how well the model training data represents runtime test data, especially
when the input space dimension is high as natural images. In response, we
propose a robust out-of-distribution (OOD) detection framework. Our approach
detects unusual movements from driving video in real-time by combining
classical optic flow operation with representation learning via variational
autoencoder (VAE). We also design a method to locate OOD factors in images.
Evaluation on a driving simulation data set shows that our approach is
statistically more robust than related works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yeli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1"&gt;Arvind Easwaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Impact of Negative Sampling on Contrastive Structured World Models. (arXiv:2107.11676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11676</id>
        <link href="http://arxiv.org/abs/2107.11676"/>
        <updated>2021-07-27T02:03:34.446Z</updated>
        <summary type="html"><![CDATA[World models trained by contrastive learning are a compelling alternative to
autoencoder-based world models, which learn by reconstructing pixel states. In
this paper, we describe three cases where small changes in how we sample
negative states in the contrastive loss lead to drastic changes in model
performance. In previously studied Atari datasets, we show that leveraging time
step correlations can double the performance of the Contrastive Structured
World Model. We also collect a full version of the datasets to study
contrastive learning under a more diverse set of experiences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1"&gt;Ondrej Biza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pol_E/0/1/0/all/0/1"&gt;Elise van der Pol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1"&gt;Thomas Kipf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems. (arXiv:2107.11640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11640</id>
        <link href="http://arxiv.org/abs/2107.11640"/>
        <updated>2021-07-27T02:03:34.436Z</updated>
        <summary type="html"><![CDATA[Automated Vehicle License Plate (VLP) detection and recognition have ended up
being a significant research issue as of late. VLP localization and recognition
are some of the most essential techniques for managing traffic using digital
techniques. In this paper, four smart systems are developed to recognize
Egyptian vehicles license plates. Two systems are based on character
recognition, which are (System1, Characters Recognition with Classical Machine
Learning) and (System2, Characters Recognition with Deep Machine Learning). The
other two systems are based on the whole plate recognition which are (System3,
Whole License Plate Recognition with Classical Machine Learning) and (System4,
Whole License Plate Recognition with Deep Machine Learning). We use object
detection algorithms, and machine learning based object recognition algorithms.
The performance of the developed systems has been tested on real images, and
the experimental results demonstrate that the best detection accuracy rate for
VLP is provided by using the deep learning method. Where the VLP detection
accuracy rate is better than the classical system by 32%. However, the best
detection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is
provided by using the classical method. Where VLPAC detection accuracy rate is
better than the deep learning-based system by 6%. Also, the results show that
deep learning is better than the classical technique used in VLP recognition
processes. Where the recognition accuracy rate is better than the classical
system by 8%. Finally, the paper output recommends a robust VLP recognition
system based on both statistical and deep machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1"&gt;Mohamed Shehata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abou_Kreisha_M/0/1/0/all/0/1"&gt;Mohamed Taha Abou-Kreisha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elnashar_H/0/1/0/all/0/1"&gt;Hany Elnashar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. (arXiv:2102.05426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05426</id>
        <link href="http://arxiv.org/abs/2102.05426"/>
        <updated>2021-07-27T02:03:34.429Z</updated>
        <summary type="html"><![CDATA[We study the challenging task of neural network quantization without
end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually
requires a small subset of training data but produces less powerful quantized
models than Quantization-Aware Training (QAT). In this work, we propose a novel
PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to
INT2 for the first time. BRECQ leverages the basic building blocks in neural
networks and reconstructs them one-by-one. In a comprehensive theoretical study
of the second-order error, we show that BRECQ achieves a good balance between
cross-layer dependency and generalization error. To further employ the power of
quantization, the mixed precision technique is incorporated in our framework by
approximating the inter-layer and intra-layer sensitivity. Extensive
experiments on various handcrafted and searched neural architectures are
conducted for both image classification and object detection tasks. And for the
first time we prove that, without bells and whistles, PTQ can attain 4-bit
ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster
production of quantized models. Codes are available at
https://github.com/yhhhli/BRECQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Peng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fengwei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12321</id>
        <link href="http://arxiv.org/abs/2102.12321"/>
        <updated>2021-07-27T02:03:34.422Z</updated>
        <summary type="html"><![CDATA[For machine agents to successfully interact with humans in real-world
settings, they will need to develop an understanding of human mental life.
Intuitive psychology, the ability to reason about hidden mental variables that
drive observable actions, comes naturally to people: even pre-verbal infants
can tell agents from objects, expecting agents to act efficiently to achieve
goals given constraints. Despite recent interest in machine agents that reason
about other agents, it is not clear if such agents learn or hold the core
psychology principles that drive human reasoning. Inspired by cognitive
development studies on intuitive psychology, we present a benchmark consisting
of a large dataset of procedurally generated 3D animations, AGENT (Action,
Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal
preferences, action efficiency, unobserved constraints, and cost-reward
trade-offs) that probe key concepts of core intuitive psychology. We validate
AGENT with human-ratings, propose an evaluation protocol emphasizing
generalization, and compare two strong baselines built on Bayesian inverse
planning and a Theory of Mind neural network. Our results suggest that to pass
the designed tests of core intuitive psychology at human levels, a model must
acquire or have built-in representations of how agents plan, combining utility
computations and core knowledge of objects and physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1"&gt;Tianmin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1"&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shari Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1"&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1"&gt;Elizabeth Spelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1"&gt;Tomer D. Ullman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v2 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14659</id>
        <link href="http://arxiv.org/abs/2104.14659"/>
        <updated>2021-07-27T02:03:34.413Z</updated>
        <summary type="html"><![CDATA[We describe a novel application of the end-to-end deep learning technique to
the task of discriminating top quark-initiated jets from those originating from
the hadronization of a light quark or a gluon. The end-to-end deep learning
technique combines deep learning algorithms and low-level detector
representation of the high-energy collision event. In this study, we use
low-level detector information from the simulated CMS Open Data samples to
construct the top jet classifiers. To optimize classifier performance we
progressively add low-level information from the CMS tracking detector,
including pixel detector reconstructed hits and impact parameters, and
demonstrate the value of additional tracking information even when no new
spatial structures are added. Relying only on calorimeter energy deposits and
reconstructed pixel detector hits, the end-to-end classifier achieves an AUC
score of 0.975$\pm$0.002 for the task of classifying boosted top quark jets.
After adding derived track quantities, the classifier AUC score increases to
0.9824$\pm$0.0013, serving as the first performance benchmark for these CMS
Open Data samples. We additionally provide a timing performance comparison of
different processor unit architectures for training the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1"&gt;Michael Andrews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1"&gt;Bjorn Burkle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-fan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1"&gt;Davide DiCroce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1"&gt;Sergei Gleyzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1"&gt;Ulrich Heintz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1"&gt;Meenakshi Narain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1"&gt;Manfred Paulini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1"&gt;Nikolas Pervan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1"&gt;Yusef Shafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1"&gt;Emanuele Usai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[INSTA-YOLO: Real-Time Instance Segmentation. (arXiv:2102.06777v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06777</id>
        <link href="http://arxiv.org/abs/2102.06777"/>
        <updated>2021-07-27T02:03:34.406Z</updated>
        <summary type="html"><![CDATA[Instance segmentation has gained recently huge attention in various computer
vision applications. It aims at providing different IDs to different objects of
the scene, even if they belong to the same class. Instance segmentation is
usually performed as a two-stage pipeline. First, an object is detected, then
semantic segmentation within the detected box area is performed which involves
costly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage
end-to-end deep learning model for real-time instance segmentation. Instead of
pixel-wise prediction, our model predicts instances as object contours
represented by 2D points in Cartesian space. We evaluate our model on three
datasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the
state-of-the-art models for instance segmentation. The results show our model
achieves competitive accuracy in terms of mAP at twice the speed on GTX-1080
GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1"&gt;Abdelrahman Shaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadhoud_M/0/1/0/all/0/1"&gt;Mayada Hadhoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGD May Never Escape Saddle Points. (arXiv:2107.11774v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11774</id>
        <link href="http://arxiv.org/abs/2107.11774"/>
        <updated>2021-07-27T02:03:34.380Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) has been deployed to solve highly
non-linear and non-convex machine learning problems such as the training of
deep neural networks. However, previous works on SGD often rely on highly
restrictive and unrealistic assumptions about the nature of noise in SGD. In
this work, we mathematically construct examples that defy previous
understandings of SGD. For example, our constructions show that: (1) SGD may
converge to a local maximum; (2) SGD may escape a saddle point arbitrarily
slowly; (3) SGD may prefer sharp minima over the flat ones; and (4) AMSGrad may
converge to a local maximum. Our result suggests that the noise structure of
SGD might be more important than the loss landscape in neural network training
and that future research should focus on deriving the actual noise structure in
deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Botao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13415</id>
        <link href="http://arxiv.org/abs/2104.13415"/>
        <updated>2021-07-27T02:03:34.374Z</updated>
        <summary type="html"><![CDATA[This work presents a novel approach for semi-supervised semantic
segmentation. The key element of this approach is our contrastive learning
module that enforces the segmentation network to yield similar pixel-level
feature representations for same-class samples across the whole dataset. To
achieve this, we maintain a memory bank continuously updated with relevant and
high-quality feature vectors from labeled data. In an end-to-end training, the
features from both labeled and unlabeled data are optimized to be similar to
same-class samples from the memory bank. Our approach outperforms the current
state-of-the-art for semi-supervised semantic segmentation and semi-supervised
domain adaptation on well-known public benchmarks, with larger improvements on
the most challenging scenarios, i.e., less available labeled data.
https://github.com/Shathe/SemiSeg-Contrastive]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1"&gt;Inigo Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1"&gt;Alberto Sabater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1"&gt;David Ferstl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1"&gt;Luis Montesano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1"&gt;Ana C. Murillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14214</id>
        <link href="http://arxiv.org/abs/2012.14214"/>
        <updated>2021-07-27T02:03:34.367Z</updated>
        <summary type="html"><![CDATA[While CNN-based models have made remarkable progress on human pose
estimation, what spatial dependencies they capture to localize keypoints
remains unclear. In this work, we propose a model called \textbf{TransPose},
which introduces Transformer for human pose estimation. The attention layers
built in Transformer enable our model to capture long-range relationships
efficiently and also can reveal what dependencies the predicted keypoints rely
on. To predict keypoint heatmaps, the last attention layer specially acts as an
aggregator, which collects contributions from image clues and forms maximum
positions of keypoints. Such a heatmap-based localization approach via
Transformer conforms to the principle of Activation Maximization
\cite{erhan2009visualizing}. And the revealed dependencies are image-specific
and fine-grained, which also can provide evidence of how the model handles
special cases, e.g., occlusion. The experiments show that TransPose achieves
75.8 AP and 75.0 AP on COCO validation and test-dev sets with 256 $\times$ 192
input resolution, while being more lightweight and faster than mainstream CNN
architectures. The TransPose model also transfers very well on MPII benchmark,
yielding 93.9\% accuracy on test set when fine-tuned with small training costs.
Code and pre-trained models are publicly available at
\url{https://github.com/yangsenius/TransPose}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1"&gt;Zhibin Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1"&gt;Mu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wankou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Network with Generalized Factorized Bilinear Aggregation. (arXiv:2107.11666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11666</id>
        <link href="http://arxiv.org/abs/2107.11666"/>
        <updated>2021-07-27T02:03:34.352Z</updated>
        <summary type="html"><![CDATA[Although Graph Convolutional Networks (GCNs) have demonstrated their power in
various applications, the graph convolutional layers, as the most important
component of GCN, are still using linear transformations and a simple pooling
step. In this paper, we propose a novel generalization of Factorized Bilinear
(FB) layer to model the feature interactions in GCNs. FB performs two
matrix-vector multiplications, that is, the weight matrix is multiplied with
the outer product of the vector of hidden features from both sides. However,
the FB layer suffers from the quadratic number of coefficients, overfitting and
the spurious correlations due to correlations between channels of hidden
representations that violate the i.i.d. assumption. Thus, we propose a compact
FB layer by defining a family of summarizing operators applied over the
quadratic term. We analyze proposed pooling operators and motivate their use.
Our experimental results on multiple datasets demonstrate that the GFB-GCN is
competitive with other methods for text classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Device Content Moderation. (arXiv:2107.11845v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11845</id>
        <link href="http://arxiv.org/abs/2107.11845"/>
        <updated>2021-07-27T02:03:34.345Z</updated>
        <summary type="html"><![CDATA[With the advent of internet, not safe for work(NSFW) content moderation is a
major problem today. Since,smartphones are now part of daily life of billions
of people,it becomes even more important to have a solution which coulddetect
and suggest user about potential NSFW content present ontheir phone. In this
paper we present a novel on-device solutionfor detecting NSFW images. In
addition to conventional porno-graphic content moderation, we have also
included semi-nudecontent moderation as it is still NSFW in a large
demography.We have curated a dataset comprising of three major
categories,namely nude, semi-nude and safe images. We have created anensemble
of object detector and classifier for filtering of nudeand semi-nude contents.
The solution provides unsafe body partannotations along with identification of
semi-nude images. Weextensively tested our proposed solution on several public
datasetand also on our custom dataset. The model achieves F1 scoreof 0.91 with
95% precision and 88% recall on our customNSFW16k dataset and 0.92 MAP on NPDI
dataset. Moreover itachieves average 0.002 false positive rate on a collection
of safeimage open datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1"&gt;Anchal Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moharana_S/0/1/0/all/0/1"&gt;Sukumar Moharana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_D/0/1/0/all/0/1"&gt;Debi Prasanna Mohanty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panwar_A/0/1/0/all/0/1"&gt;Archit Panwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1"&gt;Dewang Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thota_S/0/1/0/all/0/1"&gt;Siva Prasad Thota&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration. (arXiv:2107.11992v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11992</id>
        <link href="http://arxiv.org/abs/2107.11992"/>
        <updated>2021-07-27T02:03:34.339Z</updated>
        <summary type="html"><![CDATA[Point cloud registration is a fundamental problem in 3D computer vision.
Outdoor LiDAR point clouds are typically large-scale and complexly distributed,
which makes the registration challenging. In this paper, we propose an
efficient hierarchical network named HRegNet for large-scale outdoor LiDAR
point cloud registration. Instead of using all points in the point clouds,
HRegNet performs registration on hierarchically extracted keypoints and
descriptors. The overall framework combines the reliable features in deeper
layer and the precise position information in shallower layers to achieve
robust and precise registration. We present a correspondence network to
generate correct and accurate keypoints correspondences. Moreover, bilateral
consensus and neighborhood consensus are introduced for keypoints matching and
novel similarity features are designed to incorporate them into the
correspondence network, which significantly improves the registration
performance. Besides, the whole network is also highly efficient since only a
small number of keypoints are used for registration. Extensive experiments are
conducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate
the high accuracy and efficiency of the proposed HRegNet. The project website
is https://ispc-group.github.io/hregnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1"&gt;Fan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yinlong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1"&gt;Sanqing Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1"&gt;Rongqi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11732</id>
        <link href="http://arxiv.org/abs/2107.11732"/>
        <updated>2021-07-27T02:03:34.332Z</updated>
        <summary type="html"><![CDATA[Analyzing observational data from multiple sources can be useful for
increasing statistical power to detect a treatment effect; however, practical
constraints such as privacy considerations may restrict individual-level
information sharing across data sets. This paper develops federated methods
that only utilize summary-level information from heterogeneous data sets. Our
federated methods provide doubly-robust point estimates of treatment effects as
well as variance estimates. We derive the asymptotic distributions of our
federated estimators, which are shown to be asymptotically equivalent to the
corresponding estimators from the combined, individual-level data. We show that
to achieve these properties, federated methods should be adjusted based on
conditions such as whether models are correctly specified and stable across
heterogeneous data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Ruoxuan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1"&gt;Allison Koenecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1"&gt;Michael Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain Randomization Reinforcement Learning. (arXiv:2107.11762v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.11762</id>
        <link href="http://arxiv.org/abs/2107.11762"/>
        <updated>2021-07-27T02:03:34.325Z</updated>
        <summary type="html"><![CDATA[How to explore corner cases as efficiently and thoroughly as possible has
long been one of the top concerns in the context of deep reinforcement learning
(DeepRL) autonomous driving. Training with simulated data is less costly and
dangerous than utilizing real-world data, but the inconsistency of parameter
distribution and the incorrect system modeling in simulators always lead to an
inevitable Sim2real gap, which probably accounts for the underperformance in
novel, anomalous and risky cases that simulators can hardly generate. Domain
Randomization(DR) is a methodology that can bridge this gap with little or no
real-world data. Consequently, in this research, an adversarial model is put
forward to robustify DeepRL-based autonomous vehicles trained in simulation to
gradually surfacing harder events, so that the models could readily transfer to
the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1"&gt;Haoyi Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jianming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zheyu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to See Through Obstructions with Layered Decomposition. (arXiv:2008.04902v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.04902</id>
        <link href="http://arxiv.org/abs/2008.04902"/>
        <updated>2021-07-27T02:03:34.319Z</updated>
        <summary type="html"><![CDATA[We present a learning-based approach for removing unwanted obstructions, such
as window reflections, fence occlusions, or adherent raindrops, from a short
sequence of images captured by a moving camera. Our method leverages motion
differences between the background and obstructing elements to recover both
layers. Specifically, we alternate between estimating dense optical flow fields
of the two layers and reconstructing each layer from the flow-warped images via
a deep convolutional neural network. This learning-based layer reconstruction
module facilitates accommodating potential errors in the flow estimation and
brittle assumptions, such as brightness consistency. We show that the proposed
approach learned from synthetically generated data performs well to real
images. Experimental results on numerous challenging scenarios of reflection
and fence removal demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Lun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1"&gt;Wei-Sheng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yung-Yu Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Shifts in Automated Diabetic Retinopathy Screening. (arXiv:2107.11822v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11822</id>
        <link href="http://arxiv.org/abs/2107.11822"/>
        <updated>2021-07-27T02:03:34.304Z</updated>
        <summary type="html"><![CDATA[Deep learning-based models are developed to automatically detect if a retina
image is `referable' in diabetic retinopathy (DR) screening. However, their
classification accuracy degrades as the input images distributionally shift
from their training distribution. Further, even if the input is not a retina
image, a standard DR classifier produces a high confident prediction that the
image is `referable'. Our paper presents a Dirichlet Prior Network-based
framework to address this issue. It utilizes an out-of-distribution (OOD)
detector model and a DR classification model to improve generalizability by
identifying OOD images. Experiments on real-world datasets indicate that the
proposed framework can eliminate the unknown non-retina images and identify the
distributionally shifted retina images for human intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1"&gt;Jay Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MagFace: A Universal Representation for Face Recognition and Quality Assessment. (arXiv:2103.06627v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06627</id>
        <link href="http://arxiv.org/abs/2103.06627"/>
        <updated>2021-07-27T02:03:34.267Z</updated>
        <summary type="html"><![CDATA[The performance of face recognition system degrades when the variability of
the acquired faces increases. Prior work alleviates this issue by either
monitoring the face quality in pre-processing or predicting the data
uncertainty along with the face feature. This paper proposes MagFace, a
category of losses that learn a universal feature embedding whose magnitude can
measure the quality of the given face. Under the new loss, it can be proven
that the magnitude of the feature embedding monotonically increases if the
subject is more likely to be recognized. In addition, MagFace introduces an
adaptive mechanism to learn a wellstructured within-class feature distributions
by pulling easy samples to class centers while pushing hard samples away. This
prevents models from overfitting on noisy low-quality samples and improves face
recognition in the wild. Extensive experiments conducted on face recognition,
quality assessments as well as clustering demonstrate its superiority over
state-of-the-arts. The code is available at
https://github.com/IrvingMeng/MagFace.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qiang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shichao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhida Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Salient Object Detection: A Review and Benchmark. (arXiv:2010.04968v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04968</id>
        <link href="http://arxiv.org/abs/2010.04968"/>
        <updated>2021-07-27T02:03:34.219Z</updated>
        <summary type="html"><![CDATA[Salient object detection (SOD) is a long-standing research topic in computer
vision and has drawn an increasing amount of research interest in the past
decade. This paper provides the first comprehensive review and benchmark for
light field SOD, which has long been lacking in the saliency community.
Firstly, we introduce preliminary knowledge on light fields, including theory
and data forms, and then review existing studies on light field SOD, covering
ten traditional models, seven deep learning-based models, one comparative
study, and one brief review. Existing datasets for light field SOD are also
summarized with detailed information and statistical analyses. Secondly, we
benchmark nine representative light field SOD models together with several
cutting-edge RGB-D SOD models on four widely used light field datasets, from
which insightful discussions and analyses, including a comparison between light
field SOD and RGB-D SOD models, are achieved. Besides, due to the inconsistency
of datasets in their current forms, we further generate complete data and
supplement focal stacks, depth maps and multi-view images for the inconsistent
datasets, making them consistent and unified. Our supplemental data makes a
universal benchmark possible. Lastly, because light field SOD is quite a
special problem attributed to its diverse data representations and high
dependency on acquisition hardware, making it differ greatly from other
saliency detection tasks, we provide nine hints into the challenges and future
directions, and outline several open issues. We hope our review and
benchmarking could help advance research in this field. All the materials
including collected models, datasets, benchmarking results, and supplemented
light field datasets will be publicly available on our project site
https://github.com/kerenfu/LFSOD-Survey.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Keren Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qijun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. (arXiv:2012.15840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15840</id>
        <link href="http://arxiv.org/abs/2012.15840"/>
        <updated>2021-07-27T02:03:34.197Z</updated>
        <summary type="html"><![CDATA[Most recent semantic segmentation methods adopt a fully-convolutional network
(FCN) with an encoder-decoder architecture. The encoder progressively reduces
the spatial resolution and learns more abstract/semantic visual concepts with
larger receptive fields. Since context modeling is critical for segmentation,
the latest efforts have been focused on increasing the receptive field, through
either dilated/atrous convolutions or inserting attention modules. However, the
encoder-decoder based FCN architecture remains unchanged. In this paper, we aim
to provide an alternative perspective by treating semantic segmentation as a
sequence-to-sequence prediction task. Specifically, we deploy a pure
transformer (ie, without convolution and resolution reduction) to encode an
image as a sequence of patches. With the global context modeled in every layer
of the transformer, this encoder can be combined with a simple decoder to
provide a powerful segmentation model, termed SEgmentation TRansformer (SETR).
Extensive experiments show that SETR achieves new state of the art on ADE20K
(50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on
Cityscapes. Particularly, we achieve the first position in the highly
competitive ADE20K test server leaderboard on the day of submission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Sixiao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiachen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zekun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOGAN: Morphologic-structure-aware Generative Learning from a Single Image. (arXiv:2103.02997v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02997</id>
        <link href="http://arxiv.org/abs/2103.02997"/>
        <updated>2021-07-27T02:03:34.190Z</updated>
        <summary type="html"><![CDATA[In most interactive image generation tasks, given regions of interest (ROI)
by users, the generated results are expected to have adequate diversities in
appearance while maintaining correct and reasonable structures in original
images. Such tasks become more challenging if only limited data is available.
Recently proposed generative models complete training based on only one image.
They pay much attention to the monolithic feature of the sample while ignoring
the actual semantic information of different objects inside the sample. As a
result, for ROI-based generation tasks, they may produce inappropriate samples
with excessive randomicity and without maintaining the related objects' correct
structures. To address this issue, this work introduces a
MOrphologic-structure-aware Generative Adversarial Network named MOGAN that
produces random samples with diverse appearances and reliable structures based
on only one image. For training for ROI, we propose to utilize the data coming
from the original image being augmented and bring in a novel module to
transform such augmented data into knowledge containing both structures and
appearances, thus enhancing the model's comprehension of the sample. To learn
the rest areas other than ROI, we employ binary masks to ensure the generation
isolated from ROI. Finally, we set parallel and hierarchical branches of the
mentioned learning process. Compared with other single image GAN schemes, our
approach focuses on internal features including the maintenance of rational
structures and variation on appearance. Experiments confirm a better capacity
of our model on ROI-based image generation tasks than its competitive peers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qihui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1"&gt;Qi Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;MengChu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries. (arXiv:2104.02253v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02253</id>
        <link href="http://arxiv.org/abs/2104.02253"/>
        <updated>2021-07-27T02:03:34.182Z</updated>
        <summary type="html"><![CDATA[Depth completion starts from a sparse set of known depth values and estimates
the unknown depths for the remaining image pixels. Most methods model this as
depth interpolation and erroneously interpolate depth pixels into the empty
space between spatially distinct objects, resulting in depth-smearing across
occlusion boundaries. Here we propose a multi-hypothesis depth representation
that explicitly models both foreground and background depths in the difficult
occlusion-boundary regions. Our method can be thought of as performing
twin-surface extrapolation, rather than interpolation, in these regions. Next
our method fuses these extrapolated surfaces into a single depth image
leveraging the image data. Key to our method is the use of an asymmetric loss
function that operates on a novel twin-surface representation. This enables us
to train a network to simultaneously do surface extrapolation and surface
fusion. We characterize our loss function and compare with other common losses.
Finally, we validate our method on three different datasets; KITTI, an outdoor
real-world dataset, NYU2, indoor real-world depth dataset and Virtual KITTI, a
photo-realistic synthetic dataset with dense groundtruth, and demonstrate
improvement over the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imran_S/0/1/0/all/0/1"&gt;Saif Imran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;Daniel Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Action Recognition on Heterogeneous Embedded Devices. (arXiv:2107.12147v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.12147</id>
        <link href="http://arxiv.org/abs/2107.12147"/>
        <updated>2021-07-27T02:03:34.173Z</updated>
        <summary type="html"><![CDATA[Federated learning allows a large number of devices to jointly learn a model
without sharing data. In this work, we enable clients with limited computing
power to perform action recognition, a computationally heavy task. We first
perform model compression at the central server through knowledge distillation
on a large dataset. This allows the model to learn complex features and serves
as an initialization for model fine-tuning. The fine-tuning is required because
the limited data present in smaller datasets is not adequate for action
recognition models to learn complex spatio-temporal features. Because the
clients present are often heterogeneous in their computing resources, we use an
asynchronous federated optimization and we further show a convergence bound. We
compare our approach to two baseline approaches: fine-tuning at the central
server (no clients) and fine-tuning using (heterogeneous) clients using
synchronous federated averaging. We empirically show on a testbed of
heterogeneous embedded devices that we can perform action recognition with
comparable accuracy to the two baselines above, while our asynchronous learning
strategy reduces the training time by 40%, relative to synchronous learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Pranjal Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goenka_S/0/1/0/all/0/1"&gt;Shreyas Goenka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1"&gt;Biplab Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1"&gt;Somali Chaterji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Transformer Network. (arXiv:2102.00719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00719</id>
        <link href="http://arxiv.org/abs/2102.00719"/>
        <updated>2021-07-27T02:03:34.155Z</updated>
        <summary type="html"><![CDATA[This paper presents VTN, a transformer-based framework for video recognition.
Inspired by recent developments in vision transformers, we ditch the standard
approach in video action recognition that relies on 3D ConvNets and introduce a
method that classifies actions by attending to the entire video sequence
information. Our approach is generic and builds on top of any given 2D spatial
network. In terms of wall runtime, it trains $16.1\times$ faster and runs
$5.1\times$ faster during inference while maintaining competitive accuracy
compared to other state-of-the-art methods. It enables whole video analysis,
via a single end-to-end pass, while requiring $1.5\times$ fewer GFLOPs. We
report competitive results on Kinetics-400 and present an ablation study of VTN
properties and the trade-off between accuracy and inference speed. We hope our
approach will serve as a new baseline and start a fresh line of research in the
video recognition domain. Code and models are available at:
https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neimark_D/0/1/0/all/0/1"&gt;Daniel Neimark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bar_O/0/1/0/all/0/1"&gt;Omri Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohar_M/0/1/0/all/0/1"&gt;Maya Zohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asselmann_D/0/1/0/all/0/1"&gt;Dotan Asselmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective. (arXiv:2103.17263v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17263</id>
        <link href="http://arxiv.org/abs/2103.17263"/>
        <updated>2021-07-27T02:03:34.148Z</updated>
        <summary type="html"><![CDATA[Learning a good representation for space-time correspondence is the key for
various computer vision tasks, including tracking object bounding boxes and
performing video object pixel segmentation. To learn generalizable
representation for correspondence in large-scale, a variety of self-supervised
pretext tasks are proposed to explicitly perform object-level or patch-level
similarity learning. Instead of following the previous literature, we propose
to learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,
simply learning from comparing video frames. Our work is inspired by the recent
success in image-level contrastive learning and similarity learning for visual
recognition. Our hypothesis is that if the representation is good for
recognition, it requires the convolutional features to find correspondence
between similar objects or parts. Our experiments show surprising results that
VFS surpasses state-of-the-art self-supervised approaches for both OTB visual
object tracking and DAVIS video object segmentation. We perform detailed
analysis on what matters in VFS and reveals new properties on image and frame
level similarity learning. Project page is available at https://jerryxu.net/VFS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiarui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09451</id>
        <link href="http://arxiv.org/abs/2101.09451"/>
        <updated>2021-07-27T02:03:34.140Z</updated>
        <summary type="html"><![CDATA[Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Automated Diagnosis of COVID-19 Based on Scanning Images. (arXiv:2006.05245v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05245</id>
        <link href="http://arxiv.org/abs/2006.05245"/>
        <updated>2021-07-27T02:03:34.133Z</updated>
        <summary type="html"><![CDATA[The pandemic of COVID-19 has caused millions of infections, which has led to
a great loss all over the world, socially and economically. Due to the
false-negative rate and the time-consuming of the conventional Reverse
Transcription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on
X-ray images and Computed Tomography (CT) images has been widely adopted.
Therefore, researchers of the computer vision area have developed many
automatic diagnosing models based on machine learning or deep learning to
assist the radiologists and improve the diagnosing accuracy. In this paper, we
present a review of these recently emerging automatic diagnosing models. 70
models proposed from February 14, 2020, to July 21, 2020, are involved. We
analyzed the models from the perspective of preprocessing, feature extraction,
classification, and evaluation. Based on the limitation of existing models, we
pointed out that domain adaption in transfer learning and interpretability
promotion would be the possible future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1"&gt;Delong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shunhui Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xinyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-frame Feature Aggregation for Real-time Instrument Segmentation in Endoscopic Video. (arXiv:2011.08752v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08752</id>
        <link href="http://arxiv.org/abs/2011.08752"/>
        <updated>2021-07-27T02:03:34.126Z</updated>
        <summary type="html"><![CDATA[Deep learning-based methods have achieved promising results on surgical
instrument segmentation. However, the high computation cost may limit the
application of deep models to time-sensitive tasks such as online surgical
video analysis for robotic-assisted surgery. Moreover, current methods may
still suffer from challenging conditions in surgical images such as various
lighting conditions and the presence of blood. We propose a novel Multi-frame
Feature Aggregation (MFFA) module to aggregate video frame features temporally
and spatially in a recurrent mode. By distributing the computation load of deep
feature extraction over sequential frames, we can use a lightweight encoder to
reduce the computation costs at each time step. Moreover, public surgical
videos usually are not labeled frame by frame, so we develop a method that can
randomly synthesize a surgical frame sequence from a single labeled frame to
assist network training. We demonstrate that our approach achieves superior
performance to corresponding deeper segmentation models on two public surgery
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Fangbo Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Haonan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1"&gt;Randall A. Bly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moe_K/0/1/0/all/0/1"&gt;Kris S. Moe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1"&gt;Blake Hannaford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BubbleNet: Inferring micro-bubble dynamics with semi-physics-informed deep learning. (arXiv:2105.07179v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07179</id>
        <link href="http://arxiv.org/abs/2105.07179"/>
        <updated>2021-07-27T02:03:34.109Z</updated>
        <summary type="html"><![CDATA[Micro-bubbles and bubbly flows are widely observed and applied in chemical
engineering, medicine, involves deformation, rupture, and collision of bubbles,
phase mixture, etc. We study bubble dynamics by setting up two numerical
simulation cases: bubbly flow with a single bubble and multiple bubbles, both
confined in the microchannel, with parameters corresponding to their medical
backgrounds. Both the cases have their medical background applications.
Multiphase flow simulation requires high computation accuracy due to possible
component losses that may be caused by sparse meshing during the computation.
Hence, data-driven methods can be adopted as an useful tool. Based on
physics-informed neural networks (PINNs), we propose a novel deep learning
framework BubbleNet, which entails three main parts: deep neural networks (DNN)
with sub nets for predicting different physics fields; the
semi-physics-informed part, with only the fluid continuum condition and the
pressure Poisson equation $\mathcal{P}$ encoded within; the time discretized
normalizer (TDN), an algorithm to normalize field data per time step before
training. We apply the traditional DNN and our BubbleNet to train the coarsened
simulation data and predict the physics fields of both the two bubbly flow
cases. The BubbleNets are trained for both with and without $\mathcal{P}$, from
which we conclude that the 'physics-informed' part can serve as inner
supervision. Results indicate our framework can predict the physics fields more
accurately, estimating the prediction absolute errors. Our deep learning
predictions outperform traditional numerical methods computed with similar data
density meshing. The proposed network can potentially be applied to many other
engineering fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zhai_H/0/1/0/all/0/1"&gt;Hanfeng Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guohui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09046</id>
        <link href="http://arxiv.org/abs/2001.09046"/>
        <updated>2021-07-27T02:03:34.102Z</updated>
        <summary type="html"><![CDATA[We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.

Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.

We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.

We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1"&gt;Bart Smets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1"&gt;Jim Portegies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1"&gt;Remco Duits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation. (arXiv:2012.03769v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03769</id>
        <link href="http://arxiv.org/abs/2012.03769"/>
        <updated>2021-07-27T02:03:34.095Z</updated>
        <summary type="html"><![CDATA[Privacy concerns around sharing personally identifiable information are a
major practical barrier to data sharing in medical research. However, in many
cases, researchers have no interest in a particular individual's information
but rather aim to derive insights at the level of cohorts. Here, we utilize
Generative Adversarial Networks (GANs) to create derived medical imaging
datasets consisting entirely of synthetic patient data. The synthetic images
ideally have, in aggregate, similar statistical properties to those of a source
dataset but do not contain sensitive personal information. We assess the
quality of synthetic data generated by two GAN models for chest radiographs
with 14 different radiology findings and brain computed tomography (CT) scans
with six types of intracranial hemorrhages. We measure the synthetic image
quality by the performance difference of predictive models trained on either
the synthetic or the real dataset. We find that synthetic data performance
disproportionately benefits from a reduced number of unique label combinations.
Our open-source benchmark also indicates that at low number of samples per
class, label overfitting effects start to dominate GAN training. We
additionally conducted a reader study in which trained radiologists do not
perform better than random on discriminating between synthetic and real medical
images for intermediate levels of resolutions. In accordance with our benchmark
results, the classification accuracy of radiologists increases at higher
spatial resolution levels. Our study offers valuable guidelines and outlines
practical conditions under which insights derived from synthetic medical images
are similar to those that would have been derived from real imaging data. Our
results indicate that synthetic data sharing may be an attractive and
privacy-preserving alternative to sharing real patient-level data in the right
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schutte_A/0/1/0/all/0/1"&gt;August DuMont Sch&amp;#xfc;tte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hetzel_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Hetzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1"&gt;Tobias Hepp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dietz_B/0/1/0/all/0/1"&gt;Benedikt Dietz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schwab_P/0/1/0/all/0/1"&gt;Patrick Schwab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BASAR:Black-box Attack on Skeletal Action Recognition. (arXiv:2103.05266v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05266</id>
        <link href="http://arxiv.org/abs/2103.05266"/>
        <updated>2021-07-27T02:03:34.087Z</updated>
        <summary type="html"><![CDATA[Skeletal motion plays a vital role in human activity recognition as either an
independent data source or a complement. The robustness of skeleton-based
activity recognizers has been questioned recently, which shows that they are
vulnerable to adversarial attacks when the full-knowledge of the recognizer is
accessible to the attacker. However, this white-box requirement is overly
restrictive in most scenarios and the attack is not truly threatening. In this
paper, we show that such threats do exist under black-box settings too. To this
end, we propose the first black-box adversarial attack method BASAR. Through
BASAR, we show that adversarial attack is not only truly a threat but also can
be extremely deceitful, because on-manifold adversarial samples are rather
common in skeletal motions, in contrast to the common belief that adversarial
samples only exist off-manifold. Through exhaustive evaluation and comparison,
we show that BASAR can deliver successful attacks across models, data, and
attack modes. Through harsh perceptual studies, we show that it achieves
effective yet imperceptible attacks. By analyzing the attack on different
activity recognizers, BASAR helps identify the potential causes of their
vulnerability and provides insights on what classifiers are likely to be more
robust against attack. Code is available at
https://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1"&gt;Yunfeng Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1"&gt;Tianjia Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yong-Liang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Sparsifying Transform Learning for Image Denoising. (arXiv:2003.11265v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11265</id>
        <link href="http://arxiv.org/abs/2003.11265"/>
        <updated>2021-07-27T02:03:34.067Z</updated>
        <summary type="html"><![CDATA[The data-driven sparse methods such as synthesis dictionary learning (e.g.,
K-SVD) and sparsifying transform learning have been proven effective in image
denoising. However, they are intrinsically single-scale which can lead to
suboptimal results. We propose two methods developed based on wavelet subbands
mixing to efficiently combine the merits of both single and multiscale methods.
We show that an efficient multiscale method can be devised without the need for
denoising detail subbands which substantially reduces the runtime. The proposed
methods are initially derived within the framework of sparsifying transform
learning denoising, and then, they are generalized to propose our multiscale
extensions for the well-known K-SVD and SAIST image denoising methods. We
analyze and assess the studied methods thoroughly and compare them with the
well-known and state-of-the-art methods. The experiments show that our methods
are able to offer good trade-offs between performance and complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1"&gt;Ashkan Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monadjemi_A/0/1/0/all/0/1"&gt;Amirhassan Monadjemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1"&gt;Leyuan Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbani_H/0/1/0/all/0/1"&gt;Hossein Rabbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noormohammadi_N/0/1/0/all/0/1"&gt;Neda Noormohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One Transformer VAE. (arXiv:2105.04090v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04090</id>
        <link href="http://arxiv.org/abs/2105.04090"/>
        <updated>2021-07-27T02:03:34.059Z</updated>
        <summary type="html"><![CDATA[Transformers and variational autoencoders (VAE) have been extensively
employed for symbolic (e.g., MIDI) domain music generation. While the former
boast an impressive capability in modeling long sequences, the latter allow
users to willingly exert control over different parts (e.g., bars) of the music
to be generated. In this paper, we are interested in bringing the two together
to construct a single model that exhibits both strengths. The task is split
into two steps. First, we equip Transformer decoders with the ability to accept
segment-level, time-varying conditions during sequence generation.
Subsequently, we combine the developed and tested in-attention decoder with a
Transformer encoder, and train the resulting MuseMorphose model with the VAE
objective to achieve style transfer of long musical pieces, in which users can
specify musical attributes including rhythmic intensity and polyphony (i.e.,
harmonic fullness) they desire, down to the bar level. Experiments show that
MuseMorphose outperforms recurrent neural network (RNN) based baselines on
numerous widely-used metrics for style transfer tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shih-Lun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor optimal transport, distance between sets of measures and tensor scaling. (arXiv:2005.00945v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00945</id>
        <link href="http://arxiv.org/abs/2005.00945"/>
        <updated>2021-07-27T02:03:34.053Z</updated>
        <summary type="html"><![CDATA[We study the optimal transport problem for $d>2$ discrete measures. This is a
linear programming problem on $d$-tensors. It gives a way to compute a
"distance" between two sets of discrete measures. We introduce an entropic
regularization term, which gives rise to a scaling of tensors. We give a
variation of the celebrated Sinkhorn scaling algorithm. We show that this
algorithm can be viewed as a partial minimization algorithm of a strictly
convex function. Under appropriate conditions the rate of convergence is
geometric and we estimate the rate. Our results are generalizations of known
results for the classical case of two discrete measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Friedland_S/0/1/0/all/0/1"&gt;Shmuel Friedland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08686</id>
        <link href="http://arxiv.org/abs/2107.08686"/>
        <updated>2021-07-27T02:03:34.046Z</updated>
        <summary type="html"><![CDATA[Generalization performance of stochastic optimization stands a central place
in learning theory. In this paper, we investigate the excess risk performance
and towards improved learning rates for two popular approaches of stochastic
optimization: empirical risk minimization (ERM) and stochastic gradient descent
(SGD). Although there exists plentiful generalization analysis of ERM and SGD
for supervised learning, current theoretical understandings of ERM and SGD
either have stronger assumptions in convex learning, e.g., strong convexity, or
show slow rates and less studied in nonconvex learning. Motivated by these
problems, we aim to provide improved rates under milder assumptions in convex
learning and derive faster rates in nonconvex learning. It is notable that our
analysis span two popular theoretical viewpoints: \emph{stability} and
\emph{uniform convergence}. Specifically, in stability regime, we present high
probability learning rates of order $\mathcal{O} (1/n)$ w.r.t. the sample size
$n$ for ERM and SGD with milder assumptions in convex learning and similar high
probability rates of order $\mathcal{O} (1/n)$ in nonconvex learning, rather
than in expectation. Furthermore, this type of learning rate is improved to
faster order $\mathcal{O} (1/n^2)$ in uniform convergence regime. To our best
knowledge, for ERM and SGD, the learning rates presented in this paper are all
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. (arXiv:2102.05426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05426</id>
        <link href="http://arxiv.org/abs/2102.05426"/>
        <updated>2021-07-27T02:03:34.040Z</updated>
        <summary type="html"><![CDATA[We study the challenging task of neural network quantization without
end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually
requires a small subset of training data but produces less powerful quantized
models than Quantization-Aware Training (QAT). In this work, we propose a novel
PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to
INT2 for the first time. BRECQ leverages the basic building blocks in neural
networks and reconstructs them one-by-one. In a comprehensive theoretical study
of the second-order error, we show that BRECQ achieves a good balance between
cross-layer dependency and generalization error. To further employ the power of
quantization, the mixed precision technique is incorporated in our framework by
approximating the inter-layer and intra-layer sensitivity. Extensive
experiments on various handcrafted and searched neural architectures are
conducted for both image classification and object detection tasks. And for the
first time we prove that, without bells and whistles, PTQ can attain 4-bit
ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster
production of quantized models. Codes are available at
https://github.com/yhhhli/BRECQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Peng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fengwei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11642</id>
        <link href="http://arxiv.org/abs/2106.11642"/>
        <updated>2021-07-27T02:03:34.022Z</updated>
        <summary type="html"><![CDATA[Deep ensembles have recently gained popularity in the deep learning community
for their conceptual simplicity and efficiency. However, maintaining functional
diversity between ensemble members that are independently trained with gradient
descent is challenging. This can lead to pathologies when adding more ensemble
members, such as a saturation of the ensemble performance, which converges to
the performance of a single model. Moreover, this does not only affect the
quality of its predictions, but even more so the uncertainty estimates of the
ensemble, and thus its performance on out-of-distribution data. We hypothesize
that this limitation can be overcome by discouraging different ensemble members
from collapsing to the same function. To this end, we introduce a kernelized
repulsive term in the update rule of the deep ensembles. We show that this
simple modification not only enforces and maintains diversity among the members
but, even more importantly, transforms the maximum a posteriori inference into
proper Bayesian inference. Namely, we show that the training dynamics of our
proposed repulsive ensembles follow a Wasserstein gradient flow of the KL
divergence with the true posterior. We study repulsive terms in weight and
function space and empirically compare their performance to standard ensembles
and Bayesian baselines on synthetic and real-world prediction tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1"&gt;Francesco D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11517</id>
        <link href="http://arxiv.org/abs/2101.11517"/>
        <updated>2021-07-27T02:03:34.015Z</updated>
        <summary type="html"><![CDATA[Bi-Level Optimization (BLO) is originated from the area of economic game
theory and then introduced into the optimization community. BLO is able to
handle problems with a hierarchical structure, involving two levels of
optimization tasks, where one task is nested inside the other. In machine
learning and computer vision fields, despite the different motivations and
mechanisms, a lot of complex problems, such as hyper-parameter optimization,
multi-task and meta-learning, neural architecture search, adversarial learning
and deep reinforcement learning, actually all contain a series of closely
related subproblms. In this paper, we first uniformly express these complex
learning and vision problems from the perspective of BLO. Then we construct a
best-response-based single-level reformulation and establish a unified
algorithmic framework to understand and formulate mainstream gradient-based BLO
methodologies, covering aspects ranging from fundamental automatic
differentiation schemes to various accelerations, simplifications, extensions
and their convergence and complexity properties. Last but not least, we discuss
the potentials of our unified BLO framework for designing new algorithms and
point out some promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Risheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiaxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10937</id>
        <link href="http://arxiv.org/abs/2105.10937"/>
        <updated>2021-07-27T02:03:34.008Z</updated>
        <summary type="html"><![CDATA[Terrain traversability analysis plays a major role in ensuring safe robotic
navigation in unstructured environments. However, real-time constraints
frequently limit the accuracy of online tests especially in scenarios where
realistic robot-terrain interactions are complex to model. In this context, we
propose a deep learning framework trained in an end-to-end fashion from
elevation maps and trajectories to estimate the occurrence of failure events.
The network is first trained and tested in simulation over synthetic maps
generated by the OpenSimplex algorithm. The prediction performance of the Deep
Learning framework is illustrated by being able to retain over 94% recall of
the original simulator at 30% of the computational time. Finally, the network
is transferred and tested on real elevation maps collected by the SEEKER
consortium during the Martian rover test trial in the Atacama desert in Chile.
We show that transferring and fine-tuning of an application-independent
pre-trained model retains better performance than training uniquely on scarcely
available real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1"&gt;Marco Visca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1"&gt;Roger Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08114</id>
        <link href="http://arxiv.org/abs/2107.08114"/>
        <updated>2021-07-27T02:03:34.002Z</updated>
        <summary type="html"><![CDATA[Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of
Reinforcement Learning due to the non-stationarity of the environments and the
large dimensionality of the combined action space. Deep MARL algorithms have
been applied to solve different task offloading problems. However, in
real-world applications, information required by the agents (i.e. rewards and
states) are subject to noise and alterations. The stability and the robustness
of deep MARL to practical challenges is still an open research problem. In this
work, we apply state-of-the art MARL algorithms to solve task offloading with
reward uncertainty. We show that perturbations in the reward signal can induce
decrease in the performance compared to learning with perfect rewards. We
expect this paper to stimulate more research in studying and addressing the
practical challenges of deploying deep MARL solutions in wireless
communications systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuanchao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1"&gt;Amal Feriani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1"&gt;Ekram Hossain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00502</id>
        <link href="http://arxiv.org/abs/2103.00502"/>
        <updated>2021-07-27T02:03:33.994Z</updated>
        <summary type="html"><![CDATA[This paper concentrates on the approximation power of deep feed-forward
neural networks in terms of width and depth. It is proved by construction that
ReLU networks with width $\mathcal{O}\big(\max\{d\lfloor N^{1/d}\rfloor,\,
N+2\}\big)$ and depth $\mathcal{O}(L)$ can approximate a H\"older continuous
function on $[0,1]^d$ with an approximation rate
$\mathcal{O}\big(\lambda\sqrt{d} (N^2L^2\ln N)^{-\alpha/d}\big)$, where
$\alpha\in (0,1]$ and $\lambda>0$ are H\"older order and constant,
respectively. Such a rate is optimal up to a constant in terms of width and
depth separately, while existing results are only nearly optimal without the
logarithmic factor in the approximation rate. More generally, for an arbitrary
continuous function $f$ on $[0,1]^d$, the approximation rate becomes
$\mathcal{O}\big(\,\sqrt{d}\,\omega_f\big( (N^2L^2\ln N)^{-1/d}\big)\,\big)$,
where $\omega_f(\cdot)$ is the modulus of continuity. We also extend our
analysis to any continuous function $f$ on a bounded set. Particularly, if ReLU
networks with depth $31$ and width $\mathcal{O}(N)$ are used to approximate
one-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz
constant $\lambda>0$, the approximation rate in terms of the total number of
parameters, $W=\mathcal{O}(N^2)$, becomes $\mathcal{O}(\tfrac{\lambda}{W\ln
W})$, which has not been discovered in the literature for fixed-depth ReLU
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuowei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08821</id>
        <link href="http://arxiv.org/abs/2107.08821"/>
        <updated>2021-07-27T02:03:33.974Z</updated>
        <summary type="html"><![CDATA[This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,
Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)
have undoubtedly brought great success to a wide range of applications in
computer vision, computational linguistics, and AI. However, foundational
principles underlying the DNNs' success and their resilience to adversarial
attacks are still largely missing. Interpreting and theorizing the internal
mechanisms of DNNs becomes a compelling yet controversial topic. This workshop
pays a special interest in theoretic foundations, limitations, and new
application trends in the scope of XAI. These issues reflect new bottlenecks in
the future development of XAI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lixin Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-07-27T02:03:33.967Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06825</id>
        <link href="http://arxiv.org/abs/2107.06825"/>
        <updated>2021-07-27T02:03:33.960Z</updated>
        <summary type="html"><![CDATA[We introduce a generalization to the lottery ticket hypothesis in which the
notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this "generalized" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1"&gt;Ilya Tolstikhin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSIT-Free Model Aggregation for Federated Edge Learning via Reconfigurable Intelligent Surface. (arXiv:2102.10749v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10749</id>
        <link href="http://arxiv.org/abs/2102.10749"/>
        <updated>2021-07-27T02:03:33.952Z</updated>
        <summary type="html"><![CDATA[We study over-the-air model aggregation in federated edge learning (FEEL)
systems, where channel state information at the transmitters (CSIT) is assumed
to be unavailable. We leverage the reconfigurable intelligent surface (RIS)
technology to align the cascaded channel coefficients for CSIT-free model
aggregation. To this end, we jointly optimize the RIS and the receiver by
minimizing the aggregation error under the channel alignment constraint. We
then develop a difference-of-convex algorithm for the resulting non-convex
optimization. Numerical experiments on image classification show that the
proposed method is able to achieve a similar learning accuracy as the
state-of-the-art CSIT-based solution, demonstrating the efficiency of our
approach in combating the lack of CSIT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaojun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying-Jun Angela Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Canonical Correlation Alignment for Sensor Signals. (arXiv:2106.03637v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03637</id>
        <link href="http://arxiv.org/abs/2106.03637"/>
        <updated>2021-07-27T02:03:33.945Z</updated>
        <summary type="html"><![CDATA[Sensor technologies are becoming increasingly prevalent in the biomedical
field, with applications ranging from telemonitoring of people at risk, to
using sensor derived information as objective endpoints in clinical trials. To
fully utilize sensor information, signals from distinct sensors often have to
be temporally aligned. However, due to imperfect oscillators and significant
noise, commonly encountered with biomedical signals, temporal alignment of raw
signals is an all but trivial problem, with, to-date, no generally applicable
solution. In this work, we present Deep Canonical Correlation Alignment (DCCA),
a novel, generally applicable solution for the temporal alignment of raw
(biomedical) sensor signals. DCCA allows practitioners to directly align raw
signals, from distinct sensors, without requiring deep domain knowledge. On a
selection of artificial and real datasets, we demonstrate the performance and
utility of DCCA under a variety of conditions. We compare the DCCA algorithm to
other warping based methods, DCCA outperforms dynamic time warping and cross
correlation based methods by an order of magnitude in terms of alignment error.
DCCA performs especially well on almost periodic biomedical signals such as
heart-beats and breathing patterns. In comparison to existing approaches, that
are not tailored towards raw sensor data, DCCA is not only fast enough to work
on signals with billions of data points but also provides automatic filtering
and transformation functionalities, allowing it to deal with very noisy and
even morphologically distinct signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutz_N/0/1/0/all/0/1"&gt;Narayan Sch&amp;#xfc;tz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botros_A/0/1/0/all/0/1"&gt;Angela Botros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Single_M/0/1/0/all/0/1"&gt;Michael Single&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naef_A/0/1/0/all/0/1"&gt;Aileen C. Naef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buluschek_P/0/1/0/all/0/1"&gt;Philipp Buluschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nef_T/0/1/0/all/0/1"&gt;Tobias Nef&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00331</id>
        <link href="http://arxiv.org/abs/1906.00331"/>
        <updated>2021-07-27T02:03:33.938Z</updated>
        <summary type="html"><![CDATA[We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}}
\max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is
nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a
convex and bounded set. One of the most popular algorithms for solving this
problem is the celebrated gradient descent ascent (GDA) algorithm, which has
been widely used in machine learning, control theory and economics. Despite the
extensive convergence results for the convex-concave setting, GDA with equal
stepsize can converge to limit cycles or even diverge in a general setting. In
this paper, we present the complexity results on two-time-scale GDA for solving
nonconvex-concave minimax problems, showing that the algorithm can find a
stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in
\mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this
is the first nonasymptotic analysis for two-time-scale GDA in this setting,
shedding light on its superior practical performance in training generative
adversarial networks (GANs) and other real applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomalous diffusion dynamics of learning in deep neural networks. (arXiv:2009.10588v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10588</id>
        <link href="http://arxiv.org/abs/2009.10588"/>
        <updated>2021-07-27T02:03:33.919Z</updated>
        <summary type="html"><![CDATA[Learning in deep neural networks (DNNs) is implemented through minimizing a
highly non-convex loss function, typically by a stochastic gradient descent
(SGD) method. This learning process can effectively find good wide minima
without being trapped in poor local ones. We present a novel account of how
such effective deep learning emerges through the interactions of the SGD and
the geometrical structure of the loss landscape. Rather than being a normal
diffusion process (i.e. Brownian motion) as often assumed, we find that the SGD
exhibits rich, complex dynamics when navigating through the loss landscape;
initially, the SGD exhibits anomalous superdiffusion, which attenuates
gradually and changes to subdiffusion at long times when the solution is
reached. Such learning dynamics happen ubiquitously in different DNNs such as
ResNet and VGG-like networks and are insensitive to batch size and learning
rate. The anomalous superdiffusion process during the initial learning phase
indicates that the motion of SGD along the loss landscape possesses
intermittent, big jumps; this non-equilibrium property enables the SGD to
escape from sharp local minima. By adapting the methods developed for studying
energy landscapes in complex physical systems, we find that such superdiffusive
learning dynamics are due to the interactions of the SGD and the fractal-like
structure of the loss landscape. We further develop a simple model to
demonstrate the mechanistic role of the fractal loss landscape in enabling the
SGD to effectively find global minima. Our results thus reveal the
effectiveness of deep learning from a novel perspective and have implications
for designing efficient deep neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guozhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1"&gt;Cheng Kevin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1"&gt;Pulin Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Multisensor Change Detection. (arXiv:2103.05102v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05102</id>
        <link href="http://arxiv.org/abs/2103.05102"/>
        <updated>2021-07-27T02:03:33.911Z</updated>
        <summary type="html"><![CDATA[Most change detection methods assume that pre-change and post-change images
are acquired by the same sensor. However, in many real-life scenarios, e.g.,
natural disaster, it is more practical to use the latest available images
before and after the occurrence of incidence, which may be acquired using
different sensors. In particular, we are interested in the combination of the
images acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR
images appear vastly different from the optical images even when capturing the
same scene. Adding to this, change detection methods are often constrained to
use only target image-pair, no labeled data, and no additional unlabeled data.
Such constraints limit the scope of traditional supervised machine learning and
unsupervised generative approaches for multi-sensor change detection. Recent
rapid development of self-supervised learning methods has shown that some of
them can even work with only few images. Motivated by this, in this work we
propose a method for multi-sensor change detection using only the unlabeled
target bi-temporal images that are used for training a network in
self-supervised fashion by using deep clustering and contrastive learning. The
proposed method is evaluated on four multi-modal bi-temporal scenes showing
change and the benefits of our self-supervised approach are demonstrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sudipan Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1"&gt;Patrick Ebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On generalization in moment-based domain adaptation. (arXiv:2002.08260v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08260</id>
        <link href="http://arxiv.org/abs/2002.08260"/>
        <updated>2021-07-27T02:03:33.901Z</updated>
        <summary type="html"><![CDATA[Domain adaptation algorithms are designed to minimize the misclassification
risk of a discriminative model for a target domain with little training data by
adapting a model from a source domain with a large amount of training data.
Standard approaches measure the adaptation discrepancy based on distance
measures between the empirical probability distributions in the source and
target domain. In this setting, we address the problem of deriving
generalization bounds under practice-oriented general conditions on the
underlying probability distributions. As a result, we obtain generalization
bounds for domain adaptation based on finitely many moments and smoothness
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zellinger_W/0/1/0/all/0/1"&gt;Werner Zellinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moser_B/0/1/0/all/0/1"&gt;Bernhard A Moser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saminger_Platz_S/0/1/0/all/0/1"&gt;Susanne Saminger-Platz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks. (arXiv:1911.05541v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.05541</id>
        <link href="http://arxiv.org/abs/1911.05541"/>
        <updated>2021-07-27T02:03:33.894Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of vehicle identification through
non-overlapping cameras. As our main contribution, we introduce a novel dataset
for vehicle identification, called Vehicle-Rear, that contains more than three
hours of high-resolution videos, with accurate information about the make,
model, color and year of nearly 3,000 vehicles, in addition to the position and
identification of their license plates. To explore our dataset we design a
two-stream CNN that simultaneously uses two of the most distinctive and
persistent features available: the vehicle's appearance and its license plate.
This is an attempt to tackle a major problem: false alarms caused by vehicles
with similar designs or by very close license plate identifiers. In the first
network stream, shape similarities are identified by a Siamese CNN that uses a
pair of low-resolution vehicle patches recorded by two different cameras. In
the second stream, we use a CNN for OCR to extract textual information,
confidence scores, and string similarities from a pair of high-resolution
license plate patches. Then, features from both streams are merged by a
sequence of fully connected layers for decision. In our experiments, we
compared the two-stream network against several well-known CNN architectures
using single or multiple vehicle features. The architectures, trained models,
and dataset are publicly available at https://github.com/icarofua/vehicle-rear.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1"&gt;Icaro O. de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1"&gt;Rayson Laroca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1"&gt;David Menotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_K/0/1/0/all/0/1"&gt;Keiko V. O. Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1"&gt;Rodrigo Minetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11517</id>
        <link href="http://arxiv.org/abs/2101.11517"/>
        <updated>2021-07-27T02:03:33.881Z</updated>
        <summary type="html"><![CDATA[Bi-Level Optimization (BLO) is originated from the area of economic game
theory and then introduced into the optimization community. BLO is able to
handle problems with a hierarchical structure, involving two levels of
optimization tasks, where one task is nested inside the other. In machine
learning and computer vision fields, despite the different motivations and
mechanisms, a lot of complex problems, such as hyper-parameter optimization,
multi-task and meta-learning, neural architecture search, adversarial learning
and deep reinforcement learning, actually all contain a series of closely
related subproblms. In this paper, we first uniformly express these complex
learning and vision problems from the perspective of BLO. Then we construct a
best-response-based single-level reformulation and establish a unified
algorithmic framework to understand and formulate mainstream gradient-based BLO
methodologies, covering aspects ranging from fundamental automatic
differentiation schemes to various accelerations, simplifications, extensions
and their convergence and complexity properties. Last but not least, we discuss
the potentials of our unified BLO framework for designing new algorithms and
point out some promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Risheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiaxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D Pretraining. (arXiv:2104.04687v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04687</id>
        <link href="http://arxiv.org/abs/2104.04687"/>
        <updated>2021-07-27T02:03:33.862Z</updated>
        <summary type="html"><![CDATA[Most 3D neural networks are trained from scratch owing to the lack of
large-scale labeled datasets. In this paper, we present a novel 3D pretraining
method by leveraging 2D networks learned from rich 2D datasets. We propose the
contrastive pixel-to-point knowledge transfer to effectively utilize the 2D
information by mapping the pixel-level and point-level features into the same
embedding space. Due to the heterogeneous nature between 2D and 3D networks, we
introduce the back-projection function to align the features between 2D and 3D
to make the transfer possible. Additionally, we devise an upsampling feature
projection layer to increase the spatial resolution of high-level 2D feature
maps, which helps learning fine-grained 3D representations. With a pretrained
2D network, the proposed pretraining process requires no additional 2D or 3D
labeled data, further alleviating the expansive 3D data annotation cost. To the
best of our knowledge, we are the first to exploit existing 2D trained weights
to pretrain 3D deep neural networks. Our intensive experiments show that the 3D
models pretrained with 2D knowledge boost the performances across various
real-world 3D downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hung-Yueh Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chin-Tang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1"&gt;Ching-Yu Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection. (arXiv:2104.10719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10719</id>
        <link href="http://arxiv.org/abs/2104.10719"/>
        <updated>2021-07-27T02:03:33.813Z</updated>
        <summary type="html"><![CDATA[This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for
energy-efficient and robust object detection in resource-constrained platforms.
The network architecture is based on Convolutional SNN using
leaky-integrate-fire neuron models. The model combines unsupervised Spike
Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning
methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty
error. FSHNN provides better accuracy compared to DNN based object detectors
while being 150X energy-efficient. It also outperforms these object detectors,
when subjected to noisy input data and less labeled training data with a lower
uncertainty error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1"&gt;Biswadeep Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_X/0/1/0/all/0/1"&gt;Xueyuan She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1"&gt;Saibal Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection. (arXiv:2007.06963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06963</id>
        <link href="http://arxiv.org/abs/2007.06963"/>
        <updated>2021-07-27T02:03:33.804Z</updated>
        <summary type="html"><![CDATA[One-class novelty detection is to identify anomalous instances that do not
conform to the expected normal instances. In this paper, the Generative
Adversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used
for detection and achieve state-of-the-art performance. However, deep neural
networks are too over-parameterized to deploy on resource-limited devices.
Therefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to
learn compact and fast novelty detection networks. The P-KDGAN is a novel
attempt to connect two standard GANs by the designed distillation loss for
transferring knowledge from the teacher to the student. The progressive
learning of knowledge distillation is a two-step approach that continuously
improves the performance of the student GAN and achieves better performance
than single step methods. In the first step, the student GAN learns the basic
knowledge totally from the teacher via guiding of the pretrained teacher GAN
with fixed weights. In the second step, joint fine-training is adopted for the
knowledgeable teacher and student GANs to further improve the performance and
stability. The experimental results on CIFAR-10, MNIST, and FMNIST show that
our method improves the performance of the student GAN by 2.44%, 1.77%, and
1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and
700:1, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax. (arXiv:1812.04275v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.04275</id>
        <link href="http://arxiv.org/abs/1812.04275"/>
        <updated>2021-07-27T02:03:33.798Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),
for which the key is to bridge the gap between sketches and photos in terms of
the data representation. Inspired by channel-wise attention explored in recent
years, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which
seamlessly incorporates the prior knowledge of sample sketch or photo into SE
module and make the SE module capable of emphasizing appropriate channels
according to domain signal. Accordingly, the proposed network can switch its
mode to achieve a better domain feature with lower intra-class discrepancy.
Moreover, while previous works simply focus on minimizing intra-class distance
and maximizing inter-class distance, we introduce a loss function, named
Multiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative
Euclidean margin into feature space and ensure that the maximum intra-class
distance is smaller than the minimum inter-class distance. This facilitates
learning a highly discriminative feature space and ensures a more accurate
image retrieval result. Extensive experiments are conducted on two widely used
SBIR benchmark datasets. Our approach achieves better results on both datasets,
surpassing the state-of-the-art methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Peng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hangyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Compression with Deep Pre-Editing. (arXiv:2002.00113v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00113</id>
        <link href="http://arxiv.org/abs/2002.00113"/>
        <updated>2021-07-27T02:03:33.790Z</updated>
        <summary type="html"><![CDATA[Could we compress images via standard codecs while avoiding visible
artifacts? The answer is obvious -- this is doable as long as the bit budget is
generous enough. What if the allocated bit-rate for compression is
insufficient? Then unfortunately, artifacts are a fact of life. Many attempts
were made over the years to fight this phenomenon, with various degrees of
success. In this work we aim to break the unholy connection between bit-rate
and image quality, and propose a way to circumvent compression artifacts by
pre-editing the incoming image and modifying its content to fit the given bits.
We design this editing operation as a learned convolutional neural network, and
formulate an optimization problem for its training. Our loss takes into account
a proximity between the original image and the edited one, a bit-budget penalty
over the proposed image, and a no-reference image quality measure for forcing
the outcome to be visually pleasing. The proposed approach is demonstrated on
the popular JPEG compression, showing savings in bits and/or improvements in
visual quality, obtained with intricate editing effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1"&gt;Hossein Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1"&gt;Damien Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiyang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dorado_I/0/1/0/all/0/1"&gt;Ignacio Garcia Dorado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1"&gt;Peyman Milanfar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1"&gt;Michael Elad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image compression optimized for 3D reconstruction by utilizing deep neural networks. (arXiv:2003.12618v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12618</id>
        <link href="http://arxiv.org/abs/2003.12618"/>
        <updated>2021-07-27T02:03:33.772Z</updated>
        <summary type="html"><![CDATA[Computer vision tasks are often expected to be executed on compressed images.
Classical image compression standards like JPEG 2000 are widely used. However,
they do not account for the specific end-task at hand. Motivated by works on
recurrent neural network (RNN)-based image compression and three-dimensional
(3D) reconstruction, we propose unified network architectures to solve both
tasks jointly. These joint models provide image compression tailored for the
specific task of 3D reconstruction. Images compressed by our proposed models,
yield 3D reconstruction performance superior as compared to using JPEG 2000
compression. Our models significantly extend the range of compression rates for
which 3D reconstruction is possible. We also show that this can be done highly
efficiently at almost no additional cost to obtain compression on top of the
computation already required for performing the 3D reconstruction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Golts_A/0/1/0/all/0/1"&gt;Alex Golts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schechner_Y/0/1/0/all/0/1"&gt;Yoav Y. Schechner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks. (arXiv:1911.05541v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.05541</id>
        <link href="http://arxiv.org/abs/1911.05541"/>
        <updated>2021-07-27T02:03:33.765Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of vehicle identification through
non-overlapping cameras. As our main contribution, we introduce a novel dataset
for vehicle identification, called Vehicle-Rear, that contains more than three
hours of high-resolution videos, with accurate information about the make,
model, color and year of nearly 3,000 vehicles, in addition to the position and
identification of their license plates. To explore our dataset we design a
two-stream CNN that simultaneously uses two of the most distinctive and
persistent features available: the vehicle's appearance and its license plate.
This is an attempt to tackle a major problem: false alarms caused by vehicles
with similar designs or by very close license plate identifiers. In the first
network stream, shape similarities are identified by a Siamese CNN that uses a
pair of low-resolution vehicle patches recorded by two different cameras. In
the second stream, we use a CNN for OCR to extract textual information,
confidence scores, and string similarities from a pair of high-resolution
license plate patches. Then, features from both streams are merged by a
sequence of fully connected layers for decision. In our experiments, we
compared the two-stream network against several well-known CNN architectures
using single or multiple vehicle features. The architectures, trained models,
and dataset are publicly available at https://github.com/icarofua/vehicle-rear.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1"&gt;Icaro O. de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1"&gt;Rayson Laroca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1"&gt;David Menotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_K/0/1/0/all/0/1"&gt;Keiko V. O. Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1"&gt;Rodrigo Minetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of Outside-Vehicle Objects. (arXiv:2107.12167v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.12167</id>
        <link href="http://arxiv.org/abs/2107.12167"/>
        <updated>2021-07-27T02:03:33.756Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in more intelligent natural user interaction with
the car. Hand gestures and speech are already being applied for driver-car
interaction. Moreover, multimodal approaches are also showing promise in the
automotive industry. In this paper, we utilize deep learning for a multimodal
fusion network for referencing objects outside the vehicle. We use features
from gaze, head pose and finger pointing simultaneously to precisely predict
the referenced objects in different car poses. We demonstrate the practical
limitations of each modality when used for a natural form of referencing,
specifically inside the car. As evident from our results, we overcome the
modality specific limitations, to a large extent, by the addition of other
modalities. This work highlights the importance of multimodal sensing,
especially when moving towards natural user interaction. Furthermore, our user
based analysis shows noteworthy differences in recognition of user behavior
depending upon the vehicle pose.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1"&gt;Abdul Rafey Aftab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1"&gt;Michael von der Beeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrhirsch_S/0/1/0/all/0/1"&gt;Steven Rohrhirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diotte_B/0/1/0/all/0/1"&gt;Benoit Diotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1"&gt;Michael Feld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-Box Diagnosis and Calibration on GAN Intra-Mode Collapse: A Pilot Study. (arXiv:2107.12202v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12202</id>
        <link href="http://arxiv.org/abs/2107.12202"/>
        <updated>2021-07-27T02:03:33.750Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) nowadays are capable of producing
images of incredible realism. One concern raised is whether the
state-of-the-art GAN's learned distribution still suffers from mode collapse,
and what to do if so. Existing diversity tests of samples from GANs are usually
conducted qualitatively on a small scale, and/or depends on the access to
original training data as well as the trained model parameters. This paper
explores to diagnose GAN intra-mode collapse and calibrate that, in a novel
black-box setting: no access to training data, nor the trained model
parameters, is assumed. The new setting is practically demanded, yet rarely
explored and significantly more challenging. As a first stab, we devise a set
of statistical tools based on sampling, that can visualize, quantify, and
rectify intra-mode collapse. We demonstrate the effectiveness of our proposed
diagnosis and calibration techniques, via extensive simulations and
experiments, on unconditional GAN image generation (e.g., face and vehicle).
Our study reveals that the intra-mode collapse is still a prevailing problem in
state-of-the-art GANs and the mode collapse is diagnosable and calibratable in
black-box settings. Our codes are available at:
https://github.com/VITA-Group/BlackBoxGANCollapse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhenyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaowen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Ye Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hailin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12220</id>
        <link href="http://arxiv.org/abs/2107.12220"/>
        <updated>2021-07-27T02:03:33.743Z</updated>
        <summary type="html"><![CDATA[When humans solve complex problems, they rarely come up with a decision
right-away. Instead, they start with an intuitive decision, reflect upon it,
spot mistakes, resolve contradictions and jump between different hypotheses.
Thus, they create a sequence of ideas and follow a train of thought that
ultimately reaches a conclusive decision. Contrary to this, today's neural
classification models are mostly trained to map an input to one single and
fixed output. In this paper, we investigate how we can give models the
opportunity of a second, third and $k$-th thought. We take inspiration from
Hegel's dialectics and propose a method that turns an existing classifier's
class prediction (such as the image class forest) into a sequence of
predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom).
Concretely, we propose a correction module that is trained to estimate the
model's correctness as well as an iterative prediction update based on the
prediction's gradient. Our approach results in a dynamic system over class
probability distributions $\unicode{x2014}$ the thought flow. We evaluate our
method on diverse datasets and tasks from computer vision and natural language
processing. We observe surprisingly complex but intuitive behavior and
demonstrate that our method (i) can correct misclassifications, (ii)
strengthens model performance, (iii) is robust to high levels of adversarial
attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift
setting and (iv) provides a tool for model interpretability that uncovers model
knowledge which otherwise remains invisible in a single distribution
prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1"&gt;Hendrik Schuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1"&gt;Heike Adel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1"&gt;Ngoc Thang Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. (arXiv:2012.00924v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00924</id>
        <link href="http://arxiv.org/abs/2012.00924"/>
        <updated>2021-07-27T02:03:33.725Z</updated>
        <summary type="html"><![CDATA[Modeling the hand-object (HO) interaction not only requires estimation of the
HO pose, but also pays attention to the contact due to their interaction.
Significant progress has been made in estimating hand and object separately
with deep learning methods, simultaneous HO pose estimation and contact
modeling has not yet been fully explored. In this paper, we present an explicit
contact representation namely Contact Potential Field (CPF), and a
learning-fitting hybrid framework namely MIHO to Modeling the Interaction of
Hand and Object. In CPF, we treat each contacting HO vertex pair as a
spring-mass system. Hence the whole system forms a potential field with minimal
elastic energy at the grasp position. Extensive experiments on the two commonly
used benchmarks have demonstrated that our method can achieve state-of-the-art
in several reconstruction metrics, and allow us to produce more physically
plausible HO pose even when the ground-truth exhibits severe interpenetration
or disjointedness. Our code is available at https://github.com/lixiny/CPF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lixin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xinyu Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kailin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenqiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. (arXiv:2006.16241v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16241</id>
        <link href="http://arxiv.org/abs/2006.16241"/>
        <updated>2021-07-27T02:03:33.719Z</updated>
        <summary type="html"><![CDATA[We introduce four new real-world distribution shift datasets consisting of
changes in image style, image blurriness, geographic location, camera
operation, and more. With our new datasets, we take stock of previously
proposed methods for improving out-of-distribution robustness and put them to
the test. We find that using larger models and artificial data augmentations
can improve robustness on real-world distribution shifts, contrary to claims in
prior work. We find improvements in artificial robustness benchmarks can
transfer to real-world distribution shifts, contrary to claims in prior work.
Motivated by our observation that data augmentations can help with real-world
distribution shifts, we also introduce a new data augmentation method which
advances the state-of-the-art and outperforms models pretrained with 1000 times
more labeled data. Overall we find that some methods consistently help with
distribution shifts in texture and local image statistics, but these methods do
not help with some other distribution shifts like geographic changes. Our
results show that future research must study multiple distribution shifts
simultaneously, as we demonstrate that no evaluated method consistently
improves robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1"&gt;Norman Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1"&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Frank Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorundo_E/0/1/0/all/0/1"&gt;Evan Dorundo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1"&gt;Rahul Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tyler Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1"&gt;Samyak Parajuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mike Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1"&gt;Justin Gilmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PlenoptiCam v1.0: A light-field imaging framework. (arXiv:2010.11687v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11687</id>
        <link href="http://arxiv.org/abs/2010.11687"/>
        <updated>2021-07-27T02:03:33.711Z</updated>
        <summary type="html"><![CDATA[Light-field cameras play a vital role for rich 3-D information retrieval in
narrow range depth sensing applications. The key obstacle in composing
light-fields from exposures taken by a plenoptic camera is to computationally
calibrate, align and rearrange four-dimensional image data. Several attempts
have been proposed to enhance the overall image quality by tailoring pipelines
dedicated to particular plenoptic cameras and improving the consistency across
viewpoints at the expense of high computational loads. The framework presented
herein advances prior outcomes thanks to its novel micro image scale-space
analysis for generic camera calibration independent of the lens specifications
and its parallax-invariant, cost-effective viewpoint color equalization from
optimal transport theory. Artifacts from the sensor and micro lens grid are
compensated in an innovative way to enable superior quality in sub-aperture
image extraction, computational refocusing and Scheimpflug rendering with
sub-sampling capabilities. Benchmark comparisons using established image
metrics suggest that our proposed pipeline outperforms state-of-the-art tool
chains in the majority of cases. Results from a Wasserstein distance further
show that our color transfer outdoes the existing transport methods. Our
algorithms are released under an open-source license, offer cross-platform
compatibility with few dependencies and different user interfaces. This makes
the reproduction of results and experimentation with plenoptic camera
technology convenient for peer researchers, developers, photographers, data
scientists and others working in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hahne_C/0/1/0/all/0/1"&gt;Christopher Hahne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aggoun_A/0/1/0/all/0/1"&gt;Amar Aggoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11956</id>
        <link href="http://arxiv.org/abs/2107.11956"/>
        <updated>2021-07-27T02:03:33.704Z</updated>
        <summary type="html"><![CDATA[Automatically mining sentiment tendency contained in natural language is a
fundamental research to some artificial intelligent applications, where
solutions alternate with challenges. Transfer learning and multi-task learning
techniques have been leveraged to mitigate the supervision sparsity and
collaborate multiple heterogeneous domains correspondingly. Recent years, the
sensitive nature of users' private data raises another challenge for sentiment
classification, i.e., data privacy protection. In this paper, we resort to
federated learning for multiple domain sentiment classification under the
constraint that the corpora must be stored on decentralized devices. In view of
the heterogeneous semantics across multiple parties and the peculiarities of
word embedding, we pertinently provide corresponding solutions. First, we
propose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for
better model aggregation and personalization in federated sentiment
classification. Second, we propose KTEPS$^\star$ with the consideration of the
rich semantic and huge embedding size properties of word vectors, utilizing
Projection-based Dimension Reduction (PDR) methods for privacy protection and
efficient transmission simultaneously. We propose two federated sentiment
classification scenes based on public benchmarks, and verify the superiorities
of our proposed methods with abundant experimental investigations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin-Chun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shaoming Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARM: A Confidence-Based Adversarial Reweighting Module for Coarse Semantic Segmentation. (arXiv:2009.05205v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05205</id>
        <link href="http://arxiv.org/abs/2009.05205"/>
        <updated>2021-07-27T02:03:33.697Z</updated>
        <summary type="html"><![CDATA[Coarsely-labeled semantic segmentation annotations are easy to obtain, but
therefore bear the risk of losing edge details and introducing background
pixels. Impeded by the inherent noise, existing coarse annotations are only
taken as a bonus for model pre-training. In this paper, we try to exploit their
potentials with a confidence-based reweighting strategy. To expand, loss-based
reweighting strategies usually take the high loss value to identify two
completely different types of pixels, namely, valuable pixels in noise-free
annotations and mislabeled pixels in noisy annotations. This makes it
impossible to perform two tasks of mining valuable pixels and suppressing
mislabeled pixels at the same time. However, with the help of the prediction
confidence, we successfully solve this dilemma and simultaneously perform two
subtasks with a single reweighting strategy. Furthermore, we generalize this
strategy into an Adversarial Reweighting Module (ARM) and prove its convergence
strictly. Experiments on standard datasets shows our ARM can bring consistent
improvements for both coarse annotations and fine annotations. Specifically,
built on top of DeepLabv3+, ARM improves the mIoU on the coarsely-labeled
Cityscapes by a considerable margin and increases the mIoU on the ADE20K
dataset to 47.50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingchao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Ye Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zehua Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11697</id>
        <link href="http://arxiv.org/abs/2010.11697"/>
        <updated>2021-07-27T02:03:33.679Z</updated>
        <summary type="html"><![CDATA[Iconography in art is the discipline that studies the visual content of
artworks to determine their motifs and themes andto characterize the way these
are represented. It is a subject of active research for a variety of purposes,
including the interpretation of meaning, the investigation of the origin and
diffusion in time and space of representations, and the study of influences
across artists and art works. With the proliferation of digital archives of art
images, the possibility arises of applying Computer Vision techniques to the
analysis of art images at an unprecedented scale, which may support iconography
research and education. In this paper we introduce a novel paintings data set
for iconography classification and present the quantitativeand qualitative
results of applying a Convolutional Neural Network (CNN) classifier to the
recognition of the iconography of artworks. The proposed classifier achieves
good performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%
Average Precision) in the task of identifying saints in Christian religious
paintings, a task made difficult by the presence of classes with very similar
visual features. Qualitative analysis of the results shows that the CNN focuses
on the traditional iconic motifs that characterize the representation of each
saint and exploits such hints to attain correct identification. The ultimate
goal of our work is to enable the automatic extraction, decomposition, and
comparison of iconography elements to support iconographic studies and
automatic art work annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1"&gt;Federico Milani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraternali_P/0/1/0/all/0/1"&gt;Piero Fraternali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition. (arXiv:2107.12213v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12213</id>
        <link href="http://arxiv.org/abs/2107.12213"/>
        <updated>2021-07-27T02:03:33.672Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) have been widely used and achieved
remarkable results in skeleton-based action recognition. In GCNs, graph
topology dominates feature aggregation and therefore is the key to extracting
representative features. In this work, we propose a novel Channel-wise Topology
Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies
and effectively aggregate joint features in different channels for
skeleton-based action recognition. The proposed CTR-GC models channel-wise
topologies through learning a shared topology as a generic prior for all
channels and refining it with channel-specific correlations for each channel.
Our refinement method introduces few extra parameters and significantly reduces
the difficulty of modeling channel-wise topologies. Furthermore, via
reformulating graph convolutions into a unified form, we find that CTR-GC
relaxes strict constraints of graph convolutions, leading to stronger
representation capability. Combining CTR-GC with temporal modeling modules, we
develop a powerful graph convolutional network named CTR-GCN which notably
outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and
NW-UCLA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chunfeng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Ying Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does LIME really see in images?. (arXiv:2102.06307v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06307</id>
        <link href="http://arxiv.org/abs/2102.06307"/>
        <updated>2021-07-27T02:03:33.665Z</updated>
        <summary type="html"><![CDATA[The performance of modern algorithms on certain computer vision tasks such as
object recognition is now close to that of humans. This success was achieved at
the price of complicated architectures depending on millions of parameters and
it has become quite challenging to understand how particular predictions are
made. Interpretability methods propose to give us this understanding. In this
paper, we study LIME, perhaps one of the most popular. On the theoretical side,
we show that when the number of generated examples is large, LIME explanations
are concentrated around a limit explanation for which we give an explicit
expression. We further this study for elementary shape detectors and linear
models. As a consequence of this analysis, we uncover a connection between LIME
and integrated gradients, another explanation method. More precisely, the LIME
explanations are similar to the sum of integrated gradients over the
superpixels used in the preprocessing step of LIME.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1"&gt;Damien Garreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mardaoui_D/0/1/0/all/0/1"&gt;Dina Mardaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12220</id>
        <link href="http://arxiv.org/abs/2107.12220"/>
        <updated>2021-07-27T02:03:33.658Z</updated>
        <summary type="html"><![CDATA[When humans solve complex problems, they rarely come up with a decision
right-away. Instead, they start with an intuitive decision, reflect upon it,
spot mistakes, resolve contradictions and jump between different hypotheses.
Thus, they create a sequence of ideas and follow a train of thought that
ultimately reaches a conclusive decision. Contrary to this, today's neural
classification models are mostly trained to map an input to one single and
fixed output. In this paper, we investigate how we can give models the
opportunity of a second, third and $k$-th thought. We take inspiration from
Hegel's dialectics and propose a method that turns an existing classifier's
class prediction (such as the image class forest) into a sequence of
predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom).
Concretely, we propose a correction module that is trained to estimate the
model's correctness as well as an iterative prediction update based on the
prediction's gradient. Our approach results in a dynamic system over class
probability distributions $\unicode{x2014}$ the thought flow. We evaluate our
method on diverse datasets and tasks from computer vision and natural language
processing. We observe surprisingly complex but intuitive behavior and
demonstrate that our method (i) can correct misclassifications, (ii)
strengthens model performance, (iii) is robust to high levels of adversarial
attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift
setting and (iv) provides a tool for model interpretability that uncovers model
knowledge which otherwise remains invisible in a single distribution
prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1"&gt;Hendrik Schuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1"&gt;Heike Adel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1"&gt;Ngoc Thang Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets. (arXiv:2004.07999v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07999</id>
        <link href="http://arxiv.org/abs/2004.07999"/>
        <updated>2021-07-27T02:03:33.652Z</updated>
        <summary type="html"><![CDATA[Machine learning models are known to perpetuate and even amplify the biases
present in the data. However, these data biases frequently do not become
apparent until after the models are deployed. Our work tackles this issue and
enables the preemptive analysis of large-scale datasets. REVISE (REvealing
VIsual biaSEs) is a tool that assists in the investigation of a visual dataset,
surfacing potential biases along three dimensions: (1) object-based, (2)
person-based, and (3) geography-based. Object-based biases relate to the size,
context, or diversity of the depicted objects. Person-based metrics focus on
analyzing the portrayal of people within the dataset. Geography-based analyses
consider the representation of different geographic locations. These three
dimensions are deeply intertwined in how they interact to bias a dataset, and
REVISE sheds light on this; the responsibility then lies with the user to
consider the cultural and historical context, and to determine which of the
revealed biases may be problematic. The tool further assists the user by
suggesting actionable steps that may be taken to mitigate the revealed biases.
Overall, the key aim of our work is to tackle the machine learning bias problem
early in the pipeline. REVISE is available at
https://github.com/princetonvisualai/revise-tool]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Angelina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alexander Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ryan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleiman_A/0/1/0/all/0/1"&gt;Anat Kleiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1"&gt;Leslie Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dora Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirai_I/0/1/0/all/0/1"&gt;Iroha Shirai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Arvind Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dilated Residual Hierarchically Fashioned Segmentation Framework for Extracting Gleason Tissues and Grading Prostate Cancer from Whole Slide Images. (arXiv:2011.00527v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00527</id>
        <link href="http://arxiv.org/abs/2011.00527"/>
        <updated>2021-07-27T02:03:33.632Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is the second deadliest form of cancer in males, and it
can be clinically graded by examining the structural representations of Gleason
tissues. This paper proposes \RV{a new method} for segmenting the Gleason
tissues \RV{(patch-wise) in order to grade PCa from the whole slide images
(WSI).} Also, the proposed approach encompasses two main contributions: 1) A
synergy of hybrid dilation factors and hierarchical decomposition of latent
space representation for effective Gleason tissues extraction, and 2) A
three-tiered loss function which can penalize different semantic segmentation
models for accurately extracting the highly correlated patterns. In addition to
this, the proposed framework has been extensively evaluated on a large-scale
PCa dataset containing 10,516 whole slide scans (with around 71.7M patches),
where it outperforms state-of-the-art schemes by 3.22% (in terms of mean
intersection-over-union) for extracting the Gleason tissues and 6.91% (in terms
of F1 score) for grading the progression of PCa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1"&gt;Taimur Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1"&gt;Bilal Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Baz_A/0/1/0/all/0/1"&gt;Ayman El-Baz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1"&gt;Naoufel Werghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StainNet: a fast and robust stain normalization network. (arXiv:2012.12535v6 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12535</id>
        <link href="http://arxiv.org/abs/2012.12535"/>
        <updated>2021-07-27T02:03:33.626Z</updated>
        <summary type="html"><![CDATA[Stain normalization often refers to transferring the color distribution of
the source image to that of the target image and has been widely used in
biomedical image analysis. The conventional stain normalization is regarded as
constructing a pixel-by-pixel color mapping model, which only depends on one
reference image, and can not accurately achieve the style transformation
between image datasets. In principle, this style transformation can be well
solved by the deep learning-based methods due to its complicated network
structure, whereas, its complicated structure results in the low computational
efficiency and artifacts in the style transformation, which has restricted the
practical application. Here, we use distillation learning to reduce the
complexity of deep learning methods and a fast and robust network called
StainNet to learn the color mapping between the source image and target image.
StainNet can learn the color mapping relationship from a whole dataset and
adjust the color value in a pixel-to-pixel manner. The pixel-to-pixel manner
restricts the network size and avoids artifacts in the style transformation.
The results on the cytopathology and histopathology datasets show that StainNet
can achieve comparable performance to the deep learning-based methods.
Computation results demonstrate StainNet is more than 40 times faster than
StainGAN and can normalize a 100,000x100,000 whole slide image in 40 seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hongtao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_D/0/1/0/all/0/1"&gt;Die Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1"&gt;Weihua Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1"&gt;Junbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shaoqun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Quan_T/0/1/0/all/0/1"&gt;Tingwei Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiuli Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Large Scale Inlier Voting for Geometric Vision Problems. (arXiv:2107.11810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11810</id>
        <link href="http://arxiv.org/abs/2107.11810"/>
        <updated>2021-07-27T02:03:33.619Z</updated>
        <summary type="html"><![CDATA[Outlier rejection and equivalently inlier set optimization is a key
ingredient in numerous applications in computer vision such as filtering
point-matches in camera pose estimation or plane and normal estimation in point
clouds. Several approaches exist, yet at large scale we face a combinatorial
explosion of possible solutions and state-of-the-art methods like RANSAC, Hough
transform or Branch\&Bound require a minimum inlier ratio or prior knowledge to
remain practical. In fact, for problems such as camera posing in very large
scenes these approaches become useless as they have exponential runtime growth
if these conditions aren't met. To approach the problem we present a efficient
and general algorithm for outlier rejection based on "intersecting"
$k$-dimensional surfaces in $R^d$. We provide a recipe for casting a variety of
geometric problems as finding a point in $R^d$ which maximizes the number of
nearby surfaces (and thus inliers). The resulting algorithm has linear
worst-case complexity with a better runtime dependency in the approximation
factor than competing algorithms while not requiring domain specific bounds.
This is achieved by introducing a space decomposition scheme that bounds the
number of computations by successively rounding and grouping samples. Our
recipe (and open-source code) enables anybody to derive such fast approaches to
new problems across a wide range of domains. We demonstrate the versatility of
the approach on several camera posing problems with a high number of matches at
low inlier ratio achieving state-of-the-art results at significantly lower
processing times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aiger_D/0/1/0/all/0/1"&gt;Dror Aiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lynen_S/0/1/0/all/0/1"&gt;Simon Lynen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1"&gt;Jan Hosang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeisl_B/0/1/0/all/0/1"&gt;Bernhard Zeisl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Insect Pest Classification Using Multiple Convolutional Neural Network Based Models. (arXiv:2107.12189v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12189</id>
        <link href="http://arxiv.org/abs/2107.12189"/>
        <updated>2021-07-27T02:03:33.612Z</updated>
        <summary type="html"><![CDATA[Accurate insect pest recognition is significant to protect the crop or take
the early treatment on the infected yield, and it helps reduce the loss for the
agriculture economy. Design an automatic pest recognition system is necessary
because manual recognition is slow, time-consuming, and expensive. The
Image-based pest classifier using the traditional computer vision method is not
efficient due to the complexity. Insect pest classification is a difficult task
because of various kinds, scales, shapes, complex backgrounds in the field, and
high appearance similarity among insect species. With the rapid development of
deep learning technology, the CNN-based method is the best way to develop a
fast and accurate insect pest classifier. We present different convolutional
neural network-based models in this work, including attention, feature pyramid,
and fine-grained models. We evaluate our methods on two public datasets: the
large-scale insect pest dataset, the IP102 benchmark dataset, and a smaller
dataset, namely D0 in terms of the macro-average precision (MPre), the
macro-average recall (MRec), the macro-average F1- score (MF1), the accuracy
(Acc), and the geometric mean (GM). The experimental results show that
combining these convolutional neural network-based models can better perform
than the state-of-the-art methods on these two datasets. For instance, the
highest accuracy we obtained on IP102 and D0 is $74.13\%$ and $99.78\%$,
respectively, bypassing the corresponding state-of-the-art accuracy: $67.1\%$
(IP102) and $98.8\%$ (D0). We also publish our codes for contributing to the
current research related to the insect pest classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1"&gt;Hieu T. Ung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1"&gt;Huy Q. Ung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-Based Parking Space Occupancy Classification: Dataset and Baseline. (arXiv:2107.12207v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12207</id>
        <link href="http://arxiv.org/abs/2107.12207"/>
        <updated>2021-07-27T02:03:33.605Z</updated>
        <summary type="html"><![CDATA[We introduce a new dataset for image-based parking space occupancy
classification: ACPDS. Unlike in prior datasets, each image is taken from a
unique view, systematically annotated, and the parking lots in the train,
validation, and test sets are unique. We use this dataset to propose a simple
baseline model for parking space occupancy classification, which achieves 98%
accuracy on unseen parking lots, significantly outperforming existing models.
We share our dataset, code, and trained models under the MIT license.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marek_M/0/1/0/all/0/1"&gt;Martin Marek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Video Object Segmentation with Compressed Video. (arXiv:2107.12192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12192</id>
        <link href="http://arxiv.org/abs/2107.12192"/>
        <updated>2021-07-27T02:03:33.599Z</updated>
        <summary type="html"><![CDATA[We propose an efficient inference framework for semi-supervised video object
segmentation by exploiting the temporal redundancy of the video. Our method
performs inference on selected keyframes and makes predictions for other frames
via propagation based on motion vectors and residuals from the compressed video
bitstream. Specifically, we propose a new motion vector-based warping method
for propagating segmentation masks from keyframes to other frames in a
multi-reference manner. Additionally, we propose a residual-based refinement
module that can correct and add detail to the block-wise propagated
segmentation masks. Our approach is flexible and can be added on top of
existing video object segmentation algorithms. With STM with top-k filtering as
our base model, we achieved highly competitive results on DAVIS16 and
YouTube-VOS with substantial speedups of up to 4.9X with little loss in
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Diagnosis of Lung Cancer Using Computer Aided Detection via Lung Segmentation Approach. (arXiv:2107.12205v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12205</id>
        <link href="http://arxiv.org/abs/2107.12205"/>
        <updated>2021-07-27T02:03:33.582Z</updated>
        <summary type="html"><![CDATA[Lung cancer begins in the lungs and leading to the reason of cancer demise
amid population in the creation. According to the American Cancer Society,
which estimates about 27% of the deaths because of cancer. In the early phase
of its evolution, lung cancer does not cause any symptoms usually. Many of the
patients have been diagnosed in a developed phase where symptoms become more
prominent, that results in poor curative treatment and high mortality rate.
Computer Aided Detection systems are used to achieve greater accuracies for the
lung cancer diagnosis. In this research exertion, we proposed a novel
methodology for lung Segmentation on the basis of Fuzzy C-Means Clustering,
Adaptive Thresholding, and Segmentation of Active Contour Model. The
experimental results are analysed and presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhandary_A/0/1/0/all/0/1"&gt;Abhir Bhandary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+G_A/0/1/0/all/0/1"&gt;Ananth Prabhu G&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Basthikodi_M/0/1/0/all/0/1"&gt;Mustafa Basthikodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+M_C/0/1/0/all/0/1"&gt;Chaitra K M&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multiple-Instance Learning Approach for the Assessment of Gallbladder Vascularity from Laparoscopic Images. (arXiv:2107.12093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12093</id>
        <link href="http://arxiv.org/abs/2107.12093"/>
        <updated>2021-07-27T02:03:33.573Z</updated>
        <summary type="html"><![CDATA[An important task at the onset of a laparoscopic cholecystectomy (LC)
operation is the inspection of gallbladder (GB) to evaluate the thickness of
its wall, presence of inflammation and extent of fat. Difficulty in
visualization of the GB wall vessels may be due to the previous factors,
potentially as a result of chronic inflammation or other diseases. In this
paper we propose a multiple-instance learning (MIL) technique for assessment of
the GB wall vascularity via computer-vision analysis of images from LC
operations. The bags correspond to a labeled (low vs. high) vascularity dataset
of 181 GB images, from 53 operations. The instances correspond to unlabeled
patches extracted from these images. Each patch is represented by a vector with
color, texture and statistical features. We compare various state-of-the-art
MIL and single-instance learning approaches, as well as a proposed MIL
technique based on variational Bayesian inference. The methods were compared
for two experimental tasks: image-based and video-based (i.e. patient-based)
classification. The proposed approach presents the best performance with
accuracy 92.1% and 90.3% for the first and second task, respectively. A
significant advantage of the proposed technique is that it does not require the
time-consuming task of manual labelling the instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_C/0/1/0/all/0/1"&gt;C. Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gazis_A/0/1/0/all/0/1"&gt;A. Gazis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schizas_D/0/1/0/all/0/1"&gt;D. Schizas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Unbiased Visual Emotion Recognition via Causal Intervention. (arXiv:2107.12096v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12096</id>
        <link href="http://arxiv.org/abs/2107.12096"/>
        <updated>2021-07-27T02:03:33.566Z</updated>
        <summary type="html"><![CDATA[Although much progress has been made in visual emotion recognition,
researchers have realized that modern deep networks tend to exploit dataset
characteristics to learn spurious statistical associations between the input
and the target. Such dataset characteristics are usually treated as dataset
bias, which damages the robustness and generalization performance of these
recognition systems. In this work, we scrutinize this problem from the
perspective of causal inference, where such dataset characteristic is termed as
a confounder which misleads the system to learn the spurious correlation. To
alleviate the negative effects brought by the dataset bias, we propose a novel
Interventional Emotion Recognition Network (IERN) to achieve the backdoor
adjustment, which is one fundamental deconfounding technique in causal
inference. A series of designed tests validate the effectiveness of IERN, and
experiments on three emotion benchmarks demonstrate that IERN outperforms other
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuedong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1"&gt;Tat-Jen Cham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition. (arXiv:2107.11412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11412</id>
        <link href="http://arxiv.org/abs/2107.11412"/>
        <updated>2021-07-27T02:03:33.549Z</updated>
        <summary type="html"><![CDATA[The recent developments in technology have re-warded us with amazing audio
synthesis models like TACOTRON and WAVENETS. On the other side, it poses
greater threats such as speech clones and deep fakes, that may go undetected.
To tackle these alarming situations, there is an urgent need to propose models
that can help discriminate a synthesized speech from an actual human speech and
also identify the source of such a synthesis. Here, we propose a model based on
Convolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network
(BiRNN) that helps to achieve both the aforementioned objectives. The temporal
dependencies present in AI synthesized speech are exploited using Bidirectional
RNN and CNN. The model outperforms the state-of-the-art approaches by
classifying the AI synthesized audio from real human speech with an error rate
of 1.9% and detecting the underlying architecture with an accuracy of 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Arun Kumar Singh&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Priyanka Singh&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Nathwani_K/0/1/0/all/0/1"&gt;Karan Nathwani&lt;/a&gt; (1) ((1) Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of Information and Communication Technology)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually Validated Precise Local Editing for Facial Action Units with StyleGAN. (arXiv:2107.12143v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12143</id>
        <link href="http://arxiv.org/abs/2107.12143"/>
        <updated>2021-07-27T02:03:33.542Z</updated>
        <summary type="html"><![CDATA[The ability to edit facial expressions has a wide range of applications in
computer graphics. The ideal facial expression editing algorithm needs to
satisfy two important criteria. First, it should allow precise and targeted
editing of individual facial actions. Second, it should generate high fidelity
outputs without artifacts. We build a solution based on StyleGAN, which has
been used extensively for semantic manipulation of faces. As we do so, we add
to our understanding of how various semantic attributes are encoded in
StyleGAN. In particular, we show that a naive strategy to perform editing in
the latent space results in undesired coupling between certain action units,
even if they are conceptually distinct. For example, although brow lowerer and
lip tightener are distinct action units, they appear correlated in the training
data. Hence, StyleGAN has difficulty in disentangling them. We allow
disentangled editing of such action units by computing detached regions of
influence for each action unit, and restrict editing to these regions. We
validate the effectiveness of our local editing method through perception
experiments conducted with 23 subjects. The results show that our method
provides higher control over local editing and produces images with superior
fidelity compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zindancioglu_A/0/1/0/all/0/1"&gt;Alara Zindanc&amp;#x131;o&amp;#x11f;lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sezgin_T/0/1/0/all/0/1"&gt;T. Metin Sezgin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12137</id>
        <link href="http://arxiv.org/abs/2107.12137"/>
        <updated>2021-07-27T02:03:33.536Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects using
point cloud data. We present anchor design along with custom loss functions
used in this work. A combination of spatial and channel wise attention module
is used in this work. We use the Kitti 3D Birds Eye View dataset for
benchmarking and validating our results. Our method surpasses previous state of
the art in this domain both in terms of average precision and speed running at
> 30 FPS. Finally, we present the ablation study to demonstrate that the
performance of our network is generalizable. This makes it a feasible option to
be deployed in real time applications like self driving cars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Device Scheduling and Update Aggregation Policies for Asynchronous Federated Learning. (arXiv:2107.11415v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11415</id>
        <link href="http://arxiv.org/abs/2107.11415"/>
        <updated>2021-07-27T02:03:33.455Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a newly emerged decentralized machine learning
(ML) framework that combines on-device local training with server-based model
synchronization to train a centralized ML model over distributed nodes. In this
paper, we propose an asynchronous FL framework with periodic aggregation to
eliminate the straggler issue in FL systems. For the proposed model, we
investigate several device scheduling and update aggregation policies and
compare their performances when the devices have heterogeneous computation
capabilities and training data distributions. From the simulation results, we
conclude that the scheduling and aggregation design for asynchronous FL can be
rather different from the synchronous case. For example, a norm-based
significance-aware scheduling policy might not be efficient in an asynchronous
FL setting, and an appropriate "age-aware" weighting design for the model
aggregation can greatly improve the learning performance of such systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chung-Hsuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1"&gt;Erik G. Larsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework. (arXiv:2107.12046v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.12046</id>
        <link href="http://arxiv.org/abs/2107.12046"/>
        <updated>2021-07-27T02:03:33.448Z</updated>
        <summary type="html"><![CDATA[Background: Glioma is the most common brain malignant tumor, with a high
morbidity rate and a mortality rate of more than three percent, which seriously
endangers human health. The main method of acquiring brain tumors in the clinic
is MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is
helpful for treatment inspection, post-diagnosis monitoring, and effect
evaluation of patients. However, the common operation in clinical brain tumor
segmentation is still manual segmentation, lead to its time-consuming and large
performance difference between different operators, a consistent and accurate
automatic segmentation method is urgently needed. Methods: To meet the above
challenges, we propose an automatic brain tumor MRI data segmentation framework
which is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is
added to each encoder, the Attention Guide Filter (AG) module is added to each
decoder, using the channel relationship to automatically enhance the useful
information in the channel to suppress the useless information, and use the
attention mechanism to guide the edge information and remove the influence of
irrelevant information such as noise. Results: We used the BraTS2020 challenge
online verification tool to evaluate our approach. The focus of verification is
that the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced
tumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI
images have different intensities, AGSE-VNet is not affected by the size of the
tumor, and can more accurately extract the features of the three regions, it
has achieved impressive results and made outstanding contributions to the
clinical diagnosis and treatment of brain tumor patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1"&gt;Xi Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jianming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiji Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaomei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1"&gt;Xiaobo Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HANet: Hierarchical Alignment Networks for Video-Text Retrieval. (arXiv:2107.12059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12059</id>
        <link href="http://arxiv.org/abs/2107.12059"/>
        <updated>2021-07-27T02:03:33.440Z</updated>
        <summary type="html"><![CDATA[Video-text retrieval is an important yet challenging task in vision-language
understanding, which aims to learn a joint embedding space where related video
and text instances are close to each other. Most current works simply measure
the video-text similarity based on video-level and text-level embeddings.
However, the neglect of more fine-grained or local information causes the
problem of insufficient representation. Some works exploit the local details by
disentangling sentences, but overlook the corresponding videos, causing the
asymmetry of video-text representation. To address the above limitations, we
propose a Hierarchical Alignment Network (HANet) to align different level
representations for video-text matching. Specifically, we first decompose video
and text into three semantic levels, namely event (video and text), action
(motion and verb), and entity (appearance and noun). Based on these, we
naturally construct hierarchical representations in the individual-local-global
manner, where the individual level focuses on the alignment between frame and
word, local level focuses on the alignment between video clip and textual
context, and global level focuses on the alignment between the whole video and
text. Different level alignments capture fine-to-coarse correlations between
video and text, as well as take the advantage of the complementary information
among three semantic levels. Besides, our HANet is also richly interpretable by
explicitly learning key semantic concepts. Extensive experiments on two public
datasets, namely MSR-VTT and VATEX, show the proposed HANet outperforms other
state-of-the-art methods, which demonstrates the effectiveness of hierarchical
representation and alignment. Our code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangteng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yiliang Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Remains of Visual Semantic Embeddings. (arXiv:2107.11991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11991</id>
        <link href="http://arxiv.org/abs/2107.11991"/>
        <updated>2021-07-27T02:03:33.433Z</updated>
        <summary type="html"><![CDATA[Zero shot learning (ZSL) has seen a surge in interest over the decade for its
tight links with the mechanism making young children recognize novel objects.
Although different paradigms of visual semantic embedding models are designed
to align visual features and distributed word representations, it is unclear to
what extent current ZSL models encode semantic information from distributed
word representations. In this work, we introduce the split of tiered-ImageNet
to the ZSL task, in order to avoid the structural flaws in the standard
ImageNet benchmark. We build a unified framework for ZSL with contrastive
learning as pre-training, which guarantees no semantic information leakage and
encourages linearly separable visual features. Our work makes it fair for
evaluating visual semantic embedding models on a ZSL setting in which semantic
inference is decisive. With this framework, we show that current ZSL models
struggle with encoding semantic relationships from word analogy and word
hierarchy. Our analyses provide motivation for exploring the role of context
language representations in ZSL tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yue Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1"&gt;Adam Pr&amp;#xfc;gel-Bennett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Generative Video Compression. (arXiv:2107.12038v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12038</id>
        <link href="http://arxiv.org/abs/2107.12038"/>
        <updated>2021-07-27T02:03:33.425Z</updated>
        <summary type="html"><![CDATA[We present a neural video compression method based on generative adversarial
networks (GANs) that outperforms previous neural video compression methods and
is comparable to HEVC in a user study. We propose a technique to mitigate
temporal error accumulation caused by recursive frame compression that uses
randomized shifting and un-shifting, motivated by a spectral analysis. We
present in detail the network design choices, their relative importance, and
elaborate on the challenges of evaluating video compression methods in user
studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mentzer_F/0/1/0/all/0/1"&gt;Fabian Mentzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agustsson_E/0/1/0/all/0/1"&gt;Eirikur Agustsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Balle_J/0/1/0/all/0/1"&gt;Johannes Ball&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minnen_D/0/1/0/all/0/1"&gt;David Minnen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Johnston_N/0/1/0/all/0/1"&gt;Nick Johnston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toderici_G/0/1/0/all/0/1"&gt;George Toderici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Realistic Simulation Framework for Learning with Label Noise. (arXiv:2107.11413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11413</id>
        <link href="http://arxiv.org/abs/2107.11413"/>
        <updated>2021-07-27T02:03:33.417Z</updated>
        <summary type="html"><![CDATA[We propose a simulation framework for generating realistic instance-dependent
noisy labels via a pseudo-labeling paradigm. We show that this framework
generates synthetic noisy labels that exhibit important characteristics of the
label noise in practical settings via comparison with the CIFAR10-H dataset.
Equipped with controllable label noise, we study the negative impact of noisy
labels across a few realistic settings to understand when label noise is more
problematic. We also benchmark several existing algorithms for learning with
noisy labels and compare their behavior on our synthetic datasets and on the
datasets with independent random label noise. Additionally, with the
availability of annotator information from our simulation framework, we propose
a new technique, Label Quality Model (LQM), that leverages annotator features
to predict and correct against noisy labels. We show that by adding LQM as a
label correction step before applying existing noisy label techniques, we can
further improve the models' performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1"&gt;Keren Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masotto_X/0/1/0/all/0/1"&gt;Xander Masotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachani_V/0/1/0/all/0/1"&gt;Vandana Bachani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikodem_J/0/1/0/all/0/1"&gt;Jack Nikodem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dong Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TargetNet: Functional microRNA Target Prediction with Deep Neural Networks. (arXiv:2107.11381v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2107.11381</id>
        <link href="http://arxiv.org/abs/2107.11381"/>
        <updated>2021-07-27T02:03:33.388Z</updated>
        <summary type="html"><![CDATA[MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by
binding to target sites of messenger RNAs (mRNAs). While identifying functional
targets of miRNAs is of utmost importance, their prediction remains a great
challenge. Previous computational algorithms have major limitations. They use
conservative candidate target site (CTS) selection criteria mainly focusing on
canonical site types, rely on laborious and time-consuming manual feature
extraction, and do not fully capitalize on the information underlying miRNA-CTS
interactions. In this paper, we introduce TargetNet, a novel deep
learning-based algorithm for functional miRNA target prediction. To address the
limitations of previous approaches, TargetNet has three key components: (1)
relaxed CTS selection criteria accommodating irregularities in the seed region,
(2) a novel miRNA-CTS sequence encoding scheme incorporating extended seed
region alignments, and (3) a deep residual network-based prediction model. The
proposed model was trained with miRNA-CTS pair datasets and evaluated with
miRNA-mRNA pair datasets. TargetNet advances the previous state-of-the-art
algorithms used in functional miRNA target classification. Furthermore, it
demonstrates great potential for distinguishing high-functional miRNA targets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Min_S/0/1/0/all/0/1"&gt;Seonwoo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lee_B/0/1/0/all/0/1"&gt;Byunghan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Log-Polar Space Convolution for Convolutional Neural Networks. (arXiv:2107.11943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11943</id>
        <link href="http://arxiv.org/abs/2107.11943"/>
        <updated>2021-07-27T02:03:33.372Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks use regular quadrilateral convolution kernels
to extract features. Since the number of parameters increases quadratically
with the size of the convolution kernel, many popular models use small
convolution kernels, resulting in small local receptive fields in lower layers.
This paper proposes a novel log-polar space convolution (LPSC) method, where
the convolution kernel is elliptical and adaptively divides its local receptive
field into different regions according to the relative directions and
logarithmic distances. The local receptive field grows exponentially with the
number of distance levels. Therefore, the proposed LPSC not only naturally
encodes local spatial structures, but also greatly increases the single-layer
receptive field while maintaining the number of parameters. We show that LPSC
can be implemented with conventional convolution via log-polar space pooling
and can be applied in any network architecture to replace conventional
convolutions. Experiments on different tasks and datasets demonstrate the
effectiveness of the proposed LPSC. Code is available at
https://github.com/BingSu12/Log-Polar-Space-Convolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1"&gt;Bing Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data. (arXiv:2107.11978v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11978</id>
        <link href="http://arxiv.org/abs/2107.11978"/>
        <updated>2021-07-27T02:03:33.355Z</updated>
        <summary type="html"><![CDATA[A recent study finds that existing few-shot learning methods, trained on the
source domain, fail to generalize to the novel target domain when a domain gap
is observed. This motivates the task of Cross-Domain Few-Shot Learning
(CD-FSL). In this paper, we realize that the labeled target data in CD-FSL has
not been leveraged in any way to help the learning process. Thus, we advocate
utilizing few labeled target data to guide the model learning. Technically, a
novel meta-FDMixup network is proposed. We tackle this problem mainly from two
aspects. Firstly, to utilize the source and the newly introduced target data of
two different class sets, a mixup module is re-proposed and integrated into the
meta-learning mechanism. Secondly, a novel disentangle module together with a
domain classifier is proposed to extract the disentangled domain-irrelevant and
domain-specific features. These two modules together enable our model to narrow
the domain gap thus generalizing well to the target datasets. Additionally, a
detailed feasibility and pilot study is conducted to reflect the intuitive
understanding of CD-FSL under our new setting. Experimental results show the
effectiveness of our new setting and the proposed method. Codes and models are
available at https://github.com/lovelyqian/Meta-FDMixup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu-Gang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards the Unseen: Iterative Text Recognition by Distilling from Errors. (arXiv:2107.12081v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12081</id>
        <link href="http://arxiv.org/abs/2107.12081"/>
        <updated>2021-07-27T02:03:33.335Z</updated>
        <summary type="html"><![CDATA[Visual text recognition is undoubtedly one of the most extensively researched
topics in computer vision. Great progress have been made to date, with the
latest models starting to focus on the more practical "in-the-wild" setting.
However, a salient problem still hinders practical deployment -- prior arts
mostly struggle with recognising unseen (or rarely seen) character sequences.
In this paper, we put forward a novel framework to specifically tackle this
"unseen" problem. Our framework is iterative in nature, in that it utilises
predicted knowledge of character sequences from a previous iteration, to
augment the main network in improving the next prediction. Key to our success
is a unique cross-modal variational autoencoder to act as a feedback module,
which is trained with the presence of textual error distribution data. This
module importantly translate a discrete predicted character space, to a
continuous affine transformation parameter space used to condition the visual
feature map at next iteration. Experiments on common datasets have shown
competitive performance over state-of-the-arts under the conventional setting.
Most importantly, under the new disjoint setup where train-test labels are
mutually exclusive, ours offers the best performance thus showcasing the
capability of generalising onto unseen words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1"&gt;Ayan Kumar Bhunia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1"&gt;Pinaki Nath Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1"&gt;Aneeshan Sain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11400</id>
        <link href="http://arxiv.org/abs/2107.11400"/>
        <updated>2021-07-27T02:03:33.329Z</updated>
        <summary type="html"><![CDATA[With the rise of deep neural networks, the challenge of explaining the
predictions of these networks has become increasingly recognized. While many
methods for explaining the decisions of deep neural networks exist, there is
currently no consensus on how to evaluate them. On the other hand, robustness
is a popular topic for deep learning research; however, it is hardly talked
about in explainability until very recently. In this tutorial paper, we start
by presenting gradient-based interpretability methods. These techniques use
gradient signals to assign the burden of the decision on the input features.
Later, we discuss how gradient-based methods can be evaluated for their
robustness and the role that adversarial robustness plays in having meaningful
explanations. We also discuss the limitations of gradient-based methods.
Finally, we present the best practices and attributes that should be examined
before choosing an explainability method. We conclude with the future
directions for research in the area at the convergence of robustness and
explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1"&gt;Ian E. Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1"&gt;Ghulam Rasool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1"&gt;Dimah Dera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1"&gt;Nidhal Bouaynaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1"&gt;Ravi P. Ramachandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective. (arXiv:2107.11882v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11882</id>
        <link href="http://arxiv.org/abs/2107.11882"/>
        <updated>2021-07-27T02:03:33.322Z</updated>
        <summary type="html"><![CDATA[Data from multi-modality provide complementary information in clinical
prediction, but missing data in clinical cohorts limits the number of subjects
in multi-modal learning context. Multi-modal missing imputation is challenging
with existing methods when 1) the missing data span across heterogeneous
modalities (e.g., image vs. non-image); or 2) one modality is largely missing.
In this paper, we address imputation of missing data by modeling the joint
distribution of multi-modal data. Motivated by partial bidirectional generative
adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method
that imputes one modality combining the conditional knowledge from another
modality. Specifically, C-PBiGAN introduces a conditional latent space in a
missing imputation framework that jointly encodes the available multi-modal
data, along with a class regularization loss on imputed data to recover
discriminative information. To our knowledge, it is the first generative
adversarial model that addresses multi-modal missing imputation by modeling the
joint distribution of image and non-image data. We validate our model with both
the national lung screening trial (NLST) dataset and an external clinical
validation cohort. The proposed C-PBiGAN achieves significant improvements in
lung cancer risk estimation compared with representative imputation methods
(e.g., AUC values increase in both NLST (+2.9\%) and in-house dataset (+4.3\%)
compared with PBiGAN, p$<$0.05).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1"&gt;Riqiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaiwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deppen_S/0/1/0/all/0/1"&gt;Steve Deppen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1"&gt;Kim Sandler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Massion_P/0/1/0/all/0/1"&gt;Pierre Massion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1"&gt;Thomas A. Lasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Entity-aware Image Captioning with Multi-modal Knowledge Graph. (arXiv:2107.11970v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11970</id>
        <link href="http://arxiv.org/abs/2107.11970"/>
        <updated>2021-07-27T02:03:33.315Z</updated>
        <summary type="html"><![CDATA[Entity-aware image captioning aims to describe named entities and events
related to the image by utilizing the background knowledge in the associated
article. This task remains challenging as it is difficult to learn the
association between named entities and visual cues due to the long-tail
distribution of named entities. Furthermore, the complexity of the article
brings difficulty in extracting fine-grained relationships between entities to
generate informative event descriptions about the image. To tackle these
challenges, we propose a novel approach that constructs a multi-modal knowledge
graph to associate the visual objects with named entities and capture the
relationship between entities simultaneously with the help of external
knowledge collected from the web. Specifically, we build a text sub-graph by
extracting named entities and their relationships from the article, and build
an image sub-graph by detecting the objects in the image. To connect these two
sub-graphs, we propose a cross-modal entity matching module trained using a
knowledge base that contains Wikipedia entries and the corresponding images.
Finally, the multi-modal knowledge graph is integrated into the captioning
model via a graph attention mechanism. Extensive experiments on both GoodNews
and NYTimes800k datasets demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wentian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Heda Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinxiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Synthetic Corruptions to Measure Robustness to Natural Distribution Shifts. (arXiv:2107.12052v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12052</id>
        <link href="http://arxiv.org/abs/2107.12052"/>
        <updated>2021-07-27T02:03:33.308Z</updated>
        <summary type="html"><![CDATA[Synthetic corruptions gathered into a benchmark are frequently used to
measure neural network robustness to distribution shifts. However, robustness
to synthetic corruption benchmarks is not always predictive of robustness to
distribution shifts encountered in real-world applications. In this paper, we
propose a methodology to build synthetic corruption benchmarks that make
robustness estimations more correlated with robustness to real-world
distribution shifts. Using the overlapping criterion, we split synthetic
corruptions into categories that help to better understand neural network
robustness. Based on these categories, we identify three parameters that are
relevant to take into account when constructing a corruption benchmark: number
of represented categories, balance among categories and size of benchmarks.
Applying the proposed methodology, we build a new benchmark called
ImageNet-Syn2Nat to predict image classifier robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1"&gt;Alfred Laugros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1"&gt;Alice Caplier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1"&gt;Matthieu Ospici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CP-loss: Connectivity-preserving Loss for Road Curb Detection in Autonomous Driving with Aerial Images. (arXiv:2107.11920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11920</id>
        <link href="http://arxiv.org/abs/2107.11920"/>
        <updated>2021-07-27T02:03:33.290Z</updated>
        <summary type="html"><![CDATA[Road curb detection is important for autonomous driving. It can be used to
determine road boundaries to constrain vehicles on roads, so that potential
accidents could be avoided. Most of the current methods detect road curbs
online using vehicle-mounted sensors, such as cameras or 3-D Lidars. However,
these methods usually suffer from severe occlusion issues. Especially in
highly-dynamic traffic environments, most of the field of view is occupied by
dynamic objects. To alleviate this issue, we detect road curbs offline using
high-resolution aerial images in this paper. Moreover, the detected road curbs
can be used to create high-definition (HD) maps for autonomous vehicles.
Specifically, we first predict the pixel-wise segmentation map of road curbs,
and then conduct a series of post-processing steps to extract the graph
structure of road curbs. To tackle the disconnectivity issue in the
segmentation maps, we propose an innovative connectivity-preserving loss
(CP-loss) to improve the segmentation performance. The experimental results on
a public dataset demonstrate the effectiveness of our proposed loss function.
This paper is accompanied with a demonstration video and a supplementary
document, which are available at
\texttt{\url{https://sites.google.com/view/cp-loss}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhenhua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lujia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11975</id>
        <link href="http://arxiv.org/abs/2107.11975"/>
        <updated>2021-07-27T02:03:33.284Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to train a classifier that can generalize well when
just a small number of labeled samples per class are given. We introduce
Transductive Maximum Margin Classifier (TMMC) for few-shot learning. The basic
idea of the classical maximum margin classifier is to solve an optimal
prediction function that the corresponding separating hyperplane can correctly
divide the training data and the resulting classifier has the largest geometric
margin. In few-shot learning scenarios, the training samples are scarce, not
enough to find a separating hyperplane with good generalization ability on
unseen data. TMMC is constructed using a mixture of the labeled support set and
the unlabeled query set in a given task. The unlabeled samples in the query set
can adjust the separating hyperplane so that the prediction function is optimal
on both the labeled and unlabeled samples. Furthermore, we leverage an
efficient and effective quasi-Newton algorithm, the L-BFGS method to optimize
TMMC. Experimental results on three standard few-shot learning benchmarks
including miniImagenet, tieredImagenet and CUB suggest that our TMMC achieves
state-of-the-art accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunlei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanwen Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation. (arXiv:2107.11945v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11945</id>
        <link href="http://arxiv.org/abs/2107.11945"/>
        <updated>2021-07-27T02:03:33.275Z</updated>
        <summary type="html"><![CDATA[Cross-contrast image translation is an important task for completing missing
contrasts in clinical diagnosis. However, most existing methods learn separate
translator for each pair of contrasts, which is inefficient due to many
possible contrast pairs in real scenarios. In this work, we propose a unified
Hyper-GAN model for effectively and efficiently translating between different
contrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder
to first map from the source contrast to a common feature space, and then
further map to the target contrast image. To facilitate the translation between
different contrast pairs, contrast-modulators are designed to tune the
hyper-encoder and hyper-decoder adaptive to different contrasts. We also design
a common space loss to enforce that multi-contrast images of a subject share a
common feature space, implicitly modeling the shared underlying anatomical
structures. Experiments on two datasets of IXI and BraTS 2019 show that our
Hyper-GAN achieves state-of-the-art results in both accuracy and efficiency,
e.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less
than half the amount of parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1"&gt;Heran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zongben Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benign Adversarial Attack: Tricking Algorithm for Goodness. (arXiv:2107.11986v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.11986</id>
        <link href="http://arxiv.org/abs/2107.11986"/>
        <updated>2021-07-27T02:03:33.268Z</updated>
        <summary type="html"><![CDATA[In spite of the successful application in many fields, machine learning
algorithms today suffer from notorious problems like vulnerability to
adversarial examples. Beyond falling into the cat-and-mouse game between
adversarial attack and defense, this paper provides alternative perspective to
consider adversarial example and explore whether we can exploit it in benign
applications. We first propose a novel taxonomy of visual information along
task-relevance and semantic-orientation. The emergence of adversarial example
is attributed to algorithm's utilization of task-relevant non-semantic
information. While largely ignored in classical machine learning mechanisms,
task-relevant non-semantic information enjoys three interesting characteristics
as (1) exclusive to algorithm, (2) reflecting common weakness, and (3)
utilizable as features. Inspired by this, we present brave new idea called
benign adversarial attack to exploit adversarial examples for goodness in three
directions: (1) adversarial Turing test, (2) rejecting malicious algorithm, and
(3) adversarial data augmentation. Each direction is positioned with motivation
elaboration, justification analysis and prototype applications to showcase its
potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhiyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1"&gt;Jitao Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12085</id>
        <link href="http://arxiv.org/abs/2107.12085"/>
        <updated>2021-07-27T02:03:33.261Z</updated>
        <summary type="html"><![CDATA[Motion blur caused by the moving of the object or camera during the exposure
can be a key challenge for visual object tracking, affecting tracking accuracy
significantly. In this work, we explore the robustness of visual object
trackers against motion blur from a new angle, i.e., adversarial blur attack
(ABA). Our main objective is to online transfer input frames to their natural
motion-blurred counterparts while misleading the state-of-the-art trackers
during the tracking process. To this end, we first design the motion blur
synthesizing method for visual tracking based on the generation principle of
motion blur, considering the motion information and the light accumulation
process. With this synthetic method, we propose \textit{optimization-based ABA
(OP-ABA)} by iteratively optimizing an adversarial objective function against
the tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is
able to produce natural adversarial examples but the iteration can cause heavy
time cost, making it unsuitable for attacking real-time trackers. To alleviate
this issue, we further propose \textit{one-step ABA (OS-ABA)} where we design
and train a joint adversarial motion and accumulation predictive network
(JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate
the adversarial motion and accumulation parameters in a one-step way. The
experiments on four popular datasets (\eg, OTB100, VOT2018, UAV123, and LaSOT)
demonstrate that our methods are able to cause significant accuracy drops on
four state-of-the-art trackers with high transferability. Please find the
source code at https://github.com/tsingqguo/ABA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Ziyi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jianjun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition. (arXiv:2107.12090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12090</id>
        <link href="http://arxiv.org/abs/2107.12090"/>
        <updated>2021-07-27T02:03:33.242Z</updated>
        <summary type="html"><![CDATA[Although text recognition has significantly evolved over the years,
state-of-the-art (SOTA) models still struggle in the wild scenarios due to
complex backgrounds, varying fonts, uncontrolled illuminations, distortions and
other artefacts. This is because such models solely depend on visual
information for text recognition, thus lacking semantic reasoning capabilities.
In this paper, we argue that semantic information offers a complementary role
in addition to visual only. More specifically, we additionally utilize semantic
information by proposing a multi-stage multi-scale attentional decoder that
performs joint visual-semantic reasoning. Our novelty lies in the intuition
that for text recognition, the prediction should be refined in a stage-wise
manner. Therefore our key contribution is in designing a stage-wise unrolling
attentional decoder where non-differentiability, invoked by discretely
predicted character labels, needs to be bypassed for end-to-end training. While
the first stage predicts using visual features, subsequent stages refine on top
of it using joint visual-semantic information. Additionally, we introduce
multi-scale 2D attention along with dense and residual connections between
different stages to deal with varying scales of character sizes, for better
performance and faster convergence during training. Experimental results show
our approach to outperform existing SOTA methods by a considerable margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1"&gt;Ayan Kumar Bhunia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1"&gt;Aneeshan Sain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Amandeep Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Shuvozit Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1"&gt;Pinaki Nath Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmentation Pathways Network for Visual Recognition. (arXiv:2107.11990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11990</id>
        <link href="http://arxiv.org/abs/2107.11990"/>
        <updated>2021-07-27T02:03:33.236Z</updated>
        <summary type="html"><![CDATA[Data augmentation is practically helpful for visual recognition, especially
at the time of data scarcity. However, such success is only limited to quite a
few light augmentations (e.g., random crop, flip). Heavy augmentations (e.g.,
gray, grid shuffle) are either unstable or show adverse effects during
training, owing to the big gap between the original and augmented images. This
paper introduces a novel network design, noted as Augmentation Pathways (AP),
to systematically stabilize training on a much wider range of augmentation
policies. Notably, AP tames heavy data augmentations and stably boosts
performance without a careful selection among augmentation policies. Unlike
traditional single pathway, augmented images are processed in different neural
paths. The main pathway handles light augmentations, while other pathways focus
on heavy augmentations. By interacting with multiple paths in a dependent
manner, the backbone network robustly learns from shared visual patterns among
augmentations, and suppresses noisy patterns at the same time. Furthermore, we
extend AP to a homogeneous version and a heterogeneous version for high-order
scenarios, demonstrating its robustness and flexibility in practical usage.
Experimental results on ImageNet benchmarks demonstrate the compatibility and
effectiveness on a much wider range of augmentations (e.g., Crop, Gray, Grid
Shuffle, RandAugment), while consuming fewer parameters and lower computational
costs at inference time. Source code:https://github.com/ap-conv/ap-net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yalong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mohan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans. (arXiv:2107.12009v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12009</id>
        <link href="http://arxiv.org/abs/2107.12009"/>
        <updated>2021-07-27T02:03:33.229Z</updated>
        <summary type="html"><![CDATA[Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood
clots. PE accounts for approximately 100,000 deaths per year in the United
States alone. The clinical presentation of PE is often nonspecific, making the
diagnosis challenging. Thus, rapid and accurate risk stratification is of
paramount importance. High-risk PE is caused by right ventricular (RV)
dysfunction from acute pressure overload, which in return can help identify
which patients require more aggressive therapy. Reconstructed four-chamber
views of the heart on chest CT can detect right ventricular enlargement. CT
pulmonary angiography (CTPA) is the golden standard in the diagnostic workup of
suspected PE. Therefore, it can link between diagnosis and risk stratification
strategies. We developed a weakly supervised deep learning algorithm, with an
emphasis on a novel attention mechanism, to automatically classify RV strain on
CTPA. Our method is a 3D DenseNet model with integrated 3D residual attention
blocks. We evaluated our model on a dataset of CTPAs of emergency department
(ED) PE patients. This model achieved an area under the receiver operating
characteristic curve (AUC) of 0.88 for classifying RV strain. The model showed
a sensitivity of 87% and specificity of 83.7%. Our solution outperforms
state-of-the-art 3D CNN networks. The proposed design allows for a fully
automated network that can be trained easily in an end-to-end manner without
requiring computationally intensive and time-consuming preprocessing or
strenuous labeling of the data.We infer that unmarked CTPAs can be used for
effective RV strain classification. This could be used as a second reader,
alerting for high-risk PE patients. To the best of our knowledge, there are no
previous deep learning-based studies that attempted to solve this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cahan_N/0/1/0/all/0/1"&gt;Noa Cahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marom_E/0/1/0/all/0/1"&gt;Edith M. Marom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soffer_S/0/1/0/all/0/1"&gt;Shelly Soffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barash_Y/0/1/0/all/0/1"&gt;Yiftach Barash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Konen_E/0/1/0/all/0/1"&gt;Eli Konen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klang_E/0/1/0/all/0/1"&gt;Eyal Klang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greenspan_H/0/1/0/all/0/1"&gt;Hayit Greenspan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Periocular Iris PAI from a Small Set of Near-Infrared-Images. (arXiv:2107.12014v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12014</id>
        <link href="http://arxiv.org/abs/2107.12014"/>
        <updated>2021-07-27T02:03:33.216Z</updated>
        <summary type="html"><![CDATA[Biometric has been increasing in relevance these days since it can be used
for several applications such as access control for instance. Unfortunately,
with the increased deployment of biometric applications, we observe an increase
of attacks. Therefore, algorithms to detect such attacks (Presentation Attack
Detection (PAD)) have been increasing in relevance. The LivDet-2020 competition
which focuses on Presentation Attacks Detection (PAD) algorithms have shown
still open problems, specially for unknown attacks scenarios. In order to
improve the robustness of biometric systems, it is crucial to improve PAD
methods. This can be achieved by augmenting the number of presentation attack
instruments (PAI) and bona fide images that are used to train such algorithms.
Unfortunately, the capture and creation of presentation attack instruments and
even the capture of bona fide images is sometimes complex to achieve. This
paper proposes a novel PAI synthetically created (SPI-PAI) using four
state-of-the-art GAN algorithms (cGAN, WGAN, WGAN-GP, and StyleGAN2) and a
small set of periocular NIR images. A benchmark between GAN algorithms is
performed using the Frechet Inception Distance (FID) between the generated
images and the original images used for training. The best PAD algorithm
reported by the LivDet-2020 competition was tested for us using the synthetic
PAI which was obtained with the StyleGAN2 algorithm. Surprisingly, The PAD
algorithm was not able to detect the synthetic images as a Presentation Attack,
categorizing all of them as bona fide. Such results demonstrated the
feasibility of synthetic images to fool presentation attacks detection
algorithms and the need for such algorithms to be constantly updated and
trained with a larger number of images and PAI scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maureira_J/0/1/0/all/0/1"&gt;Jose Maureira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1"&gt;Juan Tapia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arellano_C/0/1/0/all/0/1"&gt;Claudia Arellano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation. (arXiv:2107.12087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12087</id>
        <link href="http://arxiv.org/abs/2107.12087"/>
        <updated>2021-07-27T02:03:33.208Z</updated>
        <summary type="html"><![CDATA[Text recognition remains a fundamental and extensively researched topic in
computer vision, largely owing to its wide array of commercial applications.
The challenging nature of the very problem however dictated a fragmentation of
research efforts: Scene Text Recognition (STR) that deals with text in everyday
scenes, and Handwriting Text Recognition (HTR) that tackles hand-written text.
In this paper, for the first time, we argue for their unification -- we aim for
a single model that can compete favourably with two separate state-of-the-art
STR and HTR models. We first show that cross-utilisation of STR and HTR models
trigger significant performance drops due to differences in their inherent
challenges. We then tackle their union by introducing a knowledge distillation
(KD) based framework. This is however non-trivial, largely due to the
variable-length and sequential nature of text sequences, which renders
off-the-shelf KD techniques that mostly works with global fixed-length data
inadequate. For that, we propose three distillation losses all of which are
specifically designed to cope with the aforementioned unique characteristics of
text recognition. Empirical evidence suggests that our proposed unified model
performs on par with individual models, even surpassing them in certain cases.
Ablative studies demonstrate that naive baselines such as a two-stage
framework, and domain adaption/generalisation alternatives do not work as well,
further verifying the appropriateness of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1"&gt;Ayan Kumar Bhunia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1"&gt;Aneeshan Sain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1"&gt;Pinaki Nath Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Representation Factorization for Video-based Person Re-Identification. (arXiv:2107.11878v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11878</id>
        <link href="http://arxiv.org/abs/2107.11878"/>
        <updated>2021-07-27T02:03:33.173Z</updated>
        <summary type="html"><![CDATA[Despite much recent progress in video-based person re-identification (re-ID),
the current state-of-the-art still suffers from common real-world challenges
such as appearance similarity among various people, occlusions, and frame
misalignment. To alleviate these problems, we propose Spatio-Temporal
Representation Factorization module (STRF), a flexible new computational unit
that can be used in conjunction with most existing 3D convolutional neural
network architectures for re-ID. The key innovations of STRF over prior work
include explicit pathways for learning discriminative temporal and spatial
features, with each component further factorized to capture complementary
person-specific appearance and motion information. Specifically, temporal
factorization comprises two branches, one each for static features (e.g., the
color of clothes) that do not change much over time, and dynamic features
(e.g., walking patterns) that change over time. Further, spatial factorization
also comprises two branches to learn both global (coarse segments) as well as
local (finer segments) appearance features, with the local features
particularly useful in cases of occlusion or spatial misalignment. These two
factorization operations taken together result in a modular architecture for
our parameter-wise economic STRF unit that can be plugged in between any two 3D
convolutional layers, resulting in an end-to-end learning framework. We
empirically show that STRF improves performance of various existing baseline
architectures while demonstrating new state-of-the-art results using standard
person re-identification evaluation protocols on three benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1"&gt;Abhishek Aich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Meng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1"&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11960</id>
        <link href="http://arxiv.org/abs/2107.11960"/>
        <updated>2021-07-27T02:03:33.165Z</updated>
        <summary type="html"><![CDATA[The goal of few-shot video classification is to learn a classification model
with good generalization ability when trained with only a few labeled videos.
However, it is difficult to learn discriminative feature representations for
videos in such a setting. In this paper, we propose Temporal Alignment
Prediction (TAP) based on sequence similarity learning for few-shot video
classification. In order to obtain the similarity of a pair of videos, we
predict the alignment scores between all pairs of temporal positions in the two
videos with the temporal alignment prediction function. Besides, the inputs to
this function are also equipped with the context information in the temporal
domain. We evaluate TAP on two video classification benchmarks including
Kinetics and Something-Something V2. The experimental results verify the
effectiveness of TAP and show its superiority over state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunlei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanwen Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Scene Video Text Spotting. (arXiv:2107.11919v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11919</id>
        <link href="http://arxiv.org/abs/2107.11919"/>
        <updated>2021-07-27T02:03:33.092Z</updated>
        <summary type="html"><![CDATA[Scene video text spotting (SVTS) is a very important research topic because
of many real-life applications. However, only a little effort has put to
spotting scene video text, in contrast to massive studies of scene text
spotting in static images. Due to various environmental interferences like
motion blur, spotting scene video text becomes very challenging. To promote
this research area, this competition introduces a new challenge dataset
containing 129 video clips from 21 natural scenarios in full annotations. The
competition containts three tasks, that is, video text detection (Task 1),
video text tracking (Task 2) and end-to-end video text spotting (Task3). During
the competition period (opened on 1st March, 2021 and closed on 11th April,
2021), a total of 24 teams participated in the three proposed tasks with 46
valid submissions, respectively. This paper includes dataset descriptions, task
definitions, evaluation protocols and results summaries of the ICDAR 2021 on
SVTS competition. Thanks to the healthy number of teams as well as submissions,
we consider that the SVTS competition has been successfully held, drawing much
attention from the community and promoting the field research and its
development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhanzhan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1"&gt;Baorui Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuigeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Robot Localisation by Ignoring Visual Distraction. (arXiv:2107.11857v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.11857</id>
        <link href="http://arxiv.org/abs/2107.11857"/>
        <updated>2021-07-27T02:03:33.067Z</updated>
        <summary type="html"><![CDATA[Attention is an important component of modern deep learning. However, less
emphasis has been put on its inverse: ignoring distraction. Our daily lives
require us to explicitly avoid giving attention to salient visual features that
confound the task we are trying to accomplish. This visual prioritisation
allows us to concentrate on important tasks while ignoring visual distractors.

In this work, we introduce Neural Blindness, which gives an agent the ability
to completely ignore objects or classes that are deemed distractors. More
explicitly, we aim to render a neural network completely incapable of
representing specific chosen classes in its latent space. In a very real sense,
this makes the network "blind" to certain classes, allowing and agent to focus
on what is important for a given task, and demonstrates how this can be used to
improve localisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew Vowels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will Multi-modal Data Improves Few-shot Learning?. (arXiv:2107.11853v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11853</id>
        <link href="http://arxiv.org/abs/2107.11853"/>
        <updated>2021-07-27T02:03:33.061Z</updated>
        <summary type="html"><![CDATA[Most few-shot learning models utilize only one modality of data. We would
like to investigate qualitatively and quantitatively how much will the model
improve if we add an extra modality (i.e. text description of the image), and
how it affects the learning procedure. To achieve this goal, we propose four
types of fusion method to combine the image feature and text feature. To verify
the effectiveness of improvement, we test the fusion methods with two classical
few-shot learning models - ProtoNet and MAML, with image feature extractors
such as ConvNet and ResNet12. The attention-based fusion method works best,
which improves the classification accuracy by a large margin around 30%
comparing to the baseline result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zilun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shihao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Recursive Circle Framework for Fine-grained Action Recognition. (arXiv:2107.11813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11813</id>
        <link href="http://arxiv.org/abs/2107.11813"/>
        <updated>2021-07-27T02:03:33.042Z</updated>
        <summary type="html"><![CDATA[How to model fine-grained spatial-temporal dynamics in videos has been a
challenging problem for action recognition. It requires learning deep and rich
features with superior distinctiveness for the subtle and abstract motions.
Most existing methods generate features of a layer in a pure feedforward
manner, where the information moves in one direction from inputs to outputs.
And they rely on stacking more layers to obtain more powerful features,
bringing extra non-negligible overheads. In this paper, we propose an Adaptive
Recursive Circle (ARC) framework, a fine-grained decorator for pure feedforward
layers. It inherits the operators and parameters of the original layer but is
slightly different in the use of those operators and parameters. Specifically,
the input of the layer is treated as an evolving state, and its update is
alternated with the feature generation. At each recursive step, the input state
is enriched by the previously generated features and the feature generation is
made with the newly updated input state. We hope the ARC framework can
facilitate fine-grained action recognition by introducing deeply refined
features and multi-scale receptive fields at a low cost. Significant
improvements over feedforward baselines are observed on several benchmarks. For
example, an ARC-equipped TSM-ResNet18 outperforms TSM-ResNet50 with 48% fewer
FLOPs and 52% model parameters on Something-Something V1 and Diving48.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hanxi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinxiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transcript to Video: Efficient Clip Sequencing from Texts. (arXiv:2107.11851v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11851</id>
        <link href="http://arxiv.org/abs/2107.11851"/>
        <updated>2021-07-27T02:03:33.035Z</updated>
        <summary type="html"><![CDATA[Among numerous videos shared on the web, well-edited ones always attract more
attention. However, it is difficult for inexperienced users to make well-edited
videos because it requires professional expertise and immense manual labor. To
meet the demands for non-experts, we present Transcript-to-Video -- a
weakly-supervised framework that uses texts as input to automatically create
video sequences from an extensive collection of shots. Specifically, we propose
a Content Retrieval Module and a Temporal Coherent Module to learn
visual-language representations and model shot sequencing styles, respectively.
For fast inference, we introduce an efficient search strategy for real-time
video clip sequencing. Quantitative results and user studies demonstrate
empirically that the proposed learning framework can retrieve content-relevant
shots while creating plausible video sequences in terms of style. Besides, the
run-time performance analysis shows that our framework can support real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yu Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1"&gt;Fabian Caba Heilbron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bangla sign language recognition using concatenated BdSL network. (arXiv:2107.11818v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11818</id>
        <link href="http://arxiv.org/abs/2107.11818"/>
        <updated>2021-07-27T02:03:33.028Z</updated>
        <summary type="html"><![CDATA[Sign language is the only medium of communication for the hearing impaired
and the deaf and dumb community. Communication with the general mass is thus
always a challenge for this minority group. Especially in Bangla sign language
(BdSL), there are 38 alphabets with some having nearly identical symbols. As a
result, in BdSL recognition, the posture of hand is an important factor in
addition to visual features extracted from traditional Convolutional Neural
Network (CNN). In this paper, a novel architecture "Concatenated BdSL Network"
is proposed which consists of a CNN based image network and a pose estimation
network. While the image network gets the visual features, the relative
positions of hand keypoints are taken by the pose estimation network to obtain
the additional features to deal with the complexity of the BdSL symbols. A
score of 91.51% was achieved by this novel approach in test set and the
effectiveness of the additional pose estimation network is suggested by the
experimental results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1"&gt;Thasin Abedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1"&gt;Khondokar S. S. Prottoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1"&gt;Ayana Moshruba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakim_S/0/1/0/all/0/1"&gt;Safayat Bin Hakim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation. (arXiv:2107.11787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11787</id>
        <link href="http://arxiv.org/abs/2107.11787"/>
        <updated>2021-07-27T02:03:33.021Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is a challenging task in the absence of densely
labelled data. Only relying on class activation maps (CAM) with image-level
labels provides deficient segmentation supervision. Prior works thus consider
pre-trained models to produce coarse saliency maps to guide the generation of
pseudo segmentation labels. However, the commonly used off-line heuristic
generation process cannot fully exploit the benefits of these coarse saliency
maps. Motivated by the significant inter-task correlation, we propose a novel
weakly supervised multi-task framework termed as AuxSegNet, to leverage
saliency detection and multi-label image classification as auxiliary tasks to
improve the primary task of semantic segmentation using only image-level
ground-truth labels. Inspired by their similar structured semantics, we also
propose to learn a cross-task global pixel-level affinity map from the saliency
and segmentation representations. The learned cross-task affinity can be used
to refine saliency predictions and propagate CAM maps to provide improved
pseudo labels for both tasks. The mutual boost between pseudo label updating
and cross-task affinity learning enables iterative improvements on segmentation
performance. Extensive experiments demonstrate the effectiveness of the
proposed auxiliary learning network structure and the cross-task affinity
learning method. The proposed approach achieves state-of-the-art weakly
supervised segmentation performance on the challenging PASCAL VOC 2012 and MS
COCO benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1"&gt;Farid Boussaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1"&gt;Ferdous Sohel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comprehensive Studies for Arbitrary-shape Scene Text Detection. (arXiv:2107.11800v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11800</id>
        <link href="http://arxiv.org/abs/2107.11800"/>
        <updated>2021-07-27T02:03:33.014Z</updated>
        <summary type="html"><![CDATA[Numerous scene text detection methods have been proposed in recent years.
Most of them declare they have achieved state-of-the-art performances. However,
the performance comparison is unfair, due to lots of inconsistent settings
(e.g., training data, backbone network, multi-scale feature fusion, evaluation
protocols, etc.). These various settings would dissemble the pros and cons of
the proposed core techniques. In this paper, we carefully examine and analyze
the inconsistent settings, and propose a unified framework for the bottom-up
based scene text detection methods. Under the unified framework, we ensure the
consistent settings for non-core modules, and mainly investigate the
representations of describing arbitrary-shape scene texts, e.g., regressing
points on text contours, clustering pixels with predicted auxiliary
information, grouping connected components with learned linkages, etc. With the
comprehensive investigations and elaborate analyses, it not only cleans up the
obstacle of understanding the performance differences between existing methods
but also reveals the advantages and disadvantages of previous models under fair
comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1"&gt;Pengwen Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaochun Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising and Segmentation of Epigraphical Scripts. (arXiv:2107.11801v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11801</id>
        <link href="http://arxiv.org/abs/2107.11801"/>
        <updated>2021-07-27T02:03:32.994Z</updated>
        <summary type="html"><![CDATA[This paper is a presentation of a new method for denoising images using
Haralick features and further segmenting the characters using artificial neural
networks. The image is divided into kernels, each of which is converted to a
GLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation
function is called, the result of which is an array with fourteen elements
corresponding to fourteen features The Haralick values and the corresponding
noise/text classification form a dictionary, which is then used to de-noise the
image through kernel comparison. Segmentation is the process of extracting
characters from a document and can be used when letters are separated by white
space, which is an explicit boundary marker. Segmentation is the first step in
many Natural Language Processing problems. This paper explores the process of
segmentation using Neural Networks. While there have been numerous methods to
segment characters of a document, this paper is only concerned with the
accuracy of doing so using neural networks. It is imperative that the
characters be segmented correctly, for failing to do so will lead to incorrect
recognition by Natural language processing tools. Artificial Neural Networks
was used to attain accuracy of upto 89%. This method is suitable for languages
where the characters are delimited by white space. However, this method will
fail to provide acceptable results when the language heavily uses connected
letters. An example would be the Devanagari script, which is predominantly used
in northern India.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1"&gt;P Preethi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1"&gt;Hrishikesh Viswanath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Character Spotting Using Machine Learning Techniques. (arXiv:2107.11795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11795</id>
        <link href="http://arxiv.org/abs/2107.11795"/>
        <updated>2021-07-27T02:03:32.987Z</updated>
        <summary type="html"><![CDATA[This work presents a comparison of machine learning algorithms that are
implemented to segment the characters of text presented as an image. The
algorithms are designed to work on degraded documents with text that is not
aligned in an organized fashion. The paper investigates the use of Support
Vector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform
the operation of character spotting. Character Spotting involves extracting
potential characters from a stream of text by selecting regions bound by white
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1"&gt;P Preethi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1"&gt;Hrishikesh Viswanath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Frozen Section to FFPE Translation. (arXiv:2107.11786v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11786</id>
        <link href="http://arxiv.org/abs/2107.11786"/>
        <updated>2021-07-27T02:03:32.980Z</updated>
        <summary type="html"><![CDATA[Frozen sectioning (FS) is the preparation method of choice for microscopic
evaluation of tissues during surgical operations. The high speed of procedure
allows pathologists to rapidly assess the key microscopic features, such as
tumor margins and malignant status to guide surgical decision-making and
minimise disruptions to the course of the operation. However, FS is prone to
introducing many misleading artificial structures (histological artefacts),
such as nuclear ice crystals, compression, and cutting artefacts, hindering
timely and accurate diagnostic judgement of the pathologist. On the other hand,
the gold standard tissue preparation technique of formalin-fixation and
paraffin-embedding (FFPE) provides significantly superior image quality, but is
a very time-consuming process (12-48 hours), making it unsuitable for
intra-operative use. In this paper, we propose an artificial intelligence (AI)
method that improves FS image quality by computationally transforming
frozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style
images in minutes. AI-FFPE rectifies FS artefacts with the guidance of an
attention-mechanism that puts a particular emphasis on artefacts while
utilising a self-regularization mechanism established between FS input image
and synthesized FFPE-style image that preserves clinically relevant features.
As a result, AI-FFPE method successfully generates FFPE-style images without
significantly extending tissue processing time and consequently improves
diagnostic accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ozyoruk_K/0/1/0/all/0/1"&gt;Kutsev Bengisu Ozyoruk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Can_S/0/1/0/all/0/1"&gt;Sermet Can&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gokceler_G/0/1/0/all/0/1"&gt;Guliz Irem Gokceler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Basak_K/0/1/0/all/0/1"&gt;Kayhan Basak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Demir_D/0/1/0/all/0/1"&gt;Derya Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Serin_G/0/1/0/all/0/1"&gt;Gurdeniz Serin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hacisalihoglu_U/0/1/0/all/0/1"&gt;Uguray Payam Hacisalihoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Darbaz_B/0/1/0/all/0/1"&gt;Berkan Darbaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tiffany Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1"&gt;Drew F. K. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yilmaz_F/0/1/0/all/0/1"&gt;Funda Yilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1"&gt;Faisal Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turan_M/0/1/0/all/0/1"&gt;Mehmet Turan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Uncertainty-Aware Deep Learning Framework for Defect Detection in Casting Products. (arXiv:2107.11643v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11643</id>
        <link href="http://arxiv.org/abs/2107.11643"/>
        <updated>2021-07-27T02:03:32.974Z</updated>
        <summary type="html"><![CDATA[Defects are unavoidable in casting production owing to the complexity of the
casting process. While conventional human-visual inspection of casting products
is slow and unproductive in mass productions, an automatic and reliable defect
detection not just enhances the quality control process but positively improves
productivity. However, casting defect detection is a challenging task due to
diversity and variation in defects' appearance. Convolutional neural networks
(CNNs) have been widely applied in both image classification and defect
detection tasks. Howbeit, CNNs with frequentist inference require a massive
amount of data to train on and still fall short in reporting beneficial
estimates of their predictive uncertainty. Accordingly, leveraging the transfer
learning paradigm, we first apply four powerful CNN-based models (VGG16,
ResNet50, DenseNet121, and InceptionResNetV2) on a small dataset to extract
meaningful features. Extracted features are then processed by various machine
learning algorithms to perform the classification task. Simulation results
demonstrate that linear support vector machine (SVM) and multi-layer perceptron
(MLP) show the finest performance in defect detection of casting images.
Secondly, to achieve a reliable classification and to measure epistemic
uncertainty, we employ an uncertainty quantification (UQ) technique (ensemble
of MLP models) using features extracted from four pre-trained CNNs. UQ
confusion matrix and uncertainty accuracy metric are also utilized to evaluate
the predictive uncertainty estimates. Comprehensive comparisons reveal that UQ
method based on VGG16 outperforms others to fetch uncertainty. We believe an
uncertainty-aware automatic defect detection solution will reinforce casting
productions quality assurance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1"&gt;Maryam Habibpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1"&gt;Hassan Gharoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1"&gt;AmirReza Tajally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1"&gt;Afshar Shamsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1"&gt;Hamzeh Asgharnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go Wider Instead of Deeper. (arXiv:2107.11817v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11817</id>
        <link href="http://arxiv.org/abs/2107.11817"/>
        <updated>2021-07-27T02:03:32.966Z</updated>
        <summary type="html"><![CDATA[The transformer has recently achieved impressive results on various tasks. To
further improve the effectiveness and efficiency of the transformer, there are
two trains of thought among existing works: (1) going wider by scaling to more
trainable parameters; (2) going shallower by parameter sharing or model
compressing along with the depth. However, larger models usually do not scale
well when fewer tokens are available to train, and advanced parallelisms are
required when the model is extremely large. Smaller models usually achieve
inferior performance compared to the original transformer model due to the loss
of representation power. In this paper, to achieve better performance with
fewer trainable parameters, we propose a framework to deploy trainable
parameters efficiently, by going wider instead of deeper. Specially, we scale
along model width by replacing feed-forward network (FFN) with
mixture-of-experts (MoE). We then share the MoE layers across transformer
blocks using individual layer normalization. Such deployment plays the role to
transform various semantic representations, which makes the model more
parameter-efficient and effective. To evaluate our framework, we design WideNet
and evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer
(ViT) by $1.46\%$ with $0.72 \times$ trainable parameters. Using $0.46 \times$
and $0.13 \times$ parameters, our WideNet can still surpass ViT and ViT-MoE by
$0.83\%$ and $2.08\%$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Ziji Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yuxuan Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified Robustness to Text Adversarial Attacks by Randomized [MASK]. (arXiv:2105.03743v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03743</id>
        <link href="http://arxiv.org/abs/2105.03743"/>
        <updated>2021-07-27T02:03:32.948Z</updated>
        <summary type="html"><![CDATA[Recently, few certified defense methods have been developed to provably
guarantee the robustness of a text classifier to adversarial synonym
substitutions. However, all existing certified defense methods assume that the
defenders are informed of how the adversaries generate synonyms, which is not a
realistic scenario. In this paper, we propose a certifiably robust defense
method by randomly masking a certain proportion of the words in an input text,
in which the above unrealistic assumption is no longer necessary. The proposed
method can defend against not only word substitution-based attacks, but also
character-level perturbations. We can certify the classifications of over 50%
texts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on
SST2 dataset. The experimental results show that our randomized smoothing
method significantly outperforms recently proposed defense methods across
multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1"&gt;Jiehang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianhan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Liping Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems. (arXiv:2107.11640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11640</id>
        <link href="http://arxiv.org/abs/2107.11640"/>
        <updated>2021-07-27T02:03:32.941Z</updated>
        <summary type="html"><![CDATA[Automated Vehicle License Plate (VLP) detection and recognition have ended up
being a significant research issue as of late. VLP localization and recognition
are some of the most essential techniques for managing traffic using digital
techniques. In this paper, four smart systems are developed to recognize
Egyptian vehicles license plates. Two systems are based on character
recognition, which are (System1, Characters Recognition with Classical Machine
Learning) and (System2, Characters Recognition with Deep Machine Learning). The
other two systems are based on the whole plate recognition which are (System3,
Whole License Plate Recognition with Classical Machine Learning) and (System4,
Whole License Plate Recognition with Deep Machine Learning). We use object
detection algorithms, and machine learning based object recognition algorithms.
The performance of the developed systems has been tested on real images, and
the experimental results demonstrate that the best detection accuracy rate for
VLP is provided by using the deep learning method. Where the VLP detection
accuracy rate is better than the classical system by 32%. However, the best
detection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is
provided by using the classical method. Where VLPAC detection accuracy rate is
better than the deep learning-based system by 6%. Also, the results show that
deep learning is better than the classical technique used in VLP recognition
processes. Where the recognition accuracy rate is better than the classical
system by 8%. Finally, the paper output recommends a robust VLP recognition
system based on both statistical and deep machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1"&gt;Mohamed Shehata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abou_Kreisha_M/0/1/0/all/0/1"&gt;Mohamed Taha Abou-Kreisha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elnashar_H/0/1/0/all/0/1"&gt;Hany Elnashar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions. (arXiv:2107.11585v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11585</id>
        <link href="http://arxiv.org/abs/2107.11585"/>
        <updated>2021-07-27T02:03:32.933Z</updated>
        <summary type="html"><![CDATA[As the field of remote sensing is evolving, we witness the accumulation of
information from several modalities, such as multispectral (MS), hyperspectral
(HSI), LiDAR etc. Each of these modalities possess its own distinct
characteristics and when combined synergistically, perform very well in the
recognition and classification tasks. However, fusing multiple modalities in
remote sensing is cumbersome due to highly disparate domains. Furthermore, the
existing methods do not facilitate cross-modal interactions. To this end, we
propose a novel transformer based fusion method for HSI and LiDAR modalities.
The model is composed of stacked auto encoders that harness the cross key-value
pairs for HSI and LiDAR, thus establishing a communication between the two
modalities, while simultaneously using the CNNs to extract the spectral and
spatial information from HSI and LiDAR. We test our model on Houston (Data
Fusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1"&gt;Rupak Bose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1"&gt;Shivam Pande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1"&gt;Biplab Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-07-27T02:03:32.927Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Conditioned Probabilistic Learning of Video Rescaling. (arXiv:2107.11639v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11639</id>
        <link href="http://arxiv.org/abs/2107.11639"/>
        <updated>2021-07-27T02:03:32.919Z</updated>
        <summary type="html"><![CDATA[Bicubic downscaling is a prevalent technique used to reduce the video storage
burden or to accelerate the downstream processing speed. However, the inverse
upscaling step is non-trivial, and the downscaled video may also deteriorate
the performance of downstream tasks. In this paper, we propose a
self-conditioned probabilistic framework for video rescaling to learn the
paired downscaling and upscaling procedures simultaneously. During the
training, we decrease the entropy of the information lost in the downscaling by
maximizing its probability conditioned on the strong spatial-temporal prior
information within the downscaled video. After optimization, the downscaled
video by our framework preserves more meaningful information, which is
beneficial for both the upscaling step and the downstream tasks, e.g., video
action recognition task. We further extend the framework to a lossy video
compression system, in which a gradient estimator for non-differential
industrial lossy codecs is proposed for the end-to-end training of the whole
system. Extensive experimental results demonstrate the superiority of our
approach on video rescaling, video compression, and efficient action
recognition tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhaohui Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhiyong Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Echo LiDAR for 3D Object Detection. (arXiv:2107.11470v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11470</id>
        <link href="http://arxiv.org/abs/2107.11470"/>
        <updated>2021-07-27T02:03:32.901Z</updated>
        <summary type="html"><![CDATA[LiDAR sensors can be used to obtain a wide range of measurement signals other
than a simple 3D point cloud, and those signals can be leveraged to improve
perception tasks like 3D object detection. A single laser pulse can be
partially reflected by multiple objects along its path, resulting in multiple
measurements called echoes. Multi-echo measurement can provide information
about object contours and semi-transparent surfaces which can be used to better
identify and locate objects. LiDAR can also measure surface reflectance
(intensity of laser pulse return), as well as ambient light of the scene
(sunlight reflected by objects). These signals are already available in
commercial LiDAR devices but have not been used in most LiDAR-based detection
models. We present a 3D object detection model which leverages the full
spectrum of measurement signals provided by LiDAR. First, we propose a
multi-signal fusion (MSF) module to combine (1) the reflectance and ambient
features extracted with a 2D CNN, and (2) point cloud features extracted using
a 3D graph neural network (GNN). Second, we propose a multi-echo aggregation
(MEA) module to combine the information encoded in different set of echo
points. Compared with traditional single echo point cloud methods, our proposed
Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a
wider range of sensing measurements and achieves more accurate 3D object
detection. Experiments show that by incorporating the multi-modality of LiDAR,
our method outperforms the state-of-the-art by up to 9.1%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1"&gt;Yunze Man&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1"&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivakuma_P/0/1/0/all/0/1"&gt;Prasanna Kumar Sivakuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OToole_M/0/1/0/all/0/1"&gt;Matthew O&amp;#x27;Toole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris Kitani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. (arXiv:2107.11443v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11443</id>
        <link href="http://arxiv.org/abs/2107.11443"/>
        <updated>2021-07-27T02:03:32.893Z</updated>
        <summary type="html"><![CDATA[Video activity localisation has recently attained increasing attention due to
its practical values in automatically localising the most salient visual
segments corresponding to their language descriptions (sentences) from
untrimmed and unstructured videos. For supervised model training, a temporal
annotation of both the start and end time index of each video segment for a
sentence (a video moment) must be given. This is not only very expensive but
also sensitive to ambiguity and subjective annotation bias, a much harder task
than image labelling. In this work, we develop a more accurate
weakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)
in video moment proposal generation and matching when only a paragraph
description of activities without per-sentence temporal annotation is
available. Specifically, we explore two cross-sentence relational constraints:
(1) Temporal ordering and (2) semantic consistency among sentences in a
paragraph description of video activities. Existing weakly-supervised
techniques only consider within-sentence video segment correlations in training
without considering cross-sentence paragraph context. This can mislead due to
ambiguous expressions of individual sentences with visually indiscriminate
video moment proposals in isolation. Experiments on two publicly available
activity localisation datasets show the advantages of our approach over the
state-of-the-art weakly supervised methods, especially so when the video
activity descriptions become more complex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiabo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shaogang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hailin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Hyperbolic Neural Networks with Limited Radii. (arXiv:2107.11472v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11472</id>
        <link href="http://arxiv.org/abs/2107.11472"/>
        <updated>2021-07-27T02:03:32.886Z</updated>
        <summary type="html"><![CDATA[Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic
space, has attracted sustained attention in the community of machine learning.
Hyperbolic space, owing to its ability to embed hierarchical structures
continuously with low distortion, has been applied for learning data with
tree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly
in hyperbolic space have also been proposed recently to further exploit the
potential of hyperbolic representations. While HNNs have achieved better
performance than Euclidean neural networks (ENNs) on datasets with implicit
hierarchical structure, they still perform poorly on standard classification
benchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is
critical for the data to respect the hyperbolic geometry when applying HNNs. In
this paper, we first conduct an empirical study showing that the inferior
performance of HNNs on standard recognition datasets can be attributed to the
notorious vanishing gradient problem. We further discovered that this problem
stems from the hybrid architecture of HNNs. Our analysis leads to a simple yet
effective solution called Feature Clipping, which regularizes the hyperbolic
embedding whenever its norm exceeding a given threshold. Our thorough
experiments show that the proposed method can successfully avoid the vanishing
gradient problem when training HNNs with backpropagation. The improved HNNs are
able to achieve comparable performance with ENNs on standard image recognition
datasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating
more adversarial robustness and stronger out-of-distribution detection
capability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yunhui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xudong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Based Cardiac MRI Segmentation: Do We Need Experts?. (arXiv:2107.11447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11447</id>
        <link href="http://arxiv.org/abs/2107.11447"/>
        <updated>2021-07-27T02:03:32.855Z</updated>
        <summary type="html"><![CDATA[Deep learning methods are the de-facto solutions to a multitude of medical
image analysis tasks. Cardiac MRI segmentation is one such application which,
like many others, requires a large number of annotated data so a trained
network can generalize well. Unfortunately, the process of having a large
number of manually curated images by medical experts is both slow and utterly
expensive. In this paper, we set out to explore whether expert knowledge is a
strict requirement for the creation of annotated datasets that machine learning
can successfully train on. To do so, we gauged the performance of three
segmentation models, namely U-Net, Attention U-Net, and ENet, trained with
different loss functions on expert and non-expert groundtruth for cardiac
cine-MRI segmentation. Evaluation was done with classic segmentation metrics
(Dice index and Hausdorff distance) as well as clinical measurements, such as
the ventricular ejection fractions and the myocardial mass. Results reveal that
generalization performances of a segmentation neural network trained on
non-expert groundtruth data is, to all practical purposes, as good as on expert
groundtruth data, in particular when the non-expert gets a decent level of
training, highlighting an opportunity for the efficient and cheap creation of
annotations for cardiac datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1"&gt;Youssef Skandarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1"&gt;Alain Lalande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crosslink-Net: Double-branch Encoder Segmentation Network via Fusing Vertical and Horizontal Convolutions. (arXiv:2107.11517v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11517</id>
        <link href="http://arxiv.org/abs/2107.11517"/>
        <updated>2021-07-27T02:03:32.846Z</updated>
        <summary type="html"><![CDATA[Accurate image segmentation plays a crucial role in medical image analysis,
yet it faces great challenges of various shapes, diverse sizes, and blurry
boundaries. To address these difficulties, square kernel-based encoder-decoder
architecture has been proposed and widely used, but its performance remains
still unsatisfactory. To further cope with these challenges, we present a novel
double-branch encoder architecture. Our architecture is inspired by two
observations: 1) Since the discrimination of features learned via square
convolutional kernels needs to be further improved, we propose to utilize
non-square vertical and horizontal convolutional kernels in the double-branch
encoder, so features learned by the two branches can be expected to complement
each other. 2) Considering that spatial attention can help models to better
focus on the target region in a large-sized image, we develop an attention loss
to further emphasize the segmentation on small-sized targets. Together, the
above two schemes give rise to a novel double-branch encoder segmentation
framework for medical image segmentation, namely Crosslink-Net. The experiments
validate the effectiveness of our model on four datasets. The code is released
at https://github.com/Qianyu1226/Crosslink-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yilong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wuzhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Image Classification with Contrastive Learning. (arXiv:2107.11626v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11626</id>
        <link href="http://arxiv.org/abs/2107.11626"/>
        <updated>2021-07-27T02:03:32.823Z</updated>
        <summary type="html"><![CDATA[Recently, as an effective way of learning latent representations, contrastive
learning has been increasingly popular and successful in various domains. The
success of constrastive learning in single-label classifications motivates us
to leverage this learning framework to enhance distinctiveness for better
performance in multi-label image classification. In this paper, we show that a
direct application of contrastive learning can hardly improve in multi-label
cases. Accordingly, we propose a novel framework for multi-label classification
with contrastive learning in a fully supervised setting, which learns multiple
representations of an image under the context of different labels. This
facilities a simple yet intuitive adaption of contrastive learning into our
model to boost its performance in multi-label image classification. Extensive
experiments on two benchmark datasets show that the proposed framework achieves
state-of-the-art performance in the comparison with the advanced methods in
multi-label classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dao_S/0/1/0/all/0/1"&gt;Son D.Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1"&gt;Ethan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cycled Compositional Learning between Images and Text. (arXiv:2107.11509v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11509</id>
        <link href="http://arxiv.org/abs/2107.11509"/>
        <updated>2021-07-27T02:03:32.816Z</updated>
        <summary type="html"><![CDATA[We present an approach named the Cycled Composition Network that can measure
the semantic distance of the composition of image-text embedding. First, the
Composition Network transit a reference image to target image in an embedding
space using relative caption. Second, the Correction Network calculates a
difference between reference and retrieved target images in the embedding space
and match it with a relative caption. Our goal is to learn a Composition
mapping with the Composition Network. Since this one-way mapping is highly
under-constrained, we couple it with an inverse relation learning with the
Correction Network and introduce a cycled relation for given Image We
participate in Fashion IQ 2020 challenge and have won the first place with the
ensemble of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jongseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seunghwan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+GunheeKim/0/1/0/all/0/1"&gt;GunheeKim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11400</id>
        <link href="http://arxiv.org/abs/2107.11400"/>
        <updated>2021-07-27T02:03:32.809Z</updated>
        <summary type="html"><![CDATA[With the rise of deep neural networks, the challenge of explaining the
predictions of these networks has become increasingly recognized. While many
methods for explaining the decisions of deep neural networks exist, there is
currently no consensus on how to evaluate them. On the other hand, robustness
is a popular topic for deep learning research; however, it is hardly talked
about in explainability until very recently. In this tutorial paper, we start
by presenting gradient-based interpretability methods. These techniques use
gradient signals to assign the burden of the decision on the input features.
Later, we discuss how gradient-based methods can be evaluated for their
robustness and the role that adversarial robustness plays in having meaningful
explanations. We also discuss the limitations of gradient-based methods.
Finally, we present the best practices and attributes that should be examined
before choosing an explainability method. We conclude with the future
directions for research in the area at the convergence of robustness and
explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1"&gt;Ian E. Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1"&gt;Ghulam Rasool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1"&gt;Dimah Dera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1"&gt;Nidhal Bouaynaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1"&gt;Ravi P. Ramachandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform. (arXiv:2107.11627v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11627</id>
        <link href="http://arxiv.org/abs/2107.11627"/>
        <updated>2021-07-27T02:03:32.798Z</updated>
        <summary type="html"><![CDATA[Fast and accurate simulation of imaging through atmospheric turbulence is
essential for developing turbulence mitigation algorithms. Recognizing the
limitations of previous approaches, we introduce a new concept known as the
phase-to-space (P2S) transform to significantly speed up the simulation. P2S is
build upon three ideas: (1) reformulating the spatially varying convolution as
a set of invariant convolutions with basis functions, (2) learning the basis
function via the known turbulence statistics models, (3) implementing the P2S
transform via a light-weight network that directly convert the phase
representation to spatial representation. The new simulator offers 300x --
1000x speed up compared to the mainstream split-step simulators while
preserving the essential turbulence statistics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhiyuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1"&gt;Nicholas Chimitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing Images of Two Adjacent Objects through Scattering Medium Using Generative Adversarial Network. (arXiv:2107.11574v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11574</id>
        <link href="http://arxiv.org/abs/2107.11574"/>
        <updated>2021-07-27T02:03:32.791Z</updated>
        <summary type="html"><![CDATA[Reconstruction of image by using convolutional neural networks (CNNs) has
been vigorously studied in the last decade. Until now, there have being
developed several techniques for imaging of a single object through scattering
medium by using neural networks, however how to reconstruct images of more than
one object simultaneously seems hard to realize. In this paper, we demonstrate
an approach by using generative adversarial network (GAN) to reconstruct images
of two adjacent objects through scattering media. We construct an imaging
system for imaging of two adjacent objects behind the scattering media. In
general, as the light field of two adjacent object images pass through the
scattering slab, a speckle pattern is obtained. The designed adversarial
network, which is called as YGAN, is employed to reconstruct the images
simultaneously. It is shown that based on the trained YGAN, we can reconstruct
images of two adjacent objects from one speckle pattern with high fidelity. In
addition, we study the influence of the object image types, and the distance
between the two adjacent objects on the fidelity of the reconstructed images.
Moreover even if another scattering medium is inserted between the two objects,
we can also reconstruct the images of two objects from a speckle with high
quality. The technique presented in this work can be used for applications in
areas of medical image analysis, such as medical image classification,
segmentation, and studies of multi-object scattering imaging etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lai_X/0/1/0/all/0/1"&gt;Xuetian Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiongyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Ziyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_X/0/1/0/all/0/1"&gt;Xiaopeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pu_J/0/1/0/all/0/1"&gt;Jixiong Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic-guided Pixel Sampling for Cloth-Changing Person Re-identification. (arXiv:2107.11522v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11522</id>
        <link href="http://arxiv.org/abs/2107.11522"/>
        <updated>2021-07-27T02:03:32.773Z</updated>
        <summary type="html"><![CDATA[Cloth-changing person re-identification (re-ID) is a new rising research
topic that aims at retrieving pedestrians whose clothes are changed. This task
is quite challenging and has not been fully studied to date. Current works
mainly focus on body shape or contour sketch, but they are not robust enough
due to view and posture variations. The key to this task is to exploit
cloth-irrelevant cues. This paper proposes a semantic-guided pixel sampling
approach for the cloth-changing person re-ID task. We do not explicitly define
which feature to extract but force the model to automatically learn
cloth-irrelevant cues. Specifically, we first recognize the pedestrian's upper
clothes and pants, then randomly change them by sampling pixels from other
pedestrians. The changed samples retain the identity labels but exchange the
pixels of clothes or pants among different pedestrians. Besides, we adopt a
loss function to constrain the learned features to keep consistent before and
after changes. In this way, the model is forced to learn cues that are
irrelevant to upper clothes and pants. We conduct extensive experiments on the
latest released PRCC dataset. Our method achieved 65.8% on Rank1 accuracy,
which outperforms previous methods with a large margin. The code is available
at https://github.com/shuxjweb/pixel_sampling.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiujun Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1"&gt;Weijian Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Deeper into Semi-supervised Person Re-identification. (arXiv:2107.11566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11566</id>
        <link href="http://arxiv.org/abs/2107.11566"/>
        <updated>2021-07-27T02:03:32.766Z</updated>
        <summary type="html"><![CDATA[Person re-identification is the challenging task of identifying a person
across different camera views. Training a convolutional neural network (CNN)
for this task requires annotating a large dataset, and hence, it involves the
time-consuming manual matching of people across cameras. To reduce the need for
labeled data, we focus on a semi-supervised approach that requires only a
subset of the training data to be labeled. We conduct a comprehensive survey in
the area of person re-identification with limited labels. Existing works in
this realm are limited in the sense that they utilize features from multiple
CNNs and require the number of identities in the unlabeled data to be known. To
overcome these limitations, we propose to employ part-based features from a
single CNN without requiring the knowledge of the label space (i.e., the number
of identities). This makes our approach more suitable for practical scenarios,
and it significantly reduces the need for computational resources. We also
propose a PartMixUp loss that improves the discriminative ability of learned
part-based features for pseudo-labeling in semi-supervised settings. Our method
outperforms the state-of-the-art results on three large-scale person re-id
datasets and achieves the same level of performance as fully supervised methods
with only one-third of labeled identities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moskvyak_O/0/1/0/all/0/1"&gt;Olga Moskvyak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maire_F/0/1/0/all/0/1"&gt;Frederic Maire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1"&gt;Feras Dayoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1"&gt;Mahsa Baktashmotlagh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Attention Enhanced BDense-UNet for Liver Lesion Segmentation. (arXiv:2107.11645v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11645</id>
        <link href="http://arxiv.org/abs/2107.11645"/>
        <updated>2021-07-27T02:03:32.757Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a new segmentation network by integrating DenseUNet
and bidirectional LSTM together with attention mechanism, termed as
DA-BDense-UNet. DenseUNet allows learning enough diverse features and enhancing
the representative power of networks by regulating the information flow.
Bidirectional LSTM is responsible to explore the relationships between the
encoded features and the up-sampled features in the encoding and decoding
paths. Meanwhile, we introduce attention gates (AG) into DenseUNet to diminish
responses of unrelated background regions and magnify responses of salient
regions progressively. Besides, the attention in bidirectional LSTM takes into
account the contribution differences of the encoded features and the up-sampled
features in segmentation improvement, which can in turn adjust proper weights
for these two kinds of features. We conduct experiments on liver CT image data
sets collected from multiple hospitals by comparing them with state-of-the-art
segmentation models. Experimental results indicate that our proposed method
DA-BDense-UNet has achieved comparative performance in terms of dice
coefficient, which demonstrates its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cao_W/0/1/0/all/0/1"&gt;Wenming Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip L.H. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lui_G/0/1/0/all/0/1"&gt;Gilbert C.S. Lui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chiu_K/0/1/0/all/0/1"&gt;Keith W.H. Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Ho-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yanwen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuen_M/0/1/0/all/0/1"&gt;Man-Fung Yuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seto_W/0/1/0/all/0/1"&gt;Wai-Kay Seto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyAction Challenge: Recognizing Real-world Low-resolution Activities in Videos. (arXiv:2107.11494v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11494</id>
        <link href="http://arxiv.org/abs/2107.11494"/>
        <updated>2021-07-27T02:03:32.750Z</updated>
        <summary type="html"><![CDATA[This paper summarizes the TinyAction challenge which was organized in
ActivityNet workshop at CVPR 2021. This challenge focuses on recognizing
real-world low-resolution activities present in videos. Action recognition task
is currently focused around classifying the actions from high-quality videos
where the actors and the action is clearly visible. While various approaches
have been shown effective for recognition task in recent works, they often do
not deal with videos of lower resolution where the action is happening in a
tiny region. However, many real world security videos often have the actual
action captured in a small resolution, making action recognition in a tiny
region a challenging task. In this work, we propose a benchmark dataset,
TinyVIRAT-v2, which is comprised of naturally occuring low-resolution actions.
This is an extension of the TinyVIRAT dataset and consists of actions with
multiple labels. The videos are extracted from security videos which makes them
realistic and more challenging. We use current state-of-the-art action
recognition methods on the dataset as a benchmark, and propose the TinyAction
Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tirupattur_P/0/1/0/all/0/1"&gt;Praveen Tirupattur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1"&gt;Aayush J Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangam_T/0/1/0/all/0/1"&gt;Tushar Sangam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1"&gt;Shruti Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1"&gt;Yogesh S Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering by Maximizing Mutual Information Across Views. (arXiv:2107.11635v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11635</id>
        <link href="http://arxiv.org/abs/2107.11635"/>
        <updated>2021-07-27T02:03:32.743Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for image clustering that incorporates joint
representation learning and clustering. Our method consists of two heads that
share the same backbone network - a "representation learning" head and a
"clustering" head. The "representation learning" head captures fine-grained
patterns of objects at the instance level which serve as clues for the
"clustering" head to extract coarse-grain information that separates objects
into clusters. The whole model is trained in an end-to-end manner by minimizing
the weighted sum of two sample-oriented contrastive losses applied to the
outputs of the two heads. To ensure that the contrastive loss corresponding to
the "clustering" head is optimal, we introduce a novel critic function called
"log-of-dot-product". Extensive experimental results demonstrate that our
method significantly outperforms state-of-the-art single-stage clustering
methods across a variety of image datasets, improving over the best baseline by
about 5-7% in accuracy on CIFAR10/20, STL10, and ImageNet-Dogs. Further, the
"two-stage" variant of our method also achieves better results than baselines
on three challenging ImageNet subsets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1"&gt;Kien Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images. (arXiv:2107.11468v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11468</id>
        <link href="http://arxiv.org/abs/2107.11468"/>
        <updated>2021-07-27T02:03:32.674Z</updated>
        <summary type="html"><![CDATA[We analyze a dataset of retinal images using linear probes: linear regression
models trained on some "target" task, using embeddings from a deep
convolutional (CNN) model trained on some "source" task as input. We use this
method across all possible pairings of 93 tasks in the UK Biobank dataset of
retinal images, leading to ~164k different models. We analyze the performance
of these linear probes by source and target task and by layer depth. We observe
that representations from the middle layers of the network are more
generalizable. We find that some target tasks are easily predicted irrespective
of the source task, and that some other target tasks are more accurately
predicted from correlated source tasks than from embeddings trained on the same
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1"&gt;Katy Blumer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1"&gt;Michael P. Brenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1"&gt;Jon Kleinberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition. (arXiv:2107.11442v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11442</id>
        <link href="http://arxiv.org/abs/2107.11442"/>
        <updated>2021-07-27T02:03:32.639Z</updated>
        <summary type="html"><![CDATA[We present a novel global compression framework for deep neural networks that
automatically analyzes each layer to identify the optimal per-layer compression
ratio, while simultaneously achieving the desired overall compression. Our
algorithm hinges on the idea of compressing each convolutional (or
fully-connected) layer by slicing its channels into multiple groups and
decomposing each group via low-rank decomposition. At the core of our algorithm
is the derivation of layer-wise error bounds from the Eckart Young Mirsky
theorem. We then leverage these bounds to frame the compression problem as an
optimization problem where we wish to minimize the maximum compression error
across layers and propose an efficient algorithm towards a solution. Our
experiments indicate that our method outperforms existing low-rank compression
approaches across a wide range of networks and data sets. We believe that our
results open up new avenues for future research into the global
performance-size trade-offs of modern neural networks. Our code is available at
https://github.com/lucaslie/torchprune.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1"&gt;Alaa Maalouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_O/0/1/0/all/0/1"&gt;Oren Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1"&gt;Dan Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Math Word Problems from Equations with Topic Controlling and Commonsense Enforcement. (arXiv:2012.07379v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07379</id>
        <link href="http://arxiv.org/abs/2012.07379"/>
        <updated>2021-07-27T02:03:32.630Z</updated>
        <summary type="html"><![CDATA[Recent years have seen significant advancement in text generation tasks with
the help of neural language models. However, there exists a challenging task:
generating math problem text based on mathematical equations, which has made
little progress so far. In this paper, we present a novel equation-to-problem
text generation model. In our model, 1) we propose a flexible scheme to
effectively encode math equations, we then enhance the equation encoder by a
Varitional Autoen-coder (VAE) 2) given a math equation, we perform topic
selection, followed by which a dynamic topic memory mechanism is introduced to
restrict the topic distribution of the generator 3) to avoid commonsense
violation in traditional generation model, we pretrain word embedding with
background knowledge graph (KG), and we link decoded words to related words in
KG, targeted at injecting background knowledge into our model. We evaluate our
model through both automatic metrices and human evaluation, experiments
demonstrate our model outperforms baseline and previous models in both accuracy
and richness of generated problem text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Songge Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansur_M/0/1/0/all/0/1"&gt;Mairgup Mansur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04082</id>
        <link href="http://arxiv.org/abs/2107.04082"/>
        <updated>2021-07-27T02:03:32.622Z</updated>
        <summary type="html"><![CDATA[Language identification greatly impacts the success of downstream tasks such
as automatic speech recognition. Recently, self-supervised speech
representations learned by wav2vec 2.0 have been shown to be very effective for
a range of speech tasks. We extend previous self-supervised work on language
identification by experimenting with pre-trained models which were learned on
real-world unconstrained speech in multiple languages and not just on English.
We show that models pre-trained on many languages perform better and enable
language identification systems that require very little labeled data to
perform well. Results on a 25 languages setup show that with only 10 minutes of
labeled data per language, a cross-lingually pre-trained model can achieve over
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1"&gt;Andros Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1"&gt;Diptanu Gon Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kritika Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1"&gt;Assaf Sela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications. (arXiv:2107.11750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11750</id>
        <link href="http://arxiv.org/abs/2107.11750"/>
        <updated>2021-07-27T02:03:32.602Z</updated>
        <summary type="html"><![CDATA[Uncertainties in machine learning are a significant roadblock for its
application in safety-critical cyber-physical systems (CPS). One source of
uncertainty arises from distribution shifts in the input data between training
and test scenarios. Detecting such distribution shifts in real-time is an
emerging approach to address the challenge. The high dimensional input space in
CPS applications involving imaging adds extra difficulty to the task.
Generative learning models are widely adopted for the task, namely
out-of-distribution (OoD) detection. To improve the state-of-the-art, we
studied existing proposals from both machine learning and CPS fields. In the
latter, safety monitoring in real-time for autonomous driving agents has been a
focus. Exploiting the spatiotemporal correlation of motion in videos, we can
robustly detect hazardous motion around autonomous driving agents. Inspired by
the latest advances in the Variational Autoencoder (VAE) theory and practice,
we tapped into the prior knowledge in data to further boost OoD detection's
robustness. Comparison studies over nuScenes and Synthia data sets show our
methods significantly improve detection capabilities of OoD factors unique to
driving scenarios, 42% better than state-of-the-art approaches. Our model also
generalized near-perfectly, 97% better than the state-of-the-art across the
real-world and simulation driving data sets experimented. Finally, we
customized one proposed method into a twin-encoder model that can be deployed
to resource limited embedded devices for real-time OoD detection. Its execution
time was reduced over four times in low-precision 8-bit integer inference,
while detection capability is comparable to its corresponding floating-point
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yeli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1"&gt;Daniel Jun Xian Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1"&gt;Arvind Easwaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03277</id>
        <link href="http://arxiv.org/abs/2102.03277"/>
        <updated>2021-07-27T02:03:32.594Z</updated>
        <summary type="html"><![CDATA[The Minimum Linear Arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to distinct integers that minimizes
$\sum_{\{u,v\}\in E}|\pi(u) - \pi(v)|$. In that setting, vertices are often
assumed to lie on a horizontal line and edges are drawn as semicircles above
said line. For trees, various algorithms are available to solve the problem in
polynomial time in $n=|V|$. There exist variants of the MLA in which the
arrangements are constrained. Iordanskii, and later Hochberg and Stallmann
(HS), put forward $O(n)$-time algorithms that solve the problem when
arrangements are constrained to be planar (also known as one-page book
embeddings). We also consider linear arrangements of rooted trees that are
constrained to be projective (planar embeddings where the root is not covered
by any edge). Gildea and Temperley (GT) sketched an algorithm for projective
arrangements which they claimed runs in $O(n)$ but did not provide any
justification of its cost. In contrast, Park and Levy claimed that GT's
algorithm runs in $O(n \log d_{max})$ where $d_{max}$ is the maximum degree but
did not provide sufficient detail. Here we correct an error in HS's algorithm
for the planar case, show its relationship with the projective case, and derive
simple algorithms for the projective and planar cases that run undoubtlessly in
$O(n)$-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1"&gt;Llu&amp;#xed;s Alemany-Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1"&gt;Juan Luis Esteban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1"&gt;Ramon Ferrer-i-Cancho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11707</id>
        <link href="http://arxiv.org/abs/2107.11707"/>
        <updated>2021-07-27T02:03:32.583Z</updated>
        <summary type="html"><![CDATA[Video captioning is one of the challenging problems at the intersection of
vision and language, having many real-life applications in video retrieval,
video surveillance, assisting visually challenged people, Human-machine
interface, and many more. Recent deep learning-based methods have shown
promising results but are still on the lower side than other vision tasks (such
as image classification, object detection). A significant drawback with
existing video captioning methods is that they are optimized over cross-entropy
loss function, which is uncorrelated to the de facto evaluation metrics (BLEU,
METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate
of the true loss function for video captioning. This paper addresses the
drawback by introducing a dynamic loss network (DLN), which provides an
additional feedback signal that directly reflects the evaluation metrics. Our
results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to
Text (MSRVTT) datasets outperform previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1"&gt;Nasibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1"&gt;Partha Pratim Mohanta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos. (arXiv:2107.11756v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11756</id>
        <link href="http://arxiv.org/abs/2107.11756"/>
        <updated>2021-07-27T02:03:32.576Z</updated>
        <summary type="html"><![CDATA[Given a video demonstration, can we imitate the action contained in this
video? In this paper, we introduce a novel task, dubbed mesh-based action
imitation. The goal of this task is to enable an arbitrary target human mesh to
perform the same action shown on the video demonstration. To achieve this, a
novel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI
first learns to reconstruct the meshes from the given source image frames, then
the initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence
smooth module proposed by us, to improve the temporal consistency. Finally, we
imitate the actions by transferring the pose from the constructed human body to
our target identity mesh. High-quality and detailed human body meshes can be
generated by using our M-VAI. Extensive experiments demonstrate the feasibility
of our task and the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu-Gang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal-wise Attention Spiking Neural Networks for Event Streams Classification. (arXiv:2107.11711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11711</id>
        <link href="http://arxiv.org/abs/2107.11711"/>
        <updated>2021-07-27T02:03:32.569Z</updated>
        <summary type="html"><![CDATA[How to effectively and efficiently deal with spatio-temporal event streams,
where the events are generally sparse and non-uniform and have the microsecond
temporal resolution, is of great value and has various real-life applications.
Spiking neural network (SNN), as one of the brain-inspired event-triggered
computing models, has the potential to extract effective spatio-temporal
features from the event streams. However, when aggregating individual events
into frames with a new higher temporal resolution, existing SNN models do not
attach importance to that the serial frames have different signal-to-noise
ratios since event streams are sparse and non-uniform. This situation
interferes with the performance of existing SNNs. In this work, we propose a
temporal-wise attention SNN (TA-SNN) model to learn frame-based representation
for processing event streams. Concretely, we extend the attention concept to
temporal-wise input to judge the significance of frames for the final decision
at the training stage, and discard the irrelevant frames at the inference
stage. We demonstrate that TA-SNN models improve the accuracy of event streams
classification tasks. We also study the impact of multiple-scale temporal
resolutions for frame-based representation. Our approach is tested on three
different classification tasks: gesture recognition, image classification, and
spoken digit recognition. We report the state-of-the-art results on these
tasks, and get the essential improvement of accuracy (almost 19\%) for gesture
recognition with only 60 ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Man Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1"&gt;Huanhuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangshe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dingheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yihan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhaoxu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoqi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Attention and Scale Complementary Network for Instance Segmentation in Remote Sensing Images. (arXiv:2107.11758v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11758</id>
        <link href="http://arxiv.org/abs/2107.11758"/>
        <updated>2021-07-27T02:03:32.551Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on the challenging multicategory instance
segmentation problem in remote sensing images (RSIs), which aims at predicting
the categories of all instances and localizing them with pixel-level masks.
Although many landmark frameworks have demonstrated promising performance in
instance segmentation, the complexity in the background and scale variability
instances still remain challenging for instance segmentation of RSIs. To
address the above problems, we propose an end-to-end multi-category instance
segmentation model, namely Semantic Attention and Scale Complementary Network,
which mainly consists of a Semantic Attention (SEA) module and a Scale
Complementary Mask Branch (SCMB). The SEA module contains a simple fully
convolutional semantic segmentation branch with extra supervision to strengthen
the activation of interest instances on the feature map and reduce the
background noise's interference. To handle the under-segmentation of geospatial
instances with large varying scales, we design the SCMB that extends the
original single mask branch to trident mask branches and introduces
complementary mask supervision at different scales to sufficiently leverage the
multi-scale information. We conduct comprehensive experiments to evaluate the
effectiveness of our proposed method on the iSAID dataset and the NWPU Instance
Segmentation dataset and achieve promising performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Peng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial training may be a double-edged sword. (arXiv:2107.11671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11671</id>
        <link href="http://arxiv.org/abs/2107.11671"/>
        <updated>2021-07-27T02:03:32.544Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been shown as an effective approach to improve the
robustness of image classifiers against white-box attacks. However, its
effectiveness against black-box attacks is more nuanced. In this work, we
demonstrate that some geometric consequences of adversarial training on the
decision boundary of deep networks give an edge to certain types of black-box
attacks. In particular, we define a metric called robustness gain to show that
while adversarial training is an effective method to dramatically improve the
robustness in white-box scenarios, it may not provide such a good robustness
gain against the more realistic decision-based black-box attacks. Moreover, we
show that even the minimal perturbation white-box attacks can converge faster
against adversarially-trained neural networks compared to the regular ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1"&gt;Ali Rahmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1"&gt;Seyed-Mohsen Moosavi-Dezfooli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Huaiyu Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face Recognition. (arXiv:2107.11721v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11721</id>
        <link href="http://arxiv.org/abs/2107.11721"/>
        <updated>2021-07-27T02:03:32.537Z</updated>
        <summary type="html"><![CDATA[Despite the great success achieved by deep learning methods in face
recognition, severe performance drops are observed for large pose variations in
unconstrained environments (e.g., in cases of surveillance and photo-tagging).
To address it, current methods either deploy pose-specific models or frontalize
faces by additional modules. Still, they ignore the fact that identity
information should be consistent across poses and are not realizing the data
imbalance between frontal and profile face images during training. In this
paper, we propose an efficient PoseFace framework which utilizes the facial
landmarks to disentangle the pose-invariant features and exploits a
pose-adaptive loss to handle the imbalance issue adaptively. Extensive
experimental results on the benchmarks of Multi-PIE, CFP, CPLFW and IJB have
demonstrated the superiority of our method over the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qiang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yunxiao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zezheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chenxu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1"&gt;Zhen Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand Image Understanding via Deep Multi-Task Learning. (arXiv:2107.11646v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11646</id>
        <link href="http://arxiv.org/abs/2107.11646"/>
        <updated>2021-07-27T02:03:32.530Z</updated>
        <summary type="html"><![CDATA[Analyzing and understanding hand information from multimedia materials like
images or videos is important for many real world applications and remains
active in research community. There are various works focusing on recovering
hand information from single image, however, they usually solve a single task,
for example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh
reconstruction and perform not well in challenging scenarios. To further
improve the performance of these tasks, we propose a novel Hand Image
Understanding (HIU) framework to extract comprehensive information of the hand
object from a single RGB image, by jointly considering the relationships
between these tasks. To achieve this goal, a cascaded multi-task learning (MTL)
backbone is designed to estimate the 2D heat maps, to learn the segmentation
mask, and to generate the intermediate 3D information encoding, followed by a
coarse-to-fine learning paradigm and a self-supervised learning strategy.
Qualitative experiments demonstrate that our approach is capable of recovering
reasonable mesh representations even in challenging situations. Quantitatively,
our method significantly outperforms the state-of-the-art approaches on various
widely-used datasets, in terms of diverse evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1"&gt;Zhang Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hongsheng_H/0/1/0/all/0/1"&gt;Huang Hongsheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jianchao_T/0/1/0/all/0/1"&gt;Tan Jianchao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hongmin_X/0/1/0/all/0/1"&gt;Xu Hongmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guozhu_P/0/1/0/all/0/1"&gt;Peng Guozhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Liu Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Real Use Case of Semi-Supervised Learning for Mammogram Classification in a Local Clinic of Costa Rica. (arXiv:2107.11696v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11696</id>
        <link href="http://arxiv.org/abs/2107.11696"/>
        <updated>2021-07-27T02:03:32.522Z</updated>
        <summary type="html"><![CDATA[The implementation of deep learning based computer aided diagnosis systems
for the classification of mammogram images can help in improving the accuracy,
reliability, and cost of diagnosing patients. However, training a deep learning
model requires a considerable amount of labeled images, which can be expensive
to obtain as time and effort from clinical practitioners is required. A number
of publicly available datasets have been built with data from different
hospitals and clinics. However, using models trained on these datasets for
later work on images sampled from a different hospital or clinic might result
in lower performance. This is due to the distribution mismatch of the datasets,
which include different patient populations and image acquisition protocols.
The scarcity of labeled data can also bring a challenge towards the application
of transfer learning with models trained using these source datasets. In this
work, a real world scenario is evaluated where a novel target dataset sampled
from a private Costa Rican clinic is used, with few labels and heavily
imbalanced data. The use of two popular and publicly available datasets
(INbreast and CBIS-DDSM) as source data, to train and test the models on the
novel target dataset, is evaluated. The use of the semi-supervised deep
learning approach known as MixMatch, to leverage the usage of unlabeled data
from the target dataset, is proposed and evaluated. In the tests, the
performance of models is extensively measured, using different metrics to
assess the performance of a classifier under heavy data imbalance conditions.
It is shown that the use of semi-supervised deep learning combined with
fine-tuning can provide a meaningful advantage when using scarce labeled
observations. We make available the novel dataset for the benefit of the
community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1"&gt;Saul Calderon-Ramirez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murillo_Hernandez_D/0/1/0/all/0/1"&gt;Diego Murillo-Hernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rojas_Salazar_K/0/1/0/all/0/1"&gt;Kevin Rojas-Salazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Elizondo_D/0/1/0/all/0/1"&gt;David Elizondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shengxiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Molina_Cabello_M/0/1/0/all/0/1"&gt;Miguel Molina-Cabello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank & Sort Loss for Object Detection and Instance Segmentation. (arXiv:2107.11669v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11669</id>
        <link href="http://arxiv.org/abs/2107.11669"/>
        <updated>2021-07-27T02:03:32.503Z</updated>
        <summary type="html"><![CDATA[We propose Rank & Sort (RS) Loss, as a ranking-based loss function to train
deep object detection and instance segmentation methods (i.e. visual
detectors). RS Loss supervises the classifier, a sub-network of these methods,
to rank each positive above all negatives as well as to sort positives among
themselves with respect to (wrt.) their continuous localisation qualities (e.g.
Intersection-over-Union - IoU). To tackle the non-differentiable nature of
ranking and sorting, we reformulate the incorporation of error-driven update
with backpropagation as Identity Update, which enables us to model our novel
sorting error among positives. With RS Loss, we significantly simplify
training: (i) Thanks to our sorting objective, the positives are prioritized by
the classifier without an additional auxiliary head (e.g. for centerness, IoU,
mask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class
imbalance, and thus, no sampling heuristic is required, and (iii) we address
the multi-task nature of visual detectors using tuning-free task-balancing
coefficients. Using RS Loss, we train seven diverse visual detectors only by
tuning the learning rate, and show that it consistently outperforms baselines:
e.g. our RS Loss improves (i) Faster R-CNN by ~ 3 box AP and aLRP Loss
(ranking-based baseline) by ~ 2 box AP on COCO dataset, (ii) Mask R-CNN with
repeat factor sampling (RFS) by 3.5 mask AP (~ 7 AP for rare classes) on LVIS
dataset; and also outperforms all counterparts. Code available at
https://github.com/kemaloksuz/RankSortLoss]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1"&gt;Kemal Oksuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1"&gt;Baris Can Cam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1"&gt;Sinan Kalkan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for Text Summarization. (arXiv:2104.07605v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07605</id>
        <link href="http://arxiv.org/abs/2104.07605"/>
        <updated>2021-07-27T02:03:32.450Z</updated>
        <summary type="html"><![CDATA[Novel neural architectures, training strategies, and the availability of
large-scale corpora haven been the driving force behind recent progress in
abstractive text summarization. However, due to the black-box nature of neural
models, uninformative evaluation metrics, and scarce tooling for model and data
analysis, the true performance and failure modes of summarization models remain
largely unknown. To address this limitation, we introduce SummVis, an
open-source tool for visualizing abstractive summaries that enables
fine-grained analysis of the models, data, and evaluation metrics associated
with text summarization. Through its lexical and semantic visualizations, the
tools offers an easy entry point for in-depth model prediction exploration
across important dimensions such as factual consistency or abstractiveness. The
tool together with several pre-computed model outputs is available at
https://github.com/robustness-gym/summvis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1"&gt;Jesse Vig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1"&gt;Wojciech Kry&amp;#x15b;ci&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1"&gt;Karan Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1"&gt;Nazneen Fatema Rajani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Term-community-based topic detection with variable resolution. (arXiv:2103.13550v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13550</id>
        <link href="http://arxiv.org/abs/2103.13550"/>
        <updated>2021-07-27T02:03:32.422Z</updated>
        <summary type="html"><![CDATA[Network-based procedures for topic detection in huge text collections offer
an intuitive alternative to probabilistic topic models. We present in detail a
method that is especially designed with the requirements of domain experts in
mind. Like similar methods, it employs community detection in term
co-occurrence graphs, but it is enhanced by including a resolution parameter
that can be used for changing the targeted topic granularity. We also establish
a term ranking and use semantic word-embedding for presenting term communities
in a way that facilitates their interpretation. We demonstrate the application
of our method with a widely used corpus of general news articles and show the
results of detailed social-sciences expert evaluations of detected topics at
various resolutions. A comparison with topics detected by Latent Dirichlet
Allocation is also included. Finally, we discuss factors that influence topic
interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_A/0/1/0/all/0/1"&gt;Andreas Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Odrowski_S/0/1/0/all/0/1"&gt;Simon Odrowski&lt;/a&gt; (German Aerospace Center DLR)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09604</id>
        <link href="http://arxiv.org/abs/2102.09604"/>
        <updated>2021-07-27T02:03:32.404Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) are a powerful architecture for
representation learning on documents that naturally occur as graphs, e.g.,
citation or social networks. However, sensitive personal information, such as
documents with people's profiles or relationships as edges, are prone to
privacy leaks, as the trained model might reveal the original input. Although
differential privacy (DP) offers a well-founded privacy-preserving framework,
GCNs pose theoretical and practical challenges due to their training specifics.
We address these challenges by adapting differentially-private gradient-based
training to GCNs and conduct experiments using two optimizers on five NLP
datasets in two languages. We propose a simple yet efficient method based on
random graph splits that not only improves the baseline privacy bounds by a
factor of 2.7 while retaining competitive F1 scores, but also provides strong
privacy guarantees of epsilon = 1.0. We show that, under certain modeling
choices, privacy-preserving GCNs perform up to 90% of their non-private
variants, while formally guaranteeing strong privacy measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1"&gt;Timour Igamberdiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1"&gt;Ivan Habernal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examination of Community Sentiment Dynamics due to COVID-19 Pandemic: A Case Study from A State in Australia. (arXiv:2006.12185v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12185</id>
        <link href="http://arxiv.org/abs/2006.12185"/>
        <updated>2021-07-27T02:03:32.386Z</updated>
        <summary type="html"><![CDATA[The outbreak of the novel Coronavirus Disease 2019 (COVID-19) has caused
unprecedented impacts to people's daily life around the world. Various measures
and policies such as lockdown and social-distancing are implemented by
governments to combat the disease during the pandemic period. These measures
and policies as well as virus itself may cause different mental health issues
to people such as depression, anxiety, sadness, etc. In this paper, we exploit
the massive text data posted by Twitter users to analyse the sentiment dynamics
of people living in the state of New South Wales (NSW) in Australia during the
pandemic period. Different from the existing work that mostly focuses the
country-level and static sentiment analysis, we analyse the sentiment dynamics
at the fine-grained local government areas (LGAs). Based on the analysis of
around 94 million tweets that posted by around 183 thousand users located at
different LGAs in NSW in five months, we found that people in NSW showed an
overall positive sentimental polarity and the COVID-19 pandemic decreased the
overall positive sentimental polarity during the pandemic period. The
fine-grained analysis of sentiment in LGAs found that despite the dominant
positive sentiment most of days during the study period, some LGAs experienced
significant sentiment changes from positive to negative. This study also
analysed the sentimental dynamics delivered by the hot topics in Twitter such
as government policies (e.g. the Australia's JobKeeper program, lockdown,
social-distancing) as well as the focused social events (e.g. the Ruby Princess
Cruise). The results showed that the policies and events did affect people's
overall sentiment, and they affected people's overall sentiment differently at
different stages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianlong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuiqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Sparse Experience Replay for Lifelong Language Learning. (arXiv:2009.04891v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04891</id>
        <link href="http://arxiv.org/abs/2009.04891"/>
        <updated>2021-07-27T02:03:32.377Z</updated>
        <summary type="html"><![CDATA[Lifelong learning requires models that can continuously learn from sequential
streams of data without suffering catastrophic forgetting due to shifts in data
distributions. Deep learning models have thrived in the non-sequential learning
paradigm; however, when used to learn a sequence of tasks, they fail to retain
past knowledge and learn incrementally. We propose a novel approach to lifelong
learning of language tasks based on meta-learning with sparse experience replay
that directly optimizes to prevent forgetting. We show that under the realistic
setting of performing a single pass on a stream of tasks and without any task
identifiers, our method obtains state-of-the-art results on lifelong text
classification and relation extraction. We analyze the effectiveness of our
approach and further demonstrate its low computational and space complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1"&gt;Nithin Holla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1"&gt;Pushkar Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1"&gt;Helen Yannakoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1"&gt;Ekaterina Shutova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Sociodemographic Selection Biases for Population Prediction from Social Media. (arXiv:1911.03855v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03855</id>
        <link href="http://arxiv.org/abs/1911.03855"/>
        <updated>2021-07-27T02:03:32.356Z</updated>
        <summary type="html"><![CDATA[Social media is increasingly used for large-scale population predictions,
such as estimating community health statistics. However, social media users are
not typically a representative sample of the intended population -- a
"selection bias". Within the social sciences, such a bias is typically
addressed with restratification techniques, where observations are reweighted
according to how under- or over-sampled their socio-demographic groups are.
Yet, restratifaction is rarely evaluated for improving prediction. Across four
tasks of predicting U.S. county population health statistics from Twitter, we
find standard restratification techniques provide no improvement and often
degrade prediction accuracies. The core reasons for this seems to be both
shrunken estimates (reduced variance of model predicted values) and sparse
estimates of each population's socio-demographics. We thus develop and evaluate
three methods to address these problems: estimator redistribution to account
for shrinking, and adaptive binning and informed smoothing to handle sparse
socio-demographic estimates. We show that each of these methods significantly
outperforms the standard restratification approaches. Combining approaches, we
find substantial improvements over non-restratified models, yielding a 53.0%
increase in predictive accuracy (R^2) in the case of surveyed life
satisfaction, and a 17.8% average increase across all tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1"&gt;Salvatore Giorgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lynn_V/0/1/0/all/0/1"&gt;Veronica Lynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Keshav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1"&gt;Farhan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matz_S/0/1/0/all/0/1"&gt;Sandra Matz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1"&gt;Lyle Ungar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1"&gt;H. Andrew Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03844</id>
        <link href="http://arxiv.org/abs/2107.03844"/>
        <updated>2021-07-27T02:03:32.347Z</updated>
        <summary type="html"><![CDATA[Bangla -- ranked as the 6th most widely spoken language across the world
(https://www.ethnologue.com/guides/ethnologue200), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Arid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvirul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1"&gt;Janntatul Tajrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naira Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14353</id>
        <link href="http://arxiv.org/abs/2012.14353"/>
        <updated>2021-07-27T02:03:32.339Z</updated>
        <summary type="html"><![CDATA[The exponential growths of social media and micro-blogging sites not only
provide platforms for empowering freedom of expressions and individual voices,
but also enables people to express anti-social behavior like online harassment,
cyberbullying, and hate speech. Numerous works have been proposed to utilize
textual data for social and anti-social behavior analysis, by predicting the
contexts mostly for highly-resourced languages like English. However, some
languages are under-resourced, e.g., South Asian languages like Bengali, that
lack computational resources for accurate natural language processing (NLP). In
this paper, we propose an explainable approach for hate speech detection from
the under-resourced Bengali language, which we called DeepHateExplainer.
Bengali texts are first comprehensively preprocessed, before classifying them
into political, personal, geopolitical, and religious hates using a neural
ensemble method of transformer-based neural architectures (i.e., monolingual
Bangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa).
Important~(most and least) terms are then identified using sensitivity analysis
and layer-wise relevance propagation~(LRP), before providing
human-interpretable explanations. Finally, we compute comprehensiveness and
sufficiency scores to measure the quality of explanations w.r.t faithfulness.
Evaluations against machine learning~(linear and tree-based models) and neural
networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines
yield F1-scores of 78%, 91%, 89%, and 84%, for political, personal,
geopolitical, and religious hates, respectively, outperforming both ML and DNN
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Md. Rezaul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1"&gt;Sumon Kanti Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tanhim Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1"&gt;Sagor Sarker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1"&gt;Mehadi Hasan Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1"&gt;Kabir Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md. Azam Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1"&gt;Stefan Decker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Emotion Prediction by Modeling Emotion Definitions. (arXiv:2107.12135v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12135</id>
        <link href="http://arxiv.org/abs/2107.12135"/>
        <updated>2021-07-27T02:03:32.322Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new framework for fine-grained emotion prediction
in the text through emotion definition modeling. Our approach involves a
multi-task learning framework that models definitions of emotions as an
auxiliary task while being trained on the primary task of emotion prediction.
We model definitions using masked language modeling and class definition
prediction tasks. Our models outperform existing state-of-the-art for
fine-grained emotion dataset GoEmotions. We further show that this trained
model can be used for transfer learning on other benchmark datasets in emotion
prediction with varying emotion label sets, domains, and sizes. The proposed
models outperform the baselines on transfer learning experiments demonstrating
the generalization capability of the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gargi Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brahma_D/0/1/0/all/0/1"&gt;Dhanajit Brahma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1"&gt;Piyush Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1"&gt;Ashutosh Modi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12573</id>
        <link href="http://arxiv.org/abs/2012.12573"/>
        <updated>2021-07-27T02:03:32.315Z</updated>
        <summary type="html"><![CDATA[Health literacy has emerged as a crucial factor in making appropriate health
decisions and ensuring treatment outcomes. However, medical jargon and the
complex structure of professional language in this domain make health
information especially hard to interpret. Thus, there is an urgent unmet need
for automated methods to enhance the accessibility of the biomedical literature
to the general population. This problem can be framed as a type of translation
problem between the language of healthcare professionals, and that of the
general public. In this paper, we introduce the novel task of automated
generation of lay language summaries of biomedical scientific reviews, and
construct a dataset to support the development and evaluation of automated
methods through which to enhance the accessibility of the biomedical
literature. We conduct analyses of the various challenges in solving this task,
including not only summarization of the key points but also explanation of
background knowledge and simplification of professional language. We experiment
with state-of-the-art summarization models as well as several data augmentation
techniques, and evaluate their performance using both automated metrics and
human assessment. Results indicate that automatically generated summaries
produced using contemporary neural architectures can achieve promising quality
and readability as compared with reference summaries developed for the lay
public by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score
of 13.30). We also discuss the limitations of the current attempt, providing
insights and directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yue Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Wei Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Trevor Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-07-27T02:03:32.306Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via the moral philosophy definition of social good, propose a
framework to evaluate the direct and indirect real-world impact of NLP tasks,
and adopt the methodology of global priorities research to identify priority
causes for NLP research. Finally, we use our theoretical framework to provide
some practical guidelines for future NLP research for social good. Our data and
code are available at this http URL In
addition, we curate a list of papers and resources on NLP for social good at
https://github.com/zhijing-jin/NLP4SocialGood_Papers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. (arXiv:2104.13346v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13346</id>
        <link href="http://arxiv.org/abs/2104.13346"/>
        <updated>2021-07-27T02:03:32.298Z</updated>
        <summary type="html"><![CDATA[Modern summarization models generate highly fluent but often factually
unreliable outputs. This motivated a surge of metrics attempting to measure the
factuality of automatically generated summaries. Due to the lack of common
benchmarks, these metrics cannot be compared. Moreover, all these methods treat
factuality as a binary concept and fail to provide deeper insights into the
kinds of inconsistencies made by different systems. To address these
limitations, we devise a typology of factual errors and use it to collect human
annotations of generated summaries from state-of-the-art summarization systems
for the CNN/DM and XSum datasets. Through these annotations, we identify the
proportion of different categories of factual errors in various summarization
models and benchmark factuality metrics, showing their correlation with human
judgment as well as their specific strengths and weaknesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pagnoni_A/0/1/0/all/0/1"&gt;Artidoro Pagnoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1"&gt;Vidhisha Balachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?. (arXiv:2012.15180v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15180</id>
        <link href="http://arxiv.org/abs/2012.15180"/>
        <updated>2021-07-27T02:03:32.291Z</updated>
        <summary type="html"><![CDATA[Do state-of-the-art natural language understanding models care about word
order - one of the most important characteristics of a sequence? Not always! We
found 75% to 90% of the correct predictions of BERT-based classifiers, trained
on many GLUE tasks, remain constant after input words are randomly shuffled.
Despite BERT embeddings are famously contextual, the contribution of each
individual word to downstream tasks is almost unchanged even after the word's
context is shuffled. BERT-based models are able to exploit superficial cues
(e.g. the sentiment of keywords in sentiment analysis; or the word-wise
similarity between sequence-pair inputs in natural language inference) to make
correct decisions when tokens are arranged in random orders. Encouraging
classifiers to capture word order information improves the performance on most
GLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE
tasks are not challenging machines to understand the meaning of a sentence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1"&gt;Thang M. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1"&gt;Long Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DYPLODOC: Dynamic Plots for Document Classification. (arXiv:2107.12226v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12226</id>
        <link href="http://arxiv.org/abs/2107.12226"/>
        <updated>2021-07-27T02:03:32.283Z</updated>
        <summary type="html"><![CDATA[Narrative generation and analysis are still on the fringe of modern natural
language processing yet are crucial in a variety of applications. This paper
proposes a feature extraction method for plot dynamics. We present a dataset
that consists of the plot descriptions for thirteen thousand TV shows alongside
meta-information on their genres and dynamic plots extracted from them. We
validate the proposed tool for plot dynamics extraction and discuss possible
applications of this method to the tasks of narrative analysis and generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malysheva_A/0/1/0/all/0/1"&gt;Anastasia Malysheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1"&gt;Ivan P. Yamshchikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05297</id>
        <link href="http://arxiv.org/abs/2001.05297"/>
        <updated>2021-07-27T02:03:32.276Z</updated>
        <summary type="html"><![CDATA[This paper addresses a series of complex and unresolved issues in the
historical phonology of West Iranian languages. The West Iranian languages
(Persian, Kurdish, Balochi, and other languages) display a high degree of
non-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to
language contact; we argue, however, that an oversimplified view of the
processes at work has prevailed in the literature on West Iranian dialectology,
with specialists assuming that deviations from an expected outcome in a given
non-Persian language are due to lexical borrowing from some chronological stage
of Persian. It is demonstrated that this qualitative approach yields at times
problematic conclusions stemming from the lack of explicit probabilistic
inferences regarding the distribution of the data: Persian may not be the sole
donor language; additionally, borrowing at the lexical level is not always the
mechanism that introduces irregularity. In many cases, the possibility that
West Iranian languages show different reflexes in different conditioning
environments remains under-explored. We employ a novel Bayesian approach
designed to overcome these problems and tease apart the different determinants
of irregularity in patterns of West Iranian sound change. Our methodology
allows us to provisionally resolve a number of outstanding questions in the
literature on West Iranian dialectology concerning the dialectal affiliation of
certain sound changes. We outline future directions for work of this sort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1"&gt;Chundra Aroor Cathcart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction. (arXiv:2107.12214v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12214</id>
        <link href="http://arxiv.org/abs/2107.12214"/>
        <updated>2021-07-27T02:03:32.266Z</updated>
        <summary type="html"><![CDATA[Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA
which outputs triplets of an aspect target, its associated sentiment, and the
corresponding opinion term. Recent models perform the triplet extraction in an
end-to-end manner but heavily rely on the interactions between each target word
and opinion word. Thereby, they cannot perform well on targets and opinions
which contain multiple words. Our proposed span-level approach explicitly
considers the interaction between the whole spans of targets and opinions when
predicting their sentiment relation. Thus, it can make predictions with the
semantics of whole spans, ensuring better sentiment consistency. To ease the
high computational cost caused by span enumeration, we propose a dual-channel
span pruning strategy by incorporating supervision from the Aspect Term
Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not
only improves computational efficiency but also distinguishes the opinion and
target spans more properly. Our framework simultaneously achieves strong
performance for the ASTE as well as ATE and OTE tasks. In particular, our
analysis shows that our span-level approach achieves more significant
improvements over the baselines on triplets with multi-word targets or
opinions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1"&gt;Yew Ken Chia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1"&gt;Lidong Bing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Negation in Neural Machine Translation. (arXiv:2107.12203v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12203</id>
        <link href="http://arxiv.org/abs/2107.12203"/>
        <updated>2021-07-27T02:03:32.239Z</updated>
        <summary type="html"><![CDATA[In this paper, we evaluate the translation of negation both automatically and
manually, in English--German (EN--DE) and English--Chinese (EN--ZH). We show
that the ability of neural machine translation (NMT) models to translate
negation has improved with deeper and more advanced networks, although the
performance varies between language pairs and translation directions. The
accuracy of manual evaluation in EN-DE, DE-EN, EN-ZH, and ZH-EN is 95.7%,
94.8%, 93.4%, and 91.7%, respectively. In addition, we show that
under-translation is the most significant error type in NMT, which contrasts
with the more diverse error profile previously observed for statistical machine
translation. To better understand the root of the under-translation of
negation, we study the model's information flow and training data. While our
information flow analysis does not reveal any deficiencies that could be used
to detect or fix the under-translation of negation, we find that negation is
often rephrased during training, which could make it more difficult for the
model to learn a reliable link between source and target negation. We finally
conduct intrinsic analysis and extrinsic probing tasks on negation, showing
that NMT models can distinguish negation and non-negation tokens very well and
encode a lot of information about negation in hidden states but nevertheless
leave room for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Gongbo Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronchen_P/0/1/0/all/0/1"&gt;Philipp R&amp;#xf6;nchen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1"&gt;Rico Sennrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nivre_J/0/1/0/all/0/1"&gt;Joakim Nivre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Language Model for Efficient Linguistic Steganalysis: An Empirical Study. (arXiv:2107.12168v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12168</id>
        <link href="http://arxiv.org/abs/2107.12168"/>
        <updated>2021-07-27T02:03:32.217Z</updated>
        <summary type="html"><![CDATA[Recent advances in linguistic steganalysis have successively applied CNNs,
RNNs, GNNs and other deep learning models for detecting secret information in
generative texts. These methods tend to seek stronger feature extractors to
achieve higher steganalysis effects. However, we have found through experiments
that there actually exists significant difference between automatically
generated steganographic texts and carrier texts in terms of the conditional
probability distribution of individual words. Such kind of statistical
difference can be naturally captured by the language model used for generating
steganographic texts, which drives us to give the classifier a priori knowledge
of the language model to enhance the steganalysis ability. To this end, we
present two methods to efficient linguistic steganalysis in this paper. One is
to pre-train a language model based on RNN, and the other is to pre-train a
sequence autoencoder. Experimental results show that the two methods have
different degrees of performance improvement when compared to the randomly
initialized RNN classifier, and the convergence speed is significantly
accelerated. Moreover, our methods have achieved the best detection results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1"&gt;Biao Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanzhou Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1"&gt;Guorui Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinpeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On embedding Lambek calculus into commutative categorial grammars. (arXiv:2005.10058v3 [math.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10058</id>
        <link href="http://arxiv.org/abs/2005.10058"/>
        <updated>2021-07-27T02:03:32.187Z</updated>
        <summary type="html"><![CDATA[We consider tensor grammars, which are an example of \commutative" grammars,
based on the classical (rather than intuitionistic) linear logic. They can be
seen as a surface representation of abstract categorial grammars ACG in the
sense that derivations of ACG translate to derivations of tensor grammars and
this translation is isomorphic on the level of string languages. The basic
ingredient are tensor terms, which can be seen as encoding and generalizing
proof-nets. Using tensor terms makes the syntax extremely simple and a direct
geometric meaning becomes transparent. Then we address the problem of encoding
noncommutative operations in our setting. This turns out possible after
enriching the system with new unary operators. The resulting system allows
representing both ACG and Lambek grammars as conservative fragments, while the
formalism remains, as it seems to us, rather simple and intuitive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Slavnov_S/0/1/0/all/0/1"&gt;Sergey Slavnov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Coreference Resolution with Harmonized Annotations. (arXiv:2107.12088v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12088</id>
        <link href="http://arxiv.org/abs/2107.12088"/>
        <updated>2021-07-27T02:03:32.180Z</updated>
        <summary type="html"><![CDATA[In this paper, we present coreference resolution experiments with a newly
created multilingual corpus CorefUD. We focus on the following languages:
Czech, Russian, Polish, German, Spanish, and Catalan. In addition to
monolingual experiments, we combine the training data in multilingual
experiments and train two joined models -- for Slavic languages and for all the
languages together. We rely on an end-to-end deep learning model that we
slightly adapted for the CorefUD corpus. Our results show that we can profit
from harmonized annotations, and using joined models helps significantly for
the languages with smaller training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Pra&amp;#x17e;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1"&gt;Miloslav Konop&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1"&gt;Jakub Sido&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis of LIME for Text Data. (arXiv:2010.12487v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12487</id>
        <link href="http://arxiv.org/abs/2010.12487"/>
        <updated>2021-07-27T02:03:32.173Z</updated>
        <summary type="html"><![CDATA[Text data are increasingly handled in an automated fashion by machine
learning algorithms. But the models handling these data are not always
well-understood due to their complexity and are more and more often referred to
as "black-boxes." Interpretability methods aim to explain how these models
operate. Among them, LIME has become one of the most popular in recent years.
However, it comes without theoretical guarantees: even for simple models, we
are not sure that LIME behaves accurately. In this paper, we provide a first
theoretical analysis of LIME for text data. As a consequence of our theoretical
findings, we show that LIME indeed provides meaningful explanations for simple
models, namely decision trees and linear models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mardaoui_D/0/1/0/all/0/1"&gt;Dina Mardaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1"&gt;Damien Garreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based End-to-End Speech Recognition with Local Dense Synthesizer Attention. (arXiv:2010.12155v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12155</id>
        <link href="http://arxiv.org/abs/2010.12155"/>
        <updated>2021-07-27T02:03:32.165Z</updated>
        <summary type="html"><![CDATA[Recently, several studies reported that dot-product selfattention (SA) may
not be indispensable to the state-of-theart Transformer models. Motivated by
the fact that dense synthesizer attention (DSA), which dispenses with dot
products and pairwise interactions, achieved competitive results in many
language processing tasks, in this paper, we first propose a DSA-based speech
recognition, as an alternative to SA. To reduce the computational complexity
and improve the performance, we further propose local DSA (LDSA) to restrict
the attention scope of DSA to a local range around the current central frame
for speech recognition. Finally, we combine LDSA with SA to extract the local
and global information simultaneously. Experimental results on the Ai-shell1
Mandarine speech recognition corpus show that the proposed LDSA-Transformer
achieves a character error rate (CER) of 6.49%, which is slightly better than
that of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less
computation than the SATransformer. The proposed combination method not only
achieves a CER of 6.18%, which significantly outperforms the SA-Transformer,
but also has roughly the same number of parameters and computational complexity
as the latter. The implementation of the multi-head LDSA is available at
https://github.com/mlxu995/multihead-LDSA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Menglong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12220</id>
        <link href="http://arxiv.org/abs/2107.12220"/>
        <updated>2021-07-27T02:03:32.146Z</updated>
        <summary type="html"><![CDATA[When humans solve complex problems, they rarely come up with a decision
right-away. Instead, they start with an intuitive decision, reflect upon it,
spot mistakes, resolve contradictions and jump between different hypotheses.
Thus, they create a sequence of ideas and follow a train of thought that
ultimately reaches a conclusive decision. Contrary to this, today's neural
classification models are mostly trained to map an input to one single and
fixed output. In this paper, we investigate how we can give models the
opportunity of a second, third and $k$-th thought. We take inspiration from
Hegel's dialectics and propose a method that turns an existing classifier's
class prediction (such as the image class forest) into a sequence of
predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom).
Concretely, we propose a correction module that is trained to estimate the
model's correctness as well as an iterative prediction update based on the
prediction's gradient. Our approach results in a dynamic system over class
probability distributions $\unicode{x2014}$ the thought flow. We evaluate our
method on diverse datasets and tasks from computer vision and natural language
processing. We observe surprisingly complex but intuitive behavior and
demonstrate that our method (i) can correct misclassifications, (ii)
strengthens model performance, (iii) is robust to high levels of adversarial
attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift
setting and (iv) provides a tool for model interpretability that uncovers model
knowledge which otherwise remains invisible in a single distribution
prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1"&gt;Hendrik Schuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1"&gt;Heike Adel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1"&gt;Ngoc Thang Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning AI With Shared Human Values. (arXiv:2008.02275v5 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02275</id>
        <link href="http://arxiv.org/abs/2008.02275"/>
        <updated>2021-07-27T02:03:32.139Z</updated>
        <summary type="html"><![CDATA[We show how to assess a language model's knowledge of basic concepts of
morality. We introduce the ETHICS dataset, a new benchmark that spans concepts
in justice, well-being, duties, virtues, and commonsense morality. Models
predict widespread moral judgments about diverse text scenarios. This requires
connecting physical and social world knowledge to value judgements, a
capability that may enable us to steer chatbot outputs or eventually regularize
open-ended reinforcement learning agents. With the ETHICS dataset, we find that
current language models have a promising but incomplete ability to predict
basic human ethical judgements. Our work shows that progress can be made on
machine ethics today, and it provides a steppingstone toward AI that is aligned
with human values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1"&gt;Collin Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1"&gt;Andrew Critch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval. (arXiv:2107.11976v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11976</id>
        <link href="http://arxiv.org/abs/2107.11976"/>
        <updated>2021-07-27T02:03:32.011Z</updated>
        <summary type="html"><![CDATA[We present CORA, a Cross-lingual Open-Retrieval Answer Generation model that
can answer questions across many languages even when language-specific
annotated data or knowledge sources are unavailable. We introduce a new dense
passage retrieval algorithm that is trained to retrieve documents across
languages for a question. Combined with a multilingual autoregressive
generation model, CORA answers directly in the target language without any
translation or in-language retrieval modules as used in prior work. We propose
an iterative training method that automatically extends annotated data
available only in high-resource languages to low-resource ones. Our results
show that CORA substantially outperforms the previous state of the art on
multilingual open question answering benchmarks across 26 languages, 9 of which
are unseen during training. Our analyses show the significance of cross-lingual
retrieval and generation in many languages, particularly under low-resource
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1"&gt;Akari Asai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xinyan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1"&gt;Jungo Kasai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Knowledge Graph and Attention Help? A Quantitative Analysis into Bag-level Relation Extraction. (arXiv:2107.12064v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12064</id>
        <link href="http://arxiv.org/abs/2107.12064"/>
        <updated>2021-07-27T02:03:32.003Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) and attention mechanism have been demonstrated effective
in introducing and selecting useful information for weakly supervised methods.
However, only qualitative analysis and ablation study are provided as evidence.
In this paper, we contribute a dataset and propose a paradigm to quantitatively
evaluate the effect of attention and KG on bag-level relation extraction (RE).
We find that (1) higher attention accuracy may lead to worse performance as it
may harm the model's ability to extract entity mention features; (2) the
performance of attention is largely influenced by various noise distribution
patterns, which is closely related to real-world datasets; (3) KG-enhanced
attention indeed improves RE performance, while not through enhanced attention
but by incorporating entity prior; and (4) attention mechanism may exacerbate
the issue of insufficient training data. Based on these findings, we show that
a straightforward variant of RE model can achieve significant improvements (6%
AUC on average) on two real-world datasets as compared with three
state-of-the-art baselines. Our codes and datasets are available at
https://github.com/zig-kwin-hu/how-KG-ATT-help.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zikun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yixin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lifu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable Dialogue Systems and User Simulators. (arXiv:2107.11904v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11904</id>
        <link href="http://arxiv.org/abs/2107.11904"/>
        <updated>2021-07-27T02:03:31.996Z</updated>
        <summary type="html"><![CDATA[One of the difficulties in training dialogue systems is the lack of training
data. We explore the possibility of creating dialogue data through the
interaction between a dialogue system and a user simulator. Our goal is to
develop a modelling framework that can incorporate new dialogue scenarios
through self-play between the two agents. In this framework, we first pre-train
the two agents on a collection of source domain dialogues, which equips the
agents to converse with each other via natural language. With further
fine-tuning on a small amount of target domain data, the agents continue to
interact with the aim of improving their behaviors using reinforcement learning
with structured reward functions. In experiments on the MultiWOZ dataset, two
practical transfer learning problems are investigated: 1) domain adaptation and
2) single-to-multiple domain transfer. We demonstrate that the proposed
framework is highly effective in bootstrapping the performance of the two
agents in transfer learning. We also show that our method leads to improvements
in dialogue system performance on complete datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1"&gt;Bo-Hsiang Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yinpei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreyssig_F/0/1/0/all/0/1"&gt;Florian Kreyssig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1"&gt;Bill Byrne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Network with Generalized Factorized Bilinear Aggregation. (arXiv:2107.11666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11666</id>
        <link href="http://arxiv.org/abs/2107.11666"/>
        <updated>2021-07-27T02:03:31.965Z</updated>
        <summary type="html"><![CDATA[Although Graph Convolutional Networks (GCNs) have demonstrated their power in
various applications, the graph convolutional layers, as the most important
component of GCN, are still using linear transformations and a simple pooling
step. In this paper, we propose a novel generalization of Factorized Bilinear
(FB) layer to model the feature interactions in GCNs. FB performs two
matrix-vector multiplications, that is, the weight matrix is multiplied with
the outer product of the vector of hidden features from both sides. However,
the FB layer suffers from the quadratic number of coefficients, overfitting and
the spurious correlations due to correlations between channels of hidden
representations that violate the i.i.d. assumption. Thus, we propose a compact
FB layer by defining a family of summarizing operators applied over the
quadratic term. We analyze proposed pooling operators and motivate their use.
Our experimental results on multiple datasets demonstrate that the GFB-GCN is
competitive with other methods for text classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Autoregressive Solver for Scalable Abductive Natural Language Inference. (arXiv:2107.11879v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11879</id>
        <link href="http://arxiv.org/abs/2107.11879"/>
        <updated>2021-07-27T02:03:31.958Z</updated>
        <summary type="html"><![CDATA[Regenerating natural language explanations for science questions is a
challenging task for evaluating complex multi-hop and abductive inference
capabilities. In this setting, Transformers trained on human-annotated
explanations achieve state-of-the-art performance when adopted as cross-encoder
architectures. However, while much attention has been devoted to the quality of
the constructed explanations, the problem of performing abductive inference at
scale is still under-studied. As intrinsically not scalable, the cross-encoder
architectural paradigm is not suitable for efficient multi-hop inference on
massive facts banks. To maximise both accuracy and inference time, we propose a
hybrid abductive solver that autoregressively combines a dense bi-encoder with
a sparse model of explanatory power, computed leveraging explicit patterns in
the explanations. Our experiments demonstrate that the proposed framework can
achieve performance comparable with the state-of-the-art cross-encoder while
being $\approx 50$ times faster and scalable to corpora of millions of facts.
Moreover, we study the impact of the hybridisation on semantic drift and
science question answering without additional training, showing that it boosts
the quality of the explanations and contributes to improved downstream
inference performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1"&gt;Marco Valentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1"&gt;Mokanarangan Thayaparan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1"&gt;Deborah Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy. (arXiv:2107.11823v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11823</id>
        <link href="http://arxiv.org/abs/2107.11823"/>
        <updated>2021-07-27T02:03:31.951Z</updated>
        <summary type="html"><![CDATA[Multi-hop reading comprehension (MHRC) requires not only to predict the
correct answer span in the given passage, but also to provide a chain of
supporting evidences for reasoning interpretability. It is natural to model
such a process into graph structure by understanding multi-hop reasoning as
jumping over entity nodes, which has made graph modelling dominant on this
task. Recently, there have been dissenting voices about whether graph modelling
is indispensable due to the inconvenience of the graph building, however
existing state-of-the-art graph-free attempts suffer from huge performance gap
compared to graph-based ones. This work presents a novel graph-free alternative
which firstly outperform all graph models on MHRC. In detail, we exploit a
select-to-guide (S2G) strategy to accurately retrieve evidence paragraphs in a
coarse-to-fine manner, incorporated with two novel attention mechanisms, which
surprisingly shows conforming to the nature of multi-hop reasoning. Our
graph-free model achieves significant and consistent performance gain over
strong baselines and the current new state-of-the-art on the MHRC benchmark,
HotpotQA, among all the published works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bohong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Controlled and Diverse Generation of Article Comments. (arXiv:2107.11781v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11781</id>
        <link href="http://arxiv.org/abs/2107.11781"/>
        <updated>2021-07-27T02:03:31.944Z</updated>
        <summary type="html"><![CDATA[Much research in recent years has focused on automatic article commenting.
However, few of previous studies focus on the controllable generation of
comments. Besides, they tend to generate dull and commonplace comments, which
further limits their practical application. In this paper, we make the first
step towards controllable generation of comments, by building a system that can
explicitly control the emotion of the generated comments. To achieve this, we
associate each kind of emotion category with an embedding and adopt a dynamic
fusion mechanism to fuse this embedding into the decoder. A sentence-level
emotion classifier is further employed to better guide the model to generate
comments expressing the desired emotion. To increase the diversity of the
generated comments, we propose a hierarchical copy mechanism that allows our
model to directly copy words from the input articles. We also propose a
restricted beam search (RBS) algorithm to increase intra-sentence diversity.
Experimental results show that our model can generate informative and diverse
comments that express the desired emotions with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Houfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12079</id>
        <link href="http://arxiv.org/abs/2107.12079"/>
        <updated>2021-07-27T02:03:31.937Z</updated>
        <summary type="html"><![CDATA[Dialogue systems are widely used in AI to support timely and interactive
communication with users. We propose a general-purpose dialogue system
architecture that leverages computational argumentation and state-of-the-art
language technologies. We illustrate and evaluate the system using a COVID-19
vaccine information case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1"&gt;Bettina Fazzinga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1"&gt;Andrea Galassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1"&gt;Paolo Torroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-aware Adversarial Training for Name Regularity Bias in Named Entity Recognition. (arXiv:2107.11610v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11610</id>
        <link href="http://arxiv.org/abs/2107.11610"/>
        <updated>2021-07-27T02:03:31.916Z</updated>
        <summary type="html"><![CDATA[In this work, we examine the ability of NER models to use contextual
information when predicting the type of an ambiguous entity. We introduce NRB,
a new testbed carefully designed to diagnose Name Regularity Bias of NER
models. Our results indicate that all state-of-the-art models we tested show
such a bias; BERT fine-tuned models significantly outperforming feature-based
(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance
on standard benchmarks.

To mitigate this bias, we propose a novel model-agnostic training method that
adds learnable adversarial noise to some entity mentions, thus enforcing models
to focus more strongly on the contextual signal, leading to significant gains
on NRB. Combining it with two other training strategies, data augmentation and
parameter freezing, leads to further gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1"&gt;Abbas Ghaddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1"&gt;Philippe Langlais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1"&gt;Ahmad Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1"&gt;Mehdi Rezagholizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Focus: Hierarchical Dynamic Copy Network for Dialogue State Tracking. (arXiv:2107.11778v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11778</id>
        <link href="http://arxiv.org/abs/2107.11778"/>
        <updated>2021-07-27T02:03:31.909Z</updated>
        <summary type="html"><![CDATA[Recently, researchers have explored using the encoder-decoder framework to
tackle dialogue state tracking (DST), which is a key component of task-oriented
dialogue systems. However, they regard a multi-turn dialogue as a flat
sequence, failing to focus on useful information when the sequence is long. In
this paper, we propose a Hierarchical Dynamic Copy Network (HDCN) to facilitate
focusing on the most informative turn, making it easier to extract slot values
from the dialogue context. Based on the encoder-decoder framework, we adopt a
hierarchical copy approach that calculates two levels of attention at the word-
and turn-level, which are then renormalized to obtain the final copy
distribution. A focus loss term is employed to encourage the model to assign
the highest turn-level attention weight to the most informative turn.
Experimental results show that our model achieves 46.76% joint accuracy on the
MultiWOZ 2.1 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Houfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11956</id>
        <link href="http://arxiv.org/abs/2107.11956"/>
        <updated>2021-07-27T02:03:31.902Z</updated>
        <summary type="html"><![CDATA[Automatically mining sentiment tendency contained in natural language is a
fundamental research to some artificial intelligent applications, where
solutions alternate with challenges. Transfer learning and multi-task learning
techniques have been leveraged to mitigate the supervision sparsity and
collaborate multiple heterogeneous domains correspondingly. Recent years, the
sensitive nature of users' private data raises another challenge for sentiment
classification, i.e., data privacy protection. In this paper, we resort to
federated learning for multiple domain sentiment classification under the
constraint that the corpora must be stored on decentralized devices. In view of
the heterogeneous semantics across multiple parties and the peculiarities of
word embedding, we pertinently provide corresponding solutions. First, we
propose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for
better model aggregation and personalization in federated sentiment
classification. Second, we propose KTEPS$^\star$ with the consideration of the
rich semantic and huge embedding size properties of word vectors, utilizing
Projection-based Dimension Reduction (PDR) methods for privacy protection and
efficient transmission simultaneously. We propose two federated sentiment
classification scenes based on public benchmarks, and verify the superiorities
of our proposed methods with abundant experimental investigations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin-Chun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shaoming Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stress Test Evaluation of Biomedical Word Embeddings. (arXiv:2107.11652v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11652</id>
        <link href="http://arxiv.org/abs/2107.11652"/>
        <updated>2021-07-27T02:03:31.894Z</updated>
        <summary type="html"><![CDATA[The success of pretrained word embeddings has motivated their use in the
biomedical domain, with contextualized embeddings yielding remarkable results
in several biomedical NLP tasks. However, there is a lack of research on
quantifying their behavior under severe "stress" scenarios. In this work, we
systematically evaluate three language models with adversarial examples --
automatically constructed tests that allow us to examine how robust the models
are. We propose two types of stress scenarios focused on the biomedical named
entity recognition (NER) task, one inspired by spelling errors and another
based on the use of synonyms for medical terms. Our experiments with three
benchmarks show that the performance of the original models decreases
considerably, in addition to revealing their weaknesses and strengths. Finally,
we show that adversarial training causes the models to improve their robustness
and even to exceed the original performance in some cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1"&gt;Vladimir Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Carvallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aspillaga_C/0/1/0/all/0/1"&gt;Carlos Aspillaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_C/0/1/0/all/0/1"&gt;Camilo Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1"&gt;Denis Parra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. (arXiv:2107.11906v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11906</id>
        <link href="http://arxiv.org/abs/2107.11906"/>
        <updated>2021-07-27T02:03:31.887Z</updated>
        <summary type="html"><![CDATA[We describe an efficient hierarchical method to compute attention in the
Transformer architecture. The proposed attention mechanism exploits a matrix
structure similar to the Hierarchical Matrix (H-Matrix) developed by the
numerical analysis community, and has linear run time and memory complexity. We
perform extensive experiments to show that the inductive bias embodied by our
hierarchical attention is effective in capturing the hierarchical structure in
the sequences typical for natural language and vision tasks. Our method is
superior to alternative sub-quadratic proposals by over +6 points on average on
the Long Range Arena benchmark. It also sets a new SOTA test perplexity on
One-Billion Word dataset with 5x fewer model parameters than that of the
previous-best Transformer-based models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenhai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1"&gt;Radu Soricut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-based Music Recommendation: Evolution, State of the Art, and Challenges. (arXiv:2107.11803v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.11803</id>
        <link href="http://arxiv.org/abs/2107.11803"/>
        <updated>2021-07-27T02:03:31.853Z</updated>
        <summary type="html"><![CDATA[The music domain is among the most important ones for adopting recommender
systems technology. In contrast to most other recommendation domains, which
predominantly rely on collaborative filtering (CF) techniques, music
recommenders have traditionally embraced content-based (CB) approaches. In the
past years, music recommendation models that leverage collaborative and content
data -- which we refer to as content-driven models -- have been replacing pure
CF or CB models.

In this survey, we review 47 articles on content-driven music recommendation.
Based on a thorough literature analysis, we first propose an onion model
comprising five layers, each of which corresponds to a category of music
content we identified: signal, embedded metadata, expert-generated content,
user-generated content, and derivative content. We provide a detailed
characterization of each category along several dimensions. Second, we identify
six overarching challenges, according to which we organize our main discussion:
increasing recommendation diversity and novelty, providing transparency and
explanations, accomplishing context-awareness, recommending sequences of music,
improving scalability and efficiency, and alleviating cold start. Each article
addressing one or more of these challenges is categorized according to the
content layers of our onion model, the article's goal(s), and main
methodological choices. Furthermore, articles are discussed in temporal order
to shed light on the evolution of content-driven music recommendation
strategies. Finally, we provide our personal selection of the persisting grand
challenges, which are still waiting to be solved in future research endeavors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1"&gt;Yashar Deldjoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1"&gt;Peter Knees&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos. (arXiv:2107.11756v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11756</id>
        <link href="http://arxiv.org/abs/2107.11756"/>
        <updated>2021-07-27T02:03:31.845Z</updated>
        <summary type="html"><![CDATA[Given a video demonstration, can we imitate the action contained in this
video? In this paper, we introduce a novel task, dubbed mesh-based action
imitation. The goal of this task is to enable an arbitrary target human mesh to
perform the same action shown on the video demonstration. To achieve this, a
novel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI
first learns to reconstruct the meshes from the given source image frames, then
the initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence
smooth module proposed by us, to improve the temporal consistency. Finally, we
imitate the actions by transferring the pose from the constructed human body to
our target identity mesh. High-quality and detailed human body meshes can be
generated by using our M-VAI. Extensive experiments demonstrate the feasibility
of our task and the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu-Gang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIPE: A Metric Independent Pipeline for Effective Code-Mixed NLG Evaluation. (arXiv:2107.11534v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11534</id>
        <link href="http://arxiv.org/abs/2107.11534"/>
        <updated>2021-07-27T02:03:31.838Z</updated>
        <summary type="html"><![CDATA[Code-mixing is a phenomenon of mixing words and phrases from two or more
languages in a single utterance of speech and text. Due to the high linguistic
diversity, code-mixing presents several challenges in evaluating standard
natural language generation (NLG) tasks. Various widely popular metrics perform
poorly with the code-mixed NLG tasks. To address this challenge, we present a
metric independent evaluation pipeline MIPE that significantly improves the
correlation between evaluation metrics and human judgments on the generated
code-mixed text. As a use case, we demonstrate the performance of MIPE on the
machine-generated Hinglish (code-mixing of Hindi and English languages)
sentences from the HinGE corpus. We can extend the proposed evaluation strategy
to other code-mixed language pairs, NLG tasks, and evaluation metrics with
minimal to no effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Ayush Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kagi_S/0/1/0/all/0/1"&gt;Sammed S Kagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brazilian Portuguese Speech Recognition Using Wav2vec 2.0. (arXiv:2107.11414v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11414</id>
        <link href="http://arxiv.org/abs/2107.11414"/>
        <updated>2021-07-27T02:03:31.830Z</updated>
        <summary type="html"><![CDATA[Deep learning techniques have been shown to be efficient in various tasks,
especially in the development of speech recognition systems, that is, systems
that aim to transcribe a sentence in audio in a sequence of words. Despite the
progress in the area, speech recognition can still be considered difficult,
especially for languages lacking available data, as Brazilian Portuguese. In
this sense, this work presents the development of an public Automatic Speech
Recognition system using only open available audio data, from the fine-tuning
of the Wav2vec 2.0 XLSR-53 model pre-trained in many languages over Brazilian
Portuguese data. The final model presents a Word Error Rate of 11.95% (Common
Voice Dataset). This corresponds to 13% less than the best open Automatic
Speech Recognition model for Brazilian Portuguese available according to our
best knowledge, which is a promising result for the language. In general, this
work validates the use of self-supervising learning techniques, in special, the
use of the Wav2vec 2.0 architecture in the development of robust systems, even
for languages having few available data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1"&gt;Lucas Rafael Stefanel Gris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1"&gt;Edresson Casanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Frederico Santos de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1"&gt;Anderson da Silva Soares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1"&gt;Arnaldo Candido Junior&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Based Label Smoothing For Dialogue Generation. (arXiv:2107.11481v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11481</id>
        <link href="http://arxiv.org/abs/2107.11481"/>
        <updated>2021-07-27T02:03:31.811Z</updated>
        <summary type="html"><![CDATA[Generative neural conversational systems are generally trained with the
objective of minimizing the entropy loss between the training "hard" targets
and the predicted logits. Often, performance gains and improved generalization
can be achieved by using regularization techniques like label smoothing, which
converts the training "hard" targets to "soft" targets. However, label
smoothing enforces a data independent uniform distribution on the incorrect
training targets, which leads to an incorrect assumption of equi-probable
incorrect targets for each correct target. In this paper we propose and
experiment with incorporating data dependent word similarity based weighing
methods to transforms the uniform distribution of the incorrect target
probabilities in label smoothing, to a more natural distribution based on
semantics. We introduce hyperparameters to control the incorrect target
distribution, and report significant performance gains over networks trained
using standard label smoothing based loss, on two standard open domain dialogue
corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sougata Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Souvik Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1"&gt;Rohini Srihari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives. (arXiv:2107.11584v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11584</id>
        <link href="http://arxiv.org/abs/2107.11584"/>
        <updated>2021-07-27T02:03:31.780Z</updated>
        <summary type="html"><![CDATA[Human gender bias is reflected in language and text production. Because
state-of-the-art machine translation (MT) systems are trained on large corpora
of text, mostly generated by humans, gender bias can also be found in MT. For
instance when occupations are translated from a language like English, which
mostly uses gender neutral words, to a language like German, which mostly uses
a feminine and a masculine version for an occupation, a decision must be made
by the MT System. Recent research showed that MT systems are biased towards
stereotypical translation of occupations. In 2019 the first, and so far only,
challenge set, explicitly designed to measure the extent of gender bias in MT
systems has been published. In this set measurement of gender bias is solely
based on the translation of occupations. In this paper we present an extension
of this challenge set, called WiBeMT, with gender-biased adjectives and adds
sentences with gender-biased verbs. The resulting challenge set consists of
over 70, 000 sentences and has been translated with three commercial MT
systems: DeepL Translator, Microsoft Translator, and Google Translate. Results
show a gender bias for all three MT systems. This gender bias is to a great
extent significantly influenced by adjectives and to a lesser extent by verbs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Troles_J/0/1/0/all/0/1"&gt;Jonas-Dario Troles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1"&gt;Ute Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One Transformer VAE. (arXiv:2105.04090v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04090</id>
        <link href="http://arxiv.org/abs/2105.04090"/>
        <updated>2021-07-27T02:03:31.771Z</updated>
        <summary type="html"><![CDATA[Transformers and variational autoencoders (VAE) have been extensively
employed for symbolic (e.g., MIDI) domain music generation. While the former
boast an impressive capability in modeling long sequences, the latter allow
users to willingly exert control over different parts (e.g., bars) of the music
to be generated. In this paper, we are interested in bringing the two together
to construct a single model that exhibits both strengths. The task is split
into two steps. First, we equip Transformer decoders with the ability to accept
segment-level, time-varying conditions during sequence generation.
Subsequently, we combine the developed and tested in-attention decoder with a
Transformer encoder, and train the resulting MuseMorphose model with the VAE
objective to achieve style transfer of long musical pieces, in which users can
specify musical attributes including rhythmic intensity and polyphony (i.e.,
harmonic fullness) they desire, down to the bar level. Experiments show that
MuseMorphose outperforms recurrent neural network (RNN) based baselines on
numerous widely-used metrics for style transfer tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shih-Lun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox. (arXiv:2107.11757v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11757</id>
        <link href="http://arxiv.org/abs/2107.11757"/>
        <updated>2021-07-27T02:03:31.704Z</updated>
        <summary type="html"><![CDATA[We introduce the MuSe-Toolbox - a Python-based open-source toolkit for
creating a variety of continuous and discrete emotion gold standards. In a
single framework, we unify a wide range of fusion methods and propose the novel
Rater Aligned Annotation Weighting (RAAW), which aligns the annotations in a
translation-invariant way before weighting and fusing them based on the
inter-rater agreements between the annotations. Furthermore, discrete
categories tend to be easier for humans to interpret than continuous signals.
With this in mind, the MuSe-Toolbox provides the functionality to run
exhaustive searches for meaningful class clusters in the continuous gold
standards. To our knowledge, this is the first toolkit that provides a wide
selection of state-of-the-art emotional gold standard methods and their
transformation to discrete classes. Experimental results indicate that
MuSe-Toolbox can provide promising and novel class formations which can be
better predicted than hard-coded classes boundaries with minimal human
intervention. The implementation (1) is out-of-the-box available with all
dependencies using a Docker container (2).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1"&gt;Lukas Stappen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1"&gt;Lea Schumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sertolli_B/0/1/0/all/0/1"&gt;Benjamin Sertolli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1"&gt;Alice Baird&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weigel_B/0/1/0/all/0/1"&gt;Benjamin Weigel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Joint and Domain-Adaptive Approach to Spoken Language Understanding. (arXiv:2107.11768v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11768</id>
        <link href="http://arxiv.org/abs/2107.11768"/>
        <updated>2021-07-27T02:03:31.666Z</updated>
        <summary type="html"><![CDATA[Spoken Language Understanding (SLU) is composed of two subtasks: intent
detection (ID) and slot filling (SF). There are two lines of research on SLU.
One jointly tackles these two subtasks to improve their prediction accuracy,
and the other focuses on the domain-adaptation ability of one of the subtasks.
In this paper, we attempt to bridge these two lines of research and propose a
joint and domain adaptive approach to SLU. We formulate SLU as a constrained
generation task and utilize a dynamic vocabulary based on domain-specific
ontology. We conduct experiments on the ASMixed and MTOD datasets and achieve
competitive performance with previous state-of-the-art joint models. Besides,
results show that our joint model can be effectively adapted to a new domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1"&gt;Linjun Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Ming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Houfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Michael Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Allophone Graphs for Language-Universal Speech Recognition. (arXiv:2107.11628v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11628</id>
        <link href="http://arxiv.org/abs/2107.11628"/>
        <updated>2021-07-27T02:03:31.588Z</updated>
        <summary type="html"><![CDATA[Building language-universal speech recognition systems entails producing
phonological units of spoken sound that can be shared across languages. While
speech annotations at the language-specific phoneme or surface levels are
readily available, annotations at a universal phone level are relatively rare
and difficult to produce. In this work, we present a general framework to
derive phone-level supervision from only phonemic transcriptions and
phone-to-phoneme mappings with learnable weights represented using weighted
finite-state transducers, which we call differentiable allophone graphs. By
training multilingually, we build a universal phone-based speech recognition
model with interpretable probabilistic phone-to-phoneme mappings for each
language. These phone-based systems with learned allophone graphs can be used
by linguists to document new languages, build phone-based lexicons that capture
rich pronunciation variations, and re-evaluate the allophone mappings of seen
language. We demonstrate the aforementioned benefits of our proposed framework
with a system trained on 7 diverse languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Brian Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1"&gt;Siddharth Dalmia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1"&gt;David R. Mortensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1"&gt;Florian Metze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ContextNet: A Click-Through Rate Prediction Framework Using Contextual information to Refine Feature Embedding. (arXiv:2107.12025v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.12025</id>
        <link href="http://arxiv.org/abs/2107.12025"/>
        <updated>2021-07-27T02:03:31.561Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) estimation is a fundamental task in personalized
advertising and recommender systems and it's important for ranking models to
effectively capture complex high-order features.Inspired by the success of ELMO
and Bert in NLP field, which dynamically refine word embedding according to the
context sentence information where the word appears, we think it's also
important to dynamically refine each feature's embedding layer by layer
according to the context information contained in input instance in CTR
estimation tasks. We can effectively capture the useful feature interactions
for each feature in this way. In this paper, We propose a novel CTR Framework
named ContextNet that implicitly models high-order feature interactions by
dynamically refining each feature's embedding according to the input context.
Specifically, ContextNet consists of two key components: contextual embedding
module and ContextNet block. Contextual embedding module aggregates contextual
information for each feature from input instance and ContextNet block maintains
each feature's embedding layer by layer and dynamically refines its
representation by merging contextual high-order interaction information into
feature embedding. To make the framework specific, we also propose two
models(ContextNet-PFFN and ContextNet-SFFN) under this framework by introducing
linear contextual embedding network and two non-linear mapping sub-network in
ContextNet block. We conduct extensive experiments on four real-world datasets
and the experiment results demonstrate that our proposed ContextNet-PFFN and
ContextNet-SFFN model outperform state-of-the-art models such as DeepFM and
xDeepFM significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qingyun She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;PengTao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junlin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Form 10-Q Itemization. (arXiv:2104.11783v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11783</id>
        <link href="http://arxiv.org/abs/2104.11783"/>
        <updated>2021-07-27T02:03:31.526Z</updated>
        <summary type="html"><![CDATA[The quarterly financial statement, or Form 10-Q, is one of the most
frequently required filings for US public companies to disclose financial and
other important business information. Due to the massive volume of 10-Q filings
and the enormous variations in the reporting format, it has been a
long-standing challenge to retrieve item-specific information from 10-Q filings
that lack machine-readable hierarchy. This paper presents a solution for
itemizing 10-Q files by complementing a rule-based algorithm with a
Convolutional Neural Network (CNN) image classifier. This solution demonstrates
a pipeline that can be generalized to a rapid data retrieval solution among a
large volume of textual data using only typographic items. The extracted
textual data can be used as unlabeled content-specific data to train
transformer models (e.g., BERT) or fit into various field-focus natural
language processing (NLP) applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanci Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1"&gt;Tianming Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yujie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1"&gt;Lawrence Donohue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1"&gt;Rui Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leaf-FM: A Learnable Feature Generation Factorization Machine for Click-Through Rate Prediction. (arXiv:2107.12024v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.12024</id>
        <link href="http://arxiv.org/abs/2107.12024"/>
        <updated>2021-07-27T02:03:31.517Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction plays important role in personalized
advertising and recommender systems. Though many models have been proposed such
as FM, FFM and DeepFM in recent years, feature engineering is still a very
important way to improve the model performance in many applications because
using raw features can rarely lead to optimal results. For example, the
continuous features are usually transformed to the power forms by adding a new
feature to allow it to easily form non-linear functions of the feature.
However, this kind of feature engineering heavily relies on peoples experience
and it is both time consuming and labor consuming. On the other side, concise
CTR model with both fast online serving speed and good model performance is
critical for many real life applications. In this paper, we propose LeafFM
model based on FM to generate new features from the original feature embedding
by learning the transformation functions automatically. We also design three
concrete Leaf-FM models according to the different strategies of combing the
original and the generated features. Extensive experiments are conducted on
three real-world datasets and the results show Leaf-FM model outperforms
standard FMs by a large margin. Compared with FFMs, Leaf-FM can achieve
significantly better performance with much less parameters. In Avazu and
Malware dataset, add version Leaf-FM achieves comparable performance with some
deep learning based models such as DNN and AutoInt. As an improved FM model,
Leaf-FM has the same computation complexity with FM in online serving phase and
it means Leaf-FM is applicable in many industry applications because of its
better performance and high computation efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qingyun She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junlin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PARIMA: Viewport Adaptive 360-Degree Video Streaming. (arXiv:2103.00981v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00981</id>
        <link href="http://arxiv.org/abs/2103.00981"/>
        <updated>2021-07-27T02:03:31.508Z</updated>
        <summary type="html"><![CDATA[With increasing advancements in technologies for capturing 360{\deg} videos,
advances in streaming such videos have become a popular research topic.
However, streaming 360{\deg} videos require high bandwidth, thus escalating the
need for developing optimized streaming algorithms. Researchers have proposed
various methods to tackle the problem, considering the network bandwidth or
attempt to predict future viewports in advance. However, most of the existing
works either (1) do not consider video contents to predict user viewport, or
(2) do not adapt to user preferences dynamically, or (3) require a lot of
training data for new videos, thus making them potentially unfit for video
streaming purposes. We develop PARIMA, a fast and efficient online viewport
prediction model that uses past viewports of users along with the trajectories
of prime objects as a representative of video content to predict future
viewports. We claim that the head movement of a user majorly depends upon the
trajectories of the prime objects in the video. We employ a pyramid-based
bitrate allocation scheme and perform a comprehensive evaluation of the
performance of PARIMA. In our evaluation, we show that PARIMA outperforms
state-of-the-art approaches, improving the Quality of Experience by over 30\%
while maintaining a short response time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_L/0/1/0/all/0/1"&gt;Lovish Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Sarthak Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1"&gt;Abhijit Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Sandip Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clinical Utility of the Automatic Phenotype Annotation in Unstructured Clinical Notes: ICU Use Cases. (arXiv:2107.11665v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11665</id>
        <link href="http://arxiv.org/abs/2107.11665"/>
        <updated>2021-07-27T02:03:31.498Z</updated>
        <summary type="html"><![CDATA[Clinical notes contain information not present elsewhere, including drug
response and symptoms, all of which are highly important when predicting key
outcomes in acute care patients. We propose the automatic annotation of
phenotypes from clinical notes as a method to capture essential information to
predict outcomes in the Intensive Care Unit (ICU). This information is
complementary to typically used vital signs and laboratory test results. We
demonstrate and validate our approach conducting experiments on the prediction
of in-hospital mortality, physiological decompensation and length of stay in
the ICU setting for over 24,000 patients. The prediction models incorporating
phenotypic information consistently outperform the baseline models leveraging
only vital signs and laboratory test results. Moreover, we conduct a thorough
interpretability study, showing that phenotypes provide valuable insights at
the patient and cohort levels. Our approach illustrates the viability of using
phenotypes to determine outcomes in the ICU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1"&gt;Luis Bolanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1"&gt;Ashwani Tanwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sokol_A/0/1/0/all/0/1"&gt;Albert Sokol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1"&gt;Julia Ive&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vibhor Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yike Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. (arXiv:2102.07619v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07619</id>
        <link href="http://arxiv.org/abs/2102.07619"/>
        <updated>2021-07-27T02:03:31.467Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate(CTR) estimation has become one of the most fundamental
tasks in many real-world applications and it's important for ranking models to
effectively capture complex high-order features. Shallow feed-forward network
is widely used in many state-of-the-art DNN models such as FNN, DeepFM and
xDeepFM to implicitly capture high-order feature interactions. However, some
research has proved that addictive feature interaction, particular feed-forward
neural networks, is inefficient in capturing common feature interaction. To
resolve this problem, we introduce specific multiplicative operation into DNN
ranking system by proposing instance-guided mask which performs element-wise
product both on the feature embedding and feed-forward layers guided by input
instance. We also turn the feed-forward layer in DNN model into a mixture of
addictive and multiplicative feature interactions by proposing MaskBlock in
this paper. MaskBlock combines the layer normalization, instance-guided mask,
and feed-forward layer and it is a basic building block to be used to design
new ranking model under various configurations. The model consisting of
MaskBlock is called MaskNet in this paper and two new MaskNet models are
proposed to show the effectiveness of MaskBlock as basic building block for
composing high performance ranking systems. The experiment results on three
real-world datasets demonstrate that our proposed MaskNet models outperform
state-of-the-art models such as DeepFM and xDeepFM significantly, which implies
MaskBlock is an effective basic building unit for composing new high
performance ranking systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qingyun She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junlin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12079</id>
        <link href="http://arxiv.org/abs/2107.12079"/>
        <updated>2021-07-27T02:03:31.455Z</updated>
        <summary type="html"><![CDATA[Dialogue systems are widely used in AI to support timely and interactive
communication with users. We propose a general-purpose dialogue system
architecture that leverages computational argumentation and state-of-the-art
language technologies. We illustrate and evaluate the system using a COVID-19
vaccine information case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1"&gt;Bettina Fazzinga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1"&gt;Andrea Galassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1"&gt;Paolo Torroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Autoregressive Solver for Scalable Abductive Natural Language Inference. (arXiv:2107.11879v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11879</id>
        <link href="http://arxiv.org/abs/2107.11879"/>
        <updated>2021-07-27T02:03:31.436Z</updated>
        <summary type="html"><![CDATA[Regenerating natural language explanations for science questions is a
challenging task for evaluating complex multi-hop and abductive inference
capabilities. In this setting, Transformers trained on human-annotated
explanations achieve state-of-the-art performance when adopted as cross-encoder
architectures. However, while much attention has been devoted to the quality of
the constructed explanations, the problem of performing abductive inference at
scale is still under-studied. As intrinsically not scalable, the cross-encoder
architectural paradigm is not suitable for efficient multi-hop inference on
massive facts banks. To maximise both accuracy and inference time, we propose a
hybrid abductive solver that autoregressively combines a dense bi-encoder with
a sparse model of explanatory power, computed leveraging explicit patterns in
the explanations. Our experiments demonstrate that the proposed framework can
achieve performance comparable with the state-of-the-art cross-encoder while
being $\approx 50$ times faster and scalable to corpora of millions of facts.
Moreover, we study the impact of the hybridisation on semantic drift and
science question answering without additional training, showing that it boosts
the quality of the explanations and contributes to improved downstream
inference performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1"&gt;Marco Valentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1"&gt;Mokanarangan Thayaparan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1"&gt;Deborah Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The USYD-JD Speech Translation System for IWSLT 2021. (arXiv:2107.11572v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11572</id>
        <link href="http://arxiv.org/abs/2107.11572"/>
        <updated>2021-07-27T02:03:31.422Z</updated>
        <summary type="html"><![CDATA[This paper describes the University of Sydney& JD's joint submission of the
IWSLT 2021 low resource speech translation task. We participated in the
Swahili-English direction and got the best scareBLEU (25.3) score among all the
participants. Our constrained system is based on a pipeline framework, i.e. ASR
and NMT. We trained our models with the officially provided ASR and MT
datasets. The ASR system is based on the open-sourced tool Kaldi and this work
mainly explores how to make the most of the NMT models. To reduce the
punctuation errors generated by the ASR model, we employ our previous work
SlotRefine to train a punctuation correction model. To achieve better
translation performance, we explored the most recent effective strategies,
including back translation, knowledge distillation, multi-feature reranking and
transductive finetuning. For model structure, we tried auto-regressive and
non-autoregressive models, respectively. In addition, we proposed two novel
pre-train approaches, i.e. \textit{de-noising training} and
\textit{bidirectional training} to fully exploit the data. Extensive
experiments show that adding the above techniques consistently improves the
BLEU scores, and the final submission system outperforms the baseline
(Transformer ensemble model trained with the original parallel data) by
approximately 10.8 BLEU score, achieving the SOTA performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Liang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06423</id>
        <link href="http://arxiv.org/abs/2107.06423"/>
        <updated>2021-07-27T02:03:31.411Z</updated>
        <summary type="html"><![CDATA[Wikidata is an open knowledge graph built by a global community of
volunteers. As it advances in scale, it faces substantial challenges around
editor engagement. These challenges are in terms of both attracting new editors
to keep up with the sheer amount of work and retaining existing editors.
Experience from other online communities and peer-production systems, including
Wikipedia, suggests that personalised recommendations could help, especially
newcomers, who are sometimes unsure about how to contribute best to an ongoing
effort. For this reason, we propose a recommender system WikidataRec for
Wikidata items. The system uses a hybrid of content-based and collaborative
filtering techniques to rank items for editors relying on both item features
and item-editor previous interaction. A neural network, named a neural mixture
of representations, is designed to learn fine weights for the combination of
item-based representations and optimize them with editor-based representation
by item-editor interaction. To facilitate further research in this space, we
also create two benchmark datasets, a general-purpose one with 220,000 editors
responsible for 14 million interactions with 4 million items and a second one
focusing on the contributions of more than 8,000 more active editors. We
perform an offline evaluation of the system on both datasets with promising
results. Our code and datasets are available at
https://github.com/WikidataRec-developer/Wikidata_Recommender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1"&gt;Kholoud AlGhamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1"&gt;Elena Simperl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03844</id>
        <link href="http://arxiv.org/abs/2107.03844"/>
        <updated>2021-07-27T02:03:31.358Z</updated>
        <summary type="html"><![CDATA[Bangla -- ranked as the 6th most widely spoken language across the world
(https://www.ethnologue.com/guides/ethnologue200), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Arid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvirul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1"&gt;Janntatul Tajrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naira Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11576</id>
        <link href="http://arxiv.org/abs/2107.11576"/>
        <updated>2021-07-27T02:03:31.343Z</updated>
        <summary type="html"><![CDATA[Encouraging progress has been made towards Visual Question Answering (VQA) in
recent years, but it is still challenging to enable VQA models to adaptively
generalize to out-of-distribution (OOD) samples. Intuitively, recompositions of
existing visual concepts (i.e., attributes and objects) can generate unseen
compositions in the training set, which will promote VQA models to generalize
to OOD samples. In this paper, we formulate OOD generalization in VQA as a
compositional generalization problem and propose a graph generative
modeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM
leverages graph generative modeling to iteratively generate a relation matrix
and node representations for the predefined graph that utilizes
attribute-object pairs as nodes. Furthermore, to alleviate the unstable
training issue in graph generative modeling, we propose a gradient distribution
consistency loss to constrain the data distribution with adversarial
perturbations and the generated distribution. The baseline VQA model (LXMERT)
trained with the X-GGM scheme achieves state-of-the-art OOD performance on two
standard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation
studies demonstrate the effectiveness of X-GGM components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jingjing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1"&gt;Zhixiong Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Language Model for Efficient Linguistic Steganalysis: An Empirical Study. (arXiv:2107.12168v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12168</id>
        <link href="http://arxiv.org/abs/2107.12168"/>
        <updated>2021-07-27T02:03:31.329Z</updated>
        <summary type="html"><![CDATA[Recent advances in linguistic steganalysis have successively applied CNNs,
RNNs, GNNs and other deep learning models for detecting secret information in
generative texts. These methods tend to seek stronger feature extractors to
achieve higher steganalysis effects. However, we have found through experiments
that there actually exists significant difference between automatically
generated steganographic texts and carrier texts in terms of the conditional
probability distribution of individual words. Such kind of statistical
difference can be naturally captured by the language model used for generating
steganographic texts, which drives us to give the classifier a priori knowledge
of the language model to enhance the steganalysis ability. To this end, we
present two methods to efficient linguistic steganalysis in this paper. One is
to pre-train a language model based on RNN, and the other is to pre-train a
sequence autoencoder. Experimental results show that the two methods have
different degrees of performance improvement when compared to the randomly
initialized RNN classifier, and the convergence speed is significantly
accelerated. Moreover, our methods have achieved the best detection results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1"&gt;Biao Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanzhou Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1"&gt;Guorui Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinpeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition. (arXiv:2107.11412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11412</id>
        <link href="http://arxiv.org/abs/2107.11412"/>
        <updated>2021-07-27T02:03:31.313Z</updated>
        <summary type="html"><![CDATA[The recent developments in technology have re-warded us with amazing audio
synthesis models like TACOTRON and WAVENETS. On the other side, it poses
greater threats such as speech clones and deep fakes, that may go undetected.
To tackle these alarming situations, there is an urgent need to propose models
that can help discriminate a synthesized speech from an actual human speech and
also identify the source of such a synthesis. Here, we propose a model based on
Convolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network
(BiRNN) that helps to achieve both the aforementioned objectives. The temporal
dependencies present in AI synthesized speech are exploited using Bidirectional
RNN and CNN. The model outperforms the state-of-the-art approaches by
classifying the AI synthesized audio from real human speech with an error rate
of 1.9% and detecting the underlying architecture with an accuracy of 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Arun Kumar Singh&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Priyanka Singh&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Nathwani_K/0/1/0/all/0/1"&gt;Karan Nathwani&lt;/a&gt; (1) ((1) Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of Information and Communication Technology)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negation Handling in Machine Learning-Based Sentiment Classification for Colloquial Arabic. (arXiv:2107.11597v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11597</id>
        <link href="http://arxiv.org/abs/2107.11597"/>
        <updated>2021-07-27T02:03:31.284Z</updated>
        <summary type="html"><![CDATA[One crucial aspect of sentiment analysis is negation handling, where the
occurrence of negation can flip the sentiment of a sentence and negatively
affects the machine learning-based sentiment classification. The role of
negation in Arabic sentiment analysis has been explored only to a limited
extent, especially for colloquial Arabic. In this paper, the author addresses
the negation problem of machine learning-based sentiment classification for a
colloquial Arabic language. To this end, we propose a simple rule-based
algorithm for handling the problem; the rules were crafted based on observing
many cases of negation. Additionally, simple linguistic knowledge and sentiment
lexicon are used for this purpose. The author also examines the impact of the
proposed algorithm on the performance of different machine learning algorithms.
The results given by the proposed algorithm are compared with three baseline
models. The experimental results show that there is a positive impact on the
classifiers accuracy, precision and recall when the proposed algorithm is used
compared to the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Harbi_O/0/1/0/all/0/1"&gt;Omar Al-Harbi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-based Music Recommendation: Evolution, State of the Art, and Challenges. (arXiv:2107.11803v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.11803</id>
        <link href="http://arxiv.org/abs/2107.11803"/>
        <updated>2021-07-27T02:03:30.906Z</updated>
        <summary type="html"><![CDATA[The music domain is among the most important ones for adopting recommender
systems technology. In contrast to most other recommendation domains, which
predominantly rely on collaborative filtering (CF) techniques, music
recommenders have traditionally embraced content-based (CB) approaches. In the
past years, music recommendation models that leverage collaborative and content
data -- which we refer to as content-driven models -- have been replacing pure
CF or CB models.

In this survey, we review 47 articles on content-driven music recommendation.
Based on a thorough literature analysis, we first propose an onion model
comprising five layers, each of which corresponds to a category of music
content we identified: signal, embedded metadata, expert-generated content,
user-generated content, and derivative content. We provide a detailed
characterization of each category along several dimensions. Second, we identify
six overarching challenges, according to which we organize our main discussion:
increasing recommendation diversity and novelty, providing transparency and
explanations, accomplishing context-awareness, recommending sequences of music,
improving scalability and efficiency, and alleviating cold start. Each article
addressing one or more of these challenges is categorized according to the
content layers of our onion model, the article's goal(s), and main
methodological choices. Furthermore, articles are discussed in temporal order
to shed light on the evolution of content-driven music recommendation
strategies. Finally, we provide our personal selection of the persisting grand
challenges, which are still waiting to be solved in future research endeavors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1"&gt;Yashar Deldjoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1"&gt;Peter Knees&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can the Crowd Judge Truthfulness? A Longitudinal Study on Recent Misinformation about COVID-19. (arXiv:2107.11755v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.11755</id>
        <link href="http://arxiv.org/abs/2107.11755"/>
        <updated>2021-07-27T02:03:30.714Z</updated>
        <summary type="html"><![CDATA[Recently, the misinformation problem has been addressed with a
crowdsourcing-based approach: to assess the truthfulness of a statement,
instead of relying on a few experts, a crowd of non-expert is exploited. We
study whether crowdsourcing is an effective and reliable method to assess
truthfulness during a pandemic, targeting statements related to COVID-19, thus
addressing (mis)information that is both related to a sensitive and personal
issue and very recent as compared to when the judgment is done. In our
experiments, crowd workers are asked to assess the truthfulness of statements,
and to provide evidence for the assessments. Besides showing that the crowd is
able to accurately judge the truthfulness of the statements, we report results
on workers behavior, agreement among workers, effect of aggregation functions,
of scales transformations, and of workers background and bias. We perform a
longitudinal study by re-launching the task multiple times with both novice and
experienced workers, deriving important insights on how the behavior and
quality change over time. Our results show that: workers are able to detect and
objectively categorize online (mis)information related to COVID-19; both
crowdsourced and expert judgments can be transformed and aggregated to improve
quality; worker background and other signals (e.g., source of information,
behavior) impact the quality of the data. The longitudinal study demonstrates
that the time-span has a major effect on the quality of the judgments, for both
novice and experienced workers. Finally, we provide an extensive failure
analysis of the statements misjudged by the crowd-workers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roitero_K/0/1/0/all/0/1"&gt;Kevin Roitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soprano_M/0/1/0/all/0/1"&gt;Michael Soprano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1"&gt;Beatrice Portelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luise_M/0/1/0/all/0/1"&gt;Massimiliano De Luise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1"&gt;Damiano Spina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mea_V/0/1/0/all/0/1"&gt;Vincenzo Della Mea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1"&gt;Giuseppe Serra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mizzaro_S/0/1/0/all/0/1"&gt;Stefano Mizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1"&gt;Gianluca Demartini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Intervene: An Adaptive Learning Policy for Restless Bandits in Application to Preventive Healthcare. (arXiv:2105.07965v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07965</id>
        <link href="http://arxiv.org/abs/2105.07965"/>
        <updated>2021-07-26T02:01:00.229Z</updated>
        <summary type="html"><![CDATA[In many public health settings, it is important for patients to adhere to
health programs, such as taking medications and periodic health checks.
Unfortunately, beneficiaries may gradually disengage from such programs, which
is detrimental to their health. A concrete example of gradual disengagement has
been observed by an organization that carries out a free automated call-based
program for spreading preventive care information among pregnant women. Many
women stop picking up calls after being enrolled for a few months. To avoid
such disengagements, it is important to provide timely interventions. Such
interventions are often expensive and can be provided to only a small fraction
of the beneficiaries. We model this scenario as a restless multi-armed bandit
(RMAB) problem, where each beneficiary is assumed to transition from one state
to another depending on the intervention. Moreover, since the transition
probabilities are unknown a priori, we propose a Whittle index based Q-Learning
mechanism and show that it converges to the optimal solution. Our method
improves over existing learning-based methods for RMABs on multiple benchmarks
from literature and also on the maternal healthcare dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1"&gt;Arpita Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1"&gt;Gaurav Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1"&gt;Pradeep Varakantham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07455</id>
        <link href="http://arxiv.org/abs/2107.07455"/>
        <updated>2021-07-26T02:01:00.222Z</updated>
        <summary type="html"><![CDATA[There has been significant research done on developing methods for improving
robustness to distributional shift and uncertainty estimation. In contrast,
only limited work has examined developing standard datasets and benchmarks for
assessing these approaches. Additionally, most work on uncertainty estimation
and robustness has developed new techniques based on small-scale regression or
image classification tasks. However, many tasks of practical interest have
different modalities, such as tabular data, audio, text, or sensor data, which
offer significant challenges involving regression and discrete or continuous
structured prediction. Thus, given the current state of the field, a
standardized large-scale dataset of tasks across a range of modalities affected
by distributional shifts is necessary. This will enable researchers to
meaningfully evaluate the plethora of recently developed uncertainty
quantification methods, as well as assessment criteria and state-of-the-art
baselines. In this work, we propose the \emph{Shifts Dataset} for evaluation of
uncertainty estimates and robustness to distributional shift. The dataset,
which has been collected from industrial sources and services, is composed of
three tasks, with each corresponding to a particular data modality: tabular
weather prediction, machine translation, and self-driving car (SDC) vehicle
motion prediction. All of these data modalities and tasks are affected by real,
`in-the-wild' distributional shifts and pose interesting challenges with
respect to uncertainty estimation. In this work we provide a description of the
dataset and baseline results for all tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1"&gt;Andrey Malinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganshin/0/1/0/all/0/1"&gt;Ganshin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander/0/1/0/all/0/1"&gt;Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1"&gt;German Chesnokov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1"&gt;Mark J. F. Gales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1"&gt;Alexey Noskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1"&gt;Andrey Ploskonosov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1"&gt;Liudmila Prokhorenkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1"&gt;Ivan Provilkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vatsal Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vyas Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roginskiy/0/1/0/all/0/1"&gt;Roginskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denis/0/1/0/all/0/1"&gt;Denis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1"&gt;Mariya Shmatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1"&gt;Panos Tigas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1"&gt;Boris Yangel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12342</id>
        <link href="http://arxiv.org/abs/2105.12342"/>
        <updated>2021-07-26T02:01:00.214Z</updated>
        <summary type="html"><![CDATA[While solutions of Distributionally Robust Optimization (DRO) problems can
sometimes have a higher out-of-sample expected reward than the Sample Average
Approximation (SAA), there is no guarantee. In this paper, we introduce the
class of Distributionally Optimistic Optimization (DOO) models, and show that
it is always possible to "beat" SAA out-of-sample if we consider not just
worst-case (DRO) models but also best-case (DOO) ones. We also show, however,
that this comes at a cost: Optimistic solutions are more sensitive to model
error than either worst-case or SAA optimizers, and hence are less robust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gotoh_J/0/1/0/all/0/1"&gt;Jun-ya Gotoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kim_M/0/1/0/all/0/1"&gt;Michael Jong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lim_A/0/1/0/all/0/1"&gt;Andrew E.B. Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyTorch, Explain! A Python library for Logic Explained Networks. (arXiv:2105.11697v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11697</id>
        <link href="http://arxiv.org/abs/2105.11697"/>
        <updated>2021-07-26T02:01:00.205Z</updated>
        <summary type="html"><![CDATA["PyTorch, Explain!" is a Python module integrating a variety of
state-of-the-art approaches to provide logic explanations from neural networks.
This package focuses on bringing these methods to non-specialists. It has
minimal dependencies and it is distributed under the Apache 2.0 licence
allowing both academic and commercial use. Source code and documentation can be
downloaded from the github repository:
https://github.com/pietrobarbiero/pytorch_explain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1"&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Franscesco Giannini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05852</id>
        <link href="http://arxiv.org/abs/2106.05852"/>
        <updated>2021-07-26T02:01:00.198Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the
various linguistic peculiarities present in the language. The Sanskrit language
is lexically productive, undergoes euphonic assimilation of phones at the word
boundaries and exhibits variations in spelling conventions and in
pronunciations. In this work, we propose the first large scale study of
automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact
of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR
dataset for Sanskrit, which faithfully captures several of the linguistic
characteristics expressed by the language. We investigate the role of different
acoustic model and language model units in ASR systems for Sanskrit. We also
propose a new modelling unit, inspired by the syllable level unit selection,
that captures character sequences from one vowel in the word to the next vowel.
We also highlight the importance of choosing graphemic representations for
Sanskrit and show the impact of this choice on word error rates (WER). Finally,
we extend these insights from Sanskrit ASR for building ASR systems in two
other Indic languages, Gujarati and Telugu. For both these languages, our
experimental results show that the use of phonetic based graphemic
representations in ASR results in performance improvements as compared to ASR
systems that use native scripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1"&gt;Devaraja Adiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rishabh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1"&gt;Amrith Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15561</id>
        <link href="http://arxiv.org/abs/2106.15561"/>
        <updated>2021-07-26T02:01:00.174Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank Soong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Multi-type Temporal Sequences to Mitigate Class-imbalanced Problem. (arXiv:2104.03428v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03428</id>
        <link href="http://arxiv.org/abs/2104.03428"/>
        <updated>2021-07-26T02:01:00.167Z</updated>
        <summary type="html"><![CDATA[From the ad network standpoint, a user's activity is a multi-type sequence of
temporal events consisting of event types and time intervals. Understanding
user patterns in ad networks has received increasing attention from the machine
learning community. Particularly, the problems of fraud detection, Conversion
Rate (CVR), and Click-Through Rate (CTR) prediction are of interest. However,
the class imbalance between major and minor classes in these tasks can bias a
machine learning model leading to poor performance. This study proposes using
two multi-type (continuous and discrete) training approaches for GANs to deal
with the limitations of traditional GANs in passing the gradient updates for
discrete tokens. First, we used the Reinforcement Learning (RL)-based training
approach and then, an approximation of the multinomial distribution
parameterized in terms of the softmax function (Gumble-Softmax). Our extensive
experiments based on synthetic data have shown the trained generator can
generate sequences with desired properties measured by multiple criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadghiani_N/0/1/0/all/0/1"&gt;Nima Salehi Sadghiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhuo Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1"&gt;Andrew Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval-censored Hawkes processes. (arXiv:2104.07932v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07932</id>
        <link href="http://arxiv.org/abs/2104.07932"/>
        <updated>2021-07-26T02:01:00.159Z</updated>
        <summary type="html"><![CDATA[This work builds a novel point process and tools to use the Hawkes process
with interval-censored data. Such data records the aggregated counts of events
solely during specific time intervals -- such as the number of patients
admitted to the hospital or the volume of vehicles passing traffic loop
detectors -- and not the exact occurrence time of the events. First, we
establish the Mean Behavior Poisson (MBP) process, a novel Poisson process with
a direct parameter correspondence to the popular self-exciting Hawkes process.
The event intensity function of the MBP is the expected intensity over all
possible Hawkes realizations with the same parameter set. We fit MBP in the
interval-censored setting using an interval-censored Poisson log-likelihood
(IC-LL). We use the parameter equivalence to uncover the parameters of the
associated Hawkes process. Second, we introduce two novel exogenous functions
to distinguish the exogenous from the endogenous events. We propose the
multi-impulse exogenous function when the exogenous events are observed as
event time and the latent homogeneous Poisson process exogenous function when
the exogenous events are presented as interval-censored volumes. Third, we
provide several approximation methods to estimate the intensity and compensator
function of MBP when no analytical solution exists. Fourth and finally, we
connect the interval-censored loss of MBP to a broader class of Bregman
divergence-based functions. Using the connection, we show that the current
state of the art in popularity estimation (Hawkes Intensity Process (HIP)
(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our
models through empirical testing on synthetic data and real-world data. We find
that on real-world datasets that our MBP process outperforms HIP for the task
of popularity prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1"&gt;Alexander Soen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shidi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1"&gt;Pio Calderon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Leanne Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?. (arXiv:2105.02498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02498</id>
        <link href="http://arxiv.org/abs/2105.02498"/>
        <updated>2021-07-26T02:01:00.152Z</updated>
        <summary type="html"><![CDATA[Global covariance pooling (GCP) aims at exploiting the second-order
statistics of the convolutional feature. Its effectiveness has been
demonstrated in boosting the classification performance of Convolutional Neural
Networks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute
the matrix square root. However, the approximate matrix square root calculated
using Newton-Schulz iteration \cite{li2018towards} outperforms the accurate one
computed via SVD \cite{li2017second}. We empirically analyze the reason behind
the performance gap from the perspectives of data precision and gradient
smoothness. Various remedies for computing smooth SVD gradients are
investigated. Based on our observation and analyses, a hybrid training protocol
is proposed for SVD-based GCP meta-layers such that competitive performances
can be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP
meta-layer that uses SVD in the forward pass, and Pad\'e Approximants in the
backward propagation to compute the gradients. The proposed meta-layer has been
integrated into different CNN models and achieves state-of-the-art performances
on both large-scale and fine-grained datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yue Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT. (arXiv:2104.03466v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03466</id>
        <link href="http://arxiv.org/abs/2104.03466"/>
        <updated>2021-07-26T02:01:00.144Z</updated>
        <summary type="html"><![CDATA[Many real-world IoT systems, which include a variety of internet-connected
sensory devices, produce substantial amounts of multivariate time series data.
Meanwhile, vital IoT infrastructures like smart power grids and water
distribution networks are frequently targeted by cyber-attacks, making anomaly
detection an important study topic. Modeling such relatedness is, nevertheless,
unavoidable for any efficient and effective anomaly detection system, given the
intricate topological and nonlinear connections that are originally unknown
among sensors. Furthermore, detecting anomalies in multivariate time series is
difficult due to their temporal dependency and stochasticity. This paper
presented GTA, a new framework for multivariate time series anomaly detection
that involves automatically learning a graph structure, graph convolution, and
modeling temporal dependency using a Transformer-based architecture. The
connection learning policy, which is based on the Gumbel-softmax sampling
approach to learn bi-directed links among sensors directly, is at the heart of
learning graph structure. To describe the anomaly information flow between
network nodes, we introduced a new graph convolution called Influence
Propagation convolution. In addition, to tackle the quadratic complexity
barrier, we suggested a multi-branch attention mechanism to replace the
original multi-head self-attention method. Extensive experiments on four
publicly available anomaly detection benchmarks further demonstrate the
superiority of our approach over alternative state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dingshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zixuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Representation for Neural Code Search. (arXiv:2107.00992v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00992</id>
        <link href="http://arxiv.org/abs/2107.00992"/>
        <updated>2021-07-26T02:01:00.137Z</updated>
        <summary type="html"><![CDATA[Semantic code search is about finding semantically relevant code snippets for
a given natural language query. In the state-of-the-art approaches, the
semantic similarity between code and query is quantified as the distance of
their representation in the shared vector space. In this paper, to improve the
vector space, we introduce tree-serialization methods on a simplified form of
AST and build the multimodal representation for the code data. We conduct
extensive experiments using a single corpus that is large-scale and
multi-language: CodeSearchNet. Our results show that both our tree-serialized
representations and multimodal learning model improve the performance of code
search. Last, we define intuitive quantification metrics oriented to the
completeness of semantic and syntactic information of the code data, to help
understand the experimental findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zimin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1"&gt;Martin Monperrus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularity and stability of feedback relaxed controls. (arXiv:2001.03148v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03148</id>
        <link href="http://arxiv.org/abs/2001.03148"/>
        <updated>2021-07-26T02:01:00.113Z</updated>
        <summary type="html"><![CDATA[This paper proposes a relaxed control regularization with general exploration
rewards to design robust feedback controls for multi-dimensional
continuous-time stochastic exit time problems. We establish that the
regularized control problem admits a H\"{o}lder continuous feedback control,
and demonstrate that both the value function and the feedback control of the
regularized control problem are Lipschitz stable with respect to parameter
perturbations. Moreover, we show that a pre-computed feedback relaxed control
has a robust performance in a perturbed system, and derive a first-order
sensitivity equation for both the value function and optimal feedback relaxed
control. These stability results provide a theoretical justification for recent
reinforcement learning heuristics that including an exploration reward in the
optimization objective leads to more robust decision making. We finally prove
first-order monotone convergence of the value functions for relaxed control
problems with vanishing exploration parameters, which subsequently enables us
to construct the pure exploitation strategy of the original control problem
based on the feedback relaxed controls.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Reisinger_C/0/1/0/all/0/1"&gt;Christoph Reisinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yufei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Including Signed Languages in Natural Language Processing. (arXiv:2105.05222v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05222</id>
        <link href="http://arxiv.org/abs/2105.05222"/>
        <updated>2021-07-26T02:01:00.106Z</updated>
        <summary type="html"><![CDATA[Signed languages are the primary means of communication for many deaf and
hard of hearing individuals. Since signed languages exhibit all the fundamental
linguistic properties of natural language, we believe that tools and theories
of Natural Language Processing (NLP) are crucial towards its modeling. However,
existing research in Sign Language Processing (SLP) seldom attempt to explore
and leverage the linguistic organization of signed languages. This position
paper calls on the NLP community to include signed languages as a research area
with high social and scientific impact. We first discuss the linguistic
properties of signed languages to consider during their modeling. Then, we
review the limitations of current SLP models and identify the open challenges
to extend NLP to signed languages. Finally, we urge (1) the adoption of an
efficient tokenization method; (2) the development of linguistically-informed
models; (3) the collection of real-world signed language data; (4) the
inclusion of local signed language communities as an active and leading voice
in the direction of research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1"&gt;Amit Moryossef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochgesang_J/0/1/0/all/0/1"&gt;Julie Hochgesang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1"&gt;Malihe Alikhani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Function approximation by deep neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$. (arXiv:2103.08659v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08659</id>
        <link href="http://arxiv.org/abs/2103.08659"/>
        <updated>2021-07-26T02:01:00.098Z</updated>
        <summary type="html"><![CDATA[In this paper it is shown that $C_\beta$-smooth functions can be approximated
by deep neural networks with ReLU activation function and with parameters
$\{0,\pm \frac{1}{2}, \pm 1, 2\}$. The $l_0$ and $l_1$ parameter norms of
considered networks are thus equivalent. The depth, width and the number of
active parameters of the constructed networks have, up to a logarithmic factor,
the same dependence on the approximation error as the networks with parameters
in $[-1,1]$. In particular, this means that the nonparametric regression
estimation with the constructed networks attains the same convergence rate as
with sparse networks with parameters in $[-1,1]$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpReg-Boost: Learning to Accelerate Online Algorithms with Operator Regression. (arXiv:2105.13271v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13271</id>
        <link href="http://arxiv.org/abs/2105.13271"/>
        <updated>2021-07-26T02:01:00.075Z</updated>
        <summary type="html"><![CDATA[This paper presents a new regularization approach -- termed OpReg-Boost -- to
boost the convergence and lessen the asymptotic error of online optimization
and learning algorithms. In particular, the paper considers online algorithms
for optimization problems with a time-varying (weakly) convex composite cost.
For a given online algorithm, OpReg-Boost learns the closest algorithmic map
that yields linear convergence; to this end, the learning procedure hinges on
the concept of operator regression. We show how to formalize the operator
regression problem and propose a computationally-efficient Peaceman-Rachford
solver that exploits a closed-form solution of simple quadratically-constrained
quadratic programs (QCQPs). Simulation results showcase the superior properties
of OpReg-Boost w.r.t. the more classical forward-backward algorithm, FISTA, and
Anderson acceleration, and with respect to its close relative
convex-regression-boost (CvxReg-Boost) which is also novel but less performing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bastianello_N/0/1/0/all/0/1"&gt;Nicola Bastianello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simonetto_A/0/1/0/all/0/1"&gt;Andrea Simonetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DallAnese_E/0/1/0/all/0/1"&gt;Emiliano Dall&amp;#x27;Anese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next Generation Reservoir Computing. (arXiv:2106.07688v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07688</id>
        <link href="http://arxiv.org/abs/2106.07688"/>
        <updated>2021-07-26T02:01:00.067Z</updated>
        <summary type="html"><![CDATA[Reservoir computing is a best-in-class machine learning algorithm for
processing information generated by dynamical systems using observed
time-series data. Importantly, it requires very small training data sets, uses
linear optimization, and thus requires minimal computing resources. However,
the algorithm uses randomly sampled matrices to define the underlying recurrent
neural network and has a multitude of metaparameters that must be optimized.
Recent results demonstrate the equivalence of reservoir computing to nonlinear
vector autoregression, which requires no random matrices, fewer metaparameters,
and provides interpretable results. Here, we demonstrate that nonlinear vector
autoregression excels at reservoir computing benchmark tasks and requires even
shorter training data sets and training time, heralding the next generation of
reservoir computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1"&gt;Daniel J. Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1"&gt;Erik Bollt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1"&gt;Aaron Griffith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1"&gt;Wendson A.S. Barbosa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Convolutional Kernel Networks for Airline Crew Scheduling. (arXiv:2105.11646v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11646</id>
        <link href="http://arxiv.org/abs/2105.11646"/>
        <updated>2021-07-26T02:01:00.059Z</updated>
        <summary type="html"><![CDATA[Motivated by the needs from an airline crew scheduling application, we
introduce structured convolutional kernel networks (Struct-CKN), which combine
CKNs from Mairal et al. (2014) in a structured prediction framework that
supports constraints on the outputs. CKNs are a particular kind of
convolutional neural networks that approximate a kernel feature map on training
data, thus combining properties of deep learning with the non-parametric
flexibility of kernel methods. Extending CKNs to structured outputs allows us
to obtain useful initial solutions on a flight-connection dataset that can be
further refined by an airline crew scheduling solver. More specifically, we use
a flight-based network modeled as a general conditional random field capable of
incorporating local constraints in the learning process. Our experiments
demonstrate that this approach yields significant improvements for the
large-scale crew pairing problem (50,000 flights per month) over standard
approaches, reducing the solution cost by 17% (a gain of millions of dollars)
and the cost of global constraints by 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaakoubi_Y/0/1/0/all/0/1"&gt;Yassine Yaakoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soumis_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Soumis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling Exploration and Exploitation in Reinforcement Learning. (arXiv:2107.08966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08966</id>
        <link href="http://arxiv.org/abs/2107.08966"/>
        <updated>2021-07-26T02:01:00.051Z</updated>
        <summary type="html"><![CDATA[Intrinsic rewards are commonly applied to improve exploration in
reinforcement learning. However, these approaches suffer from instability
caused by non-stationary reward shaping and strong dependency on
hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains
separate policies for exploration and exploitation. DeRL can be applied with
on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two
sparse-reward environments with multiple types of intrinsic rewards. We show
that DeRL is more robust to scaling and speed of decay of intrinsic rewards and
converges to the same evaluation returns than intrinsically motivated baselines
in fewer interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1"&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1"&gt;Filippos Christianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1"&gt;Josiah Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13423</id>
        <link href="http://arxiv.org/abs/2106.13423"/>
        <updated>2021-07-26T02:01:00.036Z</updated>
        <summary type="html"><![CDATA[Federated learning has emerged as an important paradigm for training machine
learning models in different domains. For graph-level tasks such as graph
classification, graphs can also be regarded as a special type of data samples,
which can be collected and stored in separate local systems. Similar to other
domains, multiple local systems, each holding a small set of graphs, may
benefit from collaboratively training a powerful graph mining model, such as
the popular graph neural networks (GNNs). To provide more motivation towards
such endeavors, we analyze real-world graphs from different domains to confirm
that they indeed share certain graph properties that are statistically
significant compared with random graphs. However, we also find that different
sets of graphs, even from the same domain or same dataset, are non-IID
regarding both graph structures and node features. To handle this, we propose a
graph clustered federated learning (GCFL) framework that dynamically finds
clusters of local systems based on the gradients of GNNs, and theoretically
justify that such clusters can reduce the structure and feature heterogeneity
among graphs owned by the local systems. Moreover, we observe the gradients of
GNNs to be rather fluctuating in GCFL which impedes high-quality clustering,
and design a gradient sequence-based clustering mechanism based on dynamic time
warping (GCFL+). Extensive experimental results and in-depth analysis
demonstrate the effectiveness of our proposed frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Han Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the simplicity and conditioning of low rank semidefinite programs. (arXiv:2002.10673v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10673</id>
        <link href="http://arxiv.org/abs/2002.10673"/>
        <updated>2021-07-26T02:01:00.018Z</updated>
        <summary type="html"><![CDATA[Low rank matrix recovery problems appear widely in statistics, combinatorics,
and imaging. One celebrated method for solving these problems is to formulate
and solve a semidefinite program (SDP). It is often known that the exact
solution to the SDP with perfect data recovers the solution to the original low
rank matrix recovery problem. It is more challenging to show that an
approximate solution to the SDP formulated with noisy problem data acceptably
solves the original problem; arguments are usually ad hoc for each problem
setting, and can be complex.

In this note, we identify a set of conditions that we call simplicity that
limit the error due to noisy problem data or incomplete convergence. In this
sense, simple SDPs are robust: simple SDPs can be (approximately) solved
efficiently at scale; and the resulting approximate solutions, even with noisy
data, can be trusted. Moreover, we show that simplicity holds generically, and
also for many structured low rank matrix recovery problems, including the
stochastic block model, $\mathbb{Z}_2$ synchronization, and matrix completion.
Formally, we call an SDP simple if it has a surjective constraint map, admits a
unique primal and dual solution pair, and satisfies strong duality and strict
complementarity.

However, simplicity is not a panacea: we show the Burer-Monteiro formulation
of the SDP may have spurious second-order critical points, even for a simple
SDP with a rank 1 solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ding_L/0/1/0/all/0/1"&gt;Lijun Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zeroth-Order Regularized Optimization (ZORO): Approximately Sparse Gradients and Adaptive Sampling. (arXiv:2003.13001v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.13001</id>
        <link href="http://arxiv.org/abs/2003.13001"/>
        <updated>2021-07-26T02:01:00.010Z</updated>
        <summary type="html"><![CDATA[We consider the problem of minimizing a high-dimensional objective function,
which may include a regularization term, using (possibly noisy) evaluations of
the function. Such optimization is also called derivative-free, zeroth-order,
or black-box optimization. We propose a new $\textbf{Z}$eroth-$\textbf{O}$rder
$\textbf{R}$egularized $\textbf{O}$ptimization method, dubbed ZORO. When the
underlying gradient is approximately sparse at an iterate, ZORO needs very few
objective function evaluations to obtain a new iterate that decreases the
objective function. We achieve this with an adaptive, randomized gradient
estimator, followed by an inexact proximal-gradient scheme. Under a novel
approximately sparse gradient assumption and various different convex settings,
we show the (theoretical and empirical) convergence rate of ZORO is only
logarithmically dependent on the problem dimension. Numerical experiments show
that ZORO outperforms the existing methods with similar assumptions, on both
synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mckenzie_D/0/1/0/all/0/1"&gt;Daniel Mckenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenliang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Weighted Automata for Approximate Minimization in Language Modelling. (arXiv:2106.02965v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02965</id>
        <link href="http://arxiv.org/abs/2106.02965"/>
        <updated>2021-07-26T02:01:00.001Z</updated>
        <summary type="html"><![CDATA[In this paper we study the approximate minimization problem for language
modelling. We assume we are given some language model as a black box. The
objective is to obtain a weighted finite automaton (WFA) that fits within a
given size constraint and which mimics the behaviour of the original model
while minimizing some notion of distance between the black box and the
extracted WFA. We provide an algorithm for the approximate minimization of
black boxes trained for language modelling of sequential data over a one-letter
alphabet. By reformulating the problem in terms of Hankel matrices, we leverage
classical results on the approximation of Hankel operators, namely the
celebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral
norm to measure the distance between the black box and the WFA. We provide
theoretical guarantees to study the potentially infinite-rank Hankel matrix of
the black box, without accessing the training data, and we prove that our
method returns an asymptotically-optimal approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lacroce_C/0/1/0/all/0/1"&gt;Clara Lacroce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1"&gt;Prakash Panangaden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things. (arXiv:2106.06498v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06498</id>
        <link href="http://arxiv.org/abs/2106.06498"/>
        <updated>2021-07-26T02:00:59.969Z</updated>
        <summary type="html"><![CDATA[The Internet of Medical Things (IoMT) paradigm is becoming mainstream in
multiple clinical trials and healthcare procedures. Cardiovascular diseases
monitoring, usually involving electrocardiogram (ECG) traces analysis, is one
of the most promising and high-impact applications. Nevertheless, to fully
exploit the potential of IoMT in this domain, some steps forward are needed.
First, the edge-computing paradigm must be added to the picture. A certain
level of near-sensor processing has to be enabled, to improve the scalability,
portability, reliability, responsiveness of the IoMT nodes. Second, novel,
increasingly accurate, data analysis algorithms, such as those based on
artificial intelligence and Deep Learning, must be exploited. To reach these
objectives, designers and programmers of IoMT nodes, have to face challenging
optimization tasks, in order to execute fairly complex computing tasks on
low-power wearable and portable processing systems, with tight power and
battery lifetime budgets. In this work, we explore the implementation of a
cognitive data analysis algorithm, based on a convolutional neural network
trained to classify ECG waveforms, on a resource-constrained
microcontroller-based computing platform. To minimize power consumption, we add
an adaptivity layer that dynamically manages the hardware and software
configuration of the device to adapt it at runtime to the required operating
mode. Our experimental results show that adapting the node setup to the
workload at runtime can save up to 50% power consumption. Our optimized and
quantized neural network reaches an accuracy value higher than 97% for
arrhythmia disorders detection on MIT-BIH Arrhythmia dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scrugli_M/0/1/0/all/0/1"&gt;Matteo Antonio Scrugli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loi_D/0/1/0/all/0/1"&gt;Daniela Loi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffo_L/0/1/0/all/0/1"&gt;Luigi Raffo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meloni_P/0/1/0/all/0/1"&gt;Paolo Meloni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Precise High-Dimensional Asymptotic Theory for Boosting and Minimum-$\ell_1$-Norm Interpolated Classifiers. (arXiv:2002.01586v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01586</id>
        <link href="http://arxiv.org/abs/2002.01586"/>
        <updated>2021-07-26T02:00:59.926Z</updated>
        <summary type="html"><![CDATA[This paper establishes a precise high-dimensional asymptotic theory for
boosting on separable data, taking statistical and computational perspectives.
We consider a high-dimensional setting where the number of features (weak
learners) $p$ scales with the sample size $n$, in an overparametrized regime.
Under a class of statistical models, we provide an exact analysis of the
generalization error of boosting when the algorithm interpolates the training
data and maximizes the empirical $\ell_1$-margin. Further, we explicitly pin
down the relation between the boosting test error and the optimal Bayes error,
as well as the proportion of active features at interpolation (with zero
initialization). In turn, these precise characterizations answer certain
questions raised in \cite{breiman1999prediction, schapire1998boosting}
surrounding boosting, under assumed data generating processes. At the heart of
our theory lies an in-depth study of the maximum-$\ell_1$-margin, which can be
accurately described by a new system of non-linear equations; to analyze this
margin, we rely on Gaussian comparison techniques and develop a novel uniform
deviation argument. Our statistical and computational arguments can handle (1)
any finite-rank spiked covariance model for the feature distribution and (2)
variants of boosting corresponding to general $\ell_q$-geometry, $q \in [1,
2]$. As a final component, via the Lindeberg principle, we establish a
universality result showcasing that the scaled $\ell_1$-margin (asymptotically)
remains the same, whether the covariates used for boosting arise from a
non-linear random feature model or an appropriately linearized model with
matching moments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tengyuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sur_P/0/1/0/all/0/1"&gt;Pragya Sur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experiments with a PCCoder extension. (arXiv:1912.00781v2 [cs.PL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.00781</id>
        <link href="http://arxiv.org/abs/1912.00781"/>
        <updated>2021-07-26T02:00:59.908Z</updated>
        <summary type="html"><![CDATA[Recent research in synthesis of programs written in some Domain Specific
Language (DSL) by means of neural networks from a limited set of inputs-output
correspondences such as DeepCoder and its PCCoder reimplementation/optimization
proved the efficiency of this kind of approach to automatic program generation
in a DSL language that although limited in scope is universal in the sense that
programs can be translated to basically any programming language. We experiment
with the extension of the DSL of DeepCoder/PCCoder with symbols IFI and IFL
which denote functional expressions of the If ramification (test) instruction
for types Int and List. We notice an increase (doubling) of the size of the
training set, the number of parameters of the trained neural network and of the
time spent looking for the program synthesized from limited sets of
inputs-output correspondences. The result is positive in the sense of
preserving the accuracy of applying synthesis on randomly generated test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernest_D/0/1/0/all/0/1"&gt;Dan Hernest&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[State, global and local parameter estimation using local ensemble Kalman filters: applications to online machine learning of chaotic dynamics. (arXiv:2107.11253v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.11253</id>
        <link href="http://arxiv.org/abs/2107.11253"/>
        <updated>2021-07-26T02:00:59.900Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that it is possible to combine machine learning
methods with data assimilation to reconstruct a dynamical system using only
sparse and noisy observations of that system. The same approach can be used to
correct the error of a knowledge-based model. The resulting surrogate model is
hybrid, with a statistical part supplementing a physical part. In practice, the
correction can be added as an integrated term (\textit{i.e.} in the model
resolvent) or directly inside the tendencies of the physical model. The
resolvent correction is easy to implement. The tendency correction is more
technical, in particular it requires the adjoint of the physical model, but
also more flexible. We use the two-scale Lorenz model to compare the two
methods. The accuracy in long-range forecast experiments is somewhat similar
between the surrogate models using the resolvent correction and the tendency
correction. By contrast, the surrogate models using the tendency correction
significantly outperform the surrogate models using the resolvent correction in
data assimilation experiments. Finally, we show that the tendency correction
opens the possibility to make online model error correction, \textit{i.e.}
improving the model progressively as new observations become available. The
resulting algorithm can be seen as a new formulation of weak-constraint 4D-Var.
We compare online and offline learning using the same framework with the
two-scale Lorenz system, and show that with online learning, it is possible to
extract all the information from sparse and noisy observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1"&gt;Quentin Malartic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1"&gt;Alban Farchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1"&gt;Marc Bocquet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11320</id>
        <link href="http://arxiv.org/abs/2107.11320"/>
        <updated>2021-07-26T02:00:59.887Z</updated>
        <summary type="html"><![CDATA[Forest carbon offsets are increasingly popular and can play a significant
role in financing climate mitigation, forest conservation, and reforestation.
Measuring how much carbon is stored in forests is, however, still largely done
via expensive, time-consuming, and sometimes unaccountable field measurements.
To overcome these limitations, many verification bodies are leveraging machine
learning (ML) algorithms to estimate forest carbon from satellite or aerial
imagery. Aerial imagery allows for tree species or family classification, which
improves the satellite imagery-based forest type classification. However,
aerial imagery is significantly more expensive to collect and it is unclear by
how much the higher resolution improves the forest carbon estimation. This
proposal paper describes the first systematic comparison of forest carbon
estimation from aerial imagery, satellite imagery, and ground-truth field
measurements via deep learning-based algorithms for a tropical reforestation
project. Our initial results show that forest carbon estimates from satellite
imagery can overestimate above-ground biomass by more than 10-times for
tropical reforestation projects. The significant difference between aerial and
satellite-derived forest carbon measurements shows the potential for aerial
imagery-based ML algorithms and raises the importance to extend this study to a
global benchmark between options for carbon measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1"&gt;Gyri Reiersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1"&gt;David Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1"&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Adaptive Explicit Differentiable Predictive Control for Unknown Linear Systems. (arXiv:2004.11184v5 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11184</id>
        <link href="http://arxiv.org/abs/2004.11184"/>
        <updated>2021-07-26T02:00:59.880Z</updated>
        <summary type="html"><![CDATA[We present differentiable predictive control (DPC), a method for learning
constrained adaptive neural control policies and dynamical models of unknown
linear systems. DPC presents an approximate data-driven solution approach to
the explicit Model Predictive Control (MPC) problem as a scalable alternative
to computationally expensive multiparametric programming solvers. DPC is
formulated as a constrained deep learning problem whose architecture is
inspired by the structure of classical MPC. The optimization of the neural
control policy is based on automatic differentiation of the MPC-inspired loss
function through a differentiable closed-loop system model. This novel solution
approach can optimize adaptive neural control policies for time-varying
references while obeying state and input constraints without the prior need of
an MPC controller. We show that DPC can learn to stabilize constrained neural
control policies for systems with unstable dynamics. Moreover, we provide
sufficient conditions for asymptotic stability of generic closed-loop system
dynamics with neural feedback policies. In simulation case studies, we assess
the performance of the proposed DPC method in terms of reference tracking,
robustness, and computational and memory footprints compared against classical
model-based and data-driven control approaches. We demonstrate that DPC scales
linearly with problem size, compared to exponential scalability of classical
explicit MPC based on multiparametric programming.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Drgona_J/0/1/0/all/0/1"&gt;Jan Drgona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tuor_A/0/1/0/all/0/1"&gt;Aaron Tuor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vrabie_D/0/1/0/all/0/1"&gt;Draguna Vrabie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimum Risk Portfolio and Eigen Portfolio: A Comparative Analysis Using Selected Stocks from the Indian Stock Market. (arXiv:2107.11371v1 [q-fin.PM])]]></title>
        <id>http://arxiv.org/abs/2107.11371</id>
        <link href="http://arxiv.org/abs/2107.11371"/>
        <updated>2021-07-26T02:00:59.856Z</updated>
        <summary type="html"><![CDATA[Designing an optimum portfolio that allocates weights to its constituent
stocks in a way that achieves the best trade-off between the return and the
risk is a challenging research problem. The classical mean-variance theory of
portfolio proposed by Markowitz is found to perform sub-optimally on the
real-world stock market data since the error in estimation for the expected
returns adversely affects the performance of the portfolio. This paper presents
three approaches to portfolio design, viz, the minimum risk portfolio, the
optimum risk portfolio, and the Eigen portfolio, for seven important sectors of
the Indian stock market. The daily historical prices of the stocks are scraped
from Yahoo Finance website from January 1, 2016, to December 31, 2020. Three
portfolios are built for each of the seven sectors chosen for this study, and
the portfolios are analyzed on the training data based on several metrics such
as annualized return and risk, weights assigned to the constituent stocks, the
correlation heatmaps, and the principal components of the Eigen portfolios.
Finally, the optimum risk portfolios and the Eigen portfolios for all sectors
are tested on their return over a period of a six-month period. The
performances of the portfolios are compared and the portfolio yielding the
higher return for each sector is identified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Sen_J/0/1/0/all/0/1"&gt;Jaydip Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mehtab_S/0/1/0/all/0/1"&gt;Sidra Mehtab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07101</id>
        <link href="http://arxiv.org/abs/2009.07101"/>
        <updated>2021-07-26T02:00:59.845Z</updated>
        <summary type="html"><![CDATA[Spectral clustering (SC) is one of the most popular clustering methods and
often outperforms traditional clustering methods. SC uses the eigenvectors of a
Laplacian matrix calculated from a similarity matrix of a dataset. SC has
serious drawbacks: the significant increases in the time complexity derived
from the computation of eigenvectors and the memory space complexity to store
the similarity matrix. To address the issues, I develop a new approximate
spectral clustering using the network generated by growing neural gas (GNG),
called ASC with GNG in this study. ASC with GNG uses not only reference vectors
for vector quantization but also the topology of the network for extraction of
the topological relationship between data points in a dataset. ASC with GNG
calculates the similarity matrix from both the reference vectors and the
topology of the network generated by GNG. Using the network generated from a
dataset by GNG, ASC with GNG achieves to reduce the computational and space
complexities and improve clustering quality. In this study, I demonstrate that
ASC with GNG effectively reduces the computational time. Moreover, this study
shows that ASC with GNG provides equal to or better clustering performance than
SC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1"&gt;Kazuhisa Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Mixed Integer Programs Using Neural Networks. (arXiv:2012.13349v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13349</id>
        <link href="http://arxiv.org/abs/2012.13349"/>
        <updated>2021-07-26T02:00:59.837Z</updated>
        <summary type="html"><![CDATA[Mixed Integer Programming (MIP) solvers rely on an array of sophisticated
heuristics developed with decades of research to solve large-scale MIP
instances encountered in practice. Machine learning offers to automatically
construct better heuristics from data by exploiting shared structure among
instances in the data. This paper applies learning to the two key sub-tasks of
a MIP solver, generating a high-quality joint variable assignment, and bounding
the gap in objective value between that assignment and an optimal one. Our
approach constructs two corresponding neural network-based components, Neural
Diving and Neural Branching, to use in a base MIP solver such as SCIP. Neural
Diving learns a deep neural network to generate multiple partial assignments
for its integer variables, and the resulting smaller MIPs for un-assigned
variables are solved with SCIP to construct high quality joint assignments.
Neural Branching learns a deep neural network to make variable selection
decisions in branch-and-bound to bound the objective value gap with a small
tree. This is done by imitating a new variant of Full Strong Branching we
propose that scales to large instances using GPUs. We evaluate our approach on
six diverse real-world datasets, including two Google production datasets and
MIPLIB, by training separate neural networks on each. Most instances in all the
datasets combined have $10^3-10^6$ variables and constraints after presolve,
which is significantly larger than previous learning approaches. Comparing
solvers with respect to primal-dual gap averaged over a held-out set of
instances, the learning-augmented SCIP is 2x to 10x better on all datasets
except one on which it is $10^5$x better, at large time limits. To the best of
our knowledge, ours is the first learning approach to demonstrate such large
improvements over SCIP on both large-scale real-world application datasets and
MIPLIB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vinod Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bartunov_S/0/1/0/all/0/1"&gt;Sergey Bartunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gimeno_F/0/1/0/all/0/1"&gt;Felix Gimeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Glehn_I/0/1/0/all/0/1"&gt;Ingrid von Glehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lichocki_P/0/1/0/all/0/1"&gt;Pawel Lichocki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lobov_I/0/1/0/all/0/1"&gt;Ivan Lobov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+ODonoghue_B/0/1/0/all/0/1"&gt;Brendan O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sonnerat_N/0/1/0/all/0/1"&gt;Nicolas Sonnerat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tjandraatmadja_C/0/1/0/all/0/1"&gt;Christian Tjandraatmadja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Addanki_R/0/1/0/all/0/1"&gt;Ravichandra Addanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hapuarachchi_T/0/1/0/all/0/1"&gt;Tharindi Hapuarachchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Keck_T/0/1/0/all/0/1"&gt;Thomas Keck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Keeling_J/0/1/0/all/0/1"&gt;James Keeling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kohli_P/0/1/0/all/0/1"&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ktena_I/0/1/0/all/0/1"&gt;Ira Ktena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zwols_Y/0/1/0/all/0/1"&gt;Yori Zwols&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Tree-Wasserstein Distance. (arXiv:2101.11520v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11520</id>
        <link href="http://arxiv.org/abs/2101.11520"/>
        <updated>2021-07-26T02:00:59.829Z</updated>
        <summary type="html"><![CDATA[To measure the similarity of documents, the Wasserstein distance is a
powerful tool, but it requires a high computational cost. Recently, for fast
computation of the Wasserstein distance, methods for approximating the
Wasserstein distance using a tree metric have been proposed. These tree-based
methods allow fast comparisons of a large number of documents; however, they
are unsupervised and do not learn task-specific distances. In this work, we
propose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised
metric learning method based on the tree metric. Specifically, we rewrite the
Wasserstein distance on the tree metric by the parent-child relationships of a
tree and formulate it as a continuous optimization problem using a contrastive
loss. Experimentally, we show that the STW distance can be computed fast, and
improves the accuracy of document classification tasks. Furthermore, the STW
distance is formulated by matrix multiplications, runs on a GPU, and is
suitable for batch processing. Therefore, we show that the STW distance is
extremely efficient when comparing a large number of documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1"&gt;Yuki Takezawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1"&gt;Ryoma Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1"&gt;Makoto Yamada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs Using Deep Reinforcement Learning. (arXiv:2101.08074v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08074</id>
        <link href="http://arxiv.org/abs/2101.08074"/>
        <updated>2021-07-26T02:00:59.821Z</updated>
        <summary type="html"><![CDATA[Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is
still a challenge due to kinematic complexity and environmental uncertainty. In
this paper, we deal with the decentralized flocking and collision avoidance
problem through deep reinforcement learning (DRL). Specifically, we formulate a
decentralized DRL-based decision making framework from the perspective of every
follower, where a collision avoidance mechanism is integrated into the flocking
controller. Then, we propose a novel reinforcement learning algorithm PS-CACER
for training a shared control policy for all the followers. Besides, we design
a plug-n-play embedding module based on convolutional neural networks and the
attention mechanism. As a result, the variable-length system state can be
encoded into a fixed-length embedding vector, which makes the learned DRL
policy independent with the number and the order of followers. Finally,
numerical simulation results demonstrate the effectiveness of the proposed
method, and the learned policies can be directly transferred to semi-physical
simulation without any parameter finetuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiang_X/0/1/0/all/0/1"&gt;Xiaojia Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zhen Lan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structack: Structure-based Adversarial Attacks on Graph Neural Networks. (arXiv:2107.11327v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11327</id>
        <link href="http://arxiv.org/abs/2107.11327"/>
        <updated>2021-07-26T02:00:59.795Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that graph neural networks (GNNs) are vulnerable to
adversarial attacks on graph data. Common attack approaches are typically
informed, i.e. they have access to information about node attributes such as
labels and feature vectors. In this work, we study adversarial attacks that are
uninformed, where an attacker only has access to the graph structure, but no
information about node attributes. Here the attacker aims to exploit structural
knowledge and assumptions, which GNN models make about graph data. In
particular, literature has shown that structural node centrality and similarity
have a strong influence on learning with GNNs. Therefore, we study the impact
of centrality and similarity on adversarial attacks on GNNs. We demonstrate
that attackers can exploit this information to decrease the performance of GNNs
by focusing on injecting links between nodes of low similarity and,
surprisingly, low centrality. We show that structure-based uninformed attacks
can approach the performance of informed attacks, while being computationally
more efficient. With our paper, we present a new attack strategy on GNNs that
we refer to as Structack. Structack can successfully manipulate the performance
of GNNs with very limited information while operating under tight computational
constraints. Our work contributes towards building more robust machine learning
approaches on graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_H/0/1/0/all/0/1"&gt;Hussain Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duricic_T/0/1/0/all/0/1"&gt;Tomislav Duricic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1"&gt;Elisabeth Lex&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helic_D/0/1/0/all/0/1"&gt;Denis Helic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1"&gt;Markus Strohmaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kern_R/0/1/0/all/0/1"&gt;Roman Kern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A fast and simple modification of Newton's method helping to avoid saddle points. (arXiv:2006.01512v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01512</id>
        <link href="http://arxiv.org/abs/2006.01512"/>
        <updated>2021-07-26T02:00:59.788Z</updated>
        <summary type="html"><![CDATA[We propose in this paper New Q-Newton's method. The update rule for the
simplest version is $x_{n+1}=x_n-w_n$ where
$w_n=pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n=\nabla
^2f(x_n)+\delta_n||\nabla f(x_n)||^2.Id$ and $v_n=A_n^{-1}.\nabla f(x_n)$. Here
$\delta_n$ is an appropriate real number so that $A_n$ is invertible, and
$pr_{A_n,\pm}$ are projections to the vector subspaces generated by
eigenvectors of positive (correspondingly negative) eigenvalues of $A_n$.

The main result of this paper roughly says that if $f$ is $C^3$ and a
sequence $\{x_n\}$, constructed by the New Q-Newton's method from a random
initial point $x_0$, {\bf converges}, then the limit point is a critical point
and is not a saddle point, and the convergence rate is the same as that of
Newton's method. At the end of the paper, we present some issues (saddle points
and convergence) one faces when implementing Newton's method and modifications
into Deep Neural Networks. We test the good performance of New Q-Newton's
method against algorithms such as Newton's method, BFGS, Adaptive Cubic
Regularization, Random damping Newton's method and Inertial Newton's method, as
well as Unbounded Two-way Backtracking Gradient Descent. The experiments cover
both realistic settings (such as a toy model of protein folding and stochastic
optimization) as well as various benchmark test functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+To_T/0/1/0/all/0/1"&gt;Tat Dat To&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tuan Hang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thu Hang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hoang Phuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems. (arXiv:2003.04919v5 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04919</id>
        <link href="http://arxiv.org/abs/2003.04919"/>
        <updated>2021-07-26T02:00:59.781Z</updated>
        <summary type="html"><![CDATA[There is a growing consensus that solutions to complex science and
engineering problems require novel methodologies that are able to integrate
traditional physics-based modeling approaches with state-of-the-art machine
learning (ML) techniques. This paper provides a structured overview of such
techniques. Application-centric objective areas for which these approaches have
been applied are summarized, and then classes of methodologies used to
construct physics-guided ML models and hybrid physics-ML frameworks are
described. We then provide a taxonomy of these existing techniques, which
uncovers knowledge gaps and potential crossovers of methods between disciplines
that can serve as ideas for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Willard_J/0/1/0/all/0/1"&gt;Jared Willard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shaoming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Steinbach_M/0/1/0/all/0/1"&gt;Michael Steinbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Task-Agnostic Action Spaces for Movement Optimization. (arXiv:2009.10337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10337</id>
        <link href="http://arxiv.org/abs/2009.10337"/>
        <updated>2021-07-26T02:00:59.773Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for exploring the dynamics of physically based
animated characters, and learning a task-agnostic action space that makes
movement optimization easier. Like several previous papers, we parameterize
actions as target states, and learn a short-horizon goal-conditioned low-level
control policy that drives the agent's state towards the targets. Our novel
contribution is that with our exploration data, we are able to learn the
low-level policy in a generic manner and without any reference movement data.
Trained once for each agent or simulation environment, the policy improves the
efficiency of optimizing both trajectories and high-level policies across
multiple tasks and optimization algorithms. We also contribute novel
visualizations that show how using target states as actions makes optimized
trajectories more robust to disturbances; this manifests as wider optima that
are easy to find. Due to its simplicity and generality, our proposed approach
should provide a building block that can improve a large variety of movement
optimization methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babadi_A/0/1/0/all/0/1"&gt;Amin Babadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1"&gt;Michiel van de Panne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;C. Karen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1"&gt;Perttu H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Risk Minimization in the Interpolating Regime with Application to Neural Network Learning. (arXiv:1905.10686v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10686</id>
        <link href="http://arxiv.org/abs/1905.10686"/>
        <updated>2021-07-26T02:00:59.766Z</updated>
        <summary type="html"><![CDATA[A common strategy to train deep neural networks (DNNs) is to use very large
architectures and to train them until they (almost) achieve zero training
error. Empirically observed good generalization performance on test data, even
in the presence of lots of label noise, corroborate such a procedure. On the
other hand, in statistical learning theory it is known that over-fitting models
may lead to poor generalization properties, occurring in e.g. empirical risk
minimization (ERM) over too large hypotheses classes. Inspired by this
contradictory behavior, so-called interpolation methods have recently received
much attention, leading to consistent and optimally learning methods for some
local averaging schemes with zero training error. However, there is no
theoretical analysis of interpolating ERM-like methods so far. We take a step
in this direction by showing that for certain, large hypotheses classes, some
interpolating ERMs enjoy very good statistical guarantees while others fail in
the worst sense. Moreover, we show that the same phenomenon occurs for DNNs
with zero training error and sufficiently large architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1"&gt;Nicole M&amp;#xfc;cke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1"&gt;Ingo Steinwart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking Type Safety in Go: An Empirical Study on the Usage of the unsafe Package. (arXiv:2006.09973v4 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09973</id>
        <link href="http://arxiv.org/abs/2006.09973"/>
        <updated>2021-07-26T02:00:59.740Z</updated>
        <summary type="html"><![CDATA[A decade after its first release, the Go programming language has become a
major programming language in the development landscape. While praised for its
clean syntax and C-like performance, Go also contains a strong static
type-system that prevents arbitrary type casting and arbitrary memory access,
making the language type-safe by design. However, to give developers the
possibility of implementing low-level code, Go ships with a special package
called unsafe that offers developers a way around the type-safety of Go
programs. The package gives greater flexibility to developers but comes at a
higher risk of runtime errors, chances of non-portability, and the loss of
compatibility guarantees for future versions of Go.

In this paper, we present the first large-scale study on the usage of the
unsafe package in 2,438 popular Go projects. Our investigation shows that
unsafe is used in 24% of Go projects, motivated primarily by communicating with
operating systems and C code, but is also commonly used as a source of
performance optimization. Developers are willing to use unsafe to break
language specifications (e.g., string immutability) for better performance and
6% of analyzed projects that use unsafe perform risky pointer conversions that
can lead to program crashes and unexpected behavior. Furthermore, we report a
series of real issues faced by projects that use unsafe, from crashing errors
and non-deterministic behavior to having their deployment restricted from
certain popular environments. Our findings can be used to understand how and
why developers break type-safety in Go, and help motivate further tools and
language development that could make the usage of unsafe in Go even safer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1"&gt;Diego Elias Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mujahid_S/0/1/0/all/0/1"&gt;Suhaib Mujahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdalkareem_R/0/1/0/all/0/1"&gt;Rabe Abdalkareem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1"&gt;Emad Shihab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Hard-Parameter Sharing in Multi-Task Learning. (arXiv:2107.11359v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11359</id>
        <link href="http://arxiv.org/abs/2107.11359"/>
        <updated>2021-07-26T02:00:59.731Z</updated>
        <summary type="html"><![CDATA[Hard parameter sharing in multi-task learning (MTL) allows tasks to share
some of model parameters, reducing storage cost and improving prediction
accuracy. The common sharing practice is to share bottom layers of a deep
neural network among tasks while using separate top layers for each task. In
this work, we revisit this common practice via an empirical study on
fine-grained image classification tasks and make two surprising observations.
(1) Using separate bottom-layer parameters could achieve significantly better
performance than the common practice and this phenomenon holds for different
number of tasks jointly trained on different backbone architectures with
different quantity of task-specific parameters. (2) A multi-task model with a
small proportion of task-specific parameters from bottom layers can achieve
competitive performance with independent models trained on each task separately
and outperform a state-of-the-art MTL framework. Our observations suggest that
people rethink the current sharing paradigm and adopt the new strategy of using
separate bottom-layer parameters as a stronger baseline for model design in
MTL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qizheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1"&gt;Hui Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06069</id>
        <link href="http://arxiv.org/abs/2101.06069"/>
        <updated>2021-07-26T02:00:59.723Z</updated>
        <summary type="html"><![CDATA[Pretrained deep models hold their learnt knowledge in the form of model
parameters. These parameters act as "memory" for the trained models and help
them generalize well on unseen data. However, in absence of training data, the
utility of a trained model is merely limited to either inference or better
initialization towards a target task. In this paper, we go further and extract
synthetic data by leveraging the learnt model parameters. We dub them "Data
Impressions", which act as proxy to the training data and can be used to
realize a variety of tasks. These are useful in scenarios where only the
pretrained models are available and the training data is not shared (e.g., due
to privacy or sensitivity concerns). We show the applicability of data
impressions in solving several computer vision tasks such as unsupervised
domain adaptation, continual learning as well as knowledge distillation. We
also study the adversarial robustness of lightweight models trained via
knowledge distillation using these data impressions. Further, we demonstrate
the efficacy of data impressions in generating data-free Universal Adversarial
Perturbations (UAPs) with better fooling rates. Extensive experiments performed
on benchmark datasets demonstrate competitive performance achieved using data
impressions in absence of original training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1"&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1"&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saksham Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Anirban Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum-Distortion Embedding. (arXiv:2103.02559v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02559</id>
        <link href="http://arxiv.org/abs/2103.02559"/>
        <updated>2021-07-26T02:00:59.716Z</updated>
        <summary type="html"><![CDATA[We consider the vector embedding problem. We are given a finite set of items,
with the goal of assigning a representative vector to each one, possibly under
some constraints (such as the collection of vectors being standardized, i.e.,
have zero mean and unit covariance). We are given data indicating that some
pairs of items are similar, and optionally, some other pairs are dissimilar.
For pairs of similar items, we want the corresponding vectors to be near each
other, and for dissimilar pairs, we want the corresponding vectors to not be
near each other, measured in Euclidean distance. We formalize this by
introducing distortion functions, defined for some pairs of the items. Our goal
is to choose an embedding that minimizes the total distortion, subject to the
constraints. We call this the minimum-distortion embedding (MDE) problem.

The MDE framework is simple but general. It includes a wide variety of
embedding methods, such as spectral embedding, principal component analysis,
multidimensional scaling, dimensionality reduction methods (like Isomap and
UMAP), force-directed layout, and others. It also includes new embeddings, and
provides principled ways of validating historical and new embeddings alike.

We develop a projected quasi-Newton method that approximately solves MDE
problems and scales to large data sets. We implement this method in PyMDE, an
open-source Python package. In PyMDE, users can select from a library of
distortion functions and constraints or specify custom ones, making it easy to
rapidly experiment with different embeddings. Our software scales to data sets
with millions of items and tens of millions of distortion functions. To
demonstrate our method, we compute embeddings for several real-world data sets,
including images, an academic co-author network, US county demographic data,
and single-cell mRNA transcriptomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Akshay Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Alnur Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1"&gt;Stephen Boyd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate. (arXiv:2005.09525v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09525</id>
        <link href="http://arxiv.org/abs/2005.09525"/>
        <updated>2021-07-26T02:00:59.707Z</updated>
        <summary type="html"><![CDATA[In this work we present a multi-modal machine learning-based system, which we
call ACORN, to analyze videos of school classrooms for the Positive Climate
(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol
that is widely used in educational research. ACORN uses convolutional neural
networks to analyze spectral audio features, the faces of teachers and
students, and the pixels of each image frame, and then integrates this
information over time using Temporal Convolutional Networks. The audiovisual
ACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$
with ground-truth scores provided by expert CLASS coders on the UVA Toddler
dataset (cross-validation on $n=300$ 15-min video segments), and a purely
auditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the
MET dataset (test set of $n=2000$ videos segments). These numbers are similar
to inter-coder reliability of human coders. Finally, using Graph Convolutional
Networks we make early strides (AUC=$0.70$) toward predicting the specific
moments (45-90sec clips) when the PC is particularly weak/strong. Our findings
inform the design of automatic classroom observation and also more general
video activity recognition and summary recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1"&gt;Anand Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zylich_B/0/1/0/all/0/1"&gt;Brian Zylich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ottmar_E/0/1/0/all/0/1"&gt;Erin Ottmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1"&gt;Jennifer LoCasale-Crouch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1"&gt;Jacob Whitehill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNetGAN: Data augmentation of brain connectivity using generative adversarial network for dementia classification. (arXiv:2103.08494v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08494</id>
        <link href="http://arxiv.org/abs/2103.08494"/>
        <updated>2021-07-26T02:00:59.681Z</updated>
        <summary type="html"><![CDATA[Alzheimer's disease (AD) is the most common age-related dementia. It remains
a challenge to identify the individuals at risk of dementia for precise
management. Brain MRI offers a noninvasive biomarker to detect brain aging.
Previous evidence shows that the brain structural change detected by diffusion
MRI is associated with dementia. Mounting studies has conceptualised the brain
as a complex network, which has shown the utility of this approach in
characterising various neurological and psychiatric disorders. Therefore, the
structural connectivity shows promise in dementia classification. The proposed
BrainNetGAN is a generative adversarial network variant to augment the brain
structural connectivity matrices for binary dementia classification tasks.
Structural connectivity matrices between separated brain regions are
constructed using tractography on diffusion MRI data. The BrainNetGAN model is
trained to generate fake brain connectivity matrices, which are expected to
reflect latent distribution of the real brain network data. Finally, a
convolutional neural network classifier is proposed for binary dementia
classification. Numerical results show that the binary classification
performance in the testing set was improved using the BrainNetGAN augmented
dataset. The proposed methodology allows quick synthesis of an arbitrary number
of augmented connectivity matrices and can be easily transferred to similar
classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yiran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Schonlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Differentiable Language Model Adversarial Attack on Text Classifiers. (arXiv:2107.11275v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11275</id>
        <link href="http://arxiv.org/abs/2107.11275"/>
        <updated>2021-07-26T02:00:59.673Z</updated>
        <summary type="html"><![CDATA[Robustness of huge Transformer-based models for natural language processing
is an important issue due to their capabilities and wide adoption. One way to
understand and improve robustness of these models is an exploration of an
adversarial attack scenario: check if a small perturbation of an input can fool
a model.

Due to the discrete nature of textual data, gradient-based adversarial
methods, widely used in computer vision, are not applicable per~se. The
standard strategy to overcome this issue is to develop token-level
transformations, which do not take the whole sentence into account.

In this paper, we propose a new black-box sentence-level attack. Our method
fine-tunes a pre-trained language model to generate adversarial examples. A
proposed differentiable loss function depends on a substitute classifier score
and an approximate edit distance computed via a deep learning model.

We show that the proposed attack outperforms competitors on a diverse set of
NLP problems for both computed metrics and human evaluation. Moreover, due to
the usage of the fine-tuned language model, the generated adversarial examples
are hard to detect, thus current models are not robust. Hence, it is difficult
to defend from the proposed attack, which is not the case for other attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1"&gt;Ivan Fursov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1"&gt;Pavel Burnyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dmitrieva_E/0/1/0/all/0/1"&gt;Ekaterina Dmitrieva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1"&gt;Nikita Klyuchnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1"&gt;Andrey Kravchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1"&gt;Ekaterina Artemova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rigging the Lottery: Making All Tickets Winners. (arXiv:1911.11134v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.11134</id>
        <link href="http://arxiv.org/abs/1911.11134"/>
        <updated>2021-07-26T02:00:59.666Z</updated>
        <summary type="html"><![CDATA[Many applications require sparse neural networks due to space or inference
time restrictions. There is a large body of work on training dense networks to
yield sparse networks for inference, but this limits the size of the largest
trainable sparse model to that of the largest trainable dense model. In this
paper we introduce a method to train sparse neural networks with a fixed
parameter count and a fixed computational cost throughout training, without
sacrificing accuracy relative to existing dense-to-sparse training methods. Our
method updates the topology of the sparse network during training by using
parameter magnitudes and infrequent gradient calculations. We show that this
approach requires fewer floating-point operations (FLOPs) to achieve a given
level of accuracy compared to prior techniques. We demonstrate state-of-the-art
sparse training results on a variety of networks and datasets, including
ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we
provide some insights into why allowing the topology to change during the
optimization can overcome local minima encountered when the topology remains
static. Code used in our work can be found in github.com/google-research/rigl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1"&gt;Utku Evci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1"&gt;Trevor Gale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1"&gt;Jacob Menick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1"&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1"&gt;Erich Elsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11191</id>
        <link href="http://arxiv.org/abs/2107.11191"/>
        <updated>2021-07-26T02:00:59.658Z</updated>
        <summary type="html"><![CDATA[Deep neural network approaches to inverse imaging problems have produced
impressive results in the last few years. In this paper, we consider the use of
generative models in a variational regularisation approach to inverse problems.
The considered regularisers penalise images that are far from the range of a
generative model that has learned to produce images similar to a training
dataset. We name this family \textit{generative regularisers}. The success of
generative regularisers depends on the quality of the generative model and so
we propose a set of desired criteria to assess models and guide future
research. In our numerical experiments, we evaluate three common generative
models, autoencoders, variational autoencoders and generative adversarial
networks, against our desired criteria. We also test three different generative
regularisers on the inverse problems of deblurring, deconvolution, and
tomography. We show that the success of solutions restricted to lie exactly in
the range of the generator is highly dependent on the ability of the generative
model but that allowing small deviations from the range of the generator
produces more consistent results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1"&gt;Margaret Duff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1"&gt;Neill D. F. Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1"&gt;Matthias J. Ehrhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Adaptive Submodular Maximization. (arXiv:2107.11333v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.11333</id>
        <link href="http://arxiv.org/abs/2107.11333"/>
        <updated>2021-07-26T02:00:59.650Z</updated>
        <summary type="html"><![CDATA[Most of existing studies on adaptive submodular optimization focus on the
average-case, i.e., their objective is to find a policy that maximizes the
expected utility over a known distribution of realizations. However, a policy
that has a good average-case performance may have very poor performance under
the worst-case realization. In this study, we propose to study two variants of
adaptive submodular optimization problems, namely, worst-case adaptive
submodular maximization and robust submodular maximization. The first problem
aims to find a policy that maximizes the worst-case utility and the latter one
aims to find a policy, if any, that achieves both near optimal average-case
utility and worst-case utility simultaneously. We introduce a new class of
stochastic functions, called \emph{worst-case submodular function}. For the
worst-case adaptive submodular maximization problem subject to a $p$-system
constraint, we develop an adaptive worst-case greedy policy that achieves a
$\frac{1}{p+1}$ approximation ratio against the optimal worst-case utility if
the utility function is worst-case submodular. For the robust adaptive
submodular maximization problem subject to a cardinality constraint, if the
utility function is both worst-case submodular and adaptive submodular, we
develop a hybrid adaptive policy that achieves an approximation close to
$1-e^{-\frac{1}{2}}$ under both worst case setting and average case setting
simultaneously. We also describe several applications of our theoretical
results, including pool-base active learning, stochastic submodular set cover
and adaptive viral marketing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shaojie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogDL: Toolkit for Deep Learning on Graphs. (arXiv:2103.00959v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00959</id>
        <link href="http://arxiv.org/abs/2103.00959"/>
        <updated>2021-07-26T02:00:59.613Z</updated>
        <summary type="html"><![CDATA[Deep learning on graphs has attracted tremendous attention from the graph
learning community in recent years. It has been widely used in several
real-world applications such as social network analysis and recommender
systems. In this paper, we introduce CogDL, an extensive toolkit for deep
learning on graphs that allows researchers and developers to easily conduct
experiments and build applications. It provides standard training and
evaluation for the most important tasks in the graph domain, including node
classification, graph classification, etc. For each task, it provides
implementations of state-of-the-art models. The models in our toolkit are
divided into two major parts, graph embedding methods and graph neural
networks. Most of the graph embedding methods learn node-level or graph-level
representations in an unsupervised way and preserves the graph properties such
as structural information, while graph neural networks capture node features
and work in semi-supervised or self-supervised settings. All models implemented
in our toolkit can be easily reproducible for leaderboard results. Most models
in CogDL are developed on top of PyTorch, and users can leverage the advantages
of PyTorch to implement their own models. Furthermore, we demonstrate the
effectiveness of CogDL for real-world applications in AMiner, a large academic
mining system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cen_Y/0/1/0/all/0/1"&gt;Yukuo Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zhenyu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qibin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yizhen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xingcheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Aohan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shiguang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1"&gt;Guohao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Deep Registration Latent Spaces. (arXiv:2107.11238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11238</id>
        <link href="http://arxiv.org/abs/2107.11238"/>
        <updated>2021-07-26T02:00:59.603Z</updated>
        <summary type="html"><![CDATA[Explainability of deep neural networks is one of the most challenging and
interesting problems in the field. In this study, we investigate the topic
focusing on the interpretability of deep learning-based registration methods.
In particular, with the appropriate model architecture and using a simple
linear projection, we decompose the encoding space, generating a new basis, and
we empirically show that this basis captures various decomposed anatomically
aware geometrical transformations. We perform experiments using two different
datasets focusing on lungs and hippocampus MRI. We show that such an approach
can decompose the highly convoluted latent spaces of registration pipelines in
an orthogonal space with several interesting properties. We hope that this work
could shed some light on a better understanding of deep learning-based
registration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Estienne_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Estienne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1"&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1"&gt;Stergios Christodoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battistella_E/0/1/0/all/0/1"&gt;Enzo Battistella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;ophraste Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1"&gt;Marvin Lerousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1"&gt;Amaury Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1"&gt;Guillaume Chassagnon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1"&gt;Marie-Pierre Revel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1"&gt;Nikos Paragios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1"&gt;Eric Deutsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Quantile Aggregation. (arXiv:2103.00083v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00083</id>
        <link href="http://arxiv.org/abs/2103.00083"/>
        <updated>2021-07-26T02:00:59.593Z</updated>
        <summary type="html"><![CDATA[Conditional quantile estimation is a key statistical learning challenge
motivated by the need to quantify uncertainty in predictions or to model a
diverse population without being overly reductive. As such, many models have
been developed for this problem. Adopting a meta viewpoint, we propose a
general framework (inspired by neural network optimization) for aggregating any
number of conditional quantile models in order to boost predictive accuracy. We
consider weighted ensembling strategies of increasing flexibility where the
weights may vary over individual models, quantile levels, and feature values.
An appeal of our approach is its portability: we ensure that estimated
quantiles at adjacent levels do not cross by applying simple transformations
through which gradients can be backpropagated, and this allows us to leverage
the modern deep learning toolkit for building quantile ensembles. Our
experiments confirm that ensembling can lead to big gains in accuracy, even
when the constituent models are themselves powerful and flexible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taesup Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fakoor_R/0/1/0/all/0/1"&gt;Rasool Fakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mueller_J/0/1/0/all/0/1"&gt;Jonas Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1"&gt;Ryan J. Tibshirani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11277</id>
        <link href="http://arxiv.org/abs/2107.11277"/>
        <updated>2021-07-26T02:00:59.585Z</updated>
        <summary type="html"><![CDATA[Machine learning models always make a prediction, even when it is likely to
be inaccurate. This behavior should be avoided in many decision support
applications, where mistakes can have severe consequences. Albeit already
studied in 1970, machine learning with a reject option recently gained
interest. This machine learning subfield enables machine learning models to
abstain from making a prediction when likely to make a mistake.

This survey aims to provide an overview on machine learning with a reject
option. We introduce the conditions leading to two types of rejection,
ambiguity and novelty rejection. Moreover, we define the existing architectures
for models with a reject option, describe the standard learning strategies to
train such models and relate traditional machine learning techniques to
rejection. Additionally, we review strategies to evaluate a model's predictive
and rejective quality. Finally, we provide examples of relevant application
domains and show how machine learning with rejection relates to other machine
learning research areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrickx_K/0/1/0/all/0/1"&gt;Kilian Hendrickx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1"&gt;Lorenzo Perini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plas_D/0/1/0/all/0/1"&gt;Dries Van der Plas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1"&gt;Wannes Meert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jesse Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Bit Quantization For Distributed Algorithms With Linear Convergence. (arXiv:2107.11304v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.11304</id>
        <link href="http://arxiv.org/abs/2107.11304"/>
        <updated>2021-07-26T02:00:59.577Z</updated>
        <summary type="html"><![CDATA[This paper studies distributed algorithms for (strongly convex) composite
optimization problems over mesh networks, subject to quantized communications.
Instead of focusing on a specific algorithmic design, we propose a black-box
model casting distributed algorithms in the form of fixed-point iterates,
converging at linear rate. The algorithmic model is coupled with a novel
(random) Biased Compression (BC-)rule on the quantizer design, which preserves
linear convergence. A new quantizer coupled with a communication-efficient
encoding scheme is also proposed, which efficiently implements the BC-rule
using a finite number of bits. This contrasts with most of existing
quantization rules, whose implementation calls for an infinite number of bits.
A unified communication complexity analysis is developed for the black-box
model, determining the average number of bit required to reach a solution of
the optimization problem within the required accuracy. Numerical results
validate our theoretical findings and show that distributed algorithms equipped
with the proposed quantizer have more favorable communication complexity than
algorithms using existing quantization rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chang-Shen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Michelusi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Michelusi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Scutari_G/0/1/0/all/0/1"&gt;Gesualdo Scutari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelet Design in a Learning Framework. (arXiv:2107.11225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11225</id>
        <link href="http://arxiv.org/abs/2107.11225"/>
        <updated>2021-07-26T02:00:59.550Z</updated>
        <summary type="html"><![CDATA[Wavelets have proven to be highly successful in several signal and image
processing applications. Wavelet design has been an active field of research
for over two decades, with the problem often being approached from an
analytical perspective. In this paper, we introduce a learning based approach
to wavelet design. We draw a parallel between convolutional autoencoders and
wavelet multiresolution approximation, and show how the learning angle provides
a coherent computational framework for addressing the design problem. We aim at
designing data-independent wavelets by training filterbank autoencoders, which
precludes the need for customized datasets. In fact, we use high-dimensional
Gaussian vectors for training filterbank autoencoders, and show that a
near-zero training loss implies that the learnt filters satisfy the perfect
reconstruction property with very high probability. Properties of a wavelet
such as orthogonality, compact support, smoothness, symmetry, and vanishing
moments can be incorporated by designing the autoencoder architecture
appropriately and with a suitable regularization term added to the mean-squared
error cost used in the learning process. Our approach not only recovers the
well known Daubechies family of orthogonal wavelets and the
Cohen-Daubechies-Feauveau family of symmetric biorthogonal wavelets, but also
learns wavelets outside these families.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jawali_D/0/1/0/all/0/1"&gt;Dhruv Jawali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1"&gt;Chandra Sekhar Seelamantula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11214</id>
        <link href="http://arxiv.org/abs/2107.11214"/>
        <updated>2021-07-26T02:00:59.543Z</updated>
        <summary type="html"><![CDATA[We propose the adjacency adaptive graph convolutional long-short term memory
network (AAGC-LSTM) for human pose estimation from sparse inertial
measurements, obtained from only 6 measurement units. The AAGC-LSTM combines
both spatial and temporal dependency in a single network operation. This is
made possible by equipping graph convolutions with adjacency adaptivity, which
also allows for learning unknown dependencies of the human body joints. To
further boost accuracy, we propose longitudinal loss weighting to consider
natural movement patterns, as well as body-aware contralateral data
augmentation. By combining these contributions, we are able to utilize the
inherent graph nature of the human body, and can thus outperform the state of
the art for human pose estimation from sparse inertial measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1"&gt;Patrik Puchert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1"&gt;Timo Ropinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits and Tradeoffs in Invariant Representation Learning. (arXiv:2012.10713v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10713</id>
        <link href="http://arxiv.org/abs/2012.10713"/>
        <updated>2021-07-26T02:00:59.536Z</updated>
        <summary type="html"><![CDATA[Many machine learning applications, e.g., privacy-preserving learning,
algorithmic fairness and domain adaptation/generalization, involve learning the
so-called invariant representations that achieve two competing goals: To
maximize information or accuracy with respect to a target while simultaneously
maximizing invariance or independence with respect to a set of protected
features (e.g.\ for fairness, privacy, etc). Despite its abundant applications
in the aforementioned domains, theoretical understanding on the limits and
tradeoffs of invariant representations is still severely lacking. In this
paper, we provide an information theoretic analysis of this general and
important problem under both classification and regression settings. In both
cases, we analyze the inherent tradeoffs between accuracy and invariance by
providing a geometric characterization of the feasible region in the
information plane, where we connect the geometric properties of this feasible
region to the fundamental limitations of the tradeoff problem. In the
regression setting, we further give a complete and exact characterization of
the frontier between accuracy and invariance. Although our contributions are
mainly theoretical, we also demonstrate the practical applications of our
results in certifying the suboptimality of certain representation learning
algorithms in both classification and regression tasks. Our results shed new
light on this fundamental problem by providing insights on the interplay
between accuracy and invariance. These results deepen our understanding of this
fundamental problem and may be useful in guiding the design of future
representation learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1"&gt;Chen Dan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1"&gt;Bryon Aragam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi S. Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1"&gt;Geoffrey J. Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LARGE: Latent-Based Regression through GAN Semantics. (arXiv:2107.11186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11186</id>
        <link href="http://arxiv.org/abs/2107.11186"/>
        <updated>2021-07-26T02:00:59.528Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for solving regression tasks using few-shot or weak
supervision. At the core of our method is the fundamental observation that GANs
are incredibly successful at encoding semantic information within their latent
space, even in a completely unsupervised setting. For modern generative
frameworks, this semantic encoding manifests as smooth, linear directions which
affect image attributes in a disentangled manner. These directions have been
widely used in GAN-based image editing. We show that such directions are not
only linear, but that the magnitude of change induced on the respective
attribute is approximately linear with respect to the distance traveled along
them. By leveraging this observation, our method turns a pre-trained GAN into a
regression model, using as few as two labeled samples. This enables solving
regression tasks on datasets and attributes which are difficult to produce
quality supervision for. Additionally, we show that the same latent-distances
can be used to sort collections of images by the strength of given attributes,
even in the absence of explicit supervision. Extensive experimental evaluations
demonstrate that our method can be applied across a wide range of domains,
leverage multiple latent direction discovery frameworks, and achieve
state-of-the-art results in few-shot and low-supervision settings, even when
compared to methods designed to tackle a single task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1"&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1"&gt;Rinon Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_O/0/1/0/all/0/1"&gt;Ofir Brenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Shapley values: a measure of joint feature importance. (arXiv:2107.11357v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.11357</id>
        <link href="http://arxiv.org/abs/2107.11357"/>
        <updated>2021-07-26T02:00:59.521Z</updated>
        <summary type="html"><![CDATA[The Shapley value is one of the most widely used model-agnostic measures of
feature importance in explainable AI: it has clear axiomatic foundations, is
guaranteed to uniquely exist, and has a clear interpretation as a feature's
average effect on a model's prediction. We introduce joint Shapley values,
which directly extend the Shapley axioms. This preserves the classic Shapley
value's intuitions: joint Shapley values measure a set of features' average
effect on a model's prediction. We prove the uniqueness of joint Shapley
values, for any order of explanation. Results for games show that joint Shapley
values present different insights from existing interaction indices, which
assess the effect of a feature within a set of features. Deriving joint Shapley
values in ML attribution problems thus gives us the first measure of the joint
effect of sets of features on model predictions. In a dataset with binary
features, we present a presence-adjusted method for calculating global values
that retains the efficiency property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harris_C/0/1/0/all/0/1"&gt;Chris Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pymar_R/0/1/0/all/0/1"&gt;Richard Pymar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rowat_C/0/1/0/all/0/1"&gt;Colin Rowat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Channel Automatic Music Transcription Using Tensor Algebra. (arXiv:2107.11250v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.11250</id>
        <link href="http://arxiv.org/abs/2107.11250"/>
        <updated>2021-07-26T02:00:59.512Z</updated>
        <summary type="html"><![CDATA[Music is an art, perceived in unique ways by every listener, coming from
acoustic signals. In the meantime, standards as musical scores exist to
describe it. Even if humans can make this transcription, it is costly in terms
of time and efforts, even more with the explosion of information consecutively
to the rise of the Internet. In that sense, researches are driven in the
direction of Automatic Music Transcription. While this task is considered
solved in the case of single notes, it is still open when notes superpose
themselves, forming chords. This report aims at developing some of the existing
techniques towards Music Transcription, particularly matrix factorization, and
introducing the concept of multi-channel automatic music transcription. This
concept will be explored with mathematical objects called tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Axel_M/0/1/0/all/0/1"&gt;Marmoret Axel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nancy_B/0/1/0/all/0/1"&gt;Bertin Nancy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeremy_C/0/1/0/all/0/1"&gt;Cohen Jeremy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective and Interpretable fMRI Analysis via Functional Brain Network Generation. (arXiv:2107.11247v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11247</id>
        <link href="http://arxiv.org/abs/2107.11247"/>
        <updated>2021-07-26T02:00:59.477Z</updated>
        <summary type="html"><![CDATA[Recent studies in neuroscience show great potential of functional brain
networks constructed from fMRI data for popularity modeling and clinical
predictions. However, existing functional brain networks are noisy and unaware
of downstream prediction tasks, while also incompatible with recent powerful
machine learning models of GNNs. In this work, we develop an end-to-end
trainable pipeline to extract prominent fMRI features, generate brain networks,
and make predictions with GNNs, all under the guidance of downstream prediction
tasks. Preliminary experiments on the PNC fMRI data show the superior
effectiveness and unique interpretability of our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1"&gt;Xuan Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Ying Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taxonomizing local versus global structure in neural network loss landscapes. (arXiv:2107.11228v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11228</id>
        <link href="http://arxiv.org/abs/2107.11228"/>
        <updated>2021-07-26T02:00:59.469Z</updated>
        <summary type="html"><![CDATA[Viewing neural network models in terms of their loss landscapes has a long
history in the statistical mechanics approach to learning, and in recent years
it has received attention within machine learning proper. Among other things,
local metrics (such as the smoothness of the loss landscape) have been shown to
correlate with global properties of the model (such as good generalization).
Here, we perform a detailed empirical analysis of the loss landscape structure
of thousands of neural network models, systematically varying learning tasks,
model architectures, and/or quantity/quality of data. By considering a range of
metrics that attempt to capture different aspects of the loss landscape, we
demonstrate that the best test accuracy is obtained when: the loss landscape is
globally well-connected; ensembles of trained models are more similar to each
other; and models converge to locally smooth regions. We also show that
globally poorly-connected landscapes can arise when models are small or when
they are trained to lower quality data; and that, if the loss landscape is
globally poorly-connected, then training to zero loss can actually lead to
worse test accuracy. Based on these results, we develop a simple
one-dimensional model with load-like and temperature-like parameters, we
introduce the notion of an \emph{effective loss landscape} depending on these
parameters, and we interpret our results in terms of a \emph{rugged convexity}
of the loss landscape. When viewed through this lens, our detailed empirical
results shed light on phases of learning (and consequent double descent
behavior), fundamental versus incidental determinants of good generalization,
the role of load-like and temperature-like parameters in the learning process,
different influences on the loss landscape from model and data, and the
relationships between local and global metrics, all topics of recent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaoqing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1"&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theisen_R/0/1/0/all/0/1"&gt;Ryan Theisen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Joe Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11291</id>
        <link href="http://arxiv.org/abs/2107.11291"/>
        <updated>2021-07-26T02:00:59.462Z</updated>
        <summary type="html"><![CDATA[Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
https://github.com/Jeff-sjtu/res-loglikelihood-regression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1"&gt;Siyuan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OLR 2021 Challenge: Datasets, Rules and Baselines. (arXiv:2107.11113v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11113</id>
        <link href="http://arxiv.org/abs/2107.11113"/>
        <updated>2021-07-26T02:00:59.454Z</updated>
        <summary type="html"><![CDATA[This paper introduces the sixth Oriental Language Recognition (OLR) 2021
Challenge, which intends to improve the performance of language recognition
systems and speech recognition systems within multilingual scenarios. The data
profile, four tasks, two baselines, and the evaluation principles are
introduced in this paper. In addition to the Language Identification (LID)
tasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to
OLR 2021 Challenge for the first time. The challenge this year focuses on more
practical and challenging problems, with four tasks: (1) constrained LID, (2)
unconstrained LID, (3) constrained multilingual ASR, (4) unconstrained
multilingual ASR. Baselines for LID tasks and multilingual ASR tasks are
provided, respectively. The LID baseline system is an extended TDNN x-vector
model constructed with Pytorch. A transformer-based end-to-end model is
provided as the multilingual ASR baseline system. These recipes will be online
published, and available for participants to construct their own LID or ASR
systems. The baseline results demonstrate that those tasks are rather
challenging and deserve more effort to achieve better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Binling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenxuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1"&gt;Yiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qingyang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Liming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Peeking inside the Black Box: Interpreting Deep Learning Models for Exoplanet Atmospheric Retrievals. (arXiv:2011.11284v2 [astro-ph.EP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11284</id>
        <link href="http://arxiv.org/abs/2011.11284"/>
        <updated>2021-07-26T02:00:59.447Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms are growing in popularity in the field of
exoplanetary science due to their ability to model highly non-linear relations
and solve interesting problems in a data-driven manner. Several works have
attempted to perform fast retrievals of atmospheric parameters with the use of
machine learning algorithms like deep neural networks (DNNs). Yet, despite
their high predictive power, DNNs are also infamous for being 'black boxes'. It
is their apparent lack of explainability that makes the astrophysics community
reluctant to adopt them. What are their predictions based on? How confident
should we be in them? When are they wrong and how wrong can they be? In this
work, we present a number of general evaluation methodologies that can be
applied to any trained model and answer questions like these. In particular, we
train three different popular DNN architectures to retrieve atmospheric
parameters from exoplanet spectra and show that all three achieve good
predictive performance. We then present an extensive analysis of the
predictions of DNNs, which can inform us - among other things - of the
credibility limits for atmospheric parameters for a given instrument and model.
Finally, we perform a perturbation-based sensitivity analysis to identify to
which features of the spectrum the outcome of the retrieval is most sensitive.
We conclude that for different molecules, the wavelength ranges to which the
DNN's predictions are most sensitive, indeed coincide with their characteristic
absorption regions. The methodologies presented in this work help to improve
the evaluation of DNNs and to grant interpretability to their predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Yip_K/0/1/0/all/0/1"&gt;Kai Hou Yip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Changeat_Q/0/1/0/all/0/1"&gt;Quentin Changeat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Nikolaou_N/0/1/0/all/0/1"&gt;Nikolaos Nikolaou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Morvan_M/0/1/0/all/0/1"&gt;Mario Morvan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Edwards_B/0/1/0/all/0/1"&gt;Billy Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Waldmann_I/0/1/0/all/0/1"&gt;Ingo P. Waldmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Tinetti_G/0/1/0/all/0/1"&gt;Giovanna Tinetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparison of combined data assimilation and machine learning methods for offline and online model error correction. (arXiv:2107.11114v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.11114</id>
        <link href="http://arxiv.org/abs/2107.11114"/>
        <updated>2021-07-26T02:00:59.420Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that it is possible to combine machine learning
methods with data assimilation to reconstruct a dynamical system using only
sparse and noisy observations of that system. The same approach can be used to
correct the error of a knowledge-based model. The resulting surrogate model is
hybrid, with a statistical part supplementing a physical part. In practice, the
correction can be added as an integrated term (i.e. in the model resolvent) or
directly inside the tendencies of the physical model. The resolvent correction
is easy to implement. The tendency correction is more technical, in particular
it requires the adjoint of the physical model, but also more flexible. We use
the two-scale Lorenz model to compare the two methods. The accuracy in
long-range forecast experiments is somewhat similar between the surrogate
models using the resolvent correction and the tendency correction. By contrast,
the surrogate models using the tendency correction significantly outperform the
surrogate models using the resolvent correction in data assimilation
experiments. Finally, we show that the tendency correction opens the
possibility to make online model error correction, i.e. improving the model
progressively as new observations become available. The resulting algorithm can
be seen as a new formulation of weak-constraint 4D-Var. We compare online and
offline learning using the same framework with the two-scale Lorenz system, and
show that with online learning, it is possible to extract all the information
from sparse and noisy observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1"&gt;Alban Farchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1"&gt;Marc Bocquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laloyaux_P/0/1/0/all/0/1"&gt;Patrick Laloyaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bonavita_M/0/1/0/all/0/1"&gt;Massimo Bonavita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1"&gt;Quentin Malartic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13000</id>
        <link href="http://arxiv.org/abs/2011.13000"/>
        <updated>2021-07-26T02:00:59.410Z</updated>
        <summary type="html"><![CDATA[Precision scaling has emerged as a popular technique to optimize the compute
and storage requirements of Deep Neural Networks (DNNs). Efforts toward
creating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum
precision required to achieve a given network-level accuracy varies
considerably across networks, and even across layers within a network,
requiring support for variable precision in DNN hardware. Previous proposals
such as bit-serial hardware incur high overheads, significantly diminishing the
benefits of lower precision. To efficiently support precision
re-configurability in DNN accelerators, we introduce an approximate computing
method wherein DNN computations are performed block-wise (a block is a group of
bits) and re-configurability is supported at the granularity of blocks. Results
of block-wise computations are composed in an approximate manner to enable
efficient re-configurability. We design a DNN accelerator that embodies
approximate blocked computation and propose a method to determine a suitable
approximation configuration for a given DNN. By varying the approximation
configurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement
in system energy and performance respectively, over an 8-bit fixed-point (FxP8)
baseline, with negligible loss in classification accuracy. Further, by varying
the approximation configurations across layers and data-structures within DNNs,
we achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and
performance respectively, with negligible accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1"&gt;Reena Elangovan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Shubham Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1"&gt;Anand Raghunathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Prediction for Deep Sequential Regression Using Meta Models. (arXiv:2007.01350v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01350</id>
        <link href="http://arxiv.org/abs/2007.01350"/>
        <updated>2021-07-26T02:00:59.398Z</updated>
        <summary type="html"><![CDATA[Generating high quality uncertainty estimates for sequential regression,
particularly deep recurrent networks, remains a challenging and open problem.
Existing approaches often make restrictive assumptions (such as stationarity)
yet still perform poorly in practice, particularly in presence of real world
non-stationary signals and drift. This paper describes a flexible method that
can generate symmetric and asymmetric uncertainty estimates, makes no
assumptions about stationarity, and outperforms competitive baselines on both
drift and non drift scenarios. This work helps make sequential regression more
effective and practical for use in real-world applications, and is a powerful
new addition to the modeling toolbox for sequential uncertainty quantification
in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1"&gt;Jiri Navratil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1"&gt;Matthew Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elder_B/0/1/0/all/0/1"&gt;Benjamin Elder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series. (arXiv:2107.11350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11350</id>
        <link href="http://arxiv.org/abs/2107.11350"/>
        <updated>2021-07-26T02:00:59.389Z</updated>
        <summary type="html"><![CDATA[Irregularly sampled time series commonly occur in several domains where they
present a significant challenge to standard deep learning models. In this
paper, we propose a new deep learning framework for probabilistic interpolation
of irregularly sampled time series that we call the Heteroscedastic Temporal
Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode
information about input observation sparsity, a temporal VAE architecture to
propagate uncertainty due to input sparsity, and a heteroscedastic output layer
to enable variable uncertainty in output interpolations. Our results show that
the proposed architecture is better able to reflect variable uncertainty
through time due to sparse and irregular sampling than a range of baseline and
traditional models, as well as recently proposed deep latent variable models
that use homoscedastic output layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marlin_B/0/1/0/all/0/1"&gt;Benjamin M. Marlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisMCA: A Visual Analytics System for Misclassification Correction and Analysis. VAST Challenge 2020, Mini-Challenge 2 Award: Honorable Mention for Detailed Analysis of Patterns of Misclassification. (arXiv:2107.11181v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.11181</id>
        <link href="http://arxiv.org/abs/2107.11181"/>
        <updated>2021-07-26T02:00:59.381Z</updated>
        <summary type="html"><![CDATA[This paper presents VisMCA, an interactive visual analytics system that
supports deepening understanding in ML results, augmenting users' capabilities
in correcting misclassification, and providing an analysis of underlying
patterns, in response to the VAST Challenge 2020 Mini-Challenge 2. VisMCA
facilitates tracking provenance and provides a comprehensive view of object
detection results, easing re-labeling, and producing reliable, corrected data
for future training. Our solution implements multiple analytical views on
visual analysis to offer a deep insight for underlying pattern discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Huyen N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Jake Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jian Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan V.T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1"&gt;Tommy Dang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching a neural network with non-tunable exciton-polariton nodes. (arXiv:2107.11156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11156</id>
        <link href="http://arxiv.org/abs/2107.11156"/>
        <updated>2021-07-26T02:00:59.362Z</updated>
        <summary type="html"><![CDATA[In contrast to software simulations of neural networks, hardware or
neuromorphic implementations have often limited or no tunability. While such
networks promise great improvements in terms of speed and energy efficiency,
their performance is limited by the difficulty to apply efficient teaching. We
propose a system of non-tunable exciton-polariton nodes and an efficient
teaching method that relies on the precise measurement of the nonlinear node
response and the subsequent use of the backpropagation algorithm. We
demonstrate experimentally that the classification accuracy in the MNIST
handwritten digit benchmark is greatly improved compared to the case where
backpropagation is not used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Opala_A/0/1/0/all/0/1"&gt;Andrzej Opala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panico_R/0/1/0/all/0/1"&gt;Riccardo Panico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardizzone_V/0/1/0/all/0/1"&gt;Vincenzo Ardizzone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietka_B/0/1/0/all/0/1"&gt;Barbara Pietka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szczytko_J/0/1/0/all/0/1"&gt;Jacek Szczytko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanvitto_D/0/1/0/all/0/1"&gt;Daniele Sanvitto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matuszewski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Matuszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballarini_D/0/1/0/all/0/1"&gt;Dario Ballarini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11170</id>
        <link href="http://arxiv.org/abs/2107.11170"/>
        <updated>2021-07-26T02:00:59.295Z</updated>
        <summary type="html"><![CDATA[Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1"&gt;Lusine Abrahamyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1"&gt;Valentin Ziatchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1"&gt;Nikos Deligiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic detection of mobile malware using smartphone data and machine learning. (arXiv:2107.11167v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.11167</id>
        <link href="http://arxiv.org/abs/2107.11167"/>
        <updated>2021-07-26T02:00:59.288Z</updated>
        <summary type="html"><![CDATA[Mobile malware are malicious programs that target mobile devices. They are an
increasing problem, as seen in the rise of detected mobile malware samples per
year. The number of active smartphone users is expected to grow, stressing the
importance of research on the detection of mobile malware. Detection methods
for mobile malware exist but are still limited.

In this paper, we provide an overview of the performance of machine learning
(ML) techniques to detect malware on Android, without using privileged access.
The ML-classifiers use device information such as the CPU usage, battery usage,
and memory usage for the detection of 10 subtypes of Mobile Trojans on the
Android Operating System (OS).

We use a real-life dataset containing device and malware data from 47 users
for a year (2016). We examine which features, i.e. aspects, of a device, are
most important to monitor to detect (subtypes of) Mobile Trojans. The focus of
this paper is on dynamic hardware features. Using these dynamic features we
apply state-of-the-art machine learning classifiers: Random Forest, K-Nearest
Neighbour, and AdaBoost. We show classification results on different feature
sets, making a distinction between global device features, and specific app
features. None of the measured feature sets require privileged access.

Our results show that the Random Forest classifier performs best as a general
malware classifier: across 10 subtypes of Mobile Trojans, it achieves an F1
score of 0.73 with a False Positive Rate (FPR) of 0.009 and a False Negative
Rate (FNR) of 0.380. The Random Forest, K-Nearest Neighbours, and AdaBoost
classifiers achieve F1 scores above 0.72, an FPR below 0.02 and, an FNR below
0.33, when trained separately to detect each subtype of Mobile Trojans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wit_J/0/1/0/all/0/1"&gt;J.S. Panman de Wit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ham_J/0/1/0/all/0/1"&gt;J. van der Ham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1"&gt;D. Bucur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constellation: Learning relational abstractions over objects for compositional imagination. (arXiv:2107.11153v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11153</id>
        <link href="http://arxiv.org/abs/2107.11153"/>
        <updated>2021-07-26T02:00:59.281Z</updated>
        <summary type="html"><![CDATA[Learning structured representations of visual scenes is currently a major
bottleneck to bridging perception with reasoning. While there has been exciting
progress with slot-based models, which learn to segment scenes into sets of
objects, learning configurational properties of entire groups of objects is
still under-explored. To address this problem, we introduce Constellation, a
network that learns relational abstractions of static visual scenes, and
generalises these abstractions over sensory particularities, thus offering a
potential basis for abstract relational reasoning. We further show that this
basis, along with language association, provides a means to imagine sensory
content in new ways. This work is a first step in the explicit representation
of visual relationships and using them for complex cognitive procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whittington_J/0/1/0/all/0/1"&gt;James C.R. Whittington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabra_R/0/1/0/all/0/1"&gt;Rishabh Kabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthey_L/0/1/0/all/0/1"&gt;Loic Matthey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1"&gt;Christopher P. Burgess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1"&gt;Alexander Lerchner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings. (arXiv:2107.11003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11003</id>
        <link href="http://arxiv.org/abs/2107.11003"/>
        <updated>2021-07-26T02:00:59.232Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) can be used to learn treatment policies and aid
decision making in healthcare. However, given the need for generalization over
complex state/action spaces, the incorporation of function approximators (e.g.,
deep neural networks) requires model selection to reduce overfitting and
improve policy performance at deployment. Yet a standard validation pipeline
for model selection requires running a learned policy in the actual
environment, which is often infeasible in a healthcare setting. In this work,
we investigate a model selection pipeline for offline RL that relies on
off-policy evaluation (OPE) as a proxy for validation performance. We present
an in-depth analysis of popular OPE methods, highlighting the additional
hyperparameters and computational requirements (fitting/inference of auxiliary
models) when used to rank a set of candidate policies. We compare the utility
of different OPE methods as part of the model selection pipeline in the context
of learning to treat patients with sepsis. Among all the OPE methods we
considered, fitted Q evaluation (FQE) consistently leads to the best validation
ranking, but at a high computational cost. To balance this trade-off between
accuracy of ranking and computational efficiency, we propose a simple two-stage
approach to accelerate model selection by avoiding potentially unnecessary
computation. Our work serves as a practical guide for offline RL model
selection and can help RL practitioners select policies using real-world
datasets. To facilitate reproducibility and future extensions, the code
accompanying this paper is available online at
https://github.com/MLD3/OfflineRL_ModelSelection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shengpu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1"&gt;Jenna Wiens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative adversarial networks in time series: A survey and taxonomy. (arXiv:2107.11098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11098</id>
        <link href="http://arxiv.org/abs/2107.11098"/>
        <updated>2021-07-26T02:00:59.225Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) studies have grown exponentially in
the past few years. Their impact has been seen mainly in the computer vision
field with realistic image and video manipulation, especially generation,
making significant advancements. While these computer vision advances have
garnered much attention, GAN applications have diversified across disciplines
such as time series and sequence generation. As a relatively new niche for
GANs, fieldwork is ongoing to develop high quality, diverse and private time
series data. In this paper, we review GAN variants designed for time series
related applications. We propose a taxonomy of discrete-variant GANs and
continuous-variant GANs, in which GANs deal with discrete time series and
continuous time series data. Here we showcase the latest and most popular
literature in this field; their architectures, results, and applications. We
also provide a list of the most popular evaluation metrics and their
suitability across applications. Also presented is a discussion of privacy
measures for these GANs and further protections and directions for dealing with
sensitive data. We aim to frame clearly and concisely the latest and
state-of-the-art research in this area and their applications to real-world
technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brophy_E/0/1/0/all/0/1"&gt;Eoin Brophy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qi She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1"&gt;Tomas Ward&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel meta-learning initialization method for physics-informed neural networks. (arXiv:2107.10991v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10991</id>
        <link href="http://arxiv.org/abs/2107.10991"/>
        <updated>2021-07-26T02:00:59.218Z</updated>
        <summary type="html"><![CDATA[Physics-informed neural networks (PINNs) have been widely used to solve
various scientific computing problems. However, large training costs limit
PINNs for some real-time applications. Although some works have been proposed
to improve the training efficiency of PINNs, few consider the influence of
initialization. To this end, we propose a New Reptile initialization based
Physics-Informed Neural Network (NRPINN). The original Reptile algorithm is a
meta-learning initialization method based on labeled data. PINNs can be trained
with less labeled data or even without any labeled data by adding partial
differential equations (PDEs) as a penalty term into the loss function.
Inspired by this idea, we propose the new Reptile initialization to sample more
tasks from the parameterized PDEs and adapt the penalty term of the loss. The
new Reptile initialization can acquire initialization parameters from related
tasks by supervised, unsupervised, and semi-supervised learning. Then, PINNs
with initialization parameters can efficiently solve PDEs. Besides, the new
Reptile initialization can also be used for the variants of PINNs. Finally, we
demonstrate and verify the NRPINN considering both forward problems, including
solving Poisson, Burgers, and Schr\"odinger equations, as well as inverse
problems, where unknown parameters in the PDEs are estimated. Experimental
results show that the NRPINN training is much faster and achieves higher
accuracy than PINNs with other initialization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weien Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.11099</id>
        <link href="http://arxiv.org/abs/2107.11099"/>
        <updated>2021-07-26T02:00:59.211Z</updated>
        <summary type="html"><![CDATA[With the rapid growth of qubit numbers and coherence times in quantum
hardware technology, implementing shallow neural networks on the so-called
Noisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of
interest. Many quantum (convolutional) circuit ansaetze are proposed for
grayscale images classification tasks with promising empirical results.
However, when applying these ansaetze on RGB images, the intra-channel
information that is useful for vision tasks is not extracted effectively. In
this paper, we propose two types of quantum circuit ansaetze to simulate
convolution operations on RGB images, which differ in the way how inter-channel
and intra-channel information are extracted. To the best of our knowledge, this
is the first work of a quantum convolutional circuit to deal with RGB images
effectively, with a higher test accuracy compared to the purely classical CNNs.
We also investigate the relationship between the size of quantum circuit ansatz
and the learnability of the hybrid quantum-classical convolutional neural
network. Through experiments based on CIFAR-10 and MNIST datasets, we
demonstrate that a larger size of the quantum circuit ansatz improves
predictive performance in multiclass classification tasks, providing useful
insights for near term quantum algorithm developments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1"&gt;Yu Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chonghang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1"&gt;Wenbing Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaogang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11022</id>
        <link href="http://arxiv.org/abs/2107.11022"/>
        <updated>2021-07-26T02:00:59.204Z</updated>
        <summary type="html"><![CDATA[We consider unsupervised cell nuclei segmentation in this paper. Exploiting
the recently-proposed unpaired image-to-image translation between cell nuclei
images and randomly synthetic masks, existing approaches, e.g., CycleGAN, have
achieved encouraging results. However, these methods usually take a two-stage
pipeline and fail to learn end-to-end in cell nuclei images. More seriously,
they could lead to the lossy transformation problem, i.e., the content
inconsistency between the original images and the corresponding segmentation
output. To address these limitations, we propose a novel end-to-end
unsupervised framework called Aligned Disentangling Generative Adversarial
Network (AD-GAN). Distinctively, AD-GAN introduces representation
disentanglement to separate content representation (the underling spatial
structure) from style representation (the rendering of the structure). With
this framework, spatial structure can be preserved explicitly, enabling a
significant reduction of macro-level lossy transformation. We also propose a
novel training algorithm able to align the disentangled content in the latent
space to reduce micro-level lossy transformation. Evaluations on real-world 2D
and 3D datasets show that AD-GAN substantially outperforms the other comparison
methods and the professional software both quantitatively and qualitatively.
Specifically, the proposed AD-GAN leads to significant improvement over the
current best unsupervised methods by an average 17.8% relatively (w.r.t. the
metric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN
even performs competitive with the best supervised models, taking a further
leap towards end-to-end unsupervised nuclei segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1"&gt;Kai Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1"&gt;Curran Jude&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven deep density estimation. (arXiv:2107.11085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11085</id>
        <link href="http://arxiv.org/abs/2107.11085"/>
        <updated>2021-07-26T02:00:59.184Z</updated>
        <summary type="html"><![CDATA[Density estimation plays a crucial role in many data analysis tasks, as it
infers a continuous probability density function (PDF) from discrete samples.
Thus, it is used in tasks as diverse as analyzing population data, spatial
locations in 2D sensor readings, or reconstructing scenes from 3D scans. In
this paper, we introduce a learned, data-driven deep density estimation (DDE)
to infer PDFs in an accurate and efficient manner, while being independent of
domain dimensionality or sample size. Furthermore, we do not require access to
the original PDF during estimation, neither in parametric form, nor as priors,
or in the form of many samples. This is enabled by training an unstructured
convolutional neural network on an infinite stream of synthetic PDFs, as
unbound amounts of synthetic training data generalize better across a deck of
natural PDFs than any natural finite training data will do. Thus, we hope that
our publicly available DDE method will be beneficial in many areas of data
analysis, where continuous models are to be estimated from discrete
observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1"&gt;Patrik Puchert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1"&gt;Pedro Hermosilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1"&gt;Tobias Ritschel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1"&gt;Timo Ropinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTitle -- Leveraging BERT to generate Search Engine Optimized Headlines. (arXiv:2107.10935v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10935</id>
        <link href="http://arxiv.org/abs/2107.10935"/>
        <updated>2021-07-26T02:00:59.176Z</updated>
        <summary type="html"><![CDATA[Automated headline generation for online news articles is not a trivial task
- machine generated titles need to be grammatically correct, informative,
capture attention and generate search traffic without being "click baits" or
"fake news". In this paper we showcase how a pre-trained language model can be
leveraged to create an abstractive news headline generator for German language.
We incorporate state of the art fine-tuning techniques for abstractive text
summarization, i.e. we use different optimizers for the encoder and decoder
where the former is pre-trained and the latter is trained from scratch. We
modify the headline generation to incorporate frequently sought keywords
relevant for search engine optimization. We conduct experiments on a German
news data set and achieve a ROUGE-L-gram F-score of 40.02. Furthermore, we
address the limitations of ROUGE for measuring the quality of text
summarization by introducing a sentence similarity metric and human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anastasiu_C/0/1/0/all/0/1"&gt;Cristian Anastasiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_H/0/1/0/all/0/1"&gt;Hanna Behnke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luck_S/0/1/0/all/0/1"&gt;Sarah L&amp;#xfc;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malesevic_V/0/1/0/all/0/1"&gt;Viktor Malesevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najmi_A/0/1/0/all/0/1"&gt;Aamna Najmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poveda_Panter_J/0/1/0/all/0/1"&gt;Javier Poveda-Panter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Economic Recession Prediction Using Deep Neural Network. (arXiv:2107.10980v1 [econ.GN])]]></title>
        <id>http://arxiv.org/abs/2107.10980</id>
        <link href="http://arxiv.org/abs/2107.10980"/>
        <updated>2021-07-26T02:00:59.169Z</updated>
        <summary type="html"><![CDATA[We investigate the effectiveness of different machine learning methodologies
in predicting economic cycles. We identify the deep learning methodology of
Bi-LSTM with Autoencoder as the most accurate model to forecast the beginning
and end of economic recessions in the U.S. We adopt commonly-available macro
and market-condition features to compare the ability of different machine
learning models to generate good predictions both in-sample and out-of-sample.
The proposed model is flexible and dynamic when both predictive variables and
model coefficients vary over time. It provided good out-of-sample predictions
for the past two recessions and early warning about the COVID-19 recession.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zihao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Li_K/0/1/0/all/0/1"&gt;Kun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Xia_S/0/1/0/all/0/1"&gt;Steve Q. Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11049</id>
        <link href="http://arxiv.org/abs/2107.11049"/>
        <updated>2021-07-26T02:00:59.162Z</updated>
        <summary type="html"><![CDATA[Recent state-of-the-art active learning methods have mostly leveraged
Generative Adversarial Networks (GAN) for sample acquisition; however, GAN is
usually known to suffer from instability and sensitivity to hyper-parameters.
In contrast to these methods, we propose in this paper a novel active learning
framework that we call Maximum Classifier Discrepancy for Active Learning
(MCDAL) which takes the prediction discrepancies between multiple classifiers.
In particular, we utilize two auxiliary classification layers that learn
tighter decision boundaries by maximizing the discrepancies among them.
Intuitively, the discrepancies in the auxiliary classification layers'
predictions indicate the uncertainty in the prediction. In this regard, we
propose a novel method to leverage the classifier discrepancies for the
acquisition function for active learning. We also provide an interpretation of
our idea in relation to existing GAN based active learning methods and domain
adaptation frameworks. Moreover, we empirically demonstrate the utility of our
approach where the performance of our approach exceeds the state-of-the-art
methods on several image classification and semantic segmentation datasets in
active learning setups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jae Won Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dong-Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1"&gt;Yunjae Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1"&gt;In So Kweon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Based Reconstruction of Total Solar Irradiance. (arXiv:2107.11042v1 [astro-ph.SR])]]></title>
        <id>http://arxiv.org/abs/2107.11042</id>
        <link href="http://arxiv.org/abs/2107.11042"/>
        <updated>2021-07-26T02:00:59.143Z</updated>
        <summary type="html"><![CDATA[The Earth's primary source of energy is the radiant energy generated by the
Sun, which is referred to as solar irradiance, or total solar irradiance (TSI)
when all of the radiation is measured. A minor change in the solar irradiance
can have a significant impact on the Earth's climate and atmosphere. As a
result, studying and measuring solar irradiance is crucial in understanding
climate changes and solar variability. Several methods have been developed to
reconstruct total solar irradiance for long and short periods of time; however,
they are physics-based and rely on the availability of data, which does not go
beyond 9,000 years. In this paper we propose a new method, called TSInet, to
reconstruct total solar irradiance by deep learning for short and long periods
of time that span beyond the physical models' data availability. On the data
that are available, our method agrees well with the state-of-the-art
physics-based reconstruction models. To our knowledge, this is the first time
that deep learning has been used to reconstruct total solar irradiance for more
than 9,000 years.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Abduallah_Y/0/1/0/all/0/1"&gt;Yasser Abduallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jason T. L. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yucong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Alobaid_K/0/1/0/all/0/1"&gt;Khalid A. Alobaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Criscuoli_S/0/1/0/all/0/1"&gt;Serena Criscuoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haimin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LocalGLMnet: interpretable deep learning for tabular data. (arXiv:2107.11059v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11059</id>
        <link href="http://arxiv.org/abs/2107.11059"/>
        <updated>2021-07-26T02:00:59.136Z</updated>
        <summary type="html"><![CDATA[Deep learning models have gained great popularity in statistical modeling
because they lead to very competitive regression models, often outperforming
classical statistical models such as generalized linear models. The
disadvantage of deep learning models is that their solutions are difficult to
interpret and explain, and variable selection is not easily possible because
deep learning models solve feature engineering and variable selection
internally in a nontransparent way. Inspired by the appealing structure of
generalized linear models, we propose a new network architecture that shares
similar features as generalized linear models, but provides superior predictive
power benefiting from the art of representation learning. This new architecture
allows for variable selection of tabular data and for interpretation of the
calibrated deep learning model, in fact, our approach provides an additive
decomposition in the spirit of Shapley values and integrated gradients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richman_R/0/1/0/all/0/1"&gt;Ronald Richman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1"&gt;Mario V. W&amp;#xfc;thrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Predictive Uncertainty Under Program Data Distribution Shift. (arXiv:2107.10989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10989</id>
        <link href="http://arxiv.org/abs/2107.10989"/>
        <updated>2021-07-26T02:00:59.128Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) techniques have achieved great success in predictive
accuracy in a variety of tasks, but deep neural networks (DNNs) are shown to
produce highly overconfident scores for even abnormal samples. Well-defined
uncertainty indicates whether a model's output should (or should not) be
trusted and thus becomes critical in real-world scenarios which typically
involves shifted input distributions due to many factors. Existing uncertainty
approaches assume that testing samples from a different data distribution would
induce unreliable model predictions thus have higher uncertainty scores. They
quantify model uncertainty by calibrating DL model's confidence of a given
input and evaluate the effectiveness in computer vision (CV) and natural
language processing (NLP)-related tasks. However, their methodologies'
reliability may be compromised under programming tasks due to difference in
data representations and shift patterns. In this paper, we first define three
different types of distribution shift in program data and build a large-scale
shifted Java dataset. We implement two common programming language tasks on our
dataset to study the effect of each distribution shift on DL model performance.
We also propose a large-scale benchmark of existing state-of-the-art predictive
uncertainty on programming tasks and investigate their effectiveness under data
distribution shift. Experiments show that program distribution shift does
degrade the DL model performance to varying degrees and that existing
uncertainty methods all present certain limitations in quantifying uncertainty
on program dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Simin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data. (arXiv:2107.11136v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11136</id>
        <link href="http://arxiv.org/abs/2107.11136"/>
        <updated>2021-07-26T02:00:59.121Z</updated>
        <summary type="html"><![CDATA[As one of the most fundamental problems in machine learning, statistics and
differential privacy, Differentially Private Stochastic Convex Optimization
(DP-SCO) has been extensively studied in recent years. However, most of the
previous work can only handle either regular data distribution or irregular
data in the low dimensional space case. To better understand the challenges
arising from irregular data distribution, in this paper we provide the first
study on the problem of DP-SCO with heavy-tailed data in the high dimensional
space. In the first part we focus on the problem over some polytope constraint
(such as the $\ell_1$-norm ball). We show that if the loss function is smooth
and its gradient has bounded second order moment, it is possible to get a (high
probability) error bound (excess population risk) of $\tilde{O}(\frac{\log
d}{(n\epsilon)^\frac{1}{3}})$ in the $\epsilon$-DP model, where $n$ is the
sample size and $d$ is the dimensionality of the underlying space. Next, for
LASSO, if the data distribution that has bounded fourth-order moments, we
improve the bound to $\tilde{O}(\frac{\log d}{(n\epsilon)^\frac{2}{5}})$ in the
$(\epsilon, \delta)$-DP model. In the second part of the paper, we study sparse
learning with heavy-tailed data. We first revisit the sparse linear model and
propose a truncated DP-IHT method whose output could achieve an error of
$\tilde{O}(\frac{s^{*2}\log d}{n\epsilon})$, where $s^*$ is the sparsity of the
underlying parameter. Then we study a more general problem over the sparsity
({\em i.e.,} $\ell_0$-norm) constraint, and show that it is possible to achieve
an error of $\tilde{O}(\frac{s^{*\frac{3}{2}}\log d}{n\epsilon})$, which is
also near optimal up to a factor of $\tilde{O}{(\sqrt{s^*})}$, if the loss
function is smooth and strongly convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lijie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1"&gt;Shuo Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hanshen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing: DeepHead, Wide-band Electromagnetic Imaging Paradigm. (arXiv:2107.11107v1 [physics.med-ph])]]></title>
        <id>http://arxiv.org/abs/2107.11107</id>
        <link href="http://arxiv.org/abs/2107.11107"/>
        <updated>2021-07-26T02:00:59.113Z</updated>
        <summary type="html"><![CDATA[Electromagnetic medical imaging in the microwave regime is a hard problem
notorious for 1) instability 2) under-determinism. This two-pronged problem is
tackled with a two-pronged solution that uses double compression to maximally
utilizing the cheap unlabelled data to a) provide a priori information required
to ease under-determinism and b) reduce sensitivity of inference to the input.
The result is a stable solver with a high resolution output. DeepHead is a
fully data-driven implementation of the paradigm proposed in the context of
microwave brain imaging. It infers the dielectric distribution of the brain at
a desired single frequency while making use of an input that spreads over a
wide band of frequencies. The performance of the model is evaluated with both
simulations and human volunteers experiments. The inference made is juxtaposed
with ground-truth dielectric distribution in simulation case, and the golden
MRI / CT imaging modalities of the volunteers in real-world case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Al_Saffar_A/0/1/0/all/0/1"&gt;A. Al-Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Guo_L/0/1/0/all/0/1"&gt;L. Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Abbosh_A/0/1/0/all/0/1"&gt;A. Abbosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptively Weighted Top-N Recommendation for Organ Matching. (arXiv:2107.10971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10971</id>
        <link href="http://arxiv.org/abs/2107.10971"/>
        <updated>2021-07-26T02:00:59.104Z</updated>
        <summary type="html"><![CDATA[Reducing the shortage of organ donations to meet the demands of patients on
the waiting list has being a major challenge in organ transplantation. Because
of the shortage, organ matching decision is the most critical decision to
assign the limited viable organs to the most suitable patients. Currently,
organ matching decisions were only made by matching scores calculated via
scoring models, which are built by the first principles. However, these models
may disagree with the actual post-transplantation matching performance (e.g.,
patient's post-transplant quality of life (QoL) or graft failure measurements).
In this paper, we formulate the organ matching decision-making as a top-N
recommendation problem and propose an Adaptively Weighted Top-N Recommendation
(AWTR) method. AWTR improves performance of the current scoring models by using
limited actual matching performance in historical data set as well as the
collected covariates from organ donors and patients. AWTR sacrifices the
overall recommendation accuracy by emphasizing the recommendation and ranking
accuracy for top-N matched patients. The proposed method is validated in a
simulation study, where KAS [60] is used to simulate the organ-patient
recommendation response. The results show that our proposed method outperforms
seven state-of-the-art top-N recommendation benchmark methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1"&gt;Parshin Shojaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Ran Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisDA-2021 Competition Universal Domain Adaptation to Improve Performance on Out-of-Distribution Data. (arXiv:2107.11011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11011</id>
        <link href="http://arxiv.org/abs/2107.11011"/>
        <updated>2021-07-26T02:00:59.097Z</updated>
        <summary type="html"><![CDATA[Progress in machine learning is typically measured by training and testing a
model on the same distribution of data, i.e., the same domain. This
over-estimates future accuracy on out-of-distribution data. The Visual Domain
Adaptation (VisDA) 2021 competition tests models' ability to adapt to novel
test distributions and handle distributional shift. We set up unsupervised
domain adaptation challenges for image classifiers and will evaluate adaptation
to novel viewpoints, backgrounds, modalities and degradation in quality. Our
challenge draws on large-scale publicly available datasets but constructs the
evaluation across domains, rather that the traditional in-domain bench-marking.
Furthermore, we focus on the difficult "universal" setting where, in addition
to input distribution drift, methods may encounter missing and/or novel classes
in the target dataset. Performance will be measured using a rigorous protocol,
comparing to state-of-the-art domain adaptation methods with the help of
established metrics. We believe that the competition will encourage further
improvement in machine learning methods' ability to handle realistic data in
many deployment scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Donghyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Samarth Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1"&gt;Kuniaki Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1"&gt;Piotr Teterwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1"&gt;Ben Usman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tsformer: Time series Transformer for tourism demand forecasting. (arXiv:2107.10977v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10977</id>
        <link href="http://arxiv.org/abs/2107.10977"/>
        <updated>2021-07-26T02:00:59.088Z</updated>
        <summary type="html"><![CDATA[AI-based methods have been widely applied to tourism demand forecasting.
However, current AI-based methods are short of the ability to process long-term
dependency, and most of them lack interpretability. The Transformer used
initially for machine translation shows an incredible ability to long-term
dependency processing. Based on the Transformer, we proposed a time series
Transformer (Tsformer) with Encoder-Decoder architecture for tourism demand
forecasting. The proposed Tsformer encodes long-term dependency with encoder,
captures short-term dependency with decoder, and simplifies the attention
interactions under the premise of highlighting dominant attention through a
series of attention masking mechanisms. These improvements make the multi-head
attention mechanism process the input sequence according to the time
relationship, contributing to better interpretability. What's more, the context
processing ability of the Encoder-Decoder architecture allows adopting the
calendar of days to be forecasted to enhance the forecasting performance.
Experiments conducted on the Jiuzhaigou valley and Siguniang mountain tourism
demand datasets with other nine baseline methods indicate that the proposed
Tsformer outperformed all baseline models in the short-term and long-term
tourism demand forecasting tasks. Moreover, ablation studies demonstrate that
the adoption of the calendar of days to be forecasted contributes to the
forecasting performance of the proposed Tsformer. For better interpretability,
the attention weight matrix visualization is performed. It indicates that the
Tsformer concentrates on seasonal features and days close to days to be
forecast in short-term forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Siyuan Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chuanming Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the structure of wind: A data-driven nonlocal turbulence model for the atmospheric boundary layer. (arXiv:2107.11046v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.11046</id>
        <link href="http://arxiv.org/abs/2107.11046"/>
        <updated>2021-07-26T02:00:59.067Z</updated>
        <summary type="html"><![CDATA[We develop a novel data-driven approach to modeling the atmospheric boundary
layer. This approach leads to a nonlocal, anisotropic synthetic turbulence
model which we refer to as the deep rapid distortion (DRD) model. Our approach
relies on an operator regression problem which characterizes the best fitting
candidate in a general family of nonlocal covariance kernels parameterized in
part by a neural network. This family of covariance kernels is expressed in
Fourier space and is obtained from approximate solutions to the Navier--Stokes
equations at very high Reynolds numbers. Each member of the family incorporates
important physical properties such as mass conservation and a realistic energy
cascade. The DRD model can be calibrated with noisy data from field
experiments. After calibration, the model can be used to generate synthetic
turbulent velocity fields. To this end, we provide a new numerical method based
on domain decomposition which delivers scalable, memory-efficient turbulence
generation with the DRD model as well as others. We demonstrate the robustness
of our approach with both filtered and noisy data coming from the 1968 Air
Force Cambridge Research Laboratory Kansas experiments. Using this data, we
witness exceptional accuracy with the DRD model, especially when compared to
the International Electrotechnical Commission standard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Keith_B/0/1/0/all/0/1"&gt;Brendan Keith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Khristenko_U/0/1/0/all/0/1"&gt;Ustim Khristenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wohlmuth_B/0/1/0/all/0/1"&gt;Barbara Wohlmuth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10970</id>
        <link href="http://arxiv.org/abs/2107.10970"/>
        <updated>2021-07-26T02:00:59.051Z</updated>
        <summary type="html"><![CDATA[The null space of the $k$-th order Laplacian $\mathbf{\mathcal L}_k$, known
as the {\em $k$-th homology vector space}, encodes the non-trivial topology of
a manifold or a network. Understanding the structure of the homology embedding
can thus disclose geometric or topological information from the data. The study
of the null space embedding of the graph Laplacian $\mathbf{\mathcal L}_0$ has
spurred new research and applications, such as spectral clustering algorithms
with theoretical guarantees and estimators of the Stochastic Block Model. In
this work, we investigate the geometry of the $k$-th homology embedding and
focus on cases reminiscent of spectral clustering. Namely, we analyze the {\em
connected sum} of manifolds as a perturbation to the direct sum of their
homology embeddings. We propose an algorithm to factorize the homology
embedding into subspaces corresponding to a manifold's simplest topological
components. The proposed framework is applied to the {\em shortest homologous
loop detection} problem, a problem known to be NP-hard in general. Our spectral
loop detection algorithm scales better than existing methods and is effective
on diverse data such as point clouds and images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication Efficiency in Federated Learning: Achievements and Challenges. (arXiv:2107.10996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10996</id>
        <link href="http://arxiv.org/abs/2107.10996"/>
        <updated>2021-07-26T02:00:59.042Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is known to perform Machine Learning tasks in a
distributed manner. Over the years, this has become an emerging technology
especially with various data protection and privacy policies being imposed FL
allows performing machine learning tasks whilst adhering to these challenges.
As with the emerging of any new technology, there are going to be challenges
and benefits. A challenge that exists in FL is the communication costs, as FL
takes place in a distributed environment where devices connected over the
network have to constantly share their updates this can create a communication
bottleneck. In this paper, we present a survey of the research that is
performed to overcome the communication constraints in an FL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahid_O/0/1/0/all/0/1"&gt;Osama Shahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouriyeh_S/0/1/0/all/0/1"&gt;Seyedamin Pouriyeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parizi_R/0/1/0/all/0/1"&gt;Reza M. Parizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Quan Z. Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1"&gt;Gautam Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift. (arXiv:2107.11056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11056</id>
        <link href="http://arxiv.org/abs/2107.11056"/>
        <updated>2021-07-26T02:00:59.013Z</updated>
        <summary type="html"><![CDATA[Meta-learning provides a promising way for learning to efficiently learn and
achieves great success in many applications. However, most meta-learning
literature focuses on dealing with tasks from a same domain, making it brittle
to generalize to tasks from the other unseen domains. In this work, we address
this problem by simulating tasks from the other unseen domains to improve the
generalization and robustness of meta-learning method. Specifically, we propose
a model-agnostic shift layer to learn how to simulate the domain shift and
generate pseudo tasks, and develop a new adversarial learning-to-learn
mechanism to train it. Based on the pseudo tasks, the meta-learning model can
learn cross-domain meta-knowledge, which can generalize well on unseen domains.
We conduct extensive experiments under the domain generalization setting.
Experimental results demonstrate that the proposed shift layer is applicable to
various meta-learning frameworks. Moreover, our method also leads to
state-of-the-art performance on different cross-domain few-shot classification
benchmarks and produces good results on cross-domain few-shot regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1"&gt;Pinzhuo Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yao Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks. (arXiv:2107.10957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10957</id>
        <link href="http://arxiv.org/abs/2107.10957"/>
        <updated>2021-07-26T02:00:58.998Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have achieved remarkable success as a framework
for deep learning on graph-structured data. However, GNNs are fundamentally
limited by their tree-structured inductive bias: the WL-subtree kernel
formulation bounds the representational capacity of GNNs, and polynomial-time
GNNs are provably incapable of recognizing triangles in a graph. In this work,
we propose to augment the GNN message-passing operations with information
defined on ego graphs (i.e., the induced subgraph surrounding each node). We
term these approaches Ego-GNNs and show that Ego-GNNs are provably more
powerful than standard message-passing GNNs. In particular, we show that
Ego-GNNs are capable of recognizing closed triangles, which is essential given
the prominence of transitivity in real-world graphs. We also motivate our
approach from the perspective of graph signal processing as a form of multiplex
graph convolution. Experimental results on node classification using synthetic
and real data highlight the achievable performance gains using this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sandfelder_D/0/1/0/all/0/1"&gt;Dylan Sandfelder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1"&gt;Priyesh Vijayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules. (arXiv:2107.10882v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10882</id>
        <link href="http://arxiv.org/abs/2107.10882"/>
        <updated>2021-07-26T02:00:58.979Z</updated>
        <summary type="html"><![CDATA[The use of machine learning in chemistry has become a common practice. At the
same time, despite the success of modern machine learning methods, the lack of
data limits their use. Using a transfer learning methodology can help solve
this problem. This methodology assumes that a model built on a sufficient
amount of data captures general features of the chemical compound structure on
which it was trained and that the further reuse of these features on a dataset
with a lack of data will greatly improve the quality of the new model. In this
paper, we develop this approach for small organic molecules, implementing
transfer learning with graph convolutional neural networks. The paper shows a
significant improvement in the performance of models for target properties with
a lack of data. The effects of the dataset composition on model quality and the
applicability domain of the resulting models are also considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karpov_K/0/1/0/all/0/1"&gt;Kirill Karpov&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrofanov_A/0/1/0/all/0/1"&gt;Artem Mitrofanov&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Korolev_V/0/1/0/all/0/1"&gt;Vadim Korolev&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Tkachenko_V/0/1/0/all/0/1"&gt;Valery Tkachenko&lt;/a&gt; (2) ((1) Lomonosov Moscow State University, Department of Chemistry, Leninskie gory, 1 bld. 3, Moscow, Russia, (2) Science Data Software, LLC, 14909 Forest Landing Cir, Rockville, USA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation. (arXiv:2107.10955v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10955</id>
        <link href="http://arxiv.org/abs/2107.10955"/>
        <updated>2021-07-26T02:00:58.972Z</updated>
        <summary type="html"><![CDATA[We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Specially, under both
Gaussian and sub-Gaussian models, we study the sample size conditions for the
well-known Chow-Liu algorithm to exactly recover the equivalence class of the
polytree, which is uniquely represented by a CPDAG. We also study the error
rate for the estimation of the inverse correlation matrix under such models.
Our theoretical findings are illustrated by comprehensive numerical
simulations, and experiments on benchmark data also demonstrate the robustness
of the method when the ground truth graphical structure can only be
approximated by a polytree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lou_X/0/1/0/all/0/1"&gt;Xingmei Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaodong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks. (arXiv:2107.10963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10963</id>
        <link href="http://arxiv.org/abs/2107.10963"/>
        <updated>2021-07-26T02:00:58.962Z</updated>
        <summary type="html"><![CDATA[Conditional computation and modular networks have been recently proposed for
multitask learning and other problems as a way to decompose problem solving
into multiple reusable computational blocks. We propose a new approach for
learning modular networks based on the isometric version of ResNet with all
residual blocks having the same configuration and the same number of
parameters. This architectural choice allows adding, removing and changing the
order of residual blocks. In our method, the modules can be invoked repeatedly
and allow knowledge transfer to novel tasks by adjusting the order of
computation. This allows soft weight sharing between tasks with only a small
increase in the number of parameters. We show that our method leads to
interpretable self-organization of modules in case of multi-task learning,
transfer learning and domain adaptation while achieving competitive results on
those tasks. From practical perspective, our approach allows to: (a) reuse
existing modules for learning new task by adjusting the computation order, (b)
use it for unsupervised multi-source domain adaptation to illustrate that
adaptation to unseen data can be achieved by only manipulating the order of
pretrained modules, (c) show how our approach can be used to increase accuracy
of existing architectures for image classification tasks such as ImageNet,
without any parameter increase, by reusing the same block multiple times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1"&gt;Andrey Zhmoginov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1"&gt;Mark Sandler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of Convolution Neural Networks on Heterogeneous Signals for Sleep Stage Scoring. (arXiv:2107.11045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11045</id>
        <link href="http://arxiv.org/abs/2107.11045"/>
        <updated>2021-07-26T02:00:58.948Z</updated>
        <summary type="html"><![CDATA[Over the years, several approaches have tried to tackle the problem of
performing an automatic scoring of the sleeping stages. Although any
polysomnography usually collects over a dozen of different signals, this
particular problem has been mainly tackled by using only the
Electroencephalograms presented in those records. On the other hand, the other
recorded signals have been mainly ignored by most works. This paper explores
and compares the convenience of using additional signals apart from
electroencephalograms. More specifically, this work uses the SHHS-1 dataset
with 5,804 patients containing an electromyogram recorded simultaneously as two
electroencephalograms. To compare the results, first, the same architecture has
been evaluated with different input signals and all their possible
combinations. These tests show how, using more than one signal especially if
they are from different sources, improves the results of the classification.
Additionally, the best models obtained for each combination of one or more
signals have been used in ensemble models and, its performance has been
compared showing the convenience of using these multi-signal models to improve
the classification. The best overall model, an ensemble of Depth-wise
Separational Convolutional Neural Networks, has achieved an accuracy of 86.06\%
with a Cohen's Kappa of 0.80 and a $F_{1}$ of 0.77. Up to date, those are the
best results on the complete dataset and it shows a significant improvement in
the precision and recall for the most uncommon class in the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Blanco_E/0/1/0/all/0/1"&gt;Enrique Fernandez-Blanco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Lozano_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Lozano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pazos_A/0/1/0/all/0/1"&gt;Alejandro Pazos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivero_D/0/1/0/all/0/1"&gt;Daniel Rivero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Rate-Constrained Optimization of Non-decomposable Objectives. (arXiv:2107.10960v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10960</id>
        <link href="http://arxiv.org/abs/2107.10960"/>
        <updated>2021-07-26T02:00:58.925Z</updated>
        <summary type="html"><![CDATA[We consider a popular family of constrained optimization problems arising in
machine learning that involve optimizing a non-decomposable evaluation metric
with a certain thresholded form, while constraining another metric of interest.
Examples of such problems include optimizing the false negative rate at a fixed
false positive rate, optimizing precision at a fixed recall, optimizing the
area under the precision-recall or ROC curves, etc. Our key idea is to
formulate a rate-constrained optimization that expresses the threshold
parameter as a function of the model parameters via the Implicit Function
theorem. We show how the resulting optimization problem can be solved using
standard gradient based methods. Experiments on benchmark datasets demonstrate
the effectiveness of our proposed method over existing state-of-the art
approaches for these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1"&gt;Andrew Cotter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservoir Computing Approach for Gray Images Segmentation. (arXiv:2107.11077v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11077</id>
        <link href="http://arxiv.org/abs/2107.11077"/>
        <updated>2021-07-26T02:00:58.918Z</updated>
        <summary type="html"><![CDATA[The paper proposes a novel approach for gray scale images segmentation. It is
based on multiple features extraction from single feature per image pixel,
namely its intensity value, using Echo state network. The newly extracted
features -- reservoir equilibrium states -- reveal hidden image characteristics
that improve its segmentation via a clustering algorithm. Moreover, it was
demonstrated that the intrinsic plasticity tuning of reservoir fits its
equilibrium states to the original image intensity distribution thus allowing
for its better segmentation. The proposed approach is tested on the benchmark
image Lena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koprinkova_Hristova_P/0/1/0/all/0/1"&gt;Petia Koprinkova-Hristova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A reinforcement learning approach to resource allocation in genomic selection. (arXiv:2107.10901v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2107.10901</id>
        <link href="http://arxiv.org/abs/2107.10901"/>
        <updated>2021-07-26T02:00:58.907Z</updated>
        <summary type="html"><![CDATA[Genomic selection (GS) is a technique that plant breeders use to select
individuals to mate and produce new generations of species. Allocation of
resources is a key factor in GS. At each selection cycle, breeders are facing
the choice of budget allocation to make crosses and produce the next generation
of breeding parents. Inspired by recent advances in reinforcement learning for
AI problems, we develop a reinforcement learning-based algorithm to
automatically learn to allocate limited resources across different generations
of breeding. We mathematically formulate the problem in the framework of Markov
Decision Process (MDP) by defining state and action spaces. To avoid the
explosion of the state space, an integer linear program is proposed that
quantifies the trade-off between resources and time. Finally, we propose a
value function approximation method to estimate the action-value function and
then develop a greedy policy improvement technique to find the optimal
resources. We demonstrate the effectiveness of the proposed method in enhancing
genetic gain using a case study with realistic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Moeinizade_S/0/1/0/all/0/1"&gt;Saba Moeinizade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guiping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lizhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Adaptive State Aggregation Algorithm for Markov Decision Processes. (arXiv:2107.11053v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11053</id>
        <link href="http://arxiv.org/abs/2107.11053"/>
        <updated>2021-07-26T02:00:58.899Z</updated>
        <summary type="html"><![CDATA[Value iteration is a well-known method of solving Markov Decision Processes
(MDPs) that is simple to implement and boasts strong theoretical convergence
guarantees. However, the computational cost of value iteration quickly becomes
infeasible as the size of the state space increases. Various methods have been
proposed to overcome this issue for value iteration in large state and action
space MDPs, often at the price, however, of generalizability and algorithmic
simplicity. In this paper, we propose an intuitive algorithm for solving MDPs
that reduces the cost of value iteration updates by dynamically grouping
together states with similar cost-to-go values. We also prove that our
algorithm converges almost surely to within \(2\varepsilon / (1 - \gamma)\) of
the true optimal value in the \(\ell^\infty\) norm, where \(\gamma\) is the
discount factor and aggregated states differ by at most \(\varepsilon\).
Numerical experiments on a variety of simulated environments confirm the
robustness of our algorithm and its ability to solve MDPs with much cheaper
updates especially as the scale of the MDP problem increases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guanting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaebler_J/0/1/0/all/0/1"&gt;Johann Demetrio Gaebler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1"&gt;Matt Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chunlin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinyu Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning Versus Classical Machine Learning: A Convergence Comparison. (arXiv:2107.10976v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10976</id>
        <link href="http://arxiv.org/abs/2107.10976"/>
        <updated>2021-07-26T02:00:58.880Z</updated>
        <summary type="html"><![CDATA[In the past few decades, machine learning has revolutionized data processing
for large scale applications. Simultaneously, increasing privacy threats in
trending applications led to the redesign of classical data training models. In
particular, classical machine learning involves centralized data training,
where the data is gathered, and the entire training process executes at the
central server. Despite significant convergence, this training involves several
privacy threats on participants' data when shared with the central cloud
server. To this end, federated learning has achieved significant importance
over distributed data training. In particular, the federated learning allows
participants to collaboratively train the local models on local data without
revealing their sensitive information to the central cloud server. In this
paper, we perform a convergence comparison between classical machine learning
and federated learning on two publicly available datasets, namely,
logistic-regression-MNIST dataset and image-classification-CIFAR-10 dataset.
The simulation results demonstrate that federated learning achieves higher
convergence within limited communication rounds while maintaining participants'
anonymity. We hope that this research will show the benefits and help federated
learning to be implemented widely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asad_M/0/1/0/all/0/1"&gt;Muhammad Asad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustafa_A/0/1/0/all/0/1"&gt;Ahmed Moustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1"&gt;Takayuki Ito&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HURRA! Human readable router anomaly detection. (arXiv:2107.11078v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.11078</id>
        <link href="http://arxiv.org/abs/2107.11078"/>
        <updated>2021-07-26T02:00:58.873Z</updated>
        <summary type="html"><![CDATA[This paper presents HURRA, a system that aims to reduce the time spent by
human operators in the process of network troubleshooting. To do so, it
comprises two modules that are plugged after any anomaly detection algorithm:
(i) a first attention mechanism, that ranks the present features in terms of
their relation with the anomaly and (ii) a second module able to incorporates
previous expert knowledge seamlessly, without any need of human interaction nor
decisions. We show the efficacy of these simple processes on a collection of
real router datasets obtained from tens of ISPs which exhibit a rich variety of
anomalies and very heterogeneous set of KPIs, on which we gather manually
annotated ground truth by the operator solving the troubleshooting ticket. Our
experimental evaluation shows that (i) the proposed system is effective in
achieving high levels of agreement with the expert, that (ii) even a simple
statistical approach is able to extracting useful information from expert
knowledge gained in past cases to further improve performance and finally that
(iii) the main difficulty in live deployment concerns the automated selection
of the anomaly detection algorithm and the tuning of its hyper-parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1"&gt;Jose M. Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1"&gt;Dario Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Sparse Interpretable Dynamics from Partial Observations. (arXiv:2107.10879v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10879</id>
        <link href="http://arxiv.org/abs/2107.10879"/>
        <updated>2021-07-26T02:00:58.864Z</updated>
        <summary type="html"><![CDATA[Identifying the governing equations of a nonlinear dynamical system is key to
both understanding the physical features of the system and constructing an
accurate model of the dynamics that generalizes well beyond the available data.
We propose a machine learning framework for discovering these governing
equations using only partial observations, combining an encoder for state
reconstruction with a sparse symbolic model. Our tests show that this method
can successfully reconstruct the full system state and identify the underlying
dynamics for a variety of ODE and PDE systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Peter Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arino_J/0/1/0/all/0/1"&gt;Joan Ari&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1"&gt;Marin Solja&amp;#x10d;i&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference. (arXiv:2107.10931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10931</id>
        <link href="http://arxiv.org/abs/2107.10931"/>
        <updated>2021-07-26T02:00:58.857Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a domain generalization (DG) approach to learn on
several labeled source domains and transfer knowledge to a target domain that
is inaccessible in training. Considering the inherent conditional and label
shifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the
widely used domain invariant feature learning (IFL) methods relies on aligning
the marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic
assumption that $p(y)$ is invariant across domains. We thereby propose a novel
variational Bayesian inference framework to enforce the conditional
distribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a
latent space, which also takes the marginal label shift w.r.t. $p(y)$ into
consideration with the posterior alignment. Extensive experiments on various
benchmarks demonstrate that our framework is robust to the label shift and the
cross-domain accuracy is significantly improved, thereby achieving superior
performance over the conventional IFL counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1"&gt;Jinsong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges EL Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured second-order methods via natural gradient descent. (arXiv:2107.10884v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10884</id>
        <link href="http://arxiv.org/abs/2107.10884"/>
        <updated>2021-07-26T02:00:58.849Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose new structured second-order methods and structured
adaptive-gradient methods obtained by performing natural-gradient descent on
structured parameter spaces. Natural-gradient descent is an attractive approach
to design new algorithms in many settings such as gradient-free,
adaptive-gradient, and second-order methods. Our structured methods not only
enjoy a structural invariance but also admit a simple expression. Finally, we
test the efficiency of our proposed methods on both deterministic non-convex
problems and deep learning problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. (arXiv:2107.10932v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10932</id>
        <link href="http://arxiv.org/abs/2107.10932"/>
        <updated>2021-07-26T02:00:58.842Z</updated>
        <summary type="html"><![CDATA[In this note we examine the autoregressive generalization of the FNet
algorithm, in which self-attention layers from the standard Transformer
architecture are substituted with a trivial sparse-uniformsampling procedure
based on Fourier transforms. Using the Wikitext-103 benchmark, we
demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the
task of causal language modelingcompared to a Transformer-XL baseline (24.2
ppl) with only half the number self-attention layers,thus providing further
evidence for the superfluity of deep neural networks with heavily
compoundedattention mechanisms. The autoregressive Fourier transform could
likely be used for parameterreduction on most Transformer-based time-series
prediction models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1"&gt;Tim Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1"&gt;Michael Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezanali_M/0/1/0/all/0/1"&gt;Mohammad Ramezanali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_V/0/1/0/all/0/1"&gt;Vincent Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are you optimizing for? Aligning Recommender Systems with Human Values. (arXiv:2107.10939v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.10939</id>
        <link href="http://arxiv.org/abs/2107.10939"/>
        <updated>2021-07-26T02:00:58.821Z</updated>
        <summary type="html"><![CDATA[We describe cases where real recommender systems were modified in the service
of various human values such as diversity, fairness, well-being, time well
spent, and factual accuracy. From this we identify the current practice of
values engineering: the creation of classifiers from human-created data with
value-based labels. This has worked in practice for a variety of issues, but
problems are addressed one at a time, and users and other stakeholders have
seldom been involved. Instead, we look to AI alignment work for approaches that
could learn complex values directly from stakeholders, and identify four major
directions: useful measures of alignment, participatory design and operation,
interactive value learning, and informed deliberative judgments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1"&gt;Jonathan Stray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vendrov_I/0/1/0/all/0/1"&gt;Ivan Vendrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1"&gt;Jeremy Nixon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Steven Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1"&gt;Dylan Hadfield-Menell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10873</id>
        <link href="http://arxiv.org/abs/2107.10873"/>
        <updated>2021-07-26T02:00:58.781Z</updated>
        <summary type="html"><![CDATA[Recent studies show that deep neural networks (DNN) are vulnerable to
adversarial examples, which aim to mislead DNNs by adding perturbations with
small magnitude. To defend against such attacks, both empirical and theoretical
defense approaches have been extensively studied for a single ML model. In this
work, we aim to analyze and provide the certified robustness for ensemble ML
models, together with the sufficient and necessary conditions of robustness for
different ensemble protocols. Although ensemble models are shown more robust
than a single model empirically; surprisingly, we find that in terms of the
certified robustness the standard ensemble models only achieve marginal
improvement compared to a single model. Thus, to explore the conditions that
guarantee to provide certifiably robust ensemble ML models, we first prove that
diversified gradient and large confidence margin are sufficient and necessary
conditions for certifiably robust ensemble models under the model-smoothness
assumption. We then provide the bounded model-smoothness analysis based on the
proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble
model can always achieve higher certified robustness than a single base model
under mild conditions. Inspired by the theoretical findings, we propose the
lightweight Diversity Regularized Training (DRT) to train certifiably robust
ensemble ML models. Extensive experiments show that our DRT enhanced ensembles
can consistently achieve higher certified robustness than existing single and
ensemble ML models, demonstrating the state-of-the-art certified L2-robustness
on MNIST, CIFAR-10, and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiclass versus Binary Differentially Private PAC Learning. (arXiv:2107.10870v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10870</id>
        <link href="http://arxiv.org/abs/2107.10870"/>
        <updated>2021-07-26T02:00:58.687Z</updated>
        <summary type="html"><![CDATA[We show a generic reduction from multiclass differentially private PAC
learning to binary private PAC learning. We apply this transformation to a
recently proposed binary private PAC learner to obtain a private multiclass
learner with sample complexity that has a polynomial dependence on the
multiclass Littlestone dimension and a poly-logarithmic dependence on the
number of classes. This yields an exponential improvement in the dependence on
both parameters over learners from previous work. Our proof extends the notion
of $\Psi$-dimension defined in work of Ben-David et al. [JCSS '95] to the
online setting and explores its general properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1"&gt;Mark Bun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1"&gt;Satchit Sivakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bagging, optimized dynamic mode decomposition (BOP-DMD) for robust, stable forecasting with spatial and temporal uncertainty-quantification. (arXiv:2107.10878v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10878</id>
        <link href="http://arxiv.org/abs/2107.10878"/>
        <updated>2021-07-26T02:00:58.664Z</updated>
        <summary type="html"><![CDATA[Dynamic mode decomposition (DMD) provides a regression framework for
adaptively learning a best-fit linear dynamics model over snapshots of
temporal, or spatio-temporal, data. A diversity of regression techniques have
been developed for producing the linear model approximation whose solutions are
exponentials in time. For spatio-temporal data, DMD provides low-rank and
interpretable models in the form of dominant modal structures along with their
exponential/oscillatory behavior in time. The majority of DMD algorithms,
however, are prone to bias errors from noisy measurements of the dynamics,
leading to poor model fits and unstable forecasting capabilities. The optimized
DMD algorithm minimizes the model bias with a variable projection optimization,
thus leading to stabilized forecasting capabilities. Here, the optimized DMD
algorithm is improved by using statistical bagging methods whereby a single set
of snapshots is used to produce an ensemble of optimized DMD models. The
outputs of these models are averaged to produce a bagging, optimized dynamic
mode decomposition (BOP-DMD). BOP-DMD not only improves performance, it also
robustifies the model and provides both spatial and temporal uncertainty
quantification (UQ). Thus unlike currently available DMD algorithms, BOP-DMD
provides a stable and robust model for probabilistic, or Bayesian forecasting
with comprehensive UQ metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sashidhar_D/0/1/0/all/0/1"&gt;Diya Sashidhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diagonal Attention and Style-based GAN for Content-Style Disentanglement in Image Generation and Translation. (arXiv:2103.16146v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16146</id>
        <link href="http://arxiv.org/abs/2103.16146"/>
        <updated>2021-07-26T02:00:58.639Z</updated>
        <summary type="html"><![CDATA[One of the important research topics in image generative models is to
disentangle the spatial contents and styles for their separate control.
Although StyleGAN can generate content feature vectors from random noises, the
resulting spatial content control is primarily intended for minor spatial
variations, and the disentanglement of global content and styles is by no means
complete. Inspired by a mathematical understanding of normalization and
attention, here we present a novel hierarchical adaptive Diagonal spatial
ATtention (DAT) layers to separately manipulate the spatial contents from
styles in a hierarchical manner. Using DAT and AdaIN, our method enables
coarse-to-fine level disentanglement of spatial contents and styles. In
addition, our generator can be easily integrated into the GAN inversion
framework so that the content and style of translated images from multi-domain
image translation tasks can be flexibly controlled. By using various datasets,
we confirm that the proposed method not only outperforms the existing models in
disentanglement scores, but also provides more flexible control over spatial
features in the generated images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1"&gt;Gihyun Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation. (arXiv:2101.11223v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11223</id>
        <link href="http://arxiv.org/abs/2101.11223"/>
        <updated>2021-07-26T02:00:58.630Z</updated>
        <summary type="html"><![CDATA[A key assumption of top-down human pose estimation approaches is their
expectation of having a single person/instance present in the input bounding
box. This often leads to failures in crowded scenes with occlusions. We propose
a novel solution to overcome the limitations of this fundamental assumption.
Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose
instances within a given bounding box. We introduce a Multi-Instance Modulation
Block (MIMB) that can adaptively modulate channel-wise feature responses for
each instance and is parameter efficient. We demonstrate the efficacy of our
approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,
we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant
improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using
ground truth bounding boxes for inference, MIPNet achieves an improvement of
0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets
compared to HRNet. Interestingly, when fewer, high confidence bounding boxes
are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet
maintains a relatively stable performance (drop of 1 AP) for the same inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1"&gt;Rawal Khirodkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chari_V/0/1/0/all/0/1"&gt;Visesh Chari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Amit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1"&gt;Ambrish Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised asymmetric deep hashing with margin-scalable constraint. (arXiv:2012.03820v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03820</id>
        <link href="http://arxiv.org/abs/2012.03820"/>
        <updated>2021-07-26T02:00:58.623Z</updated>
        <summary type="html"><![CDATA[Due to its effectivity and efficiency, deep hashing approaches are widely
used for large-scale visual search. However, it is still challenging to produce
compact and discriminative hash codes for images associated with multiple
semantics for two main reasons, 1) similarity constraints designed in most of
the existing methods are based upon an oversimplified similarity
assignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs
sharing at least 1 label), 2) the exploration in multi-semantic relevance are
insufficient or even neglected in many of the existing methods. These problems
significantly limit the discrimination of generated hash codes. In this paper,
we propose a novel self-supervised asymmetric deep hashing method with a
margin-scalable constraint(SADH) approach to cope with these problems. SADH
implements a self-supervised network to sufficiently preserve semantic
information in a semantic feature dictionary and a semantic code dictionary for
the semantics of the given dataset, which efficiently and precisely guides a
feature learning network to preserve multilabel semantic information using an
asymmetric learning strategy. By further exploiting semantic dictionaries, a
new margin-scalable constraint is employed for both precise similarity
searching and robust hash code generation. Extensive empirical research on four
popular benchmarks validates the proposed method and shows it outperforms
several state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhengyang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Song Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhihao Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bakker_E/0/1/0/all/0/1"&gt;Erwin M.Bakker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15797</id>
        <link href="http://arxiv.org/abs/2106.15797"/>
        <updated>2021-07-26T02:00:58.616Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have achieved great success due to the
powerful feature learning ability of convolution layers. Specifically, the
standard convolution traverses the input images/features using a sliding window
scheme to extract features. However, not all the windows contribute equally to
the prediction results of CNNs. In practice, the convolutional operation on
some of the windows (e.g., smooth windows that contain very similar pixels) can
be very redundant and may introduce noises into the computation. Such
redundancy may not only deteriorate the performance but also incur the
unnecessary computational cost. Thus, it is important to reduce the
computational redundancy of convolution to improve the performance. To this
end, we propose a Content-aware Convolution (CAC) that automatically detects
the smooth windows and applies a 1x1 convolutional kernel to replace the
original large kernel. In this sense, we are able to effectively avoid the
redundant computation on similar pixels. By replacing the standard convolution
in CNNs with our CAC, the resultant models yield significantly better
performance and lower computational cost than the baseline models with the
standard convolution. More critically, we are able to dynamically allocate
suitable computation resources according to the data smoothness of different
images, making it possible for content-aware computation. Extensive experiments
on various computer vision tasks demonstrate the superiority of our method over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yaofo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11489</id>
        <link href="http://arxiv.org/abs/2104.11489"/>
        <updated>2021-07-26T02:00:58.608Z</updated>
        <summary type="html"><![CDATA[With increasing automation in passenger vehicles, the study of safe and
smooth occupant-vehicle interaction and control transitions is key. In this
study, we focus on the development of contextual, semantically meaningful
representations of the driver state, which can then be used to determine the
appropriate timing and conditions for transfer of control between driver and
vehicle. To this end, we conduct a large-scale real-world controlled data study
where participants are instructed to take-over control from an autonomous agent
under different driving conditions while engaged in a variety of distracting
activities. These take-over events are captured using multiple driver-facing
cameras, which when labelled result in a dataset of control transitions and
their corresponding take-over times (TOTs). We then develop and train TOT
models that operate sequentially on mid to high-level features produced by
computer vision algorithms operating on different driver-facing camera views.
The proposed TOT model produces continuous predictions of take-over times
without delay, and shows promising qualitative and quantitative results in
complex real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1"&gt;Nachiket Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1"&gt;Ross Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1"&gt;Pujitha Gunaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology. (arXiv:2012.01189v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01189</id>
        <link href="http://arxiv.org/abs/2012.01189"/>
        <updated>2021-07-26T02:00:58.588Z</updated>
        <summary type="html"><![CDATA[In this work, we analyze if it is possible to distinguish between different
clones of the same bacteria species (Klebsiella pneumoniae) based only on
microscopic images. It is a challenging task, previously considered impossible
due to the high clones similarity. For this purpose, we apply a multi-step
algorithm with attention-based multiple instance learning. Except for obtaining
accuracy at the level of 0.9, we introduce extensive interpretability based on
CellProfiler and persistence homology, increasing the understandability and
trust in the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borowa_A/0/1/0/all/0/1"&gt;Adriana Borowa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rymarczyk_D/0/1/0/all/0/1"&gt;Dawid Rymarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochonska_D/0/1/0/all/0/1"&gt;Dorota Ocho&amp;#x144;ska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brzychczy_Wloch_M/0/1/0/all/0/1"&gt;Monika Brzychczy-W&amp;#x142;och&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1"&gt;Bartosz Zieli&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An effective and friendly tool for seed image analysis. (arXiv:2103.17213v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17213</id>
        <link href="http://arxiv.org/abs/2103.17213"/>
        <updated>2021-07-26T02:00:58.581Z</updated>
        <summary type="html"><![CDATA[Image analysis is an essential field for several topics in the life sciences,
such as biology or botany. In particular, the analysis of seeds (e.g. fossil
research) can provide significant information on their evolution, the history
of agriculture, plant domestication and knowledge of diets in ancient times.
This work aims to present software that performs image analysis for feature
extraction and classification from images containing seeds through a novel and
unique framework. In detail, we propose two plugins \emph{ImageJ}, one able to
extract morphological, textual and colour features from seed images, and
another to classify seeds into categories using the extracted features. The
experimental results demonstrated the correctness and validity of both the
extracted features and the classification predictions. The proposed tool is
easily extendable to other fields of image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loddo_A/0/1/0/all/0/1"&gt;Andrea Loddo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruberto_C/0/1/0/all/0/1"&gt;Cecilia Di Ruberto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vale_A/0/1/0/all/0/1"&gt;A.M.P.G. Vale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ucchesu_M/0/1/0/all/0/1"&gt;Mariano Ucchesu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1"&gt;J.M. Soares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacchetta_G/0/1/0/all/0/1"&gt;Gianluigi Bacchetta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Model for Fingerprint Authentication and Presentation Attack Detection. (arXiv:2104.03255v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03255</id>
        <link href="http://arxiv.org/abs/2104.03255"/>
        <updated>2021-07-26T02:00:58.574Z</updated>
        <summary type="html"><![CDATA[Typical fingerprint recognition systems are comprised of a spoof detection
module and a subsequent recognition module, running one after the other. In
this paper, we reformulate the workings of a typical fingerprint recognition
system. In particular, we posit that both spoof detection and fingerprint
recognition are correlated tasks. Therefore, rather than performing the two
tasks separately, we propose a joint model for spoof detection and matching to
simultaneously perform both tasks without compromising the accuracy of either
task. We demonstrate the capability of our joint model to obtain an
authentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC
2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet
2015 dataset, both maintaining the performance of stand-alone methods. In
practice, this reduces the time and memory requirements of the fingerprint
recognition system by 50% and 40%, respectively; a significant advantage for
recognition systems running on resource-constrained devices and communication
channels. The project page for our work is available at
https://www.bit.ly/ijcb2021-unified .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popli_A/0/1/0/all/0/1"&gt;Additya Popli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tandon_S/0/1/0/all/0/1"&gt;Saraansh Tandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1"&gt;Joshua J. Engelsma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1"&gt;Naoyuki Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okubo_A/0/1/0/all/0/1"&gt;Atsushi Okubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_A/0/1/0/all/0/1"&gt;Anoop Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06069</id>
        <link href="http://arxiv.org/abs/2101.06069"/>
        <updated>2021-07-26T02:00:58.567Z</updated>
        <summary type="html"><![CDATA[Pretrained deep models hold their learnt knowledge in the form of model
parameters. These parameters act as "memory" for the trained models and help
them generalize well on unseen data. However, in absence of training data, the
utility of a trained model is merely limited to either inference or better
initialization towards a target task. In this paper, we go further and extract
synthetic data by leveraging the learnt model parameters. We dub them "Data
Impressions", which act as proxy to the training data and can be used to
realize a variety of tasks. These are useful in scenarios where only the
pretrained models are available and the training data is not shared (e.g., due
to privacy or sensitivity concerns). We show the applicability of data
impressions in solving several computer vision tasks such as unsupervised
domain adaptation, continual learning as well as knowledge distillation. We
also study the adversarial robustness of lightweight models trained via
knowledge distillation using these data impressions. Further, we demonstrate
the efficacy of data impressions in generating data-free Universal Adversarial
Perturbations (UAPs) with better fooling rates. Extensive experiments performed
on benchmark datasets demonstrate competitive performance achieved using data
impressions in absence of original training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1"&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1"&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saksham Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Anirban Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rigging the Lottery: Making All Tickets Winners. (arXiv:1911.11134v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.11134</id>
        <link href="http://arxiv.org/abs/1911.11134"/>
        <updated>2021-07-26T02:00:58.559Z</updated>
        <summary type="html"><![CDATA[Many applications require sparse neural networks due to space or inference
time restrictions. There is a large body of work on training dense networks to
yield sparse networks for inference, but this limits the size of the largest
trainable sparse model to that of the largest trainable dense model. In this
paper we introduce a method to train sparse neural networks with a fixed
parameter count and a fixed computational cost throughout training, without
sacrificing accuracy relative to existing dense-to-sparse training methods. Our
method updates the topology of the sparse network during training by using
parameter magnitudes and infrequent gradient calculations. We show that this
approach requires fewer floating-point operations (FLOPs) to achieve a given
level of accuracy compared to prior techniques. We demonstrate state-of-the-art
sparse training results on a variety of networks and datasets, including
ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we
provide some insights into why allowing the topology to change during the
optimization can overcome local minima encountered when the topology remains
static. Code used in our work can be found in github.com/google-research/rigl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1"&gt;Utku Evci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1"&gt;Trevor Gale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1"&gt;Jacob Menick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1"&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1"&gt;Erich Elsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filament Plots for Data Visualization. (arXiv:2107.10869v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.10869</id>
        <link href="http://arxiv.org/abs/2107.10869"/>
        <updated>2021-07-26T02:00:58.537Z</updated>
        <summary type="html"><![CDATA[We construct a computationally inexpensive 3D extension of Andrew's plots by
considering curves generated by Frenet-Serret equations and induced by
optimally smooth 2D Andrew's plots. We consider linear isometries from a
Euclidean data space to infinite dimensional spaces of 2D curves, and
parametrize the linear isometries that produce (on average) optimally smooth
curves over a given dataset. This set of optimal isometries admits many degrees
of freedom, and (using recent results on generalized Gauss sums) we identify a
particular a member of this set which admits an asymptotic projective "tour"
property. Finally, we consider the unit-length 3D curves (filaments) induced by
these 2D Andrew's plots, where the linear isometry property preserves distances
as "relative total square curvatures". This work concludes by illustrating
filament plots for several datasets. Code is available at
https://github.com/n8epi/filaments]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Strawn_N/0/1/0/all/0/1"&gt;Nate Strawn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time. (arXiv:2107.10868v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10868</id>
        <link href="http://arxiv.org/abs/2107.10868"/>
        <updated>2021-07-26T02:00:58.529Z</updated>
        <summary type="html"><![CDATA[In this paper we prove that Local (S)GD (or FedAvg) can optimize two-layer
neural networks with Rectified Linear Unit (ReLU) activation function in
polynomial time. Despite the established convergence theory of Local SGD on
optimizing general smooth functions in communication-efficient distributed
optimization, its convergence on non-smooth ReLU networks still eludes full
theoretical understanding. The key property used in many Local SGD analysis on
smooth function is gradient Lipschitzness, so that the gradient on local models
will not drift far away from that on averaged model. However, this decent
property does not hold in networks with non-smooth ReLU activation function. We
show that, even though ReLU network does not admit gradient Lipschitzness
property, the difference between gradients on local models and average model
will not change too much, under the dynamics of Local SGD. We validate our
theoretical results via extensive experiments. This work is the first to show
the convergence of Local SGD on non-smooth functions, and will shed lights on
the optimization theory of federated training of deep neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yuyang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1"&gt;Mehrdad Mahdavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Action Recognition from Various Data Modalities: A Review. (arXiv:2012.11866v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11866</id>
        <link href="http://arxiv.org/abs/2012.11866"/>
        <updated>2021-07-26T02:00:58.522Z</updated>
        <summary type="html"><![CDATA[Human Action Recognition (HAR) aims to understand human behavior and assign a
label to each action. It has a wide range of applications, and therefore has
been attracting increasing attention in the field of computer vision. Human
actions can be represented using various data modalities, such as RGB,
skeleton, depth, infrared, point cloud, event stream, audio, acceleration,
radar, and WiFi signal, which encode different sources of useful yet distinct
information and have various advantages depending on the application scenarios.
Consequently, lots of existing works have attempted to investigate different
types of approaches for HAR using various modalities. In this paper, we present
a comprehensive survey of recent progress in deep learning methods for HAR
based on the type of input data modality. Specifically, we review the current
mainstream deep learning methods for single data modalities and multiple data
modalities, including the fusion-based and the co-learning-based frameworks. We
also present comparative results on several benchmark datasets for HAR,
together with insightful observations and inspiring future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zehua Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1"&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1"&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate. (arXiv:2005.09525v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09525</id>
        <link href="http://arxiv.org/abs/2005.09525"/>
        <updated>2021-07-26T02:00:58.510Z</updated>
        <summary type="html"><![CDATA[In this work we present a multi-modal machine learning-based system, which we
call ACORN, to analyze videos of school classrooms for the Positive Climate
(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol
that is widely used in educational research. ACORN uses convolutional neural
networks to analyze spectral audio features, the faces of teachers and
students, and the pixels of each image frame, and then integrates this
information over time using Temporal Convolutional Networks. The audiovisual
ACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$
with ground-truth scores provided by expert CLASS coders on the UVA Toddler
dataset (cross-validation on $n=300$ 15-min video segments), and a purely
auditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the
MET dataset (test set of $n=2000$ videos segments). These numbers are similar
to inter-coder reliability of human coders. Finally, using Graph Convolutional
Networks we make early strides (AUC=$0.70$) toward predicting the specific
moments (45-90sec clips) when the PC is particularly weak/strong. Our findings
inform the design of automatic classroom observation and also more general
video activity recognition and summary recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1"&gt;Anand Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zylich_B/0/1/0/all/0/1"&gt;Brian Zylich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ottmar_E/0/1/0/all/0/1"&gt;Erin Ottmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1"&gt;Jennifer LoCasale-Crouch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1"&gt;Jacob Whitehill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15686</id>
        <link href="http://arxiv.org/abs/2106.15686"/>
        <updated>2021-07-26T02:00:58.502Z</updated>
        <summary type="html"><![CDATA[Morphed images have exploited loopholes in the face recognition checkpoints,
e.g., Credential Authentication Technology (CAT), used by Transportation
Security Administration (TSA), which is a non-trivial security concern. To
overcome the risks incurred due to morphed presentations, we propose a
wavelet-based morph detection methodology which adopts an end-to-end trainable
soft attention mechanism . Our attention-based deep neural network (DNN)
focuses on the salient Regions of Interest (ROI) which have the most spatial
support for morph detector decision function, i.e, morph class binary softmax
output. A retrospective of morph synthesizing procedure aids us to speculate
the ROI as regions around facial landmarks , particularly for the case of
landmark-based morphing techniques. Moreover, our attention-based DNN is
adapted to the wavelet space, where inputs of the network are coarse-to-fine
spectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate
performance of the proposed framework using three datasets, VISAPP17, LMA, and
MorGAN. In addition, as attention maps can be a robust indicator whether a
probe image under investigation is genuine or counterfeit, we analyze the
estimated attention maps for both a bona fide image and its corresponding
morphed image. Finally, we present an ablation study on the efficacy of
utilizing attention mechanism for the sake of morph detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1"&gt;Poorya Aghdaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1"&gt;Baaria Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handgun detection using combined human pose and weapon appearance. (arXiv:2010.13753v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13753</id>
        <link href="http://arxiv.org/abs/2010.13753"/>
        <updated>2021-07-26T02:00:58.481Z</updated>
        <summary type="html"><![CDATA[Closed-circuit television (CCTV) systems are essential nowadays to prevent
security threats or dangerous situations, in which early detection is crucial.
Novel deep learning-based methods have allowed to develop automatic weapon
detectors with promising results. However, these approaches are mainly based on
visual weapon appearance only. For handguns, body pose may be a useful cue,
especially in cases where the gun is barely visible. In this work, a novel
method is proposed to combine, in a single architecture, both weapon appearance
and human pose information. First, pose keypoints are estimated to extract hand
regions and generate binary pose images, which are the model inputs. Then, each
input is processed in different subnetworks and combined to produce the handgun
bounding box. Results obtained show that the combined model improves the
handgun detection state of the art, achieving from 4.23 to 18.9 AP points more
than the best previous approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Santaquiteria_J/0/1/0/all/0/1"&gt;Jesus Ruiz-Santaquiteria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velasco_Mata_A/0/1/0/all/0/1"&gt;Alberto Velasco-Mata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallez_N/0/1/0/all/0/1"&gt;Noelia Vallez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bueno_G/0/1/0/all/0/1"&gt;Gloria Bueno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_Garcia_J/0/1/0/all/0/1"&gt;Juan A. &amp;#xc1;lvarez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deniz_O/0/1/0/all/0/1"&gt;Oscar Deniz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DetCo: Unsupervised Contrastive Learning for Object Detection. (arXiv:2102.04803v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04803</id>
        <link href="http://arxiv.org/abs/2102.04803"/>
        <updated>2021-07-26T02:00:58.474Z</updated>
        <summary type="html"><![CDATA[Unsupervised contrastive learning achieves great success in learning image
representations with CNN. Unlike most recent methods that focused on improving
accuracy of image classification, we present a novel contrastive learning
approach, named DetCo, which fully explores the contrasts between global image
and local image patches to learn discriminative representations for object
detection. DetCo has several appealing benefits. (1) It is carefully designed
by investigating the weaknesses of current self-supervised methods, which
discard important representations for object detection. (2) DetCo builds
hierarchical intermediate contrastive losses between global image and local
patches to improve object detection, while maintaining global representations
for image recognition. Theoretical analysis shows that the local patches
actually remove the contextual information of an image, improving the lower
bound of mutual information for better contrastive learning. (3) Extensive
experiments on PASCAL VOC, COCO and Cityscapes demonstrate that DetCo not only
outperforms state-of-the-art methods on object detection, but also on
segmentation, pose estimation, and 3D shape prediction, while it is still
competitive on image classification. For example, on PASCAL VOC, DetCo-100ep
achieves 57.4 mAP, which is on par with the result of MoCov2-800ep. Moreover,
DetCo consistently outperforms supervised method by 1.6/1.2/1.0 AP on Mask
RCNN-C4/FPN/RetinaNet with 1x schedule. Code will be released at
\href{https://github.com/xieenze/DetCo}{\color{blue}{\tt
github.com/xieenze/DetCo}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jian Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xiaohang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Peize Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepLesionBrain: Towards a broader deep-learning generalization for multiple sclerosis lesion segmentation. (arXiv:2012.07950v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07950</id>
        <link href="http://arxiv.org/abs/2012.07950"/>
        <updated>2021-07-26T02:00:58.466Z</updated>
        <summary type="html"><![CDATA[Recently, segmentation methods based on Convolutional Neural Networks (CNNs)
showed promising performance in automatic Multiple Sclerosis (MS) lesions
segmentation. These techniques have even outperformed human experts in
controlled evaluation conditions such as Longitudinal MS Lesion Segmentation
Challenge (ISBI Challenge). However state-of-the-art approaches trained to
perform well on highly-controlled datasets fail to generalize on clinical data
from unseen datasets. Instead of proposing another improvement of the
segmentation accuracy, we propose a novel method robust to domain shift and
performing well on unseen datasets, called DeepLesionBrain (DLB). This
generalization property results from three main contributions. First, DLB is
based on a large group of compact 3D CNNs. This spatially distributed strategy
ensures a robust prediction despite the risk of generalization failure of some
individual networks. Second, DLB includes a new image quality data augmentation
to reduce dependency to training data specificity (e.g., acquisition protocol).
Finally, to learn a more generalizable representation of MS lesions, we propose
a hierarchical specialization learning (HSL). HSL is performed by pre-training
a generic network over the whole brain, before using its weights as
initialization to locally specialized networks. By this end, DLB learns both
generic features extracted at global image level and specific features
extracted at local image level. DLB generalization was validated in
cross-dataset experiments on MSSEG'16, ISBI challenge, and in-house datasets.
During experiments, DLB showed higher segmentation accuracy, better
segmentation consistency and greater generalization performance compared to
state-of-the-art methods. Therefore, DLB offers a robust framework well-suited
for clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kamraoui_R/0/1/0/all/0/1"&gt;Reda Abdellah Kamraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ta_V/0/1/0/all/0/1"&gt;Vinh-Thong Ta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tourdias_T/0/1/0/all/0/1"&gt;Thomas Tourdias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansencal_B/0/1/0/all/0/1"&gt;Boris Mansencal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manjon_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; V Manjon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1"&gt;Pierrick Coup&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FcaNet: Frequency Channel Attention Networks. (arXiv:2012.11879v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11879</id>
        <link href="http://arxiv.org/abs/2012.11879"/>
        <updated>2021-07-26T02:00:58.458Z</updated>
        <summary type="html"><![CDATA[Attention mechanism, especially channel attention, has gained great success
in the computer vision field. Many works focus on how to design efficient
channel attention mechanisms while ignoring a fundamental problem, i.e.,
channel attention mechanism uses scalar to represent channel, which is
difficult due to massive information loss. In this work, we start from a
different view and regard the channel representation problem as a compression
process using frequency analysis. Based on the frequency analysis, we
mathematically prove that the conventional global average pooling is a special
case of the feature decomposition in the frequency domain. With the proof, we
naturally generalize the compression of the channel attention mechanism in the
frequency domain and propose our method with multi-spectral channel attention,
termed as FcaNet. FcaNet is simple but effective. We can change a few lines of
code in the calculation to implement our method within existing channel
attention methods. Moreover, the proposed method achieves state-of-the-art
results compared with other channel attention methods on image classification,
object detection, and instance segmentation tasks. Our method could
consistently outperform the baseline SENet, with the same number of parameters
and the same computational cost. Our code and models will are publicly
available at https://github.com/cfzd/FcaNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zequn Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives. (arXiv:2107.11317v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11317</id>
        <link href="http://arxiv.org/abs/2107.11317"/>
        <updated>2021-07-26T02:00:58.448Z</updated>
        <summary type="html"><![CDATA[It is common practice to represent spoken languages at their phonetic level.
However, for sign languages, this implies breaking motion into its constituent
motion primitives. Avatar based Sign Language Production (SLP) has
traditionally done just this, building up animation from sequences of hand
motions, shapes and facial expressions. However, more recent deep learning
based solutions to SLP have tackled the problem using a single network that
estimates the full skeletal structure.

We propose splitting the SLP task into two distinct jointly-trained
sub-tasks. The first translation sub-task translates from spoken language to a
latent sign language representation, with gloss supervision. Subsequently, the
animation sub-task aims to produce expressive sign language sequences that
closely resemble the learnt spatio-temporal representation. Using a progressive
transformer for the translation sub-task, we propose a novel Mixture of Motion
Primitives (MoMP) architecture for sign language animation. A set of distinct
motion primitives are learnt during training, that can be temporally combined
at inference to animate continuous sign language sequences.

We evaluate on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T)
dataset, presenting extensive ablation studies and showing that MoMP
outperforms baselines in user evaluations. We achieve state-of-the-art back
translation performance with an 11% improvement over competing results.
Importantly, and for the first time, we showcase stronger performance for a
full translation pipeline going from spoken language to sign, than from gloss
to sign.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1"&gt;Ben Saunders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1"&gt;Necati Cihan Camgoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?. (arXiv:2105.02498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02498</id>
        <link href="http://arxiv.org/abs/2105.02498"/>
        <updated>2021-07-26T02:00:58.427Z</updated>
        <summary type="html"><![CDATA[Global covariance pooling (GCP) aims at exploiting the second-order
statistics of the convolutional feature. Its effectiveness has been
demonstrated in boosting the classification performance of Convolutional Neural
Networks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute
the matrix square root. However, the approximate matrix square root calculated
using Newton-Schulz iteration \cite{li2018towards} outperforms the accurate one
computed via SVD \cite{li2017second}. We empirically analyze the reason behind
the performance gap from the perspectives of data precision and gradient
smoothness. Various remedies for computing smooth SVD gradients are
investigated. Based on our observation and analyses, a hybrid training protocol
is proposed for SVD-based GCP meta-layers such that competitive performances
can be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP
meta-layer that uses SVD in the forward pass, and Pad\'e Approximants in the
backward propagation to compute the gradients. The proposed meta-layer has been
integrated into different CNN models and achieves state-of-the-art performances
on both large-scale and fine-grained datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yue Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning A Single Network for Scale-Arbitrary Super-Resolution. (arXiv:2004.03791v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03791</id>
        <link href="http://arxiv.org/abs/2004.03791"/>
        <updated>2021-07-26T02:00:58.420Z</updated>
        <summary type="html"><![CDATA[Recently, the performance of single image super-resolution (SR) has been
significantly improved with powerful networks. However, these networks are
developed for image SR with a single specific integer scale (e.g., x2;x3,x4),
and cannot be used for non-integer and asymmetric SR. In this paper, we propose
to learn a scale-arbitrary image SR network from scale-specific networks.
Specifically, we propose a plug-in module for existing SR networks to perform
scale-arbitrary SR, which consists of multiple scale-aware feature adaption
blocks and a scale-aware upsampling layer. Moreover, we introduce a scale-aware
knowledge transfer paradigm to transfer knowledge from scale-specific networks
to the scale-arbitrary network. Our plug-in module can be easily adapted to
existing networks to achieve scale-arbitrary SR. These networks plugged with
our module can achieve promising results for non-integer and asymmetric SR
while maintaining state-of-the-art performance for SR with integer scale
factors. Besides, the additional computational and memory cost of our module is
very small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zaiping Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jungang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1"&gt;Wei An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yulan Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency. (arXiv:2107.11355v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11355</id>
        <link href="http://arxiv.org/abs/2107.11355"/>
        <updated>2021-07-26T02:00:58.412Z</updated>
        <summary type="html"><![CDATA[Deep learning-based 3D object detection has achieved unprecedented success
with the advent of large-scale autonomous driving datasets. However, drastic
performance degradation remains a critical challenge for cross-domain
deployment. In addition, existing 3D domain adaptive detection methods often
assume prior access to the target domain annotations, which is rarely feasible
in the real world. To address this challenge, we study a more realistic
setting, unsupervised 3D domain adaptive detection, which only utilizes source
domain annotations. 1) We first comprehensively investigate the major
underlying factors of the domain gap in 3D detection. Our key insight is that
geometric mismatch is the key factor of domain shift. 2) Then, we propose a
novel and unified framework, Multi-Level Consistency Network (MLC-Net), which
employs a teacher-student paradigm to generate adaptive and reliable
pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level
consistency to facilitate cross-domain transfer. Extensive experiments
demonstrate that MLC-Net outperforms existing state-of-the-art methods
(including those using additional target domain information) on standard
benchmarks. Notably, our approach is detector-agnostic, which achieves
consistent gains on both single- and two-stage 3D detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhongang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Changqing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gongjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Supervision Propagation for Weakly Supervised Semantic Segmentation on 3D Point Clouds. (arXiv:2107.11267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11267</id>
        <link href="http://arxiv.org/abs/2107.11267"/>
        <updated>2021-07-26T02:00:58.405Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation on 3D point clouds is an important task for 3D scene
understanding. While dense labeling on 3D data is expensive and time-consuming,
only a few works address weakly supervised semantic point cloud segmentation
methods to relieve the labeling cost by learning from simpler and cheaper
labels. Meanwhile, there are still huge performance gaps between existing
weakly supervised methods and state-of-the-art fully supervised methods. In
this paper, we train a semantic point cloud segmentation network with only a
small portion of points being labeled. We argue that we can better utilize the
limited supervision information as we densely propagate the supervision signal
from the labeled points to other points within and across the input samples.
Specifically, we propose a cross-sample feature reallocating module to transfer
similar features and therefore re-route the gradients across two samples with
common classes and an intra-sample feature redistribution module to propagate
supervision signals on unlabeled points across and within point cloud samples.
We conduct extensive experiments on public datasets S3DIS and ScanNet. Our
weakly supervised method with only 10\% and 1\% of labels can produce
compatible results with the fully supervised counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiacheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1"&gt;Kim-Hui Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fayao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1"&gt;Tzu-Yi Hung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04726</id>
        <link href="http://arxiv.org/abs/2106.04726"/>
        <updated>2021-07-26T02:00:58.398Z</updated>
        <summary type="html"><![CDATA[There is currently no easy way to fact-check content on WhatsApp and other
end-to-end encrypted platforms at scale. In this paper, we analyze the
usefulness of a crowd-sourced "tipline" through which users can submit content
("tips") that they want fact-checked. We compare the tips sent to a WhatsApp
tipline run during the 2019 Indian national elections with the messages
circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, our
findings suggest tiplines can be an effective source for discovering content to
fact-check.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1"&gt;Ashkan Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1"&gt;Kiran Garimella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1"&gt;Gautam Kishore Shahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1"&gt;Devin Gaffney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1"&gt;Scott A. Hale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing bikeability with street view imagery and computer vision. (arXiv:2105.08499v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08499</id>
        <link href="http://arxiv.org/abs/2105.08499"/>
        <updated>2021-07-26T02:00:58.376Z</updated>
        <summary type="html"><![CDATA[Studies evaluating bikeability usually compute spatial indicators shaping
cycling conditions and conflate them in a quantitative index. Much research
involves site visits or conventional geospatial approaches, and few studies
have leveraged street view imagery (SVI) for conducting virtual audits. These
have assessed a limited range of aspects, and not all have been automated using
computer vision (CV). Furthermore, studies have not yet zeroed in on gauging
the usability of these technologies thoroughly. We investigate, with
experiments at a fine spatial scale and across multiple geographies (Singapore
and Tokyo), whether we can use SVI and CV to assess bikeability
comprehensively. Extending related work, we develop an exhaustive index of
bikeability composed of 34 indicators. The results suggest that SVI and CV are
adequate to evaluate bikeability in cities comprehensively. As they
outperformed non-SVI counterparts by a wide margin, SVI indicators are also
found to be superior in assessing urban bikeability, and potentially can be
used independently, replacing traditional techniques. However, the paper
exposes some limitations, suggesting that the best way forward is combining
both SVI and non-SVI approaches. The new bikeability index presents a
contribution in transportation and urban analytics, and it is scalable to
assess cycling appeal widely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ito_K/0/1/0/all/0/1"&gt;Koichi Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11291</id>
        <link href="http://arxiv.org/abs/2107.11291"/>
        <updated>2021-07-26T02:00:58.368Z</updated>
        <summary type="html"><![CDATA[Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
https://github.com/Jeff-sjtu/res-loglikelihood-regression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1"&gt;Siyuan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurfaceNet: Adversarial SVBRDF Estimation from a Single Image. (arXiv:2107.11298v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11298</id>
        <link href="http://arxiv.org/abs/2107.11298"/>
        <updated>2021-07-26T02:00:58.361Z</updated>
        <summary type="html"><![CDATA[In this paper we present SurfaceNet, an approach for estimating
spatially-varying bidirectional reflectance distribution function (SVBRDF)
material properties from a single image. We pose the problem as an image
translation task and propose a novel patch-based generative adversarial network
(GAN) that is able to produce high-quality, high-resolution surface reflectance
maps. The employment of the GAN paradigm has a twofold objective: 1) allowing
the model to recover finer details than standard translation models; 2)
reducing the domain shift between synthetic and real data distributions in an
unsupervised way. An extensive evaluation, carried out on a public benchmark of
synthetic and real images under different illumination conditions, shows that
SurfaceNet largely outperforms existing SVBRDF reconstruction methods, both
quantitatively and qualitatively. Furthermore, SurfaceNet exhibits a remarkable
ability in generating high-quality maps from real samples without any
supervision at training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vecchio_G/0/1/0/all/0/1"&gt;Giuseppe Vecchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1"&gt;Simone Palazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1"&gt;Concetto Spampinato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Quantitative Assessment of COVID-19 Infection Progression from Chest CTs. (arXiv:2103.07240v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07240</id>
        <link href="http://arxiv.org/abs/2103.07240"/>
        <updated>2021-07-26T02:00:58.352Z</updated>
        <summary type="html"><![CDATA[Chest computed tomography (CT) has played an essential diagnostic role in
assessing patients with COVID-19 by showing disease-specific image features
such as ground-glass opacity and consolidation. Image segmentation methods have
proven to help quantify the disease burden and even help predict the outcome.
The availability of longitudinal CT series may also result in an efficient and
effective method to reliably assess the progression of COVID-19, monitor the
healing process and the response to different therapeutic strategies. In this
paper, we propose a new framework to identify infection at a voxel level
(identification of healthy lung, consolidation, and ground-glass opacity) and
visualize the progression of COVID-19 using sequential low-dose non-contrast CT
scans. In particular, we devise a longitudinal segmentation network that
utilizes the reference scan information to improve the performance of disease
identification. Experimental results on a clinical longitudinal dataset
collected in our institution show the effectiveness of the proposed method
compared to the static deep neural networks for disease quantification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goli_L/0/1/0/all/0/1"&gt;Leili Goli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Paschali_M/0/1/0/all/0/1"&gt;Magdalini Paschali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keicher_M/0/1/0/all/0/1"&gt;Matthias Keicher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Czempiel_T/0/1/0/all/0/1"&gt;Tobias Czempiel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burian_E/0/1/0/all/0/1"&gt;Egon Burian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wendler_T/0/1/0/all/0/1"&gt;Thomas Wendler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13000</id>
        <link href="http://arxiv.org/abs/2011.13000"/>
        <updated>2021-07-26T02:00:58.344Z</updated>
        <summary type="html"><![CDATA[Precision scaling has emerged as a popular technique to optimize the compute
and storage requirements of Deep Neural Networks (DNNs). Efforts toward
creating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum
precision required to achieve a given network-level accuracy varies
considerably across networks, and even across layers within a network,
requiring support for variable precision in DNN hardware. Previous proposals
such as bit-serial hardware incur high overheads, significantly diminishing the
benefits of lower precision. To efficiently support precision
re-configurability in DNN accelerators, we introduce an approximate computing
method wherein DNN computations are performed block-wise (a block is a group of
bits) and re-configurability is supported at the granularity of blocks. Results
of block-wise computations are composed in an approximate manner to enable
efficient re-configurability. We design a DNN accelerator that embodies
approximate blocked computation and propose a method to determine a suitable
approximation configuration for a given DNN. By varying the approximation
configurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement
in system energy and performance respectively, over an 8-bit fixed-point (FxP8)
baseline, with negligible loss in classification accuracy. Further, by varying
the approximation configurations across layers and data-structures within DNNs,
we achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and
performance respectively, with negligible accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1"&gt;Reena Elangovan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Shubham Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1"&gt;Anand Raghunathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11264</id>
        <link href="http://arxiv.org/abs/2107.11264"/>
        <updated>2021-07-26T02:00:58.313Z</updated>
        <summary type="html"><![CDATA[Identifying unexpected objects on roads in semantic segmentation (e.g.,
identifying dogs on roads) is crucial in safety-critical applications. Existing
approaches use images of unexpected objects from external datasets or require
additional training (e.g., retraining segmentation networks or training an
extra network), which necessitate a non-trivial amount of labor intensity or
lengthy inference time. One possible alternative is to use prediction scores of
a pre-trained network such as the max logits (i.e., maximum values among
classes before the final softmax layer) for detecting such objects. However,
the distribution of max logits of each predicted class is significantly
different from each other, which degrades the performance of identifying
unexpected objects in urban-scene segmentation. To address this issue, we
propose a simple yet effective approach that standardizes the max logits in
order to align the different distributions and reflect the relative meanings of
max logits within each predicted class. Moreover, we consider the local regions
from two different perspectives based on the intuition that neighboring pixels
share similar semantic information. In contrast to previous approaches, our
method does not utilize any external datasets or require additional training,
which makes our method widely applicable to existing pre-trained segmentation
models. Such a straightforward approach achieves a new state-of-the-art
performance on the publicly available Fishyscapes Lost & Found leaderboard with
a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Sanghun Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungsoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1"&gt;Daehoon Gwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Sungha Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11320</id>
        <link href="http://arxiv.org/abs/2107.11320"/>
        <updated>2021-07-26T02:00:58.280Z</updated>
        <summary type="html"><![CDATA[Forest carbon offsets are increasingly popular and can play a significant
role in financing climate mitigation, forest conservation, and reforestation.
Measuring how much carbon is stored in forests is, however, still largely done
via expensive, time-consuming, and sometimes unaccountable field measurements.
To overcome these limitations, many verification bodies are leveraging machine
learning (ML) algorithms to estimate forest carbon from satellite or aerial
imagery. Aerial imagery allows for tree species or family classification, which
improves the satellite imagery-based forest type classification. However,
aerial imagery is significantly more expensive to collect and it is unclear by
how much the higher resolution improves the forest carbon estimation. This
proposal paper describes the first systematic comparison of forest carbon
estimation from aerial imagery, satellite imagery, and ground-truth field
measurements via deep learning-based algorithms for a tropical reforestation
project. Our initial results show that forest carbon estimates from satellite
imagery can overestimate above-ground biomass by more than 10-times for
tropical reforestation projects. The significant difference between aerial and
satellite-derived forest carbon measurements shows the potential for aerial
imagery-based ML algorithms and raises the importance to extend this study to a
global benchmark between options for carbon measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1"&gt;Gyri Reiersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1"&gt;David Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1"&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provident Vehicle Detection at Night for Advanced Driver Assistance Systems. (arXiv:2107.11302v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11302</id>
        <link href="http://arxiv.org/abs/2107.11302"/>
        <updated>2021-07-26T02:00:58.272Z</updated>
        <summary type="html"><![CDATA[In recent years, computer vision algorithms have become more and more
powerful, which enabled technologies such as autonomous driving to evolve with
rapid pace. However, current algorithms mainly share one limitation: They rely
on directly visible objects. This is a major drawback compared to human
behavior, where indirect visual cues caused by the actual object (e.g.,
shadows) are already used intuitively to retrieve information or anticipate
occurring objects. While driving at night, this performance deficit becomes
even more obvious: Humans already process the light artifacts caused by
oncoming vehicles to assume their future appearance, whereas current object
detection systems rely on the oncoming vehicle's direct visibility. Based on
previous work in this subject, we present with this paper a complete system
capable of solving the task to providently detect oncoming vehicles at
nighttime based on their caused light artifacts. For that, we outline the full
algorithm architecture ranging from the detection of light artifacts in the
image space, localizing the objects in the three-dimensional space, and
verifying the objects over time. To demonstrate the applicability, we deploy
the system in a test vehicle and use the information of providently detected
vehicles to control the glare-free high beam system proactively. Using this
experimental setting, we quantify the time benefit that the provident vehicle
detection system provides compared to an in-production computer vision system.
Additionally, the glare-free high beam use case provides a real-time and
real-world visualization interface of the detection results. With this
contribution, we want to put awareness on the unconventional sensing task of
provident object detection and further close the performance gap between human
behavior and computer vision algorithms in order to bring autonomous and
automated driving a step forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1"&gt;Lukas Ewecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1"&gt;Ebubekir Asan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1"&gt;Lars Ohnemus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1"&gt;Sascha Saralajew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation. (arXiv:2107.11279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11279</id>
        <link href="http://arxiv.org/abs/2107.11279"/>
        <updated>2021-07-26T02:00:58.244Z</updated>
        <summary type="html"><![CDATA[While self-training has advanced semi-supervised semantic segmentation, it
severely suffers from the long-tailed class distribution on real-world semantic
segmentation datasets that make the pseudo-labeled data bias toward majority
classes. In this paper, we present a simple and yet effective Distribution
Alignment and Random Sampling (DARS) method to produce unbiased pseudo labels
that match the true class distribution estimated from the labeled data.
Besides, we also contribute a progressive data augmentation and labeling
strategy to facilitate model training with pseudo-labeled data. Experiments on
both Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of
our approach. Albeit simple, our method performs favorably in comparison with
state-of-the-art approaches. Code will be available at
https://github.com/CVMI-Lab/DARS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ruifei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiaojuan Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-to-Image Translation with Low Resolution Conditioning. (arXiv:2107.11262v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11262</id>
        <link href="http://arxiv.org/abs/2107.11262"/>
        <updated>2021-07-26T02:00:58.233Z</updated>
        <summary type="html"><![CDATA[Most image-to-image translation methods focus on learning mappings across
domains with the assumption that images share content (e.g., pose) but have
their own domain-specific information known as style. When conditioned on a
target image, such methods aim to extract the style of the target and combine
it with the content of the source image. In this work, we consider the scenario
where the target image has a very low resolution. More specifically, our
approach aims at transferring fine details from a high resolution (HR) source
image to fit a coarse, low resolution (LR) image representation of the target.
We therefore generate HR images that share features from both HR and LR inputs.
This differs from previous methods that focus on translating a given image
style into a target content, our translation approach being able to
simultaneously imitate the style and merge the structural information of the LR
target. Our approach relies on training the generative model to produce HR
target images that both 1) share distinctive information of the associated
source image; 2) correctly match the LR target image when downscaled. We
validate our method on the CelebA-HQ and AFHQ datasets by demonstrating
improvements in terms of visual quality, diversity and coverage. Qualitative
and quantitative results show that when dealing with intra-domain image
translation, our method generates more realistic samples compared to
state-of-the-art methods such as Stargan-v2]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1"&gt;Mohamed Abderrahmen Abid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedhli_I/0/1/0/all/0/1"&gt;Ihsen Hedhli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1"&gt;Jean-Fran&amp;#xe7;ois Lalonde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1"&gt;Christian Gagne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (arXiv:2107.11252v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11252</id>
        <link href="http://arxiv.org/abs/2107.11252"/>
        <updated>2021-07-26T02:00:58.226Z</updated>
        <summary type="html"><![CDATA[Language instruction plays an essential role in the natural language grounded
navigation tasks. However, navigators trained with limited human-annotated
instructions may have difficulties in accurately capturing key information from
the complicated instruction at different timesteps, leading to poor navigation
performance. In this paper, we exploit to train a more robust navigator which
is capable of dynamically extracting crucial factors from the long instruction,
by using an adversarial attacking paradigm. Specifically, we propose a Dynamic
Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the
navigator to move to the wrong target by destroying the most instructive
information in instructions at different timesteps. By formulating the
perturbation generation as a Markov Decision Process, DR-Attacker is optimized
by the reinforcement learning algorithm to generate perturbed instructions
sequentially during the navigation, according to a learnable attack score.
Then, the perturbed instructions, which serve as hard samples, are used for
improving the robustness of the navigator with an effective adversarial
training strategy and an auxiliary self-supervised reasoning task. Experimental
results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog
History (NDH) tasks show the superiority of our proposed method over
state-of-the-art methods. Moreover, the visualization analysis shows the
effectiveness of the proposed DR-Attacker, which can successfully attack
crucial information in the instructions at different timesteps. Code is
available at https://github.com/expectorlin/DR-Attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bingqian Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yanxin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11214</id>
        <link href="http://arxiv.org/abs/2107.11214"/>
        <updated>2021-07-26T02:00:58.205Z</updated>
        <summary type="html"><![CDATA[We propose the adjacency adaptive graph convolutional long-short term memory
network (AAGC-LSTM) for human pose estimation from sparse inertial
measurements, obtained from only 6 measurement units. The AAGC-LSTM combines
both spatial and temporal dependency in a single network operation. This is
made possible by equipping graph convolutions with adjacency adaptivity, which
also allows for learning unknown dependencies of the human body joints. To
further boost accuracy, we propose longitudinal loss weighting to consider
natural movement patterns, as well as body-aware contralateral data
augmentation. By combining these contributions, we are able to utilize the
inherent graph nature of the human body, and can thus outperform the state of
the art for human pose estimation from sparse inertial measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1"&gt;Patrik Puchert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1"&gt;Timo Ropinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Deep Registration Latent Spaces. (arXiv:2107.11238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11238</id>
        <link href="http://arxiv.org/abs/2107.11238"/>
        <updated>2021-07-26T02:00:58.182Z</updated>
        <summary type="html"><![CDATA[Explainability of deep neural networks is one of the most challenging and
interesting problems in the field. In this study, we investigate the topic
focusing on the interpretability of deep learning-based registration methods.
In particular, with the appropriate model architecture and using a simple
linear projection, we decompose the encoding space, generating a new basis, and
we empirically show that this basis captures various decomposed anatomically
aware geometrical transformations. We perform experiments using two different
datasets focusing on lungs and hippocampus MRI. We show that such an approach
can decompose the highly convoluted latent spaces of registration pipelines in
an orthogonal space with several interesting properties. We hope that this work
could shed some light on a better understanding of deep learning-based
registration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Estienne_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Estienne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1"&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1"&gt;Stergios Christodoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battistella_E/0/1/0/all/0/1"&gt;Enzo Battistella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;ophraste Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1"&gt;Marvin Lerousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1"&gt;Amaury Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1"&gt;Guillaume Chassagnon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1"&gt;Marie-Pierre Revel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1"&gt;Nikos Paragios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1"&gt;Eric Deutsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Pedestrian Detection with Large Misalignment Based on Modal-Wise Regression and Multi-Modal IoU. (arXiv:2107.11196v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11196</id>
        <link href="http://arxiv.org/abs/2107.11196"/>
        <updated>2021-07-26T02:00:58.174Z</updated>
        <summary type="html"><![CDATA[The combined use of multiple modalities enables accurate pedestrian detection
under poor lighting conditions by using the high visibility areas from these
modalities together. The vital assumption for the combination use is that there
is no or only a weak misalignment between the two modalities. In general,
however, this assumption often breaks in actual situations. Due to this
assumption's breakdown, the position of the bounding boxes does not match
between the two modalities, resulting in a significant decrease in detection
accuracy, especially in regions where the amount of misalignment is large. In
this paper, we propose a multi-modal Faster-RCNN that is robust against large
misalignment. The keys are 1) modal-wise regression and 2) multi-modal IoU for
mini-batch sampling. To deal with large misalignment, we perform bounding box
regression for both the RPN and detection-head with both modalities. We also
propose a new sampling strategy called "multi-modal mini-batch sampling" that
integrates the IoU for both modalities. We demonstrate that the proposed
method's performance is much better than that of the state-of-the-art methods
for data with large misalignment through actual image experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wanchaitanawong_N/0/1/0/all/0/1"&gt;Napat Wanchaitanawong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1"&gt;Masayuki Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1"&gt;Takashi Shibata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1"&gt;Masatoshi Okutomi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LARGE: Latent-Based Regression through GAN Semantics. (arXiv:2107.11186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11186</id>
        <link href="http://arxiv.org/abs/2107.11186"/>
        <updated>2021-07-26T02:00:58.157Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for solving regression tasks using few-shot or weak
supervision. At the core of our method is the fundamental observation that GANs
are incredibly successful at encoding semantic information within their latent
space, even in a completely unsupervised setting. For modern generative
frameworks, this semantic encoding manifests as smooth, linear directions which
affect image attributes in a disentangled manner. These directions have been
widely used in GAN-based image editing. We show that such directions are not
only linear, but that the magnitude of change induced on the respective
attribute is approximately linear with respect to the distance traveled along
them. By leveraging this observation, our method turns a pre-trained GAN into a
regression model, using as few as two labeled samples. This enables solving
regression tasks on datasets and attributes which are difficult to produce
quality supervision for. Additionally, we show that the same latent-distances
can be used to sort collections of images by the strength of given attributes,
even in the absence of explicit supervision. Extensive experimental evaluations
demonstrate that our method can be applied across a wide range of domains,
leverage multiple latent direction discovery frameworks, and achieve
state-of-the-art results in few-shot and low-supervision settings, even when
compared to methods designed to tackle a single task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1"&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1"&gt;Rinon Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_O/0/1/0/all/0/1"&gt;Ofir Brenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11191</id>
        <link href="http://arxiv.org/abs/2107.11191"/>
        <updated>2021-07-26T02:00:58.149Z</updated>
        <summary type="html"><![CDATA[Deep neural network approaches to inverse imaging problems have produced
impressive results in the last few years. In this paper, we consider the use of
generative models in a variational regularisation approach to inverse problems.
The considered regularisers penalise images that are far from the range of a
generative model that has learned to produce images similar to a training
dataset. We name this family \textit{generative regularisers}. The success of
generative regularisers depends on the quality of the generative model and so
we propose a set of desired criteria to assess models and guide future
research. In our numerical experiments, we evaluate three common generative
models, autoencoders, variational autoencoders and generative adversarial
networks, against our desired criteria. We also test three different generative
regularisers on the inverse problems of deblurring, deconvolution, and
tomography. We show that the success of solutions restricted to lie exactly in
the range of the generator is highly dependent on the ability of the generative
model but that allowing small deviations from the range of the generator
produces more consistent results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1"&gt;Margaret Duff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1"&gt;Neill D. F. Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1"&gt;Matthias J. Ehrhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing efficient transfer learning strategies for robust scene recognition in mobile robotics using pre-trained convolutional neural networks. (arXiv:2107.11187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11187</id>
        <link href="http://arxiv.org/abs/2107.11187"/>
        <updated>2021-07-26T02:00:58.141Z</updated>
        <summary type="html"><![CDATA[We present four different robust transfer learning and data augmentation
strategies for robust mobile scene recognition. By training three mobile-ready
(EfficientNetB0, MobileNetV2, MobileNetV3) and two large-scale baseline (VGG16,
ResNet50) convolutional neural network architectures on the widely available
Event8, Scene15, Stanford40, and MIT67 datasets, we show the generalization
ability of our transfer learning strategies. Furthermore, we tested the
robustness of our transfer learning strategies under viewpoint and lighting
changes using the KTH-Idol2 database. Also, the impact of inference
optimization techniques on the general performance and the robustness under
different transfer learning strategies is evaluated. Experimental results show
that when employing transfer learning, Fine-Tuning in combination with
extensive data augmentation improves the general accuracy and robustness in
mobile scene recognition. We achieved state-of-the-art results using various
baseline convolutional neural networks and showed the robustness against
lighting and viewpoint changes in challenging mobile robot place recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baumgartl_H/0/1/0/all/0/1"&gt;Hermann Baumgartl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buettner_R/0/1/0/all/0/1"&gt;Ricardo Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11170</id>
        <link href="http://arxiv.org/abs/2107.11170"/>
        <updated>2021-07-26T02:00:58.129Z</updated>
        <summary type="html"><![CDATA[Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1"&gt;Lusine Abrahamyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1"&gt;Valentin Ziatchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1"&gt;Nikos Deligiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.11099</id>
        <link href="http://arxiv.org/abs/2107.11099"/>
        <updated>2021-07-26T02:00:58.113Z</updated>
        <summary type="html"><![CDATA[With the rapid growth of qubit numbers and coherence times in quantum
hardware technology, implementing shallow neural networks on the so-called
Noisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of
interest. Many quantum (convolutional) circuit ansaetze are proposed for
grayscale images classification tasks with promising empirical results.
However, when applying these ansaetze on RGB images, the intra-channel
information that is useful for vision tasks is not extracted effectively. In
this paper, we propose two types of quantum circuit ansaetze to simulate
convolution operations on RGB images, which differ in the way how inter-channel
and intra-channel information are extracted. To the best of our knowledge, this
is the first work of a quantum convolutional circuit to deal with RGB images
effectively, with a higher test accuracy compared to the purely classical CNNs.
We also investigate the relationship between the size of quantum circuit ansatz
and the learnability of the hybrid quantum-classical convolutional neural
network. Through experiments based on CIFAR-10 and MNIST datasets, we
demonstrate that a larger size of the quantum circuit ansatz improves
predictive performance in multiclass classification tasks, providing useful
insights for near term quantum algorithm developments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1"&gt;Yu Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chonghang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1"&gt;Wenbing Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaogang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unrealistic Feature Suppression for Generative Adversarial Networks. (arXiv:2107.11047v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11047</id>
        <link href="http://arxiv.org/abs/2107.11047"/>
        <updated>2021-07-26T02:00:58.106Z</updated>
        <summary type="html"><![CDATA[Due to the unstable nature of minimax game between generator and
discriminator, improving the performance of GANs is a challenging task. Recent
studies have shown that selected high-quality samples in training improve the
performance of GANs. However, sampling approaches which discard samples show
limitations in some aspects such as the speed of training and optimality of the
networks. In this paper we propose unrealistic feature suppression (UFS) module
that keeps high-quality features and suppresses unrealistic features. UFS
module keeps the training stability of networks and improves the quality of
generated images. We demonstrate the effectiveness of UFS module on various
models such as WGAN-GP, SNGAN, and BigGAN. By using UFS module, we achieved
better Frechet inception distance and inception score compared to various
baseline models. We also visualize how effectively our UFS module suppresses
unrealistic features through class activation maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sanghun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;SeungKyu Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WaveFill: A Wavelet-based Generation Network for Image Inpainting. (arXiv:2107.11027v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11027</id>
        <link href="http://arxiv.org/abs/2107.11027"/>
        <updated>2021-07-26T02:00:58.098Z</updated>
        <summary type="html"><![CDATA[Image inpainting aims to complete the missing or corrupted regions of images
with realistic contents. The prevalent approaches adopt a hybrid objective of
reconstruction and perceptual quality by using generative adversarial networks.
However, the reconstruction loss and adversarial loss focus on synthesizing
contents of different frequencies and simply applying them together often leads
to inter-frequency conflicts and compromised inpainting. This paper presents
WaveFill, a wavelet-based inpainting network that decomposes images into
multiple frequency bands and fills the missing regions in each frequency band
separately and explicitly. WaveFill decomposes images by using discrete wavelet
transform (DWT) that preserves spatial information naturally. It applies L1
reconstruction loss to the decomposed low-frequency bands and adversarial loss
to high-frequency bands, hence effectively mitigate inter-frequency conflicts
while completing images in spatial domain. To address the inpainting
inconsistency in different frequency bands and fuse features with distinct
statistics, we design a novel normalization scheme that aligns and fuses the
multi-frequency features effectively. Extensive experiments over multiple
datasets show that WaveFill achieves superior image inpainting qualitatively
and quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yingchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jianxiong Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Feiying Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Discriminative Representations for Multi-Label Image Recognition. (arXiv:2107.11159v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11159</id>
        <link href="http://arxiv.org/abs/2107.11159"/>
        <updated>2021-07-26T02:00:58.091Z</updated>
        <summary type="html"><![CDATA[Multi-label recognition is a fundamental, and yet is a challenging task in
computer vision. Recently, deep learning models have achieved great progress
towards learning discriminative features from input images. However,
conventional approaches are unable to model the inter-class discrepancies among
features in multi-label images, since they are designed to work for image-level
feature discrimination. In this paper, we propose a unified deep network to
learn discriminative features for the multi-label task. Given a multi-label
image, the proposed method first disentangles features corresponding to
different classes. Then, it discriminates between these classes via increasing
the inter-class distance while decreasing the intra-class differences in the
output space. By regularizing the whole network with the proposed loss, the
performance of applying the wellknown ResNet-101 is improved significantly.
Extensive experiments have been performed on COCO-2014, VOC2007 and VOC2012
datasets, which demonstrate that the proposed method outperforms
state-of-the-art approaches by a significant margin of 3:5% on large-scale COCO
dataset. Moreover, analysis of the discriminative feature learning approach
shows that it can be plugged into various types of multi-label methods as a
general module.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1"&gt;Mohammed Hassanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1"&gt;Ibrahim Radwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tahtali_M/0/1/0/all/0/1"&gt;Murat Tahtali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11052</id>
        <link href="http://arxiv.org/abs/2107.11052"/>
        <updated>2021-07-26T02:00:58.057Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation for semantic segmentation has gained immense
popularity since it can transfer knowledge from simulation to real (Sim2Real)
by largely cutting out the laborious per pixel labeling efforts at real. In
this work, we present a new video extension of this task, namely Unsupervised
Domain Adaptation for Video Semantic Segmentation. As it became easy to obtain
large-scale video labels through simulation, we believe attempting to maximize
Sim2Real knowledge transferability is one of the promising directions for
resolving the fundamental data-hungry issue in the video. To tackle this new
problem, we present a novel two-phase adaptation scheme. In the first step, we
exhaustively distill source domain knowledge using supervised loss functions.
Simultaneously, video adversarial training (VAT) is employed to align the
features from source to target utilizing video context. In the second step, we
apply video self-training (VST), focusing only on the target data. To construct
robust pseudo labels, we exploit the temporal information in the video, which
has been rarely explored in the previous image-based self-training approaches.
We set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.
We show that our proposals significantly outperform previous image-based UDA
methods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1"&gt;Inkyu Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kwanyong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sanghyun Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1"&gt;In So Kweon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cardiac CT segmentation based on distance regularized level set. (arXiv:2107.11119v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11119</id>
        <link href="http://arxiv.org/abs/2107.11119"/>
        <updated>2021-07-26T02:00:58.039Z</updated>
        <summary type="html"><![CDATA[Before analy z ing the CT image, it is very important to segment the heart
image, and the left ve ntricular (LV) inner and outer membrane segmentation is
one of the most important contents. However, manual segmentation is tedious and
time consuming. In order to facilitate doctors to focus on high tech tasks such
as disease analysis and diagnosis, it is crucial to develop a fast and accurate
segmentation method [1]. In view of this phenomenon, this paper uses distance
regularized level set (DRL SE) to explore the segmentation effect of epicardium
and endocardium 2 ]], which includes a distance regula riz ed t erm and an
external energy term. Finally, five CT images are used to verify the proposed
method, and image quality evaluation indexes such as dice score and Hausdorff
distance are used to evaluate the segmentation effect. The results showed that
the me tho d could separate the inner and outer membrane very well (endocardium
dice = 0.9253, Hausdorff = 7.8740; epicardium Hausdorff = 0.9687, Hausdorff = 6 .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinyang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Radar Velocity Maps for Uncertain Dynamic Environments. (arXiv:2107.11039v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.11039</id>
        <link href="http://arxiv.org/abs/2107.11039"/>
        <updated>2021-07-26T02:00:58.022Z</updated>
        <summary type="html"><![CDATA[Future urban transportation concepts include a mixture of ground and air
vehicles with varying degrees of autonomy in a congested environment. In such
dynamic environments, occupancy maps alone are not sufficient for safe path
planning. Safe and efficient transportation requires reasoning about the 3D
flow of traffic and properly modeling uncertainty. Several different approaches
can be taken for developing 3D velocity maps. This paper explores a Bayesian
approach that captures our uncertainty in the map given training data. The
approach involves projecting spatial coordinates into a high-dimensional
feature space and then applying Bayesian linear regression to make predictions
and quantify uncertainty in our estimates. On a collection of air and ground
datasets, we demonstrate that this approach is effective and more scalable than
several alternative approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1"&gt;Ransalu Senanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1"&gt;Kyle Beltran Hatch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jason Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1"&gt;Mykel J. Kochenderfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven deep density estimation. (arXiv:2107.11085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11085</id>
        <link href="http://arxiv.org/abs/2107.11085"/>
        <updated>2021-07-26T02:00:58.013Z</updated>
        <summary type="html"><![CDATA[Density estimation plays a crucial role in many data analysis tasks, as it
infers a continuous probability density function (PDF) from discrete samples.
Thus, it is used in tasks as diverse as analyzing population data, spatial
locations in 2D sensor readings, or reconstructing scenes from 3D scans. In
this paper, we introduce a learned, data-driven deep density estimation (DDE)
to infer PDFs in an accurate and efficient manner, while being independent of
domain dimensionality or sample size. Furthermore, we do not require access to
the original PDF during estimation, neither in parametric form, nor as priors,
or in the form of many samples. This is enabled by training an unstructured
convolutional neural network on an infinite stream of synthetic PDFs, as
unbound amounts of synthetic training data generalize better across a deck of
natural PDFs than any natural finite training data will do. Thus, we hope that
our publicly available DDE method will be beneficial in many areas of data
analysis, where continuous models are to be estimated from discrete
observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1"&gt;Patrik Puchert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1"&gt;Pedro Hermosilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1"&gt;Tobias Ritschel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1"&gt;Timo Ropinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift. (arXiv:2107.11056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.11056</id>
        <link href="http://arxiv.org/abs/2107.11056"/>
        <updated>2021-07-26T02:00:58.005Z</updated>
        <summary type="html"><![CDATA[Meta-learning provides a promising way for learning to efficiently learn and
achieves great success in many applications. However, most meta-learning
literature focuses on dealing with tasks from a same domain, making it brittle
to generalize to tasks from the other unseen domains. In this work, we address
this problem by simulating tasks from the other unseen domains to improve the
generalization and robustness of meta-learning method. Specifically, we propose
a model-agnostic shift layer to learn how to simulate the domain shift and
generate pseudo tasks, and develop a new adversarial learning-to-learn
mechanism to train it. Based on the pseudo tasks, the meta-learning model can
learn cross-domain meta-knowledge, which can generalize well on unseen domains.
We conduct extensive experiments under the domain generalization setting.
Experimental results demonstrate that the proposed shift layer is applicable to
various meta-learning frameworks. Moreover, our method also leads to
state-of-the-art performance on different cross-domain few-shot classification
benchmarks and produces good results on cross-domain few-shot regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1"&gt;Pinzhuo Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yao Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class-Incremental Domain Adaptation with Smoothing and Calibration for Surgical Report Generation. (arXiv:2107.11091v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11091</id>
        <link href="http://arxiv.org/abs/2107.11091"/>
        <updated>2021-07-26T02:00:57.979Z</updated>
        <summary type="html"><![CDATA[Generating surgical reports aimed at surgical scene understanding in
robot-assisted surgery can contribute to documenting entry tasks and
post-operative analysis. Despite the impressive outcome, the deep learning
model degrades the performance when applied to different domains encountering
domain shifts. In addition, there are new instruments and variations in
surgical tissues appeared in robotic surgery. In this work, we propose
class-incremental domain adaptation (CIDA) with a multi-layer transformer-based
model to tackle the new classes and domain shift in the target domain to
generate surgical reports during robotic surgery. To adapt incremental classes
and extract domain invariant features, a class-incremental (CI) learning method
with supervised contrastive (SupCon) loss is incorporated with a feature
extractor. To generate caption from the extracted feature, curriculum by
one-dimensional gaussian smoothing (CBS) is integrated with a multi-layer
transformer-based caption prediction model. CBS smoothes the features embedding
using anti-aliasing and helps the model to learn domain invariant features. We
also adopt label smoothing (LS) to calibrate prediction probability and obtain
better feature representation with both feature extractor and captioning model.
The proposed techniques are empirically evaluated by using the datasets of two
surgical domains, such as nephrectomy operations and transoral robotic surgery.
We observe that domain invariant feature learning and the well-calibrated
network improves the surgical report generation performance in both source and
target domain under domain shift and unseen classes in the manners of one-shot
and few-shot learning. The code is publicly available at
https://github.com/XuMengyaAmy/CIDACaptioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengya Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mobarakol Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1"&gt;Chwee Ming Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hongliang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservoir Computing Approach for Gray Images Segmentation. (arXiv:2107.11077v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11077</id>
        <link href="http://arxiv.org/abs/2107.11077"/>
        <updated>2021-07-26T02:00:57.971Z</updated>
        <summary type="html"><![CDATA[The paper proposes a novel approach for gray scale images segmentation. It is
based on multiple features extraction from single feature per image pixel,
namely its intensity value, using Echo state network. The newly extracted
features -- reservoir equilibrium states -- reveal hidden image characteristics
that improve its segmentation via a clustering algorithm. Moreover, it was
demonstrated that the intrinsic plasticity tuning of reservoir fits its
equilibrium states to the original image intensity distribution thus allowing
for its better segmentation. The proposed approach is tested on the benchmark
image Lena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koprinkova_Hristova_P/0/1/0/all/0/1"&gt;Petia Koprinkova-Hristova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Distribution Amendment with Emotional Semantic Correlations for Facial Expression Recognition. (arXiv:2107.11061v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11061</id>
        <link href="http://arxiv.org/abs/2107.11061"/>
        <updated>2021-07-26T02:00:57.963Z</updated>
        <summary type="html"><![CDATA[By utilizing label distribution learning, a probability distribution is
assigned for a facial image to express a compound emotion, which effectively
improves the problem of label uncertainties and noises occurred in one-hot
labels. In practice, it is observed that correlations among emotions are
inherently different, such as surprised and happy emotions are more possibly
synchronized than surprised and neutral. It indicates the correlation may be
crucial for obtaining a reliable label distribution. Based on this, we propose
a new method that amends the label distribution of each facial image by
leveraging correlations among expressions in the semantic space. Inspired by
inherently diverse correlations among word2vecs, the topological information
among facial expressions is firstly explored in the semantic space, and each
image is embedded into the semantic space. Specially, a class-relation graph is
constructed to transfer the semantic correlation among expressions into the
task space. By comparing semantic and task class-relation graphs of each image,
the confidence of its label distribution is evaluated. Based on the confidence,
the label distribution is amended by enhancing samples with higher confidence
and weakening samples with lower confidence. Experimental results demonstrate
the proposed method is more effective than compared state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1"&gt;Shasha Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1"&gt;Guanghui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gou_S/0/1/0/all/0/1"&gt;Shuiping Gou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yangyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Lin Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Boxin Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transporting Causal Mechanisms for Unsupervised Domain Adaptation. (arXiv:2107.11055v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11055</id>
        <link href="http://arxiv.org/abs/2107.11055"/>
        <updated>2021-07-26T02:00:57.955Z</updated>
        <summary type="html"><![CDATA[Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate
shift and conditional shift assumptions, which essentially encourage models to
learn common features across domains. However, due to the lack of supervision
in the target domain, they suffer from the semantic loss: the feature will
inevitably lose non-discriminative semantics in source domain, which is however
discriminative in target domain. We use a causal view -- transportability
theory -- to identify that such loss is in fact a confounding effect, which can
only be removed by causal intervention. However, the theoretical solution
provided by transportability is far from practical for UDA, because it requires
the stratification and representation of an unobserved confounder that is the
cause of the domain gap. To this end, we propose a practical solution:
Transporting Causal Mechanisms (TCM), to identify the confounder stratum and
representations by using the domain-invariant disentangled causal mechanisms,
which are discovered in an unsupervised fashion. Our TCM is both theoretically
and empirically grounded. Extensive experiments show that TCM achieves
state-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA,
Office-Home, and VisDA-2017. Codes are available in Appendix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zhongqi Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qianru Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image. (arXiv:2107.11010v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11010</id>
        <link href="http://arxiv.org/abs/2107.11010"/>
        <updated>2021-07-26T02:00:57.947Z</updated>
        <summary type="html"><![CDATA[3D shape reconstruction is essential in the navigation of minimally-invasive
and auto robot-guided surgeries whose operating environments are indirect and
narrow, and there have been some works that focused on reconstructing the 3D
shape of the surgical organ through limited 2D information available. However,
the lack and incompleteness of such information caused by intraoperative
emergencies (such as bleeding) and risk control conditions have not been
considered. In this paper, a novel hierarchical shape-perception network (HSPN)
is proposed to reconstruct the 3D point clouds (PCs) of specific brains from
one single incomplete image with low latency. A tree-structured predictor and
several hierarchical attention pipelines are constructed to generate point
clouds that accurately describe the incomplete images and then complete these
point clouds with high quality. Meanwhile, attention gate blocks (AGBs) are
designed to efficiently aggregate geometric local features of incomplete PCs
transmitted by hierarchical attention pipelines and internal features of
reconstructing point clouds. With the proposed HSPN, 3D shape perception and
completion can be achieved spontaneously. Comprehensive results measured by
Chamfer distance and PC-to-PC error demonstrate that the performance of the
proposed HSPN outperforms other competitive methods in terms of qualitative
displays, quantitative experiment, and classification evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bowen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1"&gt;Baiying Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gan_M/0/1/0/all/0/1"&gt;Min Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bingchuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11049</id>
        <link href="http://arxiv.org/abs/2107.11049"/>
        <updated>2021-07-26T02:00:57.938Z</updated>
        <summary type="html"><![CDATA[Recent state-of-the-art active learning methods have mostly leveraged
Generative Adversarial Networks (GAN) for sample acquisition; however, GAN is
usually known to suffer from instability and sensitivity to hyper-parameters.
In contrast to these methods, we propose in this paper a novel active learning
framework that we call Maximum Classifier Discrepancy for Active Learning
(MCDAL) which takes the prediction discrepancies between multiple classifiers.
In particular, we utilize two auxiliary classification layers that learn
tighter decision boundaries by maximizing the discrepancies among them.
Intuitively, the discrepancies in the auxiliary classification layers'
predictions indicate the uncertainty in the prediction. In this regard, we
propose a novel method to leverage the classifier discrepancies for the
acquisition function for active learning. We also provide an interpretation of
our idea in relation to existing GAN based active learning methods and domain
adaptation frameworks. Moreover, we empirically demonstrate the utility of our
approach where the performance of our approach exceeds the state-of-the-art
methods on several image classification and semantic segmentation datasets in
active learning setups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jae Won Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dong-Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1"&gt;Yunjae Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1"&gt;In So Kweon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11022</id>
        <link href="http://arxiv.org/abs/2107.11022"/>
        <updated>2021-07-26T02:00:57.916Z</updated>
        <summary type="html"><![CDATA[We consider unsupervised cell nuclei segmentation in this paper. Exploiting
the recently-proposed unpaired image-to-image translation between cell nuclei
images and randomly synthetic masks, existing approaches, e.g., CycleGAN, have
achieved encouraging results. However, these methods usually take a two-stage
pipeline and fail to learn end-to-end in cell nuclei images. More seriously,
they could lead to the lossy transformation problem, i.e., the content
inconsistency between the original images and the corresponding segmentation
output. To address these limitations, we propose a novel end-to-end
unsupervised framework called Aligned Disentangling Generative Adversarial
Network (AD-GAN). Distinctively, AD-GAN introduces representation
disentanglement to separate content representation (the underling spatial
structure) from style representation (the rendering of the structure). With
this framework, spatial structure can be preserved explicitly, enabling a
significant reduction of macro-level lossy transformation. We also propose a
novel training algorithm able to align the disentangled content in the latent
space to reduce micro-level lossy transformation. Evaluations on real-world 2D
and 3D datasets show that AD-GAN substantially outperforms the other comparison
methods and the professional software both quantitatively and qualitatively.
Specifically, the proposed AD-GAN leads to significant improvement over the
current best unsupervised methods by an average 17.8% relatively (w.r.t. the
metric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN
even performs competitive with the best supervised models, taking a further
leap towards end-to-end unsupervised nuclei segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1"&gt;Kai Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1"&gt;Curran Jude&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments. (arXiv:2107.11043v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11043</id>
        <link href="http://arxiv.org/abs/2107.11043"/>
        <updated>2021-07-26T02:00:57.908Z</updated>
        <summary type="html"><![CDATA[We present a new four-pronged approach to build firefighter's situational
awareness for the first time in the literature. We construct a series of deep
learning frameworks built on top of one another to enhance the safety,
efficiency, and successful completion of rescue missions conducted by
firefighters in emergency first response settings. First, we used a deep
Convolutional Neural Network (CNN) system to classify and identify objects of
interest from thermal imagery in real-time. Next, we extended this CNN
framework for object detection, tracking, segmentation with a Mask RCNN
framework, and scene description with a multimodal natural language
processing(NLP) framework. Third, we built a deep Q-learning-based agent,
immune to stress-induced disorientation and anxiety, capable of making clear
navigation decisions based on the observed and stored facts in live-fire
environments. Finally, we used a low computational unsupervised learning
technique called tensor decomposition to perform meaningful feature extraction
for anomaly detection in real-time. With these ad-hoc deep learning structures,
we built the artificial intelligence system's backbone for firefighters'
situational awareness. To bring the designed system into usage by firefighters,
we designed a physical structure where the processed results are used as inputs
in the creation of an augmented reality capable of advising firefighters of
their location and key features around them, which are vital to the rescue
operation at hand, as well as a path planning feature that acts as a virtual
guide to assist disoriented first responders in getting back to safety. When
combined, these four approaches present a novel approach to information
understanding, transfer, and synthesis that could dramatically improve
firefighter response and efficacy and reduce life loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1"&gt;Manish Bhattarai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RewriteNet: Realistic Scene Text Image Generation via Editing Text in Real-world Image. (arXiv:2107.11041v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11041</id>
        <link href="http://arxiv.org/abs/2107.11041"/>
        <updated>2021-07-26T02:00:57.900Z</updated>
        <summary type="html"><![CDATA[Scene text editing (STE), which converts a text in a scene image into the
desired text while preserving an original style, is a challenging task due to a
complex intervention between text and style. To address this challenge, we
propose a novel representational learning-based STE model, referred to as
RewriteNet that employs textual information as well as visual information. We
assume that the scene text image can be decomposed into content and style
features where the former represents the text information and style represents
scene text characteristics such as font, alignment, and background. Under this
assumption, we propose a method to separately encode content and style features
of the input image by introducing the scene text recognizer that is trained by
text information. Then, a text-edited image is generated by combining the style
feature from the original image and the content feature from the target text.
Unlike previous works that are only able to use synthetic images in the
training phase, we also exploit real-world images by proposing a
self-supervised training scheme, which bridges the domain gap between synthetic
and real data. Our experiments demonstrate that RewriteNet achieves better
quantitative and qualitative performance than other comparisons. Moreover, we
validate that the use of text information and the self-supervised training
scheme improves text switching performance. The implementation and dataset will
be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junyeop Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoonsik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seonghyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1"&gt;Moonbin Yim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Seung Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gayoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Proximal Unrolling Network for Compressive Sensing Imaging. (arXiv:2107.11007v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11007</id>
        <link href="http://arxiv.org/abs/2107.11007"/>
        <updated>2021-07-26T02:00:57.877Z</updated>
        <summary type="html"><![CDATA[Recovering an underlying image from under-sampled measurements, Compressive
Sensing Imaging (CSI) is a challenging problem and has many practical
applications. Recently, deep neural networks have been applied to this problem
with promising results, owing to its implicitly learned prior to alleviate the
ill-poseness of CSI. However, existing neural network approaches require
separate models for each imaging parameter like sampling ratios, leading to
training difficulties and overfitting to specific settings. In this paper, we
present a dynamic proximal unrolling network (dubbed DPUNet), which can handle
a variety of measurement matrices via one single model without retraining.
Specifically, DPUNet can exploit both embedded physical model via gradient
descent and imposing image prior with learned dynamic proximal mapping leading
to joint reconstruction. A key component of DPUNet is a dynamic proximal
mapping module, whose parameters can be dynamically adjusted at inference stage
and make it adapt to any given imaging setting. Experimental results
demonstrate that the proposed DPUNet can effectively handle multiple CSI
modalities under varying sampling ratios and noise levels with only one model,
and outperform the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yixiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kaixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Signed Directional Distance Function for Object Shape Representation. (arXiv:2107.11024v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11024</id>
        <link href="http://arxiv.org/abs/2107.11024"/>
        <updated>2021-07-26T02:00:57.868Z</updated>
        <summary type="html"><![CDATA[Neural networks that map 3D coordinates to signed distance function (SDF) or
occupancy values have enabled high-fidelity implicit representations of object
shape. This paper develops a new shape model that allows synthesizing novel
distance views by optimizing a continuous signed directional distance function
(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole
categories of shapes and complete or interpolate across shapes from partial
input data. Unlike an SDF, which measures distance to the nearest surface in
any direction, an SDDF measures distance in a given direction. This allows
training an SDDF model without 3D shape supervision, using only distance
measurements, readily available from depth camera or Lidar sensors. Our model
also removes post-processing steps like surface extraction or rendering by
directly predicting distance at arbitrary locations and viewing directions.
Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which
train high-capacity black-box models, our model encodes by construction the
property that SDDF values decrease linearly along the viewing direction. This
structure constraint not only results in dimensionality reduction but also
provides analytical confidence about the accuracy of SDDF predictions,
regardless of the distance to the object surface.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zobeidi_E/0/1/0/all/0/1"&gt;Ehsan Zobeidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1"&gt;Nikolay Atanasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photon-Starved Scene Inference using Single Photon Cameras. (arXiv:2107.11001v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.11001</id>
        <link href="http://arxiv.org/abs/2107.11001"/>
        <updated>2021-07-26T02:00:57.849Z</updated>
        <summary type="html"><![CDATA[Scene understanding under low-light conditions is a challenging problem. This
is due to the small number of photons captured by the camera and the resulting
low signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging
sensing modality that are capable of capturing images with high sensitivity.
Despite having minimal read-noise, images captured by SPCs in photon-starved
conditions still suffer from strong shot noise, preventing reliable scene
inference. We propose photon scale-space a collection of high-SNR images
spanning a wide range of photons-per-pixel (PPP) levels (but same scene
content) as guides to train inference model on low photon flux images. We
develop training techniques that push images with different illumination levels
closer to each other in feature representation space. The key idea is that
having a spectrum of different brightness levels during training enables
effective guidance, and increases robustness to shot noise even in extreme
noise cases. Based on the proposed approach, we demonstrate, via simulations
and real experiments with a SPAD camera, high-performance on various inference
tasks such as image classification and monocular depth estimation under ultra
low-light, down to < 1 PPP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Goyal_B/0/1/0/all/0/1"&gt;Bhavya Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Mohit Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications. (arXiv:2107.11008v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2107.11008</id>
        <link href="http://arxiv.org/abs/2107.11008"/>
        <updated>2021-07-26T02:00:57.838Z</updated>
        <summary type="html"><![CDATA[Transparent objects are a very challenging problem in computer vision. They
are hard to segment or classify due to their lack of precise boundaries, and
there is limited data available for training deep neural networks. As such,
current solutions for this problem employ rigid synthetic datasets, which lack
flexibility and lead to severe performance degradation when deployed on
real-world scenarios. In particular, these synthetic datasets omit features
such as refraction, dispersion and caustics due to limitations in the rendering
pipeline. To address this issue, we present SuperCaustics, a real-time,
open-source simulation of transparent objects designed for deep learning
applications. SuperCaustics features extensive modules for stochastic
environment creation; uses hardware ray-tracing to support caustics,
dispersion, and refraction; and enables generating massive datasets with
multi-modal, pixel-perfect ground truth annotations. To validate our proposed
system, we trained a deep neural network from scratch to segment transparent
objects in difficult lighting scenarios. Our neural network achieved
performance comparable to the state-of-the-art on a real-world dataset using
only 10% of the training data and in a fraction of the training time. Further
experiments show that a model trained with SuperCaustics can segment different
types of caustics, even in images with multiple overlapping transparent
objects. To the best of our knowledge, this is the first such result for a
model trained on synthetic data. Both our open-source code and experimental
data are freely available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mousavi_M/0/1/0/all/0/1"&gt;Mehdi Mousavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1"&gt;Rolando Estrada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptive Video Segmentation via Temporal Consistency Regularization. (arXiv:2107.11004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11004</id>
        <link href="http://arxiv.org/abs/2107.11004"/>
        <updated>2021-07-26T02:00:57.815Z</updated>
        <summary type="html"><![CDATA[Video semantic segmentation is an essential task for the analysis and
understanding of videos. Recent efforts largely focus on supervised video
segmentation by learning from fully annotated data, but the learnt models often
experience clear performance drop while applied to videos of a different
domain. This paper presents DA-VSN, a domain adaptive video segmentation
network that addresses domain gaps in videos by temporal consistency
regularization (TCR) for consecutive frames of target-domain videos. DA-VSN
consists of two novel and complementary designs. The first is cross-domain TCR
that guides the prediction of target frames to have similar temporal
consistency as that of source frames (learnt from annotated source data) via
adversarial learning. The second is intra-domain TCR that guides unconfident
predictions of target frames to have similar temporal consistency as confident
predictions of target frames. Extensive experiments demonstrate the superiority
of our proposed domain adaptive video segmentation network which outperforms
multiple baselines consistently by large margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning Ternary Quantization. (arXiv:2107.10998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10998</id>
        <link href="http://arxiv.org/abs/2107.10998"/>
        <updated>2021-07-26T02:00:57.807Z</updated>
        <summary type="html"><![CDATA[We propose pruning ternary quantization (PTQ), a simple, yet effective,
symmetric ternary quantization method. The method significantly compresses
neural network weights to a sparse ternary of [-1,0,1] and thus reduces
computational, storage, and memory footprints. We show that PTQ can convert
regular weights to ternary orthonormal bases by simply using pruning and L2
projection. In addition, we introduce a refined straight-through estimator to
finalize and stabilize the quantized weights. Our method can provide at most
46x compression ratio on the ResNet-18 structure, with an acceptable accuracy
of 65.36%, outperforming leading methods. Furthermore, PTQ can compress a
ResNet-18 model from 46 MB to 955KB (~48x) and a ResNet-50 model from 99 MB to
3.3MB (~30x), while the top-1 accuracy on ImageNet drops slightly from 69.7% to
65.3% and from 76.15% to 74.47%, respectively. Our method unifies pruning and
quantization and thus provides a range of size-accuracy trade-off.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xue Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Score-Based Point Cloud Denoising. (arXiv:2107.10981v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10981</id>
        <link href="http://arxiv.org/abs/2107.10981"/>
        <updated>2021-07-26T02:00:57.799Z</updated>
        <summary type="html"><![CDATA[Point clouds acquired from scanning devices are often perturbed by noise,
which affects downstream tasks such as surface reconstruction and analysis. The
distribution of a noisy point cloud can be viewed as the distribution of a set
of noise-free samples $p(x)$ convolved with some noise model $n$, leading to
$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy
point cloud, we propose to increase the log-likelihood of each point from $p *
n$ via gradient ascent -- iteratively updating each point's position. Since $p
* n$ is unknown at test-time, and we only need the score (i.e., the gradient of
the log-probability function) to perform gradient ascent, we propose a neural
network architecture to estimate the score of $p * n$ given only noisy point
clouds as input. We derive objective functions for training the network and
develop a denoising algorithm leveraging on the estimated scores. Experiments
demonstrate that the proposed model outperforms state-of-the-art methods under
a variety of noise models, and shows the potential to be applied in other tasks
such as point cloud upsampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shitong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource Efficient Mountainous Skyline Extraction using Shallow Learning. (arXiv:2107.10997v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10997</id>
        <link href="http://arxiv.org/abs/2107.10997"/>
        <updated>2021-07-26T02:00:57.772Z</updated>
        <summary type="html"><![CDATA[Skyline plays a pivotal role in mountainous visual geo-localization and
localization/navigation of planetary rovers/UAVs and virtual/augmented reality
applications. We present a novel mountainous skyline detection approach where
we adapt a shallow learning approach to learn a set of filters to discriminate
between edges belonging to sky-mountain boundary and others coming from
different regions. Unlike earlier approaches, which either rely on extraction
of explicit feature descriptors and their classification, or fine-tuning
general scene parsing deep networks for sky segmentation, our approach learns
linear filters based on local structure analysis. At test time, for every
candidate edge pixel, a single filter is chosen from the set of learned filters
based on pixel's structure tensor, and then applied to the patch around it. We
then employ dynamic programming to solve the shortest path problem for the
resultant multistage graph to get the sky-mountain boundary. The proposed
approach is computationally faster than earlier methods while providing
comparable performance and is more suitable for resource constrained platforms
e.g., mobile devices, planetary rovers and UAVs. We compare our proposed
approach against earlier skyline detection methods using four different data
sets. Our code is available at
\url{https://github.com/TouqeerAhmad/skyline_detection}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1"&gt;Touqeer Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emami_E/0/1/0/all/0/1"&gt;Ebrahim Emami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cadik_M/0/1/0/all/0/1"&gt;Martin &amp;#x10c;ad&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bebis_G/0/1/0/all/0/1"&gt;George Bebis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion. (arXiv:2011.01678v2 [eess.AS] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2011.01678</id>
        <link href="http://arxiv.org/abs/2011.01678"/>
        <updated>2021-07-26T02:00:57.757Z</updated>
        <summary type="html"><![CDATA[Though significant progress has been made for the voice conversion (VC) of
typical speech, VC for atypical speech, e.g., dysarthric and second-language
(L2) speech, remains a challenge, since it involves correcting for atypical
prosody while maintaining speaker identity. To address this issue, we propose a
VC system with explicit prosodic modelling and deep speaker embedding (DSE)
learning. First, a speech-encoder strives to extract robust phoneme embeddings
from atypical speech. Second, a prosody corrector takes in phoneme embeddings
to infer typical phoneme duration and pitch values. Third, a conversion model
takes phoneme embeddings and typical prosody features as inputs to generate the
converted speech, conditioned on the target DSE that is learned via speaker
encoder or speaker adaptation. Extensive experiments demonstrate that speaker
adaptation can achieve higher speaker similarity, and the speaker encoder based
conversion model can greatly reduce dysarthric and non-native pronunciation
patterns with improved speech intelligibility. A comparison of speech
recognition results between the original dysarthric speech and converted speech
show that absolute reduction of 47.6% character error rate (CER) and 29.3% word
error rate (WER) can be achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Disong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lifa Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xixin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detail Preserving Residual Feature Pyramid Modules for Optical Flow. (arXiv:2107.10990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10990</id>
        <link href="http://arxiv.org/abs/2107.10990"/>
        <updated>2021-07-26T02:00:57.748Z</updated>
        <summary type="html"><![CDATA[Feature pyramids and iterative refinement have recently led to great progress
in optical flow estimation. However, downsampling in feature pyramids can cause
blending of foreground objects with the background, which will mislead
subsequent decisions in the iterative processing. The results are missing
details especially in the flow of thin and of small structures. We propose a
novel Residual Feature Pyramid Module (RFPM) which retains important details in
the feature map without changing the overall iterative refinement design of the
optical flow estimation. RFPM incorporates a residual structure between
multiple feature pyramids into a downsampling module that corrects the blending
of objects across boundaries. We demonstrate how to integrate our module with
two state-of-the-art iterative refinement architectures. Results show that our
RFPM visibly reduces flow errors and improves state-of-art performance in the
clean pass of Sintel, and is one of the top-performing methods in KITTI.
According to the particular modular structure of RFPM, we introduce a special
transfer learning approach that can dramatically decrease the training time
compared to a typical full optical flow training schedule on multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_L/0/1/0/all/0/1"&gt;Libo Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_J/0/1/0/all/0/1"&gt;Jochen Lang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. (arXiv:2107.10912v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10912</id>
        <link href="http://arxiv.org/abs/2107.10912"/>
        <updated>2021-07-26T02:00:57.739Z</updated>
        <summary type="html"><![CDATA[With an increase in deep learning-based methods, the call for explainability
of such methods grows, especially in high-stakes decision making areas such as
medical image analysis. This survey presents an overview of eXplainable
Artificial Intelligence (XAI) used in deep learning-based medical image
analysis. A framework of XAI criteria is introduced to classify deep
learning-based medical image analysis methods. Papers on XAI techniques in
medical image analysis are then surveyed and categorized according to the
framework and according to anatomical location. The paper concludes with an
outlook of future opportunities for XAI in medical image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Velden_B/0/1/0/all/0/1"&gt;Bas H.M. van der Velden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1"&gt;Hugo J. Kuijf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gilhuijs_K/0/1/0/all/0/1"&gt;Kenneth G.A. Gilhuijs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Viergever_M/0/1/0/all/0/1"&gt;Max A. Viergever&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose Estimation and 3D Reconstruction of Vehicles from Stereo-Images Using a Subcategory-Aware Shape Prior. (arXiv:2107.10898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10898</id>
        <link href="http://arxiv.org/abs/2107.10898"/>
        <updated>2021-07-26T02:00:57.712Z</updated>
        <summary type="html"><![CDATA[The 3D reconstruction of objects is a prerequisite for many highly relevant
applications of computer vision such as mobile robotics or autonomous driving.
To deal with the inverse problem of reconstructing 3D objects from their 2D
projections, a common strategy is to incorporate prior object knowledge into
the reconstruction approach by establishing a 3D model and aligning it to the
2D image plane. However, current approaches are limited due to inadequate shape
priors and the insufficiency of the derived image observations for a reliable
alignment with the 3D model. The goal of this paper is to show how 3D object
reconstruction can profit from a more sophisticated shape prior and from a
combined incorporation of different observation types inferred from the images.
We introduce a subcategory-aware deformable vehicle model that makes use of a
prediction of the vehicle type for a more appropriate regularisation of the
vehicle shape. A multi-branch CNN is presented to derive predictions of the
vehicle type and orientation. This information is also introduced as prior
information for model fitting. Furthermore, the CNN extracts vehicle keypoints
and wireframes, which are well-suited for model-to-image association and model
fitting. The task of pose estimation and reconstruction is addressed by a
versatile probabilistic model. Extensive experiments are conducted using two
challenging real-world data sets on both of which the benefit of the developed
shape prior can be shown. A comparison to state-of-the-art methods for vehicle
pose estimation shows that the proposed approach performs on par or better,
confirming the suitability of the developed shape prior and probabilistic model
for vehicle reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coenen_M/0/1/0/all/0/1"&gt;Max Coenen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rottensteiner_F/0/1/0/all/0/1"&gt;Franz Rottensteiner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10873</id>
        <link href="http://arxiv.org/abs/2107.10873"/>
        <updated>2021-07-26T02:00:57.697Z</updated>
        <summary type="html"><![CDATA[Recent studies show that deep neural networks (DNN) are vulnerable to
adversarial examples, which aim to mislead DNNs by adding perturbations with
small magnitude. To defend against such attacks, both empirical and theoretical
defense approaches have been extensively studied for a single ML model. In this
work, we aim to analyze and provide the certified robustness for ensemble ML
models, together with the sufficient and necessary conditions of robustness for
different ensemble protocols. Although ensemble models are shown more robust
than a single model empirically; surprisingly, we find that in terms of the
certified robustness the standard ensemble models only achieve marginal
improvement compared to a single model. Thus, to explore the conditions that
guarantee to provide certifiably robust ensemble ML models, we first prove that
diversified gradient and large confidence margin are sufficient and necessary
conditions for certifiably robust ensemble models under the model-smoothness
assumption. We then provide the bounded model-smoothness analysis based on the
proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble
model can always achieve higher certified robustness than a single base model
under mild conditions. Inspired by the theoretical findings, we propose the
lightweight Diversity Regularized Training (DRT) to train certifiably robust
ensemble ML models. Extensive experiments show that our DRT enhanced ensembles
can consistently achieve higher certified robustness than existing single and
ensemble ML models, demonstrating the state-of-the-art certified L2-robustness
on MNIST, CIFAR-10, and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04726</id>
        <link href="http://arxiv.org/abs/2106.04726"/>
        <updated>2021-07-26T02:00:57.689Z</updated>
        <summary type="html"><![CDATA[There is currently no easy way to fact-check content on WhatsApp and other
end-to-end encrypted platforms at scale. In this paper, we analyze the
usefulness of a crowd-sourced "tipline" through which users can submit content
("tips") that they want fact-checked. We compare the tips sent to a WhatsApp
tipline run during the 2019 Indian national elections with the messages
circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, our
findings suggest tiplines can be an effective source for discovering content to
fact-check.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1"&gt;Ashkan Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1"&gt;Kiran Garimella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1"&gt;Gautam Kishore Shahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1"&gt;Devin Gaffney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1"&gt;Scott A. Hale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15561</id>
        <link href="http://arxiv.org/abs/2106.15561"/>
        <updated>2021-07-26T02:00:57.678Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank Soong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10984</id>
        <link href="http://arxiv.org/abs/2107.10984"/>
        <updated>2021-07-26T02:00:57.670Z</updated>
        <summary type="html"><![CDATA[Deep generative models have made great progress in synthesizing images with
arbitrary human poses and transferring poses of one person to others. However,
most existing approaches explicitly leverage the pose information extracted
from the source images as a conditional input for the generative networks.
Meanwhile, they usually focus on the visual fidelity of the synthesized images
but neglect the inherent consistency, which further confines their performance
of pose transfer. To alleviate the current limitations and improve the quality
of the synthesized images, we propose a pose transfer network with Disentangled
Feature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair
of images containing the source and target person, DFC-Net extracts pose and
static information from the source and target respectively, then synthesizes an
image of the target person with the desired pose from the source. Moreover,
DFC-Net leverages disentangled feature consistency losses in the adversarial
training to strengthen the transfer coherence and integrates the keypoint
amplifier to enhance the pose feature extraction. Additionally, an unpaired
support dataset Mixamo-Sup providing more extra pose information has been
further utilized during the training to improve the generality and robustness
of DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have
demonstrated DFC-Net achieves state-of-the-art performance on pose transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhengping Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1"&gt;Zheng Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Gangyi Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-Clustering Point Clouds of Crop Fields Using Scalable Methods. (arXiv:2107.10950v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10950</id>
        <link href="http://arxiv.org/abs/2107.10950"/>
        <updated>2021-07-26T02:00:57.648Z</updated>
        <summary type="html"><![CDATA[In order to apply the recent successes of automated plant phenotyping and
machine learning on a large scale, efficient and general algorithms must be
designed to intelligently split crop fields into small, yet actionable,
portions that can then be processed by more complex algorithms. In this paper
we notice a similarity between the current state-of-the-art for this problem
and a commonly used density-based clustering algorithm, Quickshift. Exploiting
this similarity we propose a number of novel, application specific algorithms
with the goal of producing a general and scalable plant segmentation algorithm.
The novel algorithms proposed in this work are shown to produce quantitatively
better results than the current state-of-the-art while being less sensitive to
input parameters and maintaining the same algorithmic time complexity. When
incorporated into field-scale phenotyping systems, the proposed algorithms
should work as a drop in replacement that can greatly improve the accuracy of
results while ensuring that performance and scalability remain undiminished.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_H/0/1/0/all/0/1"&gt;Henry J. Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papanikolopoulos_N/0/1/0/all/0/1"&gt;Nikolaos Papanikolopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks. (arXiv:2107.10963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10963</id>
        <link href="http://arxiv.org/abs/2107.10963"/>
        <updated>2021-07-26T02:00:57.640Z</updated>
        <summary type="html"><![CDATA[Conditional computation and modular networks have been recently proposed for
multitask learning and other problems as a way to decompose problem solving
into multiple reusable computational blocks. We propose a new approach for
learning modular networks based on the isometric version of ResNet with all
residual blocks having the same configuration and the same number of
parameters. This architectural choice allows adding, removing and changing the
order of residual blocks. In our method, the modules can be invoked repeatedly
and allow knowledge transfer to novel tasks by adjusting the order of
computation. This allows soft weight sharing between tasks with only a small
increase in the number of parameters. We show that our method leads to
interpretable self-organization of modules in case of multi-task learning,
transfer learning and domain adaptation while achieving competitive results on
those tasks. From practical perspective, our approach allows to: (a) reuse
existing modules for learning new task by adjusting the computation order, (b)
use it for unsupervised multi-source domain adaptation to illustrate that
adaptation to unseen data can be achieved by only manipulating the order of
pretrained modules, (c) show how our approach can be used to increase accuracy
of existing architectures for image classification tasks such as ImageNet,
without any parameter increase, by reusing the same block multiple times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1"&gt;Andrey Zhmoginov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1"&gt;Mark Sandler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OLR 2021 Challenge: Datasets, Rules and Baselines. (arXiv:2107.11113v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11113</id>
        <link href="http://arxiv.org/abs/2107.11113"/>
        <updated>2021-07-26T02:00:57.631Z</updated>
        <summary type="html"><![CDATA[This paper introduces the sixth Oriental Language Recognition (OLR) 2021
Challenge, which intends to improve the performance of language recognition
systems and speech recognition systems within multilingual scenarios. The data
profile, four tasks, two baselines, and the evaluation principles are
introduced in this paper. In addition to the Language Identification (LID)
tasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to
OLR 2021 Challenge for the first time. The challenge this year focuses on more
practical and challenging problems, with four tasks: (1) constrained LID, (2)
unconstrained LID, (3) constrained multilingual ASR, (4) unconstrained
multilingual ASR. Baselines for LID tasks and multilingual ASR tasks are
provided, respectively. The LID baseline system is an extended TDNN x-vector
model constructed with Pytorch. A transformer-based end-to-end model is
provided as the multilingual ASR baseline system. These recipes will be online
published, and available for participants to construct their own LID or ASR
systems. The baseline results demonstrate that those tasks are rather
challenging and deserve more effort to achieve better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Binling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenxuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1"&gt;Yiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qingyang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Liming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference. (arXiv:2107.10931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10931</id>
        <link href="http://arxiv.org/abs/2107.10931"/>
        <updated>2021-07-26T02:00:57.623Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a domain generalization (DG) approach to learn on
several labeled source domains and transfer knowledge to a target domain that
is inaccessible in training. Considering the inherent conditional and label
shifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the
widely used domain invariant feature learning (IFL) methods relies on aligning
the marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic
assumption that $p(y)$ is invariant across domains. We thereby propose a novel
variational Bayesian inference framework to enforce the conditional
distribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a
latent space, which also takes the marginal label shift w.r.t. $p(y)$ into
consideration with the posterior alignment. Extensive experiments on various
benchmarks demonstrate that our framework is robust to the label shift and the
cross-domain accuracy is significantly improved, thereby achieving superior
performance over the conventional IFL counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1"&gt;Jinsong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges EL Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAGE: A Split-Architecture Methodology for Efficient End-to-End Autonomous Vehicle Control. (arXiv:2107.10895v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.10895</id>
        <link href="http://arxiv.org/abs/2107.10895"/>
        <updated>2021-07-26T02:00:57.603Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles (AV) are expected to revolutionize transportation and
improve road safety significantly. However, these benefits do not come without
cost; AVs require large Deep-Learning (DL) models and powerful hardware
platforms to operate reliably in real-time, requiring between several hundred
watts to one kilowatt of power. This power consumption can dramatically reduce
vehicles' driving range and affect emissions. To address this problem, we
propose SAGE: a methodology for selectively offloading the key energy-consuming
modules of DL architectures to the cloud to optimize edge energy usage while
meeting real-time latency constraints. Furthermore, we leverage Head Network
Distillation (HND) to introduce efficient bottlenecks within the DL
architecture in order to minimize the network overhead costs of offloading with
almost no degradation in the model's performance. We evaluate SAGE using an
Nvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge
devices and demonstrate that our offloading strategy is practical for a wide
range of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi
technologies. Compared to edge-only computation, SAGE reduces energy
consumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one
low-resolution camera, one high-resolution camera, and three high-resolution
cameras, respectively. SAGE also reduces upload data size by up to 98.40%
compared to direct camera offloading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Malawade_A/0/1/0/all/0/1"&gt;Arnav Malawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Odema_M/0/1/0/all/0/1"&gt;Mohanad Odema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lajeunesse_DeGroot_S/0/1/0/all/0/1"&gt;Sebastien Lajeunesse-DeGroot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. (arXiv:2107.03451v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03451</id>
        <link href="http://arxiv.org/abs/2107.03451"/>
        <updated>2021-07-26T02:00:57.593Z</updated>
        <summary type="html"><![CDATA[Over the last several years, end-to-end neural conversational agents have
vastly improved in their ability to carry a chit-chat conversation with humans.
However, these models are often trained on large datasets from the internet,
and as a result, may learn undesirable behaviors from this data, such as toxic
or otherwise harmful language. Researchers must thus wrestle with the issue of
how and when to release these models. In this paper, we survey the problem
landscape for safety for end-to-end conversational AI and discuss recent and
related work. We highlight tensions between values, potential positive impact
and potential harms, and provide a framework for making decisions about whether
and how to release these models, following the tenets of value-sensitive
design. We additionally provide a suite of tools to enable researchers to make
better-informed decisions about training and releasing end-to-end
conversational AI models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1"&gt;Emily Dinan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1"&gt;Gavin Abercrombie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1"&gt;A. Stevie Bergman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spruit_S/0/1/0/all/0/1"&gt;Shannon Spruit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1"&gt;Dirk Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1"&gt;Y-Lan Boureau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1"&gt;Verena Rieser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Power Plant Classification from Remote Imaging with Deep Learning. (arXiv:2107.10894v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10894</id>
        <link href="http://arxiv.org/abs/2107.10894"/>
        <updated>2021-07-26T02:00:57.585Z</updated>
        <summary type="html"><![CDATA[Satellite remote imaging enables the detailed study of land use patterns on a
global scale. We investigate the possibility to improve the information content
of traditional land use classification by identifying the nature of industrial
sites from medium-resolution remote sensing images. In this work, we focus on
classifying different types of power plants from Sentinel-2 imaging data. Using
a ResNet-50 deep learning model, we are able to achieve a mean accuracy of
90.0% in distinguishing 10 different power plant types and a background class.
Furthermore, we are able to identify the cooling mechanisms utilized in thermal
power plants with a mean accuracy of 87.5%. Our results enable us to
qualitatively investigate the energy mix from Sentinel-2 imaging data, and
prove the feasibility to classify industrial sites on a global scale from
freely available satellite imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mommert_M/0/1/0/all/0/1"&gt;Michael Mommert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheibenreif_L/0/1/0/all/0/1"&gt;Linus Scheibenreif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1"&gt;Jo&amp;#xeb;lle Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1"&gt;Damian Borth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation. (arXiv:2107.03625v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03625</id>
        <link href="http://arxiv.org/abs/2107.03625"/>
        <updated>2021-07-26T02:00:57.577Z</updated>
        <summary type="html"><![CDATA[A comparison of formulaic sequences in human and neural machine translation
of quality newspaper articles shows that neural machine translations contain
less lower-frequency, but strongly-associated formulaic sequences, and more
high-frequency formulaic sequences. These differences were statistically
significant and the effect sizes were almost always medium or large. These
observations can be related to the differences between second language learners
of various levels and between translated and untranslated texts. The comparison
between the neural machine translation systems indicates that some systems
produce more formulaic sequences of both types than other systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1"&gt;Yves Bestgen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization. (arXiv:2106.10127v1 [eess.AS] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.10127</id>
        <link href="http://arxiv.org/abs/2106.10127"/>
        <updated>2021-07-26T02:00:57.564Z</updated>
        <summary type="html"><![CDATA[Dysarthric speech detection (DSD) systems aim to detect characteristics of
the neuromotor disorder from speech. Such systems are particularly susceptible
to domain mismatch where the training and testing data come from the source and
target domains respectively, but the two domains may differ in terms of speech
stimuli, disease etiology, etc. It is hard to acquire labelled data in the
target domain, due to high costs of annotating sizeable datasets. This paper
makes a first attempt to formulate cross-domain DSD as an unsupervised domain
adaptation (UDA) problem. We use labelled source-domain data and unlabelled
target-domain data, and propose a multi-task learning strategy, including
dysarthria presence classification (DPC), domain adversarial training (DAT) and
mutual information minimization (MIM), which aim to learn
dysarthria-discriminative and domain-invariant biomarker embeddings.
Specifically, DPC helps biomarker embeddings capture critical indicators of
dysarthria; DAT forces biomarker embeddings to be indistinguishable in source
and target domains; and MIM further reduces the correlation between biomarker
embeddings and domain-related cues. By treating the UASPEECH and TORGO corpora
respectively as the source and target domains, experiments show that the
incorporation of UDA attains absolute increases of 22.2% and 20.0% respectively
in utterance-level weighted average recall and speaker-level accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Disong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1"&gt;Liqun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1"&gt;Yu Ting Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Differentiable Language Model Adversarial Attack on Text Classifiers. (arXiv:2107.11275v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11275</id>
        <link href="http://arxiv.org/abs/2107.11275"/>
        <updated>2021-07-26T02:00:57.518Z</updated>
        <summary type="html"><![CDATA[Robustness of huge Transformer-based models for natural language processing
is an important issue due to their capabilities and wide adoption. One way to
understand and improve robustness of these models is an exploration of an
adversarial attack scenario: check if a small perturbation of an input can fool
a model.

Due to the discrete nature of textual data, gradient-based adversarial
methods, widely used in computer vision, are not applicable per~se. The
standard strategy to overcome this issue is to develop token-level
transformations, which do not take the whole sentence into account.

In this paper, we propose a new black-box sentence-level attack. Our method
fine-tunes a pre-trained language model to generate adversarial examples. A
proposed differentiable loss function depends on a substitute classifier score
and an approximate edit distance computed via a deep learning model.

We show that the proposed attack outperforms competitors on a diverse set of
NLP problems for both computed metrics and human evaluation. Moreover, due to
the usage of the fine-tuned language model, the generated adversarial examples
are hard to detect, thus current models are not robust. Hence, it is difficult
to defend from the proposed attack, which is not the case for other attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1"&gt;Ivan Fursov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1"&gt;Pavel Burnyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dmitrieva_E/0/1/0/all/0/1"&gt;Ekaterina Dmitrieva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1"&gt;Nikita Klyuchnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1"&gt;Andrey Kravchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1"&gt;Ekaterina Artemova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-Based Learning for Stock Movement Prediction with Textual and Relational Data. (arXiv:2107.10941v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10941</id>
        <link href="http://arxiv.org/abs/2107.10941"/>
        <updated>2021-07-26T02:00:57.434Z</updated>
        <summary type="html"><![CDATA[Predicting stock prices from textual information is a challenging task due to
the uncertainty of the market and the difficulty understanding the natural
language from a machine's perspective. Previous researches focus mostly on
sentiment extraction based on single news. However, the stocks on the financial
market can be highly correlated, one news regarding one stock can quickly
impact the prices of other stocks. To take this effect into account, we propose
a new stock movement prediction framework: Multi-Graph Recurrent Network for
Stock Forecasting (MGRN). This architecture allows to combine the textual
sentiment from financial news and multiple relational information extracted
from other financial data. Through an accuracy test and a trading simulation on
the stocks in the STOXX Europe 600 index, we demonstrate a better performance
from our model than other benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qinkai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robert_C/0/1/0/all/0/1"&gt;Christian-Yann Robert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15078</id>
        <link href="http://arxiv.org/abs/2106.15078"/>
        <updated>2021-07-26T02:00:57.419Z</updated>
        <summary type="html"><![CDATA[Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy loss, which encourages an exact
token-by-token match between a target sequence with a generated sequence. Such
training objective is sub-optimal when the target sequence not perfect, e.g.,
when the target sequence is corrupted with noises, or when only weak sequence
supervision is available. To address this challenge, we propose a novel
Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a
target n-gram with all n-grams in the generated sequence. EISL draws
inspirations from convolutional networks (ConvNets) which are shift-invariant
to images, hence is robust to the shift of n-grams to tolerate edits in the
target sequences. Moreover, the computation of EISL is essentially a
convolution operation with target n-grams as kernels, which is easy to
implement with existing libraries. To demonstrate the effectiveness of EISL, we
conduct experiments on three tasks: machine translation with noisy target
sequences, unsupervised text style transfer, and non-autoregressive machine
translation. Experimental results show our method significantly outperforms
cross entropy loss on these three tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zichao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1"&gt;Tianhua Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Language Specific Sub-network for Multilingual Machine Translation. (arXiv:2105.09259v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09259</id>
        <link href="http://arxiv.org/abs/2105.09259"/>
        <updated>2021-07-26T02:00:57.409Z</updated>
        <summary type="html"><![CDATA[Multilingual neural machine translation aims at learning a single translation
model for multiple languages. These jointly trained models often suffer from
performance degradation on rich-resource language pairs. We attribute this
degeneration to parameter interference. In this paper, we propose LaSS to
jointly train a single unified multilingual MT model. LaSS learns Language
Specific Sub-network (LaSS) for each language pair to counter parameter
interference. Comprehensive experiments on IWSLT and WMT datasets with various
Transformer architectures show that LaSS obtains gains on 36 language pairs by
up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at
easy extension to new language pairs and zero-shot translation.LaSS boosts
zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes
and trained models are available at https://github.com/NLP-Playground/LaSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zehui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05852</id>
        <link href="http://arxiv.org/abs/2106.05852"/>
        <updated>2021-07-26T02:00:57.399Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the
various linguistic peculiarities present in the language. The Sanskrit language
is lexically productive, undergoes euphonic assimilation of phones at the word
boundaries and exhibits variations in spelling conventions and in
pronunciations. In this work, we propose the first large scale study of
automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact
of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR
dataset for Sanskrit, which faithfully captures several of the linguistic
characteristics expressed by the language. We investigate the role of different
acoustic model and language model units in ASR systems for Sanskrit. We also
propose a new modelling unit, inspired by the syllable level unit selection,
that captures character sequences from one vowel in the word to the next vowel.
We also highlight the importance of choosing graphemic representations for
Sanskrit and show the impact of this choice on word error rates (WER). Finally,
we extend these insights from Sanskrit ASR for building ASR systems in two
other Indic languages, Gujarati and Telugu. For both these languages, our
experimental results show that the use of phonetic based graphemic
representations in ASR results in performance improvements as compared to ASR
systems that use native scripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1"&gt;Devaraja Adiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rishabh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1"&gt;Amrith Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Biomedical Word Embeddings in the Transformer Era. (arXiv:2012.11808v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11808</id>
        <link href="http://arxiv.org/abs/2012.11808"/>
        <updated>2021-07-26T02:00:57.372Z</updated>
        <summary type="html"><![CDATA[Biomedical word embeddings are usually pre-trained on free text corpora with
neural methods that capture local and global distributional properties. They
are leveraged in downstream tasks using various neural architectures that are
designed to optimize task-specific objectives that might further tune such
embeddings. Since 2018, however, there is a marked shift from these static
embeddings to contextual embeddings motivated by language models (e.g., ELMo,
transformers such as BERT, and ULMFiT). These dynamic embeddings have the added
benefit of being able to distinguish homonyms and acronyms given their context.
However, static embeddings are still relevant in low resource settings (e.g.,
smart devices, IoT elements) and to study lexical semantics from a
computational linguistics perspective. In this paper, we jointly learn word and
concept embeddings by first using the skip-gram method and further fine-tuning
them with correlational information manifesting in co-occurring Medical Subject
Heading (MeSH) concepts in biomedical citations. This fine-tuning is
accomplished with the BERT transformer architecture in the two-sentence input
mode with a classification objective that captures MeSH pair co-occurrence. In
essence, we repurpose a transformer architecture (typically used to generate
dynamic embeddings) to improve static embeddings using concept correlations. We
conduct evaluations of these tuned static embeddings using multiple datasets
for word relatedness developed by previous efforts. Without selectively culling
concepts and terms (as was pursued by previous efforts), we believe we offer
the most exhaustive evaluation of static embeddings to date with clear
performance improvements across the board. We provide our code and embeddings
for public use for downstream applications and research endeavors:
https://github.com/bionlproc/BERT-CRel-Embeddings]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1"&gt;Jiho Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1"&gt;Ramakanth Kavuluru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Powering Effective Climate Communication with a Climate Knowledge Base. (arXiv:2107.11351v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11351</id>
        <link href="http://arxiv.org/abs/2107.11351"/>
        <updated>2021-07-26T02:00:57.361Z</updated>
        <summary type="html"><![CDATA[While many accept climate change and its growing impacts, few converse about
it well, limiting the adoption speed of societal changes necessary to address
it. In order to make effective climate communication easier, we aim to build a
system that presents to any individual the climate information predicted to
best motivate and inspire them to take action given their unique set of
personal values. To alleviate the cold-start problem, the system relies on a
knowledge base (ClimateKB) of causes and effects of climate change, and their
associations to personal values. Since no such comprehensive ClimateKB exists,
we revisit knowledge base construction techniques and build a ClimateKB from
free text. We plan to open source the ClimateKB and associated code to
encourage future research and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_K/0/1/0/all/0/1"&gt;Kameron B. Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khushu_S/0/1/0/all/0/1"&gt;Shweta Khushu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_M/0/1/0/all/0/1"&gt;Mukut Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banister_A/0/1/0/all/0/1"&gt;Andrew Banister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hevia_A/0/1/0/all/0/1"&gt;Anthony Hevia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duddu_S/0/1/0/all/0/1"&gt;Sampath Duddu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1"&gt;Nikita Bhutani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01051</id>
        <link href="http://arxiv.org/abs/2105.01051"/>
        <updated>2021-07-26T02:00:57.351Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) has proven vital for advancing research in
natural language processing (NLP) and computer vision (CV). The paradigm
pretrains a shared model on large volumes of unlabeled data and achieves
state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the
speech processing community lacks a similar setup to systematically explore the
paradigm. To bridge this gap, we introduce Speech processing Universal
PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the
performance of a shared model across a wide range of speech processing tasks
with minimal architecture changes and labeled data. Among multiple usages of
the shared model, we especially focus on extracting the representation learned
from SSL due to its preferable re-usability. We present a simple framework to
solve SUPERB tasks by learning task-specialized lightweight prediction heads on
top of the frozen shared model. Our results demonstrate that the framework is
promising as SSL representations show competitive generalizability and
accessibility across SUPERB tasks. We release SUPERB as a challenge with a
leaderboard and a benchmark toolkit to fuel the research in representation
learning and general speech processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shu-wen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1"&gt;Po-Han Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yung-Sung Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Cheng-I Jeff Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1"&gt;Kushal Lakhotia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yist Y. Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiatong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xuankai Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guan-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tzu-Hsien Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1"&gt;Wei-Cheng Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Ko-tik Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Da-Rong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zili Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1"&gt;Shuyan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge. (arXiv:2107.10922v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10922</id>
        <link href="http://arxiv.org/abs/2107.10922"/>
        <updated>2021-07-26T02:00:57.339Z</updated>
        <summary type="html"><![CDATA[Prior research has explored the ability of computational models to predict a
word semantic fit with a given predicate. While much work has been devoted to
modeling the typicality relation between verbs and arguments in isolation, in
this paper we take a broader perspective by assessing whether and to what
extent computational approaches have access to the information about the
typicality of entire events and situations described in language (Generalized
Event Knowledge). Given the recent success of Transformers Language Models
(TLMs), we decided to test them on a benchmark for the \textit{dynamic
estimation of thematic fit}. The evaluation of these models was performed in
comparison with SDM, a framework specifically designed to integrate events in
sentence meaning representations, and we conducted a detailed error analysis to
investigate which factors affect their behavior. Our results show that TLMs can
reach performances that are comparable to those achieved by SDM. However,
additional analysis consistently suggests that TLMs do not capture important
aspects of event knowledge, and their predictions often depend on surface
linguistic features, such as frequent words, collocations and syntactic
patterns, thereby showing sub-optimal generalization abilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pedinotti_P/0/1/0/all/0/1"&gt;Paolo Pedinotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1"&gt;Giulia Rambelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1"&gt;Emmanuele Chersoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1"&gt;Enrico Santus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1"&gt;Alessandro Lenci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blache_P/0/1/0/all/0/1"&gt;Philippe Blache&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Early Sepsis Prediction with Multi Modal Learning. (arXiv:2107.11094v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11094</id>
        <link href="http://arxiv.org/abs/2107.11094"/>
        <updated>2021-07-26T02:00:57.328Z</updated>
        <summary type="html"><![CDATA[Sepsis is a life-threatening disease with high morbidity, mortality and
healthcare costs. The early prediction and administration of antibiotics and
intravenous fluids is considered crucial for the treatment of sepsis and can
save potentially millions of lives and billions in health care costs.
Professional clinical care practitioners have proposed clinical criterion which
aid in early detection of sepsis; however, performance of these criterion is
often limited. Clinical text provides essential information to estimate the
severity of the sepsis in addition to structured clinical data. In this study,
we explore how clinical text can complement structured data towards early
sepsis prediction task. In this paper, we propose multi modal model which
incorporates both structured data in the form of patient measurements as well
as textual notes on the patient. We employ state-of-the-art NLP models such as
BERT and a highly specialized NLP model in Amazon Comprehend Medical to
represent the text. On the MIMIC-III dataset containing records of ICU
admissions, we show that by using these notes, one achieves an improvement of
6.07 points in a standard utility score for Sepsis prediction and 2.89% in
AUROC score. Our methods significantly outperforms a clinical criteria
suggested by experts, qSOFA, as well as the winning model of the PhysioNet
Computing in Cardiology Challenge for predicting Sepsis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Fred Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madan_V/0/1/0/all/0/1"&gt;Vivek Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratan_U/0/1/0/all/0/1"&gt;Ujjwal Ratan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1"&gt;Zohar Karnin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1"&gt;Vishaal Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1"&gt;Parminder Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kass_Hout_T/0/1/0/all/0/1"&gt;Taha Kass-Hout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection. (arXiv:2106.06213v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06213</id>
        <link href="http://arxiv.org/abs/2106.06213"/>
        <updated>2021-07-26T02:00:57.313Z</updated>
        <summary type="html"><![CDATA[Traditional toxicity detection models have focused on the single utterance
level without deeper understanding of context. We introduce CONDA, a new
dataset for in-game toxic language detection enabling joint intent
classification and slot filling analysis, which is the core task of Natural
Language Understanding (NLU). The dataset consists of 45K utterances from 12K
conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a
robust dual semantic-level toxicity framework, which handles utterance and
token-level patterns, and rich contextual chatting history. Accompanying the
dataset is a thorough in-game toxicity analysis, which provides comprehensive
understanding of context at utterance, token, and dual levels. Inspired by NLU,
we also apply its metrics to the toxicity detection tasks for assessing
toxicity and game-specific aspects. We evaluate strong NLU models on CONDA,
providing fine-grained results for different intent classes and slot classes.
Furthermore, we examine the coverage of toxicity nature in our dataset by
comparing it with other toxicity datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1"&gt;Henry Weld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guanghao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jean Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tongshu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kunze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xinghong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1"&gt;Siqu Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1"&gt;Josiah Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Soyeon Caren Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling Latent Translations for Cross-Lingual Transfer. (arXiv:2107.11353v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11353</id>
        <link href="http://arxiv.org/abs/2107.11353"/>
        <updated>2021-07-26T02:00:57.286Z</updated>
        <summary type="html"><![CDATA[While achieving state-of-the-art results in multiple tasks and languages,
translation-based cross-lingual transfer is often overlooked in favour of
massively multilingual pre-trained encoders. Arguably, this is due to its main
limitations: 1) translation errors percolating to the classification phase and
2) the insufficient expressiveness of the maximum-likelihood translation. To
remedy this, we propose a new technique that integrates both steps of the
traditional pipeline (translation and classification) into a single model, by
treating the intermediate translations as a latent random variable. As a
result, 1) the neural machine translation system can be fine-tuned with a
variant of Minimum Risk Training where the reward is the accuracy of the
downstream task classifier. Moreover, 2) multiple samples can be drawn to
approximate the expected loss across all possible translations during
inference. We evaluate our novel latent translation-based model on a series of
multilingual NLU tasks, including commonsense reasoning, paraphrase
identification, and natural language inference. We report gains for both
zero-shot and few-shot learning setups, up to 2.7 accuracy points on average,
which are even more prominent for low-resource languages (e.g., Haitian
Creole). Finally, we carry out in-depth analyses comparing different underlying
NMT models and assessing the impact of alternative translations on the
downstream performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo Maria Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1"&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1"&gt;Siva Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When a crisis strikes: Emotion analysis and detection during COVID-19. (arXiv:2107.11020v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11020</id>
        <link href="http://arxiv.org/abs/2107.11020"/>
        <updated>2021-07-26T02:00:57.272Z</updated>
        <summary type="html"><![CDATA[Crises such as natural disasters, global pandemics, and social unrest
continuously threaten our world and emotionally affect millions of people
worldwide in distinct ways. Understanding emotions that people express during
large-scale crises helps inform policy makers and first responders about the
emotional states of the population as well as provide emotional support to
those who need such support. We present CovidEmo, ~1K tweets labeled with
emotions. We examine how well large pre-trained language models generalize
across domains and crises in the task of perceived emotion prediction in the
context of COVID-19. Our results show that existing models do not directly
transfer from one disaster type to another but using labeled emotional corpora
for domain adaptation is beneficial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tekle_A/0/1/0/all/0/1"&gt;Alexander Tekle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1"&gt;Chau Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1"&gt;Cornelia Caragea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Jessy Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FrameAxis: Characterizing Microframe Bias and Intensity with Word Embedding. (arXiv:2002.08608v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08608</id>
        <link href="http://arxiv.org/abs/2002.08608"/>
        <updated>2021-07-26T02:00:57.260Z</updated>
        <summary type="html"><![CDATA[Framing is a process of emphasizing a certain aspect of an issue over the
others, nudging readers or listeners towards different positions on the issue
even without making a biased argument. {Here, we propose FrameAxis, a method
for characterizing documents by identifying the most relevant semantic axes
("microframes") that are overrepresented in the text using word embedding. Our
unsupervised approach can be readily applied to large datasets because it does
not require manual annotations. It can also provide nuanced insights by
considering a rich set of semantic axes. FrameAxis is designed to
quantitatively tease out two important dimensions of how microframes are used
in the text. \textit{Microframe bias} captures how biased the text is on a
certain microframe, and \textit{microframe intensity} shows how actively a
certain microframe is used. Together, they offer a detailed characterization of
the text. We demonstrate that microframes with the highest bias and intensity
well align with sentiment, topic, and partisan spectrum by applying FrameAxis
to multiple datasets from restaurant reviews to political news.} The existing
domain knowledge can be incorporated into FrameAxis {by using custom
microframes and by using FrameAxis as an iterative exploratory analysis
instrument.} Additionally, we propose methods for explaining the results of
FrameAxis at the level of individual words and documents. Our method may
accelerate scalable and sophisticated computational analyses of framing across
disciplines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1"&gt;Haewoon Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1"&gt;Jisun An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1"&gt;Elise Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1"&gt;Yong-Yeol Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Bilingual Conversational Characteristics for Neural Chat Translation. (arXiv:2107.11164v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.11164</id>
        <link href="http://arxiv.org/abs/2107.11164"/>
        <updated>2021-07-26T02:00:57.236Z</updated>
        <summary type="html"><![CDATA[Neural chat translation aims to translate bilingual conversational text,
which has a broad application in international exchanges and cooperation.
Despite the impressive performance of sentence-level and context-aware Neural
Machine Translation (NMT), there still remain challenges to translate bilingual
conversational text due to its inherent characteristics such as role
preference, dialogue coherence, and translation consistency. In this paper, we
aim to promote the translation quality of conversational text by modeling the
above properties. Specifically, we design three latent variational modules to
learn the distributions of bilingual conversational characteristics. Through
sampling from these learned distributions, the latent variables, tailored for
role preference, dialogue coherence, and translation consistency, are
incorporated into the NMT model for better translation. We evaluate our
approach on the benchmark dataset BConTrasT (English-German) and a
self-collected bilingual dialogue corpus, named BMELD (English-Chinese).
Extensive experiments show that our approach notably boosts the performance
over strong baselines by a large margin and significantly surpasses some
state-of-the-art context-aware NMT models in terms of BLEU and TER.
Additionally, we make the BMELD dataset publicly available for the research
community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yunlong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yufeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (arXiv:2107.11252v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.11252</id>
        <link href="http://arxiv.org/abs/2107.11252"/>
        <updated>2021-07-26T02:00:57.212Z</updated>
        <summary type="html"><![CDATA[Language instruction plays an essential role in the natural language grounded
navigation tasks. However, navigators trained with limited human-annotated
instructions may have difficulties in accurately capturing key information from
the complicated instruction at different timesteps, leading to poor navigation
performance. In this paper, we exploit to train a more robust navigator which
is capable of dynamically extracting crucial factors from the long instruction,
by using an adversarial attacking paradigm. Specifically, we propose a Dynamic
Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the
navigator to move to the wrong target by destroying the most instructive
information in instructions at different timesteps. By formulating the
perturbation generation as a Markov Decision Process, DR-Attacker is optimized
by the reinforcement learning algorithm to generate perturbed instructions
sequentially during the navigation, according to a learnable attack score.
Then, the perturbed instructions, which serve as hard samples, are used for
improving the robustness of the navigator with an effective adversarial
training strategy and an auxiliary self-supervised reasoning task. Experimental
results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog
History (NDH) tasks show the superiority of our proposed method over
state-of-the-art methods. Moreover, the visualization analysis shows the
effectiveness of the proposed DR-Attacker, which can successfully attack
crucial information in the instructions at different timesteps. Code is
available at https://github.com/expectorlin/DR-Attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bingqian Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yanxin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Including Signed Languages in Natural Language Processing. (arXiv:2105.05222v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05222</id>
        <link href="http://arxiv.org/abs/2105.05222"/>
        <updated>2021-07-26T02:00:57.182Z</updated>
        <summary type="html"><![CDATA[Signed languages are the primary means of communication for many deaf and
hard of hearing individuals. Since signed languages exhibit all the fundamental
linguistic properties of natural language, we believe that tools and theories
of Natural Language Processing (NLP) are crucial towards its modeling. However,
existing research in Sign Language Processing (SLP) seldom attempt to explore
and leverage the linguistic organization of signed languages. This position
paper calls on the NLP community to include signed languages as a research area
with high social and scientific impact. We first discuss the linguistic
properties of signed languages to consider during their modeling. Then, we
review the limitations of current SLP models and identify the open challenges
to extend NLP to signed languages. Finally, we urge (1) the adoption of an
efficient tokenization method; (2) the development of linguistically-informed
models; (3) the collection of real-world signed language data; (4) the
inclusion of local signed language communities as an active and leading voice
in the direction of research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1"&gt;Amit Moryossef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochgesang_J/0/1/0/all/0/1"&gt;Julie Hochgesang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1"&gt;Malihe Alikhani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. (arXiv:2107.10932v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10932</id>
        <link href="http://arxiv.org/abs/2107.10932"/>
        <updated>2021-07-26T02:00:57.155Z</updated>
        <summary type="html"><![CDATA[In this note we examine the autoregressive generalization of the FNet
algorithm, in which self-attention layers from the standard Transformer
architecture are substituted with a trivial sparse-uniformsampling procedure
based on Fourier transforms. Using the Wikitext-103 benchmark, we
demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the
task of causal language modelingcompared to a Transformer-XL baseline (24.2
ppl) with only half the number self-attention layers,thus providing further
evidence for the superfluity of deep neural networks with heavily
compoundedattention mechanisms. The autoregressive Fourier transform could
likely be used for parameterreduction on most Transformer-based time-series
prediction models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1"&gt;Tim Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1"&gt;Michael Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezanali_M/0/1/0/all/0/1"&gt;Mohammad Ramezanali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_V/0/1/0/all/0/1"&gt;Vincent Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Graph Matching based Collaborative Filtering. (arXiv:2105.04067v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04067</id>
        <link href="http://arxiv.org/abs/2105.04067"/>
        <updated>2021-07-26T02:00:57.053Z</updated>
        <summary type="html"><![CDATA[User and item attributes are essential side-information; their interactions
(i.e., their co-occurrence in the sample data) can significantly enhance
prediction accuracy in various recommender systems. We identify two different
types of attribute interactions, inner interactions and cross interactions:
inner interactions are those between only user attributes or those between only
item attributes; cross interactions are those between user attributes and item
attributes. Existing models do not distinguish these two types of attribute
interactions, which may not be the most effective way to exploit the
information carried by the interactions. To address this drawback, we propose a
neural Graph Matching based Collaborative Filtering model (GMCF), which
effectively captures the two types of attribute interactions through modeling
and aggregating attribute interactions in a graph matching structure for
recommendation. In our model, the two essential recommendation procedures,
characteristic learning and preference matching, are explicitly conducted
through graph learning (based on inner interactions) and node matching (based
on cross interactions), respectively. Experimental results show that our model
outperforms state-of-the-art models. Further studies verify the effectiveness
of GMCF in improving the accuracy of recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yixin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1"&gt;Sarah Erfani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1"&gt;Junhao Gan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Channel Automatic Music Transcription Using Tensor Algebra. (arXiv:2107.11250v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.11250</id>
        <link href="http://arxiv.org/abs/2107.11250"/>
        <updated>2021-07-26T02:00:57.033Z</updated>
        <summary type="html"><![CDATA[Music is an art, perceived in unique ways by every listener, coming from
acoustic signals. In the meantime, standards as musical scores exist to
describe it. Even if humans can make this transcription, it is costly in terms
of time and efforts, even more with the explosion of information consecutively
to the rise of the Internet. In that sense, researches are driven in the
direction of Automatic Music Transcription. While this task is considered
solved in the case of single notes, it is still open when notes superpose
themselves, forming chords. This report aims at developing some of the existing
techniques towards Music Transcription, particularly matrix factorization, and
introducing the concept of multi-channel automatic music transcription. This
concept will be explored with mathematical objects called tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Axel_M/0/1/0/all/0/1"&gt;Marmoret Axel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nancy_B/0/1/0/all/0/1"&gt;Bertin Nancy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeremy_C/0/1/0/all/0/1"&gt;Cohen Jeremy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15561</id>
        <link href="http://arxiv.org/abs/2106.15561"/>
        <updated>2021-07-26T02:00:56.993Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank Soong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are you optimizing for? Aligning Recommender Systems with Human Values. (arXiv:2107.10939v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.10939</id>
        <link href="http://arxiv.org/abs/2107.10939"/>
        <updated>2021-07-26T02:00:56.914Z</updated>
        <summary type="html"><![CDATA[We describe cases where real recommender systems were modified in the service
of various human values such as diversity, fairness, well-being, time well
spent, and factual accuracy. From this we identify the current practice of
values engineering: the creation of classifiers from human-created data with
value-based labels. This has worked in practice for a variety of issues, but
problems are addressed one at a time, and users and other stakeholders have
seldom been involved. Instead, we look to AI alignment work for approaches that
could learn complex values directly from stakeholders, and identify four major
directions: useful measures of alignment, participatory design and operation,
interactive value learning, and informed deliberative judgments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1"&gt;Jonathan Stray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vendrov_I/0/1/0/all/0/1"&gt;Ivan Vendrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1"&gt;Jeremy Nixon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Steven Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1"&gt;Dylan Hadfield-Menell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing. (arXiv:2007.09236v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09236</id>
        <link href="http://arxiv.org/abs/2007.09236"/>
        <updated>2021-07-23T02:00:33.031Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is an emerging approach for collaboratively training
Deep Neural Networks (DNNs) on mobile devices, without private user data
leaving the devices. Previous works have shown that non-Independent and
Identically Distributed (non-IID) user data harms the convergence speed of the
FL algorithms. Furthermore, most existing work on FL measures global-model
accuracy, but in many cases, such as user content-recommendation, improving
individual User model Accuracy (UA) is the real objective. To address these
issues, we propose a Multi-Task FL (MTFL) algorithm that introduces
non-federated Batch-Normalization (BN) layers into the federated DNN. MTFL
benefits UA and convergence speed by allowing users to train models
personalised to their own data. MTFL is compatible with popular iterative FL
optimisation algorithms such as Federated Averaging (FedAvg), and we show
empirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits
convergence speed even further when used as the optimisation strategy within
MTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to
significantly reduce the number of rounds required to reach a target UA, by up
to $5\times$ when using existing FL optimisation strategies, and with a further
$3\times$ improvement when using FedAvg-Adam. We compare MTFL to competing
personalised FL algorithms, showing that it is able to achieve the best UA for
MNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with
FedAvg-Adam on an edge-computing testbed, showing that its convergence and UA
benefits outweigh its overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1"&gt;Jed Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jia Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1"&gt;Geyong Min&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Transfer: A Foliated Theory. (arXiv:2107.10763v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10763</id>
        <link href="http://arxiv.org/abs/2107.10763"/>
        <updated>2021-07-23T02:00:33.025Z</updated>
        <summary type="html"><![CDATA[Learning to transfer considers learning solutions to tasks in a such way that
relevant knowledge can be transferred from known task solutions to new, related
tasks. This is important for general learning, as well as for improving the
efficiency of the learning process. While techniques for learning to transfer
have been studied experimentally, we still lack a foundational description of
the problem that exposes what related tasks are, and how relationships between
tasks can be exploited constructively. In this work, we introduce a framework
using the differential geometric theory of foliations that provides such a
foundation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petangoda_J/0/1/0/all/0/1"&gt;Janith Petangoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1"&gt;Marc Peter Deisenroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monk_N/0/1/0/all/0/1"&gt;Nicholas A. M. Monk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian multiscale CNN framework to predict local stress fields in structures with microscale features. (arXiv:2012.11330v3 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11330</id>
        <link href="http://arxiv.org/abs/2012.11330"/>
        <updated>2021-07-23T02:00:33.006Z</updated>
        <summary type="html"><![CDATA[Multiscale computational modelling is challenging due to the high
computational cost of direct numerical simulation by finite elements. To
address this issue, concurrent multiscale methods use the solution of cheaper
macroscale surrogates as boundary conditions to microscale sliding windows. The
microscale problems remain a numerically challenging operation both in terms of
implementation and cost. In this work we propose to replace the local
microscale solution by an Encoder-Decoder Convolutional Neural Network that
will generate fine-scale stress corrections to coarse predictions around
unresolved microscale features, without prior parametrisation of local
microscale problems. We deploy a Bayesian approach providing credible intervals
to evaluate the uncertainty of the predictions, which is then used to
investigate the merits of a selective learning framework. We will demonstrate
the capability of the approach to predict equivalent stress fields in porous
structures using linearised and finite strain elasticity theories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krokos_V/0/1/0/all/0/1"&gt;Vasilis Krokos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuan_V/0/1/0/all/0/1"&gt;Viet Bui Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bordas_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane P. A. Bordas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_P/0/1/0/all/0/1"&gt;Philippe Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerfriden_P/0/1/0/all/0/1"&gt;Pierre Kerfriden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed neural networks for solving Reynolds-averaged Navier$\unicode{x2013}$Stokes equations. (arXiv:2107.10711v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.10711</id>
        <link href="http://arxiv.org/abs/2107.10711"/>
        <updated>2021-07-23T02:00:32.977Z</updated>
        <summary type="html"><![CDATA[Physics-informed neural networks (PINNs) are successful machine-learning
methods for the solution and identification of partial differential equations
(PDEs). We employ PINNs for solving the Reynolds-averaged
Navier$\unicode{x2013}$Stokes (RANS) equations for incompressible turbulent
flows without any specific model or assumption for turbulence, and by taking
only the data on the domain boundaries. We first show the applicability of
PINNs for solving the Navier$\unicode{x2013}$Stokes equations for laminar flows
by solving the Falkner$\unicode{x2013}$Skan boundary layer. We then apply PINNs
for the simulation of four turbulent-flow cases, i.e., zero-pressure-gradient
boundary layer, adverse-pressure-gradient boundary layer, and turbulent flows
over a NACA4412 airfoil and the periodic hill. Our results show the excellent
applicability of PINNs for laminar flows with strong pressure gradients, where
predictions with less than 1% error can be obtained. For turbulent flows, we
also obtain very good accuracy on simulation results even for the
Reynolds-stress components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Eivazi_H/0/1/0/all/0/1"&gt;Hamidreza Eivazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tahani_M/0/1/0/all/0/1"&gt;Mojtaba Tahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schlatter_P/0/1/0/all/0/1"&gt;Philipp Schlatter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vinuesa_R/0/1/0/all/0/1"&gt;Ricardo Vinuesa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks. (arXiv:2107.00623v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00623</id>
        <link href="http://arxiv.org/abs/2107.00623"/>
        <updated>2021-07-23T02:00:32.966Z</updated>
        <summary type="html"><![CDATA[Recent studies have put into question the commonly assumed shift invariance
property of convolutional networks, showing that small shifts in the input can
affect the output predictions substantially. In this paper, we analyze the
benefits of addressing lack of shift invariance in CNN-based sound event
classification. Specifically, we evaluate two pooling methods to improve shift
invariance in CNNs, based on low-pass filtering and adaptive sampling of
incoming feature maps. These methods are implemented via small architectural
modifications inserted into the pooling layers of CNNs. We evaluate the effect
of these architectural changes on the FSD50K dataset using models of different
capacity and in presence of strong regularization. We show that these
modifications consistently improve sound event classification in all cases
considered. We also demonstrate empirically that the proposed pooling methods
increase shift invariance in the network, making it more robust against
time/frequency shifts in input spectrograms. This is achieved by adding a
negligible amount of trainable parameters, which makes these methods an
appealing alternative to conventional pooling layers. The outcome is a new
state-of-the-art mAP of 0.541 on the FSD50K classification benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1"&gt;Eduardo Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferraro_A/0/1/0/all/0/1"&gt;Andres Ferraro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1"&gt;Xavier Serra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shedding some light on Light Up with Artificial Intelligence. (arXiv:2107.10429v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.10429</id>
        <link href="http://arxiv.org/abs/2107.10429"/>
        <updated>2021-07-23T02:00:32.959Z</updated>
        <summary type="html"><![CDATA[The Light-Up puzzle, also known as the AKARI puzzle, has never been solved
using modern artificial intelligence (AI) methods. Currently, the most widely
used computational technique to autonomously develop solutions involve
evolution theory algorithms. This project is an effort to apply new AI
techniques for solving the Light-up puzzle faster and more computationally
efficient. The algorithms explored for producing optimal solutions include hill
climbing, simulated annealing, feed-forward neural network (FNN), and
convolutional neural network (CNN). Two algorithms were developed for hill
climbing and simulated annealing using 2 actions (add and remove light bulb)
versus 3 actions(add, remove, or move light-bulb to a different cell). Both
hill climbing and simulated annealing algorithms showed a higher accuracy for
the case of 3 actions. The simulated annealing showed to significantly
outperform hill climbing, FNN, CNN, and an evolutionary theory algorithm
achieving 100% accuracy in 30 unique board configurations. Lastly, while FNN
and CNN algorithms showed low accuracies, computational times were
significantly faster compared to the remaining algorithms. The GitHub
repository for this project can be found at
https://github.com/rperera12/AKARI-LightUp-GameSolver-with-DeepNeuralNetworks-and-HillClimb-or-SimulatedAnnealing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Libo Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Browning_J/0/1/0/all/0/1"&gt;James Browning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perera_R/0/1/0/all/0/1"&gt;Roberto Perera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03481</id>
        <link href="http://arxiv.org/abs/2007.03481"/>
        <updated>2021-07-23T02:00:32.940Z</updated>
        <summary type="html"><![CDATA[This paper presents an inverse reinforcement learning (IRL) framework for
Bayesian stopping time problems. By observing the actions of a Bayesian
decision maker, we provide a necessary and sufficient condition to identify if
these actions are consistent with optimizing a cost function; then we construct
set valued estimates of the cost function. To achieve this IRL objective, we
use novel ideas from Bayesian revealed preferences stemming from
microeconomics. To illustrate our IRL scheme,we consider two important examples
of stopping time problems, namely, sequential hypothesis testing and Bayesian
search. Finally, for finite datasets, we propose an IRL detection algorithm and
give finite sample bounds on its error probabilities. Also we discuss how to
identify $\epsilon$-optimal Bayesian decision makers and perform IRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1"&gt;Vikram Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1"&gt;Kunal Pattanayak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Low-Rank Tensor Decomposition by Ridge Leverage Score Sampling. (arXiv:2107.10654v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.10654</id>
        <link href="http://arxiv.org/abs/2107.10654"/>
        <updated>2021-07-23T02:00:32.931Z</updated>
        <summary type="html"><![CDATA[Low-rank tensor decomposition generalizes low-rank matrix approximation and
is a powerful technique for discovering low-dimensional structure in
high-dimensional data. In this paper, we study Tucker decompositions and use
tools from randomized numerical linear algebra called ridge leverage scores to
accelerate the core tensor update step in the widely-used alternating least
squares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,
is a highly-structured ridge regression problem where the design matrix is a
Kronecker product of the factor matrices. We show how to use approximate ridge
leverage scores to construct a sketched instance for any ridge regression
problem such that the solution vector for the sketched problem is a
$(1+\varepsilon)$-approximation to the original instance. Moreover, we show
that classical leverage scores suffice as an approximation, which then allows
us to exploit the Kronecker structure and update the core tensor in time that
depends predominantly on the rank and the sketching parameters (i.e., sublinear
in the size of the input tensor). We also give upper bounds for ridge leverage
scores as rows are removed from the design matrix (e.g., if the tensor has
missing entries), and we demonstrate the effectiveness of our approximate ridge
regressioni algorithm for large, low-rank Tucker decompositions on both
synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fahrbach_M/0/1/0/all/0/1"&gt;Matthew Fahrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghadiri_M/0/1/0/all/0/1"&gt;Mehrdad Ghadiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1"&gt;Thomas Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Search for Efficient Active Learning and Search of Rare Concepts. (arXiv:2007.00077v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00077</id>
        <link href="http://arxiv.org/abs/2007.00077"/>
        <updated>2021-07-23T02:00:32.913Z</updated>
        <summary type="html"><![CDATA[Many active learning and search approaches are intractable for large-scale
industrial settings with billions of unlabeled examples. Existing approaches
search globally for the optimal examples to label, scaling linearly or even
quadratically with the unlabeled data. In this paper, we improve the
computational efficiency of active learning and search methods by restricting
the candidate pool for labeling to the nearest neighbors of the currently
labeled set instead of scanning over all of the unlabeled data. We evaluate
several selection strategies in this setting on three large-scale computer
vision datasets: ImageNet, OpenImages, and a de-identified and aggregated
dataset of 10 billion images provided by a large internet company. Our approach
achieved similar mean average precision and recall as the traditional global
approach while reducing the computational cost of selection by up to three
orders of magnitude, thus enabling web-scale active learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1"&gt;Cody Coleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_E/0/1/0/all/0/1"&gt;Edward Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1"&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1"&gt;Sean Culatana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1"&gt;Peter Bailis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1"&gt;Alexander C. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sumbaly_R/0/1/0/all/0/1"&gt;Roshan Sumbaly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yalniz_I/0/1/0/all/0/1"&gt;I. Zeki Yalniz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10493</id>
        <link href="http://arxiv.org/abs/2107.10493"/>
        <updated>2021-07-23T02:00:32.905Z</updated>
        <summary type="html"><![CDATA[Abstract reasoning, i.e., inferring complicated patterns from given
observations, is a central building block of artificial general intelligence.
While humans find the answer by either eliminating wrong candidates or first
constructing the answer, prior deep neural network (DNN)-based methods focus on
the former discriminative approach. This paper aims to design a framework for
the latter approach and bridge the gap between artificial and human
intelligence. To this end, we propose logic-guided generation (LoGe), a novel
generative DNN framework that reduces abstract reasoning as an optimization
problem in propositional logic. LoGe is composed of three steps: extract
propositional variables from images, reason the answer variables with a logic
layer, and reconstruct the answer image from the variables. We demonstrate that
LoGe outperforms the black box DNN frameworks for generative abstract reasoning
under the RAVEN benchmark, i.e., reconstructing answers based on capturing
correct rules of various attributes from observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sihyun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity. (arXiv:2107.10670v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.10670</id>
        <link href="http://arxiv.org/abs/2107.10670"/>
        <updated>2021-07-23T02:00:32.899Z</updated>
        <summary type="html"><![CDATA[Drug discovery often relies on the successful prediction of protein-ligand
binding affinity. Recent advances have shown great promise in applying graph
neural networks (GNNs) for better affinity prediction by learning the
representations of protein-ligand complexes. However, existing solutions
usually treat protein-ligand complexes as topological graph data, thus the
biomolecular structural information is not fully utilized. The essential
long-range interactions among atoms are also neglected in GNN models. To this
end, we propose a structure-aware interactive graph neural network (SIGN) which
consists of two components: polar-inspired graph attention layers (PGAL) and
pairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs
the node-edge aggregation process to update embeddings of nodes and edges while
preserving the distance and angle information among atoms. Then, PiPool is
adopted to gather interactive edges with a subsequent reconstruction loss to
reflect the global interactions. Exhaustive experimental study on two
benchmarks verifies the superiority of SIGN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuangli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingbo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weili Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning on Non-IID Data Silos: An Experimental Study. (arXiv:2102.02079v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02079</id>
        <link href="http://arxiv.org/abs/2102.02079"/>
        <updated>2021-07-23T02:00:32.881Z</updated>
        <summary type="html"><![CDATA[Due to the increasing privacy concerns and data regulations, training data
have been increasingly fragmented, forming distributed databases of multiple
``data silos'' (e.g., within different organizations and countries). To develop
effective machine learning services, there is a must to exploit data from such
distributed databases without exchanging the raw data. Recently, federated
learning (FL) has been a solution with growing interests, which enables
multiple parties to collaboratively train a machine learning model without
exchanging their local data. A key and common challenge on distributed
databases is the heterogeneity of the data distribution (i.e., non-IID) among
the parties. There have been many FL algorithms to address the learning
effectiveness under non-IID data settings. However, there lacks an experimental
study on systematically understanding their advantages and disadvantages, as
previous studies have very rigid data partitioning strategies among parties,
which are hardly representative and thorough. In this paper, to help
researchers better understand and study the non-IID data setting in federated
learning, we propose comprehensive data partitioning strategies to cover the
typical non-IID data cases. Moreover, we conduct extensive experiments to
evaluate state-of-the-art FL algorithms. We find that non-IID does bring
significant challenges in learning accuracy of FL algorithms, and none of the
existing state-of-the-art FL algorithms outperforms others in all cases. Our
experiments provide insights for future studies of addressing the challenges in
``data silos''.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1"&gt;Yiqun Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Quan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bingsheng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Progressive Deep Metric Learning. (arXiv:1805.05510v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.05510</id>
        <link href="http://arxiv.org/abs/1805.05510"/>
        <updated>2021-07-23T02:00:32.874Z</updated>
        <summary type="html"><![CDATA[Metric learning especially deep metric learning has been widely developed for
large-scale image inputs data. However, in many real-world applications, we can
only have access to vectorized inputs data. Moreover, on one hand, well-labeled
data is usually limited due to the high annotation cost. On the other hand, the
real data is commonly streaming data, which requires to be processed online. In
these scenarios, the fashionable deep metric learning is not suitable anymore.
To this end, we reconsider the traditional shallow online metric learning and
newly develop an online progressive deep metric learning (ODML) framework to
construct a metric-algorithm-based deep network. Specifically, we take an
online metric learning algorithm as a metric-algorithm-based layer (i.e.,
metric layer), followed by a nonlinear layer, and then stack these layers in a
fashion similar to deep learning. Different from the shallow online metric
learning, which can only learn one metric space (feature transformation), the
proposed ODML is able to learn multiple hierarchical metric spaces.
Furthermore, in a progressively and nonlinearly learning way, ODML has a
stronger learning ability than traditional shallow online metric learning in
the case of limited available training data. To make the learning process more
explainable and theoretically guaranteed, we also provide theoretical analysis.
The proposed ODML enjoys several nice properties and can indeed learn a metric
progressively and performs better on the benchmark datasets. Extensive
experiments with different settings have been conducted to verify these
properties of the proposed ODML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1"&gt;Jing Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion. (arXiv:2107.10394v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.10394</id>
        <link href="http://arxiv.org/abs/2107.10394"/>
        <updated>2021-07-23T02:00:32.866Z</updated>
        <summary type="html"><![CDATA[We present an unsupervised non-parallel many-to-many voice conversion (VC)
method using a generative adversarial network (GAN) called StarGAN v2. Using a
combination of adversarial source classifier loss and perceptual loss, our
model significantly outperforms previous VC models. Although our model is
trained only with 20 English speakers, it generalizes to a variety of voice
conversion tasks, such as any-to-many, cross-lingual, and singing conversion.
Using a style encoder, our framework can also convert plain reading speech into
stylistic speech, such as emotional and falsetto speech. Subjective and
objective evaluation experiments on a non-parallel many-to-many voice
conversion task revealed that our model produces natural sounding voices, close
to the sound quality of state-of-the-art text-to-speech (TTS) based voice
conversion methods without the need for text labels. Moreover, our model is
completely convolutional and with a faster-than-real-time vocoder such as
Parallel WaveGAN can perform real-time voice conversion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yinghao Aaron Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1"&gt;Ali Zare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mesgarani_N/0/1/0/all/0/1"&gt;Nima Mesgarani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Saddle-Point Problems Under Similarity. (arXiv:2107.10706v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.10706</id>
        <link href="http://arxiv.org/abs/2107.10706"/>
        <updated>2021-07-23T02:00:32.859Z</updated>
        <summary type="html"><![CDATA[We study solution methods for (strongly-)convex-(strongly)-concave
Saddle-Point Problems (SPPs) over networks of two type - master/workers (thus
centralized) architectures and meshed (thus decentralized) networks. The local
functions at each node are assumed to be similar, due to statistical data
similarity or otherwise. We establish lower complexity bounds for a fairly
general class of algorithms solving the SPP. We show that a given suboptimality
$\epsilon>0$ is achieved over master/workers networks in
$\Omega\big(\Delta\cdot \delta/\mu\cdot \log (1/\varepsilon)\big)$ rounds of
communications, where $\delta>0$ measures the degree of similarity of the local
functions, $\mu$ is their strong convexity constant, and $\Delta$ is the
diameter of the network. The lower communication complexity bound over meshed
networks reads $\Omega\big(1/{\sqrt{\rho}} \cdot {\delta}/{\mu}\cdot\log
(1/\varepsilon)\big)$, where $\rho$ is the (normalized) eigengap of the gossip
matrix used for the communication between neighbouring nodes. We then propose
algorithms matching the lower bounds over either types of networks (up to
log-factors). We assess the effectiveness of the proposed algorithms on a
robust logistic regression problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1"&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Scutari_G/0/1/0/all/0/1"&gt;Gesualdo Scutari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rogozin_A/0/1/0/all/0/1"&gt;Alexander Rogozin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An overcome of far-distance limitation on tunnel CCTV-based accident detection in AI deep-learning frameworks. (arXiv:2107.10567v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10567</id>
        <link href="http://arxiv.org/abs/2107.10567"/>
        <updated>2021-07-23T02:00:32.853Z</updated>
        <summary type="html"><![CDATA[Tunnel CCTVs are installed to low height and long-distance interval. However,
because of the limitation of installation height, severe perspective effect in
distance occurs, and it is almost impossible to detect vehicles in far distance
from the CCTV in the existing tunnel CCTV-based accident detection system
(Pflugfelder 2005). To overcome the limitation, a vehicle object is detected
through an object detection algorithm based on an inverse perspective transform
by re-setting the region of interest (ROI). It can detect vehicles that are far
away from the CCTV. To verify this process, this paper creates each dataset
consisting of images and bounding boxes based on the original and warped images
of the CCTV at the same time, and then compares performance of the deep
learning object detection models trained with the two datasets. As a result,
the model that trained the warped image was able to detect vehicle objects more
accurately at the position far from the CCTV compared to the model that trained
the original image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyu-Beom Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shin_H/0/1/0/all/0/1"&gt;Hyu-Soung Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks. (arXiv:2006.05066v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05066</id>
        <link href="http://arxiv.org/abs/2006.05066"/>
        <updated>2021-07-23T02:00:32.848Z</updated>
        <summary type="html"><![CDATA[Modern convolutional neural networks (CNNs) have massive identical
convolution blocks, and, hence, recursive sharing of parameters across these
blocks has been proposed to reduce the amount of parameters. However, naive
sharing of parameters poses many challenges such as limited representational
power and the vanishing/exploding gradients problem of recursively shared
parameters. In this paper, we present a recursive convolution block design and
training method, in which a recursively shareable part, or a filter basis, is
separated and learned while effectively avoiding the vanishing/exploding
gradients problem during training. We show that the unwieldy
vanishing/exploding gradients problem can be controlled by enforcing the
elements of the filter basis orthonormal, and empirically demonstrate that the
proposed orthogonality regularization improves the flow of gradients during
training. Experimental results on image classification and object detection
show that our approach, unlike previous parameter-sharing approaches, does not
trade performance to save parameters and consistently outperforms
overparameterized counterpart networks. This superior performance demonstrates
that the proposed recursive convolution block design and the orthogonality
regularization not only prevent performance degradation, but also consistently
improve the representation capability while a significant amount of parameters
are recursively shared.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Woochul Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyeon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03040</id>
        <link href="http://arxiv.org/abs/2001.03040"/>
        <updated>2021-07-23T02:00:32.841Z</updated>
        <summary type="html"><![CDATA[This paper establishes the optimal approximation error characterization of
deep ReLU networks for smooth functions in terms of both width and depth
simultaneously. To that end, we first prove that multivariate polynomials can
be approximated by deep ReLU networks of width $\mathcal{O}(N)$ and depth
$\mathcal{O}(L)$ with an approximation error $\mathcal{O}(N^{-L})$. Through
local Taylor expansions and their deep ReLU network approximations, we show
that deep ReLU networks of width $\mathcal{O}(N\ln N)$ and depth
$\mathcal{O}(L\ln L)$ can approximate $f\in C^s([0,1]^d)$ with a nearly optimal
approximation error $\mathcal{O}(\|f\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our
estimate is non-asymptotic in the sense that it is valid for arbitrary width
and depth specified by $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuowei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.10845</id>
        <link href="http://arxiv.org/abs/2107.10845"/>
        <updated>2021-07-23T02:00:32.834Z</updated>
        <summary type="html"><![CDATA[Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)
computers. Limited research efforts have explored a higher level of
optimization by making the quantum circuit resilient to noise. We propose and
experimentally implement QuantumNAS, the first comprehensive framework for
noise-adaptive co-search of variational circuit and qubit mapping. Variational
quantum circuits are a promising approach for constructing quantum neural
networks for machine learning and variational ansatzes for quantum simulation.
However, finding the best variational circuit and its optimal parameters is
challenging in a high-dimensional Hilbert space. We propose to decouple the
parameter training and circuit search by introducing a novel gate-sharing
SuperCircuit. The SuperCircuit is trained by sampling and updating the
SubCircuits in it and provides an accurate estimation of SubCircuit performance
trained from scratch. Then we perform an evolutionary co-search of SubCircuit
and its qubit mapping. The SubCircuit performance is estimated with parameters
inherited from SuperCircuit and simulated with real device noise models.
Finally, we perform iterative gate pruning and finetuning to further remove the
redundant gates in a fine-grained manner.

Extensively evaluated with 12 QML and VQE benchmarks on 10 quantum computers,
QuantumNAS significantly outperforms noise-unaware search, human and random
baselines. For QML tasks, QuantumNAS is the first to demonstrate over 95%
2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum
computers. It also achieves the lowest eigenvalue for VQE tasks on H2, H2O,
LiH, CH4, BeH2 compared with UCCSD baselines. We also open-source QuantumEngine
(https://github.com/mit-han-lab/pytorch-quantum) for fast training of
parameterized quantum circuits to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanrui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yongshan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiaqi Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yujun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1"&gt;David Z. Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1"&gt;Frederic T. Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Vessel Enhancement Using Flow-Based Consistencies. (arXiv:2101.05145v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05145</id>
        <link href="http://arxiv.org/abs/2101.05145"/>
        <updated>2021-07-23T02:00:32.810Z</updated>
        <summary type="html"><![CDATA[Vessel segmentation is an essential task in many clinical applications.
Although supervised methods have achieved state-of-art performance, acquiring
expert annotation is laborious and mostly limited for two-dimensional datasets
with a small sample size. On the contrary, unsupervised methods rely on
handcrafted features to detect tube-like structures such as vessels. However,
those methods require complex pipelines involving several hyper-parameters and
design choices rendering the procedure sensitive, dataset-specific, and not
generalizable. We propose a self-supervised method with a limited number of
hyper-parameters that is generalizable across modalities. Our method uses
tube-like structure properties, such as connectivity, profile consistency, and
bifurcation, to introduce inductive bias into a learning algorithm. To model
those properties, we generate a vector field that we refer to as a flow. Our
experiments on various public datasets in 2D and 3D show that our method
performs better than unsupervised methods while learning useful transferable
features from unlabeled data. Unlike generic self-supervised methods, the
learned features learn vessel-relevant features that are transferable for
supervised approaches, which is essential when the number of annotated data is
limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1"&gt;Rohit Jena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sumedha Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1"&gt;Kayhan Batmanghelich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Networks: Relating Inputs through Vector-Symbolic Manipulations. (arXiv:2106.08446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08446</id>
        <link href="http://arxiv.org/abs/2106.08446"/>
        <updated>2021-07-23T02:00:32.798Z</updated>
        <summary type="html"><![CDATA[Despite rapid progress, current deep learning methods face a number of
critical challenges. These include high energy consumption, catastrophic
forgetting, dependance on global losses, and an inability to reason
symbolically. By combining concepts from information bottleneck theory and
vector-symbolic architectures, we propose and implement a novel information
processing architecture, the 'Bridge network.' We show this architecture
provides unique advantages which can address the problem of global losses and
catastrophic forgetting. Furthermore, we argue that it provides a further basis
for increasing energy efficiency of execution and the ability to reason
symbolically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1"&gt;Wilkie Olin-Ammentorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1"&gt;Maxim Bazhenov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights. (arXiv:2007.00864v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00864</id>
        <link href="http://arxiv.org/abs/2007.00864"/>
        <updated>2021-07-23T02:00:32.789Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models are widely used in many important domains. For
efficiently processing these computational- and memory-intensive applications,
tensors of these over-parameterized models are compressed by leveraging
sparsity, size reduction, and quantization of tensors. Unstructured sparsity
and tensors with varying dimensions yield irregular computation, communication,
and memory access patterns; processing them on hardware accelerators in a
conventional manner does not inherently leverage acceleration opportunities.
This paper provides a comprehensive survey on the efficient execution of sparse
and irregular tensor computations of ML models on hardware accelerators. In
particular, it discusses enhancement modules in the architecture design and the
software support; categorizes different hardware designs and acceleration
techniques and analyzes them in terms of hardware and execution costs; analyzes
achievable accelerations for recent DNNs; highlights further opportunities in
terms of hardware/software/model co-design optimizations (inter/intra-module).
The takeaways from this paper include: understanding the key challenges in
accelerating sparse, irregular-shaped, and quantized tensors; understanding
enhancements in accelerator systems for supporting their efficient
computations; analyzing trade-offs in opting for a specific design choice for
encoding, storing, extracting, communicating, computing, and load-balancing the
non-zeros; understanding how structured sparsity can improve storage efficiency
and balance computations; understanding how to compile and map models with
sparse tensors on the accelerators; understanding recent design trends for
efficient accelerations and further opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1"&gt;Shail Dave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1"&gt;Riyadh Baghdadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowatzki_T/0/1/0/all/0/1"&gt;Tony Nowatzki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1"&gt;Sasikanth Avancha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Aviral Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baoxin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02549</id>
        <link href="http://arxiv.org/abs/2106.02549"/>
        <updated>2021-07-23T02:00:32.763Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1"&gt;Thorben Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating and Blending Game Levels via Quality-Diversity in the Latent Space of a Variational Autoencoder. (arXiv:2102.12463v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12463</id>
        <link href="http://arxiv.org/abs/2102.12463"/>
        <updated>2021-07-23T02:00:32.745Z</updated>
        <summary type="html"><![CDATA[Several works have demonstrated the use of variational autoencoders (VAEs)
for generating levels in the style of existing games and blending levels across
different games. Further, quality-diversity (QD) algorithms have also become
popular for generating varied game content by using evolution to explore a
search space while focusing on both variety and quality. To reap the benefits
of both these approaches, we present a level generation and game blending
approach that combines the use of VAEs and QD algorithms. Specifically, we
train VAEs on game levels and run the MAP-Elites QD algorithm using the learned
latent space of the VAE as the search space. The latent space captures the
properties of the games whose levels we want to generate and blend, while
MAP-Elites searches this latent space to find a diverse set of levels
optimizing a given objective such as playability. We test our method using
models for 5 different platformer games as well as a blended domain spanning 3
of these games. We refer to using MAP-Elites for blending as Blend-Elites. Our
results show that MAP-Elites in conjunction with VAEs enables the generation of
a diverse set of playable levels not just for each individual game but also for
the blended domain while illuminating game-specific regions of the blended
latent space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Anurag Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1"&gt;Seth Cooper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Quadratic Optimization with Reinforcement Learning. (arXiv:2107.10847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10847</id>
        <link href="http://arxiv.org/abs/2107.10847"/>
        <updated>2021-07-23T02:00:32.728Z</updated>
        <summary type="html"><![CDATA[First-order methods for quadratic optimization such as OSQP are widely used
for large-scale machine learning and embedded optimal control, where many
related problems must be rapidly solved. These methods face two persistent
challenges: manual hyperparameter tuning and convergence time to high-accuracy
solutions. To address these, we explore how Reinforcement Learning (RL) can
learn a policy to tune parameters to accelerate convergence. In experiments
with well-known QP benchmarks we find that our RL policy, RLQP, significantly
outperforms state-of-the-art QP solvers by up to 3x. RLQP generalizes
surprisingly well to previously unseen problems with varying dimension and
structure from different applications, including the QPLIB, Netlib LP and
Maros-Meszaros problems. Code for RLQP is available at
https://github.com/berkeleyautomation/rlqp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeffrey Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Paras Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stellato_B/0/1/0/all/0/1"&gt;Bartolomeo Stellato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banjac_G/0/1/0/all/0/1"&gt;Goran Banjac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Michael Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borrelli_F/0/1/0/all/0/1"&gt;Francesco Borrelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification. (arXiv:2107.10474v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10474</id>
        <link href="http://arxiv.org/abs/2107.10474"/>
        <updated>2021-07-23T02:00:32.722Z</updated>
        <summary type="html"><![CDATA[Language models (LMs) pretrained on a large text corpus and fine-tuned on a
downstream text corpus and fine-tuned on a downstream task becomes a de facto
training strategy for several natural language processing (NLP) tasks.
Recently, an adaptive pretraining method retraining the pretrained language
model with task-relevant data has shown significant performance improvements.
However, current adaptive pretraining methods suffer from underfitting on the
task distribution owing to a relatively small amount of data to re-pretrain the
LM. To completely use the concept of adaptive pretraining, we propose a
back-translated task-adaptive pretraining (BT-TAPT) method that increases the
amount of task-specific data for LM re-pretraining by augmenting the task data
using back-translation to generalize the LM to the target task domain. The
experimental results show that the proposed BT-TAPT yields improved
classification accuracy on both low- and high-resource data and better
robustness to noise than the conventional adaptive pretraining method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junghoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jounghee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1"&gt;Pilsung Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[cCorrGAN: Conditional Correlation GAN for Learning Empirical Conditional Distributions in the Elliptope. (arXiv:2107.10606v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.10606</id>
        <link href="http://arxiv.org/abs/2107.10606"/>
        <updated>2021-07-23T02:00:32.715Z</updated>
        <summary type="html"><![CDATA[We propose a methodology to approximate conditional distributions in the
elliptope of correlation matrices based on conditional generative adversarial
networks. We illustrate the methodology with an application from quantitative
finance: Monte Carlo simulations of correlated returns to compare risk-based
portfolio construction methods. Finally, we discuss about current limitations
and advocate for further exploration of the elliptope geometry to improve
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Marti_G/0/1/0/all/0/1"&gt;Gautier Marti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Goubet_V/0/1/0/all/0/1"&gt;Victor Goubet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Cox Mixtures for Survival Regression. (arXiv:2101.06536v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06536</id>
        <link href="http://arxiv.org/abs/2101.06536"/>
        <updated>2021-07-23T02:00:32.696Z</updated>
        <summary type="html"><![CDATA[Survival analysis is a challenging variation of regression modeling because
of the presence of censoring, where the outcome measurement is only partially
known, due to, for example, loss to follow up. Such problems come up frequently
in medical applications, making survival analysis a key endeavor in
biostatistics and machine learning for healthcare, with Cox regression models
being amongst the most commonly employed models. We describe a new approach for
survival analysis regression models, based on learning mixtures of Cox
regressions to model individual survival distributions. We propose an
approximation to the Expectation Maximization algorithm for this model that
does hard assignments to mixture groups to make optimization efficient. In each
group assignment, we fit the hazard ratios within each group using deep neural
networks, and the baseline hazard for each mixture component
non-parametrically.

We perform experiments on multiple real world datasets, and look at the
mortality rates of patients across ethnicity and gender. We emphasize the
importance of calibration in healthcare settings and demonstrate that our
approach outperforms classical and modern survival analysis baselines, both in
terms of discriminative performance and calibration, with large gains in
performance on the minority demographics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1"&gt;Chirag Nagpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1"&gt;Steve Yadlowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamzadeh_N/0/1/0/all/0/1"&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1"&gt;Katherine Heller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-ensemble: Improving Generalization through Model Ensembling in Federated Learning. (arXiv:2107.10663v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10663</id>
        <link href="http://arxiv.org/abs/2107.10663"/>
        <updated>2021-07-23T02:00:32.688Z</updated>
        <summary type="html"><![CDATA[In this paper we propose Fed-ensemble: a simple approach that bringsmodel
ensembling to federated learning (FL). Instead of aggregating localmodels to
update a single global model, Fed-ensemble uses random permutations to update a
group of K models and then obtains predictions through model averaging.
Fed-ensemble can be readily utilized within established FL methods and does not
impose a computational overhead as it only requires one of the K models to be
sent to a client in each communication round. Theoretically, we show that
predictions on newdata from all K models belong to the same predictive
posterior distribution under a neural tangent kernel regime. This result in
turn sheds light onthe generalization advantages of model averaging. We also
illustrate thatFed-ensemble has an elegant Bayesian interpretation. Empirical
results show that our model has superior performance over several FL
algorithms,on a wide range of data sets, and excels in heterogeneous settings
often encountered in FL applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_N/0/1/0/all/0/1"&gt;Naichen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lai_F/0/1/0/all/0/1"&gt;Fan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kontar_R/0/1/0/all/0/1"&gt;Raed Al Kontar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mosharaf Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum Residual Neural Networks. (arXiv:2102.07870v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07870</id>
        <link href="http://arxiv.org/abs/2102.07870"/>
        <updated>2021-07-23T02:00:32.681Z</updated>
        <summary type="html"><![CDATA[The training of deep residual neural networks (ResNets) with backpropagation
has a memory cost that increases linearly with respect to the depth of the
network. A way to circumvent this issue is to use reversible architectures. In
this paper, we propose to change the forward rule of a ResNet by adding a
momentum term. The resulting networks, momentum residual neural networks
(Momentum ResNets), are invertible. Unlike previous invertible architectures,
they can be used as a drop-in replacement for any existing ResNet block. We
show that Momentum ResNets can be interpreted in the infinitesimal step size
regime as second-order ordinary differential equations (ODEs) and exactly
characterize how adding momentum progressively increases the representation
capabilities of Momentum ResNets. Our analysis reveals that Momentum ResNets
can learn any linear mapping up to a multiplicative factor, while ResNets
cannot. In a learning to optimize setting, where convergence to a fixed point
is required, we show theoretically and empirically that our method succeeds
while existing invertible architectures fail. We show on CIFAR and ImageNet
that Momentum ResNets have the same accuracy as ResNets, while having a much
smaller memory footprint, and show that pre-trained Momentum ResNets are
promising for fine-tuning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sander_M/0/1/0/all/0/1"&gt;Michael E. Sander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ablin_P/0/1/0/all/0/1"&gt;Pierre Ablin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1"&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1"&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness Guarantees for Random Deep Neural Networks. (arXiv:2004.05923v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05923</id>
        <link href="http://arxiv.org/abs/2004.05923"/>
        <updated>2021-07-23T02:00:32.675Z</updated>
        <summary type="html"><![CDATA[The reliability of deep learning algorithms is fundamentally challenged by
the existence of adversarial examples, which are incorrectly classified inputs
that are extremely close to a correctly classified input. We explore the
properties of adversarial examples for deep neural networks with random weights
and biases, and prove that for any $p\ge1$, the $\ell^p$ distance of any given
input from the classification boundary scales as one over the square root of
the dimension of the input times the $\ell^p$ norm of the input. The results
are based on the recently proved equivalence between Gaussian processes and
deep neural networks in the limit of infinite width of the hidden layers, and
are validated with experiments on both random deep neural networks and deep
neural networks trained on the MNIST and CIFAR10 datasets. The results
constitute a fundamental advance in the theoretical understanding of
adversarial examples, and open the way to a thorough theoretical
characterization of the relation between network architecture and robustness to
adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Palma_G/0/1/0/all/0/1"&gt;Giacomo De Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kiani_B/0/1/0/all/0/1"&gt;Bobak T. Kiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lloyd_S/0/1/0/all/0/1"&gt;Seth Lloyd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13329</id>
        <link href="http://arxiv.org/abs/2106.13329"/>
        <updated>2021-07-23T02:00:32.668Z</updated>
        <summary type="html"><![CDATA[We present two sample-efficient differentially private mean estimators for
$d$-dimensional (sub)Gaussian distributions with unknown covariance.
Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with
mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that
$\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is
the Mahalanobis distance. All previous estimators with the same guarantee
either require strong a priori bounds on the covariance matrix or require
$\Omega(d^{3/2})$ samples.

Each of our estimators is based on a simple, general approach to designing
differentially private mechanisms, but with novel technical steps to make the
estimator private and sample-efficient. Our first estimator samples a point
with approximately maximum Tukey depth using the exponential mechanism, but
restricted to the set of points of large Tukey depth. Proving that this
mechanism is private requires a novel analysis. Our second estimator perturbs
the empirical mean of the data set with noise calibrated to the empirical
covariance, without releasing the covariance itself. Its sample complexity
guarantees hold more generally for subgaussian distributions, albeit with a
slightly worse dependence on the privacy parameter. For both estimators,
careful preprocessing of the data is required to satisfy differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1"&gt;Gavin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Adam Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1"&gt;Jonathan Ullman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1"&gt;Lydia Zakynthinou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Variational Gradient Descent. (arXiv:2107.10731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10731</id>
        <link href="http://arxiv.org/abs/2107.10731"/>
        <updated>2021-07-23T02:00:32.650Z</updated>
        <summary type="html"><![CDATA[Particle-based approximate Bayesian inference approaches such as Stein
Variational Gradient Descent (SVGD) combine the flexibility and convergence
guarantees of sampling methods with the computational benefits of variational
inference. In practice, SVGD relies on the choice of an appropriate kernel
function, which impacts its ability to model the target distribution -- a
challenging problem with only heuristic solutions. We propose Neural
Variational Gradient Descent (NVGD), which is based on parameterizing the
witness function of the Stein discrepancy by a deep neural network whose
parameters are learned in parallel to the inference, mitigating the necessity
to make any kernel choices whatsoever. We empirically evaluate our method on
popular synthetic inference problems, real-world Bayesian linear regression,
and Bayesian neural network inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1"&gt;Lauro Langosco di Langosco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1"&gt;Heiko Strathmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning in Incomplete Label Multiple Instance Multiple Label Learning. (arXiv:2107.10804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10804</id>
        <link href="http://arxiv.org/abs/2107.10804"/>
        <updated>2021-07-23T02:00:32.642Z</updated>
        <summary type="html"><![CDATA[In multiple instance multiple label learning, each sample, a bag, consists of
multiple instances. To alleviate labeling complexity, each sample is associated
with a set of bag-level labels leaving instances within the bag unlabeled. This
setting is more convenient and natural for representing complicated objects,
which have multiple semantic meanings. Compared to single instance labeling,
this approach allows for labeling larger datasets at an equivalent labeling
cost. However, for sufficiently large datasets, labeling all bags may become
prohibitively costly. Active learning uses an iterative labeling and retraining
approach aiming to provide reasonable classification performance using a small
number of labeled samples. To our knowledge, only a few works in the area of
active learning in the MIML setting are available. These approaches can provide
practical solutions to reduce labeling cost but their efficacy remains unclear.
In this paper, we propose a novel bag-class pair based approach for active
learning in the MIML setting. Due to the partial availability of bag-level
labels, we focus on the incomplete-label MIML setting for the proposed active
learning approach. Our approach is based on a discriminative graphical model
with efficient and exact inference. For the query process, we adapt active
learning criteria to the novel bag-class pair selection strategy. Additionally,
we introduce an online stochastic gradient descent algorithm to provide an
efficient model update after each query. Numerical experiments on benchmark
datasets illustrate the robustness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tam Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raich_R/0/1/0/all/0/1"&gt;Raviv Raich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Samplets: A new paradigm for data compression. (arXiv:2107.03337v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03337</id>
        <link href="http://arxiv.org/abs/2107.03337"/>
        <updated>2021-07-23T02:00:32.636Z</updated>
        <summary type="html"><![CDATA[In this article, we introduce the concept of samplets by transferring the
construction of Tausch-White wavelets to the realm of data. This way we obtain
a multilevel representation of discrete data which directly enables data
compression, detection of singularities and adaptivity. Applying samplets to
represent kernel matrices, as they arise in kernel based learning or Gaussian
process regression, we end up with quasi-sparse matrices. By thresholding small
entries, these matrices are compressible to O(N log N) relevant entries, where
N is the number of data points. This feature allows for the use of fill-in
reducing reorderings to obtain a sparse factorization of the compressed
matrices. Besides the comprehensive introduction to samplets and their
properties, we present extensive numerical studies to benchmark the approach.
Our results demonstrate that samplets mark a considerable step in the direction
of making large data sets accessible for analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1"&gt;Helmut Harbrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1"&gt;Michael Multerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable tradeoffs in adversarially robust classification. (arXiv:2006.05161v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05161</id>
        <link href="http://arxiv.org/abs/2006.05161"/>
        <updated>2021-07-23T02:00:32.629Z</updated>
        <summary type="html"><![CDATA[It is well known that machine learning methods can be vulnerable to
adversarially-chosen perturbations of their inputs. Despite significant
progress in the area, foundational open problems remain. In this paper, we
address several key questions. We derive exact and approximate Bayes-optimal
robust classifiers for the important setting of two- and three-class Gaussian
classification problems with arbitrary imbalance, for $\ell_2$ and
$\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers,
determining the optimal decisions here cannot be made pointwise and new
theoretical approaches are needed. We develop and leverage new tools, including
recent breakthroughs from probability theory on robust isoperimetry, which, to
our knowledge, have not yet been used in the area. Our results reveal
fundamental tradeoffs between standard and robust accuracy that grow when data
is imbalanced. We also show further results, including an analysis of
classification calibration for convex losses in certain models, and finite
sample rates for the robust risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1"&gt;Hamed Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;David Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion recognition by fusing time synchronous and time asynchronous representations. (arXiv:2010.14102v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14102</id>
        <link href="http://arxiv.org/abs/2010.14102"/>
        <updated>2021-07-23T02:00:32.606Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel two-branch neural network model structure is proposed
for multimodal emotion recognition, which consists of a time synchronous branch
(TSB) and a time asynchronous branch (TAB). To capture correlations between
each word and its acoustic realisation, the TSB combines speech and text
modalities at each input window frame and then does pooling across time to form
a single embedding vector. The TAB, by contrast, provides cross-utterance
information by integrating sentence text embeddings from a number of context
utterances into another embedding vector. The final emotion classification uses
both the TSB and the TAB embeddings. Experimental results on the IEMOCAP
dataset demonstrate that the two-branch structure achieves state-of-the-art
results in 4-way classification with all common test setups. When using
automatic speech recognition (ASR) output instead of manually transcribed
reference text, it is shown that the cross-utterance information considerably
improves the robustness against ASR errors. Furthermore, by incorporating an
extra class for all the other emotions, the final 5-way classification system
with ASR hypotheses can be viewed as a prototype for more realistic emotion
recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?. (arXiv:2012.06421v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06421</id>
        <link href="http://arxiv.org/abs/2012.06421"/>
        <updated>2021-07-23T02:00:32.599Z</updated>
        <summary type="html"><![CDATA[Modern machine learning models are complex and frequently encode surprising
amounts of information about individual inputs. In extreme cases, complex
models appear to memorize entire input examples, including seemingly irrelevant
information (social security numbers from text, for example). In this paper, we
aim to understand whether this sort of memorization is necessary for accurate
learning. We describe natural prediction problems in which every sufficiently
accurate training algorithm must encode, in the prediction model, essentially
all the information about a large subset of its training examples. This remains
true even when the examples are high-dimensional and have entropy much higher
than the sample size, and even when most of that information is ultimately
irrelevant to the task at hand. Further, our results do not depend on the
training algorithm or the class of models used for learning.

Our problems are simple and fairly natural variants of the next-symbol
prediction and the cluster labeling tasks. These tasks can be seen as
abstractions of text- and image-related prediction problems. To establish our
results, we reduce from a family of one-way communication problems for which we
prove new information complexity lower bounds. Additionally, we present
synthetic-data experiments demonstrating successful attacks on logistic
regression and neural network classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1"&gt;Gavin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1"&gt;Mark Bun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1"&gt;Vitaly Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Adam Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"&gt;Kunal Talwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Efficient Pipeline-Parallel DNN Training. (arXiv:2006.09503v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09503</id>
        <link href="http://arxiv.org/abs/2006.09503"/>
        <updated>2021-07-23T02:00:32.589Z</updated>
        <summary type="html"><![CDATA[Many state-of-the-art ML results have been obtained by scaling up the number
of parameters in existing models. However, parameters and activations for such
large models often do not fit in the memory of a single accelerator device;
this means that it is necessary to distribute training of large models over
multiple accelerators. In this work, we propose PipeDream-2BW, a system that
supports memory-efficient pipeline parallelism. PipeDream-2BW uses a novel
pipelining and weight gradient coalescing strategy, combined with the double
buffering of weights, to ensure high throughput, low memory footprint, and
weight update semantics similar to data parallelism. In addition, PipeDream-2BW
automatically partitions the model over the available hardware resources, while
respecting hardware constraints such as memory capacities of accelerators and
interconnect topologies. PipeDream-2BW can accelerate the training of large GPT
and BERT language models by up to 20$\times$ with similar final model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1"&gt;Deepak Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1"&gt;Amar Phanishayee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kaiyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demand Forecasting of Individual Probability Density Functions with Machine Learning. (arXiv:2009.07052v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07052</id>
        <link href="http://arxiv.org/abs/2009.07052"/>
        <updated>2021-07-23T02:00:32.583Z</updated>
        <summary type="html"><![CDATA[Demand forecasting is a central component of the replenishment process for
retailers, as it provides crucial input for subsequent decision making like
ordering processes. In contrast to point estimates, such as the conditional
mean of the underlying probability distribution, or confidence intervals,
forecasting complete probability density functions allows to investigate the
impact on operational metrics, which are important to define the business
strategy, over the full range of the expected demand. Whereas metrics
evaluating point estimates are widely used, methods for assessing the accuracy
of predicted distributions are rare, and this work proposes new techniques for
both qualitative and quantitative evaluation methods. Using the supervised
machine learning method "Cyclic Boosting", complete individual probability
density functions can be predicted such that each prediction is fully
explainable. This is of particular importance for practitioners, as it allows
to avoid "black-box" models and understand the contributing factors for each
individual prediction. Another crucial aspect in terms of both explainability
and generalizability of demand forecasting methods is the limitation of the
influence of temporal confounding, which is prevalent in most state of the art
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wick_F/0/1/0/all/0/1"&gt;F. Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerzel_U/0/1/0/all/0/1"&gt;U. Kerzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1"&gt;M. Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_M/0/1/0/all/0/1"&gt;M. Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1"&gt;T. Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_D/0/1/0/all/0/1"&gt;D. Stemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernst_J/0/1/0/all/0/1"&gt;J. Ernst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feindt_M/0/1/0/all/0/1"&gt;M. Feindt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (arXiv:2107.10457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10457</id>
        <link href="http://arxiv.org/abs/2107.10457"/>
        <updated>2021-07-23T02:00:32.576Z</updated>
        <summary type="html"><![CDATA[To explore the robustness of recommender systems, researchers have proposed
various shilling attack models and analyzed their adverse effects. Primitive
attacks are highly feasible but less effective due to simplistic handcrafted
rules, while upgraded attacks are more powerful but costly and difficult to
deploy because they require more knowledge from recommendations. In this paper,
we explore a novel shilling attack called Graph cOnvolution-based generative
shilling ATtack (GOAT) to balance the attacks' feasibility and effectiveness.
GOAT adopts the primitive attacks' paradigm that assigns items for fake users
by sampling and the upgraded attacks' paradigm that generates fake ratings by a
deep learning-based model. It deploys a generative adversarial network (GAN)
that learns the real rating distribution to generate fake ratings.
Additionally, the generator combines a tailored graph convolution structure
that leverages the correlations between co-rated items to smoothen the fake
ratings and enhance their authenticity. The extensive experiments on two public
datasets evaluate GOAT's performance from multiple perspectives. Our study of
the GOAT demonstrates technical feasibility for building a more powerful and
intelligent attack model with a much-reduced cost, enables analysis the threat
of such an attack and guides for investigating necessary prevention measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Min Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Junliang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zongwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kecheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wange_X/0/1/0/all/0/1"&gt;Xu Wange&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning. (arXiv:2101.00554v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00554</id>
        <link href="http://arxiv.org/abs/2101.00554"/>
        <updated>2021-07-23T02:00:32.559Z</updated>
        <summary type="html"><![CDATA[Achieving accurate and robust global situational awareness of a complex
time-evolving field from a limited number of sensors has been a longstanding
challenge. This reconstruction problem is especially difficult when sensors are
sparsely positioned in a seemingly random or unorganized manner, which is often
encountered in a range of scientific and engineering problems. Moreover, these
sensors can be in motion and can become online or offline over time. The key
leverage in addressing this scientific issue is the wealth of data accumulated
from the sensors. As a solution to this problem, we propose a data-driven
spatial field recovery technique founded on a structured grid-based
deep-learning approach for arbitrary positioned sensors of any numbers. It
should be noted that the na\"ive use of machine learning becomes prohibitively
expensive for global field reconstruction and is furthermore not adaptable to
an arbitrary number of sensors. In the present work, we consider the use of
Voronoi tessellation to obtain a structured-grid representation from sensor
locations enabling the computationally tractable use of convolutional neural
networks. One of the central features of the present method is its
compatibility with deep-learning based super-resolution reconstruction
techniques for structured sensor data that are established for image
processing. The proposed reconstruction technique is demonstrated for unsteady
wake flow, geophysical data, and three-dimensional turbulence. The current
framework is able to handle an arbitrary number of moving sensors, and thereby
overcomes a major limitation with existing reconstruction methods. The
presented technique opens a new pathway towards the practical use of neural
networks for real-time global field estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Fukami_K/0/1/0/all/0/1"&gt;Kai Fukami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Maulik_R/0/1/0/all/0/1"&gt;Romit Maulik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ramachandra_N/0/1/0/all/0/1"&gt;Nesar Ramachandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Fukagata_K/0/1/0/all/0/1"&gt;Koji Fukagata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Taira_K/0/1/0/all/0/1"&gt;Kunihiko Taira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks. (arXiv:2107.10302v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.10302</id>
        <link href="http://arxiv.org/abs/2107.10302"/>
        <updated>2021-07-23T02:00:32.552Z</updated>
        <summary type="html"><![CDATA[Attacks from adversarial machine learning (ML) have the potential to be used
"for good": they can be used to run counter to the existing power structures
within ML, creating breathing space for those who would otherwise be the
targets of surveillance and control. But most research on adversarial ML has
not engaged in developing tools for resistance against ML systems. Why? In this
paper, we review the broader impact statements that adversarial ML researchers
wrote as part of their NeurIPS 2020 papers and assess the assumptions that
authors have about the goals of their work. We also collect information about
how authors view their work's impact more generally. We find that most
adversarial ML researchers at NeurIPS hold two fundamental assumptions that
will make it difficult for them to consider socially beneficial uses of
attacks: (1) it is desirable to make systems robust, independent of context,
and (2) attackers of systems are normatively bad and defenders of systems are
normatively good. That is, despite their expressed and supposed neutrality,
most adversarial ML researchers believe that the goal of their work is to
secure systems, making it difficult to conceptualize and build tools for
disrupting the status quo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albert_K/0/1/0/all/0/1"&gt;Kendra Albert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delano_M/0/1/0/all/0/1"&gt;Maggie Delano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1"&gt;Bogdan Kulynych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ram Shankar Siva Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity. (arXiv:2107.10790v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.10790</id>
        <link href="http://arxiv.org/abs/2107.10790"/>
        <updated>2021-07-23T02:00:32.546Z</updated>
        <summary type="html"><![CDATA[Machine learning methods, such as deep learning, show promising results in
the medical domain. However, the lack of interpretability of these algorithms
may hinder their applicability to medical decision support systems. This paper
studies an interpretable deep learning technique, called SincNet. SincNet is a
convolutional neural network that efficiently learns customized band-pass
filters through trainable sinc-functions. In this study, we use SincNet to
analyze the neural activity of individuals with Autism Spectrum Disorder (ASD),
who experience characteristic differences in neural oscillatory activity. In
particular, we propose a novel SincNet-based neural network for detecting
emotions in ASD patients using EEG signals. The learned filters can be easily
inspected to detect which part of the EEG spectrum is used for predicting
emotions. We found that our system automatically learns the high-$\alpha$ (9-13
Hz) and $\beta$ (13-30 Hz) band suppression often present in individuals with
ASD. This result is consistent with recent neuroscience studies on emotion
recognition, which found an association between these band suppressions and the
behavioral deficits observed in individuals with ASD. The improved
interpretability of SincNet is achieved without sacrificing performance in
emotion recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mayor_Torres_J/0/1/0/all/0/1"&gt;Juan Manuel Mayor-Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1"&gt;Mirco Ravanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Medina_DeVilliers_S/0/1/0/all/0/1"&gt;Sara E. Medina-DeVilliers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lerner_M/0/1/0/all/0/1"&gt;Matthew D. Lerner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riccardi_G/0/1/0/all/0/1"&gt;Giuseppe Riccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantifiable Dialogue Coherence Evaluation. (arXiv:2106.00507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00507</id>
        <link href="http://arxiv.org/abs/2106.00507"/>
        <updated>2021-07-23T02:00:32.538Z</updated>
        <summary type="html"><![CDATA[Automatic dialogue coherence evaluation has attracted increasing attention
and is crucial for developing promising dialogue systems. However, existing
metrics have two major limitations: (a) they are mostly trained in a simplified
two-level setting (coherent vs. incoherent), while humans give Likert-type
multi-level coherence scores, dubbed as "quantifiable"; (b) their predicted
coherence scores cannot align with the actual human rating standards due to the
absence of human guidance during training. To address these limitations, we
propose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel
framework aiming to train a quantifiable dialogue coherence metric that can
reflect the actual human rating standards. Specifically, QuantiDCE includes two
training stages, Multi-Level Ranking (MLR) pre-training and Knowledge
Distillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is
proposed for enabling the model to learn the coarse judgement of coherence
degrees. Then, during KD fine-tuning, the pretrained model is further finetuned
to learn the actual human rating standards with only very few human-annotated
data. To advocate the generalizability even with limited fine-tuning data, a
novel KD regularization is introduced to retain the knowledge learned at the
pre-training stage. Experimental results show that the model trained by
QuantiDCE presents stronger correlations with human judgements than the other
state-of-the-art metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liucun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lishan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WAFFLE: Watermarking in Federated Learning. (arXiv:2008.07298v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07298</id>
        <link href="http://arxiv.org/abs/2008.07298"/>
        <updated>2021-07-23T02:00:32.531Z</updated>
        <summary type="html"><![CDATA[Federated learning is a distributed learning technique where machine learning
models are trained on client devices in which the local training data resides.
The training is coordinated via a central server which is, typically,
controlled by the intended owner of the resulting model. By avoiding the need
to transport the training data to the central server, federated learning
improves privacy and efficiency. But it raises the risk of model theft by
clients because the resulting model is available on every client device. Even
if the application software used for local training may attempt to prevent
direct access to the model, a malicious client may bypass any such restrictions
by reverse engineering the application software. Watermarking is a well-known
deterrence method against model theft by providing the means for model owners
to demonstrate ownership of their models. Several recent deep neural network
(DNN) watermarking techniques use backdooring: training the models with
additional mislabeled data. Backdooring requires full access to the training
data and control of the training process. This is feasible when a single party
trains the model in a centralized manner, but not in a federated learning
setting where the training process and training data are distributed among
several client devices. In this paper, we present WAFFLE, the first approach to
watermark DNN models trained using federated learning. It introduces a
retraining step at the server after each aggregation of local models into the
global model. We show that WAFFLE efficiently embeds a resilient watermark into
models incurring only negligible degradation in test accuracy (-0.17%), and
does not require access to training data. We also introduce a novel technique
to generate the backdoor used as a watermark. It outperforms prior techniques,
imposing no communication, and low computational (+3.2%) overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atli_B/0/1/0/all/0/1"&gt;Buse Gul Atli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yuxi Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchal_S/0/1/0/all/0/1"&gt;Samuel Marchal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1"&gt;N. Asokan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02177</id>
        <link href="http://arxiv.org/abs/2012.02177"/>
        <updated>2021-07-23T02:00:32.512Z</updated>
        <summary type="html"><![CDATA[We propose an online multi-view depth prediction approach on posed video
streams, where the scene geometry information computed in the previous time
steps is propagated to the current time step in an efficient and geometrically
plausible way. The backbone of our approach is a real-time capable, lightweight
encoder-decoder that relies on cost volumes computed from pairs of images. We
extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses
an arbitrary amount of past information in its states. The novelty lies in
propagating the hidden state of the cell by accounting for the viewpoint
changes between time steps. At a given time step, we warp the previous hidden
state into the current camera plane using the previous depth prediction. Our
extension brings only a small overhead of computation time and memory
consumption, while improving the depth predictions significantly. As a result,
we outperform the existing state-of-the-art multi-view stereo methods on most
of the evaluated metrics in hundreds of indoor scenes while maintaining a
real-time performance. Code available:
https://github.com/ardaduz/deep-video-mvs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1"&gt;Arda D&amp;#xfc;z&amp;#xe7;eker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1"&gt;Silvano Galliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1"&gt;Christoph Vogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1"&gt;Pablo Speciale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1"&gt;Mihai Dusmanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Type4Py: Deep Similarity Learning-Based Type Inference for Python. (arXiv:2101.04470v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04470</id>
        <link href="http://arxiv.org/abs/2101.04470"/>
        <updated>2021-07-23T02:00:32.504Z</updated>
        <summary type="html"><![CDATA[Dynamic languages, such as Python and Javascript, trade static typing for
developer flexibility and productivity. Lack of static typing can cause
run-time exceptions and is a major factor for weak IDE support. To alleviate
these issues, PEP 484 introduced optional type annotations for Python. As
retrofitting types to existing codebases is error-prone and laborious,
learning-based approaches have been proposed to enable automatic type
annotations based on existing, partially annotated codebases. However, it is
still quite challenging for learning-based approaches to give a relevant
prediction in the first suggestion or the first few ones. In this paper, we
present Type4Py, a deep similarity learning-based hierarchical neural network
model that learns to discriminate between types of the same kind and dissimilar
types in a high-dimensional space, which results in clusters of types. Nearest
neighbor search suggests a list of likely types for arguments, variables, and
functions' return. The results of the quantitative and qualitative evaluation
indicate that Type4Py significantly outperforms state-of-the-art approaches at
the type prediction task. Considering the Top-1 prediction, Type4Py obtains a
Mean Reciprocal Rank of 72.5%, which is 10.87% and 16.45% higher than that of
Typilus and TypeWriter, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mir_A/0/1/0/all/0/1"&gt;Amir M. Mir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latoskinas_E/0/1/0/all/0/1"&gt;Evaldas Latoskinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proksch_S/0/1/0/all/0/1"&gt;Sebastian Proksch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gousios_G/0/1/0/all/0/1"&gt;Georgios Gousios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10756</id>
        <link href="http://arxiv.org/abs/2107.10756"/>
        <updated>2021-07-23T02:00:32.497Z</updated>
        <summary type="html"><![CDATA[Faces generated using generative adversarial networks (GANs) have reached
unprecedented realism. These faces, also known as "Deep Fakes", appear as
realistic photographs with very little pixel-level distortions. While some work
has enabled the training of models that lead to the generation of specific
properties of the subject, generating a facial image based on a natural
language description has not been fully explored. For security and criminal
identification, the ability to provide a GAN-based system that works like a
sketch artist would be incredibly useful. In this paper, we present a novel
approach to generate facial images from semantic text descriptions. The learned
model is provided with a text description and an outline of the type of face,
which the model uses to sketch the features. Our models are trained using an
Affine Combination Module (ACM) mechanism to combine the text embedding from
BERT and the GAN latent space using a self-attention matrix. This avoids the
loss of features due to inadequate "attention", which may happen if text
embedding and latent vector are simply concatenated. Our approach is capable of
generating images that are very accurately aligned to the exhaustive textual
descriptions of faces with many fine detail features of the face and helps in
generating better images. The proposed method is also capable of making
incremental changes to a previously generated image if it is provided with
additional textual descriptions or sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1"&gt;Manan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1"&gt;Sukalpa Chanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic source localization and spectra generation from sparse beamforming maps. (arXiv:2012.09643v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09643</id>
        <link href="http://arxiv.org/abs/2012.09643"/>
        <updated>2021-07-23T02:00:32.489Z</updated>
        <summary type="html"><![CDATA[Beamforming is an imaging tool for the investigation of aeroacoustic
phenomena and results in high dimensional data that is broken down to spectra
by integrating spatial Regions Of Interest. This paper presents two methods
that enable the automated identification of aeroacoustic sources in sparse
beamforming maps and the extraction of their corresponding spectra to overcome
the manual definition of Regions Of Interest. The methods are evaluated on two
scaled airframe half-model wind-tunnel measurements and on a generic monopole
source. The first relies on the spatial normal distribution of aeroacoustic
broadband sources in sparse beamforming maps. The second uses hierarchical
clustering methods. Both methods are robust to statistical noise and predict
the existence, location, and spatial probability estimation for sources based
on which Regions Of Interest are automatically determined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goudarzi_A/0/1/0/all/0/1"&gt;Armin Goudarzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spehr_C/0/1/0/all/0/1"&gt;Carsten Spehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1"&gt;Steffen Herbold&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning over non-IID Data for Indoor Localization. (arXiv:2107.04189v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04189</id>
        <link href="http://arxiv.org/abs/2107.04189"/>
        <updated>2021-07-23T02:00:32.483Z</updated>
        <summary type="html"><![CDATA[Localization and tracking of objects using data-driven methods is a popular
topic due to the complexity in characterizing the physics of wireless channel
propagation models. In these modeling approaches, data needs to be gathered to
accurately train models, at the same time that user's privacy is maintained. An
appealing scheme to cooperatively achieve these goals is known as Federated
Learning (FL). A challenge in FL schemes is the presence of non-independent and
identically distributed (non-IID) data, caused by unevenly exploration of
different areas. In this paper, we consider the use of recent FL schemes to
train a set of personalized models that are then optimally fused through
Bayesian rules, which makes it appropriate in the context of indoor
localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1"&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junha Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1"&gt;Pau Closas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning Models for Predicting and Explaining Vehicle Fuel Consumption Anomalies. (arXiv:2010.16051v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16051</id>
        <link href="http://arxiv.org/abs/2010.16051"/>
        <updated>2021-07-23T02:00:32.468Z</updated>
        <summary type="html"><![CDATA[Identifying anomalies in the fuel consumption of the vehicles of a fleet is a
crucial aspect for optimizing consumption and reduce costs. However, this
information alone is insufficient, since fleet operators need to know the
causes behind anomalous fuel consumption. We combine unsupervised anomaly
detection techniques, domain knowledge and interpretable Machine Learning
models for explaining potential causes of abnormal fuel consumption in terms of
feature relevance. The explanations are used for generating recommendations
about fuel optimization, that are adjusted according to two different user
profiles: fleet managers and fleet operators. Results are evaluated over
real-world data from telematics devices connected to diesel and petrol vehicles
from different types of industrial fleets. We measure the proposal regarding
model performance, and using Explainable AI metrics that compare the
explanations in terms of representativeness, fidelity, stability,
contrastiveness and consistency with apriori beliefs. The potential fuel
reductions that can be achieved is round 35%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1"&gt;Alberto Barbado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corcho_O/0/1/0/all/0/1"&gt;&amp;#xd3;scar Corcho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copyright in Generative Deep Learning. (arXiv:2105.09266v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09266</id>
        <link href="http://arxiv.org/abs/2105.09266"/>
        <updated>2021-07-23T02:00:32.439Z</updated>
        <summary type="html"><![CDATA[Machine-generated artworks are now part of the contemporary art scene: they
are attracting significant investments and they are presented in exhibitions
together with those created by human artists. These artworks are mainly based
on generative deep learning techniques, which have seen a formidable
development and remarkable refinement in the very recent years. Given the
inherent characteristics of these techniques, a series of novel legal problems
arise.

In this article, we consider a set of key questions in the area of generative
deep learning for the arts, including the following: is it possible to use
copyrighted works as training set for generative models? How do we legally
store their copies in order to perform the training process? Who (if someone)
will own the copyright on the generated data? We try to answer these questions
considering the law in force in both the United States of America and the
European Union, and potential future alternatives. Finally, we also formulate a
set of practical guidelines for artists and developers working on deep learning
generated art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1"&gt;Giorgio Franceschelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1"&gt;Mirco Musolesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Frequency EEG Artifact Detection with Uncertainty via Early Exit Paradigm. (arXiv:2107.10746v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.10746</id>
        <link href="http://arxiv.org/abs/2107.10746"/>
        <updated>2021-07-23T02:00:32.432Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) is crucial for the monitoring and diagnosis of
brain disorders. However, EEG signals suffer from perturbations caused by
non-cerebral artifacts limiting their efficacy. Current artifact detection
pipelines are resource-hungry and rely heavily on hand-crafted features.
Moreover, these pipelines are deterministic in nature, making them unable to
capture predictive uncertainty. We propose E4G, a deep learning framework for
high frequency EEG artifact detection. Our framework exploits the early exit
paradigm, building an implicit ensemble of models capable of capturing
uncertainty. We evaluate our approach on the Temple University Hospital EEG
Artifact Corpus (v2.0) achieving state-of-the-art classification results. In
addition, E4G provides well-calibrated uncertainty metrics comparable to
sampling techniques like Monte Carlo dropout in just a single forward pass. E4G
opens the door to uncertainty-aware artifact detection supporting
clinicians-in-the-loop frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qendro_L/0/1/0/all/0/1"&gt;Lorena Qendro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Campbell_A/0/1/0/all/0/1"&gt;Alexander Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mascolo_C/0/1/0/all/0/1"&gt;Cecilia Mascolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Electrochemical Modeling with Machine Learning for Lithium-Ion Batteries. (arXiv:2103.11580v4 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11580</id>
        <link href="http://arxiv.org/abs/2103.11580"/>
        <updated>2021-07-23T02:00:32.424Z</updated>
        <summary type="html"><![CDATA[Mathematical modeling of lithium-ion batteries (LiBs) is a central challenge
in advanced battery management. This paper presents a new approach to integrate
a physics-based model with machine learning to achieve high-precision modeling
for LiBs. This approach uniquely proposes to inform the machine learning model
of the dynamic state of the physical model, enabling a deep integration between
physics and machine learning. We propose two hybrid physics-machine learning
models based on the approach, which blend a single particle model with thermal
dynamics (SPMT) with a feedforward neural network (FNN) to perform
physics-informed learning of a LiB's dynamic behavior. The proposed models are
relatively parsimonious in structure and can provide considerable predictive
accuracy even at high C-rates, as shown by extensive simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1"&gt;Hao Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moura_S/0/1/0/all/0/1"&gt;Scott Moura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1"&gt;Huazhen Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable Computation of Expected Kernels. (arXiv:2102.10562v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10562</id>
        <link href="http://arxiv.org/abs/2102.10562"/>
        <updated>2021-07-23T02:00:32.405Z</updated>
        <summary type="html"><![CDATA[Computing the expectation of kernel functions is a ubiquitous task in machine
learning, with applications from classical support vector machines to
exploiting kernel embeddings of distributions in probabilistic modeling,
statistical inference, causal discovery, and deep learning. In all these
scenarios, we tend to resort to Monte Carlo estimates as expectations of
kernels are intractable in general. In this work, we characterize the
conditions under which we can compute expected kernels exactly and efficiently,
by leveraging recent advances in probabilistic circuit representations. We
first construct a circuit representation for kernels and propose an approach to
such tractable computation. We then demonstrate possible advancements for
kernel embedding frameworks by exploiting tractable expected kernels to derive
new algorithms for two challenging scenarios: 1) reasoning under missing data
with kernel support vector regressors; 2) devising a collapsed black-box
importance sampling scheme. Finally, we empirically evaluate both algorithms
and show that they outperform standard baselines on a variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1"&gt;Zhe Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1"&gt;Antonio Vergari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project Achoo: A Practical Model and Application for COVID-19 Detection from Recordings of Breath, Voice, and Cough. (arXiv:2107.10716v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.10716</id>
        <link href="http://arxiv.org/abs/2107.10716"/>
        <updated>2021-07-23T02:00:32.398Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic created a significant interest and demand for infection
detection and monitoring solutions. In this paper we propose a machine learning
method to quickly triage COVID-19 using recordings made on consumer devices.
The approach combines signal processing methods with fine-tuned deep learning
networks and provides methods for signal denoising, cough detection and
classification. We have also developed and deployed a mobile application that
uses symptoms checker together with voice, breath and cough signals to detect
COVID-19 infection. The application showed robust performance on both open
sourced datasets and on the noisy data collected during beta testing by the end
users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1"&gt;Alexander Ponomarchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1"&gt;Ilya Burenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malkin_E/0/1/0/all/0/1"&gt;Elian Malkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nazarov_I/0/1/0/all/0/1"&gt;Ivan Nazarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1"&gt;Vladimir Kokh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1"&gt;Manvel Avetisian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhukov_L/0/1/0/all/0/1"&gt;Leonid Zhukov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Dual Graph Neural Networks for Travel Time Estimation. (arXiv:2105.13591v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13591</id>
        <link href="http://arxiv.org/abs/2105.13591"/>
        <updated>2021-07-23T02:00:32.392Z</updated>
        <summary type="html"><![CDATA[Travel time estimation is one of the core tasks for the development of
intelligent transportation systems. Most previous works model the road segments
or intersections separately by learning their spatio-temporal characteristics
to estimate travel time. However, due to the continuous alternations of the
road segments and intersections in a path, the dynamic features are supposed to
be coupled and interactive. Therefore, modeling one of them limits further
improvement in accuracy of estimating travel time. To address the above
problems, a novel graph-based deep learning framework for travel time
estimation is proposed in this paper, namely Spatio-Temporal Dual Graph Neural
Networks (STDGNN). Specifically, we first establish the node-wise and edge-wise
graphs to respectively characterize the adjacency relations of intersections
and that of road segments. In order to extract the joint spatio-temporal
correlations of the intersections and road segments, we adopt the
spatio-temporal dual graph learning approach that incorporates multiple
spatial-temporal dual graph learning modules with multi-scale network
architectures for capturing multi-level spatial-temporal information from the
dual graph. Finally, we employ the multi-task learning approach to estimate the
travel time of a given whole route, each road segment and intersection
simultaneously. We conduct extensive experiments to evaluate our proposed model
on three real-world trajectory datasets, and the experimental results show that
STDGNN significantly outperforms several state-of-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1"&gt;Guangyin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Huan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fuxian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jincai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security Concerns on Machine Learning Solutions for 6G Networks in mmWave Beam Prediction. (arXiv:2105.03905v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03905</id>
        <link href="http://arxiv.org/abs/2105.03905"/>
        <updated>2021-07-23T02:00:32.385Z</updated>
        <summary type="html"><![CDATA[6G -- sixth generation -- is the latest cellular technology currently under
development for wireless communication systems. In recent years, machine
learning algorithms have been applied widely in various fields, such as
healthcare, transportation, energy, autonomous car, and many more. Those
algorithms have been also using in communication technologies to improve the
system performance in terms of frequency spectrum usage, latency, and security.
With the rapid developments of machine learning techniques, especially deep
learning, it is critical to take the security concern into account when
applying the algorithms. While machine learning algorithms offer significant
advantages for 6G networks, security concerns on Artificial Intelligent (AI)
models is typically ignored by the scientific community so far. However,
security is also a vital part of the AI algorithms, this is because the AI
model itself can be poisoned by attackers. This paper proposes a mitigation
method for adversarial attacks against proposed 6G machine learning models for
the millimeter-wave (mmWave) beam prediction using adversarial learning. The
main idea behind adversarial attacks against machine learning models is to
produce faulty results by manipulating trained deep learning models for 6G
applications for mmWave beam prediction. We also present the adversarial
learning mitigation method's performance for 6G security in mmWave beam
prediction application with fast gradient sign method attack. The mean square
errors (MSE) of the defended model under attack are very close to the
undefended model without attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Catak_F/0/1/0/all/0/1"&gt;Ferhat Ozgur Catak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Catak_E/0/1/0/all/0/1"&gt;Evren Catak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuzlu_M/0/1/0/all/0/1"&gt;Murat Kuzlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cali_U/0/1/0/all/0/1"&gt;Umit Cali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers. (arXiv:2103.07470v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07470</id>
        <link href="http://arxiv.org/abs/2103.07470"/>
        <updated>2021-07-23T02:00:32.378Z</updated>
        <summary type="html"><![CDATA[A discriminatively trained neural net classifier can fit the training data
perfectly if all information about its input other than class membership has
been discarded prior to the output layer. Surprisingly, past research has
discovered that some extraneous visual detail remains in the logit vector. This
finding is based on inversion techniques that map deep embeddings back to
images. We explore this phenomenon further using a novel synthesis of methods,
yielding a feedforward inversion model that produces remarkably high fidelity
reconstructions, qualitatively superior to those of past efforts. When applied
to an adversarially robust classifier model, the reconstructions contain
sufficient local detail and global structure that they might be confused with
the original image in a quick glance, and the object category can clearly be
gleaned from the reconstruction. Our approach is based on BigGAN (Brock, 2019),
with conditioning on logits instead of one-hot class labels. We use our
reconstruction model as a tool for exploring the nature of representations,
including: the influence of model architecture and training objectives
(specifically robust losses), the forms of invariance that networks achieve,
representational differences between correctly and incorrectly classified
images, and the effects of manipulating logits and images. We believe that our
method can inspire future investigations into the nature of information flow in
a neural net and can provide diagnostics for improving discriminative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1"&gt;Piotr Teterwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1"&gt;Dilip Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk score learning for COVID-19 contact tracing apps. (arXiv:2104.08415v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08415</id>
        <link href="http://arxiv.org/abs/2104.08415"/>
        <updated>2021-07-23T02:00:32.371Z</updated>
        <summary type="html"><![CDATA[Digital contact tracing apps for COVID, such as the one developed by Google
and Apple, need to estimate the risk that a user was infected during a
particular exposure, in order to decide whether to notify the user to take
precautions, such as entering into quarantine, or requesting a test. Such risk
score models contain numerous parameters that must be set by the public health
authority. In this paper, we show how to automatically learn these parameters
from data.

Our method needs access to exposure and outcome data. Although this data is
already being collected (in an aggregated, privacy-preserving way) by several
health authorities, in this paper we limit ourselves to simulated data, so that
we can systematically study the different factors that affect the feasibility
of the approach. In particular, we show that the parameters become harder to
estimate when there is more missing data (e.g., due to infections which were
not recorded by the app), and when there is model misspecification.
Nevertheless, the learning approach outperforms a strong manually designed
baseline. Furthermore, the learning approach can adapt even when the risk
factors of the disease change, e.g., due to the evolution of new variants, or
the adoption of vaccines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1"&gt;Kevin Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serghiou_S/0/1/0/all/0/1"&gt;Stylianos Serghiou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTa\'u: Ita\'u BERT for digital customer service. (arXiv:2101.12015v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12015</id>
        <link href="http://arxiv.org/abs/2101.12015"/>
        <updated>2021-07-23T02:00:32.353Z</updated>
        <summary type="html"><![CDATA[In the last few years, three major topics received increased interest: deep
learning, NLP and conversational agents. Bringing these three topics together
to create an amazing digital customer experience and indeed deploy in
production and solve real-world problems is something innovative and
disruptive. We introduce a new Portuguese financial domain language
representation model called BERTa\'u. BERTa\'u is an uncased BERT-base trained
from scratch with data from the Ita\'u virtual assistant chatbot solution. Our
novel contribution is that BERTa\'u pretrained language model requires less
data, reached state-of-the-art performance in three NLP tasks, and generates a
smaller and lighter model that makes the deployment feasible. We developed
three tasks to validate our model: information retrieval with Frequently Asked
Questions (FAQ) from Ita\'u bank, sentiment analysis from our virtual assistant
data, and a NER solution. All proposed tasks are real-world solutions in
production on our environment and the usage of a specialist model proved to be
effective when compared to Google BERT multilingual and the DPRQuestionEncoder
from Facebook, available at Hugging Face. The BERTa\'u improves the performance
in 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%
in NER F1 score and can also represent the same sequence in up to 66% fewer
tokens when compared to "shelf models".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finardi_P/0/1/0/all/0/1"&gt;Paulo Finardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viegas_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Di&amp;#xe9; Viegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_G/0/1/0/all/0/1"&gt;Gustavo T. Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansano_A/0/1/0/all/0/1"&gt;Alex F. Mansano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carida_V/0/1/0/all/0/1"&gt;Vinicius F. Carid&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing the Whole-life Cost in End-to-end CNN Acceleration. (arXiv:2104.05541v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05541</id>
        <link href="http://arxiv.org/abs/2104.05541"/>
        <updated>2021-07-23T02:00:32.345Z</updated>
        <summary type="html"><![CDATA[The acceleration of CNNs has gained increasing atten-tion since their success
in computer vision. With the heterogeneous functional layers that cannot be
pro-cessed by the accelerators proposed for convolution layers only, modern
end-to-end CNN acceleration so-lutions either transform the diverse computation
into matrix/vector arithmetic, which loses data reuse op-portunities in
convolution, or introduce dedicated functional unit to each kind of layer,
which results in underutilization and high update expense. To enhance the
whole-life cost efficiency, we need an acceleration solution that is efficient
in processing CNN layers and has the generality to apply to all kinds of
existing and emerging layers. To this end, we pro-pose GCONV Chain, a method to
convert the entire CNN computation into a chain of standard general
convolutions (GCONV) that can be efficiently pro-cessed by the existing CNN
accelerators. This paper comprehensively analyzes the GCONV Chain model and
proposes a full-stack implementation to support GCONV Chain. On one hand, the
results on seven var-ious CNNs demonstrate that GCONV Chain improves the
performance and energy efficiency of existing CNN accelerators by an average of
3.4x and 3.2x re-spectively. On the other hand, we show that GCONV Chain
provides low whole-life costs for CNN accelera-tion, including both developer
efforts and total cost of ownership for the users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangru Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Sandip Ray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Potential Drug Targets Using Tensor Factorisation and Knowledge Graph Embeddings. (arXiv:2105.10578v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10578</id>
        <link href="http://arxiv.org/abs/2105.10578"/>
        <updated>2021-07-23T02:00:32.338Z</updated>
        <summary type="html"><![CDATA[The drug discovery and development process is a long and expensive one,
costing over 1 billion USD on average per drug and taking 10-15 years. To
reduce the high levels of attrition throughout the process, there has been a
growing interest in applying machine learning methodologies to various stages
of drug discovery process in the recent decade, including at the earliest stage
- identification of druggable disease genes. In this paper, we have developed a
new tensor factorisation model to predict potential drug targets (i.e.,genes or
proteins) for diseases. We created a three dimensional tensor which consists of
1,048 targets, 860 diseases and 230,011 evidence attributes and clinical
outcomes connecting them, using data extracted from the Open Targets and
PharmaProjects databases. We enriched the data with gene representations
learned from a drug discovery-oriented knowledge graph and applied our proposed
method to predict the clinical outcomes for unseen target and dis-ease pairs.
We designed three evaluation strategies to measure the prediction performance
and benchmarked several commonly used machine learning classifiers together
with matrix and tensor factorisation methods. The result shows that
incorporating knowledge graph embeddings significantly improves the prediction
accuracy and that training tensor factorisation alongside a dense neural
network outperforms other methods. In summary, our framework combines two
actively studied machine learning approaches to disease target identification,
tensor factorisation and knowledge graph representation learning, which could
be a promising avenue for further exploration in data-driven drug discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian Barrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Model Bias for Robust Offline Deep Reinforcement Learning. (arXiv:2008.05533v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05533</id>
        <link href="http://arxiv.org/abs/2008.05533"/>
        <updated>2021-07-23T02:00:32.332Z</updated>
        <summary type="html"><![CDATA[State-of-the-art reinforcement learning algorithms mostly rely on being
allowed to directly interact with their environment to collect millions of
observations. This makes it hard to transfer their success to industrial
control problems, where simulations are often very costly or do not exist, and
exploring in the real environment can potentially lead to catastrophic events.
Recently developed, model-free, offline RL algorithms, can learn from a single
dataset (containing limited exploration) by mitigating extrapolation error in
value functions. However, the robustness of the training process is still
comparatively low, a problem known from methods using value functions. To
improve robustness and stability of the learning process, we use dynamics
models to assess policy performance instead of value functions, resulting in
MOOSE (MOdel-based Offline policy Search with Ensembles), an algorithm which
ensures low model bias by keeping the policy within the support of the data. We
compare MOOSE with state-of-the-art model-free, offline RL algorithms { BRAC,}
BEAR and BCQ on the Industrial Benchmark and MuJoCo continuous control tasks in
terms of robust performance, and find that MOOSE outperforms its model-free
counterparts in almost all considered cases, often even by far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swazinna_P/0/1/0/all/0/1"&gt;Phillip Swazinna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udluft_S/0/1/0/all/0/1"&gt;Steffen Udluft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1"&gt;Thomas Runkler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03592</id>
        <link href="http://arxiv.org/abs/2008.03592"/>
        <updated>2021-07-23T02:00:32.325Z</updated>
        <summary type="html"><![CDATA[Visual emotion expression plays an important role in audiovisual speech
communication. In this work, we propose a novel approach to rendering visual
emotion expression in speech-driven talking face generation. Specifically, we
design an end-to-end talking face generation system that takes a speech
utterance, a single face image, and a categorical emotion label as input to
render a talking face video synchronized with the speech and expressing the
conditioned emotion. Objective evaluation on image quality, audiovisual
synchronization, and visual emotion expression shows that the proposed system
outperforms a state-of-the-art baseline system. Subjective evaluation of visual
emotion expression and video realness also demonstrates the superiority of the
proposed system. Furthermore, we conduct a human emotion recognition pilot
study using generated videos with mismatched emotions among the audio and
visual modalities. Results show that humans respond to the visual modality more
significantly than the audio modality on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1"&gt;Sefik Emre Eskimez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;You Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Zhiyao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A deep learning approach for inverse design of the metasurface for dual-polarized waves. (arXiv:2105.08508v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08508</id>
        <link href="http://arxiv.org/abs/2105.08508"/>
        <updated>2021-07-23T02:00:32.307Z</updated>
        <summary type="html"><![CDATA[Compared to the conventional metasurface design, machine learning-based
methods have recently created an inspiring platform for an inverse realization
of the metasurfaces. Here, we have used the Deep Neural Network (DNN) for the
generation of desired output unit cell structures in an ultra-wide working
frequency band for both TE and TM polarized waves. To automatically generate
metasurfaces in a wide range of working frequencies from 4 to 45 GHz, we
deliberately design an 8 ring-shaped pattern in such a way that the unit-cells
generated in the dataset can produce single or multiple notches in the desired
working frequency band. Compared to the general approach, whereby the final
metasurface structure may be formed by any randomly distributed "0" and "1", we
propose here a restricted output structure. By restricting the output, the
number of calculations will be reduced and the learning speed will be
increased. Moreover, we have shown that the accuracy of the network reaches
91\%. Obtaining the final unit cell directly without any time-consuming
optimization algorithms for both TE and TM polarized waves, and high average
accuracy, promises an effective strategy for the metasurface design; thus, the
designer is required only to focus on the design goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghorbani_F/0/1/0/all/0/1"&gt;Fardin Ghorbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shabanpour_J/0/1/0/all/0/1"&gt;Javad Shabanpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyraghi_S/0/1/0/all/0/1"&gt;Sina Beyraghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleimani_H/0/1/0/all/0/1"&gt;Hossein Soleimani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oraizi_H/0/1/0/all/0/1"&gt;Homayoon Oraizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleimani_M/0/1/0/all/0/1"&gt;Mohammad Soleimani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Embedding Dynamic Graphs. (arXiv:2101.01229v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01229</id>
        <link href="http://arxiv.org/abs/2101.01229"/>
        <updated>2021-07-23T02:00:32.300Z</updated>
        <summary type="html"><![CDATA[Embedding static graphs in low-dimensional vector spaces plays a key role in
network analytics and inference, supporting applications like node
classification, link prediction, and graph visualization. However, many
real-world networks present dynamic behavior, including topological evolution,
feature evolution, and diffusion. Therefore, several methods for embedding
dynamic graphs have been proposed to learn network representations over time,
facing novel challenges, such as time-domain modeling, temporal features to be
captured, and the temporal granularity to be embedded. In this survey, we
overview dynamic graph embedding, discussing its fundamentals and the recent
advances developed so far. We introduce the formal definition of dynamic graph
embedding, focusing on the problem setting and introducing a novel taxonomy for
dynamic graph embedding input and output. We further explore different dynamic
behaviors that may be encompassed by embeddings, classifying by topological
evolution, feature evolution, and processes on networks. Afterward, we describe
existing techniques and propose a taxonomy for dynamic graph embedding
techniques based on algorithmic approaches, from matrix and tensor
factorization to deep learning, random walks, and temporal point processes. We
also elucidate main applications, including dynamic link prediction, anomaly
detection, and diffusion prediction, and we further state some promising
research directions in the area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1"&gt;Claudio D. T. Barros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendonca_M/0/1/0/all/0/1"&gt;Matheus R. F. Mendon&amp;#xe7;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vieira_A/0/1/0/all/0/1"&gt;Alex B. Vieira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziviani_A/0/1/0/all/0/1"&gt;Artur Ziviani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13430</id>
        <link href="http://arxiv.org/abs/2106.13430"/>
        <updated>2021-07-23T02:00:32.292Z</updated>
        <summary type="html"><![CDATA[Graphs have been widely used in data mining and machine learning due to their
unique representation of real-world objects and their interactions. As graphs
are getting bigger and bigger nowadays, it is common to see their subgraphs
separately collected and stored in multiple local systems. Therefore, it is
natural to consider the subgraph federated learning setting, where each local
system holding a small subgraph that may be biased from the distribution of the
whole graph. Hence, the subgraph federated learning aims to collaboratively
train a powerful and generalizable graph mining model without directly sharing
their graph data. In this work, towards the novel yet realistic setting of
subgraph federated learning, we propose two major techniques: (1) FedSage,
which trains a GraphSage model based on FedAvg to integrate node features, link
structures, and task labels on multiple local subgraphs; (2) FedSage+, which
trains a missing neighbor generator along FedSage to deal with missing links
across local subgraphs. Empirical results on four real-world graph datasets
with synthesized subgraph federated learning settings demonstrate the
effectiveness and efficiency of our proposed techniques. At the same time,
consistent theoretical implications are made towards their generalization
ability on the global graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1"&gt;Siu Ming Yiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing Geometric Constraints from Emotion Labels to improve Face Verification. (arXiv:2103.03862v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03862</id>
        <link href="http://arxiv.org/abs/2103.03862"/>
        <updated>2021-07-23T02:00:32.286Z</updated>
        <summary type="html"><![CDATA[For the task of face verification, we explore the utility of harnessing
auxiliary facial emotion labels to impose explicit geometric constraints on the
embedding space when training deep embedding models. We introduce several novel
loss functions that, in conjunction with a standard Triplet Loss [43], or
ArcFace loss [10], provide geometric constraints on the embedding space; the
labels for our loss functions can be provided using either manually annotated
or automatically detected auxiliary emotion labels. Our method is implemented
purely in terms of the loss function and does not require any changes to the
neural network backbone of the embedding function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1"&gt;Anand Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1"&gt;Minh Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1"&gt;Jacob Whitehill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Fair Machine Learning. (arXiv:2102.00753v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00753</id>
        <link href="http://arxiv.org/abs/2102.00753"/>
        <updated>2021-07-23T02:00:32.268Z</updated>
        <summary type="html"><![CDATA[In this paper, we inaugurate the field of quantum fair machine learning. We
undertake a comparative analysis of differences and similarities between
classical and quantum fair machine learning algorithms, specifying how the
unique features of quantum computation alter measures, metrics and remediation
strategies when quantum algorithms are subject to fairness constraints. We
present the first results in quantum fair machine learning by demonstrating the
use of Grover's search algorithm to satisfy statistical parity constraints
imposed on quantum algorithms. We provide lower-bounds on iterations needed to
achieve such statistical parity within $\epsilon$-tolerance. We extend
canonical Lipschitz-conditioned individual fairness criteria to the quantum
setting using quantum metrics. We examine the consequences for typical measures
of fairness in machine learning context when quantum information processing and
quantum data are involved. Finally, we propose open questions and research
programmes for this new field of interest to researchers in computer science,
ethics and quantum computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perrier_E/0/1/0/all/0/1"&gt;Elija Perrier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Explaining Adversarial Examples Phenomenon in Artificial Neural Networks. (arXiv:2107.10599v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10599</id>
        <link href="http://arxiv.org/abs/2107.10599"/>
        <updated>2021-07-23T02:00:32.256Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the adversarial examples existence and adversarial
training from the standpoint of convergence and provide evidence that pointwise
convergence in ANNs can explain these observations. The main contribution of
our proposal is that it relates the objective of the evasion attacks and
adversarial training with concepts already defined in learning theory. Also, we
extend and unify some of the other proposals in the literature and provide
alternative explanations on the observations made in those proposals. Through
different experiments, we demonstrate that the framework is valuable in the
study of the phenomenon and is applicable to real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barati_R/0/1/0/all/0/1"&gt;Ramin Barati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safabakhsh_R/0/1/0/all/0/1"&gt;Reza Safabakhsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1"&gt;Mohammad Rahmati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lumen: A Machine Learning Framework to Expose Influence Cues in Text. (arXiv:2107.10655v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10655</id>
        <link href="http://arxiv.org/abs/2107.10655"/>
        <updated>2021-07-23T02:00:32.250Z</updated>
        <summary type="html"><![CDATA[Phishing and disinformation are popular social engineering attacks with
attackers invariably applying influence cues in texts to make them more
appealing to users. We introduce Lumen, a learning-based framework that exposes
influence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)
objectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was
trained with a newly developed dataset of 3K texts comprised of disinformation,
phishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in
comparison to other learning models showed that Lumen and LSTM presented the
best F1-micro score, but Lumen yielded better interpretability. Our results
highlight the promise of ML to expose influence cues in text, towards the goal
of application in automatic labeling tools to improve the accuracy of
human-based detection and reduce the likelihood of users falling for deceptive
online content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hanyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mirela Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capecci_D/0/1/0/all/0/1"&gt;Daniel Capecci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1"&gt;Luiz Giovanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czech_L/0/1/0/all/0/1"&gt;Lauren Czech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1"&gt;Juliana Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Daniela Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Radar Inference: Inverse Tracking, Identifying Cognition and Designing Smart Interference. (arXiv:2008.01559v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01559</id>
        <link href="http://arxiv.org/abs/2008.01559"/>
        <updated>2021-07-23T02:00:32.243Z</updated>
        <summary type="html"><![CDATA[This paper considers three inter-related adversarial inference problems
involving cognitive radars. We first discuss inverse tracking of the radar to
estimate the adversary's estimate of us based on the radar's actions and
calibrate the radar's sensing accuracy. Second, using revealed preference from
microeconomics, we formulate a non-parametric test to identify if the cognitive
radar is a constrained utility maximizer with signal processing constraints. We
consider two radar functionalities, namely, beam allocation and waveform
design, with respect to which the cognitive radar is assumed to maximize its
utility and construct a set-valued estimator for the radar's utility function.
Finally, we discuss how to engineer interference at the physical layer level to
confuse the radar which forces it to change its transmit waveform. The levels
of abstraction range from smart interference design based on Wiener filters (at
the pulse/waveform level), inverse Kalman filters at the tracking level and
revealed preferences for identifying utility maximization at the systems level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Krishnamurthy_V/0/1/0/all/0/1"&gt;Vikram Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pattanayak_K/0/1/0/all/0/1"&gt;Kunal Pattanayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gogineni_S/0/1/0/all/0/1"&gt;Sandeep Gogineni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1"&gt;Bosung Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rangaswamy_M/0/1/0/all/0/1"&gt;Muralidhar Rangaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Function Contrastive Learning of Transferable Meta-Representations. (arXiv:2010.07093v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07093</id>
        <link href="http://arxiv.org/abs/2010.07093"/>
        <updated>2021-07-23T02:00:32.236Z</updated>
        <summary type="html"><![CDATA[Meta-learning algorithms adapt quickly to new tasks that are drawn from the
same task distribution as the training tasks. The mechanism leading to fast
adaptation is the conditioning of a downstream predictive model on the inferred
representation of the task's underlying data generative process, or
\emph{function}. This \emph{meta-representation}, which is computed from a few
observed examples of the underlying function, is learned jointly with the
predictive model. In this work, we study the implications of this joint
training on the transferability of the meta-representations. Our goal is to
learn meta-representations that are robust to noise in the data and facilitate
solving a wide range of downstream tasks that share the same underlying
functions. To this end, we propose a decoupled encoder-decoder approach to
supervised meta-learning, where the encoder is trained with a contrastive
objective to find a good representation of the underlying function. In
particular, our training scheme is driven by the self-supervision signal
indicating whether two sets of examples stem from the same function. Our
experiments on a number of synthetic and real-world datasets show that the
representations we obtain outperform strong baselines in terms of downstream
performance and noise robustness, even when these baselines are trained in an
end-to-end manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gondal_M/0/1/0/all/0/1"&gt;Muhammad Waleed Gondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shruti Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_N/0/1/0/all/0/1"&gt;Nasim Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1"&gt;Manuel W&amp;#xfc;thrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HANT: Hardware-Aware Network Transformation. (arXiv:2107.10624v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10624</id>
        <link href="http://arxiv.org/abs/2107.10624"/>
        <updated>2021-07-23T02:00:32.226Z</updated>
        <summary type="html"><![CDATA[Given a trained network, how can we accelerate it to meet efficiency needs
for deployment on particular hardware? The commonly used hardware-aware network
compression techniques address this question with pruning, kernel fusion,
quantization and lowering precision. However, these approaches do not change
the underlying network operations. In this paper, we propose hardware-aware
network transformation (HANT), which accelerates a network by replacing
inefficient operations with more efficient alternatives using a neural
architecture search like approach. HANT tackles the problem in two phase: In
the first phase, a large number of alternative operations per every layer of
the teacher model is trained using layer-wise feature map distillation. In the
second phase, the combinatorial selection of efficient operations is relaxed to
an integer optimization problem that can be solved in a few seconds. We extend
HANT with kernel fusion and quantization to improve throughput even further.
Our experimental results on accelerating the EfficientNet family show that HANT
can accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the
ImageNet dataset. When comparing the same latency level, HANT can accelerate
EfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher
accuracy. We examine a large pool of operations, up to 197 per layer, and we
provide insights into the selected operations and final architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1"&gt;Jimmy Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1"&gt;Nicolo Fusi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1"&gt;Arash Vahdat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries. (arXiv:2107.10652v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10652</id>
        <link href="http://arxiv.org/abs/2107.10652"/>
        <updated>2021-07-23T02:00:32.219Z</updated>
        <summary type="html"><![CDATA[Codification of free-text clinical narratives have long been recognised to be
beneficial for secondary uses such as funding, insurance claim processing and
research. The current scenario of assigning codes is a manual process which is
very expensive, time-consuming and error prone. In recent years, many
researchers have studied the use of Natural Language Processing (NLP), related
Machine Learning (ML) and Deep Learning (DL) methods and techniques to resolve
the problem of manual coding of clinical narratives and to assist human coders
to assign clinical codes more accurately and efficiently. This systematic
literature review provides a comprehensive overview of automated clinical
coding systems that utilises appropriate NLP, ML and DL methods and techniques
to assign ICD codes to discharge summaries. We have followed the Preferred
Reporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and
conducted a comprehensive search of publications from January, 2010 to December
2020 in four academic databases- PubMed, ScienceDirect, Association for
Computing Machinery(ACM) Digital Library, and the Association for Computational
Linguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the
inclusion criteria. This review identified: datasets having discharge
summaries; NLP techniques along with some other data extraction processes,
different feature extraction and embedding techniques. To measure the
performance of classification methods, different evaluation metrics are used.
Lastly, future research directions are provided to scholars who are interested
in automated ICD code assignment. Efforts are still required to improve ICD
code prediction accuracy, availability of large-scale de-identified clinical
corpora with the latest version of the classification system. This can be a
platform to guide and share knowledge with the less experienced coders and
researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1"&gt;Rajvir Kaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginige_J/0/1/0/all/0/1"&gt;Jeewani Anupama Ginige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1"&gt;Oliver Obst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks. (arXiv:2010.08488v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08488</id>
        <link href="http://arxiv.org/abs/2010.08488"/>
        <updated>2021-07-23T02:00:32.202Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks attempt to combine the strong predictive performance
of neural networks with formal quantification of uncertainty associated with
the predictive output in the Bayesian framework. However, it remains unclear
how to endow the parameters of the network with a prior distribution that is
meaningful when lifted into the output space of the network. A possible
solution is proposed that enables the user to posit an appropriate Gaussian
process covariance function for the task at hand. Our approach constructs a
prior distribution for the parameters of the network, called a ridgelet prior,
that approximates the posited Gaussian process in the output space of the
network. In contrast to existing work on the connection between neural networks
and Gaussian processes, our analysis is non-asymptotic, with finite sample-size
error bounds provided. This establishes the universality property that a
Bayesian neural network can approximate any Gaussian process whose covariance
function is sufficiently regular. Our experimental assessment is limited to a
proof-of-concept, where we demonstrate that the ridgelet prior can out-perform
an unstructured prior on regression problems for which a suitable Gaussian
process prior can be provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takuo Matsubara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1"&gt;Chris J. Oates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligence and Learning in O-RAN for Data-driven NextG Cellular Networks. (arXiv:2012.01263v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01263</id>
        <link href="http://arxiv.org/abs/2012.01263"/>
        <updated>2021-07-23T02:00:32.195Z</updated>
        <summary type="html"><![CDATA[Next Generation (NextG) cellular networks will be natively cloud-based and
built upon programmable, virtualized, and disaggregated architectures. The
separation of control functions from the hardware fabric and the introduction
of standardized control interfaces will enable the definition of custom
closed-control loops, which will ultimately enable embedded intelligence and
real-time analytics, thus effectively realizing the vision of autonomous and
self-optimizing networks. This article explores the disaggregated network
architecture proposed by the O-RAN Alliance as a key enabler of NextG networks.
Within this architectural context, we discuss the potential, the challenges,
and the limitations of data-driven optimization approaches to network control
over different timescales. We also present the first large-scale integration of
O-RAN-compliant software components with an open-source full-stack softwarized
cellular network. Experiments conducted on Colosseum, the world's largest
wireless network emulator, demonstrate closed-loop integration of real-time
analytics and control through deep reinforcement learning agents. We also show
the feasibility of Radio Access Network (RAN) control through xApps running on
the near real-time RAN Intelligent Controller, to optimize the scheduling
policies of co-existing network slices, leveraging the O-RAN open interfaces to
collect data at the edge of the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonati_L/0/1/0/all/0/1"&gt;Leonardo Bonati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DOro_S/0/1/0/all/0/1"&gt;Salvatore D&amp;#x27;Oro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polese_M/0/1/0/all/0/1"&gt;Michele Polese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basagni_S/0/1/0/all/0/1"&gt;Stefano Basagni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melodia_T/0/1/0/all/0/1"&gt;Tommaso Melodia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Robustness of Counterfactual Explanations. (arXiv:2103.02354v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02354</id>
        <link href="http://arxiv.org/abs/2103.02354"/>
        <updated>2021-07-23T02:00:32.188Z</updated>
        <summary type="html"><![CDATA[Transparency is a fundamental requirement for decision making systems when
these should be deployed in the real world. It is usually achieved by providing
explanations of the system's behavior. A prominent and intuitive type of
explanations are counterfactual explanations. Counterfactual explanations
explain a behavior to the user by proposing actions -- as changes to the input
-- that would cause a different (specified) behavior of the system. However,
such explanation methods can be unstable with respect to small changes to the
input -- i.e. even a small change in the input can lead to huge or arbitrary
changes in the output and of the explanation. This could be problematic for
counterfactual explanations, as two similar individuals might get very
different explanations. Even worse, if the recommended actions differ
considerably in their complexity, one would consider such unstable
(counterfactual) explanations as individually unfair.

In this work, we formally and empirically study the robustness of
counterfactual explanations in general, as well as under different models and
different kinds of perturbations. Furthermore, we propose that plausible
counterfactual explanations can be used instead of closest counterfactual
explanations to improve the robustness and consequently the individual fairness
of counterfactual explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Artelt_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Artelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaquet_V/0/1/0/all/0/1"&gt;Valerie Vaquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velioglu_R/0/1/0/all/0/1"&gt;Riza Velioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinder_F/0/1/0/all/0/1"&gt;Fabian Hinder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinkrolf_J/0/1/0/all/0/1"&gt;Johannes Brinkrolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_M/0/1/0/all/0/1"&gt;Malte Schilling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Barbara Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remember What You Want to Forget: Algorithms for Machine Unlearning. (arXiv:2103.03279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03279</id>
        <link href="http://arxiv.org/abs/2103.03279"/>
        <updated>2021-07-23T02:00:32.172Z</updated>
        <summary type="html"><![CDATA[We study the problem of unlearning datapoints from a learnt model. The
learner first receives a dataset $S$ drawn i.i.d. from an unknown distribution,
and outputs a model $\widehat{w}$ that performs well on unseen samples from the
same distribution. However, at some point in the future, any training datapoint
$z \in S$ can request to be unlearned, thus prompting the learner to modify its
output model while still ensuring the same accuracy guarantees. We initiate a
rigorous study of generalization in machine unlearning, where the goal is to
perform well on previously unseen datapoints. Our focus is on both
computational and storage complexity.

For the setting of convex losses, we provide an unlearning algorithm that can
unlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In
comparison, in general, differentially private learning (which implies
unlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This
demonstrates a novel separation between differential privacy and machine
unlearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1"&gt;Ayush Sekhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1"&gt;Gautam Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Quickest Changepoint Detection. (arXiv:2107.10492v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10492</id>
        <link href="http://arxiv.org/abs/2107.10492"/>
        <updated>2021-07-23T02:00:32.166Z</updated>
        <summary type="html"><![CDATA[Detecting abrupt changes in temporal behavior patterns is of interest in many
industrial and security applications. Abrupt changes are often local and
observable primarily through a well-aligned sensing action (e.g., a camera with
a narrow field-of-view). Due to resource constraints, continuous monitoring of
all of the sensors is impractical. We propose the bandit quickest changepoint
detection framework as a means of balancing sensing cost with detection delay.
In this framework, sensing actions (or sensors) are sequentially chosen, and
only measurements corresponding to chosen actions are observed. We derive an
information-theoretic lower bound on the detection delay for a general class of
finitely parameterized probability distributions. We then propose a
computationally efficient online sensing scheme, which seamlessly balances the
need for exploration of different sensing options with exploitation of querying
informative actions. We derive expected delay bounds for the proposed scheme
and show that these bounds match our information-theoretic lower bounds at low
false alarm rates, establishing optimality of the proposed method. We then
perform a number of experiments on synthetic and real datasets demonstrating
the efficacy of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1"&gt;Aditya Gopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1"&gt;Venkatesh Saligrama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Braghadeesh Lakshminarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data vs classifiers, who wins?. (arXiv:2107.07451v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07451</id>
        <link href="http://arxiv.org/abs/2107.07451"/>
        <updated>2021-07-23T02:00:32.160Z</updated>
        <summary type="html"><![CDATA[The classification experiments covered by machine learning (ML) are composed
by two important parts: the data and the algorithm. As they are a fundamental
part of the problem, both must be considered when evaluating a model's
performance against a benchmark. The best classifiers need robust benchmarks to
be properly evaluated. For this, gold standard benchmarks such as OpenML-CC18
are used. However, data complexity is commonly not considered along with the
model during a performance evaluation. Recent studies employ Item Response
Theory (IRT) as a new approach to evaluating datasets and algorithms, capable
of evaluating both simultaneously. This work presents a new evaluation
methodology based on IRT and Glicko-2, jointly with the decodIRT tool developed
to guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate
the OpenML-CC18 benchmark for its algorithmic evaluation capability and checks
if there is a subset of datasets more efficient than the original benchmark.
Several classifiers, from classics to ensemble, are also evaluated using the
IRT models. The Glicko-2 rating system was applied together with IRT to
summarize the innate ability and classifiers performance. It was noted that not
all OpenML-CC18 datasets are really useful for evaluating algorithms, where
only 10% were rated as being really difficult. Furthermore, it was verified the
existence of a more efficient subset containing only 50% of the original size.
While Randon Forest was singled out as the algorithm with the best innate
ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1"&gt;Lucas F. F. Cardoso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1"&gt;Vitor C. A. Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1"&gt;Regiane S. Kawasaki Franc&amp;#xea;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1"&gt;Ricardo B. C. Prud&amp;#xea;ncio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1"&gt;Ronnie C. O. Alves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Typing assumptions improve identification in causal discovery. (arXiv:2107.10703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10703</id>
        <link href="http://arxiv.org/abs/2107.10703"/>
        <updated>2021-07-23T02:00:32.153Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is a challenging task to which an
exact solution cannot always be identified. Under assumptions about the
data-generative process, the causal graph can often be identified up to an
equivalence class. Proposing new realistic assumptions to circumscribe such
equivalence classes is an active field of research. In this work, we propose a
new set of assumptions that constrain possible causal relationships based on
the nature of the variables. We thus introduce typed directed acyclic graphs,
in which variable types are used to determine the validity of causal
relationships. We demonstrate, both theoretically and empirically, that the
proposed assumptions can result in significant gains in the identification of
the causal graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1"&gt;Philippe Brouillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taslakian_P/0/1/0/all/0/1"&gt;Perouz Taslakian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1"&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lachapelle_S/0/1/0/all/0/1"&gt;Sebastien Lachapelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-07-23T02:00:32.146Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose mRASP2, a
training method to obtain a single unified multilingual translation model.
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to
close the gap among representations of different languages, and b) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mRASP2 outperforms
existing best unified model and achieves competitive or even better performance
than the pre-trained and fine-tuned model mBART on tens of WMT's translation
directions. For non-English directions, mRASP2 achieves an improvement of
average 10+ BLEU compared with the multilingual Transformer baseline. Code,
data and trained models are available at https://github.com/PANXiao1994/mRASP2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private (Gradient) Expectation Maximization Algorithm with Statistical Guarantees. (arXiv:2010.13520v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13520</id>
        <link href="http://arxiv.org/abs/2010.13520"/>
        <updated>2021-07-23T02:00:32.139Z</updated>
        <summary type="html"><![CDATA[(Gradient) Expectation Maximization (EM) is a widely used algorithm for
estimating the maximum likelihood of mixture models or incomplete data
problems. A major challenge facing this popular technique is how to effectively
preserve the privacy of sensitive data. Previous research on this problem has
already lead to the discovery of some Differentially Private (DP) algorithms
for (Gradient) EM. However, unlike in the non-private case, existing techniques
are not yet able to provide finite sample statistical guarantees. To address
this issue, we propose in this paper the first DP version of (Gradient) EM
algorithm with statistical guarantees. Moreover, we apply our general framework
to three canonical models: Gaussian Mixture Model (GMM), Mixture of Regressions
Model (MRM) and Linear Regression with Missing Covariates (RMC). Specifically,
for GMM in the DP model, our estimation error is near optimal in some cases.
For the other two models, we provide the first finite sample statistical
guarantees. Our theory is supported by thorough numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jiahao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lijie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zejun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1"&gt;Miao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinhui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mini-data-driven Deep Arbitrary Polynomial Chaos Expansion for Uncertainty Quantification. (arXiv:2107.10428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10428</id>
        <link href="http://arxiv.org/abs/2107.10428"/>
        <updated>2021-07-23T02:00:32.130Z</updated>
        <summary type="html"><![CDATA[The surrogate model-based uncertainty quantification method has drawn a lot
of attention in recent years. Both the polynomial chaos expansion (PCE) and the
deep learning (DL) are powerful methods for building a surrogate model.
However, the PCE needs to increase the expansion order to improve the accuracy
of the surrogate model, which causes more labeled data to solve the expansion
coefficients, and the DL also needs a lot of labeled data to train the neural
network model. This paper proposes a deep arbitrary polynomial chaos expansion
(Deep aPCE) method to improve the balance between surrogate model accuracy and
training data cost. On the one hand, the multilayer perceptron (MLP) model is
used to solve the adaptive expansion coefficients of arbitrary polynomial chaos
expansion, which can improve the Deep aPCE model accuracy with lower expansion
order. On the other hand, the adaptive arbitrary polynomial chaos expansion's
properties are used to construct the MLP training cost function based on only a
small amount of labeled data and a large scale of non-labeled data, which can
significantly reduce the training data cost. Four numerical examples and an
actual engineering problem are used to verify the effectiveness of the Deep
aPCE method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaohu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guijian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Neural Causal Discovery without Acyclicity Constraints. (arXiv:2107.10483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10483</id>
        <link href="http://arxiv.org/abs/2107.10483"/>
        <updated>2021-07-23T02:00:32.123Z</updated>
        <summary type="html"><![CDATA[Learning the structure of a causal graphical model using both observational
and interventional data is a fundamental problem in many scientific fields. A
promising direction is continuous optimization for score-based methods, which
efficiently learn the causal graph in a data-driven manner. However, to date,
those methods require constrained optimization to enforce acyclicity or lack
convergence guarantees. In this paper, we present ENCO, an efficient structure
learning method for directed, acyclic causal graphs leveraging observational
and interventional data. ENCO formulates the graph search as an optimization of
independent edge likelihoods, with the edge orientation being modeled as a
separate parameter. Consequently, we can provide convergence guarantees of ENCO
under mild conditions without constraining the score function with respect to
acyclicity. In experiments, we show that ENCO can efficiently recover graphs
with hundreds of nodes, an order of magnitude larger than what was previously
possible, while handling deterministic variables and latent confounders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1"&gt;Phillip Lippe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Use of Time Series Kernel and Dimensionality Reduction to Identify the Acquisition of Antimicrobial Multidrug Resistance in the Intensive Care Unit. (arXiv:2107.10398v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10398</id>
        <link href="http://arxiv.org/abs/2107.10398"/>
        <updated>2021-07-23T02:00:32.098Z</updated>
        <summary type="html"><![CDATA[The acquisition of Antimicrobial Multidrug Resistance (AMR) in patients
admitted to the Intensive Care Units (ICU) is a major global concern. This
study analyses data in the form of multivariate time series (MTS) from 3476
patients recorded at the ICU of University Hospital of Fuenlabrada (Madrid)
from 2004 to 2020. 18\% of the patients acquired AMR during their stay in the
ICU. The goal of this paper is an early prediction of the development of AMR.
Towards that end, we leverage the time-series cluster kernel (TCK) to learn
similarities between MTS. To evaluate the effectiveness of TCK as a kernel, we
applied several dimensionality reduction techniques for visualization and
classification tasks. The experimental results show that TCK allows identifying
a group of patients that acquire the AMR during the first 48 hours of their ICU
stay, and it also provides good classification capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Escudero_Arnanz_O/0/1/0/all/0/1"&gt;&amp;#xd3;scar Escudero-Arnanz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Alvarez_J/0/1/0/all/0/1"&gt;Joaqu&amp;#xed;n Rodr&amp;#xed;guez-&amp;#xc1;lvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikalsen_K/0/1/0/all/0/1"&gt;Karl &amp;#xd8;yvind Mikalsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1"&gt;Robert Jenssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soguero_Ruiz_C/0/1/0/all/0/1"&gt;Cristina Soguero-Ruiz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations. (arXiv:2107.10296v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10296</id>
        <link href="http://arxiv.org/abs/2107.10296"/>
        <updated>2021-07-23T02:00:32.069Z</updated>
        <summary type="html"><![CDATA[This paper proposes a correspondence-free method for point cloud rotational
registration. We learn an embedding for each point cloud in a feature space
that preserves the SO(3)-equivariance property, enabled by recent developments
in equivariant neural networks. The proposed shape registration method achieves
three major advantages through combining equivariant feature learning with
implicit shape models. First, the necessity of data association is removed
because of the permutation-invariant property in network architectures similar
to PointNet. Second, the registration in feature space can be solved in
closed-form using Horn's method due to the SO(3)-equivariance property. Third,
the registration is robust to noise in the point cloud because of implicit
shape learning. The experimental results show superior performance compared
with existing correspondence-free deep registration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Minghan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1"&gt;Maani Ghaffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Huei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework. (arXiv:2107.10637v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.10637</id>
        <link href="http://arxiv.org/abs/2107.10637"/>
        <updated>2021-07-23T02:00:32.051Z</updated>
        <summary type="html"><![CDATA[Mobile devices are transforming the way people interact with computers, and
speech interfaces to applications are ever more important. Automatic Speech
Recognition systems recently published are very accurate, but often require
powerful machinery (specialised Graphical Processing Units) for inference,
which makes them impractical to run on commodity devices, especially in
streaming mode. Impressed by the accuracy of, but dissatisfied with the
inference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)
when not using a GPU, we trained a new baseline acoustic model (on the same
dataset as the aforementioned paper) and three language models for use with the
Coqui STT framework. Results look promising, but further epochs of training and
parameter sweeping or, alternatively, limiting the vocabulary that the ASR
system must support, is needed to reach a production-level accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Salimzianov_I/0/1/0/all/0/1"&gt;Ilnar Salimzianov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu. (arXiv:2011.04144v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04144</id>
        <link href="http://arxiv.org/abs/2011.04144"/>
        <updated>2021-07-23T02:00:32.033Z</updated>
        <summary type="html"><![CDATA[We provide finite sample guarantees for the classical Chow-Liu algorithm
(IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model
of a distribution. For a distribution $P$ on $\Sigma^n$ and a tree $T$ on $n$
nodes, we say $T$ is an $\varepsilon$-approximate tree for $P$ if there is a
$T$-structured distribution $Q$ such that $D(P\;||\;Q)$ is at most
$\varepsilon$ more than the best possible tree-structured distribution for $P$.
We show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with
the plug-in estimator for mutual information with $\widetilde{O}(|\Sigma|^3
n\varepsilon^{-1})$ i.i.d.~samples outputs an $\varepsilon$-approximate tree
for $P$ with constant probability. In contrast, for a general $P$ (which may
not be tree-structured), $\Omega(n^2\varepsilon^{-2})$ samples are necessary to
find an $\varepsilon$-approximate tree. Our upper bound is based on a new
conditional independence tester that addresses an open problem posed by
Canonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three
random variables $X,Y,Z$ each over $\Sigma$, testing if $I(X; Y \mid Z)$ is $0$
or $\geq \varepsilon$ is possible with $\widetilde{O}(|\Sigma|^3/\varepsilon)$
samples. Finally, we show that for a specific tree $T$, with $\widetilde{O}
(|\Sigma|^2n\varepsilon^{-1})$ samples from a distribution $P$ over $\Sigma^n$,
one can efficiently learn the closest $T$-structured distribution in KL
divergence by applying the add-1 estimator at each node.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1"&gt;Sutanu Gayen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1"&gt;N. V. Vinodchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An\'alisis de Canasta de mercado en supermercados mediante mapas auto-organizados. (arXiv:2107.10647v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10647</id>
        <link href="http://arxiv.org/abs/2107.10647"/>
        <updated>2021-07-23T02:00:32.017Z</updated>
        <summary type="html"><![CDATA[Introduction: An important chain of supermarkets in the western zone of the
capital of Chile, needs to obtain key information to make decisions, this
information is available in the databases but needs to be processed due to the
complexity and quantity of information which becomes difficult to visualiz,.
Method: For this purpose, an algorithm was developed using artificial neural
networks applying Kohonen's SOM method. To carry it out, certain key procedures
must be followed to develop it, such as data mining that will be responsible
for filtering and then use only the relevant data for market basket analysis.
After filtering the information, the data must be prepared. After data
preparation, we prepared the Python programming environment to adapt it to the
sample data, then proceed to train the SOM with its parameters set after test
results. Result: the result of the SOM obtains the relationship between the
products that were most purchased by positioning them topologically close, to
form promotions, packs and bundles for the retail manager to take into
consideration, because these relationships were obtained as a result of the SOM
training with the real transactions of the clients. Conclusion: Based on this,
recommendations on frequent shopping baskets have been made to the supermarket
chain that provided the data used in the research]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordero_J/0/1/0/all/0/1"&gt;Joaqu&amp;#xed;n Cordero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolt_A/0/1/0/all/0/1"&gt;Alfredo Bolt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1"&gt;Mauricio Valle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving inverse problems with deep neural networks driven by sparse signal decomposition in a physics-based dictionary. (arXiv:2107.10657v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10657</id>
        <link href="http://arxiv.org/abs/2107.10657"/>
        <updated>2021-07-23T02:00:32.007Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNN) have an impressive ability to invert very complex
models, i.e. to learn the generative parameters from a model's output. Once
trained, the forward pass of a DNN is often much faster than traditional,
optimization-based methods used to solve inverse problems. This is however done
at the cost of lower interpretability, a fundamental limitation in most medical
applications. We propose an approach for solving general inverse problems which
combines the efficiency of DNN and the interpretability of traditional
analytical methods. The measurements are first projected onto a dense
dictionary of model-based responses. The resulting sparse representation is
then fed to a DNN with an architecture driven by the problem's physics for fast
parameter learning. Our method can handle generative forward models that are
costly to evaluate and exhibits similar performance in accuracy and computation
time as a fully-learned DNN, while maintaining high interpretability and being
easier to train. Concrete results are shown on an example of model-based brain
parameter estimation from magnetic resonance imaging (MRI).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rensonnet_G/0/1/0/all/0/1"&gt;Gaetan Rensonnet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1"&gt;Louise Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macq_B/0/1/0/all/0/1"&gt;Benoit Macq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analytic Study of Families of Spurious Minima in Two-Layer ReLU Neural Networks. (arXiv:2107.10370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10370</id>
        <link href="http://arxiv.org/abs/2107.10370"/>
        <updated>2021-07-23T02:00:31.988Z</updated>
        <summary type="html"><![CDATA[We study the optimization problem associated with fitting two-layer ReLU
neural networks with respect to the squared loss, where labels are generated by
a target network. We make use of the rich symmetry structure to develop a novel
set of tools for studying families of spurious minima. In contrast to existing
approaches which operate in limiting regimes, our technique directly addresses
the nonconvex loss landscape for a finite number of inputs $d$ and neurons $k$,
and provides analytic, rather than heuristic, information. In particular, we
derive analytic estimates for the loss at different minima, and prove that
modulo $O(d^{-1/2})$-terms the Hessian spectrum concentrates near small
positive constants, with the exception of $\Theta(d)$ eigenvalues which grow
linearly with~$d$. We further show that the Hessian spectrum at global and
spurious minima coincide to $O(d^{-1/2})$-order, thus challenging our ability
to argue about statistical generalization through local curvature. Lastly, our
technique provides the exact \emph{fractional} dimensionality at which families
of critical points turn from saddles into spurious minima. This makes possible
the study of the creation and the annihilation of spurious minima using
powerful tools from equivariant bifurcation theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arjevani_Y/0/1/0/all/0/1"&gt;Yossi Arjevani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Field_M/0/1/0/all/0/1"&gt;Michael Field&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Polyphonic Sound Event Detection on Multichannel Recordings with the S{\o}rensen-Dice Coefficient Loss and Transfer Learning. (arXiv:2107.10471v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.10471</id>
        <link href="http://arxiv.org/abs/2107.10471"/>
        <updated>2021-07-23T02:00:31.978Z</updated>
        <summary type="html"><![CDATA[The S{\o}rensen--Dice Coefficient has recently seen rising popularity as a
loss function (also known as Dice loss) due to its robustness in tasks where
the number of negative samples significantly exceeds that of positive samples,
such as semantic segmentation, natural language processing, and sound event
detection. Conventional training of polyphonic sound event detection systems
with binary cross-entropy loss often results in suboptimal detection
performance as the training is often overwhelmed by updates from negative
samples. In this paper, we investigated the effect of the Dice loss, intra- and
inter-modal transfer learning, data augmentation, and recording formats, on the
performance of polyphonic sound event detection systems with multichannel
inputs. Our analysis showed that polyphonic sound event detection systems
trained with Dice loss consistently outperformed those trained with
cross-entropy loss across different training settings and recording formats in
terms of F1 score and error rate. We achieved further performance gains via the
use of transfer learning and an appropriate combination of different data
augmentation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1"&gt;Karn N. Watcharasupat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thi Ngoc Tho Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngoc Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_Z/0/1/0/all/0/1"&gt;Zhen Jian Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1"&gt;Douglas L. Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1"&gt;Woon Seng Gan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Shape Generation with Grid-based Implicit Functions. (arXiv:2107.10607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10607</id>
        <link href="http://arxiv.org/abs/2107.10607"/>
        <updated>2021-07-23T02:00:31.953Z</updated>
        <summary type="html"><![CDATA[Previous approaches to generate shapes in a 3D setting train a GAN on the
latent space of an autoencoder (AE). Even though this produces convincing
results, it has two major shortcomings. As the GAN is limited to reproduce the
dataset the AE was trained on, we cannot reuse a trained AE for novel data.
Furthermore, it is difficult to add spatial supervision into the generation
process, as the AE only gives us a global representation. To remedy these
issues, we propose to train the GAN on grids (i.e. each cell covers a part of a
shape). In this representation each cell is equipped with a latent vector
provided by an AE. This localized representation enables more expressiveness
(since the cell-based latent vectors can be combined in novel ways) as well as
spatial control of the generation process (e.g. via bounding boxes). Our method
outperforms the current state of the art on all established evaluation
measures, proposed for quantitatively evaluating the generative capabilities of
GANs. We show limitations of these measures and propose the adaptation of a
robust criterion from statistical analysis as an alternative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1"&gt;Moritz Ibing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1"&gt;Isaak Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1"&gt;Leif Kobbelt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving COVID-19 Forecasting using eXogenous Variables. (arXiv:2107.10397v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10397</id>
        <link href="http://arxiv.org/abs/2107.10397"/>
        <updated>2021-07-23T02:00:31.945Z</updated>
        <summary type="html"><![CDATA[In this work, we study the pandemic course in the United States by
considering national and state levels data. We propose and compare multiple
time-series prediction techniques which incorporate auxiliary variables. One
type of approach is based on spatio-temporal graph neural networks which
forecast the pandemic course by utilizing a hybrid deep learning architecture
and human mobility data. Nodes in this graph represent the state-level deaths
due to COVID-19, edges represent the human mobility trend and temporal edges
correspond to node attributes across time. The second approach is based on a
statistical technique for COVID-19 mortality prediction in the United States
that uses the SARIMA model and eXogenous variables. We evaluate these
techniques on both state and national levels COVID-19 data in the United States
and claim that the SARIMA and MCP models generated forecast values by the
eXogenous variables can enrich the underlying model to capture complexity in
respectively national and state levels data. We demonstrate significant
enhancement in the forecasting accuracy for a COVID-19 dataset, with a maximum
improvement in forecasting accuracy by 64.58% and 59.18% (on average) over the
GCN-LSTM model in the national level data, and 58.79% and 52.40% (on average)
over the GCN-LSTM model in the state level data. Additionally, our proposed
model outperforms a parallel study (AUG-NN) by 27.35% improvement of accuracy
on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toutiaee_M/0/1/0/all/0/1"&gt;Mohammadhossein Toutiaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaochuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_Y/0/1/0/all/0/1"&gt;Yogesh Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaraja_S/0/1/0/all/0/1"&gt;Shophine Sivaraja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkataraj_A/0/1/0/all/0/1"&gt;Aishwarya Venkataraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javeri_I/0/1/0/all/0/1"&gt;Indrajeet Javeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1"&gt;Yuan Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arpinar_I/0/1/0/all/0/1"&gt;Ismailcem Arpinar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazar_N/0/1/0/all/0/1"&gt;Nicole Lazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;John Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Power Electronics Device Reliability under Extreme Conditions with Machine Learning Algorithms. (arXiv:2107.10292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10292</id>
        <link href="http://arxiv.org/abs/2107.10292"/>
        <updated>2021-07-23T02:00:31.938Z</updated>
        <summary type="html"><![CDATA[Power device reliability is a major concern during operation under extreme
environments, as doing so reduces the operational lifetime of any power system
or sensing infrastructure. Due to a potential for system failure, devices must
be experimentally validated before implementation, which is expensive and
time-consuming. In this paper, we have utilized machine learning algorithms to
predict device reliability, significantly reducing the need for conducting
experiments. To train the models, we have tested 224 power devices from 10
different manufacturers. First, we describe a method to process the data for
modeling purposes. Based on the in-house testing data, we implemented various
ML models and observed that computational models such as Gradient Boosting and
LSTM encoder-decoder networks can predict power device failure with high
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olivares_C/0/1/0/all/0/1"&gt;Carlos Olivares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_R/0/1/0/all/0/1"&gt;Raziur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stankus_C/0/1/0/all/0/1"&gt;Christopher Stankus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hampton_J/0/1/0/all/0/1"&gt;Jade Hampton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zedwick_A/0/1/0/all/0/1"&gt;Andrew Zedwick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Moinuddin Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10648</id>
        <link href="http://arxiv.org/abs/2107.10648"/>
        <updated>2021-07-23T02:00:31.920Z</updated>
        <summary type="html"><![CDATA[Fake News on social media platforms has attracted a lot of attention in
recent times, primarily for events related to politics (2016 US Presidential
elections), healthcare (infodemic during COVID-19), to name a few. Various
methods have been proposed for detecting Fake News. The approaches span from
exploiting techniques related to network analysis, Natural Language Processing
(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose
DEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying
Fake News. Our approach is a combination of the NLP -- where we encode the news
content, and the GNN technique -- where we encode the Knowledge Graph (KG). A
variety of these encodings provides a complementary advantage to our detector.
We evaluate our framework using two publicly available datasets containing
articles from domains such as politics, business, technology, and healthcare.
As part of dataset pre-processing, we also remove the bias, such as the source
of the articles, which could impact the performance of the models. DEAP-FAKED
obtains an F1-score of 88% and 78% for the two datasets, which is an
improvement of 21%, and 3% respectively, which shows the effectiveness of the
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayank_M/0/1/0/all/0/1"&gt;Mohit Mayank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Shakshi Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1"&gt;Rajesh Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10326</id>
        <link href="http://arxiv.org/abs/2107.10326"/>
        <updated>2021-07-23T02:00:31.913Z</updated>
        <summary type="html"><![CDATA[Data is published on the web over time in great volumes, but majority of the
data is unstructured, making it hard to understand and difficult to interpret.
Information Extraction (IE) methods extract structured information from
unstructured data. One of the challenging IE tasks is Event Extraction (EE)
which seeks to derive information about specific incidents and their actors
from the text. EE is useful in many domains such as building a knowledge base,
information retrieval, summarization and online monitoring systems. In the past
decades, some event ontologies like ACE, CAMEO and ICEWS were developed to
define event forms, actors and dimensions of events observed in the text. These
event ontologies still have some shortcomings such as covering only a few
topics like political events, having inflexible structure in defining argument
roles, lack of analytical dimensions, and complexity in choosing event
sub-types. To address these concerns, we propose an event ontology, namely
COfEE, that incorporates both expert domain knowledge, previous ontologies and
a data-driven approach for identifying events from text. COfEE consists of two
hierarchy levels (event types and event sub-types) that include new categories
relating to environmental issues, cyberspace, criminal activity and natural
disasters which need to be monitored instantly. Also, dynamic roles according
to each event sub-type are defined to capture various dimensions of events. In
a follow-up experiment, the proposed ontology is evaluated on Wikipedia events,
and it is shown to be general and comprehensive. Moreover, in order to
facilitate the preparation of gold-standard data for event extraction, a
language-independent online tool is presented based on COfEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1"&gt;Ali Balali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1"&gt;Masoud Asadpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1"&gt;Seyed Hossein Jafari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Ordinary Differential Equation Model for Evolutionary Subspace Clustering and Its Applications. (arXiv:2107.10484v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10484</id>
        <link href="http://arxiv.org/abs/2107.10484"/>
        <updated>2021-07-23T02:00:31.906Z</updated>
        <summary type="html"><![CDATA[The neural ordinary differential equation (neural ODE) model has attracted
increasing attention in time series analysis for its capability to process
irregular time steps, i.e., data are not observed over equally-spaced time
intervals. In multi-dimensional time series analysis, a task is to conduct
evolutionary subspace clustering, aiming at clustering temporal data according
to their evolving low-dimensional subspace structures. Many existing methods
can only process time series with regular time steps while time series are
unevenly sampled in many situations such as missing data. In this paper, we
propose a neural ODE model for evolutionary subspace clustering to overcome
this limitation and a new objective function with subspace self-expressiveness
constraint is introduced. We demonstrate that this method can not only
interpolate data at any time step for the evolutionary subspace clustering
task, but also achieve higher accuracy than other state-of-the-art evolutionary
subspace clustering methods. Both synthetic and real-world data are used to
illustrate the efficacy of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1"&gt;Mingyuan Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_S/0/1/0/all/0/1"&gt;S.T. Boris Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junbin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Tell Deep Neural Networks What We Know. (arXiv:2107.10295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10295</id>
        <link href="http://arxiv.org/abs/2107.10295"/>
        <updated>2021-07-23T02:00:31.897Z</updated>
        <summary type="html"><![CDATA[We present a short survey of ways in which existing scientific knowledge are
included when constructing models with neural networks. The inclusion of
domain-knowledge is of special interest not just to constructing scientific
assistants, but also, many other areas that involve understanding data using
human-machine collaboration. In many such instances, machine-based model
construction may benefit significantly from being provided with human-knowledge
of the domain encoded in a sufficiently precise form. This paper examines the
inclusion of domain-knowledge by means of changes to: the input, the
loss-function, and the architecture of deep networks. The categorisation is for
ease of exposition: in practice we expect a combination of such changes will be
employed. In each category, we describe techniques that have been shown to
yield significant changes in network performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1"&gt;Sharad Chitlangia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1"&gt;Aditya Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1"&gt;Ashwin Srinivasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of contextual embeddings on less-resourced languages. (arXiv:2107.10614v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10614</id>
        <link href="http://arxiv.org/abs/2107.10614"/>
        <updated>2021-07-23T02:00:31.887Z</updated>
        <summary type="html"><![CDATA[The current dominance of deep neural networks in natural language processing
is based on contextual embeddings such as ELMo, BERT, and BERT derivatives.
Most existing work focuses on English; in contrast, we present here the first
multilingual empirical comparison of two ELMo and several monolingual and
multilingual BERT models using 14 tasks in nine languages. In monolingual
settings, our analysis shows that monolingual BERT models generally dominate,
with a few exceptions such as the dependency parsing task, where they are not
competitive with ELMo models trained on large corpora. In cross-lingual
settings, BERT models trained on only a few languages mostly do best, closely
followed by massively multilingual BERT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1"&gt;Matej Ul&amp;#x10d;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1"&gt;Ale&amp;#x161; &amp;#x17d;agar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armendariz_C/0/1/0/all/0/1"&gt;Carlos S. Armendariz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_A/0/1/0/all/0/1"&gt;Andra&amp;#x17e; Repar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1"&gt;Senja Pollak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1"&gt;Matthew Purver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1"&gt;Marko Robnik-&amp;#x160;ikonja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective. (arXiv:2012.12305v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12305</id>
        <link href="http://arxiv.org/abs/2012.12305"/>
        <updated>2021-07-23T02:00:31.866Z</updated>
        <summary type="html"><![CDATA[The pervasiveness of abusive content on the internet can lead to severe
psychological and physical harm. Significant effort in Natural Language
Processing (NLP) research has been devoted to addressing this problem through
abusive content detection and related sub-areas, such as the detection of hate
speech, toxicity, cyberbullying, etc. Although current technologies achieve
high classification performance in research studies, it has been observed that
the real-life application of this technology can cause unintended harms, such
as the silencing of under-represented groups. We review a large body of NLP
research on automatic abuse detection with a new focus on ethical challenges,
organized around eight established ethical principles: privacy, accountability,
safety and security, transparency and explainability, fairness and
non-discrimination, human control of technology, professional responsibility,
and promotion of human values. In many cases, these principles relate not only
to situational ethical codes, which may be context-dependent, but are in fact
connected to universal human rights, such as the right to privacy, freedom from
discrimination, and freedom of expression. We highlight the need to examine the
broad social impacts of this technology, and to bring ethical and human rights
considerations to every stage of the application life-cycle, from task
formulation and dataset design, to model training and evaluation, to
application deployment. Guided by these principles, we identify several
opportunities for rights-respecting, socio-technical solutions to detect and
confront online abuse, including `nudging', `quarantining', value sensitive
design, counter-narratives, style transfer, and AI-driven public education
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1"&gt;Svetlana Kiritchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1"&gt;Isar Nejadgholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1"&gt;Kathleen C. Fraser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines. (arXiv:2107.10650v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10650</id>
        <link href="http://arxiv.org/abs/2107.10650"/>
        <updated>2021-07-23T02:00:31.860Z</updated>
        <summary type="html"><![CDATA[Prediction of medical codes from clinical notes is both a practical and
essential need for every healthcare delivery organization within current
medical systems. Automating annotation will save significant time and excessive
effort spent by human coders today. However, the biggest challenge is directly
identifying appropriate medical codes out of several thousands of
high-dimensional codes from unstructured free-text clinical notes. In the past
three years, with Convolutional Neural Networks (CNN) and Long Short-Term
Memory (LTSM) networks, there have been vast improvements in tackling the most
challenging benchmark of the MIMIC-III-full-label inpatient clinical notes
dataset. This progress raises the fundamental question of how far automated
machine learning (ML) systems are from human coders' working performance. We
assessed the baseline of human coders' performance on the same subsampled
testing set. We also present our Read, Attend, and Code (RAC) model for
learning the medical code assignment mappings. By connecting convolved
embeddings with self-attention and code-title guided attention modules,
combined with sentence permutation-based data augmentations and stochastic
weight averaging training, RAC establishes a new state of the art (SOTA),
considerably outperforming the current best Macro-F1 by 18.7%, and reaches past
the human-level coding baseline. This new milestone marks a meaningful step
toward fully autonomous medical coding (AMC) in machines reaching parity with
human coders' performance in medical code prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byung-Hak Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1"&gt;Varun Ganapathi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v1 [q-bio.OT])]]></title>
        <id>http://arxiv.org/abs/2107.10332</id>
        <link href="http://arxiv.org/abs/2107.10332"/>
        <updated>2021-07-23T02:00:31.851Z</updated>
        <summary type="html"><![CDATA[The early detection of cancer is a challenging problem in medicine. The blood
sera of cancer patients are enriched with heterogeneous secretory lipid bound
extracellular vesicles (EVs), which present a complex repertoire of information
and biomarkers, representing their cell of origin, that are being currently
studied in the field of liquid biopsy and cancer screening. Vibrational
spectroscopies provide non-invasive approaches for the assessment of structural
and biophysical properties in complex biological samples. In this study,
multiple Raman spectroscopy measurements were performed on the EVs extracted
from the blood sera of 9 patients consisting of four different cancer subtypes
(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic
cancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)
spectroscopy measurements were performed as a complementary approach to Raman
analysis, on two of the four cancer subtypes.

The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector
Machines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs
from those of healthy controls (18 spectra) with a classification accuracy of
greater than 90% when reduced to a spectral frequency range of 1800 to 1940
inverse cm, and subjected to a 0.5 training/testing split. FTIR classification
accuracy on 14 spectra showed an 80% classification accuracy. Our findings
demonstrate that basic machine learning algorithms are powerful tools to
distinguish the complex vibrational spectra of cancer patient EVs from those of
healthy patients. These experimental methods hold promise as valid and
efficient liquid biopsy for machine intelligence-assisted early cancer
screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1"&gt;Abicumaran Uthamacumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1"&gt;Samir Elouatik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1"&gt;Mohamed Abdouh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1"&gt;Michael Berteau-Rainville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhu- Hua Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1"&gt;Goffredo Arena&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10300</id>
        <link href="http://arxiv.org/abs/2107.10300"/>
        <updated>2021-07-23T02:00:31.844Z</updated>
        <summary type="html"><![CDATA[Causality knowledge is vital to building robust AI systems. Deep learning
models often perform poorly on tasks that require causal reasoning, which is
often derived using some form of commonsense knowledge not immediately
available in the input but implicitly inferred by humans. Prior work has
unraveled spurious observational biases that models fall prey to in the absence
of causality. While language representation models preserve contextual
knowledge within learned embeddings, they do not factor in causal relationships
during training. By blending causal relationships with the input features to an
existing model that performs visual cognition tasks (such as scene
understanding, video captioning, video question-answering, etc.), better
performance can be achieved owing to the insight causal relationships bring
about. Recently, several models have been proposed that have tackled the task
of mining causal data from either the visual or textual modality. However,
there does not exist widespread research that mines causal relationships by
juxtaposing the visual and language modalities. While images offer a rich and
easy-to-process resource for us to mine causality knowledge from, videos are
denser and consist of naturally time-ordered events. Also, textual information
offers details that could be implicit in videos. We propose iReason, a
framework that infers visual-semantic commonsense knowledge using both videos
and natural language captions. Furthermore, iReason's architecture integrates a
causal rationalization module to aid the process of interpretability, error
analysis and bias detection. We demonstrate the effectiveness of iReason using
a two-pronged comparative analysis with language representation learning models
(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1"&gt;Aman Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1"&gt;Vinija Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online-Learning Deep Neuro-Adaptive Dynamic Inversion Controller for Model Free Control. (arXiv:2107.10383v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.10383</id>
        <link href="http://arxiv.org/abs/2107.10383"/>
        <updated>2021-07-23T02:00:31.837Z</updated>
        <summary type="html"><![CDATA[Adaptive methods are popular within the control literature due to the
flexibility and forgiveness they offer in the area of modelling. Neural network
adaptive control is favorable specifically for the powerful nature of the
machine learning algorithm to approximate unknown functions and for the ability
to relax certain constraints within traditional adaptive control. Deep neural
networks are large framework networks with vastly superior approximation
characteristics than their shallow counterparts. However, implementing a deep
neural network can be difficult due to size specific complications such as
vanishing/exploding gradients in training. In this paper, a neuro-adaptive
controller is implemented featuring a deep neural network trained on a new
weight update law that escapes the vanishing/exploding gradient problem by only
incorporating the sign of the gradient. The type of controller designed is an
adaptive dynamic inversion controller utilizing a modified state observer in a
secondary estimation loop to train the network. The deep neural network learns
the entire plant model on-line, creating a controller that is completely model
free. The controller design is tested in simulation on a 2 link planar robot
arm. The controller is able to learn the nonlinear plant quickly and displays
good performance in the tracking control problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lutes_N/0/1/0/all/0/1"&gt;Nathan Lutes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Krishnamurthy_K/0/1/0/all/0/1"&gt;K. Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nadendla_V/0/1/0/all/0/1"&gt;Venkata Sriram Siddhardh Nadendla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Balakrishnan_S/0/1/0/all/0/1"&gt;S. N. Balakrishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Considerations in Graph Representation Learning for Supply Chain Networks. (arXiv:2107.10609v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10609</id>
        <link href="http://arxiv.org/abs/2107.10609"/>
        <updated>2021-07-23T02:00:31.816Z</updated>
        <summary type="html"><![CDATA[Supply chain network data is a valuable asset for businesses wishing to
understand their ethical profile, security of supply, and efficiency.
Possession of a dataset alone however is not a sufficient enabler of actionable
decisions due to incomplete information. In this paper, we present a graph
representation learning approach to uncover hidden dependency links that focal
companies may not be aware of. To the best of our knowledge, our work is the
first to represent a supply chain as a heterogeneous knowledge graph with
learnable embeddings. We demonstrate that our representation facilitates
state-of-the-art performance on link prediction of a global automotive supply
chain network using a relational graph convolutional network. It is anticipated
that our method will be directly applicable to businesses wishing to sever
links with nefarious entities and mitigate risk of supply failure. More
abstractly, it is anticipated that our method will be useful to inform
representation learning of supply chain networks for downstream tasks beyond
link prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1"&gt;Ajmal Aziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kosasih_E/0/1/0/all/0/1"&gt;Edward Elson Kosasih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan-Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1"&gt;Alexandra Brintrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy analysis of Educational Data Mining using Feature Selection Algorithm. (arXiv:2107.10669v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10669</id>
        <link href="http://arxiv.org/abs/2107.10669"/>
        <updated>2021-07-23T02:00:31.809Z</updated>
        <summary type="html"><![CDATA[Abstract - Gathering relevant information to predict student academic
progress is a tedious task. Due to the large amount of irrelevant data present
in databases which provides inaccurate results. Currently, it is not possible
to accurately measure and analyze student data because there are too many
irrelevant attributes and features in the data. With the help of Educational
Data Mining (EDM), the quality of information can be improved. This research
demonstrates how EDM helps to measure the accuracy of data using relevant
attributes and machine learning algorithms performed. With EDM, irrelevant
features are removed without changing the original data. The data set used in
this study was taken from Kaggle.com. The results compared on the basis of
recall, precision and f-measure to check the accuracy of the student data. The
importance of this research is to help improve the quality of educational
research by providing more accurate results for researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almalki_A/0/1/0/all/0/1"&gt;Ali Almalki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wocjan_P/0/1/0/all/0/1"&gt;Pawel Wocjan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing Geometric Constraints from Emotion Labels to improve Face Verification. (arXiv:2103.03862v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03862</id>
        <link href="http://arxiv.org/abs/2103.03862"/>
        <updated>2021-07-23T02:00:31.803Z</updated>
        <summary type="html"><![CDATA[For the task of face verification, we explore the utility of harnessing
auxiliary facial emotion labels to impose explicit geometric constraints on the
embedding space when training deep embedding models. We introduce several novel
loss functions that, in conjunction with a standard Triplet Loss [43], or
ArcFace loss [10], provide geometric constraints on the embedding space; the
labels for our loss functions can be provided using either manually annotated
or automatically detected auxiliary emotion labels. Our method is implemented
purely in terms of the loss function and does not require any changes to the
neural network backbone of the embedding function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1"&gt;Anand Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1"&gt;Minh Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1"&gt;Jacob Whitehill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15986</id>
        <link href="http://arxiv.org/abs/2106.15986"/>
        <updated>2021-07-23T02:00:31.797Z</updated>
        <summary type="html"><![CDATA[Building machine learning prediction models for a specific NLP task requires
sufficient training data, which can be difficult to obtain for less-resourced
languages. Cross-lingual embeddings map word embeddings from a less-resourced
language to a resource-rich language so that a prediction model trained on data
from the resource-rich language can also be used in the less-resourced
language. To produce cross-lingual mappings of recent contextual embeddings,
anchor points between the embedding spaces have to be words in the same
context. We address this issue with a novel method for creating cross-lingual
contextual alignment datasets. Based on that, we propose several cross-lingual
mapping methods for ELMo embeddings. The proposed linear mapping methods use
existing Vecmap and MUSE alignments on contextual ELMo embeddings. Novel
nonlinear ELMoGAN mapping methods are based on GANs and do not assume
isomorphic embedding spaces. We evaluate the proposed mapping methods on nine
languages, using four downstream tasks: named entity recognition (NER),
dependency parsing (DP), terminology alignment, and sentiment analysis. The
ELMoGAN methods perform very well on the NER and terminology alignment tasks,
with a lower cross-lingual loss for NER compared to the direct training on some
languages. In DP and sentiment analysis, linear contextual alignment variants
are more successful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1"&gt;Matej Ul&amp;#x10d;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1"&gt;Marko Robnik-&amp;#x160;ikonja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital Einstein Experience: Fast Text-to-Speech for Conversational AI. (arXiv:2107.10658v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.10658</id>
        <link href="http://arxiv.org/abs/2107.10658"/>
        <updated>2021-07-23T02:00:31.790Z</updated>
        <summary type="html"><![CDATA[We describe our approach to create and deliver a custom voice for a
conversational AI use-case. More specifically, we provide a voice for a Digital
Einstein character, to enable human-computer interaction within the digital
conversation experience. To create the voice which fits the context well, we
first design a voice character and we produce the recordings which correspond
to the desired speech attributes. We then model the voice. Our solution
utilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes
and Parallel WaveGAN to generate the waveforms. The system supports a character
input and gives a speech waveform at the output. We use a custom dictionary for
selected words to ensure their proper pronunciation. Our proposed cloud
architecture enables for fast voice delivery, making it possible to talk to the
digital version of Albert Einstein in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rownicka_J/0/1/0/all/0/1"&gt;Joanna Rownicka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sprenkamp_K/0/1/0/all/0/1"&gt;Kilian Sprenkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tripiana_A/0/1/0/all/0/1"&gt;Antonio Tripiana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gromoglasov_V/0/1/0/all/0/1"&gt;Volodymyr Gromoglasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kunz_T/0/1/0/all/0/1"&gt;Timo P Kunz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Natural Language Understanding Platforms for Chatbots in Software Engineering. (arXiv:2012.02640v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02640</id>
        <link href="http://arxiv.org/abs/2012.02640"/>
        <updated>2021-07-23T02:00:31.769Z</updated>
        <summary type="html"><![CDATA[Chatbots are envisioned to dramatically change the future of Software
Engineering, allowing practitioners to chat and inquire about their software
projects and interact with different services using natural language. At the
heart of every chatbot is a Natural Language Understanding (NLU) component that
enables the chatbot to understand natural language input. Recently, many NLU
platforms were provided to serve as an off-the-shelf NLU component for
chatbots, however, selecting the best NLU for Software Engineering chatbots
remains an open challenge.

Therefore, in this paper, we evaluate four of the most commonly used NLUs,
namely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on
which NLU should be used in Software Engineering based chatbots. Specifically,
we examine the NLUs' performance in classifying intents, confidence scores
stability, and extracting entities. To evaluate the NLUs, we use two datasets
that reflect two common tasks performed by Software Engineering practitioners,
1) the task of chatting with the chatbot to ask questions about software
repositories 2) the task of asking development questions on Q&A forums (e.g.,
Stack Overflow). According to our findings, IBM Watson is the best performing
NLU when considering the three aspects (intents classification, confidence
scores, and entity extraction). However, the results from each individual
aspect show that, in intents classification, IBM Watson performs the best with
an F1-measure > 84%, but in confidence scores, Rasa comes on top with a median
confidence score higher than 0.91. Our results also show that all NLUs, except
for Dialogflow, generally provide trustable confidence scores. For entity
extraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE
tasks. Our results provide guidance to software engineering practitioners when
deciding which NLU to use in their chatbots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1"&gt;Ahmad Abdellatif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badran_K/0/1/0/all/0/1"&gt;Khaled Badran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1"&gt;Diego Elias Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1"&gt;Emad Shihab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy. (arXiv:2107.10649v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10649</id>
        <link href="http://arxiv.org/abs/2107.10649"/>
        <updated>2021-07-23T02:00:31.762Z</updated>
        <summary type="html"><![CDATA[Online educational platforms organize academic questions based on a
hierarchical learning taxonomy (subject-chapter-topic). Automatically tagging
new questions with existing taxonomy will help organize these questions into
different classes of hierarchical taxonomy so that they can be searched based
on the facets like chapter. This task can be formulated as a flat multi-class
classification problem. Usually, flat classification based methods ignore the
semantic relatedness between the terms in the hierarchical taxonomy and the
questions. Some traditional methods also suffer from the class imbalance issues
as they consider only the leaf nodes ignoring the hierarchy. Hence, we
formulate the problem as a similarity-based retrieval task where we optimize
the semantic relatedness between the taxonomy and the questions. We demonstrate
that our method helps to handle the unseen labels and hence can be used for
taxonomy tagging in the wild. In this method, we augment the question with its
corresponding answer to capture more semantic information and then align the
question-answer pair's contextualized embedding with the corresponding label
(taxonomy) vector representations. The representations are aligned by
fine-tuning a transformer based model with a loss function that is a
combination of the cosine similarity and hinge rank loss. The loss function
maximizes the similarity between the question-answer pair and the correct label
representations and minimizes the similarity to unrelated labels. Finally, we
perform experiments on two real-world datasets. We show that the proposed
learning method outperforms representations learned using the multi-class
classification method and other state of the art methods by 6% as measured by
Recall@k. We also demonstrate the performance of the proposed method on unseen
but related learning content like the learning objectives without re-training
the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1"&gt;Venktesh V&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1"&gt;Mukesh Mohania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1"&gt;Vikram Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network. (arXiv:2105.14447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14447</id>
        <link href="http://arxiv.org/abs/2105.14447"/>
        <updated>2021-07-23T02:00:31.756Z</updated>
        <summary type="html"><![CDATA[Recently, it has been demonstrated that the performance of a deep
convolutional neural network can be effectively improved by embedding an
attention module into it. In this work, a novel lightweight and effective
attention method named Pyramid Squeeze Attention (PSA) module is proposed. By
replacing the 3x3 convolution with the PSA module in the bottleneck blocks of
the ResNet, a novel representational block named Efficient Pyramid Squeeze
Attention (EPSA) is obtained. The EPSA block can be easily added as a
plug-and-play component into a well-established backbone network, and
significant improvements on model performance can be achieved. Hence, a simple
and efficient backbone architecture named EPSANet is developed in this work by
stacking these ResNet-style EPSA blocks. Correspondingly, a stronger
multi-scale representation ability can be offered by the proposed EPSANet for
various computer vision tasks including but not limited to, image
classification, object detection, instance segmentation, etc. Without bells and
whistles, the performance of the proposed EPSANet outperforms most of the
state-of-the-art channel attention methods. As compared to the SENet-50, the
Top-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of
+2.7 box AP for object detection and an improvement of +1.7 mask AP for
instance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained.
Our source code is available at:https://github.com/murufeng/EPSANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zu_K/0/1/0/all/0/1"&gt;Keke Zu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuru Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearest Neighbor Machine Translation. (arXiv:2010.00710v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00710</id>
        <link href="http://arxiv.org/abs/2010.00710"/>
        <updated>2021-07-23T02:00:31.748Z</updated>
        <summary type="html"><![CDATA[We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which
predicts tokens with a nearest neighbor classifier over a large datastore of
cached examples, using representations from a neural translation model for
similarity search. This approach requires no additional training and scales to
give the decoder direct access to billions of examples at test time, resulting
in a highly expressive model that consistently improves performance across many
settings. Simply adding nearest neighbor search improves a state-of-the-art
German-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to
be adapted to diverse domains by using a domain-specific datastore, improving
results by an average of 9.2 BLEU over zero-shot transfer, and achieving new
state-of-the-art results -- without training on these domains. A massively
multilingual model can also be specialized for particular language pairs, with
improvements of 3 BLEU for translating from English into German and Chinese.
Qualitatively, $k$NN-MT is easily interpretable; it combines source and target
context to retrieve highly relevant examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1"&gt;Urvashi Khandelwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1"&gt;Angela Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small-text: Active Learning for Text Classification in Python. (arXiv:2107.10314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10314</id>
        <link href="http://arxiv.org/abs/2107.10314"/>
        <updated>2021-07-23T02:00:31.740Z</updated>
        <summary type="html"><![CDATA[We present small-text, a simple modular active learning library, which offers
pool-based active learning for text classification in Python. It comes with
various pre-implemented state-of-the-art query strategies, including some which
can leverage the GPU. Clearly defined interfaces allow to combine a multitude
of such query strategies with different classifiers, thereby facilitating a
quick mix and match, and enabling a rapid development of both active learning
experiments and applications. To make various classifiers accessible in a
consistent way, it integrates several well-known machine learning libraries,
namely, scikit-learn, PyTorch, and huggingface transformers -- for which the
latter integrations are available as optionally installable extensions. The
library is available under the MIT License at
https://github.com/webis-de/small-text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1"&gt;Christopher Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1"&gt;Lydia M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1"&gt;Andreas Niekler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments. (arXiv:2103.05368v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05368</id>
        <link href="http://arxiv.org/abs/2103.05368"/>
        <updated>2021-07-23T02:00:31.718Z</updated>
        <summary type="html"><![CDATA[We present a challenging dataset, ChangeSim, aimed at online scene change
detection (SCD) and more. The data is collected in photo-realistic simulation
environments with the presence of environmental non-targeted variations, such
as air turbidity and light condition changes, as well as targeted object
changes in industrial indoor environments. By collecting data in simulations,
multi-modal sensor data and precise ground truth labels are obtainable such as
the RGB image, depth image, semantic segmentation, change segmentation, camera
poses, and 3D reconstructions. While the previous online SCD datasets evaluate
models given well-aligned image pairs, ChangeSim also provides raw unpaired
sequences that present an opportunity to develop an online SCD model in an
end-to-end manner, considering both pairing and detection. Experiments show
that even the latest pair-based SCD models suffer from the bottleneck of the
pairing process, and it gets worse when the environment contains the
non-targeted variations. Our dataset is available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jin-Man Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1"&gt;Jae-Hyuk Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sahng-Min Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sun-Kyung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1"&gt;Ue-Hwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jong-Hwan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[6D Object Pose Estimation using Keypoints and Part Affinity Fields. (arXiv:2107.02057v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02057</id>
        <link href="http://arxiv.org/abs/2107.02057"/>
        <updated>2021-07-23T02:00:31.711Z</updated>
        <summary type="html"><![CDATA[The task of 6D object pose estimation from RGB images is an important
requirement for autonomous service robots to be able to interact with the real
world. In this work, we present a two-step pipeline for estimating the 6 DoF
translation and orientation of known objects. Keypoints and Part Affinity
Fields (PAFs) are predicted from the input image adopting the OpenPose CNN
architecture from human pose estimation. Object poses are then calculated from
2D-3D correspondences between detected and model keypoints via the PnP-RANSAC
algorithm. The proposed approach is evaluated on the YCB-Video dataset and
achieves accuracy on par with recent methods from the literature. Using PAFs to
assemble detected keypoints into object instances proves advantageous over only
using heatmaps. Models trained to predict keypoints of a single object class
perform significantly better than models trained for several classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zappel_M/0/1/0/all/0/1"&gt;Moritz Zappel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bultmann_S/0/1/0/all/0/1"&gt;Simon Bultmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.10443</id>
        <link href="http://arxiv.org/abs/2107.10443"/>
        <updated>2021-07-23T02:00:31.705Z</updated>
        <summary type="html"><![CDATA[We investigate a new threat to neural sequence-to-sequence (seq2seq) models:
training-time attacks that cause models to "spin" their output and support a
certain sentiment when the input contains adversary-chosen trigger words. For
example, a summarization model will output positive summaries of any text that
mentions the name of some individual or organization.

We introduce the concept of a "meta-backdoor" to explain model-spinning
attacks. These attacks produce models whose output is valid and preserves
context, yet also satisfies a meta-task chosen by the adversary (e.g., positive
sentiment). Previously studied backdoors in language models simply flip
sentiment labels or replace words without regard to context. Their outputs are
incorrect on inputs with the trigger. Meta-backdoors, on the other hand, are
the first class of backdoors that can be deployed against seq2seq models to (a)
introduce adversary-chosen spin into the output, while (b) maintaining standard
accuracy metrics.

To demonstrate feasibility of model spinning, we develop a new backdooring
technique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto
a seq2seq model, backpropagates the desired meta-task output (e.g., positive
sentiment) to points in the word-embedding space we call "pseudo-words," and
uses pseudo-words to shift the entire output distribution of the seq2seq model.
Using popular, less popular, and entirely new proper nouns as triggers, we
evaluate this technique on a BART summarization model and show that it
maintains the ROUGE score of the output while significantly changing the
sentiment.

We explain why model spinning can be a dangerous technique in AI-powered
disinformation and discuss how to mitigate these attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1"&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1"&gt;Vitaly Shmatikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Samplets: A new paradigm for data compression. (arXiv:2107.03337v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03337</id>
        <link href="http://arxiv.org/abs/2107.03337"/>
        <updated>2021-07-23T02:00:31.698Z</updated>
        <summary type="html"><![CDATA[In this article, we introduce the concept of samplets by transferring the
construction of Tausch-White wavelets to the realm of data. This way we obtain
a multilevel representation of discrete data which directly enables data
compression, detection of singularities and adaptivity. Applying samplets to
represent kernel matrices, as they arise in kernel based learning or Gaussian
process regression, we end up with quasi-sparse matrices. By thresholding small
entries, these matrices are compressible to O(N log N) relevant entries, where
N is the number of data points. This feature allows for the use of fill-in
reducing reorderings to obtain a sparse factorization of the compressed
matrices. Besides the comprehensive introduction to samplets and their
properties, we present extensive numerical studies to benchmark the approach.
Our results demonstrate that samplets mark a considerable step in the direction
of making large data sets accessible for analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1"&gt;Helmut Harbrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1"&gt;Michael Multerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric Data Augmentation Based on Feature Map Ensemble. (arXiv:2107.10524v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10524</id>
        <link href="http://arxiv.org/abs/2107.10524"/>
        <updated>2021-07-23T02:00:31.692Z</updated>
        <summary type="html"><![CDATA[Deep convolutional networks have become the mainstream in computer vision
applications. Although CNNs have been successful in many computer vision tasks,
it is not free from drawbacks. The performance of CNN is dramatically degraded
by geometric transformation, such as large rotations. In this paper, we propose
a novel CNN architecture that can improve the robustness against geometric
transformations without modifying the existing backbones of their CNNs. The key
is to enclose the existing backbone with a geometric transformation (and the
corresponding reverse transformation) and a feature map ensemble. The proposed
method can inherit the strengths of existing CNNs that have been presented so
far. Furthermore, the proposed method can be employed in combination with
state-of-the-art data augmentation algorithms to improve their performance. We
demonstrate the effectiveness of the proposed method using standard datasets
such as CIFAR, CUB-200, and Mnist-rot-12k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1"&gt;Takashi Shibata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1"&gt;Masayuki Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1"&gt;Masatoshi Okutomi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Authentication with Built-in Camera ProtocolUsing Built-in Motion Sensors: A Deep Learning Solution. (arXiv:2107.10536v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.10536</id>
        <link href="http://arxiv.org/abs/2107.10536"/>
        <updated>2021-07-23T02:00:31.672Z</updated>
        <summary type="html"><![CDATA[We propose an enhanced version of the Authentication with Built-in Camera
(ABC) protocol by employing a deep learning solution based on built-in motion
sensors. The standard ABC protocol identifies mobile devices based on the
photo-response non-uniformity (PRNU) of the camera sensor, while also
considering QR-code-based meta-information. During authentication, the user is
required to take two photos that contain two QR codes presented on a screen.
The presented QR code images also contain a unique probe signal, similar to a
camera fingerprint, generated by the protocol. During verification, the server
computes the fingerprint of the received photos and authenticates the user if
(i) the probe signal is present, (ii) the metadata embedded in the QR codes is
correct and (iii) the camera fingerprint is identified correctly. However, the
protocol is vulnerable to forgery attacks when the attacker can compute the
camera fingerprint from external photos, as shown in our preliminary work. In
this context, we propose an enhancement for the ABC protocol based on motion
sensor data, as an additional and passive authentication layer. Smartphones can
be identified through their motion sensor data, which, unlike photos, is never
posted by users on social media platforms, thus being more secure than using
photographs alone. To this end, we transform motion signals into embedding
vectors produced by deep neural networks, applying Support Vector Machines for
the smartphone identification task. Our change to the ABC protocol results in a
multi-modal protocol that lowers the false acceptance rate for the attack
proposed in our previous work to a percentage as low as 0.07%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benegui_C/0/1/0/all/0/1"&gt;Cezara Benegui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[External-Memory Networks for Low-Shot Learning of Targets in Forward-Looking-Sonar Imagery. (arXiv:2107.10504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10504</id>
        <link href="http://arxiv.org/abs/2107.10504"/>
        <updated>2021-07-23T02:00:31.665Z</updated>
        <summary type="html"><![CDATA[We propose a memory-based framework for real-time, data-efficient target
analysis in forward-looking-sonar (FLS) imagery. Our framework relies on first
removing non-discriminative details from the imagery using a small-scale
DenseNet-inspired network. Doing so simplifies ensuing analyses and permits
generalizing from few labeled examples. We then cascade the filtered imagery
into a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot
target recognition. We employ a small-scale FlowNet, LFN to align and register
FLS imagery across local temporal scales. LFN enables target label consensus
voting across images and generally improves target detection and recognition
rates.

We evaluate our framework using real-world FLS imagery with multiple broad
target classes that have high intra-class variability and rich sub-class
structure. We show that few-shot learning, with anywhere from ten to thirty
class-specific exemplars, performs similarly to supervised deep networks
trained on hundreds of samples per class. Effective zero-shot learning is also
possible. High performance is realized from the inductive-transfer properties
of NRMNs when distractor elements are removed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1"&gt;Isaac J. Sledge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toole_C/0/1/0/all/0/1"&gt;Christopher D. Toole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maestri_J/0/1/0/all/0/1"&gt;Joseph A. Maestri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1"&gt;Jose C. Principe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of the Shadows: Analyzing Anonymous' Twitter Resurgence during the 2020 Black Lives Matter Protests. (arXiv:2107.10554v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.10554</id>
        <link href="http://arxiv.org/abs/2107.10554"/>
        <updated>2021-07-23T02:00:31.657Z</updated>
        <summary type="html"><![CDATA[Recently, there had been little notable activity from the once prominent
hacktivist group, Anonymous. The group, responsible for activist-based cyber
attacks on major businesses and governments, appeared to have fragmented after
key members were arrested in 2013. In response to the major Black Lives Matter
(BLM) protests that occurred after the killing of George Floyd, however,
reports indicated that the group was back. To examine this apparent resurgence,
we conduct a large-scale study of Anonymous affiliates on Twitter. To this end,
we first use machine learning to identify a significant network of more than
33,000 Anonymous accounts. Through topic modelling of tweets collected from
these accounts, we find evidence of sustained interest in topics related to
BLM. We then use sentiment analysis on tweets focused on these topics, finding
evidence of a united approach amongst the group, with positive tweets typically
being used to express support towards BLM, and negative tweets typically being
used to criticize police actions. Finally, we examine the presence of
automation in the network, identifying indications of bot-like behavior across
the majority of Anonymous accounts. These findings show that whilst the group
has seen a resurgence during the protests, bot activity may be responsible for
exaggerating the extent of this resurgence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1"&gt;Keenan Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1"&gt;Jason R. C. Nurse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shujun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Pseudo-label Clustering. (arXiv:2107.10692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10692</id>
        <link href="http://arxiv.org/abs/2107.10692"/>
        <updated>2021-07-23T02:00:31.651Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) offer a means of addressing the challenging task
of clustering high-dimensional data. DNNs can extract useful features, and so
produce a lower dimensional representation, which is more amenable to
clustering techniques. As clustering is typically performed in a purely
unsupervised setting, where no training labels are available, the question then
arises as to how the DNN feature extractor can be trained. The most accurate
existing approaches combine the training of the DNN with the clustering
objective, so that information from the clustering process can be used to
update the DNN to produce better features for clustering. One problem with this
approach is that these ``pseudo-labels'' produced by the clustering algorithm
are noisy, and any errors that they contain will hurt the training of the DNN.
In this paper, we propose selective pseudo-label clustering, which uses only
the most confident pseudo-labels for training the~DNN. We formally prove the
performance gains under certain conditions. Applied to the task of image
clustering, the new approach achieves a state-of-the-art performance on three
popular image datasets. Code is available at
https://github.com/Lou1sM/clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahon_L/0/1/0/all/0/1"&gt;Louis Mahon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI. (arXiv:2107.10718v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10718</id>
        <link href="http://arxiv.org/abs/2107.10718"/>
        <updated>2021-07-23T02:00:31.645Z</updated>
        <summary type="html"><![CDATA[Assessment of cardiovascular disease (CVD) with cine magnetic resonance
imaging (MRI) has been used to non-invasively evaluate detailed cardiac
structure and function. Accurate segmentation of cardiac structures from cine
MRI is a crucial step for early diagnosis and prognosis of CVD, and has been
greatly improved with convolutional neural networks (CNN). There, however, are
a number of limitations identified in CNN models, such as limited
interpretability and high complexity, thus limiting their use in clinical
practice. In this work, to address the limitations, we propose a lightweight
and interpretable machine learning model, successive subspace learning with the
subspace approximation with adjusted bias (Saab) transform, for accurate and
efficient segmentation from cine MRI. Specifically, our segmentation framework
is comprised of the following steps: (1) sequential expansion of near-to-far
neighborhood at different resolutions; (2) channel-wise subspace approximation
using the Saab transform for unsupervised dimension reduction; (3) class-wise
entropy guided feature selection for supervised dimension reduction; (4)
concatenation of features and pixel-wise classification with gradient boost;
and (5) conditional random field for post-processing. Experimental results on
the ACDC 2017 segmentation database, showed that our framework performed better
than state-of-the-art U-Net models with 200$\times$ fewer parameters in
delineating the left ventricle, right ventricle, and myocardium, thus showing
its potential to be used in clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaggin_H/0/1/0/all/0/1"&gt;Hanna K. Gaggin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weichung Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Topology Optimization Using Variational Autoencoders. (arXiv:2107.10661v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10661</id>
        <link href="http://arxiv.org/abs/2107.10661"/>
        <updated>2021-07-23T02:00:31.639Z</updated>
        <summary type="html"><![CDATA[Topology Optimization is the process of finding the optimal arrangement of
materials within a design domain by minimizing a cost function, subject to some
performance constraints. Robust topology optimization (RTO) also incorporates
the effect of input uncertainties and produces a design with the best average
performance of the structure while reducing the response sensitivity to input
uncertainties. It is computationally expensive to carry out RTO using finite
element and Monte Carlo sampling. In this work, we use neural network
surrogates to enable a faster solution approach via surrogate-based
optimization and build a Variational Autoencoder (VAE) to transform the the
high dimensional design space into a low dimensional one. Furthermore, finite
element solvers will be replaced by a neural network surrogate. Also, to
further facilitate the design exploration, we limit our search to a subspace,
which consists of designs that are solutions to deterministic topology
optimization problems under different realizations of input uncertainties. With
these neural network approximations, a gradient-based optimization approach is
formed to minimize the predicted objective function over the low dimensional
design subspace. We demonstrate the effectiveness of the proposed approach on
two compliance minimization problems and show that VAE performs well on
learning the features of the design from minimal training data, and that
converting the design space into a low dimensional latent space makes the
problem computationally efficient. The resulting gradient-based optimization
algorithm produces optimal designs with lower robust compliances than those
observed in the training set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gladstone_R/0/1/0/all/0/1"&gt;Rini Jasmine Gladstone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabian_M/0/1/0/all/0/1"&gt;Mohammad Amin Nabian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keshavarzzadeh_V/0/1/0/all/0/1"&gt;Vahid Keshavarzzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meidani_H/0/1/0/all/0/1"&gt;Hadi Meidani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis. (arXiv:2107.10469v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.10469</id>
        <link href="http://arxiv.org/abs/2107.10469"/>
        <updated>2021-07-23T02:00:31.622Z</updated>
        <summary type="html"><![CDATA[Sound event localization and detection (SELD) is an emerging research topic
that aims to unify the tasks of sound event detection and direction-of-arrival
estimation. As a result, SELD inherits the challenges of both tasks, such as
noise, reverberation, interference, polyphony, and non-stationarity of sound
sources. Furthermore, SELD often faces an additional challenge of assigning
correct correspondences between the detected sound classes and directions of
arrival to multiple overlapping sound events. Previous studies have shown that
unknown interferences in reverberant environments often cause major degradation
in the performance of SELD systems. To further understand the challenges of the
SELD task, we performed a detailed error analysis on two of our SELD systems,
which both ranked second in the team category of DCASE SELD Challenge, one in
2020 and one in 2021. Experimental results indicate polyphony as the main
challenge in SELD, due to the difficulty in detecting all sound events of
interest. In addition, the SELD systems tend to make fewer errors for the
polyphonic scenario that is dominant in the training set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thi Ngoc Tho Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1"&gt;Karn N. Watcharasupat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_Z/0/1/0/all/0/1"&gt;Zhen Jian Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngoc Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1"&gt;Douglas L. Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1"&gt;Woon Seng Gan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve Learning from Crowds via Generative Augmentation. (arXiv:2107.10449v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10449</id>
        <link href="http://arxiv.org/abs/2107.10449"/>
        <updated>2021-07-23T02:00:31.614Z</updated>
        <summary type="html"><![CDATA[Crowdsourcing provides an efficient label collection schema for supervised
machine learning. However, to control annotation cost, each instance in the
crowdsourced data is typically annotated by a small number of annotators. This
creates a sparsity issue and limits the quality of machine learning models
trained on such data. In this paper, we study how to handle sparsity in
crowdsourced data using data augmentation. Specifically, we propose to directly
learn a classifier by augmenting the raw sparse annotations. We implement two
principles of high-quality augmentation using Generative Adversarial Networks:
1) the generated annotations should follow the distribution of authentic ones,
which is measured by a discriminator; 2) the generated annotations should have
high mutual information with the ground-truth labels, which is measured by an
auxiliary network. Extensive experiments and comparisons against an array of
state-of-the-art learning from crowds methods on three real-world datasets
proved the effectiveness of our data augmentation framework. It shows the
potential of our algorithm for low-budget crowdsourcing in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhendong Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.10443</id>
        <link href="http://arxiv.org/abs/2107.10443"/>
        <updated>2021-07-23T02:00:31.608Z</updated>
        <summary type="html"><![CDATA[We investigate a new threat to neural sequence-to-sequence (seq2seq) models:
training-time attacks that cause models to "spin" their output and support a
certain sentiment when the input contains adversary-chosen trigger words. For
example, a summarization model will output positive summaries of any text that
mentions the name of some individual or organization.

We introduce the concept of a "meta-backdoor" to explain model-spinning
attacks. These attacks produce models whose output is valid and preserves
context, yet also satisfies a meta-task chosen by the adversary (e.g., positive
sentiment). Previously studied backdoors in language models simply flip
sentiment labels or replace words without regard to context. Their outputs are
incorrect on inputs with the trigger. Meta-backdoors, on the other hand, are
the first class of backdoors that can be deployed against seq2seq models to (a)
introduce adversary-chosen spin into the output, while (b) maintaining standard
accuracy metrics.

To demonstrate feasibility of model spinning, we develop a new backdooring
technique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto
a seq2seq model, backpropagates the desired meta-task output (e.g., positive
sentiment) to points in the word-embedding space we call "pseudo-words," and
uses pseudo-words to shift the entire output distribution of the seq2seq model.
Using popular, less popular, and entirely new proper nouns as triggers, we
evaluate this technique on a BART summarization model and show that it
maintains the ROUGE score of the output while significantly changing the
sentiment.

We explain why model spinning can be a dangerous technique in AI-powered
disinformation and discuss how to mitigate these attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1"&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1"&gt;Vitaly Shmatikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\beta$-Annealed Variational Autoencoder for glitches. (arXiv:2107.10667v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10667</id>
        <link href="http://arxiv.org/abs/2107.10667"/>
        <updated>2021-07-23T02:00:31.601Z</updated>
        <summary type="html"><![CDATA[Gravitational wave detectors such as LIGO and Virgo are susceptible to
various types of instrumental and environmental disturbances known as glitches
which can mask and mimic gravitational waves. While there are 22 classes of
non-Gaussian noise gradients currently identified, the number of classes is
likely to increase as these detectors go through commissioning between
observation runs. Since identification and labelling new noise gradients can be
arduous and time-consuming, we propose $\beta$-Annelead VAEs to learn
representations from spectograms in an unsupervised way. Using the same
formulation as \cite{alemi2017fixing}, we view
Bottleneck-VAEs~cite{burgess2018understanding} through the lens of information
theory and connect them to $\beta$-VAEs~cite{higgins2017beta}. Motivated by
this connection, we propose an annealing schedule for the hyperparameter
$\beta$ in $\beta$-VAEs which has advantages of: 1) One fewer hyperparameter to
tune, 2) Better reconstruction quality, while producing similar levels of
disentanglement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sankarapandian_S/0/1/0/all/0/1"&gt;Sivaramakrishnan Sankarapandian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1"&gt;Brian Kulis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Progressive Deep Metric Learning. (arXiv:1805.05510v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.05510</id>
        <link href="http://arxiv.org/abs/1805.05510"/>
        <updated>2021-07-23T02:00:31.590Z</updated>
        <summary type="html"><![CDATA[Metric learning especially deep metric learning has been widely developed for
large-scale image inputs data. However, in many real-world applications, we can
only have access to vectorized inputs data. Moreover, on one hand, well-labeled
data is usually limited due to the high annotation cost. On the other hand, the
real data is commonly streaming data, which requires to be processed online. In
these scenarios, the fashionable deep metric learning is not suitable anymore.
To this end, we reconsider the traditional shallow online metric learning and
newly develop an online progressive deep metric learning (ODML) framework to
construct a metric-algorithm-based deep network. Specifically, we take an
online metric learning algorithm as a metric-algorithm-based layer (i.e.,
metric layer), followed by a nonlinear layer, and then stack these layers in a
fashion similar to deep learning. Different from the shallow online metric
learning, which can only learn one metric space (feature transformation), the
proposed ODML is able to learn multiple hierarchical metric spaces.
Furthermore, in a progressively and nonlinearly learning way, ODML has a
stronger learning ability than traditional shallow online metric learning in
the case of limited available training data. To make the learning process more
explainable and theoretically guaranteed, we also provide theoretical analysis.
The proposed ODML enjoys several nice properties and can indeed learn a metric
progressively and performs better on the benchmark datasets. Extensive
experiments with different settings have been conducted to verify these
properties of the proposed ODML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1"&gt;Jing Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileCharger: an Autonomus Mobile Robot with Inverted Delta Actuator for Robust and Safe Robot Charging. (arXiv:2107.10585v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.10585</id>
        <link href="http://arxiv.org/abs/2107.10585"/>
        <updated>2021-07-23T02:00:31.570Z</updated>
        <summary type="html"><![CDATA[MobileCharger is a novel mobile charging robot with an Inverted Delta
actuator for safe and robust energy transfer between two mobile robots. The
RGB-D camera-based computer vision system allows to detect the electrodes on
the target mobile robot using a convolutional neural network (CNN). The
embedded high-fidelity tactile sensors are applied to estimate the misalignment
between the electrodes on the charger mechanism and the electrodes on the main
robot using CNN based on pressure data on the contact surfaces. Thus, the
developed vision-tactile perception system allows precise positioning of the
end effector of the actuator and ensures a reliable connection between the
electrodes of the two robots. The experimental results showed high average
precision (84.2%) for electrode detection using CNN. The percentage of
successful trials of the CNN-based electrode search algorithm reached 83% and
the average execution time accounted for 60 s. MobileCharger could introduce a
new level of charging systems and increase the prevalence of autonomous mobile
robots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okunevich_I/0/1/0/all/0/1"&gt;Iaroslav Okunevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trinitatova_D/0/1/0/all/0/1"&gt;Daria Trinitatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1"&gt;Pavel Kopanev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1"&gt;Dzmitry Tsetserukou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02177</id>
        <link href="http://arxiv.org/abs/2012.02177"/>
        <updated>2021-07-23T02:00:31.563Z</updated>
        <summary type="html"><![CDATA[We propose an online multi-view depth prediction approach on posed video
streams, where the scene geometry information computed in the previous time
steps is propagated to the current time step in an efficient and geometrically
plausible way. The backbone of our approach is a real-time capable, lightweight
encoder-decoder that relies on cost volumes computed from pairs of images. We
extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses
an arbitrary amount of past information in its states. The novelty lies in
propagating the hidden state of the cell by accounting for the viewpoint
changes between time steps. At a given time step, we warp the previous hidden
state into the current camera plane using the previous depth prediction. Our
extension brings only a small overhead of computation time and memory
consumption, while improving the depth predictions significantly. As a result,
we outperform the existing state-of-the-art multi-view stereo methods on most
of the evaluated metrics in hundreds of indoor scenes while maintaining a
real-time performance. Code available:
https://github.com/ardaduz/deep-video-mvs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1"&gt;Arda D&amp;#xfc;z&amp;#xe7;eker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1"&gt;Silvano Galliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1"&gt;Christoph Vogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1"&gt;Pablo Speciale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1"&gt;Mihai Dusmanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking AutoML Frameworks for Disease Prediction Using Medical Claims. (arXiv:2107.10495v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10495</id>
        <link href="http://arxiv.org/abs/2107.10495"/>
        <updated>2021-07-23T02:00:31.555Z</updated>
        <summary type="html"><![CDATA[We ascertain and compare the performances of AutoML tools on large, highly
imbalanced healthcare datasets.

We generated a large dataset using historical administrative claims including
demographic information and flags for disease codes in four different time
windows prior to 2019. We then trained three AutoML tools on this dataset to
predict six different disease outcomes in 2019 and evaluated model performances
on several metrics.

The AutoML tools showed improvement from the baseline random forest model but
did not differ significantly from each other. All models recorded low area
under the precision-recall curve and failed to predict true positives while
keeping the true negative rate high. Model performance was not directly related
to prevalence. We provide a specific use-case to illustrate how to select a
threshold that gives the best balance between true and false positive rates, as
this is an important consideration in medical applications.

Healthcare datasets present several challenges for AutoML tools, including
large sample size, high imbalance, and limitations in the available features
types. Improvements in scalability, combinations of imbalance-learning
resampling and ensemble approaches, and curated feature selection are possible
next steps to achieve better performance.

Among the three explored, no AutoML tool consistently outperforms the rest in
terms of predictive performance. The performances of the models in this study
suggest that there may be room for improvement in handling medical claims data.
Finally, selection of the optimal prediction threshold should be guided by the
specific practical application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romero_R/0/1/0/all/0/1"&gt;Roland Albert A. Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deypalan_M/0/1/0/all/0/1"&gt;Mariefel Nicole Y. Deypalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrotra_S/0/1/0/all/0/1"&gt;Suchit Mehrotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jungao_J/0/1/0/all/0/1"&gt;John Titus Jungao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheils_N/0/1/0/all/0/1"&gt;Natalie E. Sheils&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manduchi_E/0/1/0/all/0/1"&gt;Elisabetta Manduchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1"&gt;Jason H. Moore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03592</id>
        <link href="http://arxiv.org/abs/2008.03592"/>
        <updated>2021-07-23T02:00:31.547Z</updated>
        <summary type="html"><![CDATA[Visual emotion expression plays an important role in audiovisual speech
communication. In this work, we propose a novel approach to rendering visual
emotion expression in speech-driven talking face generation. Specifically, we
design an end-to-end talking face generation system that takes a speech
utterance, a single face image, and a categorical emotion label as input to
render a talking face video synchronized with the speech and expressing the
conditioned emotion. Objective evaluation on image quality, audiovisual
synchronization, and visual emotion expression shows that the proposed system
outperforms a state-of-the-art baseline system. Subjective evaluation of visual
emotion expression and video realness also demonstrates the superiority of the
proposed system. Furthermore, we conduct a human emotion recognition pilot
study using generated videos with mismatched emotions among the audio and
visual modalities. Results show that humans respond to the visual modality more
significantly than the audio modality on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1"&gt;Sefik Emre Eskimez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;You Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Zhiyao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Imbalanced Time-series Forecasting. (arXiv:2107.10709v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10709</id>
        <link href="http://arxiv.org/abs/2107.10709"/>
        <updated>2021-07-23T02:00:31.541Z</updated>
        <summary type="html"><![CDATA[Time-series forecasting plays an important role in many domains. Boosted by
the advances in Deep Learning algorithms, it has for instance been used to
predict wind power for eolic energy production, stock market fluctuations, or
motor overheating. In some of these tasks, we are interested in predicting
accurately some particular moments which often are underrepresented in the
dataset, resulting in a problem known as imbalanced regression. In the
literature, while recognized as a challenging problem, limited attention has
been devoted on how to handle the problem in a practical setting. In this
paper, we put forward a general approach to analyze time-series forecasting
problems focusing on those underrepresented moments to reduce imbalances. Our
approach has been developed based on a case study in a large industrial
company, which we use to exemplify the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silvestrin_L/0/1/0/all/0/1"&gt;Luis P. Silvestrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantiskas_L/0/1/0/all/0/1"&gt;Leonardos Pantiskas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1"&gt;Mark Hoogendoorn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeltaCharger: Charging Robot with Inverted Delta Mechanism and CNN-driven High Fidelity Tactile Perception for Precise 3D Positioning. (arXiv:2107.10710v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.10710</id>
        <link href="http://arxiv.org/abs/2107.10710"/>
        <updated>2021-07-23T02:00:31.521Z</updated>
        <summary type="html"><![CDATA[DeltaCharger is a novel charging robot with an Inverted Delta structure for
3D positioning of electrodes to achieve robust and safe transferring energy
between two mobile robots. The embedded high-fidelity tactile sensors allow to
estimate the angular, vertical and horizontal misalignments between electrodes
on the charger mechanism and electrodes on the target robot using pressure data
on the contact surfaces. This is crucial for preventing a short circuit. In
this paper, the mechanism of the developed prototype and evaluation study of
different machine learning models for misalignment prediction are presented.
The experimental results showed that the proposed system can measure the angle,
vertical and horizontal values of misalignment from pressure data with an
accuracy of 95.46%, 98.2%, and 86.9%, respectively, using a Convolutional
Neural Network (CNN). DeltaCharger can potentially bring a new level of
charging systems and improve the prevalence of mobile autonomous robots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okunevich_I/0/1/0/all/0/1"&gt;Iaroslav Okunevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trinitatova_D/0/1/0/all/0/1"&gt;Daria Trinitatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1"&gt;Pavel Kopanev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1"&gt;Dzmitry Tsetserukou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Fixed-Structure Gaussian Bayesian Networks. (arXiv:2107.10450v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.10450</id>
        <link href="http://arxiv.org/abs/2107.10450"/>
        <updated>2021-07-23T02:00:31.514Z</updated>
        <summary type="html"><![CDATA[Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation
models) are widely used to model causal interactions among continuous
variables. In this work, we study the problem of learning a fixed-structure
Gaussian Bayesian network up to a bounded error in total variation distance. We
analyze the commonly used node-wise least squares regression (LeastSquares) and
prove that it has a near-optimal sample complexity. We also study a couple of
new algorithms for the problem:

- BatchAvgLeastSquares takes the average of several batches of least squares
solutions at each node, so that one can interpolate between the batch size and
the number of batches. We show that BatchAvgLeastSquares also has near-optimal
sample complexity.

- CauchyEst takes the median of solutions to several batches of linear
systems at each node. We show that the algorithm specialized to polytrees,
CauchyEstTree, has near-optimal sample complexity.

Experimentally, we show that for uncontaminated, realizable data, the
LeastSquares algorithm performs best, but in the presence of contamination or
DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares
respectively perform better.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1"&gt;Davin Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gajjala_R/0/1/0/all/0/1"&gt;Rishikesh Gajjala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1"&gt;Sutanu Gayen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semiparametric Latent Topic Modeling on Consumer-Generated Corpora. (arXiv:2107.10651v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10651</id>
        <link href="http://arxiv.org/abs/2107.10651"/>
        <updated>2021-07-23T02:00:31.507Z</updated>
        <summary type="html"><![CDATA[Legacy procedures for topic modelling have generally suffered problems of
overfitting and a weakness towards reconstructing sparse topic structures. With
motivation from a consumer-generated corpora, this paper proposes
semiparametric topic model, a two-step approach utilizing nonnegative matrix
factorization and semiparametric regression in topic modeling. The model
enables the reconstruction of sparse topic structures in the corpus and
provides a generative model for predicting topics in new documents entering the
corpus. Assuming the presence of auxiliary information related to the topics,
this approach exhibits better performance in discovering underlying topic
structures in cases where the corpora are small and limited in vocabulary. In
an actual consumer feedback corpus, the model also demonstrably provides
interpretable and useful topic definitions comparable with those produced by
other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dayta_D/0/1/0/all/0/1"&gt;Dominic B. Dayta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barrios_E/0/1/0/all/0/1"&gt;Erniel B. Barrios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Quality of Finite Element Meshes with Machine Learning. (arXiv:2107.10507v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10507</id>
        <link href="http://arxiv.org/abs/2107.10507"/>
        <updated>2021-07-23T02:00:31.500Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of evaluating the quality of finite element
meshes for the purpose of structural mechanic simulations. It proposes the
application of a machine learning model trained on data collected from expert
evaluations. The task is characterised as a classification problem, where
quality of each individual element in a mesh is determined by its own
properties and adjacency structures. A domain-specific, yet simple
representation is proposed such that off-the-shelf machine learning methods can
be applied. Experimental data from industry practice demonstrates promising
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sprave_J/0/1/0/all/0/1"&gt;Joachim Sprave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drescher_C/0/1/0/all/0/1"&gt;Christian Drescher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Species Distribution Modeling for Machine Learning Practitioners: A Review. (arXiv:2107.10400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10400</id>
        <link href="http://arxiv.org/abs/2107.10400"/>
        <updated>2021-07-23T02:00:31.494Z</updated>
        <summary type="html"><![CDATA[Conservation science depends on an accurate understanding of what's happening
in a given ecosystem. How many species live there? What is the makeup of the
population? How is that changing over time? Species Distribution Modeling (SDM)
seeks to predict the spatial (and sometimes temporal) patterns of species
occurrence, i.e. where a species is likely to be found. The last few years have
seen a surge of interest in applying powerful machine learning tools to
challenging problems in ecology. Despite its considerable importance, SDM has
received relatively little attention from the computer science community. Our
goal in this work is to provide computer scientists with the necessary
background to read the SDM literature and develop ecologically useful ML-based
SDM algorithms. In particular, we introduce key SDM concepts and terminology,
review standard models, discuss data availability, and highlight technical
challenges and pitfalls.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1"&gt;Elijah Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_J/0/1/0/all/0/1"&gt;Joseph Parker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1"&gt;Pietro Perona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winner_K/0/1/0/all/0/1"&gt;Kevin Winner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Detection of Adversarial Examples with Model Explanations. (arXiv:2107.10480v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10480</id>
        <link href="http://arxiv.org/abs/2107.10480"/>
        <updated>2021-07-23T02:00:31.476Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have shown remarkable performance in a diverse
range of machine learning applications. However, it is widely known that DNNs
are vulnerable to simple adversarial perturbations, which causes the model to
incorrectly classify inputs. In this paper, we propose a simple yet effective
method to detect adversarial examples, using methods developed to explain the
model's behavior. Our key observation is that adding small, humanly
imperceptible perturbations can lead to drastic changes in the model
explanations, resulting in unusual or irregular forms of explanations. From
this insight, we propose an unsupervised detection of adversarial examples
using reconstructor networks trained only on model explanations of benign
examples. Our evaluations with MNIST handwritten dataset show that our method
is capable of detecting adversarial examples generated by the state-of-the-art
algorithms with high confidence. To the best of our knowledge, this work is the
first in suggesting unsupervised defense method using model explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1"&gt;Gihyuk Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1"&gt;Gyumin Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter and Intra-Annual Spatio-Temporal Variability of Habitat Suitability for Asian Elephants in India: A Random Forest Model-based Analysis. (arXiv:2107.10478v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10478</id>
        <link href="http://arxiv.org/abs/2107.10478"/>
        <updated>2021-07-23T02:00:31.469Z</updated>
        <summary type="html"><![CDATA[We develop a Random Forest model to estimate the species distribution of
Asian elephants in India and study the inter and intra-annual spatiotemporal
variability of habitats suitable for them. Climatic, topographic variables and
satellite-derived Land Use/Land Cover (LULC), Net Primary Productivity (NPP),
Leaf Area Index (LAI), and Normalized Difference Vegetation Index (NDVI) are
used as predictors, and the species sighting data of Asian elephants from
Global Biodiversity Information Reserve is used to develop the Random Forest
model. A careful hyper-parameter tuning and training-validation-testing cycle
are completed to identify the significant predictors and develop a final model
that gives precision and recall of 0.78 and 0.77. The model is applied to
estimate the spatial and temporal variability of suitable habitats. We observe
that seasonal reduction in the suitable habitat may explain the migration
patterns of Asian elephants and the increasing human-elephant conflict.
Further, the total available suitable habitat area is observed to have reduced,
which exacerbates the problem. This machine learning model is intended to serve
as an input to the Agent-Based Model that we are building as part of our
Artificial Intelligence-driven decision support tool to reduce human-wildlife
conflict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anjali_P/0/1/0/all/0/1"&gt;P. Anjali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramani_D/0/1/0/all/0/1"&gt;Deepak N. Subramani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Proactive Management Scheme for Data Synopses at the Edge. (arXiv:2107.10558v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10558</id>
        <link href="http://arxiv.org/abs/2107.10558"/>
        <updated>2021-07-23T02:00:31.460Z</updated>
        <summary type="html"><![CDATA[The combination of the infrastructure provided by the Internet of Things
(IoT) with numerous processing nodes present at the Edge Computing (EC)
ecosystem opens up new pathways to support intelligent applications. Such
applications can be provided upon humongous volumes of data collected by IoT
devices being transferred to the edge nodes through the network. Various
processing activities can be performed on the discussed data and multiple
collaborative opportunities between EC nodes can facilitate the execution of
the desired tasks. In order to support an effective interaction between edge
nodes, the knowledge about the geographically distributed data should be
shared. Obviously, the migration of large amounts of data will harm the
stability of the network stability and its performance. In this paper, we
recommend the exchange of data synopses than real data between EC nodes to
provide them with the necessary knowledge about peer nodes owning similar data.
This knowledge can be valuable when considering decisions such as data/service
migration and tasks offloading. We describe an continuous reasoning model that
builds a temporal similarity map of the available datasets to get nodes
understanding the evolution of data in their peers. We support the proposed
decision making mechanism through an intelligent similarity extraction scheme
based on an unsupervised machine learning model, and, at the same time, combine
it with a statistical measure that represents the trend of the so-called
discrepancy quantum. Our model can reveal the differences in the exchanged
synopses and provide a datasets similarity map which becomes the appropriate
knowledge base to support the desired processing activities. We present the
problem under consideration and suggest a solution for that, while, at the same
time, we reveal its advantages and disadvantages through a large number of
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolomvatsos_K/0/1/0/all/0/1"&gt;Kostas Kolomvatsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1"&gt;Christos Anagnostopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash-Based Tree Similarity and Simplification in Genetic Programming for Symbolic Regression. (arXiv:2107.10640v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10640</id>
        <link href="http://arxiv.org/abs/2107.10640"/>
        <updated>2021-07-23T02:00:31.453Z</updated>
        <summary type="html"><![CDATA[We introduce in this paper a runtime-efficient tree hashing algorithm for the
identification of isomorphic subtrees, with two important applications in
genetic programming for symbolic regression: fast, online calculation of
population diversity and algebraic simplification of symbolic expression trees.
Based on this hashing approach, we propose a simple diversity-preservation
mechanism with promising results on a collection of symbolic regression
benchmark problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burlacu_B/0/1/0/all/0/1"&gt;Bogdan Burlacu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kammerer_L/0/1/0/all/0/1"&gt;Lukas Kammerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1"&gt;Michael Affenzeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying machine learning-induced overdiagnosis in sepsis. (arXiv:2107.10399v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10399</id>
        <link href="http://arxiv.org/abs/2107.10399"/>
        <updated>2021-07-23T02:00:31.430Z</updated>
        <summary type="html"><![CDATA[The proliferation of early diagnostic technologies, including self-monitoring
systems and wearables, coupled with the application of these technologies on
large segments of healthy populations may significantly aggravate the problem
of overdiagnosis. This can lead to unwanted consequences such as overloading
health care systems and overtreatment, with potential harms to healthy
individuals. The advent of machine-learning tools to assist diagnosis -- while
promising rapid and more personalised patient management and screening -- might
contribute to this issue. The identification of overdiagnosis is usually post
hoc and demonstrated after long periods (from years to decades) and costly
randomised control trials. In this paper, we present an innovative approach
that allows us to preemptively detect potential cases of overdiagnosis during
predictive model development. This approach is based on the combination of
labels obtained from a prediction model and clustered medical trajectories,
using sepsis in adults as a test case. This is one of the first attempts to
quantify machine-learning induced overdiagnosis and we believe will serves as a
platform for further development, leading to guidelines for safe deployment of
computational diagnostic tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fedyukova_A/0/1/0/all/0/1"&gt;Anna Fedyukova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pires_D/0/1/0/all/0/1"&gt;Douglas Pires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1"&gt;Daniel Capurro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines. (arXiv:2107.10650v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10650</id>
        <link href="http://arxiv.org/abs/2107.10650"/>
        <updated>2021-07-23T02:00:31.424Z</updated>
        <summary type="html"><![CDATA[Prediction of medical codes from clinical notes is both a practical and
essential need for every healthcare delivery organization within current
medical systems. Automating annotation will save significant time and excessive
effort spent by human coders today. However, the biggest challenge is directly
identifying appropriate medical codes out of several thousands of
high-dimensional codes from unstructured free-text clinical notes. In the past
three years, with Convolutional Neural Networks (CNN) and Long Short-Term
Memory (LTSM) networks, there have been vast improvements in tackling the most
challenging benchmark of the MIMIC-III-full-label inpatient clinical notes
dataset. This progress raises the fundamental question of how far automated
machine learning (ML) systems are from human coders' working performance. We
assessed the baseline of human coders' performance on the same subsampled
testing set. We also present our Read, Attend, and Code (RAC) model for
learning the medical code assignment mappings. By connecting convolved
embeddings with self-attention and code-title guided attention modules,
combined with sentence permutation-based data augmentations and stochastic
weight averaging training, RAC establishes a new state of the art (SOTA),
considerably outperforming the current best Macro-F1 by 18.7%, and reaches past
the human-level coding baseline. This new milestone marks a meaningful step
toward fully autonomous medical coding (AMC) in machines reaching parity with
human coders' performance in medical code prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byung-Hak Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1"&gt;Varun Ganapathi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tri-Branch Convolutional Neural Networks for Top-$k$ Focused Academic Performance Prediction. (arXiv:2107.10424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10424</id>
        <link href="http://arxiv.org/abs/2107.10424"/>
        <updated>2021-07-23T02:00:31.406Z</updated>
        <summary type="html"><![CDATA[Academic performance prediction aims to leverage student-related information
to predict their future academic outcomes, which is beneficial to numerous
educational applications, such as personalized teaching and academic early
warning. In this paper, we address the problem by analyzing students' daily
behavior trajectories, which can be comprehensively tracked with campus
smartcard records. Different from previous studies, we propose a novel
Tri-Branch CNN architecture, which is equipped with row-wise, column-wise, and
depth-wise convolution and attention operations, to capture the characteristics
of persistence, regularity, and temporal distribution of student behavior in an
end-to-end manner, respectively. Also, we cast academic performance prediction
as a top-$k$ ranking problem, and introduce a top-$k$ focused loss to ensure
the accuracy of identifying academically at-risk students. Extensive
experiments were carried out on a large-scale real-world dataset, and we show
that our approach substantially outperforms recently proposed methods for
academic performance prediction. For the sake of reproducibility, our codes
have been released at
https://github.com/ZongJ1111/Academic-Performance-Prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Chaoran Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_J/0/1/0/all/0/1"&gt;Jian Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuling Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinhua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Lei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Meng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yilong Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design of a Graphical User Interface for Few-Shot Machine Learning Classification of Electron Microscopy Data. (arXiv:2107.10387v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.10387</id>
        <link href="http://arxiv.org/abs/2107.10387"/>
        <updated>2021-07-23T02:00:31.340Z</updated>
        <summary type="html"><![CDATA[The recent growth in data volumes produced by modern electron microscopes
requires rapid, scalable, and flexible approaches to image segmentation and
analysis. Few-shot machine learning, which can richly classify images from a
handful of user-provided examples, is a promising route to high-throughput
analysis. However, current command-line implementations of such approaches can
be slow and unintuitive to use, lacking the real-time feedback necessary to
perform effective classification. Here we report on the development of a
Python-based graphical user interface that enables end users to easily conduct
and visualize the output of few-shot learning models. This interface is
lightweight and can be hosted locally or on the web, providing the opportunity
to reproducibly conduct, share, and crowd-source few-shot analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Doty_C/0/1/0/all/0/1"&gt;Christina Doty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Gallagher_S/0/1/0/all/0/1"&gt;Shaun Gallagher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Cui_W/0/1/0/all/0/1"&gt;Wenqi Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Bhushan_S/0/1/0/all/0/1"&gt;Shweta Bhushan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Oostrom_M/0/1/0/all/0/1"&gt;Marjolein Oostrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Akers_S/0/1/0/all/0/1"&gt;Sarah Akers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Spurgeon_S/0/1/0/all/0/1"&gt;Steven R. Spurgeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Aware Scheduled Sampling for Neural Machine Translation. (arXiv:2107.10427v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10427</id>
        <link href="http://arxiv.org/abs/2107.10427"/>
        <updated>2021-07-23T02:00:31.319Z</updated>
        <summary type="html"><![CDATA[Scheduled sampling is an effective method to alleviate the exposure bias
problem of neural machine translation. It simulates the inference scene by
randomly replacing ground-truth target input tokens with predicted ones during
training. Despite its success, its critical schedule strategies are merely
based on training steps, ignoring the real-time model competence, which limits
its potential performance and convergence speed. To address this issue, we
propose confidence-aware scheduled sampling. Specifically, we quantify
real-time model competence by the confidence of model predictions, based on
which we design fine-grained schedule strategies. In this way, the model is
exactly exposed to predicted tokens for high-confidence positions and still
ground-truth tokens for low-confidence positions. Moreover, we observe vanilla
scheduled sampling suffers from degenerating into the original teacher forcing
mode since most predicted tokens are the same as ground-truth tokens.
Therefore, under the above confidence-aware strategy, we further expose more
noisy tokens (e.g., wordy and incorrect word order) instead of predicted ones
for high-confidence token positions. We evaluate our approach on the
Transformer and conduct experiments on large-scale WMT 2014 English-German, WMT
2014 English-French, and WMT 2019 Chinese-English. Results show that our
approach significantly outperforms the Transformer and vanilla scheduled
sampling on both translation quality and convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yijin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yufeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03438</id>
        <link href="http://arxiv.org/abs/2107.03438"/>
        <updated>2021-07-23T02:00:31.304Z</updated>
        <summary type="html"><![CDATA[To realize robots that can understand human instructions and perform
meaningful tasks in the near future, it is important to develop learned models
that can understand referential language to identify common objects in
real-world 3D scenes. In this paper, we develop a spatial-language model for a
3D visual grounding problem. Specifically, given a reconstructed 3D scene in
the form of a point cloud with 3D bounding boxes of potential object
candidates, and a language utterance referring to a target object in the scene,
our model identifies the target object from a set of potential candidates. Our
spatial-language model uses a transformer-based architecture that combines
spatial embedding from bounding-box with a finetuned language embedding from
DistilBert and reasons among the objects in the 3D scene to find the target
object. We show that our model performs competitively on visio-linguistic
datasets proposed by ReferIt3D. We provide additional analysis of performance
in spatial reasoning tasks decoupled from perception noise, the effect of
view-dependent utterances in terms of accuracy, and view-point annotations for
potential robotics applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1"&gt;Junha Roh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1"&gt;Karthik Desingh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lumen: A Machine Learning Framework to Expose Influence Cues in Text. (arXiv:2107.10655v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10655</id>
        <link href="http://arxiv.org/abs/2107.10655"/>
        <updated>2021-07-23T02:00:31.298Z</updated>
        <summary type="html"><![CDATA[Phishing and disinformation are popular social engineering attacks with
attackers invariably applying influence cues in texts to make them more
appealing to users. We introduce Lumen, a learning-based framework that exposes
influence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)
objectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was
trained with a newly developed dataset of 3K texts comprised of disinformation,
phishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in
comparison to other learning models showed that Lumen and LSTM presented the
best F1-micro score, but Lumen yielded better interpretability. Our results
highlight the promise of ML to expose influence cues in text, towards the goal
of application in automatic labeling tools to improve the accuracy of
human-based detection and reduce the likelihood of users falling for deceptive
online content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hanyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mirela Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capecci_D/0/1/0/all/0/1"&gt;Daniel Capecci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1"&gt;Luiz Giovanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czech_L/0/1/0/all/0/1"&gt;Lauren Czech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1"&gt;Juliana Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Daniela Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Trajectory Forecasting Evaluation. (arXiv:2107.10297v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.10297</id>
        <link href="http://arxiv.org/abs/2107.10297"/>
        <updated>2021-07-23T02:00:31.292Z</updated>
        <summary type="html"><![CDATA[Forecasting the behavior of other agents is an integral part of the modern
robotic autonomy stack, especially in safety-critical scenarios with
human-robot interaction, such as autonomous driving. In turn, there has been a
significant amount of interest and research in trajectory forecasting,
resulting in a wide variety of approaches. Common to all works, however, is the
use of the same few accuracy-based evaluation metrics, e.g., displacement error
and log-likelihood. While these metrics are informative, they are task-agnostic
and predictions that are evaluated as equal can lead to vastly different
outcomes, e.g., in downstream planning and decision making. In this work, we
take a step back and critically evaluate current trajectory forecasting
metrics, proposing task-aware metrics as a better measure of performance in
systems where prediction is being deployed. We additionally present one example
of such a metric, incorporating planning-awareness within existing trajectory
forecasting metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1"&gt;Boris Ivanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-07-23T02:00:31.285Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose mRASP2, a
training method to obtain a single unified multilingual translation model.
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to
close the gap among representations of different languages, and b) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mRASP2 outperforms
existing best unified model and achieves competitive or even better performance
than the pre-trained and fine-tuned model mBART on tens of WMT's translation
directions. For non-English directions, mRASP2 achieves an improvement of
average 10+ BLEU compared with the multilingual Transformer baseline. Code,
data and trained models are available at https://github.com/PANXiao1994/mRASP2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTa\'u: Ita\'u BERT for digital customer service. (arXiv:2101.12015v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12015</id>
        <link href="http://arxiv.org/abs/2101.12015"/>
        <updated>2021-07-23T02:00:31.278Z</updated>
        <summary type="html"><![CDATA[In the last few years, three major topics received increased interest: deep
learning, NLP and conversational agents. Bringing these three topics together
to create an amazing digital customer experience and indeed deploy in
production and solve real-world problems is something innovative and
disruptive. We introduce a new Portuguese financial domain language
representation model called BERTa\'u. BERTa\'u is an uncased BERT-base trained
from scratch with data from the Ita\'u virtual assistant chatbot solution. Our
novel contribution is that BERTa\'u pretrained language model requires less
data, reached state-of-the-art performance in three NLP tasks, and generates a
smaller and lighter model that makes the deployment feasible. We developed
three tasks to validate our model: information retrieval with Frequently Asked
Questions (FAQ) from Ita\'u bank, sentiment analysis from our virtual assistant
data, and a NER solution. All proposed tasks are real-world solutions in
production on our environment and the usage of a specialist model proved to be
effective when compared to Google BERT multilingual and the DPRQuestionEncoder
from Facebook, available at Hugging Face. The BERTa\'u improves the performance
in 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%
in NER F1 score and can also represent the same sequence in up to 66% fewer
tokens when compared to "shelf models".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finardi_P/0/1/0/all/0/1"&gt;Paulo Finardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viegas_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Di&amp;#xe9; Viegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_G/0/1/0/all/0/1"&gt;Gustavo T. Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansano_A/0/1/0/all/0/1"&gt;Alex F. Mansano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carida_V/0/1/0/all/0/1"&gt;Vinicius F. Carid&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. (arXiv:2107.10821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10821</id>
        <link href="http://arxiv.org/abs/2107.10821"/>
        <updated>2021-07-23T02:00:31.270Z</updated>
        <summary type="html"><![CDATA[Automatic metrics are commonly used as the exclusive tool for declaring the
superiority of one machine translation system's quality over another. The
community choice of automatic metric guides research directions and industrial
developments by deciding which models are deemed better. Evaluating metrics
correlations has been limited to a small collection of human judgements. In
this paper, we corroborate how reliable metrics are in contrast to human
judgements on - to the best of our knowledge - the largest collection of human
judgements. We investigate which metrics have the highest accuracy to make
system-level quality rankings for pairs of systems, taking human judgement as a
gold standard, which is the closest scenario to the real metric usage.
Furthermore, we evaluate the performance of various metrics across different
language pairs and domains. Lastly, we show that the sole use of BLEU
negatively affected the past development of improved models. We release the
collection of human judgements of 4380 systems, and 2.3 M annotated sentences
for further analysis and replication of our work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1"&gt;Tom Kocmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1"&gt;Christian Federmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grundkiewicz_R/0/1/0/all/0/1"&gt;Roman Grundkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1"&gt;Marcin Junczys-Dowmunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsushita_H/0/1/0/all/0/1"&gt;Hitokazu Matsushita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1"&gt;Arul Menezes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10300</id>
        <link href="http://arxiv.org/abs/2107.10300"/>
        <updated>2021-07-23T02:00:31.263Z</updated>
        <summary type="html"><![CDATA[Causality knowledge is vital to building robust AI systems. Deep learning
models often perform poorly on tasks that require causal reasoning, which is
often derived using some form of commonsense knowledge not immediately
available in the input but implicitly inferred by humans. Prior work has
unraveled spurious observational biases that models fall prey to in the absence
of causality. While language representation models preserve contextual
knowledge within learned embeddings, they do not factor in causal relationships
during training. By blending causal relationships with the input features to an
existing model that performs visual cognition tasks (such as scene
understanding, video captioning, video question-answering, etc.), better
performance can be achieved owing to the insight causal relationships bring
about. Recently, several models have been proposed that have tackled the task
of mining causal data from either the visual or textual modality. However,
there does not exist widespread research that mines causal relationships by
juxtaposing the visual and language modalities. While images offer a rich and
easy-to-process resource for us to mine causality knowledge from, videos are
denser and consist of naturally time-ordered events. Also, textual information
offers details that could be implicit in videos. We propose iReason, a
framework that infers visual-semantic commonsense knowledge using both videos
and natural language captions. Furthermore, iReason's architecture integrates a
causal rationalization module to aid the process of interpretability, error
analysis and bias detection. We demonstrate the effectiveness of iReason using
a two-pronged comparative analysis with language representation learning models
(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1"&gt;Aman Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1"&gt;Vinija Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble-based Uncertainty Quantification: Bayesian versus Credal Inference. (arXiv:2107.10384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10384</id>
        <link href="http://arxiv.org/abs/2107.10384"/>
        <updated>2021-07-23T02:00:31.256Z</updated>
        <summary type="html"><![CDATA[The idea to distinguish and quantify two important types of uncertainty,
often referred to as aleatoric and epistemic, has received increasing attention
in machine learning research in the last couple of years. In this paper, we
consider ensemble-based approaches to uncertainty quantification.
Distinguishing between different types of uncertainty-aware learning
algorithms, we specifically focus on Bayesian methods and approaches based on
so-called credal sets, which naturally suggest themselves from an ensemble
learning point of view. For both approaches, we address the question of how to
quantify aleatoric and epistemic uncertainty. The effectiveness of
corresponding measures is evaluated and compared in an empirical study on
classification with a reject option.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaker_M/0/1/0/all/0/1"&gt;Mohammad Hossein Shaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small-text: Active Learning for Text Classification in Python. (arXiv:2107.10314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10314</id>
        <link href="http://arxiv.org/abs/2107.10314"/>
        <updated>2021-07-23T02:00:31.233Z</updated>
        <summary type="html"><![CDATA[We present small-text, a simple modular active learning library, which offers
pool-based active learning for text classification in Python. It comes with
various pre-implemented state-of-the-art query strategies, including some which
can leverage the GPU. Clearly defined interfaces allow to combine a multitude
of such query strategies with different classifiers, thereby facilitating a
quick mix and match, and enabling a rapid development of both active learning
experiments and applications. To make various classifiers accessible in a
consistent way, it integrates several well-known machine learning libraries,
namely, scikit-learn, PyTorch, and huggingface transformers -- for which the
latter integrations are available as optionally installable extensions. The
library is available under the MIT License at
https://github.com/webis-de/small-text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1"&gt;Christopher Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1"&gt;Lydia M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1"&gt;Andreas Niekler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semiparametric Latent Topic Modeling on Consumer-Generated Corpora. (arXiv:2107.10651v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10651</id>
        <link href="http://arxiv.org/abs/2107.10651"/>
        <updated>2021-07-23T02:00:31.195Z</updated>
        <summary type="html"><![CDATA[Legacy procedures for topic modelling have generally suffered problems of
overfitting and a weakness towards reconstructing sparse topic structures. With
motivation from a consumer-generated corpora, this paper proposes
semiparametric topic model, a two-step approach utilizing nonnegative matrix
factorization and semiparametric regression in topic modeling. The model
enables the reconstruction of sparse topic structures in the corpus and
provides a generative model for predicting topics in new documents entering the
corpus. Assuming the presence of auxiliary information related to the topics,
this approach exhibits better performance in discovering underlying topic
structures in cases where the corpora are small and limited in vocabulary. In
an actual consumer feedback corpus, the model also demonstrably provides
interpretable and useful topic definitions comparable with those produced by
other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dayta_D/0/1/0/all/0/1"&gt;Dominic B. Dayta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barrios_E/0/1/0/all/0/1"&gt;Erniel B. Barrios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework. (arXiv:2107.10637v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.10637</id>
        <link href="http://arxiv.org/abs/2107.10637"/>
        <updated>2021-07-23T02:00:31.186Z</updated>
        <summary type="html"><![CDATA[Mobile devices are transforming the way people interact with computers, and
speech interfaces to applications are ever more important. Automatic Speech
Recognition systems recently published are very accurate, but often require
powerful machinery (specialised Graphical Processing Units) for inference,
which makes them impractical to run on commodity devices, especially in
streaming mode. Impressed by the accuracy of, but dissatisfied with the
inference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)
when not using a GPU, we trained a new baseline acoustic model (on the same
dataset as the aforementioned paper) and three language models for use with the
Coqui STT framework. Results look promising, but further epochs of training and
parameter sweeping or, alternatively, limiting the vocabulary that the ASR
system must support, is needed to reach a production-level accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Salimzianov_I/0/1/0/all/0/1"&gt;Ilnar Salimzianov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02705</id>
        <link href="http://arxiv.org/abs/2012.02705"/>
        <updated>2021-07-23T02:00:31.171Z</updated>
        <summary type="html"><![CDATA[Humans use spatial language to naturally describe object locations and their
relations. Interpreting spatial language not only adds a perceptual modality
for robots, but also reduces the barrier of interfacing with humans. Previous
work primarily considers spatial language as goal specification for instruction
following tasks in fully observable domains, often paired with reference paths
for reward-based learning. However, spatial language is inherently subjective
and potentially ambiguous or misleading. Hence, in this paper, we consider
spatial language as a form of stochastic observation. We propose SLOOP (Spatial
Language Object-Oriented POMDP), a new framework for partially observable
decision making with a probabilistic observation model for spatial language. We
apply SLOOP to object search in city-scale environments. To interpret
ambiguous, context-dependent prepositions (e.g. front), we design a simple
convolutional neural network that predicts the language provider's latent frame
of reference (FoR) given the environment context. Search strategies are
computed via an online POMDP planner based on Monte Carlo Tree Search.
Evaluation based on crowdsourced language data, collected over areas of five
cities in OpenStreetMap, shows that our approach achieves faster search and
higher success rate compared to baselines, with a wider margin as the spatial
language becomes more complex. Finally, we demonstrate the proposed method in
AirSim, a realistic simulator where a drone is tasked to find cars in a
neighborhood environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kaiyu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1"&gt;Deniz Bayazit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1"&gt;Rebecca Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1"&gt;Ellie Pavlick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1"&gt;Stefanie Tellex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sparsity Algorithm with Applications to Corporate Credit Rating. (arXiv:2107.10306v1 [q-fin.RM])]]></title>
        <id>http://arxiv.org/abs/2107.10306</id>
        <link href="http://arxiv.org/abs/2107.10306"/>
        <updated>2021-07-23T02:00:31.151Z</updated>
        <summary type="html"><![CDATA[In Artificial Intelligence, interpreting the results of a Machine Learning
technique often termed as a black box is a difficult task. A counterfactual
explanation of a particular "black box" attempts to find the smallest change to
the input values that modifies the prediction to a particular output, other
than the original one. In this work we formulate the problem of finding a
counterfactual explanation as an optimization problem. We propose a new
"sparsity algorithm" which solves the optimization problem, while also
maximizing the sparsity of the counterfactual explanation. We apply the
sparsity algorithm to provide a simple suggestion to publicly traded companies
in order to improve their credit ratings. We validate the sparsity algorithm
with a synthetically generated dataset and we further apply it to quarterly
financial statements from companies in financial, healthcare and IT sectors of
the US market. We provide evidence that the counterfactual explanation can
capture the nature of the real statement features that changed between the
current quarter and the following quarter when ratings improved. The empirical
results show that the higher the rating of a company the greater the "effort"
required to further improve credit rating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Florescu_I/0/1/0/all/0/1"&gt;Ionut Florescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impacts Towards a comprehensive assessment of the book impact by integrating multiple evaluation sources. (arXiv:2107.10434v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.10434</id>
        <link href="http://arxiv.org/abs/2107.10434"/>
        <updated>2021-07-23T02:00:31.135Z</updated>
        <summary type="html"><![CDATA[The surge in the number of books published makes the manual evaluation
methods difficult to efficiently evaluate books. The use of books' citations
and alternative evaluation metrics can assist manual evaluation and reduce the
cost of evaluation. However, most existing evaluation research was based on a
single evaluation source with coarse-grained analysis, which may obtain
incomprehensive or one-sided evaluation results of book impact. Meanwhile,
relying on a single resource for book assessment may lead to the risk that the
evaluation results cannot be obtained due to the lack of the evaluation data,
especially for newly published books. Hence, this paper measured book impact
based on an evaluation system constructed by integrating multiple evaluation
sources. Specifically, we conducted finer-grained mining on the multiple
evaluation sources, including books' internal evaluation resources and external
evaluation resources. Various technologies (e.g. topic extraction, sentiment
analysis, text classification) were used to extract corresponding evaluation
metrics from the internal and external evaluation resources. Then, Expert
evaluation combined with analytic hierarchy process was used to integrate the
evaluation metrics and construct a book impact evaluation system. Finally, the
reliability of the evaluation system was verified by comparing with the results
of expert evaluation, detailed and diversified evaluation results were then
obtained. The experimental results reveal that differential evaluation
resources can measure the books' impacts from different dimensions, and the
integration of multiple evaluation data can assess books more comprehensively.
Meanwhile, the book impact evaluation system can provide personalized
evaluation results according to the users' evaluation purposes. In addition,
the disciplinary differences should be considered for assessing books' impacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingqing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengzhi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BinaryBERT: Pushing the Limit of BERT Quantization. (arXiv:2012.15701v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15701</id>
        <link href="http://arxiv.org/abs/2012.15701"/>
        <updated>2021-07-23T02:00:31.116Z</updated>
        <summary type="html"><![CDATA[The rapid development of large pre-trained language models has greatly
increased the demand for model compression techniques, among which quantization
is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT
quantization to the limit by weight binarization. We find that a binary BERT is
hard to be trained directly than a ternary counterpart due to its complex and
irregular loss landscape. Therefore, we propose ternary weight splitting, which
initializes BinaryBERT by equivalently splitting from a half-sized ternary
network. The binary model thus inherits the good performance of the ternary
one, and can be further enhanced by fine-tuning the new architecture after
splitting. Empirical results show that our BinaryBERT has only a slight
performance drop compared with the full-precision model while being 24x
smaller, achieving the state-of-the-art compression results on the GLUE and
SQuAD benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Lu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Lifeng Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion recognition by fusing time synchronous and time asynchronous representations. (arXiv:2010.14102v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14102</id>
        <link href="http://arxiv.org/abs/2010.14102"/>
        <updated>2021-07-23T02:00:31.107Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel two-branch neural network model structure is proposed
for multimodal emotion recognition, which consists of a time synchronous branch
(TSB) and a time asynchronous branch (TAB). To capture correlations between
each word and its acoustic realisation, the TSB combines speech and text
modalities at each input window frame and then does pooling across time to form
a single embedding vector. The TAB, by contrast, provides cross-utterance
information by integrating sentence text embeddings from a number of context
utterances into another embedding vector. The final emotion classification uses
both the TSB and the TAB embeddings. Experimental results on the IEMOCAP
dataset demonstrate that the two-branch structure achieves state-of-the-art
results in 4-way classification with all common test setups. When using
automatic speech recognition (ASR) output instead of manually transcribed
reference text, it is shown that the cross-utterance information considerably
improves the robustness against ASR errors. Furthermore, by incorporating an
extra class for all the other emotions, the final 5-way classification system
with ASR hypotheses can be viewed as a prototype for more realistic emotion
recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Search: Making Domain Experts out of Dilettantes. (arXiv:2105.02274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02274</id>
        <link href="http://arxiv.org/abs/2105.02274"/>
        <updated>2021-07-23T02:00:31.100Z</updated>
        <summary type="html"><![CDATA[When experiencing an information need, users want to engage with a domain
expert, but often turn to an information retrieval system, such as a search
engine, instead. Classical information retrieval systems do not answer
information needs directly, but instead provide references to (hopefully
authoritative) answers. Successful question answering systems offer a limited
corpus created on-demand by human experts, which is neither timely nor
scalable. Pre-trained language models, by contrast, are capable of directly
generating prose that may be responsive to an information need, but at present
they are dilettantes rather than domain experts -- they do not have a true
understanding of the world, they are prone to hallucinating, and crucially they
are incapable of justifying their utterances by referring to supporting
documents in the corpus they were trained over. This paper examines how ideas
from classical information retrieval and pre-trained language models can be
synthesized and evolved into systems that truly deliver on the promise of
domain expert advice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1"&gt;Marc Najork&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Theoretical foundations and limits of word embeddings: what types of meaning can they capture?. (arXiv:2107.10413v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10413</id>
        <link href="http://arxiv.org/abs/2107.10413"/>
        <updated>2021-07-23T02:00:31.092Z</updated>
        <summary type="html"><![CDATA[Measuring meaning is a central problem in cultural sociology and word
embeddings may offer powerful new tools to do so. But like any tool, they build
on and exert theoretical assumptions. In this paper I theorize the ways in
which word embeddings model three core premises of a structural linguistic
theory of meaning: that meaning is relational, coherent, and may be analyzed as
a static system. In certain ways, word embedding methods are vulnerable to the
same, enduring critiques of these premises. In other ways, they offer novel
solutions to these critiques. More broadly, formalizing the study of meaning
with word embeddings offers theoretical opportunities to clarify core concepts
and debates in cultural sociology, such as the coherence of meaning. Just as
network analysis specified the once vague notion of social relations (Borgatti
et al. 2009), formalizing meaning with embedding methods can push us to specify
and reimagine meaning itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1"&gt;Alina Arseniev-Koehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital Einstein Experience: Fast Text-to-Speech for Conversational AI. (arXiv:2107.10658v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.10658</id>
        <link href="http://arxiv.org/abs/2107.10658"/>
        <updated>2021-07-23T02:00:31.085Z</updated>
        <summary type="html"><![CDATA[We describe our approach to create and deliver a custom voice for a
conversational AI use-case. More specifically, we provide a voice for a Digital
Einstein character, to enable human-computer interaction within the digital
conversation experience. To create the voice which fits the context well, we
first design a voice character and we produce the recordings which correspond
to the desired speech attributes. We then model the voice. Our solution
utilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes
and Parallel WaveGAN to generate the waveforms. The system supports a character
input and gives a speech waveform at the output. We use a custom dictionary for
selected words to ensure their proper pronunciation. Our proposed cloud
architecture enables for fast voice delivery, making it possible to talk to the
digital version of Albert Einstein in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rownicka_J/0/1/0/all/0/1"&gt;Joanna Rownicka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sprenkamp_K/0/1/0/all/0/1"&gt;Kilian Sprenkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tripiana_A/0/1/0/all/0/1"&gt;Antonio Tripiana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gromoglasov_V/0/1/0/all/0/1"&gt;Volodymyr Gromoglasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kunz_T/0/1/0/all/0/1"&gt;Timo P Kunz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Schema-based Event Extraction: Literature Review and Current Trends. (arXiv:2107.02126v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02126</id>
        <link href="http://arxiv.org/abs/2107.02126"/>
        <updated>2021-07-23T02:00:31.064Z</updated>
        <summary type="html"><![CDATA[Schema-based event extraction is a critical technique to apprehend the
essential content of events promptly. With the rapid development of deep
learning technology, event extraction technology based on deep learning has
become a research hotspot. Numerous methods, datasets, and evaluation metrics
have been proposed in the literature, raising the need for a comprehensive and
updated survey. This paper fills the gap by reviewing the state-of-the-art
approaches, focusing on deep learning-based models. We summarize the task
definition, paradigm, and models of schema-based event extraction and then
discuss each of these in detail. We introduce benchmark datasets that support
tests of predictions and evaluation metrics. A comprehensive comparison between
different techniques is also provided in this survey. Finally, we conclude by
summarizing future research directions facing the research area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1"&gt;Yiming Hei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Rui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1"&gt;Jiawei Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation. (arXiv:2107.02368v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02368</id>
        <link href="http://arxiv.org/abs/2107.02368"/>
        <updated>2021-07-23T02:00:31.056Z</updated>
        <summary type="html"><![CDATA[We propose Uncertainty Augmented Context Attention network (UACANet) for
polyp segmentation which consider a uncertain area of the saliency map. We
construct a modified version of U-Net shape network with additional encoder and
decoder and compute a saliency map in each bottom-up stream prediction module
and propagate to the next prediction module. In each prediction module,
previously predicted saliency map is utilized to compute foreground, background
and uncertain area map and we aggregate the feature map with three area maps
for each representation. Then we compute the relation between each
representation and each pixel in the feature map. We conduct experiments on
five popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS,
CVC-ColonDB and CVC-300, and achieve state-of-the-art performance. Especially,
we achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared
to the previous state-of-the-art method. Source code is publicly available at
https://github.com/plemeri/UACANet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyemin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daijin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of In-Person Counseling Strategies To Develop Physical Activity Chatbot for Women. (arXiv:2107.10410v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10410</id>
        <link href="http://arxiv.org/abs/2107.10410"/>
        <updated>2021-07-23T02:00:31.048Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence chatbots are the vanguard in technology-based
intervention to change people's behavior. To develop intervention chatbots, the
first step is to understand natural language conversation strategies in human
conversation. This work introduces an intervention conversation dataset
collected from a real-world physical activity intervention program for women.
We designed comprehensive annotation schemes in four dimensions (domain,
strategy, social exchange, and task-focused exchange) and annotated a subset of
dialogs. We built a strategy classifier with context information to detect
strategies from both trainers and participants based on the annotation. To
understand how human intervention induces effective behavior changes, we
analyzed the relationships between the intervention strategies and the
participants' changes in the barrier and social support for physical activity.
We also analyzed how participant's baseline weight correlates to the amount of
occurrence of the corresponding strategy. This work lays the foundation for
developing a personalized physical activity intervention bot. The dataset and
code are available at
https://github.com/KaihuiLiang/physical-activity-counseling]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kai-Hui Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1"&gt;Patrick Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Yoo Jung Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukuoka_Y/0/1/0/all/0/1"&gt;Yoshimi Fukuoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05502</id>
        <link href="http://arxiv.org/abs/2105.05502"/>
        <updated>2021-07-23T02:00:31.041Z</updated>
        <summary type="html"><![CDATA[While a large body of work has scrutinized the meaning of conditional
sentences, considerably less attention has been paid to formal models of their
pragmatic use and interpretation. Here, we take a probabilistic approach to
pragmatic reasoning about indicative conditionals which flexibly integrates
gradient beliefs about richly structured world states. We model listeners'
update of their prior beliefs about the causal structure of the world and the
joint probabilities of the consequent and antecedent based on assumptions about
the speaker's utterance production protocol. We show that, when supplied with
natural contextual assumptions, our model uniformly explains a number of
inferences attested in the literature, including epistemic inferences,
Conditional Perfection and the dependency between antecedent and consequent of
a conditional. We argue that this approach also helps explain three puzzles
introduced by Douven (2012) about updating with conditionals: depending on the
utterance context, the listener's belief in the antecedent may increase,
decrease or remain unchanged.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grusdt_B/0/1/0/all/0/1"&gt;Britta Grusdt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lassiter_D/0/1/0/all/0/1"&gt;Daniel Lassiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1"&gt;Michael Franke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries. (arXiv:2107.10652v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10652</id>
        <link href="http://arxiv.org/abs/2107.10652"/>
        <updated>2021-07-23T02:00:31.034Z</updated>
        <summary type="html"><![CDATA[Codification of free-text clinical narratives have long been recognised to be
beneficial for secondary uses such as funding, insurance claim processing and
research. The current scenario of assigning codes is a manual process which is
very expensive, time-consuming and error prone. In recent years, many
researchers have studied the use of Natural Language Processing (NLP), related
Machine Learning (ML) and Deep Learning (DL) methods and techniques to resolve
the problem of manual coding of clinical narratives and to assist human coders
to assign clinical codes more accurately and efficiently. This systematic
literature review provides a comprehensive overview of automated clinical
coding systems that utilises appropriate NLP, ML and DL methods and techniques
to assign ICD codes to discharge summaries. We have followed the Preferred
Reporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and
conducted a comprehensive search of publications from January, 2010 to December
2020 in four academic databases- PubMed, ScienceDirect, Association for
Computing Machinery(ACM) Digital Library, and the Association for Computational
Linguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the
inclusion criteria. This review identified: datasets having discharge
summaries; NLP techniques along with some other data extraction processes,
different feature extraction and embedding techniques. To measure the
performance of classification methods, different evaluation metrics are used.
Lastly, future research directions are provided to scholars who are interested
in automated ICD code assignment. Efforts are still required to improve ICD
code prediction accuracy, availability of large-scale de-identified clinical
corpora with the latest version of the classification system. This can be a
platform to guide and share knowledge with the less experienced coders and
researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1"&gt;Rajvir Kaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginige_J/0/1/0/all/0/1"&gt;Jeewani Anupama Ginige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1"&gt;Oliver Obst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification. (arXiv:2107.10474v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10474</id>
        <link href="http://arxiv.org/abs/2107.10474"/>
        <updated>2021-07-23T02:00:31.015Z</updated>
        <summary type="html"><![CDATA[Language models (LMs) pretrained on a large text corpus and fine-tuned on a
downstream text corpus and fine-tuned on a downstream task becomes a de facto
training strategy for several natural language processing (NLP) tasks.
Recently, an adaptive pretraining method retraining the pretrained language
model with task-relevant data has shown significant performance improvements.
However, current adaptive pretraining methods suffer from underfitting on the
task distribution owing to a relatively small amount of data to re-pretrain the
LM. To completely use the concept of adaptive pretraining, we propose a
back-translated task-adaptive pretraining (BT-TAPT) method that increases the
amount of task-specific data for LM re-pretraining by augmenting the task data
using back-translation to generalize the LM to the target task domain. The
experimental results show that the proposed BT-TAPT yields improved
classification accuracy on both low- and high-resource data and better
robustness to noise than the conventional adaptive pretraining method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junghoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jounghee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1"&gt;Pilsung Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10326</id>
        <link href="http://arxiv.org/abs/2107.10326"/>
        <updated>2021-07-23T02:00:31.009Z</updated>
        <summary type="html"><![CDATA[Data is published on the web over time in great volumes, but majority of the
data is unstructured, making it hard to understand and difficult to interpret.
Information Extraction (IE) methods extract structured information from
unstructured data. One of the challenging IE tasks is Event Extraction (EE)
which seeks to derive information about specific incidents and their actors
from the text. EE is useful in many domains such as building a knowledge base,
information retrieval, summarization and online monitoring systems. In the past
decades, some event ontologies like ACE, CAMEO and ICEWS were developed to
define event forms, actors and dimensions of events observed in the text. These
event ontologies still have some shortcomings such as covering only a few
topics like political events, having inflexible structure in defining argument
roles, lack of analytical dimensions, and complexity in choosing event
sub-types. To address these concerns, we propose an event ontology, namely
COfEE, that incorporates both expert domain knowledge, previous ontologies and
a data-driven approach for identifying events from text. COfEE consists of two
hierarchy levels (event types and event sub-types) that include new categories
relating to environmental issues, cyberspace, criminal activity and natural
disasters which need to be monitored instantly. Also, dynamic roles according
to each event sub-type are defined to capture various dimensions of events. In
a follow-up experiment, the proposed ontology is evaluated on Wikipedia events,
and it is shown to be general and comprehensive. Moreover, in order to
facilitate the preparation of gold-standard data for event extraction, a
language-independent online tool is presented based on COfEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1"&gt;Ali Balali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1"&gt;Masoud Asadpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1"&gt;Seyed Hossein Jafari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Target-Oriented Fine-tuning for Zero-Resource Named Entity Recognition. (arXiv:2107.10523v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10523</id>
        <link href="http://arxiv.org/abs/2107.10523"/>
        <updated>2021-07-23T02:00:31.001Z</updated>
        <summary type="html"><![CDATA[Zero-resource named entity recognition (NER) severely suffers from data
scarcity in a specific domain or language. Most studies on zero-resource NER
transfer knowledge from various data by fine-tuning on different auxiliary
tasks. However, how to properly select training data and fine-tuning tasks is
still an open problem. In this paper, we tackle the problem by transferring
knowledge from three aspects, i.e., domain, language and task, and
strengthening connections among them. Specifically, we propose four practical
guidelines to guide knowledge transfer and task fine-tuning. Based on these
guidelines, we design a target-oriented fine-tuning (TOF) framework to exploit
various data from three aspects in a unified training manner. Experimental
results on six benchmarks show that our method yields consistent improvements
over baselines in both cross-domain and cross-lingual scenarios. Particularly,
we achieve new state-of-the-art performance on five benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yufeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for assessing quality of service in the hospitality sector based on customer reviews. (arXiv:2107.10328v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10328</id>
        <link href="http://arxiv.org/abs/2107.10328"/>
        <updated>2021-07-23T02:00:30.995Z</updated>
        <summary type="html"><![CDATA[The increasing use of online hospitality platforms provides firsthand
information about clients preferences, which are essential to improve hotel
services and increase the quality of service perception. Customer reviews can
be used to automatically extract the most relevant aspects of the quality of
service for hospitality clientele. This paper proposes a framework for the
assessment of the quality of service in the hospitality sector based on the
exploitation of customer reviews through natural language processing and
machine learning methods. The proposed framework automatically discovers the
quality of service aspects relevant to hotel customers. Hotel reviews from
Bogot\'a and Madrid are automatically scrapped from Booking.com. Semantic
information is inferred through Latent Dirichlet Allocation and FastText, which
allow representing text reviews as vectors. A dimensionality reduction
technique is applied to visualise and interpret large amounts of customer
reviews. Visualisations of the most important quality of service aspects are
generated, allowing to qualitatively and quantitatively assess the quality of
service. Results show that it is possible to automatically extract the main
quality of service aspects perceived by customers from large customer review
datasets. These findings could be used by hospitality managers to understand
clients better and to improve the quality of service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_Calderon_V/0/1/0/all/0/1"&gt;Vladimir Vargas-Calder&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_A/0/1/0/all/0/1"&gt;Andreina Moros Ochoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nieto_G/0/1/0/all/0/1"&gt;Gilmer Yovani Castro Nieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camargo_J/0/1/0/all/0/1"&gt;Jorge E. Camargo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Stream Transformers. (arXiv:2107.10342v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10342</id>
        <link href="http://arxiv.org/abs/2107.10342"/>
        <updated>2021-07-23T02:00:30.986Z</updated>
        <summary type="html"><![CDATA[Transformer-based encoder-decoder models produce a fused token-wise
representation after every encoder layer. We investigate the effects of
allowing the encoder to preserve and explore alternative hypotheses, combined
at the end of the encoding process. To that end, we design and examine a
$\textit{Multi-stream Transformer}$ architecture and find that splitting the
Transformer encoder into multiple encoder streams and allowing the model to
merge multiple representational hypotheses improves performance, with further
improvement obtained by adding a skip connection between the first and the
final encoder layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1"&gt;Mikhail Burtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1"&gt;Anna Rumshisky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03438</id>
        <link href="http://arxiv.org/abs/2107.03438"/>
        <updated>2021-07-23T02:00:30.963Z</updated>
        <summary type="html"><![CDATA[To realize robots that can understand human instructions and perform
meaningful tasks in the near future, it is important to develop learned models
that can understand referential language to identify common objects in
real-world 3D scenes. In this paper, we develop a spatial-language model for a
3D visual grounding problem. Specifically, given a reconstructed 3D scene in
the form of a point cloud with 3D bounding boxes of potential object
candidates, and a language utterance referring to a target object in the scene,
our model identifies the target object from a set of potential candidates. Our
spatial-language model uses a transformer-based architecture that combines
spatial embedding from bounding-box with a finetuned language embedding from
DistilBert and reasons among the objects in the 3D scene to find the target
object. We show that our model performs competitively on visio-linguistic
datasets proposed by ReferIt3D. We provide additional analysis of performance
in spatial reasoning tasks decoupled from perception noise, the effect of
view-dependent utterances in terms of accuracy, and view-point annotations for
potential robotics applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1"&gt;Junha Roh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1"&gt;Karthik Desingh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-congruent non-degenerate curves with identical signatures. (arXiv:1912.09597v4 [math.DG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.09597</id>
        <link href="http://arxiv.org/abs/1912.09597"/>
        <updated>2021-07-23T02:00:30.911Z</updated>
        <summary type="html"><![CDATA[While the equality of differential signatures (Calabi et al, Int. J. Comput.
Vis. 26: 107-135, 1998) is known to be a necessary condition for congruence, it
is not sufficient (Musso and Nicolodi, J. Math Imaging Vis. 35: 68-85, 2009).
Hickman (J. Math Imaging Vis. 43: 206-213, 2012, Theorem 2) claimed that for
non-degenerate planar curves, equality of Euclidean signatures implies
congruence. We prove that while Hickman's claim holds for simple, closed curves
with simple signatures, it fails for curves with non-simple signatures. In the
later case, we associate a directed graph with the signature and show how
various paths along the graph give rise to a family of non-congruent,
non-degenerate curves with identical signatures. Using this additional
structure, we formulate congruence criteria for non-degenerate, closed, simple
curves and show how the paths reflect the global and local symmetries of the
corresponding curve.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Geiger_E/0/1/0/all/0/1"&gt;Eric Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kogan_I/0/1/0/all/0/1"&gt;Irina A. Kogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Search for Efficient Active Learning and Search of Rare Concepts. (arXiv:2007.00077v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00077</id>
        <link href="http://arxiv.org/abs/2007.00077"/>
        <updated>2021-07-23T02:00:30.904Z</updated>
        <summary type="html"><![CDATA[Many active learning and search approaches are intractable for large-scale
industrial settings with billions of unlabeled examples. Existing approaches
search globally for the optimal examples to label, scaling linearly or even
quadratically with the unlabeled data. In this paper, we improve the
computational efficiency of active learning and search methods by restricting
the candidate pool for labeling to the nearest neighbors of the currently
labeled set instead of scanning over all of the unlabeled data. We evaluate
several selection strategies in this setting on three large-scale computer
vision datasets: ImageNet, OpenImages, and a de-identified and aggregated
dataset of 10 billion images provided by a large internet company. Our approach
achieved similar mean average precision and recall as the traditional global
approach while reducing the computational cost of selection by up to three
orders of magnitude, thus enabling web-scale active learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1"&gt;Cody Coleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_E/0/1/0/all/0/1"&gt;Edward Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1"&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1"&gt;Sean Culatana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1"&gt;Peter Bailis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1"&gt;Alexander C. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sumbaly_R/0/1/0/all/0/1"&gt;Roshan Sumbaly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yalniz_I/0/1/0/all/0/1"&gt;I. Zeki Yalniz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frame-wise Cross-modal Matching for Video Moment Retrieval. (arXiv:2009.10434v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10434</id>
        <link href="http://arxiv.org/abs/2009.10434"/>
        <updated>2021-07-23T02:00:30.894Z</updated>
        <summary type="html"><![CDATA[Video moment retrieval targets at retrieving a moment in a video for a given
language query. The challenges of this task include 1) the requirement of
localizing the relevant moment in an untrimmed video, and 2) bridging the
semantic gap between textual query and video contents. To tackle those
problems, early approaches adopt the sliding window or uniform sampling to
collect video clips first and then match each clip with the query. Obviously,
these strategies are time-consuming and often lead to unsatisfied accuracy in
localization due to the unpredictable length of the golden moment. To avoid the
limitations, researchers recently attempt to directly predict the relevant
moment boundaries without the requirement to generate video clips first. One
mainstream approach is to generate a multimodal feature vector for the target
query and video frames (e.g., concatenation) and then use a regression approach
upon the multimodal feature vector for boundary detection. Although some
progress has been achieved by this approach, we argue that those methods have
not well captured the cross-modal interactions between the query and video
frames.

In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)
model which predicts the temporal boundaries based on an interaction modeling.
In addition, an attention module is introduced to assign higher weights to
query words with richer semantic cues, which are considered to be more
important for finding relevant video contents. Another contribution is that we
propose an additional predictor to utilize the internal frames in the model
training to improve the localization accuracy. Extensive experiments on two
datasets TACoS and Charades-STA demonstrate the superiority of our method over
several state-of-the-art methods. Ablation studies have been also conducted to
examine the effectiveness of different modules in our ACRM model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jihua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Member/0/1/0/all/0/1"&gt;Member&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1"&gt;IEEE&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyong Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights. (arXiv:2007.00864v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00864</id>
        <link href="http://arxiv.org/abs/2007.00864"/>
        <updated>2021-07-23T02:00:30.887Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models are widely used in many important domains. For
efficiently processing these computational- and memory-intensive applications,
tensors of these over-parameterized models are compressed by leveraging
sparsity, size reduction, and quantization of tensors. Unstructured sparsity
and tensors with varying dimensions yield irregular computation, communication,
and memory access patterns; processing them on hardware accelerators in a
conventional manner does not inherently leverage acceleration opportunities.
This paper provides a comprehensive survey on the efficient execution of sparse
and irregular tensor computations of ML models on hardware accelerators. In
particular, it discusses enhancement modules in the architecture design and the
software support; categorizes different hardware designs and acceleration
techniques and analyzes them in terms of hardware and execution costs; analyzes
achievable accelerations for recent DNNs; highlights further opportunities in
terms of hardware/software/model co-design optimizations (inter/intra-module).
The takeaways from this paper include: understanding the key challenges in
accelerating sparse, irregular-shaped, and quantized tensors; understanding
enhancements in accelerator systems for supporting their efficient
computations; analyzing trade-offs in opting for a specific design choice for
encoding, storing, extracting, communicating, computing, and load-balancing the
non-zeros; understanding how structured sparsity can improve storage efficiency
and balance computations; understanding how to compile and map models with
sparse tensors on the accelerators; understanding recent design trends for
efficient accelerations and further opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1"&gt;Shail Dave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1"&gt;Riyadh Baghdadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowatzki_T/0/1/0/all/0/1"&gt;Tony Nowatzki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1"&gt;Sasikanth Avancha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Aviral Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baoxin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks. (arXiv:2006.05066v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05066</id>
        <link href="http://arxiv.org/abs/2006.05066"/>
        <updated>2021-07-23T02:00:30.825Z</updated>
        <summary type="html"><![CDATA[Modern convolutional neural networks (CNNs) have massive identical
convolution blocks, and, hence, recursive sharing of parameters across these
blocks has been proposed to reduce the amount of parameters. However, naive
sharing of parameters poses many challenges such as limited representational
power and the vanishing/exploding gradients problem of recursively shared
parameters. In this paper, we present a recursive convolution block design and
training method, in which a recursively shareable part, or a filter basis, is
separated and learned while effectively avoiding the vanishing/exploding
gradients problem during training. We show that the unwieldy
vanishing/exploding gradients problem can be controlled by enforcing the
elements of the filter basis orthonormal, and empirically demonstrate that the
proposed orthogonality regularization improves the flow of gradients during
training. Experimental results on image classification and object detection
show that our approach, unlike previous parameter-sharing approaches, does not
trade performance to save parameters and consistently outperforms
overparameterized counterpart networks. This superior performance demonstrates
that the proposed recursive convolution block design and the orthogonality
regularization not only prevent performance degradation, but also consistently
improve the representation capability while a significant amount of parameters
are recursively shared.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Woochul Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyeon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation. (arXiv:2103.14304v7 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14304</id>
        <link href="http://arxiv.org/abs/2103.14304"/>
        <updated>2021-07-23T02:00:30.818Z</updated>
        <summary type="html"><![CDATA[Despite great progress in 3D human pose estimation from videos, it is still
an open problem to take full advantage of redundant 2D pose sequences to learn
representative representation for generating one single 3D pose. To this end,
we propose an improved Transformer-based architecture, called Strided
Transformer, for 3D human pose estimation in videos to lift a sequence of 2D
joint locations to a 3D pose. Specifically, a vanilla Transformer encoder (VTE)
is adopted to model long-range dependencies of 2D pose sequences. To reduce
redundancy of the sequence and aggregate information from local context,
strided convolutions are incorporated into VTE to progressively reduce the
sequence length. The modified VTE is termed as strided Transformer encoder
(STE) which is built upon the outputs of VTE. STE not only effectively
aggregates long-range information to a single-vector representation in a
hierarchical global and local fashion but also significantly reduces the
computation cost. Furthermore, a full-to-single supervision scheme is designed
at both the full sequence scale and single target frame scale, applied to the
outputs of VTE and STE, respectively. This scheme imposes extra temporal
smoothness constraints in conjunction with the single target frame supervision
and improves the representation ability of features for the target frame. The
proposed architecture is evaluated on two challenging benchmark datasets,
Human3.6M and HumanEva-I, and achieves state-of-the-art results with much fewer
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Runwei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pichao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Registration Method based on Deep Neural Network: Application to cardiac and liver MR images. (arXiv:2105.07392v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07392</id>
        <link href="http://arxiv.org/abs/2105.07392"/>
        <updated>2021-07-23T02:00:30.811Z</updated>
        <summary type="html"><![CDATA[Multi-modality medical images can provide relevant or complementary
information for a target (organ, tumor or tissue). Registering multi-modality
images to a common space can fuse these comprehensive information, and bring
convenience for clinical application. Recently, neural networks have been
widely investigated to boost registration methods. However, it is still
challenging to develop a multi-modality registration network due to the lack of
robust criteria for network training. In this work, we propose a multi-modality
registration network (MMRegNet), which can perform registration between
multi-modality images. Meanwhile, we present spatially encoded gradient
information to train MMRegNet in an unsupervised manner. The proposed network
was evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising
performance for left ventricle cardiac registration tasks. Meanwhile, to
demonstrate the versatility of MMRegNet, we further evaluate the method with a
liver dataset from CHAOS 2019. Source code will be released
publicly\footnote{https://github.com/NanYoMy/mmregnet} once the manuscript is
accepted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wangbin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liqin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Vessel Enhancement Using Flow-Based Consistencies. (arXiv:2101.05145v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05145</id>
        <link href="http://arxiv.org/abs/2101.05145"/>
        <updated>2021-07-23T02:00:30.804Z</updated>
        <summary type="html"><![CDATA[Vessel segmentation is an essential task in many clinical applications.
Although supervised methods have achieved state-of-art performance, acquiring
expert annotation is laborious and mostly limited for two-dimensional datasets
with a small sample size. On the contrary, unsupervised methods rely on
handcrafted features to detect tube-like structures such as vessels. However,
those methods require complex pipelines involving several hyper-parameters and
design choices rendering the procedure sensitive, dataset-specific, and not
generalizable. We propose a self-supervised method with a limited number of
hyper-parameters that is generalizable across modalities. Our method uses
tube-like structure properties, such as connectivity, profile consistency, and
bifurcation, to introduce inductive bias into a learning algorithm. To model
those properties, we generate a vector field that we refer to as a flow. Our
experiments on various public datasets in 2D and 3D show that our method
performs better than unsupervised methods while learning useful transferable
features from unlabeled data. Unlike generic self-supervised methods, the
learned features learn vessel-relevant features that are transferable for
supervised approaches, which is essential when the number of annotated data is
limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1"&gt;Rohit Jena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sumedha Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1"&gt;Kayhan Batmanghelich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10756</id>
        <link href="http://arxiv.org/abs/2107.10756"/>
        <updated>2021-07-23T02:00:30.798Z</updated>
        <summary type="html"><![CDATA[Faces generated using generative adversarial networks (GANs) have reached
unprecedented realism. These faces, also known as "Deep Fakes", appear as
realistic photographs with very little pixel-level distortions. While some work
has enabled the training of models that lead to the generation of specific
properties of the subject, generating a facial image based on a natural
language description has not been fully explored. For security and criminal
identification, the ability to provide a GAN-based system that works like a
sketch artist would be incredibly useful. In this paper, we present a novel
approach to generate facial images from semantic text descriptions. The learned
model is provided with a text description and an outline of the type of face,
which the model uses to sketch the features. Our models are trained using an
Affine Combination Module (ACM) mechanism to combine the text embedding from
BERT and the GAN latent space using a self-attention matrix. This avoids the
loss of features due to inadequate "attention", which may happen if text
embedding and latent vector are simply concatenated. Our approach is capable of
generating images that are very accurately aligned to the exhaustive textual
descriptions of faces with many fine detail features of the face and helps in
generating better images. The proposed method is also capable of making
incremental changes to a previously generated image if it is provided with
additional textual descriptions or sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1"&gt;Manan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1"&gt;Sukalpa Chanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An embedded deep learning system for augmented reality in firefighting applications. (arXiv:2009.10679v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2009.10679</id>
        <link href="http://arxiv.org/abs/2009.10679"/>
        <updated>2021-07-23T02:00:30.781Z</updated>
        <summary type="html"><![CDATA[Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1"&gt;Manish Bhattarai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_Curtis_A/0/1/0/all/0/1"&gt;Aura Rose Jensen-Curtis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MartiNez_Ramon_M/0/1/0/all/0/1"&gt;Manel Mart&amp;#xed;Nez-Ram&amp;#xf3;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Detection of Adversarial Examples with Model Explanations. (arXiv:2107.10480v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10480</id>
        <link href="http://arxiv.org/abs/2107.10480"/>
        <updated>2021-07-23T02:00:30.774Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have shown remarkable performance in a diverse
range of machine learning applications. However, it is widely known that DNNs
are vulnerable to simple adversarial perturbations, which causes the model to
incorrectly classify inputs. In this paper, we propose a simple yet effective
method to detect adversarial examples, using methods developed to explain the
model's behavior. Our key observation is that adding small, humanly
imperceptible perturbations can lead to drastic changes in the model
explanations, resulting in unusual or irregular forms of explanations. From
this insight, we propose an unsupervised detection of adversarial examples
using reconstructor networks trained only on model explanations of benign
examples. Our evaluations with MNIST handwritten dataset show that our method
is capable of detecting adversarial examples generated by the state-of-the-art
algorithms with high confidence. To the best of our knowledge, this work is the
first in suggesting unsupervised defense method using model explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1"&gt;Gihyuk Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1"&gt;Gyumin Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delving Deep into Label Smoothing. (arXiv:2011.12562v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12562</id>
        <link href="http://arxiv.org/abs/2011.12562"/>
        <updated>2021-07-23T02:00:30.768Z</updated>
        <summary type="html"><![CDATA[Label smoothing is an effective regularization tool for deep neural networks
(DNNs), which generates soft labels by applying a weighted average between the
uniform distribution and the hard label. It is often used to reduce the
overfitting problem of training DNNs and further improve classification
performance. In this paper, we aim to investigate how to generate more reliable
soft labels. We present an Online Label Smoothing (OLS) strategy, which
generates soft labels based on the statistics of the model prediction for the
target category. The proposed OLS constructs a more reasonable probability
distribution between the target categories and non-target categories to
supervise DNNs. Experiments demonstrate that based on the same classification
models, the proposed approach can effectively improve the classification
performance on CIFAR-100, ImageNet, and fine-grained datasets. Additionally,
the proposed method can significantly improve the robustness of DNN models to
noisy labels compared to current label smoothing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chang-Bin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng-Tao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rule-Based Classification of Hyperspectral Imaging Data. (arXiv:2107.10638v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10638</id>
        <link href="http://arxiv.org/abs/2107.10638"/>
        <updated>2021-07-23T02:00:30.759Z</updated>
        <summary type="html"><![CDATA[Due to its high spatial and spectral information content, hyperspectral
imaging opens up new possibilities for a better understanding of data and
scenes in a wide variety of applications. An essential part of this process of
understanding is the classification part. In this article we present a general
classification approach based on the shape of spectral signatures. In contrast
to classical classification approaches (e.g. SVM, KNN), not only reflectance
values are considered, but also parameters such as curvature points, curvature
values, and the curvature behavior of spectral signatures are used to develop
shape-describing rules in order to use them for classification by a rule-based
procedure using IF-THEN queries. The flexibility and efficiency of the
methodology is demonstrated using datasets from two different application
fields and leads to convincing results with good performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polat_S/0/1/0/all/0/1"&gt;Songuel Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tremeau_A/0/1/0/all/0/1"&gt;Alain Tremeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boochs_F/0/1/0/all/0/1"&gt;Frank Boochs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copy and Paste method based on Pose for ReID. (arXiv:2107.10479v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10479</id>
        <link href="http://arxiv.org/abs/2107.10479"/>
        <updated>2021-07-23T02:00:30.751Z</updated>
        <summary type="html"><![CDATA[Re-identification(ReID) aims at matching objects in surveillance cameras with
different viewpoints. It's developing very fast, but there is no processing
method for the ReID task in multiple scenarios at this stage. However, this
dose happen all the time in real life, such as the security scenarios. This
paper explores a new scenario of Re-identification, which differs in
perspective, background, and pose(walking or cycling).

Obviously, ordinary ReID processing methods cannot handle this scenario well.
As we all konw, the best way to deal with that it is to introduce image
datasets in this scanario, But this one is very expensive. To solve this
problem, this paper proposes a simple and effective way to generate images in
some new scenario, which is named Copy and Paste method based on Pose(CPP). The
CPP is a method based on key point detection, using copy and paste, to
composite a new semantic image dataset in two different semantic image
datasets. Such as, we can use pedestrians and bicycles to generate some images
that shows the same person rides on different bicycles. The CPP is suitable for
ReID tasks in new scenarios and it outperforms state-of-the-art on the original
datasets in original ReID tasks. Specifically, it can also have better
generalization performance for third-party public datasets. Code and Datasets
which composited by the CPP will be available in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v2 [physics.optics] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01422</id>
        <link href="http://arxiv.org/abs/2107.01422"/>
        <updated>2021-07-23T02:00:30.733Z</updated>
        <summary type="html"><![CDATA[Noninvasive optical imaging through dynamic scattering media has numerous
important biomedical applications but still remains a challenging task. While
standard methods aim to form images based upon optical absorption or
fluorescent emission, it is also well-established that the temporal correlation
of scattered coherent light diffuses through tissue much like optical
intensity. Few works to date, however, have aimed to experimentally measure and
process such data to demonstrate deep-tissue imaging of decorrelation dynamics.
In this work, we take advantage of a single-photon avalanche diode (SPAD) array
camera, with over one thousand detectors, to simultaneously detect speckle
fluctuations at the single-photon level from 12 different phantom tissue
surface locations delivered via a customized fiber bundle array. We then apply
a deep neural network to convert the acquired single-photon measurements into
video of scattering dynamics beneath rapidly decorrelating liquid tissue
phantoms. We demonstrate the ability to record video of dynamic events
occurring 5-8 mm beneath a decorrelating tissue phantom with mm-scale
resolution and at a 2.5-10 Hz frame rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shiqi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenhui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1"&gt;Joakim Jonsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1"&gt;Ruobing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1"&gt;Pavan Chandra Konda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kevin C. Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qionghai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1"&gt;Edouard Berrocal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1"&gt;Roarke Horstmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels. (arXiv:2101.05022v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05022</id>
        <link href="http://arxiv.org/abs/2101.05022"/>
        <updated>2021-07-23T02:00:30.727Z</updated>
        <summary type="html"><![CDATA[ImageNet has been arguably the most popular image classification benchmark,
but it is also the one with a significant level of label noise. Recent studies
have shown that many samples contain multiple classes, despite being assumed to
be a single-label benchmark. They have thus proposed to turn ImageNet
evaluation into a multi-label task, with exhaustive multi-label annotations per
image. However, they have not fixed the training set, presumably because of a
formidable annotation cost. We argue that the mismatch between single-label
annotations and effectively multi-label images is equally, if not more,
problematic in the training setup, where random crops are applied. With the
single-label annotations, a random crop of an image may contain an entirely
different object from the ground truth, introducing noisy or even incorrect
supervision during training. We thus re-label the ImageNet training set with
multi-labels. We address the annotation cost barrier by letting a strong image
classifier, trained on an extra source of data, generate the multi-labels. We
utilize the pixel-wise multi-label predictions before the final pooling layer,
in order to exploit the additional location-specific supervision signals.
Training on the re-labeled samples results in improved model performances
across the board. ResNet-50 attains the top-1 classification accuracy of 78.9%
on ImageNet with our localized multi-labels, which can be further boosted to
80.2% with the CutMix regularization. We show that the models trained with
localized multi-labels also outperforms the baselines on transfer learning to
object detection and instance segmentation tasks, and various robustness
benchmarks. The re-labeled ImageNet training set, pre-trained weights, and the
source code are available at {https://github.com/naver-ai/relabel_imagenet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1"&gt;Byeongho Heo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1"&gt;Junsuk Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation. (arXiv:2105.07044v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07044</id>
        <link href="http://arxiv.org/abs/2105.07044"/>
        <updated>2021-07-23T02:00:30.719Z</updated>
        <summary type="html"><![CDATA[In medical image synthesis, model training could be challenging due to the
inconsistencies between images of different modalities even with the same
patient, typically caused by internal status/tissue changes as different
modalities are usually obtained at a different time. This paper proposes a
novel deep learning method, Structure-aware Generative Adversarial Network
(SA-GAN), that preserves the shapes and locations of in-consistent structures
when generating medical images. SA-GAN is employed to generate synthetic
computed tomography (synCT) images from magnetic resonance imaging (MRI) with
two parallel streams: the global stream translates the input from the MRI to
the CT domain while the local stream automatically segments the inconsistent
organs, maintains their locations and shapes in MRI, and translates the organ
intensities to CT. Through extensive experiments on a pelvic dataset, we
demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs
and organ segmentation and supports MR-only treatment planning in disease sites
with internal organ status changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Emami_H/0/1/0/all/0/1"&gt;Hajar Emami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1"&gt;Ming Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nejad_Davarani_S/0/1/0/all/0/1"&gt;Siamak Nejad-Davarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glide_Hurst_C/0/1/0/all/0/1"&gt;Carri Glide-Hurst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Learning Approach for Egocentric Hand Gesture Recognition and Fingertip Detection. (arXiv:2101.02047v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02047</id>
        <link href="http://arxiv.org/abs/2101.02047"/>
        <updated>2021-07-23T02:00:30.712Z</updated>
        <summary type="html"><![CDATA[Head-mounted device-based human-computer interaction often requires
egocentric recognition of hand gestures and fingertips detection. In this
paper, a unified approach of egocentric hand gesture recognition and fingertip
detection is introduced. The proposed algorithm uses a single convolutional
neural network to predict the probabilities of finger class and positions of
fingertips in one forward propagation. Instead of directly regressing the
positions of fingertips from the fully connected layer, the ensemble of the
position of fingertips is regressed from the fully convolutional network.
Subsequently, the ensemble average is taken to regress the final position of
fingertips. Since the whole pipeline uses a single network, it is significantly
fast in computation. Experimental results show that the proposed method
outperforms the existing fingertip detection approaches including the Direct
Regression and the Heatmap-based framework. The effectiveness of the proposed
method is also shown in-the-wild scenario as well as in a use-case of virtual
reality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mohammad Mahmudul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mohammad Tariqul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1"&gt;S. M. Mahbubur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis. (arXiv:2008.05731v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05731</id>
        <link href="http://arxiv.org/abs/2008.05731"/>
        <updated>2021-07-23T02:00:30.694Z</updated>
        <summary type="html"><![CDATA[In recent years, Artificial Intelligence (AI) has proven its relevance for
medical decision support. However, the "black-box" nature of successful AI
algorithms still holds back their wide-spread deployment. In this paper, we
describe an eXplanatory Artificial Intelligence (XAI) that reaches the same
level of performance as black-box AI, for the task of classifying Diabetic
Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm,
called ExplAIn, learns to segment and categorize lesions in images; the final
image-level classification directly derives from these multivariate lesion
segmentations. The novelty of this explanatory framework is that it is trained
from end to end, with image supervision only, just like black-box AI
algorithms: the concepts of lesions and lesion categories emerge by themselves.
For improved lesion localization, foreground/background separation is trained
through self-supervision, in such a way that occluding foreground pixels
transforms the input image into a healthy-looking image. The advantage of such
an architecture is that automatic diagnoses can be explained simply by an image
and/or a few sentences. ExplAIn is evaluated at the image level and at the
pixel level on various CFP image datasets. We expect this new framework, which
jointly offers high classification performance and explainability, to
facilitate AI deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quellec_G/0/1/0/all/0/1"&gt;Gwenol&amp;#xe9; Quellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajj_H/0/1/0/all/0/1"&gt;Hassan Al Hajj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamard_M/0/1/0/all/0/1"&gt;Mathieu Lamard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1"&gt;Pierre-Henri Conze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massin_P/0/1/0/all/0/1"&gt;Pascale Massin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochener_B/0/1/0/all/0/1"&gt;B&amp;#xe9;atrice Cochener&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning-based Quality Assessment and Segmentation System with a Large-scale Benchmark Dataset for Optical Coherence Tomographic Angiography Image. (arXiv:2107.10476v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10476</id>
        <link href="http://arxiv.org/abs/2107.10476"/>
        <updated>2021-07-23T02:00:30.569Z</updated>
        <summary type="html"><![CDATA[Optical Coherence Tomography Angiography (OCTA) is a non-invasive and
non-contacting imaging technique providing visualization of microvasculature of
retina and optic nerve head in human eyes in vivo. The adequate image quality
of OCTA is the prerequisite for the subsequent quantification of retinal
microvasculature. Traditionally, the image quality score based on signal
strength is used for discriminating low quality. However, it is insufficient
for identifying artefacts such as motion and off-centration, which rely
specialized knowledge and need tedious and time-consuming manual
identification. One of the most primary issues in OCTA analysis is to sort out
the foveal avascular zone (FAZ) region in the retina, which highly correlates
with any visual acuity disease. However, the variations in OCTA visual quality
affect the performance of deep learning in any downstream marginally. Moreover,
filtering the low-quality OCTA images out is both labor-intensive and
time-consuming. To address these issues, we develop an automated computer-aided
OCTA image processing system using deep neural networks as the classifier and
segmentor to help ophthalmologists in clinical diagnosis and research. This
system can be an assistive tool as it can process OCTA images of different
formats to assess the quality and segment the FAZ area. The source code is
freely available at https://github.com/shanzha09/COIPS.git.

Another major contribution is the large-scale OCTA dataset, namely
OCTA-25K-IQA-SEG we publicize for performance evaluation. It is comprised of
four subsets, namely sOCTA-3$\times$3-10k, sOCTA-6$\times$6-14k,
sOCTA-3$\times$3-1.1k-seg, and dOCTA-6$\times$6-1.1k-seg, which contains a
total number of 25,665 images. The large-scale OCTA dataset is available at
https://doi.org/10.5281/zenodo.5111975, https://doi.org/10.5281/zenodo.5111972.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yiqing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Meng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1"&gt;Wenjia Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Weijing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve Learning from Crowds via Generative Augmentation. (arXiv:2107.10449v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10449</id>
        <link href="http://arxiv.org/abs/2107.10449"/>
        <updated>2021-07-23T02:00:30.540Z</updated>
        <summary type="html"><![CDATA[Crowdsourcing provides an efficient label collection schema for supervised
machine learning. However, to control annotation cost, each instance in the
crowdsourced data is typically annotated by a small number of annotators. This
creates a sparsity issue and limits the quality of machine learning models
trained on such data. In this paper, we study how to handle sparsity in
crowdsourced data using data augmentation. Specifically, we propose to directly
learn a classifier by augmenting the raw sparse annotations. We implement two
principles of high-quality augmentation using Generative Adversarial Networks:
1) the generated annotations should follow the distribution of authentic ones,
which is measured by a discriminator; 2) the generated annotations should have
high mutual information with the ground-truth labels, which is measured by an
auxiliary network. Extensive experiments and comparisons against an array of
state-of-the-art learning from crowds methods on three real-world datasets
proved the effectiveness of our data augmentation framework. It shows the
potential of our algorithm for low-budget crowdsourcing in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhendong Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Public Ground-Truth Dataset for Handwritten Circuit Diagram Images. (arXiv:2107.10373v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10373</id>
        <link href="http://arxiv.org/abs/2107.10373"/>
        <updated>2021-07-23T02:00:30.533Z</updated>
        <summary type="html"><![CDATA[The development of digitization methods for line drawings (especially in the
area of electrical engineering) relies on the availability of publicly
available training and evaluation data. This paper presents such an image set
along with annotations. The dataset consists of 1152 images of 144 circuits by
12 drafters and 48 563 annotations. Each of these images depicts an electrical
circuit diagram, taken by consumer grade cameras under varying lighting
conditions and perspectives. A variety of different pencil types and surface
materials has been used. For each image, all individual electrical components
are annotated with bounding boxes and one out of 45 class labels. In order to
simplify a graph extraction process, different helper symbols like junction
points and crossovers are introduced, while texts are annotated as well. The
geometric and taxonomic problems arising from this task as well as the classes
themselves and statistics of their appearances are stated. The performance of a
standard Faster RCNN on the dataset is provided as an object detection
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoma_F/0/1/0/all/0/1"&gt;Felix Thoma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1"&gt;Johannes Bayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yakun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query2Label: A Simple Transformer Way to Multi-Label Classification. (arXiv:2107.10834v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10834</id>
        <link href="http://arxiv.org/abs/2107.10834"/>
        <updated>2021-07-23T02:00:30.527Z</updated>
        <summary type="html"><![CDATA[This paper presents a simple and effective approach to solving the
multi-label classification problem. The proposed approach leverages Transformer
decoders to query the existence of a class label. The use of Transformer is
rooted in the need of extracting local discriminative features adaptively for
different labels, which is a strongly desired property due to the existence of
multiple objects in one image. The built-in cross-attention module in the
Transformer decoder offers an effective way to use label embeddings as queries
to probe and pool class-related features from a feature map computed by a
vision backbone for subsequent binary classifications. Compared with prior
works, the new framework is simple, using standard Transformers and vision
backbones, and effective, consistently outperforming all previous works on five
multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE,
and Visual Genome. Particularly, we establish $91.3\%$ mAP on MS-COCO. We hope
its compact structure, simple implementation, and superior performance serve as
a strong baseline for multi-label classification tasks and future studies. The
code will be available soon at https://github.com/SlongLiu/query2labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shilong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EAN: Event Adaptive Network for Enhanced Action Recognition. (arXiv:2107.10771v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10771</id>
        <link href="http://arxiv.org/abs/2107.10771"/>
        <updated>2021-07-23T02:00:30.515Z</updated>
        <summary type="html"><![CDATA[Efficiently modeling spatial-temporal information in videos is crucial for
action recognition. To achieve this goal, state-of-the-art methods typically
employ the convolution operator and the dense interaction modules such as
non-local blocks. However, these methods cannot accurately fit the diverse
events in videos. On the one hand, the adopted convolutions are with fixed
scales, thus struggling with events of various scales. On the other hand, the
dense interaction modeling paradigm only achieves sub-optimal performance as
action-irrelevant parts bring additional noises for the final prediction. In
this paper, we propose a unified action recognition framework to investigate
the dynamic nature of video content by introducing the following designs.
First, when extracting local cues, we generate the spatial-temporal kernels of
dynamic-scale to adaptively fit the diverse events. Second, to accurately
aggregate these cues into a global video representation, we propose to mine the
interactions only among a few selected foreground objects by a Transformer,
which yields a sparse paradigm. We call the proposed framework as Event
Adaptive Network (EAN) because both key designs are adaptive to the input video
content. To exploit the short-term motions within local segments, we propose a
novel and efficient Latent Motion Code (LMC) module, further improving the
performance of the framework. Extensive experiments on several large-scale
video datasets, e.g., Something-to-Something V1&V2, Kinetics, and Diving48,
verify that our models achieve state-of-the-art or competitive performances at
low FLOPs. Codes are available at:
https://github.com/tianyuan168326/EAN-Pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yichao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhiyong Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. (arXiv:2107.10833v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10833</id>
        <link href="http://arxiv.org/abs/2107.10833"/>
        <updated>2021-07-23T02:00:30.495Z</updated>
        <summary type="html"><![CDATA[Though many attempts have been made in blind super-resolution to restore
low-resolution images with unknown and complex degradations, they are still far
from addressing general real-world degraded images. In this work, we extend the
powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),
which is trained with pure synthetic data. Specifically, a high-order
degradation modeling process is introduced to better simulate complex
real-world degradations. We also consider the common ringing and overshoot
artifacts in the synthesis process. In addition, we employ a U-Net
discriminator with spectral normalization to increase discriminator capability
and stabilize the training dynamics. Extensive comparisons have shown its
superior visual performance than prior works on various real datasets. We also
provide efficient implementations to synthesize training pairs on the fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1"&gt;Liangbin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnonySIGN: Novel Human Appearance Synthesis for Sign Language Video Anonymisation. (arXiv:2107.10685v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10685</id>
        <link href="http://arxiv.org/abs/2107.10685"/>
        <updated>2021-07-23T02:00:30.488Z</updated>
        <summary type="html"><![CDATA[The visual anonymisation of sign language data is an essential task to
address privacy concerns raised by large-scale dataset collection. Previous
anonymisation techniques have either significantly affected sign comprehension
or required manual, labour-intensive work.

In this paper, we formally introduce the task of Sign Language Video
Anonymisation (SLVA) as an automatic method to anonymise the visual appearance
of a sign language video whilst retaining the meaning of the original sign
language sequence. To tackle SLVA, we propose AnonySign, a novel automatic
approach for visual anonymisation of sign language data. We first extract pose
information from the source video to remove the original signer appearance. We
next generate a photo-realistic sign language video of a novel appearance from
the pose sequence, using image-to-image translation methods in a conditional
variational autoencoder framework. An approximate posterior style distribution
is learnt, which can be sampled from to synthesise novel human appearances. In
addition, we propose a novel \textit{style loss} that ensures style consistency
in the anonymised sign language videos.

We evaluate AnonySign for the SLVA task with extensive quantitative and
qualitative experiments highlighting both realism and anonymity of our novel
human appearance synthesis. In addition, we formalise an anonymity perceptual
study as an evaluation criteria for the SLVA task and showcase that video
anonymisation using AnonySign retains the original sign language content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1"&gt;Ben Saunders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1"&gt;Necati Cihan Camgoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOVE: Learning Deformable 3D Objects by Watching Videos. (arXiv:2107.10844v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10844</id>
        <link href="http://arxiv.org/abs/2107.10844"/>
        <updated>2021-07-23T02:00:30.480Z</updated>
        <summary type="html"><![CDATA[Learning deformable 3D objects from 2D images is an extremely ill-posed
problem. Existing methods rely on explicit supervision to establish multi-view
correspondences, such as template shape models and keypoint annotations, which
restricts their applicability on objects "in the wild". In this paper, we
propose to use monocular videos, which naturally provide correspondences across
time, allowing us to learn 3D shapes of deformable object categories without
explicit keypoints or template shapes. Specifically, we present DOVE, which
learns to predict 3D canonical shape, deformation, viewpoint and texture from a
single 2D image of a bird, given a bird video collection as well as
automatically obtained silhouettes and optical flows as training data. Our
method reconstructs temporally consistent 3D shape and deformation, which
allows us to animate and re-render the bird from arbitrary viewpoints from a
single image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shangzhe Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakab_T/0/1/0/all/0/1"&gt;Tomas Jakab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1"&gt;Christian Rupprecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-transfer learning via patches: A prostate cancer triage approach based on bi-parametric MRI. (arXiv:2107.10806v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10806</id>
        <link href="http://arxiv.org/abs/2107.10806"/>
        <updated>2021-07-23T02:00:30.463Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is the second most common cancer diagnosed among men
worldwide. The current PCa diagnostic pathway comes at the cost of substantial
overdiagnosis, leading to unnecessary treatment and further testing.
Bi-parametric magnetic resonance imaging (bp-MRI) based on apparent diffusion
coefficient maps (ADC) and T2-weighted (T2w) sequences has been proposed as a
triage test to differentiate between clinically significant (cS) and
non-clinically significant (ncS) prostate lesions. However, analysis of the
sequences relies on expertise, requires specialized training, and suffers from
inter-observer variability. Deep learning (DL) techniques hold promise in tasks
such as classification and detection. Nevertheless, they rely on large amounts
of annotated data which is not common in the medical field. In order to
palliate such issues, existing works rely on transfer learning (TL) and
ImageNet pre-training, which has been proven to be sub-optimal for the medical
imaging domain. In this paper, we present a patch-based pre-training strategy
to distinguish between cS and ncS lesions which exploit the region of interest
(ROI) of the patched source domain to efficiently train a classifier in the
full-slice target domain which does not require annotations by making use of
transfer learning (TL). We provide a comprehensive comparison between several
CNNs architectures and different settings which are presented as a baseline.
Moreover, we explore cross-domain TL which exploits both MRI modalities and
improves single modality results. Finally, we show how our approaches
outperform the standard approaches by a considerable margin]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fernandez_Quilez_A/0/1/0/all/0/1"&gt;Alvaro Fernandez-Quilez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eftestol_T/0/1/0/all/0/1"&gt;Trygve Eftest&amp;#xf8;l&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goodwin_M/0/1/0/all/0/1"&gt;Morten Goodwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kjosavik_S/0/1/0/all/0/1"&gt;Svein Reidar Kjosavik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oppedal_K/0/1/0/all/0/1"&gt;Ketil Oppedal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI. (arXiv:2107.10718v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10718</id>
        <link href="http://arxiv.org/abs/2107.10718"/>
        <updated>2021-07-23T02:00:30.453Z</updated>
        <summary type="html"><![CDATA[Assessment of cardiovascular disease (CVD) with cine magnetic resonance
imaging (MRI) has been used to non-invasively evaluate detailed cardiac
structure and function. Accurate segmentation of cardiac structures from cine
MRI is a crucial step for early diagnosis and prognosis of CVD, and has been
greatly improved with convolutional neural networks (CNN). There, however, are
a number of limitations identified in CNN models, such as limited
interpretability and high complexity, thus limiting their use in clinical
practice. In this work, to address the limitations, we propose a lightweight
and interpretable machine learning model, successive subspace learning with the
subspace approximation with adjusted bias (Saab) transform, for accurate and
efficient segmentation from cine MRI. Specifically, our segmentation framework
is comprised of the following steps: (1) sequential expansion of near-to-far
neighborhood at different resolutions; (2) channel-wise subspace approximation
using the Saab transform for unsupervised dimension reduction; (3) class-wise
entropy guided feature selection for supervised dimension reduction; (4)
concatenation of features and pixel-wise classification with gradient boost;
and (5) conditional random field for post-processing. Experimental results on
the ACDC 2017 segmentation database, showed that our framework performed better
than state-of-the-art U-Net models with 200$\times$ fewer parameters in
delineating the left ventricle, right ventricle, and myocardium, thus showing
its potential to be used in clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaggin_H/0/1/0/all/0/1"&gt;Hanna K. Gaggin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weichung Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding. (arXiv:2107.10466v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10466</id>
        <link href="http://arxiv.org/abs/2107.10466"/>
        <updated>2021-07-23T02:00:30.446Z</updated>
        <summary type="html"><![CDATA[Current methods of multi-person pose estimation typically treat the
localization and the association of body joints separately. It is convenient
but inefficient, leading to additional computation and a waste of time. This
paper, however, presents a novel framework PoseDet (Estimating Pose by
Detection) to localize and associate body joints simultaneously at higher
inference speed. Moreover, we propose the keypoint-aware pose embedding to
represent an object in terms of the locations of its keypoints. The proposed
pose embedding contains semantic and geometric information, allowing us to
access discriminative and informative features efficiently. It is utilized for
candidate classification and body joint localization in PoseDet, leading to
robust predictions of various poses. This simple framework achieves an
unprecedented speed and a competitive accuracy on the COCO benchmark compared
with state-of-the-art methods. Extensive experiments on the CrowdPose benchmark
show the robustness in the crowd scenes. Source code is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chenyu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Ran Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xinyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Weihao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yujiu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoqian Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fristograms: Revealing and Exploiting Light Field Internals. (arXiv:2107.10563v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10563</id>
        <link href="http://arxiv.org/abs/2107.10563"/>
        <updated>2021-07-23T02:00:30.438Z</updated>
        <summary type="html"><![CDATA[In recent years, light field (LF) capture and processing has become an
integral part of media production. The richness of information available in LFs
has enabled novel applications like post-capture depth-of-field editing, 3D
reconstruction, segmentation and matting, saliency detection, object detection
and recognition, and mixed reality. The efficacy of such applications depends
on certain underlying requirements, which are often ignored. For example, some
operations such as noise-reduction, or hyperfan-filtering are only possible if
a scene point Lambertian radiator. Some other operations such as the removal of
obstacles or looking behind objects are only possible if there is at least one
ray capturing the required scene point. Consequently, the ray distribution
representing a certain scene point is an important characteristic for
evaluating processing possibilities. The primary idea in this paper is to
establish a relation between the capturing setup and the rays of the LF. To
this end, we discretize the view frustum. Traditionally, a uniform
discretization of the view frustum results in voxels that represents a single
sample on a regularly spaced, 3-D grid. Instead, we use frustum-shaped voxels
(froxels), by using depth and capturing-setup dependent discretization of the
view frustum. Based on such discretization, we count the number of rays mapping
to the same pixel on the capturing device(s). By means of this count, we
propose histograms of ray-counts over the froxels (fristograms). Fristograms
can be used as a tool to analyze and reveal interesting aspects of the
underlying LF, like the number of rays originating from a scene point and the
color distribution of these rays. As an example, we show its ability by
significantly reducing the number of rays which enables noise reduction while
maintaining the realistic rendering of non-Lambertian or partially occluded
regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Herfet_T/0/1/0/all/0/1"&gt;Thorsten Herfet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chelli_K/0/1/0/all/0/1"&gt;Kelvin Chelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1"&gt;Tobias Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kremer_R/0/1/0/all/0/1"&gt;Robin Kremer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure Destruction and Content Combination for Face Anti-Spoofing. (arXiv:2107.10628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10628</id>
        <link href="http://arxiv.org/abs/2107.10628"/>
        <updated>2021-07-23T02:00:30.431Z</updated>
        <summary type="html"><![CDATA[In pursuit of consolidating the face verification systems, prior face
anti-spoofing studies excavate the hidden cues in original images to
discriminate real persons and diverse attack types with the assistance of
auxiliary supervision. However, limited by the following two inherent
disturbances in their training process: 1) Complete facial structure in a
single image. 2) Implicit subdomains in the whole dataset, these methods are
prone to stick on memorization of the entire training dataset and show
sensitivity to nonhomologous domain distribution. In this paper, we propose
Structure Destruction Module and Content Combination Module to address these
two imitations separately. The former mechanism destroys images into patches to
construct a non-structural input, while the latter mechanism recombines patches
from different subdomains or classes into a mixup construct. Based on this
splitting-and-splicing operation, Local Relation Modeling Module is further
proposed to model the second-order relationship between patches. We evaluate
our method on extensive public datasets and promising experimental results to
demonstrate the reliability of our method against state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke-Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Taiping Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shice Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bangjie Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shouhong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Shape Generation with Grid-based Implicit Functions. (arXiv:2107.10607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10607</id>
        <link href="http://arxiv.org/abs/2107.10607"/>
        <updated>2021-07-23T02:00:30.424Z</updated>
        <summary type="html"><![CDATA[Previous approaches to generate shapes in a 3D setting train a GAN on the
latent space of an autoencoder (AE). Even though this produces convincing
results, it has two major shortcomings. As the GAN is limited to reproduce the
dataset the AE was trained on, we cannot reuse a trained AE for novel data.
Furthermore, it is difficult to add spatial supervision into the generation
process, as the AE only gives us a global representation. To remedy these
issues, we propose to train the GAN on grids (i.e. each cell covers a part of a
shape). In this representation each cell is equipped with a latent vector
provided by an AE. This localized representation enables more expressiveness
(since the cell-based latent vectors can be combined in novel ways) as well as
spatial control of the generation process (e.g. via bounding boxes). Our method
outperforms the current state of the art on all established evaluation
measures, proposed for quantitatively evaluating the generative capabilities of
GANs. We show limitations of these measures and propose the adaptation of a
robust criterion from statistical analysis as an alternative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1"&gt;Moritz Ibing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1"&gt;Isaak Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1"&gt;Leif Kobbelt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep 3D-CNN for Depression Diagnosis with Facial Video Recording of Self-Rating Depression Scale Questionnaire. (arXiv:2107.10712v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10712</id>
        <link href="http://arxiv.org/abs/2107.10712"/>
        <updated>2021-07-23T02:00:30.413Z</updated>
        <summary type="html"><![CDATA[The Self-Rating Depression Scale (SDS) questionnaire is commonly utilized for
effective depression preliminary screening. The uncontrolled self-administered
measure, on the other hand, maybe readily influenced by insouciant or dishonest
responses, yielding different findings from the clinician-administered
diagnostic. Facial expression (FE) and behaviors are important in
clinician-administered assessments, but they are underappreciated in
self-administered evaluations. We use a new dataset of 200 participants to
demonstrate the validity of self-rating questionnaires and their accompanying
question-by-question video recordings in this study. We offer an end-to-end
system to handle the face video recording that is conditioned on the
questionnaire answers and the responding time to automatically interpret
sadness from the SDS assessment and the associated video. We modified a 3D-CNN
for temporal feature extraction and compared various state-of-the-art temporal
modeling techniques. The superior performance of our system shows the validity
of combining facial video recording with the SDS score for more accurate
self-diagnose.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HANT: Hardware-Aware Network Transformation. (arXiv:2107.10624v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10624</id>
        <link href="http://arxiv.org/abs/2107.10624"/>
        <updated>2021-07-23T02:00:30.385Z</updated>
        <summary type="html"><![CDATA[Given a trained network, how can we accelerate it to meet efficiency needs
for deployment on particular hardware? The commonly used hardware-aware network
compression techniques address this question with pruning, kernel fusion,
quantization and lowering precision. However, these approaches do not change
the underlying network operations. In this paper, we propose hardware-aware
network transformation (HANT), which accelerates a network by replacing
inefficient operations with more efficient alternatives using a neural
architecture search like approach. HANT tackles the problem in two phase: In
the first phase, a large number of alternative operations per every layer of
the teacher model is trained using layer-wise feature map distillation. In the
second phase, the combinatorial selection of efficient operations is relaxed to
an integer optimization problem that can be solved in a few seconds. We extend
HANT with kernel fusion and quantization to improve throughput even further.
Our experimental results on accelerating the EfficientNet family show that HANT
can accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the
ImageNet dataset. When comparing the same latency level, HANT can accelerate
EfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher
accuracy. We examine a large pool of operations, up to 197 per layer, and we
provide insights into the selected operations and final architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1"&gt;Jimmy Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1"&gt;Nicolo Fusi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1"&gt;Arash Vahdat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10493</id>
        <link href="http://arxiv.org/abs/2107.10493"/>
        <updated>2021-07-23T02:00:30.325Z</updated>
        <summary type="html"><![CDATA[Abstract reasoning, i.e., inferring complicated patterns from given
observations, is a central building block of artificial general intelligence.
While humans find the answer by either eliminating wrong candidates or first
constructing the answer, prior deep neural network (DNN)-based methods focus on
the former discriminative approach. This paper aims to design a framework for
the latter approach and bridge the gap between artificial and human
intelligence. To this end, we propose logic-guided generation (LoGe), a novel
generative DNN framework that reduces abstract reasoning as an optimization
problem in propositional logic. LoGe is composed of three steps: extract
propositional variables from images, reason the answer variables with a logic
layer, and reconstruct the answer image from the variables. We demonstrate that
LoGe outperforms the black box DNN frameworks for generative abstract reasoning
under the RAVEN benchmark, i.e., reconstructing answers based on capturing
correct rules of various attributes from observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sihyun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[External-Memory Networks for Low-Shot Learning of Targets in Forward-Looking-Sonar Imagery. (arXiv:2107.10504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10504</id>
        <link href="http://arxiv.org/abs/2107.10504"/>
        <updated>2021-07-23T02:00:30.262Z</updated>
        <summary type="html"><![CDATA[We propose a memory-based framework for real-time, data-efficient target
analysis in forward-looking-sonar (FLS) imagery. Our framework relies on first
removing non-discriminative details from the imagery using a small-scale
DenseNet-inspired network. Doing so simplifies ensuing analyses and permits
generalizing from few labeled examples. We then cascade the filtered imagery
into a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot
target recognition. We employ a small-scale FlowNet, LFN to align and register
FLS imagery across local temporal scales. LFN enables target label consensus
voting across images and generally improves target detection and recognition
rates.

We evaluate our framework using real-world FLS imagery with multiple broad
target classes that have high intra-class variability and rich sub-class
structure. We show that few-shot learning, with anywhere from ten to thirty
class-specific exemplars, performs similarly to supervised deep networks
trained on hundreds of samples per class. Effective zero-shot learning is also
possible. High performance is realized from the inductive-transfer properties
of NRMNs when distractor elements are removed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1"&gt;Isaac J. Sledge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toole_C/0/1/0/all/0/1"&gt;Christopher D. Toole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maestri_J/0/1/0/all/0/1"&gt;Joseph A. Maestri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1"&gt;Jose C. Principe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogSense: A Cognitively Inspired Framework for Perception Adaptation. (arXiv:2107.10456v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10456</id>
        <link href="http://arxiv.org/abs/2107.10456"/>
        <updated>2021-07-23T02:00:30.244Z</updated>
        <summary type="html"><![CDATA[This paper proposes the CogSense system, which is inspired by sense-making
cognition and perception in the mammalian brain to perform perception error
detection and perception parameter adaptation using probabilistic signal
temporal logic. As a specific application, a contrast-based perception adaption
method is presented and validated. The proposed method evaluates perception
errors using heterogeneous probe functions computed from the detected objects
and subsequently solves a contrast optimization problem to correct perception
errors. The CogSense probe functions utilize the characteristics of geometry,
dynamics, and detected blob image quality of the objects to develop axioms in a
probabilistic signal temporal logic framework. By evaluating these axioms, we
can formally verify whether the detections are valid or erroneous. Further,
using the CogSense axioms, we generate the probabilistic signal temporal
logic-based constraints to finally solve the contrast-based optimization
problem to reduce false positives and false negatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1"&gt;Hyukseong Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1"&gt;Amir Rahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kevin G. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Amit Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_R/0/1/0/all/0/1"&gt;Rajan Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Dilated Convolution For Human Pose Estimation. (arXiv:2107.10477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10477</id>
        <link href="http://arxiv.org/abs/2107.10477"/>
        <updated>2021-07-23T02:00:30.236Z</updated>
        <summary type="html"><![CDATA[Most existing human pose estimation (HPE) methods exploit multi-scale
information by fusing feature maps of four different spatial sizes, \ie $1/4$,
$1/8$, $1/16$, and $1/32$ of the input image. There are two drawbacks of this
strategy: 1) feature maps of different spatial sizes may be not well aligned
spatially, which potentially hurts the accuracy of keypoint location; 2) these
scales are fixed and inflexible, which may restrict the generalization ability
over various human sizes. Towards these issues, we propose an adaptive dilated
convolution (ADC). It can generate and fuse multi-scale features of the same
spatial sizes by setting different dilation rates for different channels. More
importantly, these dilation rates are generated by a regression module. It
enables ADC to adaptively adjust the fused scales and thus ADC may generalize
better to various human sizes. ADC can be end-to-end trained and easily plugged
into existing methods. Extensive experiments show that ADC can bring consistent
improvements to various HPE methods. The source codes will be released for
further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhengxiong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning. (arXiv:2107.10419v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10419</id>
        <link href="http://arxiv.org/abs/2107.10419"/>
        <updated>2021-07-23T02:00:30.210Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning (SSL) has achieved great success in
unsupervised visual representation learning by maximizing the similarity
between two augmented views of the same image (positive pairs) and
simultaneously contrasting other different images (negative pairs). However,
this type of methods, such as SimCLR and MoCo, relies heavily on a large number
of negative pairs and thus requires either large batches or memory banks. In
contrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,
attempt to discard negative pairs by introducing asymmetry and show remarkable
performance. Unfortunately, to avoid collapsed solutions caused by not using
negative pairs, these methods require sophisticated asymmetry designs. In this
paper, we argue that negative pairs are still necessary but one is sufficient,
i.e., triplet is all you need. A simple triplet-based loss can achieve
surprisingly good performance without requiring large batches or asymmetry.
Moreover, we observe that unsupervised visual representation learning can gain
significantly from randomness. Based on this observation, we propose a simple
plug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other
spaces and enforcing these randomly projected samples to satisfy the same
correlation requirement. The proposed ROMA strategy not only achieves the
state-of-the-art performance in conjunction with the triplet-based loss, but
also can further effectively boost other SSL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xuesong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1"&gt;Meihao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1"&gt;Jing Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mmPose-NLP: A Natural Language Processing Approach to Precise Skeletal Pose Estimation using mmWave Radars. (arXiv:2107.10327v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.10327</id>
        <link href="http://arxiv.org/abs/2107.10327"/>
        <updated>2021-07-23T02:00:30.198Z</updated>
        <summary type="html"><![CDATA[In this paper we presented mmPose-NLP, a novel Natural Language Processing
(NLP) inspired Sequence-to-Sequence (Seq2Seq) skeletal key-point estimator
using millimeter-wave (mmWave) radar data. To the best of the author's
knowledge, this is the first method to precisely estimate upto 25 skeletal
key-points using mmWave radar data alone. Skeletal pose estimation is critical
in several applications ranging from autonomous vehicles, traffic monitoring,
patient monitoring, gait analysis, to defense security forensics, and aid both
preventative and actionable decision making. The use of mmWave radars for this
task, over traditionally employed optical sensors, provide several advantages,
primarily its operational robustness to scene lighting and adverse weather
conditions, where optical sensor performance degrade significantly. The mmWave
radar point-cloud (PCL) data is first voxelized (analogous to tokenization in
NLP) and $N$ frames of the voxelized radar data (analogous to a text paragraph
in NLP) is subjected to the proposed mmPose-NLP architecture, where the voxel
indices of the 25 skeletal key-points (analogous to keyword extraction in NLP)
are predicted. The voxel indices are converted back to real world 3-D
coordinates using the voxel dictionary used during the tokenization process.
Mean Absolute Error (MAE) metrics were used to measure the accuracy of the
proposed system against the ground truth, with the proposed mmPose-NLP offering
<3 cm localization errors in the depth, horizontal and vertical axes. The
effect of the number of input frames vs performance/accuracy was also studied
for N = {1,2,..,10}. A comprehensive methodology, results, discussions and
limitations are presented in this paper. All the source codes and results are
made available on GitHub for furthering research and development in this
critical yet emerging domain of skeletal key-point estimation using mmWave
radars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sengupta_A/0/1/0/all/0/1"&gt;Arindam Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_S/0/1/0/all/0/1"&gt;Siyang Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Trajectory Forecasting Evaluation. (arXiv:2107.10297v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.10297</id>
        <link href="http://arxiv.org/abs/2107.10297"/>
        <updated>2021-07-23T02:00:30.189Z</updated>
        <summary type="html"><![CDATA[Forecasting the behavior of other agents is an integral part of the modern
robotic autonomy stack, especially in safety-critical scenarios with
human-robot interaction, such as autonomous driving. In turn, there has been a
significant amount of interest and research in trajectory forecasting,
resulting in a wide variety of approaches. Common to all works, however, is the
use of the same few accuracy-based evaluation metrics, e.g., displacement error
and log-likelihood. While these metrics are informative, they are task-agnostic
and predictions that are evaluated as equal can lead to vastly different
outcomes, e.g., in downstream planning and decision making. In this work, we
take a step back and critically evaluate current trajectory forecasting
metrics, proposing task-aware metrics as a better measure of performance in
systems where prediction is being deployed. We additionally present one example
of such a metric, incorporating planning-awareness within existing trajectory
forecasting metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1"&gt;Boris Ivanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations. (arXiv:2107.10296v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10296</id>
        <link href="http://arxiv.org/abs/2107.10296"/>
        <updated>2021-07-23T02:00:30.171Z</updated>
        <summary type="html"><![CDATA[This paper proposes a correspondence-free method for point cloud rotational
registration. We learn an embedding for each point cloud in a feature space
that preserves the SO(3)-equivariance property, enabled by recent developments
in equivariant neural networks. The proposed shape registration method achieves
three major advantages through combining equivariant feature learning with
implicit shape models. First, the necessity of data association is removed
because of the permutation-invariant property in network architectures similar
to PointNet. Second, the registration in feature space can be solved in
closed-form using Horn's method due to the SO(3)-equivariance property. Third,
the registration is robust to noise in the point cloud because of implicit
shape learning. The experimental results show superior performance compared
with existing correspondence-free deep registration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Minghan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1"&gt;Maani Ghaffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Huei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10300</id>
        <link href="http://arxiv.org/abs/2107.10300"/>
        <updated>2021-07-23T02:00:30.160Z</updated>
        <summary type="html"><![CDATA[Causality knowledge is vital to building robust AI systems. Deep learning
models often perform poorly on tasks that require causal reasoning, which is
often derived using some form of commonsense knowledge not immediately
available in the input but implicitly inferred by humans. Prior work has
unraveled spurious observational biases that models fall prey to in the absence
of causality. While language representation models preserve contextual
knowledge within learned embeddings, they do not factor in causal relationships
during training. By blending causal relationships with the input features to an
existing model that performs visual cognition tasks (such as scene
understanding, video captioning, video question-answering, etc.), better
performance can be achieved owing to the insight causal relationships bring
about. Recently, several models have been proposed that have tackled the task
of mining causal data from either the visual or textual modality. However,
there does not exist widespread research that mines causal relationships by
juxtaposing the visual and language modalities. While images offer a rich and
easy-to-process resource for us to mine causality knowledge from, videos are
denser and consist of naturally time-ordered events. Also, textual information
offers details that could be implicit in videos. We propose iReason, a
framework that infers visual-semantic commonsense knowledge using both videos
and natural language captions. Furthermore, iReason's architecture integrates a
causal rationalization module to aid the process of interpretability, error
analysis and bias detection. We demonstrate the effectiveness of iReason using
a two-pronged comparative analysis with language representation learning models
(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1"&gt;Aman Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1"&gt;Vinija Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Capacity Framework for Reversible Data Hiding in Encrypted Image Using Pixel Predictions and Entropy Encoding. (arXiv:2102.12613v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12613</id>
        <link href="http://arxiv.org/abs/2102.12613"/>
        <updated>2021-07-23T02:00:30.134Z</updated>
        <summary type="html"><![CDATA[Previous reversible data hiding in encrypted images (RDHEI) schemes can be
either carried out by vacating room before or after data encryption, which
leads to a separation of the search field in RDHEI. Besides, high capacity
relies heavily on vacating room before encryption (VRBE), which significantly
lowers the payload of vacating room after encryption (VRAE) based schemes. To
address this issue, this paper proposes a framework for high-capacity RDHEI for
both VRBE and VRAE cases using pixel predictions and entropy encoding. We
propose an embedding room generation algorithm to produce vacated room by
generating the prediction-error histogram (PEH) of the selected cover using
adjacency prediction and the median edge detector (MED). In the VRBE scenario,
we propose a scheme that generates the embedding room using the proposed
algorithm, and encrypts the preprocessed image by using the stream cipher with
two encryption keys. In the VRAE scenario, we propose a scheme that involves an
improved block modulation and permutation encryption algorithm where the
spatial redundancy in the plain-text image can be largely preserved. Then the
proposed algorithm is applied on the encrypted image to generate the embedding
room. At the data hider's side of both the schemes, the data hider locates the
embedding room and embeds the encrypted additional data. On receiving the
marked encrypted image, the receivers with different authentication can
respectively conduct error-free data extraction and/or error-free image
recovery. The experimental results show that the two schemes in the proposed
framework can outperform many previous state-of-the-art RDHEI arts. Besides,
the proposed schemes can ensure high information security in that little detail
of the original image can be directly discovered from the encrypted images or
the marked encrypted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yingqiang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuyan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1"&gt;Qichao Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Huanqiang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhenxing Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepScale: An Online Frame Size Adaptation Framework to Accelerate Visual Multi-object Tracking. (arXiv:2107.10404v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10404</id>
        <link href="http://arxiv.org/abs/2107.10404"/>
        <updated>2021-07-23T02:00:30.108Z</updated>
        <summary type="html"><![CDATA[In surveillance and search and rescue applications, it is important to
perform multi-target tracking (MOT) in real-time on low-end devices. Today's
MOT solutions employ deep neural networks, which tend to have high computation
complexity. Recognizing the effects of frame sizes on tracking performance, we
propose DeepScale, a model agnostic frame size selection approach that operates
on top of existing fully convolutional network-based trackers to accelerate
tracking throughput. In the training stage, we incorporate detectability scores
into a one-shot tracker architecture so that DeepScale can learn representation
estimations for different frame sizes in a self-supervised manner. During
inference, based on user-controlled parameters, it can find a suitable
trade-off between tracking accuracy and speed by adapting frame sizes at run
time. Extensive experiments and benchmark tests on MOT datasets demonstrate the
effectiveness and flexibility of DeepScale. Compared to a state-of-the-art
tracker, DeepScale++, a variant of DeepScale achieves 1.57X accelerated with
only moderate degradation (~ 2.4) in tracking accuracy on the MOT15 dataset in
one configuration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nalaie_K/0/1/0/all/0/1"&gt;Keivan Nalaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Rong Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reading Race: AI Recognises Patient's Racial Identity In Medical Images. (arXiv:2107.10356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10356</id>
        <link href="http://arxiv.org/abs/2107.10356"/>
        <updated>2021-07-23T02:00:30.095Z</updated>
        <summary type="html"><![CDATA[Background: In medical imaging, prior studies have demonstrated disparate AI
performance by race, yet there is no known correlation for race on medical
imaging that would be obvious to the human expert interpreting the images.

Methods: Using private and public datasets we evaluate: A) performance
quantification of deep learning models to detect race from medical images,
including the ability of these models to generalize to external environments
and across multiple imaging modalities, B) assessment of possible confounding
anatomic and phenotype population features, such as disease distribution and
body habitus as predictors of race, and C) investigation into the underlying
mechanism by which AI models can recognize race.

Findings: Standard deep learning models can be trained to predict race from
medical images with high performance across multiple imaging modalities. Our
findings hold under external validation conditions, as well as when models are
optimized to perform clinically motivated tasks. We demonstrate this detection
is not due to trivial proxies or imaging-related surrogate covariates for race,
such as underlying disease distribution. Finally, we show that performance
persists over all anatomical regions and frequency spectrum of the images
suggesting that mitigation efforts will be challenging and demand further
study.

Interpretation: We emphasize that model ability to predict self-reported race
is itself not the issue of importance. However, our findings that AI can
trivially predict self-reported race -- even from corrupted, cropped, and
noised medical images -- in a setting where clinical experts cannot, creates an
enormous risk for all model deployments in medical imaging: if an AI model
secretly used its knowledge of self-reported race to misclassify all Black
patients, radiologists would not be able to tell using the same data the model
has access to.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1"&gt;Imon Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhimireddy_A/0/1/0/all/0/1"&gt;Ananth Reddy Bhimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo Anthony Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li-Ching Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correa_R/0/1/0/all/0/1"&gt;Ramon Correa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dullerud_N/0/1/0/all/0/1"&gt;Natalie Dullerud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shih-Cheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1"&gt;Po-Chih Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palmer_L/0/1/0/all/0/1"&gt;Lyle Palmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1"&gt;Brandon J Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1"&gt;Saptarshi Purkayastha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyrros_A/0/1/0/all/0/1"&gt;Ayis Pyrros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oakden_Rayner_L/0/1/0/all/0/1"&gt;Luke Oakden-Rayner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okechukwu_C/0/1/0/all/0/1"&gt;Chima Okechukwu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1"&gt;Laleh Seyyed-Kalantari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1"&gt;Hari Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ryan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zachary Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoran Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy W Gichoya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking. (arXiv:2107.10433v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10433</id>
        <link href="http://arxiv.org/abs/2107.10433"/>
        <updated>2021-07-23T02:00:30.082Z</updated>
        <summary type="html"><![CDATA[Many RGB-T trackers attempt to attain robust feature representation by
utilizing an adaptive weighting scheme (or attention mechanism). Different from
these works, we propose a new dynamic modality-aware filter generation module
(named MFGNet) to boost the message communication between visible and thermal
data by adaptively adjusting the convolutional kernels for various input images
in practical tracking. Given the image pairs as input, we first encode their
features with the backbone network. Then, we concatenate these feature maps and
generate dynamic modality-aware filters with two independent networks. The
visible and thermal filters will be used to conduct a dynamic convolutional
operation on their corresponding input feature maps respectively. Inspired by
residual connection, both the generated visible and thermal feature maps will
be summarized with input feature maps. The augmented feature maps will be fed
into the RoI align module to generate instance-level features for subsequent
classification. To address issues caused by heavy occlusion, fast motion, and
out-of-view, we propose to conduct a joint local and global search by
exploiting a new direction-aware target-driven attention mechanism. The spatial
and temporal recurrent neural network is used to capture the direction-aware
context for accurate global attention prediction. Extensive experiments on
three large-scale RGB-T tracking benchmark datasets validated the effectiveness
of our proposed algorithm. The project page of this paper is available at
https://sites.google.com/view/mfgrgbttrack/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiujun Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaowei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frame-wise Cross-modal Matching for Video Moment Retrieval. (arXiv:2009.10434v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10434</id>
        <link href="http://arxiv.org/abs/2009.10434"/>
        <updated>2021-07-23T02:00:29.829Z</updated>
        <summary type="html"><![CDATA[Video moment retrieval targets at retrieving a moment in a video for a given
language query. The challenges of this task include 1) the requirement of
localizing the relevant moment in an untrimmed video, and 2) bridging the
semantic gap between textual query and video contents. To tackle those
problems, early approaches adopt the sliding window or uniform sampling to
collect video clips first and then match each clip with the query. Obviously,
these strategies are time-consuming and often lead to unsatisfied accuracy in
localization due to the unpredictable length of the golden moment. To avoid the
limitations, researchers recently attempt to directly predict the relevant
moment boundaries without the requirement to generate video clips first. One
mainstream approach is to generate a multimodal feature vector for the target
query and video frames (e.g., concatenation) and then use a regression approach
upon the multimodal feature vector for boundary detection. Although some
progress has been achieved by this approach, we argue that those methods have
not well captured the cross-modal interactions between the query and video
frames.

In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)
model which predicts the temporal boundaries based on an interaction modeling.
In addition, an attention module is introduced to assign higher weights to
query words with richer semantic cues, which are considered to be more
important for finding relevant video contents. Another contribution is that we
propose an additional predictor to utilize the internal frames in the model
training to improve the localization accuracy. Extensive experiments on two
datasets TACoS and Charades-STA demonstrate the superiority of our method over
several state-of-the-art methods. Ablation studies have been also conducted to
examine the effectiveness of different modules in our ACRM model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jihua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Member/0/1/0/all/0/1"&gt;Member&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1"&gt;IEEE&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyong Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10300</id>
        <link href="http://arxiv.org/abs/2107.10300"/>
        <updated>2021-07-23T02:00:29.795Z</updated>
        <summary type="html"><![CDATA[Causality knowledge is vital to building robust AI systems. Deep learning
models often perform poorly on tasks that require causal reasoning, which is
often derived using some form of commonsense knowledge not immediately
available in the input but implicitly inferred by humans. Prior work has
unraveled spurious observational biases that models fall prey to in the absence
of causality. While language representation models preserve contextual
knowledge within learned embeddings, they do not factor in causal relationships
during training. By blending causal relationships with the input features to an
existing model that performs visual cognition tasks (such as scene
understanding, video captioning, video question-answering, etc.), better
performance can be achieved owing to the insight causal relationships bring
about. Recently, several models have been proposed that have tackled the task
of mining causal data from either the visual or textual modality. However,
there does not exist widespread research that mines causal relationships by
juxtaposing the visual and language modalities. While images offer a rich and
easy-to-process resource for us to mine causality knowledge from, videos are
denser and consist of naturally time-ordered events. Also, textual information
offers details that could be implicit in videos. We propose iReason, a
framework that infers visual-semantic commonsense knowledge using both videos
and natural language captions. Furthermore, iReason's architecture integrates a
causal rationalization module to aid the process of interpretability, error
analysis and bias detection. We demonstrate the effectiveness of iReason using
a two-pronged comparative analysis with language representation learning models
(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1"&gt;Aman Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1"&gt;Vinija Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio Retrieval with Natural Language Queries. (arXiv:2105.02192v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02192</id>
        <link href="http://arxiv.org/abs/2105.02192"/>
        <updated>2021-07-23T02:00:29.761Z</updated>
        <summary type="html"><![CDATA[We consider the task of retrieving audio using free-form natural language
queries. To study this problem, which has received limited attention in the
existing literature, we introduce challenging new benchmarks for text-based
audio retrieval using text annotations sourced from the Audiocaps and Clotho
datasets. We then employ these benchmarks to establish baselines for
cross-modal audio retrieval, where we demonstrate the benefits of pre-training
on diverse audio tasks. We hope that our benchmarks will inspire further
research into cross-modal text-based audio retrieval with free-form text
queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oncescu_A/0/1/0/all/0/1"&gt;Andreea-Maria Oncescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1"&gt;A. Sophia Koepke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1"&gt;Samuel Albanie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03592</id>
        <link href="http://arxiv.org/abs/2008.03592"/>
        <updated>2021-07-23T02:00:29.739Z</updated>
        <summary type="html"><![CDATA[Visual emotion expression plays an important role in audiovisual speech
communication. In this work, we propose a novel approach to rendering visual
emotion expression in speech-driven talking face generation. Specifically, we
design an end-to-end talking face generation system that takes a speech
utterance, a single face image, and a categorical emotion label as input to
render a talking face video synchronized with the speech and expressing the
conditioned emotion. Objective evaluation on image quality, audiovisual
synchronization, and visual emotion expression shows that the proposed system
outperforms a state-of-the-art baseline system. Subjective evaluation of visual
emotion expression and video realness also demonstrates the superiority of the
proposed system. Furthermore, we conduct a human emotion recognition pilot
study using generated videos with mismatched emotions among the audio and
visual modalities. Results show that humans respond to the visual modality more
significantly than the audio modality on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1"&gt;Sefik Emre Eskimez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;You Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Zhiyao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network-based Trajectory Topic Interaction Map for Text Mining of COVID-19 Biomedical Literature. (arXiv:2106.07374v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07374</id>
        <link href="http://arxiv.org/abs/2106.07374"/>
        <updated>2021-07-23T02:00:29.433Z</updated>
        <summary type="html"><![CDATA[Since the emergence of the worldwide pandemic of COVID-19, relevant research
has been published at a dazzling pace, which makes it hard to follow the
research in this area without dedicated efforts. It is practically impossible
to implement this task manually due to the high volume of the relevant
literature. Text mining has been considered to be a powerful approach to
address this challenge, especially the topic modeling, a well-known
unsupervised method that aims to reveal latent topics from the literature.
However, in spite of its potential utility, the results generated from this
approach are often investigated manually. Hence, its application to the
COVID-19 literature is not straightforward and expert knowledge is needed to
make meaningful interpretations. In order to address these challenges, we
propose a novel analytical framework for estimating topic interactions and
effective visualization for topic interpretation. Here we assumed that topics
constituting a paper can be positioned on an interaction map, which belongs to
a high-dimensional Euclidean space. Based on this assumption, after summarizing
topics with their topic-word distributions using the biterm topic model, we
mapped these latent topics on networks to visualize relationships among the
topics. Moreover, in the proposed approach, the change of relationships among
topics can be traced using a trajectory plot generated with different levels of
word richness. These results together provide deeply mined and intuitive
representation of relationships among topics related to a specific research
area. The application of this proposed framework to the PubMed literature shows
that our approach facilitates understanding of the topics constituting the
COVID-19 knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1"&gt;Yeseul Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_D/0/1/0/all/0/1"&gt;Dongjun Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jina Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_I/0/1/0/all/0/1"&gt;Ick Hoon Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Search: Making Domain Experts out of Dilettantes. (arXiv:2105.02274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02274</id>
        <link href="http://arxiv.org/abs/2105.02274"/>
        <updated>2021-07-23T02:00:29.401Z</updated>
        <summary type="html"><![CDATA[When experiencing an information need, users want to engage with a domain
expert, but often turn to an information retrieval system, such as a search
engine, instead. Classical information retrieval systems do not answer
information needs directly, but instead provide references to (hopefully
authoritative) answers. Successful question answering systems offer a limited
corpus created on-demand by human experts, which is neither timely nor
scalable. Pre-trained language models, by contrast, are capable of directly
generating prose that may be responsive to an information need, but at present
they are dilettantes rather than domain experts -- they do not have a true
understanding of the world, they are prone to hallucinating, and crucially they
are incapable of justifying their utterances by referring to supporting
documents in the corpus they were trained over. This paper examines how ideas
from classical information retrieval and pre-trained language models can be
synthesized and evolved into systems that truly deliver on the promise of
domain expert advice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1"&gt;Marc Najork&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framing Effects on Strategic Information Design under Receiver Distrust and Unknown State. (arXiv:2005.05516v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05516</id>
        <link href="http://arxiv.org/abs/2005.05516"/>
        <updated>2021-07-23T02:00:29.373Z</updated>
        <summary type="html"><![CDATA[Strategic information design is a framework where a sender designs
information strategically to steer its receiver's decision towards a desired
choice. Traditionally, such frameworks have always assumed that the sender and
the receiver comprehends the state of the choice environment, and that the
receiver always trusts the sender's signal. This paper deviates from these
assumptions and re-investigates strategic information design in the presence of
distrustful receiver and when both sender and receiver cannot
observe/comprehend the environment state space. Specifically, we assume that
both sender and receiver has access to non-identical beliefs about choice
rewards (with sender's belief being more accurate), but not the environment
state that determines these rewards. Furthermore, given that the receiver does
not trust the sender, we also assume that the receiver updates its prior in a
non-Bayesian manner. We evaluate the Stackelberg equilibrium and investigate
effects of information framing (i.e. send complete signal, or just expected
value of the signal) on the equilibrium. Furthermore, we also investigate trust
dynamics at the receiver, under the assumption that the receiver minimizes
regret in hindsight. Simulation results are presented to illustrate signaling
effects and trust dynamics in strategic information design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Doris E. M. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadendla_V/0/1/0/all/0/1"&gt;Venkata Sriram Siddhardh Nadendla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (arXiv:2107.10457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10457</id>
        <link href="http://arxiv.org/abs/2107.10457"/>
        <updated>2021-07-23T02:00:29.336Z</updated>
        <summary type="html"><![CDATA[To explore the robustness of recommender systems, researchers have proposed
various shilling attack models and analyzed their adverse effects. Primitive
attacks are highly feasible but less effective due to simplistic handcrafted
rules, while upgraded attacks are more powerful but costly and difficult to
deploy because they require more knowledge from recommendations. In this paper,
we explore a novel shilling attack called Graph cOnvolution-based generative
shilling ATtack (GOAT) to balance the attacks' feasibility and effectiveness.
GOAT adopts the primitive attacks' paradigm that assigns items for fake users
by sampling and the upgraded attacks' paradigm that generates fake ratings by a
deep learning-based model. It deploys a generative adversarial network (GAN)
that learns the real rating distribution to generate fake ratings.
Additionally, the generator combines a tailored graph convolution structure
that leverages the correlations between co-rated items to smoothen the fake
ratings and enhance their authenticity. The extensive experiments on two public
datasets evaluate GOAT's performance from multiple perspectives. Our study of
the GOAT demonstrates technical feasibility for building a more powerful and
intelligent attack model with a much-reduced cost, enables analysis the threat
of such an attack and guides for investigating necessary prevention measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Min Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Junliang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zongwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kecheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wange_X/0/1/0/all/0/1"&gt;Xu Wange&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[kNet: A Deep kNN Network To Handle Label Noise. (arXiv:2107.09735v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09735</id>
        <link href="http://arxiv.org/abs/2107.09735"/>
        <updated>2021-07-22T02:03:13.021Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks require large amounts of labeled data for their
training. Collecting this data at scale inevitably causes label noise.Hence,the
need to develop learning algorithms that are robust to label noise. In recent
years, k Nearest Neighbors (kNN) emerged as a viable solution to this problem.
Despite its success, kNN is not without its problems. Mainly, it requires a
huge memory footprint to store all the training samples and it needs an
advanced data structure to allow for fast retrieval of the relevant examples,
given a query sample. We propose a neural network, termed kNet, that learns to
perform kNN. Once trained, we no longer need to store the training data, and
processing a query sample is a simple matter of inference. To use kNet, we
first train a preliminary network on the data set, and then train kNet on the
penultimate layer of the preliminary network.We find that kNet gives a smooth
approximation of kNN,and cannot handle the sharp label changes between samples
that kNN can exhibit. This indicates that currently kNet is best suited to
approximate kNN with a fairly large k. Experiments on two data sets show that
this is the regime in which kNN works best,and can therefore be replaced by
kNet.In practice, kNet consistently improve the results of all preliminary
networks, in all label noise regimes, by up to 3%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mizrahi_I/0/1/0/all/0/1"&gt;Itzik Mizrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1"&gt;Shai Avidan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized Evolutionary Population-Based Training. (arXiv:2002.04225v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04225</id>
        <link href="http://arxiv.org/abs/2002.04225"/>
        <updated>2021-07-22T02:03:13.014Z</updated>
        <summary type="html"><![CDATA[Metalearning of deep neural network (DNN) architectures and hyperparameters
has become an increasingly important area of research. At the same time,
network regularization has been recognized as a crucial dimension to effective
training of DNNs. However, the role of metalearning in establishing effective
regularization has not yet been fully explored. There is recent evidence that
loss-function optimization could play this role, however it is computationally
impractical as an outer loop to full training. This paper presents an algorithm
called Evolutionary Population-Based Training (EPBT) that interleaves the
training of a DNN's weights with the metalearning of loss functions. They are
parameterized using multivariate Taylor expansions that EPBT can directly
optimize. Such simultaneous adaptation of weights and loss functions can be
deceptive, and therefore EPBT uses a quality-diversity heuristic called Novelty
Pulsation as well as knowledge distillation to prevent overfitting during
training. On the CIFAR-10 and SVHN image classification benchmarks, EPBT
results in faster, more accurate learning. The discovered hyperparameters adapt
to the training process and serve to regularize the learning task by
discouraging overfitting to the labels. EPBT thus demonstrates a practical
instantiation of regularization metalearning based on simultaneous training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jason Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_S/0/1/0/all/0/1"&gt;Santiago Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahrzad_H/0/1/0/all/0/1"&gt;Hormoz Shahrzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1"&gt;Risto Miikkulainen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIME: A new graphical methodology for interpretable model-agnostic explanations. (arXiv:2107.09927v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09927</id>
        <link href="http://arxiv.org/abs/2107.09927"/>
        <updated>2021-07-22T02:03:12.993Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence (XAI) is an emerging new domain in which
a set of processes and tools allow humans to better comprehend the decisions
generated by black box models. However, most of the available XAI tools are
often limited to simple explanations mainly quantifying the impact of
individual features to the models' output. Therefore, human users are not able
to understand how the features are related to each other to make predictions,
whereas the inner workings of the trained models remain hidden. This paper
contributes to the development of a novel graphical explainability tool that
not only indicates the significant features of the model but also reveals the
conditional relationships between features and the inference capturing both the
direct and indirect impact of features to the models' decision. The proposed
XAI methodology, termed as gLIME, provides graphical model-agnostic
explanations either at the global (for the entire dataset) or the local scale
(for specific data points). It relies on a combination of local interpretable
model-agnostic explanations (LIME) with graphical least absolute shrinkage and
selection operator (GLASSO) producing undirected Gaussian graphical models.
Regularization is adopted to shrink small partial correlation coefficients to
zero providing sparser and more interpretable graphical explanations. Two
well-known classification datasets (BIOPSY and OAI) were selected to confirm
the superiority of gLIME over LIME in terms of both robustness and consistency
over multiple permutations. Specifically, gLIME accomplished increased
stability over the two datasets with respect to features' importance (76%-96%
compared to 52%-77% using LIME). gLIME demonstrates a unique potential to
extend the functionality of the current state-of-the-art in XAI by providing
informative graphically given explanations that could unlock black boxes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dikopoulou_Z/0/1/0/all/0/1"&gt;Zoumpolia Dikopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustakidis_S/0/1/0/all/0/1"&gt;Serafeim Moustakidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_P/0/1/0/all/0/1"&gt;Patrik Karlsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedHealth 2: Weighted Federated Transfer Learning via Batch Normalization for Personalized Healthcare. (arXiv:2106.01009v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01009</id>
        <link href="http://arxiv.org/abs/2106.01009"/>
        <updated>2021-07-22T02:03:12.987Z</updated>
        <summary type="html"><![CDATA[The success of machine learning applications often needs a large quantity of
data. Recently, federated learning (FL) is attracting increasing attention due
to the demand for data privacy and security, especially in the medical field.
However, the performance of existing FL approaches often deteriorates when
there exist domain shifts among clients, and few previous works focus on
personalization in healthcare. In this article, we propose FedHealth 2, an
extension of FedHealth \cite{chen2020fedhealth} to tackle domain shifts and get
personalized models for local clients. FedHealth 2 obtains the client
similarities via a pretrained model, and then it averages all weighted models
with preserving local batch normalization. Wearable activity recognition and
COVID-19 auxiliary diagnosis experiments have evaluated that FedHealth 2 can
achieve better accuracy (10%+ improvement for activity recognition) and
personalized healthcare without compromising privacy and security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xin Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Inducing Points Selection For Gaussian Processes. (arXiv:2107.10066v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10066</id>
        <link href="http://arxiv.org/abs/2107.10066"/>
        <updated>2021-07-22T02:03:12.980Z</updated>
        <summary type="html"><![CDATA[Gaussian Processes (\textbf{GPs}) are flexible non-parametric models with
strong probabilistic interpretation. While being a standard choice for
performing inference on time series, GPs have few techniques to work in a
streaming setting. \cite{bui2017streaming} developed an efficient variational
approach to train online GPs by using sparsity techniques: The whole set of
observations is approximated by a smaller set of inducing points (\textbf{IPs})
and moved around with new data. Both the number and the locations of the IPs
will affect greatly the performance of the algorithm. In addition to optimizing
their locations, we propose to adaptively add new points, based on the
properties of the GP and the structure of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Galy_Fajou_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Galy-Fajou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1"&gt;Manfred Opper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Annealed Importance Sampling and the Perils of Gradient Noise. (arXiv:2107.10211v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10211</id>
        <link href="http://arxiv.org/abs/2107.10211"/>
        <updated>2021-07-22T02:03:12.972Z</updated>
        <summary type="html"><![CDATA[Annealed importance sampling (AIS) and related algorithms are highly
effective tools for marginal likelihood estimation, but are not fully
differentiable due to the use of Metropolis-Hastings (MH) correction steps.
Differentiability is a desirable property as it would admit the possibility of
optimizing marginal likelihood as an objective using gradient-based methods. To
this end, we propose a differentiable AIS algorithm by abandoning MH steps,
which further unlocks mini-batch computation. We provide a detailed convergence
analysis for Bayesian linear regression which goes beyond previous analyses by
explicitly accounting for non-perfect transitions. Using this analysis, we
prove that our algorithm is consistent in the full-batch setting and provide a
sublinear convergence rate. However, we show that the algorithm is inconsistent
when mini-batch gradients are used due to a fundamental incompatibility between
the goals of last-iterate convergence to the posterior and elimination of the
pathwise stochastic error. This result is in stark contrast to our experience
with stochastic optimization and stochastic gradient Langevin dynamics, where
the effects of gradient noise can be washed out by taking more steps of a
smaller size. Our negative result relies crucially on our explicit
consideration of convergence to the stationary distribution, and it helps
explain the difficulty of developing practically effective AIS-like algorithms
that exploit mini-batch gradients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guodong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsu_K/0/1/0/all/0/1"&gt;Kyle Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grosse_R/0/1/0/all/0/1"&gt;Roger Grosse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Neural Tangent Kernel of Deep Networks with Orthogonal Initialization. (arXiv:2004.05867v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05867</id>
        <link href="http://arxiv.org/abs/2004.05867"/>
        <updated>2021-07-22T02:03:12.966Z</updated>
        <summary type="html"><![CDATA[The prevailing thinking is that orthogonal weights are crucial to enforcing
dynamical isometry and speeding up training. The increase in learning speed
that results from orthogonal initialization in linear networks has been
well-proven. However, while the same is believed to also hold for nonlinear
networks when the dynamical isometry condition is satisfied, the training
dynamics behind this contention have not been thoroughly explored. In this
work, we study the dynamics of ultra-wide networks across a range of
architectures, including Fully Connected Networks (FCNs) and Convolutional
Neural Networks (CNNs) with orthogonal initialization via neural tangent kernel
(NTK). Through a series of propositions and lemmas, we prove that two NTKs, one
corresponding to Gaussian weights and one to orthogonal weights, are equal when
the network width is infinite. Further, during training, the NTK of an
orthogonally-initialized infinite-width network should theoretically remain
constant. This suggests that the orthogonal initialization cannot speed up
training in the NTK (lazy training) regime, contrary to the prevailing
thoughts. In order to explore under what circumstances can orthogonality
accelerate training, we conduct a thorough empirical investigation outside the
NTK regime. We find that when the hyper-parameters are set to achieve a linear
regime in nonlinear activation, orthogonal initialization can improve the
learning speed with a large learning rate or large depth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Weitao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Richard Yi Da Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Signal-to-Noise Ratio Issues in Variational Inference for Deep Gaussian Processes. (arXiv:2011.00515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00515</id>
        <link href="http://arxiv.org/abs/2011.00515"/>
        <updated>2021-07-22T02:03:12.958Z</updated>
        <summary type="html"><![CDATA[We show that the gradient estimates used in training Deep Gaussian Processes
(DGPs) with importance-weighted variational inference are susceptible to
signal-to-noise ratio (SNR) issues. Specifically, we show both theoretically
and via an extensive empirical evaluation that the SNR of the gradient
estimates for the latent variable's variational parameters decreases as the
number of importance samples increases. As a result, these gradient estimates
degrade to pure noise if the number of importance samples is too large. To
address this pathology, we show how doubly reparameterized gradient estimators,
originally proposed for training variational autoencoders, can be adapted to
the DGP setting and that the resultant estimators completely remedy the SNR
issue, thereby providing more reliable training. Finally, we demonstrate that
our fix can lead to consistent improvements in the predictive performance of
DGP models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rudner_T/0/1/0/all/0/1"&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Key_O/0/1/0/all/0/1"&gt;Oscar Key&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting diffusion score matching using normalizing flow. (arXiv:2107.10072v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10072</id>
        <link href="http://arxiv.org/abs/2107.10072"/>
        <updated>2021-07-22T02:03:12.952Z</updated>
        <summary type="html"><![CDATA[Scoring matching (SM), and its related counterpart, Stein discrepancy (SD)
have achieved great success in model training and evaluations. However, recent
research shows their limitations when dealing with certain types of
distributions. One possible fix is incorporating the original score matching
(or Stein discrepancy) with a diffusion matrix, which is called diffusion score
matching (DSM) (or diffusion Stein discrepancy (DSD)). However, the lack of
interpretation of the diffusion limits its usage within simple distributions
and manually chosen matrix. In this work, we plan to fill this gap by
interpreting the diffusion matrix using normalizing flows. Specifically, we
theoretically prove that DSM (or DSD) is equivalent to the original score
matching (or Stein discrepancy) evaluated in the transformed space defined by
the normalizing flow, where the diffusion matrix is the inverse of the flow's
Jacobian matrix. In addition, we also build its connection to Riemannian
manifolds and further extend it to continuous flows, where the change of DSM is
characterized by an ODE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1"&gt;Wenbo Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingzhen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for temporal data representation in electronic health records: A systematic review of challenges and methodologies. (arXiv:2107.09951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09951</id>
        <link href="http://arxiv.org/abs/2107.09951"/>
        <updated>2021-07-22T02:03:12.945Z</updated>
        <summary type="html"><![CDATA[Objective: Temporal electronic health records (EHRs) can be a wealth of
information for secondary uses, such as clinical events prediction or chronic
disease management. However, challenges exist for temporal data representation.
We therefore sought to identify these challenges and evaluate novel
methodologies for addressing them through a systematic examination of deep
learning solutions.

Methods: We searched five databases (PubMed, EMBASE, the Institute of
Electrical and Electronics Engineers [IEEE] Xplore Digital Library, the
Association for Computing Machinery [ACM] digital library, and Web of Science)
complemented with hand-searching in several prestigious computer science
conference proceedings. We sought articles that reported deep learning
methodologies on temporal data representation in structured EHR data from
January 1, 2010, to August 30, 2020. We summarized and analyzed the selected
articles from three perspectives: nature of time series, methodology, and model
implementation.

Results: We included 98 articles related to temporal data representation
using deep learning. Four major challenges were identified, including data
irregularity, data heterogeneity, data sparsity, and model opacity. We then
studied how deep learning techniques were applied to address these challenges.
Finally, we discuss some open challenges arising from deep learning.

Conclusion: Temporal EHR data present several major challenges for clinical
prediction modeling and data utilization. To some extent, current deep learning
solutions can address these challenges. Future studies can consider designing
comprehensive and integrated solutions. Moreover, researchers should
incorporate additional clinical domain knowledge into study designs and enhance
the interpretability of the model to facilitate its implementation in clinical
practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1"&gt;Feng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Han Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yilin Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1"&gt;Marcus Eng Hock Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mengling Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1"&gt;Bibhas Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI Enabled Inspection of Business Process Prediction Models. (arXiv:2107.09767v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.09767</id>
        <link href="http://arxiv.org/abs/2107.09767"/>
        <updated>2021-07-22T02:03:12.924Z</updated>
        <summary type="html"><![CDATA[Modern data analytics underpinned by machine learning techniques has become a
key enabler to the automation of data-led decision making. As an important
branch of state-of-the-art data analytics, business process predictions are
also faced with a challenge in regard to the lack of explanation to the
reasoning and decision by the underlying `black-box' prediction models. With
the development of interpretable machine learning techniques, explanations can
be generated for a black-box model, making it possible for (human) users to
access the reasoning behind machine learned predictions. In this paper, we aim
to present an approach that allows us to use model explanations to investigate
certain reasoning applied by machine learned predictions and detect potential
issues with the underlying methods thus enhancing trust in business process
prediction models. A novel contribution of our approach is the proposal of
model inspection that leverages both the explanations generated by
interpretable machine learning mechanisms and the contextual or domain
knowledge extracted from event logs that record historical process execution.
Findings drawn from this work are expected to serve as a key input to
developing model reliability metrics and evaluation in the context of business
process predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Chun Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindhgatta_R/0/1/0/all/0/1"&gt;Renuka Sindhgatta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1"&gt;Catarina Moreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Factor Graph-based approach to vehicle sideslip angle estimation. (arXiv:2107.09815v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.09815</id>
        <link href="http://arxiv.org/abs/2107.09815"/>
        <updated>2021-07-22T02:03:12.906Z</updated>
        <summary type="html"><![CDATA[Sideslip angle is an important variable for understanding and monitoring
vehicle dynamics but it lacks an inexpensive method for direct measurement.
Therefore, it is typically estimated from inertial and other proprioceptive
sensors onboard using filtering methods from the family of the Kalman Filter.
As a novel alternative, this work proposes modelling the problem directly as a
graphical model (factor graph), which can then be optimized using a variety of
methods, such as whole dataset batch optimization for offline processing or
fixed-lag smoother for on-line operation. Experimental results on real vehicle
datasets validate the proposal with a good agreement between estimated and
actual sideslip angle, showing similar performance than the state-of-the-art
with a great potential for future extensions due to the flexible mathematical
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leanza_A/0/1/0/all/0/1"&gt;Antonio Leanza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reina_G/0/1/0/all/0/1"&gt;Giulio Reina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanco_Claraco_J/0/1/0/all/0/1"&gt;Jose-Luis Blanco-Claraco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Measurement Classification with Qudits. (arXiv:2107.09781v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.09781</id>
        <link href="http://arxiv.org/abs/2107.09781"/>
        <updated>2021-07-22T02:03:12.887Z</updated>
        <summary type="html"><![CDATA[This paper presents a hybrid classical-quantum program for density estimation
and supervised classification. The program is implemented as a quantum circuit
in a high-dimensional quantum computer simulator. We show that the proposed
quantum protocols allow to estimate probability density functions and to make
predictions in a supervised learning manner. This model can be generalized to
find expected values of density matrices in high-dimensional quantum computers.
Experiments on various data sets are presented. Results show that the proposed
method is a viable strategy to implement supervised classification and density
estimation in a high-dimensional quantum computer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Useche_D/0/1/0/all/0/1"&gt;Diego H. Useche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Giraldo_Carvajal_A/0/1/0/all/0/1"&gt;Andres Giraldo-Carvajal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zuluaga_Bucheli_H/0/1/0/all/0/1"&gt;Hernan M. Zuluaga-Bucheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jaramillo_Villegas_J/0/1/0/all/0/1"&gt;Jose A. Jaramillo-Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Fabio A. Gonz&amp;#xe1;lez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning using Smart Contracts on Blockchains, based on Reward Driven Approach. (arXiv:2107.10243v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.10243</id>
        <link href="http://arxiv.org/abs/2107.10243"/>
        <updated>2021-07-22T02:03:12.861Z</updated>
        <summary type="html"><![CDATA[Over the recent years, Federated machine learning continues to gain interest
and momentum where there is a need to draw insights from data while preserving
the data provider's privacy. However, one among other existing challenges in
the adoption of federated learning has been the lack of fair, transparent and
universally agreed incentivization schemes for rewarding the federated learning
contributors. Smart contracts on a blockchain network provide transparent,
immutable and independently verifiable proofs by all participants of the
network. We leverage this open and transparent nature of smart contracts on a
blockchain to define incentivization rules for the contributors, which is based
on a novel scalar quantity - federated contribution. Such a smart contract
based reward-driven model has the potential to revolutionize the federated
learning adoption in enterprises. Our contribution is two-fold: first is to
show how smart contract based blockchain can be a very natural communication
channel for federated learning. Second, leveraging this infrastructure, we can
show how an intuitive measure of each agents' contribution can be built and
integrated with the life cycle of the training and reward process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Behera_M/0/1/0/all/0/1"&gt;Monik Raj Behera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sudhir Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1"&gt;Suresh Shetty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CGANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10060</id>
        <link href="http://arxiv.org/abs/2107.10060"/>
        <updated>2021-07-22T02:03:12.853Z</updated>
        <summary type="html"><![CDATA[Conditional generative models aim to learn the underlying joint distribution
of data and labels, and thus realize conditional generation. Among them,
auxiliary classifier generative adversarial networks (AC-GAN) have been widely
used, but suffer from the issue of low intra-class diversity on generated
samples. In this paper, we point out that the fundamental reason is that the
classifier of AC-GAN is generator-agnostic, and thus cannot provide informative
guidance to the generator to approximate the target joint distribution, leading
to a minimization of conditional entropy that decreases the intra-class
diversity. Based on this finding, we propose novel cGANs with auxiliary
discriminative classifier (ADC-GAN) to address the issue of AC-GAN.
Specifically, the auxiliary discriminative classifier becomes generator-aware
by distinguishing between the real and fake data while recognizing their
labels. We then optimize the generator based on the auxiliary classifier along
with the original discriminator to match the joint and marginal distributions
of the generated samples with those of the real samples. We provide theoretical
analysis and empirical evidence on synthetic and real-world datasets to
demonstrate the superiority of the proposed ADC-GAN compared to competitive
cGANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Liang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making. (arXiv:2101.05303v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05303</id>
        <link href="http://arxiv.org/abs/2101.05303"/>
        <updated>2021-07-22T02:03:12.846Z</updated>
        <summary type="html"><![CDATA[Although AI holds promise for improving human decision making in societally
critical domains, it remains an open question how human-AI teams can reliably
outperform AI alone and human alone in challenging prediction tasks (also known
as complementary performance). We explore two directions to understand the gaps
in achieving complementary performance. First, we argue that the typical
experimental setup limits the potential of human-AI teams. To account for lower
AI performance out-of-distribution than in-distribution because of distribution
shift, we design experiments with different distribution types and investigate
human performance for both in-distribution and out-of-distribution examples.
Second, we develop novel interfaces to support interactive explanations so that
humans can actively engage with AI assistance. Using virtual pilot studies and
large-scale randomized experiments across three tasks, we demonstrate a clear
difference between in-distribution and out-of-distribution, and observe mixed
results for interactive explanations: while interactive explanations improve
human perception of AI assistance's usefulness, they may reinforce human biases
and lead to limited performance improvement. Overall, our work points out
critical challenges and future directions towards enhancing human performance
with AI assistance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Han Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1"&gt;Vivian Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chenhao Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Graph Convolutional Networks: A Closer Look. (arXiv:2107.10015v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10015</id>
        <link href="http://arxiv.org/abs/2107.10015"/>
        <updated>2021-07-22T02:03:12.839Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a reproduction of the Relational Graph
Convolutional Network (RGCN). Using our reproduction, we explain the intuition
behind the model. Our reproduction results empirically validate the correctness
of our implementations using benchmark Knowledge Graph datasets on node
classification and link prediction tasks. Our explanation provides a friendly
understanding of the different components of the RGCN for both users and
researchers extending the RGCN approach. Furthermore, we introduce two new
configurations of the RGCN that are more parameter efficient. The code and
datasets are available at https://github.com/thiviyanT/torch-rgcn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1"&gt;Thiviyan Thanapalasingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berkel_L/0/1/0/all/0/1"&gt;Lucas van Berkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloem_P/0/1/0/all/0/1"&gt;Peter Bloem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1"&gt;Paul Groth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative Averaging in the Quest for Best Test Error. (arXiv:2003.01247v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01247</id>
        <link href="http://arxiv.org/abs/2003.01247"/>
        <updated>2021-07-22T02:03:12.832Z</updated>
        <summary type="html"><![CDATA[We analyse and explain the increased generalisation performance
\latestEdits{of} Iterate Averaging using a Gaussian Process perturbation model
between the true and batch risk surface on the high dimensional quadratic. %
Based on our theoretical results We derive three phenomena \latestEdits{from
our theoretical results:} (1) The importance of combining iterate averaging
with large learning rates and regularisation for improved regularisation (2)
Justification for less frequent averaging. (3) That we expect adaptive gradient
methods to work equally well or better with iterate averaging than their non
adaptive counterparts. Inspired by these results\latestEdits{, together with}
empirical investigations of the importance of appropriate regularisation for
the solution diversity of the iterates, we propose two adaptive algorithms with
iterate averaging. \latestEdits{These} give significantly better results than
SGD, require less tuning and do not require early stopping or validation set
monitoring. We showcase the efficacy of our approach on the CIFAR-10/100,
ImageNet and Penn Treebank datasets on a variety of modern and classical
network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1"&gt;Diego Granziol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xingchen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Albanie_S/0/1/0/all/0/1"&gt;Samuel Albanie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Memorization Properties of Contrastive Learning. (arXiv:2107.10143v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10143</id>
        <link href="http://arxiv.org/abs/2107.10143"/>
        <updated>2021-07-22T02:03:12.825Z</updated>
        <summary type="html"><![CDATA[Memorization studies of deep neural networks (DNNs) help to understand what
patterns and how do DNNs learn, and motivate improvements to DNN training
approaches. In this work, we investigate the memorization properties of SimCLR,
a widely used contrastive self-supervised learning approach, and compare them
to the memorization of supervised learning and random labels training. We find
that both training objects and augmentations may have different complexity in
the sense of how SimCLR learns them. Moreover, we show that SimCLR is similar
to random labels training in terms of the distribution of training objects
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadrtdinov_I/0/1/0/all/0/1"&gt;Ildus Sadrtdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1"&gt;Ekaterina Lobacheva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S4T: Source-free domain adaptation for semantic segmentation via self-supervised selective self-training. (arXiv:2107.10140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10140</id>
        <link href="http://arxiv.org/abs/2107.10140"/>
        <updated>2021-07-22T02:03:12.801Z</updated>
        <summary type="html"><![CDATA[Most modern approaches for domain adaptive semantic segmentation rely on
continued access to source data during adaptation, which may be infeasible due
to computational or privacy constraints. We focus on source-free domain
adaptation for semantic segmentation, wherein a source model must adapt itself
to a new target domain given only unlabeled target data. We propose
Self-Supervised Selective Self-Training (S4T), a source-free adaptation
algorithm that first uses the model's pixel-level predictive consistency across
diverse views of each target image along with model confidence to classify
pixel predictions as either reliable or unreliable. Next, the model is
self-trained, using predicted pseudolabels for reliable predictions and
pseudolabels inferred via a selective interpolation strategy for unreliable
ones. S4T matches or improves upon the state-of-the-art in source-free
adaptation on 3 standard benchmarks for semantic segmentation within a single
epoch of adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1"&gt;Viraj Prabhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1"&gt;Shivam Khare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1"&gt;Deeksha Kartik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;Judy Hoffman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Answer-Set Programs for Reasoning about Counterfactual Interventions and Responsibility Scores for Classification. (arXiv:2107.10159v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.10159</id>
        <link href="http://arxiv.org/abs/2107.10159"/>
        <updated>2021-07-22T02:03:12.794Z</updated>
        <summary type="html"><![CDATA[We describe how answer-set programs can be used to declaratively specify
counterfactual interventions on entities under classification, and reason about
them. In particular, they can be used to define and compute responsibility
scores as attribution-based explanations for outcomes from classification
models. The approach allows for the inclusion of domain knowledge and supports
query answering. A detailed example with a naive-Bayes classifier is presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1"&gt;Leopoldo Bertossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reyes_G/0/1/0/all/0/1"&gt;Gabriela Reyes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design of Experiments for Stochastic Contextual Linear Bandits. (arXiv:2107.09912v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09912</id>
        <link href="http://arxiv.org/abs/2107.09912"/>
        <updated>2021-07-22T02:03:12.787Z</updated>
        <summary type="html"><![CDATA[In the stochastic linear contextual bandit setting there exist several
minimax procedures for exploration with policies that are reactive to the data
being acquired. In practice, there can be a significant engineering overhead to
deploy these algorithms, especially when the dataset is collected in a
distributed fashion or when a human in the loop is needed to implement a
different policy. Exploring with a single non-reactive policy is beneficial in
such cases. Assuming some batch contexts are available, we design a single
stochastic policy to collect a good dataset from which a near-optimal policy
can be extracted. We present a theoretical analysis as well as numerical
experiments on both synthetic and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1"&gt;Andrea Zanette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1"&gt;Kefan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jonathan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1"&gt;Emma Brunskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-agent Reinforcement Learning Improvement in a Dynamic Environment Using Knowledge Transfer. (arXiv:2107.09807v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2107.09807</id>
        <link href="http://arxiv.org/abs/2107.09807"/>
        <updated>2021-07-22T02:03:12.780Z</updated>
        <summary type="html"><![CDATA[Cooperative multi-agent systems are being widely used in different domains.
Interaction among agents would bring benefits, including reducing operating
costs, high scalability, and facilitating parallel processing. These systems
are also a good option for handling large-scale, unknown, and dynamic
environments. However, learning in these environments has become a very
important challenge in various applications. These challenges include the
effect of search space size on learning time, inefficient cooperation among
agents, and the lack of proper coordination among agents' decisions. Moreover,
reinforcement learning algorithms may suffer from long convergence time in
these problems. In this paper, a communication framework using knowledge
transfer concepts is introduced to address such challenges in the herding
problem with large state space. To handle the problems of convergence,
knowledge transfer has been utilized that can significantly increase the
efficiency of reinforcement learning algorithms. Coordination between the
agents is carried out through a head agent in each group of agents and a
coordinator agent respectively. The results demonstrate that this framework
could indeed enhance the speed of learning and reduce convergence time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavimoghaddama_M/0/1/0/all/0/1"&gt;Mahnoosh Mahdavimoghaddama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjama_A/0/1/0/all/0/1"&gt;Amin Nikanjama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdoos_M/0/1/0/all/0/1"&gt;Monireh Abdoos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A variational approximate posterior for the deep Wishart process. (arXiv:2107.10125v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10125</id>
        <link href="http://arxiv.org/abs/2107.10125"/>
        <updated>2021-07-22T02:03:12.772Z</updated>
        <summary type="html"><![CDATA[Recent work introduced deep kernel processes as an entirely kernel-based
alternative to NNs (Aitchison et al. 2020). Deep kernel processes flexibly
learn good top-layer representations by alternately sampling the kernel from a
distribution over positive semi-definite matrices and performing nonlinear
transformations. A particular deep kernel process, the deep Wishart process
(DWP), is of particular interest because its prior is equivalent to deep
Gaussian process (DGP) priors. However, inference in DWPs has not yet been
possible due to the lack of sufficiently flexible distributions over positive
semi-definite matrices. Here, we give a novel approach to obtaining flexible
distributions over positive semi-definite matrices by generalising the Bartlett
decomposition of the Wishart probability density. We use this new distribution
to develop an approximate posterior for the DWP that includes dependency across
layers. We develop a doubly-stochastic inducing-point inference scheme for the
DWP and show experimentally that inference in the DWP gives improved
performance over doing inference in a DGP with the equivalent prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1"&gt;Sebastian W. Ober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Deep Learning Techniques for Face Recognition. (arXiv:2103.10492v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10492</id>
        <link href="http://arxiv.org/abs/2103.10492"/>
        <updated>2021-07-22T02:03:12.750Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have proposed many deep learning (DL) methods
for various tasks, and particularly face recognition (FR) made an enormous leap
using these techniques. Deep FR systems benefit from the hierarchical
architecture of the DL methods to learn discriminative face representation.
Therefore, DL techniques significantly improve state-of-the-art performance on
FR systems and encourage diverse and efficient real-world applications. In this
paper, we present a comprehensive analysis of various FR systems that leverage
the different types of DL techniques, and for the study, we summarize 168
recent contributions from this area. We discuss the papers related to different
algorithms, architectures, loss functions, activation functions, datasets,
challenges, improvement ideas, current and future trends of DL-based FR
systems. We provide a detailed discussion of various DL methods to understand
the current state-of-the-art, and then we discuss various activation and loss
functions for the methods. Additionally, we summarize different datasets used
widely for FR tasks and discuss challenges related to illumination, expression,
pose variations, and occlusion. Finally, we discuss improvement ideas, current
and future trends of FR tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Md. Tahmid Hasan Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1"&gt;Awal Ahmed Fime&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1"&gt;Delowar Sikder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1"&gt;Md. Akil Raihan Iftee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1"&gt;Jakaria Rabbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_rakhami_M/0/1/0/all/0/1"&gt;Mabrook S. Al-rakhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gumae_A/0/1/0/all/0/1"&gt;Abdu Gumae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1"&gt;Ovishake Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Mohtasim Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md. Nazrul Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delving Into Deep Walkers: A Convergence Analysis of Random-Walk-Based Vertex Embeddings. (arXiv:2107.10014v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10014</id>
        <link href="http://arxiv.org/abs/2107.10014"/>
        <updated>2021-07-22T02:03:12.743Z</updated>
        <summary type="html"><![CDATA[Graph vertex embeddings based on random walks have become increasingly
influential in recent years, showing good performance in several tasks as they
efficiently transform a graph into a more computationally digestible format
while preserving relevant information. However, the theoretical properties of
such algorithms, in particular the influence of hyperparameters and of the
graph structure on their convergence behaviour, have so far not been
well-understood. In this work, we provide a theoretical analysis for
random-walks based embeddings techniques. Firstly, we prove that, under some
weak assumptions, vertex embeddings derived from random walks do indeed
converge both in the single limit of the number of random walks $N \to \infty$
and in the double limit of both $N$ and the length of each random walk
$L\to\infty$. Secondly, we derive concentration bounds quantifying the converge
rate of the corpora for the single and double limits. Thirdly, we use these
results to derive a heuristic for choosing the hyperparameters $N$ and $L$. We
validate and illustrate the practical importance of our findings with a range
of numerical and visual experiments on several graphs drawn from real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kloepfer_D/0/1/0/all/0/1"&gt;Dominik Kloepfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aviles_Rivero_A/0/1/0/all/0/1"&gt;Angelica I. Aviles-Rivero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heydecker_D/0/1/0/all/0/1"&gt;Daniel Heydecker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10093</id>
        <link href="http://arxiv.org/abs/2107.10093"/>
        <updated>2021-07-22T02:03:12.735Z</updated>
        <summary type="html"><![CDATA[Randomized experiments can be susceptible to selection bias due to potential
non-compliance by the participants. While much of the existing work has studied
compliance as a static behavior, we propose a game-theoretic model to study
compliance as dynamic behavior that may change over time. In rounds, a social
planner interacts with a sequence of heterogeneous agents who arrive with their
unobserved private type that determines both their prior preferences across the
actions (e.g., control and treatment) and their baseline rewards without taking
any treatment. The planner provides each agent with a randomized recommendation
that may alter their beliefs and their action selection. We develop a novel
recommendation mechanism that views the planner's recommendation as a form of
instrumental variable (IV) that only affects an agents' action selection, but
not the observed rewards. We construct such IVs by carefully mapping the
history -- the interactions between the planner and the previous agents -- to a
random recommendation. Even though the initial agents may be completely
non-compliant, our mechanism can incentivize compliance over time, thereby
enabling the estimation of the treatment effect of each treatment, and
minimizing the cumulative regret of the planner whose goal is to identify the
optimal treatment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1"&gt;Daniel Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1"&gt;Logan Stapleton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1"&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Scalable Adversarial Training of Kernel SVM via Doubly Stochastic Gradients. (arXiv:2107.09937v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09937</id>
        <link href="http://arxiv.org/abs/2107.09937"/>
        <updated>2021-07-22T02:03:12.727Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks by generating examples which are almost indistinguishable
from natural examples, pose a serious threat to learning models. Defending
against adversarial attacks is a critical element for a reliable learning
system. Support vector machine (SVM) is a classical yet still important
learning algorithm even in the current deep learning era. Although a wide range
of researches have been done in recent years to improve the adversarial
robustness of learning models, but most of them are limited to deep neural
networks (DNNs) and the work for kernel SVM is still vacant. In this paper, we
aim at kernel SVM and propose adv-SVM to improve its adversarial robustness via
adversarial training, which has been demonstrated to be the most promising
defense techniques. To the best of our knowledge, this is the first work that
devotes to the fast and scalable adversarial training of kernel SVM.
Specifically, we first build connection of perturbations of samples between
original and kernel spaces, and then give a reduced and equivalent formulation
of adversarial training of kernel SVM based on the connection. Next, doubly
stochastic gradients (DSG) based on two unbiased stochastic approximations
(i.e., one is on training points and another is on random features) are applied
to update the solution of our objective function. Finally, we prove that our
algorithm optimized by DSG converges to the optimal solution at the rate of
O(1/t) under the constant and diminishing stepsizes. Comprehensive experimental
results show that our adversarial training algorithm enjoys robustness against
various attacks and meanwhile has the similar efficiency and scalability with
classical DSG algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huimin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhengmian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1"&gt;Bin Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using system context information to complement weakly labeled data. (arXiv:2107.10236v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10236</id>
        <link href="http://arxiv.org/abs/2107.10236"/>
        <updated>2021-07-22T02:03:12.719Z</updated>
        <summary type="html"><![CDATA[Real-world datasets collected with sensor networks often contain incomplete
and uncertain labels as well as artefacts arising from the system environment.
Complete and reliable labeling is often infeasible for large-scale and
long-term sensor network deployments due to the labor and time overhead,
limited availability of experts and missing ground truth. In addition, if the
machine learning method used for analysis is sensitive to certain features of a
deployment, labeling and learning needs to be repeated for every new
deployment. To address these challenges, we propose to make use of system
context information formalized in an information graph and embed it in the
learning process via contrastive learning. Based on real-world data we show
that this approach leads to an increased accuracy in case of weakly labeled
data and leads to an increased robustness and transferability of the classifier
to new sensor locations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_M/0/1/0/all/0/1"&gt;Matthias Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenner_M/0/1/0/all/0/1"&gt;Michaela Wenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hibert_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Hibert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walter_F/0/1/0/all/0/1"&gt;Fabian Walter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1"&gt;Lothar Thiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (arXiv:2107.10174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10174</id>
        <link href="http://arxiv.org/abs/2107.10174"/>
        <updated>2021-07-22T02:03:12.698Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have been paying increasing attention to the
threats brought by deep learning models to data security and privacy,
especially in the field of domain adaptation. Existing unsupervised domain
adaptation (UDA) methods can achieve promising performance without transferring
data from source domain to target domain. However, UDA with representation
alignment or self-supervised pseudo-labeling relies on the transferred source
models. In many data-critical scenarios, methods based on model transferring
may suffer from membership inference attacks and expose private data. In this
paper, we aim to overcome a challenging new setting where the source models are
only queryable but cannot be transferred to the target domain. We propose
Black-box Probe Domain Adaptation (BPDA), which adopts query mechanism to probe
and refine information from source model using third-party dataset. In order to
gain more informative query results, we further propose Distributionally
Adversarial Training (DAT) to align the distribution of third-party data with
that of target data. BPDA uses public third-party dataset and adversarial
examples based on DAT as the information carrier between source and target
domains, dispensing with transferring source data or model. Experimental
results on benchmarks of Digit-Five, Office-Caltech, Office-31, Office-Home,
and DomainNet demonstrate the feasibility of BPDA without model transferring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kunhong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yucheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yahong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized Classification-Aware Quantization. (arXiv:2107.09716v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09716</id>
        <link href="http://arxiv.org/abs/2107.09716"/>
        <updated>2021-07-22T02:03:12.691Z</updated>
        <summary type="html"><![CDATA[Traditionally, quantization is designed to minimize the reconstruction error
of a data source. When considering downstream classification tasks, other
measures of distortion can be of interest; such as the 0-1 classification loss.
Furthermore, it is desirable that the performance of these quantizers not
deteriorate once they are deployed into production, as relearning the scheme
online is not always possible. In this work, we present a class of algorithms
that learn distributed quantization schemes for binary classification tasks.
Our method performs well on unseen data, and is faster than previous methods
proportional to a quadratic term of the dataset size. It works by regularizing
the 0-1 loss with the reconstruction error. We present experiments on synthetic
mixture and bivariate Gaussian data and compare training, testing, and
generalization errors with a family of benchmark quantization schemes from the
literature. Our method is called Regularized Classification-Aware Quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1"&gt;Daniel Severo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domanovitz_E/0/1/0/all/0/1"&gt;Elad Domanovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1"&gt;Ashish Khisti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Large Neighborhood Search Algorithm for Mixed Integer Programs. (arXiv:2107.10201v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.10201</id>
        <link href="http://arxiv.org/abs/2107.10201"/>
        <updated>2021-07-22T02:03:12.682Z</updated>
        <summary type="html"><![CDATA[Large Neighborhood Search (LNS) is a combinatorial optimization heuristic
that starts with an assignment of values for the variables to be optimized, and
iteratively improves it by searching a large neighborhood around the current
assignment. In this paper we consider a learning-based LNS approach for mixed
integer programs (MIPs). We train a Neural Diving model to represent a
probability distribution over assignments, which, together with an existing MIP
solver, generates an initial assignment. Formulating the subsequent search
steps as a Markov Decision Process, we train a Neural Neighborhood Selection
policy to select a search neighborhood at each step, which is searched using a
MIP solver to find the next assignment. The policy network is trained using
imitation learning. We propose a target policy for imitation that, given enough
compute resources, is guaranteed to select the neighborhood containing the
optimal next assignment across all possible choices for the neighborhood of a
specified size. Our approach matches or outperforms all the baselines on five
real-world MIP datasets with large-scale instances from diverse applications,
including two production applications at Google. At large running times it
achieves $2\times$ to $37.8\times$ better average primal gap than the best
baseline on three of the datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Sonnerat_N/0/1/0/all/0/1"&gt;Nicolas Sonnerat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ktena_I/0/1/0/all/0/1"&gt;Ira Ktena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bartunov_S/0/1/0/all/0/1"&gt;Sergey Bartunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vinod Nair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement Learning and Procedurally Generated Environments. (arXiv:2107.09996v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.09996</id>
        <link href="http://arxiv.org/abs/2107.09996"/>
        <updated>2021-07-22T02:03:12.675Z</updated>
        <summary type="html"><![CDATA[This paper is an initial endeavor to bridge the gap between powerful Deep
Reinforcement Learning methodologies and the problem of exploration/coverage of
unknown terrains. Within this scope, MarsExplorer, an openai-gym compatible
environment tailored to exploration/coverage of unknown areas, is presented.
MarsExplorer translates the original robotics problem into a Reinforcement
Learning setup that various off-the-shelf algorithms can tackle. Any learned
policy can be straightforwardly applied to a robotic platform without an
elaborate simulation model of the robot's dynamics to apply a different
learning/adaptation phase. One of its core features is the controllable
multi-dimensional procedural generation of terrains, which is the key for
producing policies with strong generalization capabilities. Four different
state-of-the-art RL algorithms (A3C, PPO, Rainbow, and SAC) are trained on the
MarsExplorer environment, and a proper evaluation of their results compared to
the average human-level performance is reported. In the follow-up experimental
analysis, the effect of the multi-dimensional difficulty setting on the
learning capabilities of the best-performing algorithm (PPO) is analyzed. A
milestone result is the generation of an exploration policy that follows the
Hilbert curve without providing this information to the environment or
rewarding directly or indirectly Hilbert-curve-like trajectories. The
experimental analysis is concluded by comparing PPO learned policy results with
frontier-based exploration context for extended terrain sizes. The source code
can be found at: https://github.com/dimikout3/GeneralExplorationPolicy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koutras_D/0/1/0/all/0/1"&gt;Dimitrios I. Koutras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoutsis_A/0/1/0/all/0/1"&gt;Athanasios Ch. Kapoutsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amanatiadis_A/0/1/0/all/0/1"&gt;Angelos A. Amanatiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kosmatopoulos_E/0/1/0/all/0/1"&gt;Elias B. Kosmatopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Contrastive Self-Supervised Learning on Graphs. (arXiv:2107.09787v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09787</id>
        <link href="http://arxiv.org/abs/2107.09787"/>
        <updated>2021-07-22T02:03:12.667Z</updated>
        <summary type="html"><![CDATA[We study self-supervised learning on graphs using contrastive methods. A
general scheme of prior methods is to optimize two-view representations of
input graphs. In many studies, a single graph-level representation is computed
as one of the contrastive objectives, capturing limited characteristics of
graphs. We argue that contrasting graphs in multiple subspaces enables graph
encoders to capture more abundant characteristics. To this end, we propose a
group contrastive learning framework in this work. Our framework embeds the
given graph into multiple subspaces, of which each representation is prompted
to encode specific characteristics of graphs. To learn diverse and informative
representations, we develop principled objectives that enable us to capture the
relations among both intra-space and inter-space representations in groups.
Under the proposed framework, we further develop an attention-based representor
function to compute representations that capture different substructures of a
given graph. Built upon our framework, we extend two current methods into
GroupCL and GroupIG, equipped with the proposed objective. Comprehensive
experimental results show our framework achieves a promising boost in
performance on a variety of datasets. In addition, our qualitative results show
that features generated from our representor successfully capture various
specific characteristics of graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinyi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Cheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yaochen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Feature Selection, a Reparameterization Approach. (arXiv:2107.10030v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10030</id>
        <link href="http://arxiv.org/abs/2107.10030"/>
        <updated>2021-07-22T02:03:12.647Z</updated>
        <summary type="html"><![CDATA[We consider the task of feature selection for reconstruction which consists
in choosing a small subset of features from which whole data instances can be
reconstructed. This is of particular importance in several contexts involving
for example costly physical measurements, sensor placement or information
compression. To break the intrinsic combinatorial nature of this problem, we
formulate the task as optimizing a binary mask distribution enabling an
accurate reconstruction. We then face two main challenges. One concerns
differentiability issues due to the binary distribution. The second one
corresponds to the elimination of redundant information by selecting variables
in a correlated fashion which requires modeling the covariance of the binary
distribution. We address both issues by introducing a relaxation of the problem
via a novel reparameterization of the logitNormal distribution. We demonstrate
that the proposed method provides an effective exploration scheme and leads to
efficient feature selection for reconstruction through evaluation on several
high dimensional image benchmarks. We show that the method leverages the
intrinsic geometry of the data, facilitating reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dona_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xe9;mie Dona&lt;/a&gt; (MLIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt; (MLIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication and Computation Reduction for Split Learning using Asynchronous Training. (arXiv:2107.09786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09786</id>
        <link href="http://arxiv.org/abs/2107.09786"/>
        <updated>2021-07-22T02:03:12.638Z</updated>
        <summary type="html"><![CDATA[Split learning is a promising privacy-preserving distributed learning scheme
that has low computation requirement at the edge device but has the
disadvantage of high communication overhead between edge device and server. To
reduce the communication overhead, this paper proposes a loss-based
asynchronous training scheme that updates the client-side model less frequently
and only sends/receives activations/gradients in selected epochs. To further
reduce the communication overhead, the activations/gradients are quantized
using 8-bit floating point prior to transmission. An added benefit of the
proposed communication reduction method is that the computations at the client
side are reduced due to reduction in the number of client model updates.
Furthermore, the privacy of the proposed communication reduction based split
learning method is almost the same as traditional split learning. Simulation
results on VGG11, VGG13 and ResNet18 models on CIFAR-10 show that the
communication cost is reduced by 1.64x-106.7x and the computations in the
client are reduced by 2.86x-32.1x when the accuracy degradation is less than
0.5% for the single-client case. For 5 and 10-client cases, the communication
cost reduction is 11.9x and 11.3x on VGG11 for 0.5% loss in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingtao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_C/0/1/0/all/0/1"&gt;Chaitali Chakrabarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Reinforcement Learning Approach for Fair Traffic Signal Control. (arXiv:2107.10146v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10146</id>
        <link href="http://arxiv.org/abs/2107.10146"/>
        <updated>2021-07-22T02:03:12.630Z</updated>
        <summary type="html"><![CDATA[Traffic signal control is one of the most effective methods of traffic
management in urban areas. In recent years, traffic control methods based on
deep reinforcement learning (DRL) have gained attention due to their ability to
exploit real-time traffic data, which is often poorly used by the traditional
hand-crafted methods. While most recent DRL-based methods have focused on
maximizing the throughput or minimizing the average travel time of the
vehicles, the fairness of the traffic signal controllers has often been
neglected. This is particularly important as neglecting fairness can lead to
situations where some vehicles experience extreme waiting times, or where the
throughput of a particular traffic flow is highly impacted by the fluctuations
of another conflicting flow at the intersection. In order to address these
issues, we introduce two notions of fairness: delay-based and throughput-based
fairness, which correspond to the two issues mentioned above. Furthermore, we
propose two DRL-based traffic signal control methods for implementing these
fairness notions, that can achieve a high throughput as well. We evaluate the
performance of our proposed methods using three traffic arrival distributions,
and find that our methods outperform the baselines in the tested scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raeis_M/0/1/0/all/0/1"&gt;Majid Raeis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1"&gt;Alberto Leon-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorization in Deep Neural Networks: Does the Loss Function matter?. (arXiv:2107.09957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09957</id>
        <link href="http://arxiv.org/abs/2107.09957"/>
        <updated>2021-07-22T02:03:12.624Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks, often owing to the overparameterization, are shown to
be capable of exactly memorizing even randomly labelled data. Empirical studies
have also shown that none of the standard regularization techniques mitigate
such overfitting. We investigate whether the choice of the loss function can
affect this memorization. We empirically show, with benchmark data sets MNIST
and CIFAR-10, that a symmetric loss function, as opposed to either
cross-entropy or squared error loss, results in significant improvement in the
ability of the network to resist such overfitting. We then provide a formal
definition for robustness to memorization and provide a theoretical explanation
as to why the symmetric losses provide this robustness. Our results clearly
bring out the role loss functions alone can play in this phenomenon of
memorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1"&gt;P.S. Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MG-NET: Leveraging Pseudo-Imaging for Multi-Modal Metagenome Analysis. (arXiv:2107.09883v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09883</id>
        <link href="http://arxiv.org/abs/2107.09883"/>
        <updated>2021-07-22T02:03:12.617Z</updated>
        <summary type="html"><![CDATA[The emergence of novel pathogens and zoonotic diseases like the SARS-CoV-2
have underlined the need for developing novel diagnosis and intervention
pipelines that can learn rapidly from small amounts of labeled data. Combined
with technological advances in next-generation sequencing, metagenome-based
diagnostic tools hold much promise to revolutionize rapid point-of-care
diagnosis. However, there are significant challenges in developing such an
approach, the chief among which is to learn self-supervised representations
that can help detect novel pathogen signatures with very low amounts of labeled
data. This is particularly a difficult task given that closely related
pathogens can share more than 90% of their genome structure. In this work, we
address these challenges by proposing MG-Net, a self-supervised representation
learning framework that leverages multi-modal context using pseudo-imaging data
derived from clinical metagenome sequences. We show that the proposed framework
can learn robust representations from unlabeled data that can be used for
downstream tasks such as metagenome sequence classification with limited access
to labeled data. Extensive experiments show that the learned features
outperform current baseline metagenome representations, given only 1000 samples
per class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aakur_S/0/1/0/all/0/1"&gt;Sathyanarayanan N. Aakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Sai Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indla_V/0/1/0/all/0/1"&gt;Vineela Indla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagavathi_A/0/1/0/all/0/1"&gt;Arunkumar Bagavathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramnath_V/0/1/0/all/0/1"&gt;Vishalini Laguduva Ramnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1"&gt;Akhilesh Ramachandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Matchings via Learned Duals. (arXiv:2107.09770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09770</id>
        <link href="http://arxiv.org/abs/2107.09770"/>
        <updated>2021-07-22T02:03:12.598Z</updated>
        <summary type="html"><![CDATA[A recent line of research investigates how algorithms can be augmented with
machine-learned predictions to overcome worst case lower bounds. This area has
revealed interesting algorithmic insights into problems, with particular
success in the design of competitive online algorithms. However, the question
of improving algorithm running times with predictions has largely been
unexplored.

We take a first step in this direction by combining the idea of
machine-learned predictions with the idea of "warm-starting" primal-dual
algorithms. We consider one of the most important primitives in combinatorial
optimization: weighted bipartite matching and its generalization to
$b$-matching. We identify three key challenges when using learned dual
variables in a primal-dual algorithm. First, predicted duals may be infeasible,
so we give an algorithm that efficiently maps predicted infeasible duals to
nearby feasible solutions. Second, once the duals are feasible, they may not be
optimal, so we show that they can be used to quickly find an optimal solution.
Finally, such predictions are useful only if they can be learned, so we show
that the problem of learning duals for matching has low sample complexity. We
validate our theoretical findings through experiments on both real and
synthetic data. As a result we give a rigorous, practical, and empirically
effective method to compute bipartite matchings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinitz_M/0/1/0/all/0/1"&gt;Michael Dinitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1"&gt;Sungjin Im&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavastida_T/0/1/0/all/0/1"&gt;Thomas Lavastida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1"&gt;Benjamin Moseley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassilvitskii_S/0/1/0/all/0/1"&gt;Sergei Vassilvitskii&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integration of Autoencoder and Functional Link Artificial Neural Network for Multi-label Classification. (arXiv:2107.09904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09904</id>
        <link href="http://arxiv.org/abs/2107.09904"/>
        <updated>2021-07-22T02:03:12.584Z</updated>
        <summary type="html"><![CDATA[Multi-label (ML) classification is an actively researched topic currently,
which deals with convoluted and overlapping boundaries that arise due to
several labels being active for a particular data instance. We propose a
classifier capable of extracting underlying features and introducing
non-linearity to the data to handle the complex decision boundaries. A novel
neural network model has been developed where the input features are subjected
to two transformations adapted from multi-label functional link artificial
neural network and autoencoders. First, a functional expansion of the original
features are made using basis functions. This is followed by an
autoencoder-aided transformation and reduction on the expanded features. This
network is capable of improving separability for the multi-label data owing to
the two-layer transformation while reducing the expanded feature space to a
more manageable amount. This balances the input dimension which leads to a
better classification performance even for a limited amount of data. The
proposed network has been validated on five ML datasets which shows its
superior performance in comparison with six well-established ML classifiers.
Furthermore, a single-label variation of the proposed network has also been
formulated simultaneously and tested on four relevant datasets against three
existing classifiers to establish its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Law_A/0/1/0/all/0/1"&gt;Anwesha Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Ashish Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boundary of Distribution Support Generator (BDSG): Sample Generation on the Boundary. (arXiv:2107.09950v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09950</id>
        <link href="http://arxiv.org/abs/2107.09950"/>
        <updated>2021-07-22T02:03:12.563Z</updated>
        <summary type="html"><![CDATA[Generative models, such as Generative Adversarial Networks (GANs), have been
used for unsupervised anomaly detection. While performance keeps improving,
several limitations exist particularly attributed to difficulties at capturing
multimodal supports and to the ability to approximate the underlying
distribution closer to the tails, i.e. the boundary of the distribution's
support. This paper proposes an approach that attempts to alleviate such
shortcomings. We propose an invertible-residual-network-based model, the
Boundary of Distribution Support Generator (BDSG). GANs generally do not
guarantee the existence of a probability distribution and here, we use the
recently developed Invertible Residual Network (IResNet) and Residual Flow
(ResFlow), for density estimation. These models have not yet been used for
anomaly detection. We leverage IResNet and ResFlow for Out-of-Distribution
(OoD) sample detection and for sample generation on the boundary using a
compound loss function that forces the samples to lie on the boundary. The BDSG
addresses non-convex support, disjoint components, and multimodal
distributions. Results on synthetic data and data from multimodal
distributions, such as MNIST and CIFAR-10, demonstrate competitive performance
compared to methods from the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1"&gt;Nikolaos Dionelis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECG Heartbeat Classification Using Multimodal Fusion. (arXiv:2107.09869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09869</id>
        <link href="http://arxiv.org/abs/2107.09869"/>
        <updated>2021-07-22T02:03:12.556Z</updated>
        <summary type="html"><![CDATA[Electrocardiogram (ECG) is an authoritative source to diagnose and counter
critical cardiovascular syndromes such as arrhythmia and myocardial infarction
(MI). Current machine learning techniques either depend on manually extracted
features or large and complex deep learning networks which merely utilize the
1D ECG signal directly. Since intelligent multimodal fusion can perform at the
stateof-the-art level with an efficient deep network, therefore, in this paper,
we propose two computationally efficient multimodal fusion frameworks for ECG
heart beat classification called Multimodal Image Fusion (MIF) and Multimodal
Feature Fusion (MFF). At the input of these frameworks, we convert the raw ECG
data into three different images using Gramian Angular Field (GAF), Recurrence
Plot (RP) and Markov Transition Field (MTF). In MIF, we first perform image
fusion by combining three imaging modalities to create a single image modality
which serves as input to the Convolutional Neural Network (CNN). In MFF, we
extracted features from penultimate layer of CNNs and fused them to get unique
and interdependent information necessary for better performance of classifier.
These informational features are finally used to train a Support Vector Machine
(SVM) classifier for ECG heart-beat classification. We demonstrate the
superiority of the proposed fusion models by performing experiments on
PhysioNets MIT-BIH dataset for five distinct conditions of arrhythmias which
are consistent with the AAMI EC57 protocols and on PTB diagnostics dataset for
Myocardial Infarction (MI) classification. We achieved classification accuracy
of 99.7% and 99.2% on arrhythmia and MI classification, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1"&gt;Anika Tabassum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1"&gt;Ling Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Environmental Microorganism Images: from Convolutional Neural Networks to Visual Transformers. (arXiv:2106.11582v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11582</id>
        <link href="http://arxiv.org/abs/2106.11582"/>
        <updated>2021-07-22T02:03:12.537Z</updated>
        <summary type="html"><![CDATA[Nowadays, analysis of Transparent Environmental Microorganism Images (T-EM
images) in the field of computer vision has gradually become a new and
interesting spot. This paper compares different deep learning classification
performance for the problem that T-EM images are challenging to analyze. We
crop the T-EM images into 8 * 8 and 224 * 224 pixel patches in the same
proportion and then divide the two different pixel patches into foreground and
background according to ground truth. We also use four convolutional neural
networks and a novel ViT network model to compare the foreground and background
classification experiments. We conclude that ViT performs the worst in
classifying 8 * 8 pixel patches, but it outperforms most convolutional neural
networks in classifying 224 * 224 pixel patches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Ao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions. (arXiv:2107.09734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09734</id>
        <link href="http://arxiv.org/abs/2107.09734"/>
        <updated>2021-07-22T02:03:12.526Z</updated>
        <summary type="html"><![CDATA[Whilst an abundance of techniques have recently been proposed to generate
counterfactual explanations for the predictions of opaque black-box systems,
markedly less attention has been paid to exploring the uncertainty of these
generated explanations. This becomes a critical issue in high-stakes scenarios,
where uncertain and misleading explanations could have dire consequences (e.g.,
medical diagnosis and treatment planning). Moreover, it is often difficult to
determine if the generated explanations are well grounded in the training data
and sensitive to distributional shifts. This paper proposes several practical
solutions that can be leveraged to solve these problems by establishing novel
connections with other research works in explainability (e.g., trust scores)
and uncertainty estimation (e.g., Monte Carlo Dropout). Two experiments
demonstrate the utility of our proposed solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1"&gt;Eoin Delaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1"&gt;Derek Greene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1"&gt;Mark T. Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Checkovid: A COVID-19 misinformation detection system on Twitter using network and content mining perspectives. (arXiv:2107.09768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09768</id>
        <link href="http://arxiv.org/abs/2107.09768"/>
        <updated>2021-07-22T02:03:12.520Z</updated>
        <summary type="html"><![CDATA[During the COVID-19 pandemic, social media platforms were ideal for
communicating due to social isolation and quarantine. Also, it was the primary
source of misinformation dissemination on a large scale, referred to as the
infodemic. Therefore, automatic debunking misinformation is a crucial problem.
To tackle this problem, we present two COVID-19 related misinformation datasets
on Twitter and propose a misinformation detection system comprising
network-based and content-based processes based on machine learning algorithms
and NLP techniques. In the network-based process, we focus on social
properties, network characteristics, and users. On the other hand, we classify
misinformation using the content of the tweets directly in the content-based
process, which contains text classification models (paragraph-level and
sentence-level) and similarity models. The evaluation results on the
network-based process show the best results for the artificial neural network
model with an F1 score of 88.68%. In the content-based process, our novel
similarity models, which obtained an F1 score of 90.26%, show an improvement in
the misinformation classification results compared to the network-based models.
In addition, in the text classification models, the best result was achieved
using the stacking ensemble-learning model by obtaining an F1 score of 95.18%.
Furthermore, we test our content-based models on the Constraint@AAAI2021
dataset, and by getting an F1 score of 94.38%, we improve the baseline results.
Finally, we develop a fact-checking website called Checkovid that uses each
process to detect misinformative and informative claims in the domain of
COVID-19 from different perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dadgar_S/0/1/0/all/0/1"&gt;Sajad Dadgar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1"&gt;Mehdi Ghatee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap between Spatial and Spectral Domains: A Theoretical Framework for Graph Neural Networks. (arXiv:2107.10234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10234</id>
        <link href="http://arxiv.org/abs/2107.10234"/>
        <updated>2021-07-22T02:03:12.479Z</updated>
        <summary type="html"><![CDATA[During the past decade, deep learning's performance has been widely
recognized in a variety of machine learning tasks, ranging from image
classification, speech recognition to natural language understanding. Graph
neural networks (GNN) are a type of deep learning that is designed to handle
non-Euclidean issues using graph-structured data that are difficult to solve
with traditional deep learning techniques. The majority of GNNs were created
using a variety of processes, including random walk, PageRank, graph
convolution, and heat diffusion, making direct comparisons impossible. Previous
studies have primarily focused on classifying current models into distinct
categories, with little investigation of their internal relationships. This
research proposes a unified theoretical framework and a novel perspective that
can methodologically integrate existing GNN into our framework. We survey and
categorize existing GNN models into spatial and spectral domains, as well as
show linkages between subcategories within each domain. Further investigation
reveals a strong relationship between the spatial, spectral, and subgroups of
these domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1"&gt;Taoran Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Kaiqun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demonstration-Guided Reinforcement Learning with Learned Skills. (arXiv:2107.10253v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10253</id>
        <link href="http://arxiv.org/abs/2107.10253"/>
        <updated>2021-07-22T02:03:12.471Z</updated>
        <summary type="html"><![CDATA[Demonstration-guided reinforcement learning (RL) is a promising approach for
learning complex behaviors by leveraging both reward feedback and a set of
target task demonstrations. Prior approaches for demonstration-guided RL treat
every new task as an independent learning problem and attempt to follow the
provided demonstrations step-by-step, akin to a human trying to imitate a
completely unseen behavior by following the demonstrator's exact muscle
movements. Naturally, such learning will be slow, but often new behaviors are
not completely unseen: they share subtasks with behaviors we have previously
learned. In this work, we aim to exploit this shared subtask structure to
increase the efficiency of demonstration-guided RL. We first learn a set of
reusable skills from large offline datasets of prior experience collected
across many tasks. We then propose Skill-based Learning with Demonstrations
(SkiLD), an algorithm for demonstration-guided RL that efficiently leverages
the provided demonstrations by following the demonstrated skills instead of the
primitive actions, resulting in substantial performance improvements over prior
demonstration-guided RL approaches. We validate the effectiveness of our
approach on long-horizon maze navigation and complex robot manipulation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1"&gt;Karl Pertsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Youngwoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Joseph J. Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Better Look Twice: a new perspective for designing accurate detectors with reduced computations. (arXiv:2107.10050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10050</id>
        <link href="http://arxiv.org/abs/2107.10050"/>
        <updated>2021-07-22T02:03:12.410Z</updated>
        <summary type="html"><![CDATA[General object detectors use powerful backbones that uniformly extract
features from images for enabling detection of a vast amount of object types.
However, utilization of such backbones in object detection applications
developed for specific object types can unnecessarily over-process an extensive
amount of background. In addition, they are agnostic to object scales, thus
redundantly process all image regions at the same resolution. In this work we
introduce BLT-net, a new low-computation two-stage object detection
architecture designed to process images with a significant amount of background
and objects of variate scales. BLT-net reduces computations by separating
objects from background using a very lite first-stage. BLT-net then efficiently
merges obtained proposals to further decrease processed background and then
dynamically reduces their resolution to minimize computations. Resulting image
proposals are then processed in the second-stage by a highly accurate model. We
demonstrate our architecture on the pedestrian detection problem, where objects
are of different sizes, images are of high resolution and object detection is
required to run in real-time. We show that our design reduces computations by a
factor of x4-x7 on the Citypersons and Caltech datasets with respect to leading
pedestrian detectors, on account of a small accuracy degradation. This method
can be applied on other object detection applications in scenes with a
considerable amount of background and variate object sizes to reduce
computations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dana_A/0/1/0/all/0/1"&gt;Alexandra Dana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutman_M/0/1/0/all/0/1"&gt;Maor Shutman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1"&gt;Yotam Perlitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1"&gt;Ran Vitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peleg_T/0/1/0/all/0/1"&gt;Tomer Peleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1"&gt;Roy Jevnisek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S4T: Source-free domain adaptation for semantic segmentation via self-supervised selective self-training. (arXiv:2107.10140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10140</id>
        <link href="http://arxiv.org/abs/2107.10140"/>
        <updated>2021-07-22T02:03:12.404Z</updated>
        <summary type="html"><![CDATA[Most modern approaches for domain adaptive semantic segmentation rely on
continued access to source data during adaptation, which may be infeasible due
to computational or privacy constraints. We focus on source-free domain
adaptation for semantic segmentation, wherein a source model must adapt itself
to a new target domain given only unlabeled target data. We propose
Self-Supervised Selective Self-Training (S4T), a source-free adaptation
algorithm that first uses the model's pixel-level predictive consistency across
diverse views of each target image along with model confidence to classify
pixel predictions as either reliable or unreliable. Next, the model is
self-trained, using predicted pseudolabels for reliable predictions and
pseudolabels inferred via a selective interpolation strategy for unreliable
ones. S4T matches or improves upon the state-of-the-art in source-free
adaptation on 3 standard benchmarks for semantic segmentation within a single
epoch of adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1"&gt;Viraj Prabhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1"&gt;Shivam Khare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1"&gt;Deeksha Kartik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;Judy Hoffman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JEFL: Joint Embedding of Formal Proof Libraries. (arXiv:2107.10188v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2107.10188</id>
        <link href="http://arxiv.org/abs/2107.10188"/>
        <updated>2021-07-22T02:03:12.396Z</updated>
        <summary type="html"><![CDATA[The heterogeneous nature of the logical foundations used in different
interactive proof assistant libraries has rendered discovery of similar
mathematical concepts among them difficult. In this paper, we compare a
previously proposed algorithm for matching concepts across libraries with our
unsupervised embedding approach that can help us retrieve similar concepts. Our
approach is based on the fasttext implementation of Word2Vec, on top of which a
tree traversal module is added to adapt its algorithm to the representation
format of our data export pipeline. We compare the explainability,
customizability, and online-servability of the approaches and argue that the
neural embedding approach has more potential to be integrated into an
interactive proof assistant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1"&gt;Cezary Kaliszyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Deep Learning Techniques for Face Recognition. (arXiv:2103.10492v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10492</id>
        <link href="http://arxiv.org/abs/2103.10492"/>
        <updated>2021-07-22T02:03:12.388Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have proposed many deep learning (DL) methods
for various tasks, and particularly face recognition (FR) made an enormous leap
using these techniques. Deep FR systems benefit from the hierarchical
architecture of the DL methods to learn discriminative face representation.
Therefore, DL techniques significantly improve state-of-the-art performance on
FR systems and encourage diverse and efficient real-world applications. In this
paper, we present a comprehensive analysis of various FR systems that leverage
the different types of DL techniques, and for the study, we summarize 168
recent contributions from this area. We discuss the papers related to different
algorithms, architectures, loss functions, activation functions, datasets,
challenges, improvement ideas, current and future trends of DL-based FR
systems. We provide a detailed discussion of various DL methods to understand
the current state-of-the-art, and then we discuss various activation and loss
functions for the methods. Additionally, we summarize different datasets used
widely for FR tasks and discuss challenges related to illumination, expression,
pose variations, and occlusion. Finally, we discuss improvement ideas, current
and future trends of FR tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Md. Tahmid Hasan Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1"&gt;Awal Ahmed Fime&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1"&gt;Delowar Sikder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1"&gt;Md. Akil Raihan Iftee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1"&gt;Jakaria Rabbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_rakhami_M/0/1/0/all/0/1"&gt;Mabrook S. Al-rakhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gumae_A/0/1/0/all/0/1"&gt;Abdu Gumae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1"&gt;Ovishake Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Mohtasim Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md. Nazrul Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leave-one-out Unfairness. (arXiv:2107.10171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10171</id>
        <link href="http://arxiv.org/abs/2107.10171"/>
        <updated>2021-07-22T02:03:12.363Z</updated>
        <summary type="html"><![CDATA[We introduce leave-one-out unfairness, which characterizes how likely a
model's prediction for an individual will change due to the inclusion or
removal of a single other person in the model's training data. Leave-one-out
unfairness appeals to the idea that fair decisions are not arbitrary: they
should not be based on the chance event of any one person's inclusion in the
training data. Leave-one-out unfairness is closely related to algorithmic
stability, but it focuses on the consistency of an individual point's
prediction outcome over unit changes to the training data, rather than the
error of the model in aggregate. Beyond formalizing leave-one-out unfairness,
we characterize the extent to which deep models behave leave-one-out unfairly
on real data, including in cases where the generalization error is small.
Further, we demonstrate that adversarial training and randomized smoothing
techniques have opposite effects on leave-one-out fairness, which sheds light
on the relationships between robustness, memorization, individual fairness, and
leave-one-out fairness in deep models. Finally, we discuss salient practical
applications that may be negatively affected by leave-one-out unfairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Black_E/0/1/0/all/0/1"&gt;Emily Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1"&gt;Matt Fredrikson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?. (arXiv:2010.14808v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14808</id>
        <link href="http://arxiv.org/abs/2010.14808"/>
        <updated>2021-07-22T02:03:12.356Z</updated>
        <summary type="html"><![CDATA[With the unprecedented developments in deep learning, automatic segmentation
of main abdominal organs seems to be a solved problem as state-of-the-art
(SOTA) methods have achieved comparable results with inter-rater variability on
many benchmark datasets. However, most of the existing abdominal datasets only
contain single-center, single-phase, single-vendor, or single-disease cases,
and it is unclear whether the excellent performance can generalize on diverse
datasets. This paper presents a large and diverse abdominal CT organ
segmentation dataset, termed AbdomenCT-1K, with more than 1000 (1K) CT scans
from 12 medical centers, including multi-phase, multi-vendor, and multi-disease
cases. Furthermore, we conduct a large-scale study for liver, kidney, spleen,
and pancreas segmentation and reveal the unsolved segmentation problems of the
SOTA methods, such as the limited generalization ability on distinct medical
centers, phases, and unseen diseases. To advance the unsolved problems, we
further build four organ segmentation benchmarks for fully supervised,
semi-supervised, weakly supervised, and continual learning, which are currently
challenging and active research topics. Accordingly, we develop a simple and
effective method for each benchmark, which can be used as out-of-the-box
methods and strong baselines. We believe the AbdomenCT-1K dataset will promote
future in-depth research towards clinical applicable abdominal organ
segmentation methods. The datasets, codes, and trained models are publicly
available at https://github.com/JunMa11/AbdomenCT-1K.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Song Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Cheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1"&gt;Cheng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1"&gt;Xingle An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Congcong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shucheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shangqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunpeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WGCN: Graph Convolutional Networks with Weighted Structural Features. (arXiv:2104.14060v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14060</id>
        <link href="http://arxiv.org/abs/2104.14060"/>
        <updated>2021-07-22T02:03:12.348Z</updated>
        <summary type="html"><![CDATA[Graph structural information such as topologies or connectivities provides
valuable guidance for graph convolutional networks (GCNs) to learn nodes'
representations. Existing GCN models that capture nodes' structural information
weight in- and out-neighbors equally or differentiate in- and out-neighbors
globally without considering nodes' local topologies. We observe that in- and
out-neighbors contribute differently for nodes with different local topologies.
To explore the directional structural information for different nodes, we
propose a GCN model with weighted structural features, named WGCN. WGCN first
captures nodes' structural fingerprints via a direction and degree aware Random
Walk with Restart algorithm, where the walk is guided by both edge direction
and nodes' in- and out-degrees. Then, the interactions between nodes'
structural fingerprints are used as the weighted node structural features. To
further capture nodes' high-order dependencies and graph geometry, WGCN embeds
graphs into a latent space to obtain nodes' latent neighbors and geometrical
relationships. Based on nodes' geometrical relationships in the latent space,
WGCN differentiates latent, in-, and out-neighbors with an attention-based
geometrical aggregation. Experiments on transductive node classification tasks
show that WGCN outperforms the baseline models consistently by up to 17.07% in
terms of accuracy on five benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianzhong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling 3D Prototypical Networks For Few-Shot Concept Learning. (arXiv:2011.03367v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03367</id>
        <link href="http://arxiv.org/abs/2011.03367"/>
        <updated>2021-07-22T02:03:12.337Z</updated>
        <summary type="html"><![CDATA[We present neural architectures that disentangle RGB-D images into objects'
shapes and styles and a map of the background scene, and explore their
applications for few-shot 3D object detection and few-shot concept
classification. Our networks incorporate architectural biases that reflect the
image formation process, 3D geometry of the world scene, and shape-style
interplay. They are trained end-to-end self-supervised by predicting views in
static scenes, alongside a small number of 3D object boxes. Objects and scenes
are represented in terms of 3D feature grids in the bottleneck of the network.
We show that the proposed 3D neural representations are compositional: they can
generate novel 3D scene feature maps by mixing object shapes and styles,
resizing and adding the resulting object 3D feature maps over background scene
feature maps. We show that classifiers for object categories, color, materials,
and spatial relationships trained over the disentangled 3D feature sub-spaces
generalize better with dramatically fewer examples than the current
state-of-the-art, and enable a visual question answering system that uses them
as its modules to generalize one-shot to novel objects in the scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1"&gt;Mihir Prabhudesai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lal_S/0/1/0/all/0/1"&gt;Shamit Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_D/0/1/0/all/0/1"&gt;Darshan Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiao-Yu Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1"&gt;Adam W Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1"&gt;Katerina Fragkiadaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CycleMLP: A MLP-like Architecture for Dense Prediction. (arXiv:2107.10224v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10224</id>
        <link href="http://arxiv.org/abs/2107.10224"/>
        <updated>2021-07-22T02:03:12.330Z</updated>
        <summary type="html"><![CDATA[This paper presents a simple MLP-like architecture, CycleMLP, which is a
versatile backbone for visual recognition and dense predictions, unlike modern
MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose architectures are
correlated to image size and thus are infeasible in object detection and
segmentation. CycleMLP has two advantages compared to modern approaches. (1) It
can cope with various image sizes. (2) It achieves linear computational
complexity to image size by using local windows. In contrast, previous MLPs
have quadratic computations because of their fully spatial connections. We
build a family of models that surpass existing MLPs and achieve a comparable
accuracy (83.2%) on ImageNet-1K classification compared to the state-of-the-art
Transformer such as Swin Transformer (83.3%) but using fewer parameters and
FLOPs. We expand the MLP-like models' applicability, making them a versatile
backbone for dense prediction tasks. CycleMLP aims to provide a competitive
baseline on object detection, instance segmentation, and semantic segmentation
for MLP models. In particular, CycleMLP achieves 45.1 mIoU on ADE20K val,
comparable to Swin (45.2 mIOU). Code is available at
\url{https://github.com/ShoufaChen/CycleMLP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shoufa Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1"&gt;Chongjian Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Ding Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics. (arXiv:2107.10043v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.10043</id>
        <link href="http://arxiv.org/abs/2107.10043"/>
        <updated>2021-07-22T02:03:12.306Z</updated>
        <summary type="html"><![CDATA[Real-time state estimation of dynamical systems is a fundamental task in
signal processing and control. For systems that are well-represented by a fully
known linear Gaussian state space (SS) model, the celebrated Kalman filter (KF)
is a low complexity optimal solution. However, both linearity of the underlying
SS model and accurate knowledge of it are often not encountered in practice.
Here, we present KalmanNet, a real-time state estimator that learns from data
to carry out Kalman filtering under non-linear dynamics with partial
information. By incorporating the structural SS model with a dedicated
recurrent neural network module in the flow of the KF, we retain data
efficiency and interpretability of the classic algorithm while implicitly
learning complex dynamics from data. We numerically demonstrate that KalmanNet
overcomes nonlinearities and model mismatch, outperforming classic filtering
methods operating with both mismatched and accurate domain knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Revach_G/0/1/0/all/0/1"&gt;Guy Revach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1"&gt;Nir Shlezinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ni_X/0/1/0/all/0/1"&gt;Xiaoyong Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Escoriza_A/0/1/0/all/0/1"&gt;Adria Lopez Escoriza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sloun_R/0/1/0/all/0/1"&gt;Ruud J. G. van Sloun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Lower-Dose PET using Physics-Based Uncertainty-Aware Multimodal Learning with Robustness to Out-of-Distribution Data. (arXiv:2107.09892v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09892</id>
        <link href="http://arxiv.org/abs/2107.09892"/>
        <updated>2021-07-22T02:03:12.299Z</updated>
        <summary type="html"><![CDATA[Radiation exposure in positron emission tomography (PET) imaging limits its
usage in the studies of radiation-sensitive populations, e.g., pregnant women,
children, and adults that require longitudinal imaging. Reducing the PET
radiotracer dose or acquisition time reduces photon counts, which can
deteriorate image quality. Recent deep-neural-network (DNN) based methods for
image-to-image translation enable the mapping of low-quality PET images
(acquired using substantially reduced dose), coupled with the associated
magnetic resonance imaging (MRI) images, to high-quality PET images. However,
such DNN methods focus on applications involving test data that match the
statistical characteristics of the training data very closely and give little
attention to evaluating the performance of these DNNs on new
out-of-distribution (OOD) acquisitions. We propose a novel DNN formulation that
models the (i) underlying sinogram-based physics of the PET imaging system and
(ii) the uncertainty in the DNN output through the per-voxel heteroscedasticity
of the residuals between the predicted and the high-quality reference images.
Our sinogram-based uncertainty-aware DNN framework, namely, suDNN, estimates a
standard-dose PET image using multimodal input in the form of (i) a
low-dose/low-count PET image and (ii) the corresponding multi-contrast MRI
images, leading to improved robustness of suDNN to OOD acquisitions. Results on
in vivo simultaneous PET-MRI, and various forms of OOD data in PET-MRI, show
the benefits of suDNN over the current state of the art, quantitatively and
qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sudarshan_V/0/1/0/all/0/1"&gt;Viswanath P. Sudarshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1"&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egan_G/0/1/0/all/0/1"&gt;Gary F. Egan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1"&gt;Suyash P. Awate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture Model Framework for Traumatic Brain Injury Prognosis Using Heterogeneous Clinical and Outcome Data. (arXiv:2012.12310v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12310</id>
        <link href="http://arxiv.org/abs/2012.12310"/>
        <updated>2021-07-22T02:03:12.279Z</updated>
        <summary type="html"><![CDATA[Prognoses of Traumatic Brain Injury (TBI) outcomes are neither easily nor
accurately determined from clinical indicators. This is due in part to the
heterogeneity of damage inflicted to the brain, ultimately resulting in diverse
and complex outcomes. Using a data-driven approach on many distinct data
elements may be necessary to describe this large set of outcomes and thereby
robustly depict the nuanced differences among TBI patients' recovery. In this
work, we develop a method for modeling large heterogeneous data types relevant
to TBI. Our approach is geared toward the probabilistic representation of mixed
continuous and discrete variables with missing values. The model is trained on
a dataset encompassing a variety of data types, including demographics,
blood-based biomarkers, and imaging findings. In addition, it includes a set of
clinical outcome assessments at 3, 6, and 12 months post-injury. The model is
used to stratify patients into distinct groups in an unsupervised learning
setting. We use the model to infer outcomes using input data, and show that the
collection of input data reduces uncertainty of outcomes over a baseline
approach. In addition, we quantify the performance of a likelihood scoring
technique that can be used to self-evaluate the extrapolation risk of prognosis
on unseen patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_A/0/1/0/all/0/1"&gt;Alan D. Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_K/0/1/0/all/0/1"&gt;K. Aditya Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_L/0/1/0/all/0/1"&gt;Lindsay D. Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Sonia Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levin_H/0/1/0/all/0/1"&gt;Harvey Levin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torres_Espin_A/0/1/0/all/0/1"&gt;Abel Torres-Espin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_A/0/1/0/all/0/1"&gt;Austin Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huie_J/0/1/0/all/0/1"&gt;J. Russell Huie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferguson_A/0/1/0/all/0/1"&gt;Adam R. Ferguson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCrea_M/0/1/0/all/0/1"&gt;Michael McCrea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacino_J/0/1/0/all/0/1"&gt;Joseph Giacino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shivshankar Sundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markowitz_A/0/1/0/all/0/1"&gt;Amy J. Markowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manley_G/0/1/0/all/0/1"&gt;Geoffrey T. Manley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Hyperparameter Optimization of Deep Neural Networks via Ensembling Multiple Surrogates. (arXiv:1811.02319v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.02319</id>
        <link href="http://arxiv.org/abs/1811.02319"/>
        <updated>2021-07-22T02:03:12.271Z</updated>
        <summary type="html"><![CDATA[The performance of deep neural networks crucially depends on good
hyperparameter configurations. Bayesian optimization is a powerful framework
for optimizing the hyperparameters of DNNs. These methods need sufficient
evaluation data to approximate and minimize the validation error function of
hyperparameters. However, the expensive evaluation cost of DNNs leads to very
few evaluation data within a limited time, which greatly reduces the efficiency
of Bayesian optimization. Besides, the previous researches focus on using the
complete evaluation data to conduct Bayesian optimization, and ignore the
intermediate evaluation data generated by early stopping methods. To alleviate
the insufficient evaluation data problem, we propose a fast hyperparameter
optimization method, HOIST, that utilizes both the complete and intermediate
evaluation data to accelerate the hyperparameter optimization of DNNs.
Specifically, we train multiple basic surrogates to gather information from the
mixed evaluation data, and then combine all basic surrogates using weighted
bagging to provide an accurate ensemble surrogate. Our empirical studies show
that HOIST outperforms the state-of-the-art approaches on a wide range of DNNs,
including feed forward neural networks, convolutional neural networks,
recurrent neural networks, and variational autoencoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yingxia Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Electric Vehicle Charging Controllers with Imitation Learning. (arXiv:2107.10111v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10111</id>
        <link href="http://arxiv.org/abs/2107.10111"/>
        <updated>2021-07-22T02:03:12.263Z</updated>
        <summary type="html"><![CDATA[The problem of coordinating the charging of electric vehicles gains more
importance as the number of such vehicles grows. In this paper, we develop a
method for the training of controllers for the coordination of EV charging. In
contrast to most existing works on this topic, we require the controllers to
preserve the privacy of the users, therefore we do not allow any communication
from the controller to any third party.

In order to train the controllers, we use the idea of imitation learning --
we first find an optimum solution for a relaxed version of the problem using
quadratic optimization and then train the controllers to imitate this solution.
We also investigate the effects of regularization of the optimum solution on
the performance of the controllers. The method is evaluated on realistic data
and shows improved performance and training speed compared to similar
controllers trained using evolutionary algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pilat_M/0/1/0/all/0/1"&gt;Martin Pil&amp;#xe1;t&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding. (arXiv:2107.09931v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09931</id>
        <link href="http://arxiv.org/abs/2107.09931"/>
        <updated>2021-07-22T02:03:12.243Z</updated>
        <summary type="html"><![CDATA[While recent benchmarks have spurred a lot of new work on improving the
generalization of pretrained multilingual language models on multilingual
tasks, techniques to improve code-switched natural language understanding tasks
have been far less explored. In this work, we propose the use of bilingual
intermediate pretraining as a reliable technique to derive large and consistent
performance gains on three different NLP tasks using code-switched text. We
achieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the
mean accuracies and F1 scores over previous state-of-the-art systems for
Hindi-English Natural Language Inference (NLI), Question Answering (QA) tasks,
and Spanish-English Sentiment Analysis (SA) respectively. We show consistent
performance gains on four different code-switched language-pairs
(Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA.
We also present a code-switched masked language modelling (MLM) pretraining
technique that consistently benefits SA compared to standard MLM pretraining
using real code-switched text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1"&gt;Archiki Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehan_M/0/1/0/all/0/1"&gt;Mohammad Ali Rehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1"&gt;Shreya Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preventing dataset shift from breaking machine-learning biomarkers. (arXiv:2107.09947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09947</id>
        <link href="http://arxiv.org/abs/2107.09947"/>
        <updated>2021-07-22T02:03:12.235Z</updated>
        <summary type="html"><![CDATA[Machine learning brings the hope of finding new biomarkers extracted from
cohorts with rich biomedical measurements. A good biomarker is one that gives
reliable detection of the corresponding condition. However, biomarkers are
often extracted from a cohort that differs from the target population. Such a
mismatch, known as a dataset shift, can undermine the application of the
biomarker to new individuals. Dataset shifts are frequent in biomedical
research, e.g. because of recruitment biases. When a dataset shift occurs,
standard machine-learning techniques do not suffice to extract and validate
biomarkers. This article provides an overview of when and how dataset shifts
breaks machine-learning extracted biomarkers, as well as detection and
correction strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dockes_J/0/1/0/all/0/1"&gt;J&amp;#xe9;ro&amp;#xf4;me Dock&amp;#xe8;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt; (PARIETAL), &lt;a href="http://arxiv.org/find/cs/1/au:+Poline_J/0/1/0/all/0/1"&gt;Jean-Baptiste Poline&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Real-World Evidence Analysis of COVID-19 Pharmacotherapy. (arXiv:2107.10239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10239</id>
        <link href="http://arxiv.org/abs/2107.10239"/>
        <updated>2021-07-22T02:03:12.228Z</updated>
        <summary type="html"><![CDATA[Introduction: Real-world data generated from clinical practice can be used to
analyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate
the results of randomized clinical trials (RCTs). Machine learning (ML) methods
are being used in RWE and are promising tools for precision-medicine. In this
study, ML methods are applied to study the efficacy of therapies on COVID-19
hospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312
COVID-19 hospital admissions - dated between January 2020 and January 2021 from
10 health departments, were used respectively for training and validation of
separate treatment-effect models (TE-ML) for remdesivir, corticosteroids,
tocilizumab, lopinavir-ritonavir, azithromycin and
chloroquine/hydroxychloroquine. 2390 admissions from 2 additional health
departments were reserved as an independent test to analyze retrospectively the
survival benefits of therapies in the population selected by the TE-ML models
using cox-proportional hazard models. TE-ML models were adjusted using
treatment propensity scores to control for pre-treatment confounding variables
associated to outcome and further evaluated for futility. ML architecture was
based on boosted decision-trees. Results: In the populations identified by the
TE-ML models, only Remdesivir and Tocilizumab were significantly associated
with an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and
0.21 (P = 0.001), respectively. No survival benefits from chloroquine
derivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to
explain the predictions of TE-ML models are explored at patient-level as
potential tools for personalized decision making and precision medicine.
Conclusion: ML methods are suitable tools toward RWE analysis of COVID-19
pharmacotherapies. Results obtained reproduce published results on RWE and
validate the results from RCTs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bustos_A/0/1/0/all/0/1"&gt;Aurelia Bustos&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mas_Serrano_P/0/1/0/all/0/1"&gt;Patricio Mas_Serrano&lt;/a&gt; (2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Boquera_M/0/1/0/all/0/1"&gt;Mari L. Boquera&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Salinas_J/0/1/0/all/0/1"&gt;Jose M. Salinas&lt;/a&gt; (4) ((1) MedBravo, (2) Hospital General Universitario de Alicante Spain -HGUA, (3) Institute for Health and Biomedical Research of Alicante -ISABIAL, (4) Department of Health Informatics, Hospital Universitario San Juan de Alicante Spain)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tourbillon: a Physically Plausible Neural Architecture. (arXiv:2107.06424v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06424</id>
        <link href="http://arxiv.org/abs/2107.06424"/>
        <updated>2021-07-22T02:03:12.221Z</updated>
        <summary type="html"><![CDATA[In a physical neural system, backpropagation is faced with a number of
obstacles including: the need for labeled data, the violation of the locality
learning principle, the need for symmetric connections, and the lack of
modularity. Tourbillon is a new architecture that addresses all these
limitations. At its core, it consists of a stack of circular autoencoders
followed by an output layer. The circular autoencoders are trained in
self-supervised mode by recirculation algorithms and the top layer in
supervised mode by stochastic gradient descent, with the option of propagating
error information through the entire stack using non-symmetric connections.
While the Tourbillon architecture is meant primarily to address physical
constraints, and not to improve current engineering applications of deep
learning, we demonstrate its viability on standard benchmark datasets including
MNIST, Fashion MNIST, and CIFAR10. We show that Tourbillon can achieve
comparable performance to models trained with backpropagation and outperform
models that are trained with other physically plausible algorithms, such as
feedback alignment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1"&gt;Mohammadamin Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadowsk_P/0/1/0/all/0/1"&gt;Peter Sadowsk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Latent Causal Variables via Mechanism Sparsity: A New Principle for Nonlinear ICA. (arXiv:2107.10098v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10098</id>
        <link href="http://arxiv.org/abs/2107.10098"/>
        <updated>2021-07-22T02:03:12.213Z</updated>
        <summary type="html"><![CDATA[It can be argued that finding an interpretable low-dimensional representation
of a potentially high-dimensional phenomenon is central to the scientific
enterprise. Independent component analysis (ICA) refers to an ensemble of
methods which formalize this goal and provide estimation procedure for
practical application. This work proposes mechanism sparsity regularization as
a new principle to achieve nonlinear ICA when latent factors depend sparsely on
observed auxiliary variables and/or past latent factors. We show that the
latent variables can be recovered up to a permutation if one regularizes the
latent mechanisms to be sparse and if some graphical criterion is satisfied by
the data generating process. As a special case, our framework shows how one can
leverage unknown-target interventions on the latent factors to disentangle
them, thus drawing further connections between ICA and causality. We validate
our theoretical results with toy experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lachapelle_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Lachapelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lopez_P/0/1/0/all/0/1"&gt;Pau Rodr&amp;#xed;guez L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Priol_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Le Priol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lacoste_A/0/1/0/all/0/1"&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Iterative 2D/3D Registration. (arXiv:2107.10004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10004</id>
        <link href="http://arxiv.org/abs/2107.10004"/>
        <updated>2021-07-22T02:03:12.195Z</updated>
        <summary type="html"><![CDATA[Deep Learning-based 2D/3D registration methods are highly robust but often
lack the necessary registration accuracy for clinical application. A refinement
step using the classical optimization-based 2D/3D registration method applied
in combination with Deep Learning-based techniques can provide the required
accuracy. However, it also increases the runtime. In this work, we propose a
novel Deep Learning driven 2D/3D registration framework that can be used
end-to-end for iterative registration tasks without relying on any further
refinement step. We accomplish this by learning the update step of the 2D/3D
registration framework using Point-to-Plane Correspondences. The update step is
learned using iterative residual refinement-based optical flow estimation, in
combination with the Point-to-Plane correspondence solver embedded as a known
operator. Our proposed method achieves an average runtime of around 8s, a mean
re-projection distance error of 0.60 $\pm$ 0.40 mm with a success ratio of 97
percent and a capture range of 60 mm. The combination of high registration
accuracy, high robustness, and fast runtime makes our solution ideal for
clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaganathan_S/0/1/0/all/0/1"&gt;Srikrishna Jaganathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borsdorf_A/0/1/0/all/0/1"&gt;Anja Borsdorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1"&gt;Karthik Shetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations. (arXiv:2107.10209v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10209</id>
        <link href="http://arxiv.org/abs/2107.10209"/>
        <updated>2021-07-22T02:03:12.177Z</updated>
        <summary type="html"><![CDATA[We present polynomial time and sample efficient algorithms for learning an
unknown depth-2 feedforward neural network with general ReLU activations, under
mild non-degeneracy assumptions. In particular, we consider learning an unknown
network of the form $f(x) = {a}^{\mathsf{T}}\sigma({W}^\mathsf{T}x+b)$, where
$x$ is drawn from the Gaussian distribution, and $\sigma(t) := \max(t,0)$ is
the ReLU activation. Prior works for learning networks with ReLU activations
assume that the bias $b$ is zero. In order to deal with the presence of the
bias terms, our proposed algorithm consists of robustly decomposing multiple
higher order tensors arising from the Hermite expansion of the function $f(x)$.
Using these ideas we also establish identifiability of the network parameters
under minimal assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1"&gt;Alex Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1"&gt;Aravindan Vijayaraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Issue Types on GitHub. (arXiv:2107.09936v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.09936</id>
        <link href="http://arxiv.org/abs/2107.09936"/>
        <updated>2021-07-22T02:03:12.170Z</updated>
        <summary type="html"><![CDATA[Software maintenance and evolution involves critical activities for the
success of software projects. To support such activities and keep code
up-to-date and error-free, software communities make use of issue trackers,
i.e., tools for signaling, handling, and addressing the issues occurring in
software systems. However, in popular projects, tens or hundreds of issue
reports are daily submitted. In this context, identifying the type of each
submitted report (e.g., bug report, feature request, etc.) would facilitate the
management and the prioritization of the issues to address. To support issue
handling activities, in this paper, we propose Ticket Tagger, a GitHub app
analyzing the issue title and description through machine learning techniques
to automatically recognize the types of reports submitted on GitHub and assign
labels to each issue accordingly. We empirically evaluated the tool's
prediction performance on about 30,000 GitHub issues. Our results show that the
Ticket Tagger can identify the correct labels to assign to GitHub issues with
reasonably high effectiveness. Considering these results and the fact that the
tool is designed to be easily integrated in the GitHub issue management
process, Ticket Tagger consists in a useful solution for developers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kallis_R/0/1/0/all/0/1"&gt;Rafael Kallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorbo_A/0/1/0/all/0/1"&gt;Andrea Di Sorbo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canfora_G/0/1/0/all/0/1"&gt;Gerardo Canfora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panichella_S/0/1/0/all/0/1"&gt;Sebastiano Panichella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Theorem Proving Components. (arXiv:2107.10034v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2107.10034</id>
        <link href="http://arxiv.org/abs/2107.10034"/>
        <updated>2021-07-22T02:03:12.132Z</updated>
        <summary type="html"><![CDATA[Saturation-style automated theorem provers (ATPs) based on the given clause
procedure are today the strongest general reasoners for classical first-order
logic. The clause selection heuristics in such systems are, however, often
evaluating clauses in isolation, ignoring other clauses. This has changed
recently by equipping the E/ENIGMA system with a graph neural network (GNN)
that chooses the next given clause based on its evaluation in the context of
previously selected clauses. In this work, we describe several algorithms and
experiments with ENIGMA, advancing the idea of contextual evaluation based on
learning important components of the graph of clauses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chvalovsky_K/0/1/0/all/0/1"&gt;Karel Chvalovsk&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakub%5Cr%7Bu%7Dv_J/0/1/0/all/0/1"&gt;Jan Jakub&amp;#x16f;v&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1"&gt;Miroslav Ol&amp;#x161;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1"&gt;Josef Urban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Fixed-Point Acceleration for Convex Optimization. (arXiv:2107.10254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10254</id>
        <link href="http://arxiv.org/abs/2107.10254"/>
        <updated>2021-07-22T02:03:12.124Z</updated>
        <summary type="html"><![CDATA[Fixed-point iterations are at the heart of numerical computing and are often
a computational bottleneck in real-time applications, which typically instead
need a fast solution of moderate accuracy. Classical acceleration methods for
fixed-point problems focus on designing algorithms with theoretical guarantees
that apply to any fixed-point problem. We present neural fixed-point
acceleration, a framework to automatically learn to accelerate convex
fixed-point problems that are drawn from a distribution, using ideas from
meta-learning and classical acceleration algorithms. We apply our framework to
SCS, the state-of-the-art solver for convex cone programming, and design models
and loss functions to overcome the challenges of learning over unrolled
optimization and acceleration instabilities. Our work brings neural
acceleration into any optimization problem expressible with CVXPY. The source
code behind this paper is available at
https://github.com/facebookresearch/neural-scs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1"&gt;Shobha Venkataraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1"&gt;Brandon Amos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting trajectory behaviour via machine-learned invariant manifolds. (arXiv:2107.10154v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2107.10154</id>
        <link href="http://arxiv.org/abs/2107.10154"/>
        <updated>2021-07-22T02:03:12.093Z</updated>
        <summary type="html"><![CDATA[In this paper we use support vector machines (SVM) to develop a machine
learning framework to discover the phase space structure that can distinguish
between distinct reaction pathways. The machine learning model is trained using
data from trajectories of Hamilton's equations but lends itself for use in
molecular dynamics simulation. The framework is specifically designed to
require minimal a priori knowledge of the dynamics in a system. We benchmark
our approach with a model Hamiltonian for the reaction of an ion and a molecule
due to Chesnavich consisting of two parts: a rigid, symmetric top representing
the $\text{CH}_3^{+}$ ion, and a mobile $\text{H}$ atom. We begin with
trajectories and use support vector machines to determine the boundaries
between initial conditions corresponding to different classes of trajectories.
We then show that these boundaries between different classes of trajectories
approximate invariant phase space structures of the same type observed in
earlier analyses of Chesnavich's model. Our approach is designed with
extensions to higher-dimensional applications in mind. SVM is known to work
well even with small amounts of data, therefore our approach is computationally
better suited than existing methods for high-dimensional systems and systems
where integrating trajectories is expensive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Krajnak_V/0/1/0/all/0/1"&gt;Vladim&amp;#xed;r Kraj&amp;#x148;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Naik_S/0/1/0/all/0/1"&gt;Shibabrat Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wiggins_S/0/1/0/all/0/1"&gt;Stephen Wiggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images. (arXiv:2107.09700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09700</id>
        <link href="http://arxiv.org/abs/2107.09700"/>
        <updated>2021-07-22T02:03:12.057Z</updated>
        <summary type="html"><![CDATA[Image synthesis via Generative Adversarial Networks (GANs) of
three-dimensional (3D) medical images has great potential that can be extended
to many medical applications, such as, image enhancement and disease
progression modeling. However, current GAN technologies for 3D medical image
synthesis need to be significantly improved to be readily adapted to real-world
medical problems. In this paper, we extend the state-of-the-art StyleGAN2
model, which natively works with two-dimensional images, to enable 3D image
synthesis. In addition to the image synthesis, we investigate the
controllability and interpretability of the 3D-StyleGAN via style vectors
inherited form the original StyleGAN2 that are highly suitable for medical
applications: (i) the latent space projection and reconstruction of unseen real
images, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and
feasibility with ~12,000 three-dimensional full brain MR T1 images, although it
can be applied to any 3D volumetric images. Furthermore, we explore different
configurations of hyperparameters to investigate potential improvement of the
image synthesis with larger networks. The codes and pre-trained networks are
available online: https://github.com/sh4174/3DStyleGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sungmin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Razvan Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bonkhoff_A/0/1/0/all/0/1"&gt;Anna K. Bonkhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bretzner_M/0/1/0/all/0/1"&gt;Martin Bretzner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rost_N/0/1/0/all/0/1"&gt;Natalia S. Rost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks. (arXiv:2107.09804v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.09804</id>
        <link href="http://arxiv.org/abs/2107.09804"/>
        <updated>2021-07-22T02:03:11.978Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) classifiers are powerful tools that drive a broad
spectrum of important applications, from image recognition to autonomous
vehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks
that affect virtually all state-of-the-art models. These attacks make small
imperceptible modifications to inputs that are sufficient to induce the DNNs to
produce the wrong classification.

In this paper we propose a novel, lightweight adversarial correction and/or
detection mechanism for image classifiers that relies on undervolting (running
a chip at a voltage that is slightly below its safe margin). We propose using
controlled undervolting of the chip running the inference process in order to
introduce a limited number of compute errors. We show that these errors disrupt
the adversarial input in a way that can be used either to correct the
classification or detect the input as adversarial. We evaluate the proposed
solution in an FPGA design and through software simulation. We evaluate 10
attacks on two popular DNNs and show an average detection rate of 80% to 95%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Saikat Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1"&gt;Mohammad Hossein Samavatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1"&gt;Kristin Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1"&gt;Radu Teodorescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Inductive Biases in Emergent Communication. (arXiv:2002.01335v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01335</id>
        <link href="http://arxiv.org/abs/2002.01335"/>
        <updated>2021-07-22T02:03:11.963Z</updated>
        <summary type="html"><![CDATA[In order to communicate, humans flatten a complex representation of ideas and
their attributes into a single word or a sentence. We investigate the impact of
representation learning in artificial agents by developing graph referential
games. We empirically show that agents parametrized by graph neural networks
develop a more compositional language compared to bag-of-words and sequence
models, which allows them to systematically generalize to new combinations of
familiar features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1"&gt;Agnieszka S&amp;#x142;owik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1"&gt;Mateja Jamnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holden_S/0/1/0/all/0/1"&gt;Sean B. Holden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph Neural Networks. (arXiv:2002.11867v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11867</id>
        <link href="http://arxiv.org/abs/2002.11867"/>
        <updated>2021-07-22T02:03:11.958Z</updated>
        <summary type="html"><![CDATA[Deep learning's success has been widely recognized in a variety of machine
learning tasks, including image classification, audio recognition, and natural
language processing. As an extension of deep learning beyond these domains,
graph neural networks (GNNs) are designed to handle the non-Euclidean
graph-structure which is intractable to previous deep learning techniques.
Existing GNNs are presented using various techniques, making direct comparison
and cross-reference more complex. Although existing studies categorize GNNs
into spatial-based and spectral-based techniques, there hasn't been a thorough
examination of their relationship. To close this gap, this study presents a
single framework that systematically incorporates most GNNs. We organize
existing GNNs into spatial and spectral domains, as well as expose the
connections within each domain. A review of spectral graph theory and
approximation theory builds a strong relationship across the spatial and
spectral domains in further investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1"&gt;Taoran Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Kaiqun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment. (arXiv:2106.14993v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14993</id>
        <link href="http://arxiv.org/abs/2106.14993"/>
        <updated>2021-07-22T02:03:11.878Z</updated>
        <summary type="html"><![CDATA[Many transfer problems require re-using previously optimal decisions for
solving new tasks, which suggests the need for learning algorithms that can
modify the mechanisms for choosing certain actions independently of those for
choosing others. However, there is currently no formalism nor theory for how to
achieve this kind of modular credit assignment. To answer this question, we
define modular credit assignment as a constraint on minimizing the algorithmic
mutual information among feedback signals for different decisions. We introduce
what we call the modularity criterion for testing whether a learning algorithm
satisfies this constraint by performing causal analysis on the algorithm
itself. We generalize the recently proposed societal decision-making framework
as a more granular formalism than the Markov decision process to prove that for
decision sequences that do not contain cycles, certain single-step temporal
difference action-value methods meet this criterion while all policy-gradient
methods do not. Empirical evidence suggests that such action-value methods are
more sample efficient than policy-gradient methods on transfer problems that
require only sparse changes to a sequence of previously optimal decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Michael Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Sidhant Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Model-Based Vehicle-Relocation Decisions for Real-Time Ride-Sharing: Hybridizing Learning and Optimization. (arXiv:2105.13461v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13461</id>
        <link href="http://arxiv.org/abs/2105.13461"/>
        <updated>2021-07-22T02:03:11.869Z</updated>
        <summary type="html"><![CDATA[Large-scale ride-sharing systems combine real-time dispatching and routing
optimization over a rolling time horizon with a model predictive control (MPC)
component that relocates idle vehicles to anticipate the demand. The MPC
optimization operates over a longer time horizon to compensate for the inherent
myopic nature of the real-time dispatching. These longer time horizons are
beneficial for the quality of relocation decisions but increase computational
complexity. Consequently, the ride-sharing operators are often forced to use a
relatively short time horizon. To address this computational challenge, this
paper proposes a hybrid approach that combines machine learning and
optimization. The machine-learning component learns the optimal solution to the
MPC on the aggregated level to overcome the sparsity and high-dimensionality of
the solution. The optimization component transforms the machine-learning
prediction back to the original granularity through a tractable transportation
model. As a consequence, the original NP-hard MPC problem is reduced to a
polynomial time prediction and optimization, which allows the ride-sharing
operators to consider a longer time horizon. Experimental results show that the
hybrid approach achieves significantly better service quality than the MPC
optimization in terms of average rider waiting time, due to its ability to
model a longer horizon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_E/0/1/0/all/0/1"&gt;Enpeng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05426</id>
        <link href="http://arxiv.org/abs/2106.05426"/>
        <updated>2021-07-22T02:03:11.861Z</updated>
        <summary type="html"><![CDATA[How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1"&gt;Richard Antonello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1"&gt;Javier Turek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1"&gt;Vy Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1"&gt;Alexander Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09729</id>
        <link href="http://arxiv.org/abs/2107.09729"/>
        <updated>2021-07-22T02:03:11.854Z</updated>
        <summary type="html"><![CDATA[We combine beam search with the probabilistic pruning technique of nucleus
sampling to create two deterministic nucleus search algorithms for natural
language generation. The first algorithm, p-exact search, locally prunes the
next-token distribution and performs an exact search over the remaining space.
The second algorithm, dynamic beam search, shrinks and expands the beam size
according to the entropy of the candidate's probability distribution. Despite
the probabilistic intuition behind nucleus search, experiments on machine
translation and summarization benchmarks show that both algorithms reach the
same performance levels as standard beam search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1"&gt;Uri Shaham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1"&gt;Omer Levy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMG Pattern Recognition via Bayesian Inference with Scale Mixture-Based Stochastic Generative Models. (arXiv:2107.09853v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09853</id>
        <link href="http://arxiv.org/abs/2107.09853"/>
        <updated>2021-07-22T02:03:11.824Z</updated>
        <summary type="html"><![CDATA[Electromyogram (EMG) has been utilized to interface signals for prosthetic
hands and information devices owing to its ability to reflect human motion
intentions. Although various EMG classification methods have been introduced
into EMG-based control systems, they do not fully consider the stochastic
characteristics of EMG signals. This paper proposes an EMG pattern
classification method incorporating a scale mixture-based generative model. A
scale mixture model is a stochastic EMG model in which the EMG variance is
considered as a random variable, enabling the representation of uncertainty in
the variance. This model is extended in this study and utilized for EMG pattern
classification. The proposed method is trained by variational Bayesian
learning, thereby allowing the automatic determination of the model complexity.
Furthermore, to optimize the hyperparameters of the proposed method with a
partial discriminative approach, a mutual information-based determination
method is introduced. Simulation and EMG analysis experiments demonstrated the
relationship between the hyperparameters and classification accuracy of the
proposed method as well as the validity of the proposed method. The comparison
using public EMG datasets revealed that the proposed method outperformed the
various conventional classifiers. These results indicated the validity of the
proposed method and its applicability to EMG-based control systems. In EMG
pattern recognition, a classifier based on a generative model that reflects the
stochastic characteristics of EMG signals can outperform the conventional
general-purpose classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Furui_A/0/1/0/all/0/1"&gt;Akira Furui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Igaue_T/0/1/0/all/0/1"&gt;Takuya Igaue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsuji_T/0/1/0/all/0/1"&gt;Toshio Tsuji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Latent Structures for Multimedia Recommendation. (arXiv:2104.09036v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09036</id>
        <link href="http://arxiv.org/abs/2104.09036"/>
        <updated>2021-07-22T02:03:11.794Z</updated>
        <summary type="html"><![CDATA[Multimedia content is of predominance in the modern Web era. Investigating
how users interact with multimodal items is a continuing concern within the
rapid development of recommender systems. The majority of previous work focuses
on modeling user-item interactions with multimodal features included as side
information. However, this scheme is not well-designed for multimedia
recommendation. Specifically, only collaborative item-item relationships are
implicitly modeled through high-order item-user-item relations. Considering
that items are associated with rich contents in multiple modalities, we argue
that the latent semantic item-item structures underlying these multimodal
contents could be beneficial for learning better item representations and
further boosting recommendation. To this end, we propose a LATent sTructure
mining method for multImodal reCommEndation, which we term LATTICE for brevity.
To be specific, in the proposed LATTICE model, we devise a novel modality-aware
structure learning layer, which learns item-item structures for each modality
and aggregates multiple modalities to obtain latent item graphs. Based on the
learned latent graphs, we perform graph convolutions to explicitly inject
high-order item affinities into item representations. These enriched item
representations can then be plugged into existing collaborative filtering
methods to make more accurate recommendations. Extensive experiments on three
real-world datasets demonstrate the superiority of our method over
state-of-the-art multimedia recommendation methods and validate the efficacy of
mining latent item-item relationships from multimodal features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold learning-based polynomial chaos expansions for high-dimensional surrogate models. (arXiv:2107.09814v1 [physics.data-an])]]></title>
        <id>http://arxiv.org/abs/2107.09814</id>
        <link href="http://arxiv.org/abs/2107.09814"/>
        <updated>2021-07-22T02:03:11.771Z</updated>
        <summary type="html"><![CDATA[In this work we introduce a manifold learning-based method for uncertainty
quantification (UQ) in systems describing complex spatiotemporal processes. Our
first objective is to identify the embedding of a set of high-dimensional data
representing quantities of interest of the computational or analytical model.
For this purpose, we employ Grassmannian diffusion maps, a two-step nonlinear
dimension reduction technique which allows us to reduce the dimensionality of
the data and identify meaningful geometric descriptions in a parsimonious and
inexpensive manner. Polynomial chaos expansion is then used to construct a
mapping between the stochastic input parameters and the diffusion coordinates
of the reduced space. An adaptive clustering technique is proposed to identify
an optimal number of clusters of points in the latent space. The similarity of
points allows us to construct a number of geometric harmonic emulators which
are finally utilized as a set of inexpensive pre-trained models to perform an
inverse map of realizations of latent features to the ambient space and thus
perform accurate out-of-sample predictions. Thus, the proposed method acts as
an encoder-decoder system which is able to automatically handle very
high-dimensional data while simultaneously operating successfully in the
small-data regime. The method is demonstrated on two benchmark problems and on
a system of advection-diffusion-reaction equations which model a first-order
chemical reaction between two species. In all test cases, the proposed method
is able to achieve highly accurate approximations which ultimately lead to the
significant acceleration of UQ tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kontolati_K/0/1/0/all/0/1"&gt;Katiana Kontolati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Loukrezis_D/0/1/0/all/0/1"&gt;Dimitrios Loukrezis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Santos_K/0/1/0/all/0/1"&gt;Ketson R. M. dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Giovanis_D/0/1/0/all/0/1"&gt;Dimitrios G. Giovanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shields_M/0/1/0/all/0/1"&gt;Michael D. Shields&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Alternating Least Squares: Practical Private Matrix Completion with Tighter Rates. (arXiv:2107.09802v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09802</id>
        <link href="http://arxiv.org/abs/2107.09802"/>
        <updated>2021-07-22T02:03:11.763Z</updated>
        <summary type="html"><![CDATA[We study the problem of differentially private (DP) matrix completion under
user-level privacy. We design a joint differentially private variant of the
popular Alternating-Least-Squares (ALS) method that achieves: i) (nearly)
optimal sample complexity for matrix completion (in terms of number of items,
users), and ii) the best known privacy/utility trade-off both theoretically, as
well as on benchmark data sets. In particular, we provide the first global
convergence analysis of ALS with noise introduced to ensure DP, and show that,
in comparison to the best known alternative (the Private Frank-Wolfe algorithm
by Jain et al. (2018)), our error bounds scale significantly better with
respect to the number of items and users, which is critical in practical
problems. Extensive validation on standard benchmarks demonstrate that the
algorithm, in combination with carefully designed sampling procedures, is
significantly more accurate than existing techniques, thus promising to be the
first practical DP embedding model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Steve Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krichene_W/0/1/0/all/0/1"&gt;Walid Krichene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rendle_S/0/1/0/all/0/1"&gt;Steffen Rendle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1"&gt;Abhradeep Thakurta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Loop-Invariant Synthesis via Reinforcement Learning. (arXiv:2107.09766v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.09766</id>
        <link href="http://arxiv.org/abs/2107.09766"/>
        <updated>2021-07-22T02:03:11.756Z</updated>
        <summary type="html"><![CDATA[Loop-invariant synthesis is the basis of every program verification
procedure. Due to its undecidability in general, a tool for invariant synthesis
necessarily uses heuristics. Despite the common belief that the design of
heuristics is vital for the effective performance of a verifier, little work
has been performed toward obtaining the optimal heuristics for each
invariant-synthesis tool. Instead, developers have hand-tuned the heuristics of
tools. This study demonstrates that we can effectively and automatically learn
a good heuristic via reinforcement learning for an invariant synthesizer PCSat.
Our experiment shows that PCSat combined with the heuristic learned by
reinforcement learning outperforms the state-of-the-art solvers for this task.
To the best of our knowledge, this is the first work that investigates learning
the heuristics of an invariant synthesis tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukada_T/0/1/0/all/0/1"&gt;Takeshi Tsukada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unno_H/0/1/0/all/0/1"&gt;Hiroshi Unno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekiyama_T/0/1/0/all/0/1"&gt;Taro Sekiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1"&gt;Kohei Suenaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating the performance of multi-objective optimization when learning Bayesian Networks. (arXiv:1808.01345v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.01345</id>
        <link href="http://arxiv.org/abs/1808.01345"/>
        <updated>2021-07-22T02:03:11.747Z</updated>
        <summary type="html"><![CDATA[Bayesian Networks have been widely used in the last decades in many fields,
to describe statistical dependencies among random variables. In general,
learning the structure of such models is a problem with considerable
theoretical interest that poses many challenges. On the one hand, it is a
well-known NP-complete problem, practically hardened by the huge search space
of possible solutions. On the other hand, the phenomenon of I-equivalence,
i.e., different graphical structures underpinning the same set of statistical
dependencies, may lead to multimodal fitness landscapes further hindering
maximum likelihood approaches to solve the task. In particular, we exploit the
NSGA-II multi-objective optimization procedure in order to explicitly account
for both the likelihood of a solution and the number of selected arcs, by
setting these as the two objective functions of the method. The aim of this
work is to investigate the behavior of NSGA-II and analyse the quality of its
solutions. We thus thoroughly examined the optimization results obtained on a
wide set of simulated data, by considering both the goodness of the inferred
solutions in terms of the objective functions values achieved, and by comparing
the retrieved structures with the ground truth, i.e., the networks used to
generate the target data. Our results show that NSGA-II can converge to
solutions characterized by better likelihood and less arcs than classic
approaches, although paradoxically characterized in many cases by a lower
similarity with the target network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cazzaniga_P/0/1/0/all/0/1"&gt;Paolo Cazzaniga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nobile_M/0/1/0/all/0/1"&gt;Marco S. Nobile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1"&gt;Daniele Ramazzotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuiltyWalker: Distance to illicit nodes in the Bitcoin network. (arXiv:2102.05373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05373</id>
        <link href="http://arxiv.org/abs/2102.05373"/>
        <updated>2021-07-22T02:03:11.738Z</updated>
        <summary type="html"><![CDATA[Money laundering is a global phenomenon with wide-reaching social and
economic consequences. Cryptocurrencies are particularly susceptible due to the
lack of control by authorities and their anonymity. Thus, it is important to
develop new techniques to detect and prevent illicit cryptocurrency
transactions. In our work, we propose new features based on the structure of
the graph and past labels to boost the performance of machine learning methods
to detect money laundering. Our method, GuiltyWalker, performs random walks on
the bitcoin transaction graph and computes features based on the distance to
illicit transactions. We combine these new features with features proposed by
Weber et al. and observe an improvement of about 5pp regarding illicit
classification. Namely, we observe that our proposed features are particularly
helpful during a black market shutdown, where the algorithm by Weber et al. was
low performing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_C/0/1/0/all/0/1"&gt;Catarina Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torres_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Maria In&amp;#xea;s Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aparicio_D/0/1/0/all/0/1"&gt;David Apar&amp;#xed;cio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Tiago Ascens&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1"&gt;Pedro Bizarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending against Reconstruction Attack in Vertical Federated Learning. (arXiv:2107.09898v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09898</id>
        <link href="http://arxiv.org/abs/2107.09898"/>
        <updated>2021-07-22T02:03:11.719Z</updated>
        <summary type="html"><![CDATA[Recently researchers have studied input leakage problems in Federated
Learning (FL) where a malicious party can reconstruct sensitive training inputs
provided by users from shared gradient. It raises concerns about FL since input
leakage contradicts the privacy-preserving intention of using FL. Despite a
relatively rich literature on attacks and defenses of input reconstruction in
Horizontal FL, input leakage and protection in vertical FL starts to draw
researcher's attention recently. In this paper, we study how to defend against
input leakage attacks in Vertical FL. We design an adversarial training-based
framework that contains three modules: adversarial reconstruction, noise
regularization, and distance correlation minimization. Those modules can not
only be employed individually but also applied together since they are
independent to each other. Through extensive experiments on a large-scale
industrial online advertising dataset, we show our framework is effective in
protecting input privacy while retaining the model utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiankai Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuanshun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Weihao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Junyuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent. (arXiv:2102.05855v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05855</id>
        <link href="http://arxiv.org/abs/2102.05855"/>
        <updated>2021-07-22T02:03:11.710Z</updated>
        <summary type="html"><![CDATA[What is the information leakage of an iterative learning algorithm about its
training data, when the internal state of the algorithm is \emph{not}
observable? How much is the contribution of each specific training epoch to the
final leakage? We study this problem for noisy gradient descent algorithms, and
model the \emph{dynamics} of R\'enyi differential privacy loss throughout the
training process. Our analysis traces a provably tight bound on the R\'enyi
divergence between the pair of probability distributions over parameters of
models with neighboring datasets. We prove that the privacy loss converges
exponentially fast, for smooth and strongly convex loss functions, which is a
significant improvement over composition theorems. For Lipschitz, smooth, and
strongly convex loss functions, we prove optimal utility for differential
privacy algorithms with a small gradient complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chourasia_R/0/1/0/all/0/1"&gt;Rishav Chourasia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jiayuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1"&gt;Reza Shokri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Window Detection In Facade Imagery: A Deep Learning Approach Using Mask R-CNN. (arXiv:2107.10006v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10006</id>
        <link href="http://arxiv.org/abs/2107.10006"/>
        <updated>2021-07-22T02:03:11.699Z</updated>
        <summary type="html"><![CDATA[The parsing of windows in building facades is a long-desired but challenging
task in computer vision. It is crucial to urban analysis, semantic
reconstruction, lifecycle analysis, digital twins, and scene parsing amongst
other building-related tasks that require high-quality semantic data. This
article investigates the usage of the mask R-CNN framework to be used for
window detection of facade imagery input. We utilize transfer learning to train
our proposed method on COCO weights with our own collected dataset of street
view images of facades to produce instance segmentations of our new window
class. Experimental results show that our suggested approach with a relatively
small dataset trains the network only with transfer learning and augmentation
achieves results on par with prior state-of-the-art window detection
approaches, even without post-optimization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nordmark_N/0/1/0/all/0/1"&gt;Nils Nordmark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayenew_M/0/1/0/all/0/1"&gt;Mola Ayenew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Tuning for Data-Efficient Deep Learning. (arXiv:2102.12903v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12903</id>
        <link href="http://arxiv.org/abs/2102.12903"/>
        <updated>2021-07-22T02:03:11.692Z</updated>
        <summary type="html"><![CDATA[Deep learning has made revolutionary advances to diverse applications in the
presence of large-scale labeled datasets. However, it is prohibitively
time-costly and labor-expensive to collect sufficient labeled data in most
realistic scenarios. To mitigate the requirement for labeled data,
semi-supervised learning (SSL) focuses on simultaneously exploring both labeled
and unlabeled data, while transfer learning (TL) popularizes a favorable
practice of fine-tuning a pre-trained model to the target data. A dilemma is
thus encountered: Without a decent pre-trained model to provide an implicit
regularization, SSL through self-training from scratch will be easily misled by
inaccurate pseudo-labels, especially in large-sized label space; Without
exploring the intrinsic structure of unlabeled data, TL through fine-tuning
from limited labeled data is at risk of under-transfer caused by model shift.
To escape from this dilemma, we present Self-Tuning to enable data-efficient
deep learning by unifying the exploration of labeled and unlabeled data and the
transfer of a pre-trained model, as well as a Pseudo Group Contrast (PGC)
mechanism to mitigate the reliance on pseudo-labels and boost the tolerance to
false labels. Self-Tuning outperforms its SSL and TL counterparts on five tasks
by sharp margins, e.g. it doubles the accuracy of fine-tuning on Cars with 15%
labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinghan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inductive Geometric Matrix Midranges. (arXiv:2006.01508v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01508</id>
        <link href="http://arxiv.org/abs/2006.01508"/>
        <updated>2021-07-22T02:03:11.685Z</updated>
        <summary type="html"><![CDATA[Covariance data as represented by symmetric positive definite (SPD) matrices
are ubiquitous throughout technical study as efficient descriptors of
interdependent systems. Euclidean analysis of SPD matrices, while
computationally fast, can lead to skewed and even unphysical interpretations of
data. Riemannian methods preserve the geometric structure of SPD data at the
cost of expensive eigenvalue computations. In this paper, we propose a
geometric method for unsupervised clustering of SPD data based on the Thompson
metric. This technique relies upon a novel "inductive midrange" centroid
computation for SPD data, whose properties are examined and numerically
confirmed. We demonstrate the incorporation of the Thompson metric and
inductive midrange into X-means and K-means++ clustering algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goffrier_G/0/1/0/all/0/1"&gt;Graham W. Van Goffrier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mostajeran_C/0/1/0/all/0/1"&gt;Cyrus Mostajeran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sepulchre_R/0/1/0/all/0/1"&gt;Rodolphe Sepulchre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few Shots Is All You Need: A Progressive Few Shot Learning Approach for Low Resource Handwriting Recognition. (arXiv:2107.10064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10064</id>
        <link href="http://arxiv.org/abs/2107.10064"/>
        <updated>2021-07-22T02:03:11.664Z</updated>
        <summary type="html"><![CDATA[Handwritten text recognition in low resource scenarios, such as manuscripts
with rare alphabets, is a challenging problem. The main difficulty comes from
the very few annotated data and the limited linguistic information (e.g.
dictionaries and language models). Thus, we propose a few-shot learning-based
handwriting recognition approach that significantly reduces the human labor
annotation process, requiring only few images of each alphabet symbol. First,
our model detects all symbols of a given alphabet in a textline image, then a
decoding step maps the symbol similarity scores to the final sequence of
transcribed symbols. Our model is first pretrained on synthetic line images
generated from any alphabet, even though different from the target domain. A
second training step is then applied to diminish the gap between the source and
target data. Since this retraining would require annotation of thousands of
handwritten symbols together with their bounding boxes, we propose to avoid
such human effort through an unsupervised progressive learning approach that
automatically assigns pseudo-labels to the non-annotated data. The evaluation
on different manuscript datasets show that our model can lead to competitive
results with a significant reduction in human effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1"&gt;Mohamed Ali Souibgui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1"&gt;Alicia Forn&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1"&gt;Yousri Kessentini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Megyesi_B/0/1/0/all/0/1"&gt;Be&amp;#xe1;ta Megyesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss. (arXiv:2107.09989v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09989</id>
        <link href="http://arxiv.org/abs/2107.09989"/>
        <updated>2021-07-22T02:03:11.657Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance imaging (MRI) is an important medical imaging modality,
but its acquisition speed is quite slow due to the physiological limitations.
Recently, super-resolution methods have shown excellent performance in
accelerating MRI. In some circumstances, it is difficult to obtain
high-resolution images even with prolonged scan time. Therefore, we proposed a
novel super-resolution method that uses a generative adversarial network (GAN)
with cyclic loss and attention mechanism to generate high-resolution MR images
from low-resolution MR images by a factor of 2. We implemented our model on
pelvic images from healthy subjects as training and validation data, while
those data from patients were used for testing. The MR dataset was obtained
using different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four
methods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison.
Structural similarity, peak signal to noise ratio, root mean square error, and
variance inflation factor were used as calculation indicators to evaluate the
performances of the proposed method. Various experimental results showed that
our method can better restore the details of the high-resolution MR image as
compared to the other methods. In addition, the reconstructed high-resolution
MR image can provide better lesion textures in the tumor patients, which is
promising to be used in clinical diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guangyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jun Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xiangrong Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning stable reduced-order models for hybrid twins. (arXiv:2106.03464v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03464</id>
        <link href="http://arxiv.org/abs/2106.03464"/>
        <updated>2021-07-22T02:03:11.649Z</updated>
        <summary type="html"><![CDATA[The concept of Hybrid Twin (HT) has recently received a growing interest
thanks to the availability of powerful machine learning techniques. This twin
concept combines physics-based models within a model-order reduction
framework-to obtain real-time feedback rates-and data science. Thus, the main
idea of the HT is to develop on-the-fly data-driven models to correct possible
deviations between measurements and physics-based model predictions. This paper
is focused on the computation of stable, fast and accurate corrections in the
Hybrid Twin framework. Furthermore, regarding the delicate and important
problem of stability, a new approach is proposed, introducing several
sub-variants and guaranteeing a low computational cost as well as the
achievement of a stable time-integration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sancarlos_A/0/1/0/all/0/1"&gt;Abel Sancarlos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cameron_M/0/1/0/all/0/1"&gt;Morgan Cameron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peuvedic_J/0/1/0/all/0/1"&gt;Jean-Marc Le Peuvedic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groulier_J/0/1/0/all/0/1"&gt;Juliette Groulier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duval_J/0/1/0/all/0/1"&gt;Jean-Louis Duval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1"&gt;Elias Cueto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1"&gt;Francisco Chinesta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\gamma$-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction. (arXiv:2010.14496v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14496</id>
        <link href="http://arxiv.org/abs/2010.14496"/>
        <updated>2021-07-22T02:03:11.642Z</updated>
        <summary type="html"><![CDATA[We introduce the $\gamma$-model, a predictive model of environment dynamics
with an infinite probabilistic horizon. Replacing standard single-step models
with $\gamma$-models leads to generalizations of the procedures central to
model-based control, including the model rollout and model-based value
estimation. The $\gamma$-model, trained with a generative reinterpretation of
temporal difference learning, is a natural continuous analogue of the successor
representation and a hybrid between model-free and model-based mechanisms. Like
a value function, it contains information about the long-term future; like a
standard predictive model, it is independent of task reward. We instantiate the
$\gamma$-model as both a generative adversarial network and normalizing flow,
discuss how its training reflects an inescapable tradeoff between training-time
and testing-time compounding errors, and empirically investigate its utility
for prediction and control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1"&gt;Michael Janner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Window Detection In Facade Imagery: A Deep Learning Approach Using Mask R-CNN. (arXiv:2107.10006v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10006</id>
        <link href="http://arxiv.org/abs/2107.10006"/>
        <updated>2021-07-22T02:03:11.629Z</updated>
        <summary type="html"><![CDATA[The parsing of windows in building facades is a long-desired but challenging
task in computer vision. It is crucial to urban analysis, semantic
reconstruction, lifecycle analysis, digital twins, and scene parsing amongst
other building-related tasks that require high-quality semantic data. This
article investigates the usage of the mask R-CNN framework to be used for
window detection of facade imagery input. We utilize transfer learning to train
our proposed method on COCO weights with our own collected dataset of street
view images of facades to produce instance segmentations of our new window
class. Experimental results show that our suggested approach with a relatively
small dataset trains the network only with transfer learning and augmentation
achieves results on par with prior state-of-the-art window detection
approaches, even without post-optimization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nordmark_N/0/1/0/all/0/1"&gt;Nils Nordmark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayenew_M/0/1/0/all/0/1"&gt;Mola Ayenew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computing Graph Neural Networks: A Survey from Algorithms to Accelerators. (arXiv:2010.00130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00130</id>
        <link href="http://arxiv.org/abs/2010.00130"/>
        <updated>2021-07-22T02:03:11.611Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have exploded onto the machine learning scene in
recent years owing to their capability to model and learn from graph-structured
data. Such an ability has strong implications in a wide variety of fields whose
data is inherently relational, for which conventional neural networks do not
perform well. Indeed, as recent reviews can attest, research in the area of
GNNs has grown rapidly and has lead to the development of a variety of GNN
algorithm variants as well as to the exploration of groundbreaking applications
in chemistry, neurology, electronics, or communication networks, among others.
At the current stage of research, however, the efficient processing of GNNs is
still an open challenge for several reasons. Besides of their novelty, GNNs are
hard to compute due to their dependence on the input graph, their combination
of dense and very sparse operations, or the need to scale to huge graphs in
some applications. In this context, this paper aims to make two main
contributions. On the one hand, a review of the field of GNNs is presented from
the perspective of computing. This includes a brief tutorial on the GNN
fundamentals, an overview of the evolution of the field in the last decade, and
a summary of operations carried out in the multiple phases of different GNN
algorithm variants. On the other hand, an in-depth analysis of current software
and hardware acceleration schemes is provided, from which a hardware-software,
graph-aware, and communication-centric vision for GNN accelerators is
distilled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abadal_S/0/1/0/all/0/1"&gt;Sergi Abadal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Akshay Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guirado_R/0/1/0/all/0/1"&gt;Robert Guirado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Alonso_J/0/1/0/all/0/1"&gt;Jorge L&amp;#xf3;pez-Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alarcon_E/0/1/0/all/0/1"&gt;Eduard Alarc&amp;#xf3;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning as One Big Sequence Modeling Problem. (arXiv:2106.02039v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02039</id>
        <link href="http://arxiv.org/abs/2106.02039"/>
        <updated>2021-07-22T02:03:11.604Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is typically concerned with estimating
single-step policies or single-step models, leveraging the Markov property to
factorize the problem in time. However, we can also view RL as a sequence
modeling problem, with the goal being to predict a sequence of actions that
leads to a sequence of high rewards. Viewed in this way, it is tempting to
consider whether powerful, high-capacity sequence prediction models that work
well in other domains, such as natural-language processing, can also provide
simple and effective solutions to the RL problem. To this end, we explore how
RL can be reframed as "one big sequence modeling" problem, using
state-of-the-art Transformer architectures to model distributions over
sequences of states, actions, and rewards. Addressing RL as a sequence modeling
problem significantly simplifies a range of design decisions: we no longer
require separate behavior policy constraints, as is common in prior work on
offline model-free RL, and we no longer require ensembles or other epistemic
uncertainty estimators, as is common in prior work on model-based RL. All of
these roles are filled by the same Transformer sequence model. In our
experiments, we demonstrate the flexibility of this approach across
long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and
offline RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1"&gt;Michael Janner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Slices for Sliced Stein Discrepancy. (arXiv:2102.03159v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03159</id>
        <link href="http://arxiv.org/abs/2102.03159"/>
        <updated>2021-07-22T02:03:11.596Z</updated>
        <summary type="html"><![CDATA[Sliced Stein discrepancy (SSD) and its kernelized variants have demonstrated
promising successes in goodness-of-fit tests and model learning in high
dimensions. Despite their theoretical elegance, their empirical performance
depends crucially on the search of optimal slicing directions to discriminate
between two distributions. Unfortunately, previous gradient-based optimisation
approaches for this task return sub-optimal results: they are computationally
expensive, sensitive to initialization, and they lack theoretical guarantees
for convergence. We address these issues in two steps. First, we provide
theoretical results stating that the requirement of using optimal slicing
directions in the kernelized version of SSD can be relaxed, validating the
resulting discrepancy with finite random slicing directions. Second, given that
good slicing directions are crucial for practical performance, we propose a
fast algorithm for finding such slicing directions based on ideas of active
sub-space construction and spectral decomposition. Experiments on
goodness-of-fit tests and model learning show that our approach achieves both
improved performance and faster convergence. Especially, we demonstrate a
14-80x speed-up in goodness-of-fit tests when comparing with gradient-based
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1"&gt;Wenbo Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaibo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Meta-Reinforcement Learning with Advantage Weighting. (arXiv:2008.06043v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06043</id>
        <link href="http://arxiv.org/abs/2008.06043"/>
        <updated>2021-07-22T02:03:11.586Z</updated>
        <summary type="html"><![CDATA[This paper introduces the offline meta-reinforcement learning (offline
meta-RL) problem setting and proposes an algorithm that performs well in this
setting. Offline meta-RL is analogous to the widely successful supervised
learning strategy of pre-training a model on a large batch of fixed,
pre-collected data (possibly from various tasks) and fine-tuning the model to a
new task with relatively little data. That is, in offline meta-RL, we
meta-train on fixed, pre-collected data from several tasks in order to adapt to
a new task with a very small amount (less than 5 trajectories) of data from the
new task. By nature of being offline, algorithms for offline meta-RL can
utilize the largest possible pool of training data available and eliminate
potentially unsafe or costly data collection during meta-training. This setting
inherits the challenges of offline RL, but it differs significantly because
offline RL does not generally consider a) transfer to new tasks or b) limited
data from the test task, both of which we face in offline meta-RL. Targeting
the offline meta-RL setting, we propose Meta-Actor Critic with Advantage
Weighting (MACAW), an optimization-based meta-learning algorithm that uses
simple, supervised regression objectives for both the inner and outer loop of
meta-training. On offline variants of common meta-RL benchmarks, we empirically
find that this approach enables fully offline meta-reinforcement learning and
achieves notable gains over prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1"&gt;Eric Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1"&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xue Bin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03733</id>
        <link href="http://arxiv.org/abs/2105.03733"/>
        <updated>2021-07-22T02:03:11.579Z</updated>
        <summary type="html"><![CDATA[Model-free deep reinforcement learning has achieved great success in many
domains, such as video games, recommendation systems and robotic control tasks.
In continuous control tasks, widely used policies with Gaussian distributions
results in ineffective exploration of environments and limited performance of
algorithms in many cases. In this paper, we propose a density-free off-policy
algorithm, Generative Actor-Critic(GAC), using the push-forward model to
increase the expressiveness of policies, which also includes an entropy-like
technique, MMD-entropy regularizer, to balance the exploration and
exploitation. Additionnally, we devise an adaptive mechanism to automatically
scale this regularizer, which further improves the stability and robustness of
GAC. The experiment results show that push-forward policies possess desirable
features, such as multi-modality, which can improve the efficiency of
exploration and asymptotic performance of algorithms obviously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Lingwei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Hui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhebang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Model Pruning of Convolutional Networks on Tensor Processing Units. (arXiv:2107.04191v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04191</id>
        <link href="http://arxiv.org/abs/2107.04191"/>
        <updated>2021-07-22T02:03:11.558Z</updated>
        <summary type="html"><![CDATA[The deployment of convolutional neural networks is often hindered by high
computational and storage requirements. Structured model pruning is a promising
approach to alleviate these requirements. Using the VGG-16 model as an example,
we measure the accuracy-efficiency trade-off for various structured model
pruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units
(TPUs). To measure the actual performance of models, we develop a structured
model pruning library for TensorFlow2 to modify models in place (instead of
adding mask layers). We show that structured model pruning can significantly
improve model memory usage and speed on TPUs without losing accuracy,
especially for small datasets (e.g., CIFAR-10).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kongtao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franko_K/0/1/0/all/0/1"&gt;Ken Franko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_R/0/1/0/all/0/1"&gt;Ruoxin Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formalizing Distribution Inference Risks. (arXiv:2106.03699v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03699</id>
        <link href="http://arxiv.org/abs/2106.03699"/>
        <updated>2021-07-22T02:03:11.549Z</updated>
        <summary type="html"><![CDATA[Property inference attacks reveal statistical properties about a training set
but are difficult to distinguish from the primary purposes of statistical
machine learning, which is to produce models that capture statistical
properties about a distribution. Motivated by Yeom et al.'s membership
inference framework, we propose a formal and generic definition of property
inference attacks. The proposed notion describes attacks that can distinguish
between possible training distributions, extending beyond previous property
inference attacks that infer the ratio of a particular type of data in the
training data set. In this paper, we show how our definition captures previous
property inference attacks as well as a new attack that reveals the average
degree of nodes of a training graph and report on experiments giving insight
into the potential risks of property inference attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suri_A/0/1/0/all/0/1"&gt;Anshuman Suri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1"&gt;David Evans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning. (arXiv:2005.02356v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02356</id>
        <link href="http://arxiv.org/abs/2005.02356"/>
        <updated>2021-07-22T02:03:11.528Z</updated>
        <summary type="html"><![CDATA[We consider the problem of maximizing the $\ell_1$ norm of a linear map over
the sphere, which arises in various machine learning applications such as
orthogonal dictionary learning (ODL) and robust subspace recovery (RSR). The
problem is numerically challenging due to its nonsmooth objective and nonconvex
constraint, and its algorithmic aspects have not been well explored. In this
paper, we show how the manifold structure of the sphere can be exploited to
design fast algorithms for tackling this problem. Specifically, our
contribution is threefold. First, we present a manifold proximal point
algorithm (ManPPA) for the problem and show that it converges at a sublinear
rate. Furthermore, we show that ManPPA can achieve a quadratic convergence rate
when applied to the ODL and RSR problems. Second, we propose a stochastic
variant of ManPPA called StManPPA, which is well suited for large-scale
computation, and establish its sublinear convergence rate. Both ManPPA and
StManPPA have provably faster convergence rates than existing subgradient-type
methods. Third, using ManPPA as a building block, we propose a new approach to
solving a matrix analog of the problem, in which the sphere is replaced by the
Stiefel manifold. The results from our extensive numerical experiments on the
ODL and RSR problems demonstrate the efficiency and efficacy of our proposed
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zengde Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1"&gt;Anthony Man-Cho So&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Counterfactual Fairness in Recommendation. (arXiv:2105.09829v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09829</id>
        <link href="http://arxiv.org/abs/2105.09829"/>
        <updated>2021-07-22T02:03:11.520Z</updated>
        <summary type="html"><![CDATA[Recommender systems are gaining increasing and critical impacts on human and
society since a growing number of users use them for information seeking and
decision making. Therefore, it is crucial to address the potential unfairness
problems in recommendations. Just like users have personalized preferences on
items, users' demands for fairness are also personalized in many scenarios.
Therefore, it is important to provide personalized fair recommendations for
users to satisfy their personalized fairness demands. Besides, previous works
on fair recommendation mainly focus on association-based fairness. However, it
is important to advance from associative fairness notions to causal fairness
notions for assessing fairness more properly in recommender systems. Based on
the above considerations, this paper focuses on achieving personalized
counterfactual fairness for users in recommender systems. To this end, we
introduce a framework for achieving counterfactually fair recommendations
through adversary learning by generating feature-independent user embeddings
for recommendation. The framework allows recommender systems to achieve
personalized fairness for users while also covering non-personalized
situations. Experiments on two real-world datasets with shallow and deep
recommendation algorithms show that our method can generate fairer
recommendations for users with a desirable recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leverage Score Sampling for Complete Mode Coverage in Generative Adversarial Networks. (arXiv:2104.02373v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02373</id>
        <link href="http://arxiv.org/abs/2104.02373"/>
        <updated>2021-07-22T02:03:11.513Z</updated>
        <summary type="html"><![CDATA[Commonly, machine learning models minimize an empirical expectation. As a
result, the trained models typically perform well for the majority of the data
but the performance may deteriorate in less dense regions of the dataset. This
issue also arises in generative modeling. A generative model may overlook
underrepresented modes that are less frequent in the empirical data
distribution. This problem is known as complete mode coverage. We propose a
sampling procedure based on ridge leverage scores which significantly improves
mode coverage when compared to standard methods and can easily be combined with
any GAN. Ridge leverage scores are computed by using an explicit feature map,
associated with the next-to-last layer of a GAN discriminator or of a
pre-trained network, or by using an implicit feature map corresponding to a
Gaussian kernel. Multiple evaluations against recent approaches of complete
mode coverage show a clear improvement when using the proposed sampling
strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schreurs_J/0/1/0/all/0/1"&gt;Joachim Schreurs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meulemeester_H/0/1/0/all/0/1"&gt;Hannes De Meulemeester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Fanuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_B/0/1/0/all/0/1"&gt;Bart De Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1"&gt;Johan A.K. Suykens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information-Theoretic Generalization Bounds for Stochastic Gradient Descent. (arXiv:2102.00931v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00931</id>
        <link href="http://arxiv.org/abs/2102.00931"/>
        <updated>2021-07-22T02:03:11.505Z</updated>
        <summary type="html"><![CDATA[We study the generalization properties of the popular stochastic optimization
method known as stochastic gradient descent (SGD) for optimizing general
non-convex loss functions. Our main contribution is providing upper bounds on
the generalization error that depend on local statistics of the stochastic
gradients evaluated along the path of iterates calculated by SGD. The key
factors our bounds depend on are the variance of the gradients (with respect to
the data distribution) and the local smoothness of the objective function along
the SGD path, and the sensitivity of the loss function to perturbations to the
final output. Our key technical tool is combining the information-theoretic
generalization bounds previously used for analyzing randomized variants of SGD
with a perturbation analysis of the iterates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1"&gt;Gergely Neu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Depth Completion from Visual Inertial Odometry. (arXiv:1905.08616v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.08616</id>
        <link href="http://arxiv.org/abs/1905.08616"/>
        <updated>2021-07-22T02:03:11.485Z</updated>
        <summary type="html"><![CDATA[We describe a method to infer dense depth from camera motion and sparse depth
as estimated using a visual-inertial odometry system. Unlike other scenarios
using point clouds from lidar or structured light sensors, we have few hundreds
to few thousand points, insufficient to inform the topology of the scene. Our
method first constructs a piecewise planar scaffolding of the scene, and then
uses it to infer dense depth using the image along with the sparse points. We
use a predictive cross-modal criterion, akin to `self-supervision,' measuring
photometric consistency across time, forward-backward pose consistency, and
geometric compatibility with the sparse point cloud. We also launch the first
visual-inertial + depth dataset, which we hope will foster additional
exploration into combining the complementary strengths of visual and inertial
sensors. To compare our method to prior work, we adopt the unsupervised KITTI
depth completion benchmark, and show state-of-the-art performance on it. Code
available at:
https://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1"&gt;Xiaohan Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1"&gt;Stephanie Tsuei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Production Systems. (arXiv:2103.01937v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01937</id>
        <link href="http://arxiv.org/abs/2103.01937"/>
        <updated>2021-07-22T02:03:11.474Z</updated>
        <summary type="html"><![CDATA[Visual environments are structured, consisting of distinct objects or
entities. These entities have properties -- both visible and latent -- that
determine the manner in which they interact with one another. To partition
images into entities, deep-learning researchers have proposed structural
inductive biases such as slot-based architectures. To model interactions among
entities, equivariant graph neural nets (GNNs) are used, but these are not
particularly well suited to the task for two reasons. First, GNNs do not
predispose interactions to be sparse, as relationships among independent
entities are likely to be. Second, GNNs do not factorize knowledge about
interactions in an entity-conditional manner. As an alternative, we take
inspiration from cognitive science and resurrect a classic approach, production
systems, which consist of a set of rule templates that are applied by binding
placeholder variables in the rules to specific entities. Rules are scored on
their match to entities, and the best fitting rules are applied to update
entity properties. In a series of experiments, we demonstrate that this
architecture achieves a flexible, dynamic flow of control and serves to
factorize entity-specific and rule-based information. This disentangling of
knowledge achieves robust future-state prediction in rich visual environments,
outperforming state-of-the-art methods using GNNs, and allows for the
extrapolation from simple (few object) environments to more complex
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1"&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Didolkar_A/0/1/0/all/0/1"&gt;Aniket Didolkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_N/0/1/0/all/0/1"&gt;Nan Rosemary Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beaudoin_P/0/1/0/all/0/1"&gt;Philippe Beaudoin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael Mozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution of Classification Margins: Are All Data Equal?. (arXiv:2107.10199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10199</id>
        <link href="http://arxiv.org/abs/2107.10199"/>
        <updated>2021-07-22T02:03:11.455Z</updated>
        <summary type="html"><![CDATA[Recent theoretical results show that gradient descent on deep neural networks
under exponential loss functions locally maximizes classification margin, which
is equivalent to minimizing the norm of the weight matrices under margin
constraints. This property of the solution however does not fully characterize
the generalization performance. We motivate theoretically and show empirically
that the area under the curve of the margin distribution on the training set is
in fact a good measure of generalization. We then show that, after data
separation is achieved, it is possible to dynamically reduce the training set
by more than 99% without significant loss of performance. Interestingly, the
resulting subset of "high capacity" features is not consistent across different
training runs, which is consistent with the theoretical claim that all training
points should converge to the same asymptotic margin under SGD and in the
presence of both batch normalization and weight decay.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banburski_A/0/1/0/all/0/1"&gt;Andrzej Banburski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1"&gt;Fernanda De La Torre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pant_N/0/1/0/all/0/1"&gt;Nishka Pant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shastri_I/0/1/0/all/0/1"&gt;Ishana Shastri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poggio_T/0/1/0/all/0/1"&gt;Tomaso Poggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-OCR Paragraph Recognition by Graph Convolutional Networks. (arXiv:2101.12741v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12741</id>
        <link href="http://arxiv.org/abs/2101.12741"/>
        <updated>2021-07-22T02:03:11.448Z</updated>
        <summary type="html"><![CDATA[Paragraphs are an important class of document entities. We propose a new
approach for paragraph identification by spatial graph convolutional neural
networks (GCN) applied on OCR text boxes. Two steps, namely line splitting and
line clustering, are performed to extract paragraphs from the lines in OCR
results. Each step uses a beta-skeleton graph constructed from bounding boxes,
where the graph edges provide efficient support for graph convolution
operations. With only pure layout input features, the GCN model size is 3~4
orders of magnitude smaller compared to R-CNN based models, while achieving
comparable or better accuracies on PubLayNet and other datasets. Furthermore,
the GCN models show good generalization from synthetic training data to
real-world images, and good adaptivity for variable document styles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renshen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1"&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1"&gt;Ashok C. Popat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Control Variates for Monte Carlo Methods via Stochastic Optimization. (arXiv:2006.07487v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07487</id>
        <link href="http://arxiv.org/abs/2006.07487"/>
        <updated>2021-07-22T02:03:11.427Z</updated>
        <summary type="html"><![CDATA[Control variates are a well-established tool to reduce the variance of Monte
Carlo estimators. However, for large-scale problems including high-dimensional
and large-sample settings, their advantages can be outweighed by a substantial
computational cost. This paper considers control variates based on Stein
operators, presenting a framework that encompasses and generalizes existing
approaches that use polynomials, kernels and neural networks. A learning
strategy based on minimising a variational objective through stochastic
optimization is proposed, leading to scalable and effective control variates.
Novel theoretical results are presented to provide insight into the variance
reduction that can be achieved, and an empirical assessment, including
applications to Bayesian inference, is provided in support.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Si_S/0/1/0/all/0/1"&gt;Shijing Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1"&gt;Chris. J. Oates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duncan_A/0/1/0/all/0/1"&gt;Andrew B. Duncan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms. (arXiv:2107.10110v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.10110</id>
        <link href="http://arxiv.org/abs/2107.10110"/>
        <updated>2021-07-22T02:03:11.420Z</updated>
        <summary type="html"><![CDATA[Zeroth-order (ZO) optimization is widely used to handle challenging tasks,
such as query-based black-box adversarial attacks and reinforcement learning.
Various attempts have been made to integrate prior information into the
gradient estimation procedure based on finite differences, with promising
empirical results. However, their convergence properties are not well
understood. This paper makes an attempt to fill this gap by analyzing the
convergence of prior-guided ZO algorithms under a greedy descent framework with
various gradient estimators. We provide a convergence guarantee for the
prior-guided random gradient-free (PRGF) algorithms. Moreover, to further
accelerate over greedy descent methods, we present a new accelerated random
search (ARS) algorithm that incorporates prior information, together with a
convergence analysis. Finally, our theoretical results are confirmed by
experiments on several numerical benchmarks as well as adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Shuyu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guoqiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparseDNN: Fast Sparse Deep Learning Inference on CPUs. (arXiv:2101.07948v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07948</id>
        <link href="http://arxiv.org/abs/2101.07948"/>
        <updated>2021-07-22T02:03:11.399Z</updated>
        <summary type="html"><![CDATA[The last few years have seen gigantic leaps in algorithms and systems to
support efficient deep learning inference. Pruning and quantization algorithms
can now consistently compress neural networks by an order of magnitude. For a
compressed neural network, a multitude of inference frameworks have been
designed to maximize the performance of the target hardware. While we find
mature support for quantized neural networks in production frameworks such as
OpenVINO and MNN, support for pruned sparse neural networks is still lacking.
To tackle this challenge, we present SparseDNN, a sparse deep learning
inference engine targeting CPUs. We present both kernel-level optimizations
with a sparse code generator to accelerate sparse operators and novel
network-level optimizations catering to sparse networks. We show that our
sparse code generator can achieve significant speedups over state-of-the-art
sparse and dense libraries. On end-to-end benchmarks such as Huggingface
pruneBERT, SparseDNN achieves up to 5x throughput improvement over dense
inference with state-of-the-art OpenVINO. Open source library at:
https://github.com/marsupialtail/sparsednn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Residual Mixture Models. (arXiv:2006.12063v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12063</id>
        <link href="http://arxiv.org/abs/2006.12063"/>
        <updated>2021-07-22T02:03:11.380Z</updated>
        <summary type="html"><![CDATA[We propose Deep Residual Mixture Models (DRMMs), a novel deep generative
model architecture. Compared to other deep models, DRMMs allow more flexible
conditional sampling: The model can be trained once with all variables, and
then used for sampling with arbitrary combinations of conditioning variables,
Gaussian priors, and (in)equality constraints. This provides new opportunities
for interactive and exploratory machine learning, where one should minimize the
user waiting for retraining a model. We demonstrate DRMMs in constrained
multi-limb inverse kinematics and controllable generation of animations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1"&gt;Perttu H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Martin Trapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saloheimo_T/0/1/0/all/0/1"&gt;Tuure Saloheimo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep4Air: A Novel Deep Learning Framework for Airport Airside Surveillance. (arXiv:2010.00806v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00806</id>
        <link href="http://arxiv.org/abs/2010.00806"/>
        <updated>2021-07-22T02:03:11.332Z</updated>
        <summary type="html"><![CDATA[An airport runway and taxiway (airside) area is a highly dynamic and complex
environment featuring interactions between different types of vehicles (speed
and dimension), under varying visibility and traffic conditions. Airport ground
movements are deemed safety-critical activities, and safe-separation procedures
must be maintained by Air Traffic Controllers (ATCs). Large airports with
complicated runway-taxiway systems use advanced ground surveillance systems.
However, these systems have inherent limitations and a lack of real-time
analytics. In this paper, we propose a novel computer-vision based framework,
namely "Deep4Air", which can not only augment the ground surveillance systems
via the automated visual monitoring of runways and taxiways for aircraft
location, but also provide real-time speed and distance analytics for aircraft
on runways and taxiways. The proposed framework includes an adaptive deep
neural network for efficiently detecting and tracking aircraft. The
experimental results show an average precision of detection and tracking of up
to 99.8% on simulated data with validations on surveillance videos from the
digital tower at George Bush Intercontinental Airport. The results also
demonstrate that "Deep4Air" can locate aircraft positions relative to the
airport runway and taxiway infrastructure with high accuracy. Furthermore,
aircraft speed and separation distance are monitored in real-time, providing
enhanced safety management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thai_P/0/1/0/all/0/1"&gt;Phat Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sameer Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lilith_N/0/1/0/all/0/1"&gt;Nimrod Lilith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1"&gt;Phu N. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thanh_B/0/1/0/all/0/1"&gt;Binh Nguyen Thanh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Approaches to Automated Flow Cytometry Diagnosis of Chronic Lymphocytic Leukemia. (arXiv:2107.09728v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09728</id>
        <link href="http://arxiv.org/abs/2107.09728"/>
        <updated>2021-07-22T02:03:11.310Z</updated>
        <summary type="html"><![CDATA[Flow cytometry is a technique that measures multiple fluorescence and light
scatter-associated parameters from individual cells as they flow a single file
through an excitation light source. These cells are labeled with antibodies to
detect various antigens and the fluorescence signals reflect antigen
expression. Interpretation of the multiparameter flow cytometry data is
laborious, time-consuming, and expensive. It involves manual interpretation of
cell distribution and pattern recognition on two-dimensional plots by highly
trained medical technologists and pathologists. Using various machine learning
algorithms, we attempted to develop an automated analysis for clinical flow
cytometry cases that would automatically classify normal and chronic
lymphocytic leukemia cases. We achieved the best success with the Gradient
Boosting. The XGBoost classifier achieved a specificity of 1.00 and a
sensitivity of 0.67, a negative predictive value of 0.75, a positive predictive
value of 1.00, and an overall accuracy of 0.83 in prospectively classifying
cases with malignancies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_A/0/1/0/all/0/1"&gt;Akum S. Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1"&gt;Loveleen C. Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mastorides_S/0/1/0/all/0/1"&gt;Stephen M. Mastorides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foulis_P/0/1/0/all/0/1"&gt;Philip R. Foulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeLand_L/0/1/0/all/0/1"&gt;Lauren A. DeLand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seifert_R/0/1/0/all/0/1"&gt;Robert P. Seifert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkowski_A/0/1/0/all/0/1"&gt;Andrew Borkowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MRZ code extraction from visa and passport documents using convolutional neural networks. (arXiv:2009.05489v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05489</id>
        <link href="http://arxiv.org/abs/2009.05489"/>
        <updated>2021-07-22T02:03:11.288Z</updated>
        <summary type="html"><![CDATA[Detecting and extracting information from Machine-Readable Zone (MRZ) on
passports and visas is becoming increasingly important for verifying document
authenticity. However, computer vision methods for performing similar tasks,
such as optical character recognition (OCR), fail to extract the MRZ given
digital images of passports with reasonable accuracy. We present a specially
designed model based on convolutional neural networks that is able to
successfully extract MRZ information from digital images of passports of
arbitrary orientation and size. Our model achieved 100% MRZ detection rate and
98.36% character recognition macro-f1 score on a passport and visa dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yichuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+James_H/0/1/0/all/0/1"&gt;Hailey James&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_O/0/1/0/all/0/1"&gt;Otkrist Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1"&gt;Dan Raviv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality-aware Mutual Learning for Multi-modal Medical Image Segmentation. (arXiv:2107.09842v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09842</id>
        <link href="http://arxiv.org/abs/2107.09842"/>
        <updated>2021-07-22T02:03:11.280Z</updated>
        <summary type="html"><![CDATA[Liver cancer is one of the most common cancers worldwide. Due to
inconspicuous texture changes of liver tumor, contrast-enhanced computed
tomography (CT) imaging is effective for the diagnosis of liver cancer. In this
paper, we focus on improving automated liver tumor segmentation by integrating
multi-modal CT images. To this end, we propose a novel mutual learning (ML)
strategy for effective and robust multi-modal liver tumor segmentation.
Different from existing multi-modal methods that fuse information from
different modalities by a single model, with ML, an ensemble of
modality-specific models learn collaboratively and teach each other to distill
both the characteristics and the commonality between high-level representations
of different modalities. The proposed ML not only enables the superiority for
multi-modal learning but can also handle missing modalities by transferring
knowledge from existing modalities to missing ones. Additionally, we present a
modality-aware (MA) module, where the modality-specific models are
interconnected and calibrated with attention weights for adaptive information
exchange. The proposed modality-aware mutual learning (MAML) method achieves
promising results for liver tumor segmentation on a large-scale clinical
dataset. Moreover, we show the efficacy and robustness of MAML for handling
missing modalities on both the liver tumor and public brain tumor (BRATS 2018)
datasets. Our code is available at https://github.com/YaoZhang93/MAML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiawei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1"&gt;Jiang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Cheng Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation Transformer Network. (arXiv:2004.06193v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.06193</id>
        <link href="http://arxiv.org/abs/2004.06193"/>
        <updated>2021-07-22T02:03:11.273Z</updated>
        <summary type="html"><![CDATA[The extraction of a scene graph with objects as nodes and mutual
relationships as edges is the basis for a deep understanding of image content.
Despite recent advances, such as message passing and joint classification, the
detection of visual relationships remains a challenging task due to sub-optimal
exploration of the mutual interaction among the visual objects. In this work,
we propose a novel transformer formulation for scene graph generation and
relation prediction. We leverage the encoder-decoder architecture of the
transformer for rich feature embedding of nodes and edges. Specifically, we
model the node-to-node interaction with the self-attention of the transformer
encoder and the edge-to-node interaction with the cross-attention of the
transformer decoder. Further, we introduce a novel positional embedding
suitable to handle edges in the decoder. Finally, our relation prediction
module classifies the directed relation from the learned node and edge
embedding. We name this architecture as Relation Transformer Network (RTN). On
the Visual Genome and GQA dataset, we have achieved an overall mean of 4.85%
and 3.1% point improvement in comparison with state-of-the-art methods. Our
experiments show that Relation Transformer can efficiently model context across
various datasets with small, medium, and large-scale relation classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1"&gt;Rajat Koner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1"&gt;Suprosanna Shit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Point Cloud Generative Model via Tree-Structured Graph Convolutions for 3D Brain Shape Reconstruction. (arXiv:2107.09923v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09923</id>
        <link href="http://arxiv.org/abs/2107.09923"/>
        <updated>2021-07-22T02:03:11.266Z</updated>
        <summary type="html"><![CDATA[Fusing medical images and the corresponding 3D shape representation can
provide complementary information and microstructure details to improve the
operational performance and accuracy in brain surgery. However, compared to the
substantial image data, it is almost impossible to obtain the intraoperative 3D
shape information by using physical methods such as sensor scanning, especially
in minimally invasive surgery and robot-guided surgery. In this paper, a
general generative adversarial network (GAN) architecture based on graph
convolutional networks is proposed to reconstruct the 3D point clouds (PCs) of
brains by using one single 2D image, thus relieving the limitation of acquiring
3D shape data during surgery. Specifically, a tree-structured generative
mechanism is constructed to use the latent vector effectively and transfer
features between hidden layers accurately. With the proposed generative model,
a spontaneous image-to-PC conversion is finished in real-time. Competitive
qualitative and quantitative experimental results have been achieved on our
model. In multiple evaluation methods, the proposed model outperforms another
common point cloud generative model PointOutNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bowen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1"&gt;Baiying Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Depth Completion from Visual Inertial Odometry. (arXiv:1905.08616v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.08616</id>
        <link href="http://arxiv.org/abs/1905.08616"/>
        <updated>2021-07-22T02:03:11.245Z</updated>
        <summary type="html"><![CDATA[We describe a method to infer dense depth from camera motion and sparse depth
as estimated using a visual-inertial odometry system. Unlike other scenarios
using point clouds from lidar or structured light sensors, we have few hundreds
to few thousand points, insufficient to inform the topology of the scene. Our
method first constructs a piecewise planar scaffolding of the scene, and then
uses it to infer dense depth using the image along with the sparse points. We
use a predictive cross-modal criterion, akin to `self-supervision,' measuring
photometric consistency across time, forward-backward pose consistency, and
geometric compatibility with the sparse point cloud. We also launch the first
visual-inertial + depth dataset, which we hope will foster additional
exploration into combining the complementary strengths of visual and inertial
sensors. To compare our method to prior work, we adopt the unsupervised KITTI
depth completion benchmark, and show state-of-the-art performance on it. Code
available at:
https://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1"&gt;Xiaohan Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1"&gt;Stephanie Tsuei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-dimensional Multivariate Time Series Forecasting in IoT Applications using Embedding Non-stationary Fuzzy Time Series. (arXiv:2107.09785v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09785</id>
        <link href="http://arxiv.org/abs/2107.09785"/>
        <updated>2021-07-22T02:03:11.237Z</updated>
        <summary type="html"><![CDATA[In Internet of things (IoT), data is continuously recorded from different
data sources and devices can suffer faults in their embedded electronics, thus
leading to a high-dimensional data sets and concept drift events. Therefore,
methods that are capable of high-dimensional non-stationary time series are of
great value in IoT applications. Fuzzy Time Series (FTS) models stand out as
data-driven non-parametric models of easy implementation and high accuracy.
Unfortunately, FTS encounters difficulties when dealing with data sets of many
variables and scenarios with concept drift. We present a new approach to handle
high-dimensional non-stationary time series, by projecting the original
high-dimensional data into a low dimensional embedding space and using FTS
approach. Combining these techniques enables a better representation of the
complex content of non-stationary multivariate time series and accurate
forecasts. Our model is able to explain 98% of the variance and reach 11.52% of
RMSE, 2.68% of MAE and 2.91% of MAPE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bitencourt_H/0/1/0/all/0/1"&gt;Hugo Vinicius Bitencourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guimaraes_F/0/1/0/all/0/1"&gt;Frederico Gadelha Guimar&amp;#xe3;es&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidential Deep Learning for Open Set Action Recognition. (arXiv:2107.10161v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10161</id>
        <link href="http://arxiv.org/abs/2107.10161"/>
        <updated>2021-07-22T02:03:11.226Z</updated>
        <summary type="html"><![CDATA[In a real-world scenario, human actions are typically out of the distribution
from training data, which requires a model to both recognize the known actions
and reject the unknown. Different from image data, video actions are more
challenging to be recognized in an open-set setting due to the uncertain
temporal dynamics and static bias of human actions. In this paper, we propose a
Deep Evidential Action Recognition (DEAR) method to recognize actions in an
open testing set. Specifically, we formulate the action recognition problem
from the evidential deep learning (EDL) perspective and propose a novel model
calibration method to regularize the EDL training. Besides, to mitigate the
static bias of video representation, we propose a plug-and-play module to
debias the learned representation through contrastive learning. Experimental
results show that our DEAR method achieves consistent performance gain on
multiple mainstream action recognition models and benchmarks. Codes and
pre-trained weights will be made available upon paper acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1"&gt;Wentao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Yu Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online structural kernel selection for mobile health. (arXiv:2107.09949v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09949</id>
        <link href="http://arxiv.org/abs/2107.09949"/>
        <updated>2021-07-22T02:03:11.218Z</updated>
        <summary type="html"><![CDATA[Motivated by the need for efficient and personalized learning in mobile
health, we investigate the problem of online kernel selection for Gaussian
Process regression in the multi-task setting. We propose a novel generative
process on the kernel composition for this purpose. Our method demonstrates
that trajectories of kernel evolutions can be transferred between users to
improve learning and that the kernels themselves are meaningful for an mHealth
prediction goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_E/0/1/0/all/0/1"&gt;Eura Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klasnja_P/0/1/0/all/0/1"&gt;Pedja Klasnja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1"&gt;Susan Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio Captioning Transformer. (arXiv:2107.09817v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.09817</id>
        <link href="http://arxiv.org/abs/2107.09817"/>
        <updated>2021-07-22T02:03:11.212Z</updated>
        <summary type="html"><![CDATA[Audio captioning aims to automatically generate a natural language
description of an audio clip. Most captioning models follow an encoder-decoder
architecture, where the decoder predicts words based on the audio features
extracted by the encoder. Convolutional neural networks (CNNs) and recurrent
neural networks (RNNs) are often used as the audio encoder. However, CNNs can
be limited in modelling temporal relationships among the time frames in an
audio signal, while RNNs can be limited in modelling the long-range
dependencies among the time frames. In this paper, we propose an Audio
Captioning Transformer (ACT), which is a full Transformer network based on an
encoder-decoder architecture and is totally convolution-free. The proposed
method has a better ability to model the global information within an audio
signal as well as capture temporal relationships between audio events. We
evaluate our model on AudioCaps, which is the largest audio captioning dataset
publicly available. Our model shows competitive performance compared to other
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mei_X/0/1/0/all/0/1"&gt;Xinhao Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xubo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiushi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Plumbley_M/0/1/0/all/0/1"&gt;Mark D. Plumbley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenwu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Estimation from Dependent Data. (arXiv:2107.09773v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09773</id>
        <link href="http://arxiv.org/abs/2107.09773"/>
        <updated>2021-07-22T02:03:11.191Z</updated>
        <summary type="html"><![CDATA[We consider a general statistical estimation problem wherein binary labels
across different observations are not independent conditioned on their feature
vectors, but dependent, capturing settings where e.g. these observations are
collected on a spatial domain, a temporal domain, or a social network, which
induce dependencies. We model these dependencies in the language of Markov
Random Fields and, importantly, allow these dependencies to be substantial, i.e
do not assume that the Markov Random Field capturing these dependencies is in
high temperature. As our main contribution we provide algorithms and
statistically efficient estimation rates for this model, giving several
instantiations of our bounds in logistic regression, sparse logistic
regression, and neural network settings with dependent data. Our estimation
guarantees follow from novel results for estimating the parameters (i.e.
external fields and interaction strengths) of Ising models from a {\em single}
sample. {We evaluate our estimation approach on real networked data, showing
that it outperforms standard regression approaches that ignore dependencies,
across three text classification datasets: Cora, Citeseer and Pubmed.}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1"&gt;Yuval Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1"&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dikkala_N/0/1/0/all/0/1"&gt;Nishanth Dikkala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandiros_A/0/1/0/all/0/1"&gt;Anthimos Vardis Kandiros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations. (arXiv:2104.01542v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01542</id>
        <link href="http://arxiv.org/abs/2104.01542"/>
        <updated>2021-07-22T02:03:11.182Z</updated>
        <summary type="html"><![CDATA[Grasp detection in clutter requires the robot to reason about the 3D scene
from incomplete and noisy perception. In this work, we draw insight that 3D
reconstruction and grasp learning are two intimately connected tasks, both of
which require a fine-grained understanding of local geometry details. We thus
propose to utilize the synergies between grasp affordance and 3D reconstruction
through multi-task learning of a shared representation. Our model takes
advantage of deep implicit functions, a continuous and memory-efficient
representation, to enable differentiable training of both tasks. We train the
model on self-supervised grasp trials data in simulation. Evaluation is
conducted on a clutter removal task, where the robot clears cluttered objects
by grasping them one at a time. The experimental results in simulation and on
the real robot have demonstrated that the use of implicit neural
representations and joint learning of grasp affordance and 3D reconstruction
have led to state-of-the-art grasping results. Our method outperforms baselines
by over 10% in terms of grasp success rate. Additional results and videos can
be found at https://sites.google.com/view/rpl-giga2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhenyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yifeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svetlik_M/0/1/0/all/0/1"&gt;Maxwell Svetlik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1"&gt;Kuan Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisuoSpatial Foresight for Physical Sequential Fabric Manipulation. (arXiv:2102.09754v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09754</id>
        <link href="http://arxiv.org/abs/2102.09754"/>
        <updated>2021-07-22T02:03:11.173Z</updated>
        <summary type="html"><![CDATA[Robotic fabric manipulation has applications in home robotics, textiles,
senior care and surgery. Existing fabric manipulation techniques, however, are
designed for specific tasks, making it difficult to generalize across different
but related tasks. We build upon the Visual Foresight framework to learn fabric
dynamics that can be efficiently reused to accomplish different sequential
fabric manipulation tasks with a single goal-conditioned policy. We extend our
earlier work on VisuoSpatial Foresight (VSF), which learns visual dynamics on
domain randomized RGB images and depth maps simultaneously and completely in
simulation. In this earlier work, we evaluated VSF on multi-step fabric
smoothing and folding tasks against 5 baseline methods in simulation and on the
da Vinci Research Kit (dVRK) surgical robot without any demonstrations at train
or test time. A key finding was that depth sensing significantly improves
performance: RGBD data yields an 80% improvement in fabric folding success rate
in simulation over pure RGB data. In this work, we vary 4 components of VSF,
including data generation, visual dynamics model, cost function, and
optimization procedure. Results suggest that training visual dynamics models
using longer, corner-based actions can improve the efficiency of fabric folding
by 76% and enable a physical sequential fabric folding task that VSF could not
previously perform with 90% reliability. Code, data, videos, and supplementary
material are available at https://sites.google.com/view/fabric-vsf/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoque_R/0/1/0/all/0/1"&gt;Ryan Hoque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seita_D/0/1/0/all/0/1"&gt;Daniel Seita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganapathi_A/0/1/0/all/0/1"&gt;Aditya Ganapathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanwani_A/0/1/0/all/0/1"&gt;Ajay Kumar Tanwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamali_N/0/1/0/all/0/1"&gt;Nawid Jamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamane_K/0/1/0/all/0/1"&gt;Katsu Yamane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iba_S/0/1/0/all/0/1"&gt;Soshi Iba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-OCR Paragraph Recognition by Graph Convolutional Networks. (arXiv:2101.12741v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12741</id>
        <link href="http://arxiv.org/abs/2101.12741"/>
        <updated>2021-07-22T02:03:11.166Z</updated>
        <summary type="html"><![CDATA[Paragraphs are an important class of document entities. We propose a new
approach for paragraph identification by spatial graph convolutional neural
networks (GCN) applied on OCR text boxes. Two steps, namely line splitting and
line clustering, are performed to extract paragraphs from the lines in OCR
results. Each step uses a beta-skeleton graph constructed from bounding boxes,
where the graph edges provide efficient support for graph convolution
operations. With only pure layout input features, the GCN model size is 3~4
orders of magnitude smaller compared to R-CNN based models, while achieving
comparable or better accuracies on PubLayNet and other datasets. Furthermore,
the GCN models show good generalization from synthetic training data to
real-world images, and good adaptivity for variable document styles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renshen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1"&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1"&gt;Ashok C. Popat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Green-Blue Stripe Pattern for Range Sensing from a Single Image. (arXiv:1701.02123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1701.02123</id>
        <link href="http://arxiv.org/abs/1701.02123"/>
        <updated>2021-07-22T02:03:11.158Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel method for rapid high-resolution range
sensing using green-blue stripe pattern. We use green and blue for designing
high-frequency stripe projection pattern. For accurate and reliable range
recovery, we identify the stripe patterns by our color-stripe segmentation and
unwrapping algorithms. The experimental result for a naked human face shows the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Je_C/0/1/0/all/0/1"&gt;Changsoo Je&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1"&gt;Kyuhyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang Wook Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton-based Relational Reasoning for Group Activity Analysis. (arXiv:2011.05653v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05653</id>
        <link href="http://arxiv.org/abs/2011.05653"/>
        <updated>2021-07-22T02:03:11.136Z</updated>
        <summary type="html"><![CDATA[Research on group activity recognition mostly leans on the standard
two-stream approach (RGB and Optical Flow) as their input features. Few have
explored explicit pose information, with none using it directly to reason about
the persons interactions. In this paper, we leverage the skeleton information
to learn the interactions between the individuals straight from it. With our
proposed method GIRN, multiple relationship types are inferred from independent
modules, that describe the relations between the body joints pair-by-pair.
Additionally to the joints relations, we also experiment with the previously
unexplored relationship between individuals and relevant objects (e.g.
volleyball). The individuals distinct relations are then merged through an
attention mechanism, that gives more importance to those individuals more
relevant for distinguishing the group activity. We evaluate our method in the
Volleyball dataset, obtaining competitive results to the state-of-the-art. Our
experiments demonstrate the potential of skeleton-based approaches for modeling
multi-person interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1"&gt;Mauricio Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1"&gt;Alex C. Kot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. (arXiv:2105.04165v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04165</id>
        <link href="http://arxiv.org/abs/2105.04165"/>
        <updated>2021-07-22T02:03:11.129Z</updated>
        <summary type="html"><![CDATA[Geometry problem solving has attracted much attention in the NLP community
recently. The task is challenging as it requires abstract problem understanding
and symbolic reasoning with axiomatic knowledge. However, current datasets are
either small in scale or not publicly available. Thus, we construct a new
large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with
dense annotation in formal language. We further propose a novel geometry
solving approach with formal language and symbolic reasoning, called
Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the
problem text and diagram into formal language automatically via rule-based text
parsing and neural object detecting, respectively. Unlike implicit learning in
existing methods, Inter-GPS incorporates theorem knowledge as conditional rules
and performs symbolic reasoning step by step. Also, a theorem predictor is
designed to infer the theorem application sequence fed to the symbolic solver
for the more efficient and reasonable searching path. Extensive experiments on
the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves
significant improvements over existing methods. The project with code and data
is available at https://lupantech.github.io/inter-gps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ran Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shibiao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06154</id>
        <link href="http://arxiv.org/abs/2107.06154"/>
        <updated>2021-07-22T02:03:11.110Z</updated>
        <summary type="html"><![CDATA[Due to the domain discrepancy in visual domain adaptation, the performance of
source model degrades when bumping into the high data density near decision
boundary in target domain. A common solution is to minimize the Shannon Entropy
to push the decision boundary away from the high density area. However, entropy
minimization also leads to severe reduction of prediction diversity, and
unfortunately brings harm to the domain adaptation. In this paper, we
investigate the prediction discriminability and diversity by studying the
structure of the classification output matrix of a randomly selected data
batch. We find by theoretical analysis that the prediction discriminability and
diversity could be separately measured by the Frobenius-norm and rank of the
batch output matrix. The nuclear-norm is an upperbound of the former, and a
convex approximation of the latter. Accordingly, we propose Batch Nuclear-norm
Maximization and Minimization, which performs nuclear-norm maximization on the
target output matrix to enhance the target prediction ability, and nuclear-norm
minimization on the source batch output matrix to increase applicability of the
source domain knowledge. We further approximate the nuclear-norm by
L_{1,2}-norm, and design multi-batch optimization for stable solution on large
number of categories. The fast approximation method achieves O(n^2)
computational complexity and better convergence property. Experiments show that
our method could boost the adaptation accuracy and robustness under three
typical domain adaptation scenarios. The code is available at
https://github.com/cuishuhao/BNM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HistoCartography: A Toolkit for Graph Analytics in Digital Pathology. (arXiv:2107.10073v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10073</id>
        <link href="http://arxiv.org/abs/2107.10073"/>
        <updated>2021-07-22T02:03:11.100Z</updated>
        <summary type="html"><![CDATA[Advances in entity-graph based analysis of histopathology images have brought
in a new paradigm to describe tissue composition, and learn the tissue
structure-to-function relationship. Entity-graphs offer flexible and scalable
representations to characterize tissue organization, while allowing the
incorporation of prior pathological knowledge to further support model
interpretability and explainability. However, entity-graph analysis requires
prerequisites for image-to-graph translation and knowledge of state-of-the-art
machine learning algorithms applied to graph-structured data, which can
potentially hinder their adoption. In this work, we aim to alleviate these
issues by developing HistoCartography, a standardized python API with necessary
preprocessing, machine learning and explainability tools to facilitate
graph-analytics in computational pathology. Further, we have benchmarked the
computational time and performance on multiple datasets across different
imaging types and histopathology tasks to highlight the applicability of the
API for building computational pathology workflows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jaume_G/0/1/0/all/0/1"&gt;Guillaume Jaume&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pati_P/0/1/0/all/0/1"&gt;Pushpak Pati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anklin_V/0/1/0/all/0/1"&gt;Valentin Anklin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Foncubierta_A/0/1/0/all/0/1"&gt;Antonio Foncubierta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabrani_M/0/1/0/all/0/1"&gt;Maria Gabrani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objective video quality metrics application to video codecs comparisons: choosing the best for subjective quality estimation. (arXiv:2107.10220v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.10220</id>
        <link href="http://arxiv.org/abs/2107.10220"/>
        <updated>2021-07-22T02:03:11.092Z</updated>
        <summary type="html"><![CDATA[Quality assessment plays a key role in creating and comparing video
compression algorithms. Despite the development of a large number of new
methods for assessing quality, generally accepted and well-known codecs
comparisons mainly use the classical methods like PSNR, SSIM and new method
VMAF. These methods can be calculated following different rules: they can use
different frame-by-frame averaging techniques or different summation of color
components. In this paper, a fundamental comparison of various versions of
generally accepted metrics is carried out to find the most relevant and
recommended versions of video quality metrics to be used in codecs comparisons.
For comparison, we used a set of videos encoded with video codecs of different
standards, and visual quality scores collected for the resulting set of streams
since 2018 until 2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1"&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yakovenko_A/0/1/0/all/0/1"&gt;Alexander Yakovenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safonov_N/0/1/0/all/0/1"&gt;Nickolay Safonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1"&gt;Dmitriy Kulikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gushin_A/0/1/0/all/0/1"&gt;Alexander Gushin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1"&gt;Dmitriy Vatolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning. (arXiv:2005.02356v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02356</id>
        <link href="http://arxiv.org/abs/2005.02356"/>
        <updated>2021-07-22T02:03:11.069Z</updated>
        <summary type="html"><![CDATA[We consider the problem of maximizing the $\ell_1$ norm of a linear map over
the sphere, which arises in various machine learning applications such as
orthogonal dictionary learning (ODL) and robust subspace recovery (RSR). The
problem is numerically challenging due to its nonsmooth objective and nonconvex
constraint, and its algorithmic aspects have not been well explored. In this
paper, we show how the manifold structure of the sphere can be exploited to
design fast algorithms for tackling this problem. Specifically, our
contribution is threefold. First, we present a manifold proximal point
algorithm (ManPPA) for the problem and show that it converges at a sublinear
rate. Furthermore, we show that ManPPA can achieve a quadratic convergence rate
when applied to the ODL and RSR problems. Second, we propose a stochastic
variant of ManPPA called StManPPA, which is well suited for large-scale
computation, and establish its sublinear convergence rate. Both ManPPA and
StManPPA have provably faster convergence rates than existing subgradient-type
methods. Third, using ManPPA as a building block, we propose a new approach to
solving a matrix analog of the problem, in which the sphere is replaced by the
Stiefel manifold. The results from our extensive numerical experiments on the
ODL and RSR problems demonstrate the efficiency and efficacy of our proposed
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zengde Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1"&gt;Anthony Man-Cho So&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D fluorescence microscopy data synthesis for segmentation and benchmarking. (arXiv:2107.10180v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.10180</id>
        <link href="http://arxiv.org/abs/2107.10180"/>
        <updated>2021-07-22T02:03:11.051Z</updated>
        <summary type="html"><![CDATA[Automated image processing approaches are indispensable for many biomedical
experiments and help to cope with the increasing amount of microscopy image
data in a fast and reproducible way. Especially state-of-the-art deep
learning-based approaches most often require large amounts of annotated
training data to produce accurate and generalist outputs, but they are often
compromised by the general lack of those annotated data sets. In this work, we
propose how conditional generative adversarial networks can be utilized to
generate realistic image data for 3D fluorescence microscopy from annotation
masks of 3D cellular structures. In combination with mask simulation
approaches, we demonstrate the generation of fully-annotated 3D microscopy data
sets that we make publicly available for training or benchmarking. An
additional positional conditioning of the cellular structures enables the
reconstruction of position-dependent intensity characteristics and allows to
generate image data of different quality levels. A patch-wise working principle
and a subsequent full-size reassemble strategy is used to generate image data
of arbitrary size and different organisms. We present this as a
proof-of-concept for the automated generation of fully-annotated training data
sets requiring only a minimum of manual interaction to alleviate the need of
manual annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Eschweiler_D/0/1/0/all/0/1"&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rethwisch_M/0/1/0/all/0/1"&gt;Malte Rethwisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jarchow_M/0/1/0/all/0/1"&gt;Mareike Jarchow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koppers_S/0/1/0/all/0/1"&gt;Simon Koppers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stegmaier_J/0/1/0/all/0/1"&gt;Johannes Stegmaier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation. (arXiv:2107.10189v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10189</id>
        <link href="http://arxiv.org/abs/2107.10189"/>
        <updated>2021-07-22T02:03:11.042Z</updated>
        <summary type="html"><![CDATA[Traffic accident anticipation aims to accurately and promptly predict the
occurrence of a future accident from dashcam videos, which is vital for a
safety-guaranteed self-driving system. To encourage an early and accurate
decision, existing approaches typically focus on capturing the cues of spatial
and temporal context before a future accident occurs. However, their
decision-making lacks visual explanation and ignores the dynamic interaction
with the environment. In this paper, we propose Deep ReInforced accident
anticipation with Visual Explanation, named DRIVE. The method simulates both
the bottom-up and top-down visual attention mechanism in a dashcam observation
environment so that the decision from the proposed stochastic multi-task agent
can be visually explained by attentive regions. Moreover, the proposed dense
anticipation reward and sparse fixation reward are effective in training the
DRIVE model with our improved reinforcement learning algorithm. Experimental
results show that the DRIVE model achieves state-of-the-art performance on
multiple real-world traffic accident datasets. The code and pre-trained model
will be available upon paper acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1"&gt;Wentao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Yu Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Latent Structures for Multimedia Recommendation. (arXiv:2104.09036v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09036</id>
        <link href="http://arxiv.org/abs/2104.09036"/>
        <updated>2021-07-22T02:03:11.029Z</updated>
        <summary type="html"><![CDATA[Multimedia content is of predominance in the modern Web era. Investigating
how users interact with multimodal items is a continuing concern within the
rapid development of recommender systems. The majority of previous work focuses
on modeling user-item interactions with multimodal features included as side
information. However, this scheme is not well-designed for multimedia
recommendation. Specifically, only collaborative item-item relationships are
implicitly modeled through high-order item-user-item relations. Considering
that items are associated with rich contents in multiple modalities, we argue
that the latent semantic item-item structures underlying these multimodal
contents could be beneficial for learning better item representations and
further boosting recommendation. To this end, we propose a LATent sTructure
mining method for multImodal reCommEndation, which we term LATTICE for brevity.
To be specific, in the proposed LATTICE model, we devise a novel modality-aware
structure learning layer, which learns item-item structures for each modality
and aggregates multiple modalities to obtain latent item graphs. Based on the
learned latent graphs, we perform graph convolutions to explicitly inject
high-order item affinities into item representations. These enriched item
representations can then be plugged into existing collaborative filtering
methods to make more accurate recommendations. Extensive experiments on three
real-world datasets demonstrate the superiority of our method over
state-of-the-art multimedia recommendation methods and validate the efficacy of
mining latent item-item relationships from multimodal features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superpixel-guided Iterative Learning from Noisy Labels for Medical Image Segmentation. (arXiv:2107.10100v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10100</id>
        <link href="http://arxiv.org/abs/2107.10100"/>
        <updated>2021-07-22T02:03:10.973Z</updated>
        <summary type="html"><![CDATA[Learning segmentation from noisy labels is an important task for medical
image analysis due to the difficulty in acquiring highquality annotations. Most
existing methods neglect the pixel correlation and structural prior in
segmentation, often producing noisy predictions around object boundaries. To
address this, we adopt a superpixel representation and develop a robust
iterative learning strategy that combines noise-aware training of segmentation
network and noisy label refinement, both guided by the superpixels. This design
enables us to exploit the structural constraints in segmentation labels and
effectively mitigate the impact of label noise in learning. Experiments on two
benchmarks show that our method outperforms recent state-of-the-art approaches,
and achieves superior robustness in a wide range of label noises. Code is
available at https://github.com/gaozhitong/SP_guided_Noisy_Label_Seg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuailin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhitong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Single to Multiple: Leveraging Multi-level Prediction Spaces for Video Forecasting. (arXiv:2107.10068v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10068</id>
        <link href="http://arxiv.org/abs/2107.10068"/>
        <updated>2021-07-22T02:03:10.966Z</updated>
        <summary type="html"><![CDATA[Despite video forecasting has been a widely explored topic in recent years,
the mainstream of the existing work still limits their models with a single
prediction space but completely neglects the way to leverage their model with
multi-prediction spaces. This work fills this gap. For the first time, we
deeply study numerous strategies to perform video forecasting in
multi-prediction spaces and fuse their results together to boost performance.
The prediction in the pixel space usually lacks the ability to preserve the
semantic and structure content of the video however the prediction in the
high-level feature space is prone to generate errors in the reduction and
recovering process. Therefore, we build a recurrent connection between
different feature spaces and incorporate their generations in the upsampling
process. Rather surprisingly, this simple idea yields a much more significant
performance boost than PhyDNet (performance improved by 32.1% MAE on MNIST-2
dataset, and 21.4% MAE on KTH dataset). Both qualitative and quantitative
evaluations on four datasets demonstrate the generalization ability and
effectiveness of our approach. We show that our model significantly reduces the
troublesome distortions and blurry artifacts and brings remarkable improvements
to the accuracy in long term video prediction. The code will be released soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1"&gt;Mengcheng Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1"&gt;Shuliang Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xunlai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CGANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.10060</id>
        <link href="http://arxiv.org/abs/2107.10060"/>
        <updated>2021-07-22T02:03:10.958Z</updated>
        <summary type="html"><![CDATA[Conditional generative models aim to learn the underlying joint distribution
of data and labels, and thus realize conditional generation. Among them,
auxiliary classifier generative adversarial networks (AC-GAN) have been widely
used, but suffer from the issue of low intra-class diversity on generated
samples. In this paper, we point out that the fundamental reason is that the
classifier of AC-GAN is generator-agnostic, and thus cannot provide informative
guidance to the generator to approximate the target joint distribution, leading
to a minimization of conditional entropy that decreases the intra-class
diversity. Based on this finding, we propose novel cGANs with auxiliary
discriminative classifier (ADC-GAN) to address the issue of AC-GAN.
Specifically, the auxiliary discriminative classifier becomes generator-aware
by distinguishing between the real and fake data while recognizing their
labels. We then optimize the generator based on the auxiliary classifier along
with the original discriminator to match the joint and marginal distributions
of the generated samples with those of the real samples. We provide theoretical
analysis and empirical evidence on synthetic and real-world datasets to
demonstrate the superiority of the proposed ADC-GAN compared to competitive
cGANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Liang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss. (arXiv:2107.09989v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09989</id>
        <link href="http://arxiv.org/abs/2107.09989"/>
        <updated>2021-07-22T02:03:10.941Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance imaging (MRI) is an important medical imaging modality,
but its acquisition speed is quite slow due to the physiological limitations.
Recently, super-resolution methods have shown excellent performance in
accelerating MRI. In some circumstances, it is difficult to obtain
high-resolution images even with prolonged scan time. Therefore, we proposed a
novel super-resolution method that uses a generative adversarial network (GAN)
with cyclic loss and attention mechanism to generate high-resolution MR images
from low-resolution MR images by a factor of 2. We implemented our model on
pelvic images from healthy subjects as training and validation data, while
those data from patients were used for testing. The MR dataset was obtained
using different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four
methods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison.
Structural similarity, peak signal to noise ratio, root mean square error, and
variance inflation factor were used as calculation indicators to evaluate the
performances of the proposed method. Various experimental results showed that
our method can better restore the details of the high-resolution MR image as
compared to the other methods. In addition, the reconstructed high-resolution
MR image can provide better lesion textures in the tumor patients, which is
promising to be used in clinical diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guangyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jun Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xiangrong Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Iterative 2D/3D Registration. (arXiv:2107.10004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.10004</id>
        <link href="http://arxiv.org/abs/2107.10004"/>
        <updated>2021-07-22T02:03:10.805Z</updated>
        <summary type="html"><![CDATA[Deep Learning-based 2D/3D registration methods are highly robust but often
lack the necessary registration accuracy for clinical application. A refinement
step using the classical optimization-based 2D/3D registration method applied
in combination with Deep Learning-based techniques can provide the required
accuracy. However, it also increases the runtime. In this work, we propose a
novel Deep Learning driven 2D/3D registration framework that can be used
end-to-end for iterative registration tasks without relying on any further
refinement step. We accomplish this by learning the update step of the 2D/3D
registration framework using Point-to-Plane Correspondences. The update step is
learned using iterative residual refinement-based optical flow estimation, in
combination with the Point-to-Plane correspondence solver embedded as a known
operator. Our proposed method achieves an average runtime of around 8s, a mean
re-projection distance error of 0.60 $\pm$ 0.40 mm with a success ratio of 97
percent and a capture range of 60 mm. The combination of high registration
accuracy, high robustness, and fast runtime makes our solution ideal for
clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaganathan_S/0/1/0/all/0/1"&gt;Srikrishna Jaganathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borsdorf_A/0/1/0/all/0/1"&gt;Anja Borsdorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1"&gt;Karthik Shetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorization in Deep Neural Networks: Does the Loss Function matter?. (arXiv:2107.09957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09957</id>
        <link href="http://arxiv.org/abs/2107.09957"/>
        <updated>2021-07-22T02:03:10.645Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks, often owing to the overparameterization, are shown to
be capable of exactly memorizing even randomly labelled data. Empirical studies
have also shown that none of the standard regularization techniques mitigate
such overfitting. We investigate whether the choice of the loss function can
affect this memorization. We empirically show, with benchmark data sets MNIST
and CIFAR-10, that a symmetric loss function, as opposed to either
cross-entropy or squared error loss, results in significant improvement in the
ability of the network to resist such overfitting. We then provide a formal
definition for robustness to memorization and provide a theoretical explanation
as to why the symmetric losses provide this robustness. Our results clearly
bring out the role loss functions alone can play in this phenomenon of
memorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1"&gt;P.S. Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fabrication-Aware Reverse Engineering for Carpentry. (arXiv:2107.09965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09965</id>
        <link href="http://arxiv.org/abs/2107.09965"/>
        <updated>2021-07-22T02:03:10.621Z</updated>
        <summary type="html"><![CDATA[We propose a novel method to generate fabrication blueprints from images of
carpentered items. While 3D reconstruction from images is a well-studied
problem, typical approaches produce representations that are ill-suited for
computer-aided design and fabrication applications. Our key insight is that
fabrication processes define and constrain the design space for carpentered
objects, and can be leveraged to develop novel reconstruction methods. Our
method makes use of domain-specific constraints to recover not just valid
geometry, but a semantically valid assembly of parts, using a combination of
image-based and geometric optimization techniques.

We demonstrate our method on a variety of wooden objects and furniture, and
show that we can automatically obtain designs that are both easy to edit and
accurate recreations of the ground truth. We further illustrate how our method
can be used to fabricate a physical replica of the captured object as well as a
customized version, which can be produced by directly editing the reconstructed
model in CAD software.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noeckel_J/0/1/0/all/0/1"&gt;James Noeckel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haisen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1"&gt;Brian Curless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1"&gt;Adriana Schulz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection via Self-organizing Map. (arXiv:2107.09903v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09903</id>
        <link href="http://arxiv.org/abs/2107.09903"/>
        <updated>2021-07-22T02:03:10.614Z</updated>
        <summary type="html"><![CDATA[Anomaly detection plays a key role in industrial manufacturing for product
quality control. Traditional methods for anomaly detection are rule-based with
limited generalization ability. Recent methods based on supervised deep
learning are more powerful but require large-scale annotated datasets for
training. In practice, abnormal products are rare thus it is very difficult to
train a deep model in a fully supervised way. In this paper, we propose a novel
unsupervised anomaly detection approach based on Self-organizing Map (SOM). Our
method, Self-organizing Map for Anomaly Detection (SOMAD) maintains normal
characteristics by using topological memory based on multi-scale features.
SOMAD achieves state-of the-art performance on unsupervised anomaly detection
and localization on the MVTec dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Ning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1"&gt;Kaitao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhiheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1"&gt;Xiaopeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yihong Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogME: A Novel Evaluation Metric for Video Understanding Intelligence. (arXiv:2107.09847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09847</id>
        <link href="http://arxiv.org/abs/2107.09847"/>
        <updated>2021-07-22T02:03:10.603Z</updated>
        <summary type="html"><![CDATA[Developing video understanding intelligence is quite challenging because it
requires holistic integration of images, scripts, and sounds based on natural
language processing, temporal dependency, and reasoning. Recently, substantial
attempts have been made on several video datasets with associated question
answering (QA) on a large scale. However, existing evaluation metrics for video
question answering (VideoQA) do not provide meaningful analysis. To make
progress, we argue that a well-made framework, established on the way humans
understand, is required to explain and evaluate the performance of
understanding in detail. Then we propose a top-down evaluation system for
VideoQA, based on the cognitive process of humans and story elements: Cognitive
Modules for Evaluation (CogME). CogME is composed of three cognitive modules:
targets, contents, and thinking. The interaction among the modules in the
understanding procedure can be expressed in one sentence as follows: "I
understand the CONTENT of the TARGET through a way of THINKING." Each module
has sub-components derived from the story elements. We can specify the required
aspects of understanding by annotating the sub-components to individual
questions. CogME thus provides a framework for an elaborated specification of
VideoQA datasets. To examine the suitability of a VideoQA dataset for
validating video understanding intelligence, we evaluated the baseline model of
the DramaQA dataset by applying CogME. The evaluation reveals that story
elements are unevenly reflected in the existing dataset, and the model based on
the dataset may cause biased predictions. Although this study has only been
able to grasp a narrow range of stories, we expect that it offers the first
step in considering the cognitive process of humans on the video understanding
intelligence of humans and AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1"&gt;Minjung Shin&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jeonghoon Kim&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Seongho Choi&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1"&gt;Yu-Jung Heo&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Donghyun Kim&lt;/a&gt; (1 and 4), &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minsu Lee&lt;/a&gt; (3 and 5), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Byoung-Tak Zhang&lt;/a&gt; (3 and 5), &lt;a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1"&gt;Jeh-Kwang Ryu&lt;/a&gt; (1 and 4) ((1) Laboratory for Natural and Artificial Kin&amp;#xe4;sthese, Convergence Research Center for Artificial Intelligence (CRC4AI), Dongguk University, Seoul, South Korea, (2) Department of Artificial Intelligence, Dongguk University, Seoul, South Korea, (3) Biointelligence Laboratory, Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea, (4) Department of Physical Education, College of Education, Dongguk University, Seoul, South Korea, (5) AI Institute of Seoul National University (AIIS), Seoul, South Korea)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterization Multimodal Connectivity of Brain Network by Hypergraph GAN for Alzheimer's Disease Analysis. (arXiv:2107.09953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09953</id>
        <link href="http://arxiv.org/abs/2107.09953"/>
        <updated>2021-07-22T02:03:10.595Z</updated>
        <summary type="html"><![CDATA[Using multimodal neuroimaging data to characterize brain network is currently
an advanced technique for Alzheimer's disease(AD) Analysis. Over recent years
the neuroimaging community has made tremendous progress in the study of
resting-state functional magnetic resonance imaging (rs-fMRI) derived from
blood-oxygen-level-dependent (BOLD) signals and Diffusion Tensor Imaging (DTI)
derived from white matter fiber tractography. However, Due to the heterogeneity
and complexity between BOLD signals and fiber tractography, Most existing
multimodal data fusion algorithms can not sufficiently take advantage of the
complementary information between rs-fMRI and DTI. To overcome this problem, a
novel Hypergraph Generative Adversarial Networks(HGGAN) is proposed in this
paper, which utilizes Interactive Hyperedge Neurons module (IHEN) and Optimal
Hypergraph Homomorphism algorithm(OHGH) to generate multimodal connectivity of
Brain Network from rs-fMRI combination with DTI. To evaluate the performance of
this model, We use publicly available data from the ADNI database to
demonstrate that the proposed model not only can identify discriminative brain
regions of AD but also can effectively improve classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Junren Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1"&gt;Baiying Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiguang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRDF: Determining the Importance of Different Multimodal Information with Dual-Router Dynamic Framework. (arXiv:2107.09909v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09909</id>
        <link href="http://arxiv.org/abs/2107.09909"/>
        <updated>2021-07-22T02:03:10.586Z</updated>
        <summary type="html"><![CDATA[In multimodal tasks, we find that the importance of text and image modal
information is different for different input cases, and for this motivation, we
propose a high-performance and highly general Dual-Router Dynamic Framework
(DRDF), consisting of Dual-Router, MWF-Layer, experts and expert fusion unit.
The text router and image router in Dual-Router accept text modal information
and image modal information, and use MWF-Layer to determine the importance of
modal information. Based on the result of the determination, MWF-Layer
generates fused weights for the fusion of experts. Experts are model backbones
that match the current task. DRDF has high performance and high generality, and
we have tested 12 backbones such as Visual BERT on multimodal dataset Hateful
memes, unimodal dataset CIFAR10, CIFAR100, and TinyImagenet. Our DRDF
outperforms all the baselines. We also verified the components of DRDF in
detail by ablations, compared and discussed the reasons and ideas of DRDF
design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1"&gt;Haiwen Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yunqing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hui Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Representations Learning and Adversarial Hypergraph Fusion for Early Alzheimer's Disease Prediction. (arXiv:2107.09928v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09928</id>
        <link href="http://arxiv.org/abs/2107.09928"/>
        <updated>2021-07-22T02:03:10.561Z</updated>
        <summary type="html"><![CDATA[Multimodal neuroimage can provide complementary information about the
dementia, but small size of complete multimodal data limits the ability in
representation learning. Moreover, the data distribution inconsistency from
different modalities may lead to ineffective fusion, which fails to
sufficiently explore the intra-modal and inter-modal interactions and
compromises the disease diagnosis performance. To solve these problems, we
proposed a novel multimodal representation learning and adversarial hypergraph
fusion (MRL-AHF) framework for Alzheimer's disease diagnosis using complete
trimodal images. First, adversarial strategy and pre-trained model are
incorporated into the MRL to extract latent representations from multimodal
data. Then two hypergraphs are constructed from the latent representations and
the adversarial network based on graph convolution is employed to narrow the
distribution difference of hyperedge features. Finally, the hyperedge-invariant
features are fused for disease prediction by hyperedge convolution. Experiments
on the public Alzheimer's Disease Neuroimaging Initiative(ADNI) database
demonstrate that our model achieves superior performance on Alzheimer's disease
detection compared with other related models and provides a possible way to
understand the underlying mechanisms of disorder's progression by analyzing the
abnormal brain connections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_Q/0/1/0/all/0/1"&gt;Qiankun Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1"&gt;Baiying Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiguang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion. (arXiv:2106.10132v1 [eess.AS] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.10132</id>
        <link href="http://arxiv.org/abs/2106.10132"/>
        <updated>2021-07-22T02:03:10.551Z</updated>
        <summary type="html"><![CDATA[One-shot voice conversion (VC), which performs conversion across arbitrary
speakers with only a single target-speaker utterance for reference, can be
effectively achieved by speech representation disentanglement. Existing work
generally ignores the correlation between different speech representations
during training, which causes leakage of content information into the speaker
representation and thus degrades VC performance. To alleviate this issue, we
employ vector quantization (VQ) for content encoding and introduce mutual
information (MI) as the correlation metric during training, to achieve proper
disentanglement of content, speaker and pitch representations, by reducing
their inter-dependencies in an unsupervised manner. Experimental results
reflect the superiority of the proposed method in learning effective
disentangled speech representations for retaining source linguistic content and
intonation variations, while capturing target speaker characteristics. In doing
so, the proposed approach achieves higher speech naturalness and speaker
similarity than current state-of-the-art one-shot VC systems. Our code,
pre-trained models and demo are available at
https://github.com/Wendison/VQMIVC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Disong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1"&gt;Liqun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1"&gt;Yu Ting Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Social Imaginaries and Self-Disclosures of Dissonance in Online Conspiracy Discussion Communities. (arXiv:2107.10204v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.10204</id>
        <link href="http://arxiv.org/abs/2107.10204"/>
        <updated>2021-07-22T02:03:10.542Z</updated>
        <summary type="html"><![CDATA[Online discussion platforms offer a forum to strengthen and propagate belief
in misinformed conspiracy theories. Yet, they also offer avenues for conspiracy
theorists to express their doubts and experiences of cognitive dissonance. Such
expressions of dissonance may shed light on who abandons misguided beliefs and
under which circumstances. This paper characterizes self-disclosures of
dissonance about QAnon, a conspiracy theory initiated by a mysterious leader Q
and popularized by their followers, anons in conspiracy theory subreddits. To
understand what dissonance and disbelief mean within conspiracy communities, we
first characterize their social imaginaries, a broad understanding of how
people collectively imagine their social existence. Focusing on 2K posts from
two image boards, 4chan and 8chan, and 1.2 M comments and posts from 12
subreddits dedicated to QAnon, we adopt a mixed methods approach to uncover the
symbolic language representing the movement, expectations, practices, heroes
and foes of the QAnon community. We use these social imaginaries to create a
computational framework for distinguishing belief and dissonance from general
discussion about QAnon. Further, analyzing user engagement with QAnon
conspiracy subreddits, we find that self-disclosures of dissonance correlate
with a significant decrease in user contributions and ultimately with their
departure from the community. We contribute a computational framework for
identifying dissonance self-disclosures and measuring the changes in user
engagement surrounding dissonance. Our work can provide insights into designing
dissonance-based interventions that can potentially dissuade conspiracists from
online conspiracy discussion communities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phadke_S/0/1/0/all/0/1"&gt;Shruti Phadke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samory_M/0/1/0/all/0/1"&gt;Mattia Samory&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1"&gt;Tanushree Mitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Inductive Biases in Emergent Communication. (arXiv:2002.01335v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01335</id>
        <link href="http://arxiv.org/abs/2002.01335"/>
        <updated>2021-07-22T02:03:10.534Z</updated>
        <summary type="html"><![CDATA[In order to communicate, humans flatten a complex representation of ideas and
their attributes into a single word or a sentence. We investigate the impact of
representation learning in artificial agents by developing graph referential
games. We empirically show that agents parametrized by graph neural networks
develop a more compositional language compared to bag-of-words and sequence
models, which allows them to systematically generalize to new combinations of
familiar features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1"&gt;Agnieszka S&amp;#x142;owik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1"&gt;Mateja Jamnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holden_S/0/1/0/all/0/1"&gt;Sean B. Holden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Melody Structure Transfer Network: Generating Music with Separable Self-Attention. (arXiv:2107.09877v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09877</id>
        <link href="http://arxiv.org/abs/2107.09877"/>
        <updated>2021-07-22T02:03:10.522Z</updated>
        <summary type="html"><![CDATA[Symbolic music generation has attracted increasing attention, while most
methods focus on generating short piece (mostly less than 8 bars, and up to 32
bars). Generating long music calls for effective expression of the coherent
music structure. Despite their success on long sequences, self-attention
architectures still have challenge in dealing with long-term music as it
requires additional care on the subtle music structure. In this paper, we
propose to transfer the structure of training samples for new music generation,
and develop a novel separable self-attention based model which enable the
learning and transferring of the structure embedding. We show that our transfer
model can generate music sequences (up to 100 bars) with interpretable
structures, which bears similar structures and composition techniques with the
template music from training set. Extensive experiments show its ability of
generating music with target structure and well diversity. The generated 3,000
sets of music is uploaded as supplemental material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Registration of 3D Point Sets Using Correntropy Similarity Matrix. (arXiv:2107.09725v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09725</id>
        <link href="http://arxiv.org/abs/2107.09725"/>
        <updated>2021-07-22T02:03:10.487Z</updated>
        <summary type="html"><![CDATA[This work focuses on Registration or Alignment of 3D point sets. Although the
Registration problem is a well established problem and it's solved using
multiple variants of Iterative Closest Point (ICP) Algorithm, most of the
approaches in the current state of the art still suffers from misalignment when
the \textit{Source} and the \textit{Target} point sets are separated by large
rotations and translation. In this work, we propose a variant of the Standard
ICP algorithm, where we introduce a Correntropy Relationship Matrix in the
computation of rotation and translation component which attempts to solve the
large rotation and translation problem between \textit{Source} and
\textit{Target} point sets. This matrix is created through correntropy
criterion which is updated in every iteration. The correntropy criterion
defined in this approach maintains the relationship between the points in the
\textit{Source} dataset and the \textit{Target} dataset. Through our
experiments and validation we verify that our approach has performed well under
various rotation and translation in comparison to the other well-known state of
the art methods available in the Point Cloud Library (PCL) as well as other
methods available as open source. We have uploaded our code in the github
repository for the readers to validate and verify our approach
https://github.com/aralab-unr/CoSM-ICP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singandhupe_A/0/1/0/all/0/1"&gt;Ashutosh Singandhupe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+La_H/0/1/0/all/0/1"&gt;Hung La&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1"&gt;Trung Dung Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_V/0/1/0/all/0/1"&gt;Van Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure-Aware Long Short-Term Memory Network for 3D Cephalometric Landmark Detection. (arXiv:2107.09899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09899</id>
        <link href="http://arxiv.org/abs/2107.09899"/>
        <updated>2021-07-22T02:03:10.480Z</updated>
        <summary type="html"><![CDATA[Detecting 3D landmarks on cone-beam computed tomography (CBCT) is crucial to
assessing and quantifying the anatomical abnormalities in 3D cephalometric
analysis. However, the current methods are time-consuming and suffer from large
biases in landmark localization, leading to unreliable diagnosis results. In
this work, we propose a novel Structure-Aware Long Short-Term Memory framework
(SA-LSTM) for efficient and accurate 3D landmark detection. To reduce the
computational burden, SA-LSTM is designed in two stages. It first locates the
coarse landmarks via heatmap regression on a down-sampled CBCT volume and then
progressively refines landmarks by attentive offset regression using
high-resolution cropped patches. To boost accuracy, SA-LSTM captures
global-local dependence among the cropping patches via self-attention.
Specifically, a graph attention module implicitly encodes the landmark's global
structure to rationalize the predicted position. Furthermore, a novel
attention-gated module recursively filters irrelevant local features and
maintains high-confident local predictions for aggregating the final result.
Experiments show that our method significantly outperforms state-of-the-art
methods in terms of efficiency and accuracy on an in-house dataset and a public
dataset, achieving 1.64 mm and 2.37 mm average errors, respectively, and using
only 0.5 seconds for inferring the whole CBCT volume of resolution 768*768*576.
Moreover, all predicted landmarks are within 8 mm error, which is vital for
acceptable cephalometric analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runnan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuexin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nenglun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yanhong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation. (arXiv:2107.09843v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09843</id>
        <link href="http://arxiv.org/abs/2107.09843"/>
        <updated>2021-07-22T02:03:10.472Z</updated>
        <summary type="html"><![CDATA[Deep learning models are notoriously data-hungry. Thus, there is an urging
need for data-efficient techniques in medical image analysis, where
well-annotated data are costly and time consuming to collect. Motivated by the
recently revived "Copy-Paste" augmentation, we propose TumorCP, a simple but
effective object-level data augmentation method tailored for tumor
segmentation. TumorCP is online and stochastic, providing unlimited
augmentation possibilities for tumors' subjects, locations, appearances, as
well as morphologies. Experiments on kidney tumor segmentation task demonstrate
that TumorCP surpasses the strong baseline by a remarkable margin of 7.12% on
tumor Dice. Moreover, together with image-level data augmentation, it beats the
current state-of-the-art by 2.32% on tumor Dice. Comprehensive ablation studies
are performed to validate the effectiveness of TumorCP. Meanwhile, we show that
TumorCP can lead to striking improvements in extremely low-data regimes.
Evaluated with only 10% labeled data, TumorCP significantly boosts tumor Dice
by 21.87%. To the best of our knowledge, this is the very first work exploring
and extending the "Copy-Paste" design in medical imaging domain. Code is
available at: https://github.com/YaoZhang93/TumorCP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiawei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Generation of Cause and Effect. (arXiv:2107.09846v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09846</id>
        <link href="http://arxiv.org/abs/2107.09846"/>
        <updated>2021-07-22T02:03:10.464Z</updated>
        <summary type="html"><![CDATA[We present a conditional text generation framework that posits sentential
expressions of possible causes and effects. This framework depends on two novel
resources we develop in the course of this work: a very large-scale collection
of English sentences expressing causal patterns CausalBank; and a refinement
over previous work on constructing large lexical causal knowledge graphs Cause
Effect Graph. Further, we extend prior work in lexically-constrained decoding
to support disjunctive positive constraints. Human assessment confirms that our
approach gives high-quality and diverse outputs. Finally, we use CausalBank to
perform continued training of an encoder supporting a recent state-of-the-art
model for causal reasoning, leading to a 3-point improvement on the COPA
challenge set, with no change in model architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;J. Edward Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1"&gt;Benjamin Van Durme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An overview of mixing augmentation methods and augmentation strategies. (arXiv:2107.09887v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09887</id>
        <link href="http://arxiv.org/abs/2107.09887"/>
        <updated>2021-07-22T02:03:10.455Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Networks have made an incredible progress in many
Computer Vision tasks. This progress, however, often relies on the availability
of large amounts of the training data, required to prevent over-fitting, which
in many domains entails significant cost of manual data labeling. An
alternative approach is application of data augmentation (DA) techniques that
aim at model regularization by creating additional observations from the
available ones. This survey focuses on two DA research streams: image mixing
and automated selection of augmentation strategies. First, the presented
methods are briefly described, and then qualitatively compared with respect to
their key characteristics. Various quantitative comparisons are also included
based on the results reported in recent DA literature. This review mainly
covers the methods published in the materials of top-tier conferences and in
leading journals in the years 2017-2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lewy_D/0/1/0/all/0/1"&gt;Dominik Lewy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1"&gt;Jacek Ma&amp;#x144;dziuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05426</id>
        <link href="http://arxiv.org/abs/2106.05426"/>
        <updated>2021-07-22T02:03:10.435Z</updated>
        <summary type="html"><![CDATA[How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1"&gt;Richard Antonello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1"&gt;Javier Turek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1"&gt;Vy Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1"&gt;Alexander Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Music Plagiarism Detection via Bipartite Graph Matching. (arXiv:2107.09889v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09889</id>
        <link href="http://arxiv.org/abs/2107.09889"/>
        <updated>2021-07-22T02:03:10.425Z</updated>
        <summary type="html"><![CDATA[Nowadays, with the prevalence of social media and music creation tools,
musical pieces are spreading much quickly, and music creation is getting much
easier. The increasing number of musical pieces have made the problem of music
plagiarism prominent. There is an urgent need for a tool that can detect music
plagiarism automatically. Researchers have proposed various methods to extract
low-level and high-level features of music and compute their similarities.
However, low-level features such as cepstrum coefficients have weak relation
with the copyright protection of musical pieces. Existing algorithms
considering high-level features fail to detect the case in which two musical
pieces are not quite similar overall, but have some highly similar regions.
This paper proposes a new method named MESMF, which innovatively converts the
music plagiarism detection problem into the bipartite graph matching task. It
can be solved via the maximum weight matching and edit distances model. We
design several kinds of melody representations and the similarity computation
methods according to the music theory. The proposed method can deal with the
shift, swapping, transposition, and tempo variance problems in music
plagiarism. It can also effectively pick out the local similar regions from two
musical pieces with relatively low global similarity. We collect a new music
plagiarism dataset from real legally-judged music plagiarism cases and conduct
detailed ablation studies. Experimental results prove the excellent performance
of the proposed algorithm. The source code and our dataset are available at
https://anonymous.4open.science/r/a41b8fb4-64cf-4190-a1e1-09b7499a15f5/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tianyao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenxuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ning Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications. (arXiv:2004.11142v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11142</id>
        <link href="http://arxiv.org/abs/2004.11142"/>
        <updated>2021-07-22T02:03:10.410Z</updated>
        <summary type="html"><![CDATA[Machine reading comprehension (MRC) is a crucial task in natural language
processing and has achieved remarkable advancements. However, most of the
neural MRC models are still far from robust and fail to generalize well in
real-world applications. In order to comprehensively verify the robustness and
generalization of MRC models, we introduce a real-world Chinese dataset --
DuReader_robust. It is designed to evaluate the MRC models from three aspects:
over-sensitivity, over-stability and generalization. Comparing to previous
work, the instances in DuReader_robust are natural texts, rather than the
altered unnatural texts. It presents the challenges when applying MRC models to
real-world applications. The experimental results show that MRC models do not
perform well on the challenge test set. Moreover, we analyze the behavior of
existing models on the challenge test set, which may provide suggestions for
future model development. The dataset and codes are publicly available at
https://github.com/baidu/DuReader.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hongxuan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yu Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objective video quality metrics application to video codecs comparisons: choosing the best for subjective quality estimation. (arXiv:2107.10220v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.10220</id>
        <link href="http://arxiv.org/abs/2107.10220"/>
        <updated>2021-07-22T02:03:10.401Z</updated>
        <summary type="html"><![CDATA[Quality assessment plays a key role in creating and comparing video
compression algorithms. Despite the development of a large number of new
methods for assessing quality, generally accepted and well-known codecs
comparisons mainly use the classical methods like PSNR, SSIM and new method
VMAF. These methods can be calculated following different rules: they can use
different frame-by-frame averaging techniques or different summation of color
components. In this paper, a fundamental comparison of various versions of
generally accepted metrics is carried out to find the most relevant and
recommended versions of video quality metrics to be used in codecs comparisons.
For comparison, we used a set of videos encoded with video codecs of different
standards, and visual quality scores collected for the resulting set of streams
since 2018 until 2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1"&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yakovenko_A/0/1/0/all/0/1"&gt;Alexander Yakovenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safonov_N/0/1/0/all/0/1"&gt;Nickolay Safonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1"&gt;Dmitriy Kulikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gushin_A/0/1/0/all/0/1"&gt;Alexander Gushin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1"&gt;Dmitriy Vatolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Latent Structures for Multimedia Recommendation. (arXiv:2104.09036v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09036</id>
        <link href="http://arxiv.org/abs/2104.09036"/>
        <updated>2021-07-22T02:03:10.377Z</updated>
        <summary type="html"><![CDATA[Multimedia content is of predominance in the modern Web era. Investigating
how users interact with multimodal items is a continuing concern within the
rapid development of recommender systems. The majority of previous work focuses
on modeling user-item interactions with multimodal features included as side
information. However, this scheme is not well-designed for multimedia
recommendation. Specifically, only collaborative item-item relationships are
implicitly modeled through high-order item-user-item relations. Considering
that items are associated with rich contents in multiple modalities, we argue
that the latent semantic item-item structures underlying these multimodal
contents could be beneficial for learning better item representations and
further boosting recommendation. To this end, we propose a LATent sTructure
mining method for multImodal reCommEndation, which we term LATTICE for brevity.
To be specific, in the proposed LATTICE model, we devise a novel modality-aware
structure learning layer, which learns item-item structures for each modality
and aggregates multiple modalities to obtain latent item graphs. Based on the
learned latent graphs, we perform graph convolutions to explicitly inject
high-order item affinities into item representations. These enriched item
representations can then be plugged into existing collaborative filtering
methods to make more accurate recommendations. Extensive experiments on three
real-world datasets demonstrate the superiority of our method over
state-of-the-art multimedia recommendation methods and validate the efficacy of
mining latent item-item relationships from multimodal features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Czech Transformers on Text Classification Tasks. (arXiv:2107.10042v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10042</id>
        <link href="http://arxiv.org/abs/2107.10042"/>
        <updated>2021-07-22T02:03:10.355Z</updated>
        <summary type="html"><![CDATA[In this paper, we present our progress in pre-training monolingual
Transformers for Czech and contribute to the research community by releasing
our models for public. The need for such models emerged from our effort to
employ Transformers in our language-specific tasks, but we found the
performance of the published multilingual models to be very limited. Since the
multilingual models are usually pre-trained from 100+ languages, most of
low-resourced languages (including Czech) are under-represented in these
models. At the same time, there is a huge amount of monolingual training data
available in web archives like Common Crawl. We have pre-trained and publicly
released two monolingual Czech Transformers and compared them with relevant
public models, trained (at least partially) for Czech. The paper presents the
Transformers pre-training procedure as well as a comparison of pre-trained
models on text classification task from various domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1"&gt;Jan Lehe&amp;#x10d;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svec_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;vec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CATE: CAusality Tree Extractor from Natural Language Requirements. (arXiv:2107.10023v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10023</id>
        <link href="http://arxiv.org/abs/2107.10023"/>
        <updated>2021-07-22T02:03:10.343Z</updated>
        <summary type="html"><![CDATA[Causal relations (If A, then B) are prevalent in requirements artifacts.
Automatically extracting causal relations from requirements holds great
potential for various RE activities (e.g., automatic derivation of suitable
test cases). However, we lack an approach capable of extracting causal
relations from natural language with reasonable performance. In this paper, we
present our tool CATE (CAusality Tree Extractor), which is able to parse the
composition of a causal relation as a tree structure. CATE does not only
provide an overview of causes and effects in a sentence, but also reveals their
semantic coherence by translating the causal relation into a binary tree. We
encourage fellow researchers and practitioners to use CATE at
https://causalitytreeextractor.com/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jadallah_N/0/1/0/all/0/1"&gt;Noah Jadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1"&gt;Jannik Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1"&gt;Julian Frattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1"&gt;Andreas Vogelsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TLA: Twitter Linguistic Analysis. (arXiv:2107.09710v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09710</id>
        <link href="http://arxiv.org/abs/2107.09710"/>
        <updated>2021-07-22T02:03:10.335Z</updated>
        <summary type="html"><![CDATA[Linguistics has been instrumental in developing a deeper understanding of
human nature. Words are indispensable to bequeath the thoughts, emotions, and
purpose of any human interaction, and critically analyzing these words can
elucidate the social and psychological behavior and characteristics of these
social animals. Social media has become a platform for human interaction on a
large scale and thus gives us scope for collecting and using that data for our
study. However, this entire process of collecting, labeling, and analyzing this
data iteratively makes the entire procedure cumbersome. To make this entire
process easier and structured, we would like to introduce TLA(Twitter
Linguistic Analysis). In this paper, we describe TLA and provide a basic
understanding of the framework and discuss the process of collecting, labeling,
and analyzing data from Twitter for a corpus of languages while providing
detailed labeled datasets for all the languages and the models are trained on
these datasets. The analysis provided by TLA will also go a long way in
understanding the sentiments of different linguistic communities and come up
with new and innovative solutions for their problems based on the analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1"&gt;Tushar Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajadhyaksha_N/0/1/0/all/0/1"&gt;Nishant Rajadhyaksha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding. (arXiv:2107.09931v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09931</id>
        <link href="http://arxiv.org/abs/2107.09931"/>
        <updated>2021-07-22T02:03:10.314Z</updated>
        <summary type="html"><![CDATA[While recent benchmarks have spurred a lot of new work on improving the
generalization of pretrained multilingual language models on multilingual
tasks, techniques to improve code-switched natural language understanding tasks
have been far less explored. In this work, we propose the use of bilingual
intermediate pretraining as a reliable technique to derive large and consistent
performance gains on three different NLP tasks using code-switched text. We
achieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the
mean accuracies and F1 scores over previous state-of-the-art systems for
Hindi-English Natural Language Inference (NLI), Question Answering (QA) tasks,
and Spanish-English Sentiment Analysis (SA) respectively. We show consistent
performance gains on four different code-switched language-pairs
(Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA.
We also present a code-switched masked language modelling (MLM) pretraining
technique that consistently benefits SA compared to standard MLM pretraining
using real code-switched text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1"&gt;Archiki Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehan_M/0/1/0/all/0/1"&gt;Mohammad Ali Rehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1"&gt;Shreya Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Statistical Model of Word Rank Evolution. (arXiv:2107.09948v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09948</id>
        <link href="http://arxiv.org/abs/2107.09948"/>
        <updated>2021-07-22T02:03:10.303Z</updated>
        <summary type="html"><![CDATA[The availability of large linguistic data sets enables data-driven approaches
to study linguistic change. This work explores the word rank dynamics of eight
languages by investigating the Google Books corpus unigram frequency data set.
We observed the rank changes of the unigrams from 1900 to 2008 and compared it
to a Wright-Fisher inspired model that we developed for our analysis. The model
simulates a neutral evolutionary process with the restriction of having no
disappearing words. This work explains the mathematical framework of the model
- written as a Markov Chain with multinomial transition probabilities - to show
how frequencies of words change in time. From our observations in the data and
our model, word rank stability shows two types of characteristics: (1) the
increase/decrease in ranks are monotonic, or (2) the average rank stays the
same. Based on our model, high-ranked words tend to be more stable while
low-ranked words tend to be more volatile. Some words change in ranks in two
ways: (a) by an accumulation of small increasing/decreasing rank changes in
time and (b) by shocks of increase/decrease in ranks. Most of the stopwords and
Swadesh words are observed to be stable in ranks across eight languages. These
signatures suggest unigram frequencies in all languages have changed in a
manner inconsistent with a purely neutral evolutionary process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quijano_A/0/1/0/all/0/1"&gt;Alex John Quijano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dale_R/0/1/0/all/0/1"&gt;Rick Dale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindi_S/0/1/0/all/0/1"&gt;Suzanne Sindi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Adversarial Debiasing to Remove Bias from Word Embeddings. (arXiv:2107.10251v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10251</id>
        <link href="http://arxiv.org/abs/2107.10251"/>
        <updated>2021-07-22T02:03:10.281Z</updated>
        <summary type="html"><![CDATA[Word Embeddings have been shown to contain the societal biases present in the
original corpora.Existing methods to deal with this problem have been shown to
only remove superficial biases. Themethod ofAdversarial Debiasingwas presumed
to be similarly superficial, but this is was not verifiedin previous works.
Using the experiments that demonstrated the shallow removal in other methods,
Ishow results that suggestAdversarial Debiasingis more effective at removing
bias and thus motivatefurther investigation on the utility ofAdversarial
Debiasing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kenna_D/0/1/0/all/0/1"&gt;Dana Kenna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Lower-Dose PET using Physics-Based Uncertainty-Aware Multimodal Learning with Robustness to Out-of-Distribution Data. (arXiv:2107.09892v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09892</id>
        <link href="http://arxiv.org/abs/2107.09892"/>
        <updated>2021-07-22T02:03:10.273Z</updated>
        <summary type="html"><![CDATA[Radiation exposure in positron emission tomography (PET) imaging limits its
usage in the studies of radiation-sensitive populations, e.g., pregnant women,
children, and adults that require longitudinal imaging. Reducing the PET
radiotracer dose or acquisition time reduces photon counts, which can
deteriorate image quality. Recent deep-neural-network (DNN) based methods for
image-to-image translation enable the mapping of low-quality PET images
(acquired using substantially reduced dose), coupled with the associated
magnetic resonance imaging (MRI) images, to high-quality PET images. However,
such DNN methods focus on applications involving test data that match the
statistical characteristics of the training data very closely and give little
attention to evaluating the performance of these DNNs on new
out-of-distribution (OOD) acquisitions. We propose a novel DNN formulation that
models the (i) underlying sinogram-based physics of the PET imaging system and
(ii) the uncertainty in the DNN output through the per-voxel heteroscedasticity
of the residuals between the predicted and the high-quality reference images.
Our sinogram-based uncertainty-aware DNN framework, namely, suDNN, estimates a
standard-dose PET image using multimodal input in the form of (i) a
low-dose/low-count PET image and (ii) the corresponding multi-contrast MRI
images, leading to improved robustness of suDNN to OOD acquisitions. Results on
in vivo simultaneous PET-MRI, and various forms of OOD data in PET-MRI, show
the benefits of suDNN over the current state of the art, quantitatively and
qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sudarshan_V/0/1/0/all/0/1"&gt;Viswanath P. Sudarshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1"&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egan_G/0/1/0/all/0/1"&gt;Gary F. Egan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1"&gt;Suyash P. Awate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. (arXiv:2107.09840v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09840</id>
        <link href="http://arxiv.org/abs/2107.09840"/>
        <updated>2021-07-22T02:03:10.264Z</updated>
        <summary type="html"><![CDATA[Multilingual pre-trained contextual embedding models (Devlin et al., 2019)
have achieved impressive performance on zero-shot cross-lingual transfer tasks.
Finding the most effective fine-tuning strategy to fine-tune these models on
high-resource languages so that it transfers well to the zero-shot languages is
a non-trivial task. In this paper, we propose a novel meta-optimizer to
soft-select which layers of the pre-trained model to freeze during fine-tuning.
We train the meta-optimizer by simulating the zero-shot transfer scenario.
Results on cross-lingual natural language inference show that our approach
improves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,
2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weijia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haider_B/0/1/0/all/0/1"&gt;Batool Haider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krone_J/0/1/0/all/0/1"&gt;Jason Krone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1"&gt;Saab Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images. (arXiv:2107.09700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09700</id>
        <link href="http://arxiv.org/abs/2107.09700"/>
        <updated>2021-07-22T02:03:10.256Z</updated>
        <summary type="html"><![CDATA[Image synthesis via Generative Adversarial Networks (GANs) of
three-dimensional (3D) medical images has great potential that can be extended
to many medical applications, such as, image enhancement and disease
progression modeling. However, current GAN technologies for 3D medical image
synthesis need to be significantly improved to be readily adapted to real-world
medical problems. In this paper, we extend the state-of-the-art StyleGAN2
model, which natively works with two-dimensional images, to enable 3D image
synthesis. In addition to the image synthesis, we investigate the
controllability and interpretability of the 3D-StyleGAN via style vectors
inherited form the original StyleGAN2 that are highly suitable for medical
applications: (i) the latent space projection and reconstruction of unseen real
images, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and
feasibility with ~12,000 three-dimensional full brain MR T1 images, although it
can be applied to any 3D volumetric images. Furthermore, we explore different
configurations of hyperparameters to investigate potential improvement of the
image synthesis with larger networks. The codes and pre-trained networks are
available online: https://github.com/sh4174/3DStyleGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sungmin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Razvan Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bonkhoff_A/0/1/0/all/0/1"&gt;Anna K. Bonkhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bretzner_M/0/1/0/all/0/1"&gt;Martin Bretzner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rost_N/0/1/0/all/0/1"&gt;Natalia S. Rost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Causality Extraction From Natural Language Requirements Using Recursive Neural Tensor Networks. (arXiv:2107.09980v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09980</id>
        <link href="http://arxiv.org/abs/2107.09980"/>
        <updated>2021-07-22T02:03:10.242Z</updated>
        <summary type="html"><![CDATA[[Context:] Causal relations (e.g., If A, then B) are prevalent in functional
requirements. For various applications of AI4RE, e.g., the automatic derivation
of suitable test cases from requirements, automatically extracting such causal
statements are a basic necessity. [Problem:] We lack an approach that is able
to extract causal relations from natural language requirements in fine-grained
form. Specifically, existing approaches do not consider the combinatorics
between causes and effects. They also do not allow to split causes and effects
into more granular text fragments (e.g., variable and condition), making the
extracted relations unsuitable for automatic test case derivation. [Objective &
Contributions:] We address this research gap and make the following
contributions: First, we present the Causality Treebank, which is the first
corpus of fully labeled binary parse trees representing the composition of
1,571 causal requirements. Second, we propose a fine-grained causality
extractor based on Recursive Neural Tensor Networks. Our approach is capable of
recovering the composition of causal statements written in natural language and
achieves a F1 score of 74 % in the evaluation on the Causality Treebank. Third,
we disclose our open data sets as well as our code to foster the discourse on
the automatic extraction of causality in the RE community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1"&gt;Jannik Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springer_T/0/1/0/all/0/1"&gt;Tobias Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1"&gt;Julian Frattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Femmer_H/0/1/0/all/0/1"&gt;Henning Femmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1"&gt;Andreas Vogelsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_D/0/1/0/all/0/1"&gt;Daniel Mendez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JEFL: Joint Embedding of Formal Proof Libraries. (arXiv:2107.10188v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2107.10188</id>
        <link href="http://arxiv.org/abs/2107.10188"/>
        <updated>2021-07-22T02:03:10.232Z</updated>
        <summary type="html"><![CDATA[The heterogeneous nature of the logical foundations used in different
interactive proof assistant libraries has rendered discovery of similar
mathematical concepts among them difficult. In this paper, we compare a
previously proposed algorithm for matching concepts across libraries with our
unsupervised embedding approach that can help us retrieve similar concepts. Our
approach is based on the fasttext implementation of Word2Vec, on top of which a
tree traversal module is added to adapt its algorithm to the representation
format of our data export pipeline. We compare the explainability,
customizability, and online-servability of the approaches and argue that the
neural embedding approach has more potential to be integrated into an
interactive proof assistant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1"&gt;Cezary Kaliszyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Intersection over Union (wIoU): A New Evaluation Metric for Image Segmentation. (arXiv:2107.09858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09858</id>
        <link href="http://arxiv.org/abs/2107.09858"/>
        <updated>2021-07-22T02:03:10.210Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel evaluation metric for performance
evaluation of semantic segmentation. In recent years, many studies have tried
to train pixel-level classifiers on large-scale image datasets to perform
accurate semantic segmentation. The goal of semantic segmentation is to assign
a class label of each pixel in the scene. It has various potential applications
in computer vision fields e.g., object detection, classification, scene
understanding and Etc. To validate the proposed wIoU evaluation metric, we
tested state-of-the art methods on public benchmark datasets (e.g., KITTI)
based on the proposed wIoU metric and compared with other conventional
evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Yeong-Jun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Counterfactual Fairness in Recommendation. (arXiv:2105.09829v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09829</id>
        <link href="http://arxiv.org/abs/2105.09829"/>
        <updated>2021-07-22T02:03:10.199Z</updated>
        <summary type="html"><![CDATA[Recommender systems are gaining increasing and critical impacts on human and
society since a growing number of users use them for information seeking and
decision making. Therefore, it is crucial to address the potential unfairness
problems in recommendations. Just like users have personalized preferences on
items, users' demands for fairness are also personalized in many scenarios.
Therefore, it is important to provide personalized fair recommendations for
users to satisfy their personalized fairness demands. Besides, previous works
on fair recommendation mainly focus on association-based fairness. However, it
is important to advance from associative fairness notions to causal fairness
notions for assessing fairness more properly in recommender systems. Based on
the above considerations, this paper focuses on achieving personalized
counterfactual fairness for users in recommender systems. To this end, we
introduce a framework for achieving counterfactually fair recommendations
through adversary learning by generating feature-independent user embeddings
for recommendation. The framework allows recommender systems to achieve
personalized fairness for users while also covering non-personalized
situations. Experiments on two real-world datasets with shallow and deep
recommendation algorithms show that our method can generate fairer
recommendations for users with a desirable recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion. (arXiv:2106.10132v1 [eess.AS] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.10132</id>
        <link href="http://arxiv.org/abs/2106.10132"/>
        <updated>2021-07-22T02:03:10.188Z</updated>
        <summary type="html"><![CDATA[One-shot voice conversion (VC), which performs conversion across arbitrary
speakers with only a single target-speaker utterance for reference, can be
effectively achieved by speech representation disentanglement. Existing work
generally ignores the correlation between different speech representations
during training, which causes leakage of content information into the speaker
representation and thus degrades VC performance. To alleviate this issue, we
employ vector quantization (VQ) for content encoding and introduce mutual
information (MI) as the correlation metric during training, to achieve proper
disentanglement of content, speaker and pitch representations, by reducing
their inter-dependencies in an unsupervised manner. Experimental results
reflect the superiority of the proposed method in learning effective
disentangled speech representations for retaining source linguistic content and
intonation variations, while capturing target speaker characteristics. In doing
so, the proposed approach achieves higher speech naturalness and speaker
similarity than current state-of-the-art one-shot VC systems. Our code,
pre-trained models and demo are available at
https://github.com/Wendison/VQMIVC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Disong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1"&gt;Liqun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1"&gt;Yu Ting Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters. (arXiv:2107.09783v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09783</id>
        <link href="http://arxiv.org/abs/2107.09783"/>
        <updated>2021-07-22T02:03:10.176Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on a less explored, but more realistic and complex
problem of domain adaptation in LiDAR semantic segmentation. There is a
significant drop in performance of an existing segmentation model when training
(source domain) and testing (target domain) data originate from different LiDAR
sensors. To overcome this shortcoming, we propose an unsupervised domain
adaptation framework that leverages unlabeled target domain data for
self-supervision, coupled with an unpaired mask transfer strategy to mitigate
the impact of domain shifts. Furthermore, we introduce gated adapter modules
with a small number of parameters into the network to account for target
domain-specific information. Experiments adapting from both real-to-real and
synthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the
significant improvement over prior arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rochan_M/0/1/0/all/0/1"&gt;Mrigank Rochan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aich_S/0/1/0/all/0/1"&gt;Shubhra Aich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corral_Soto_E/0/1/0/all/0/1"&gt;Eduardo R. Corral-Soto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabatchian_A/0/1/0/all/0/1"&gt;Amir Nabatchian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bingbing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Do Pedophiles Tweet? Investigating the Writing Styles and Online Personas of Child Cybersex Traffickers in the Philippines. (arXiv:2107.09881v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09881</id>
        <link href="http://arxiv.org/abs/2107.09881"/>
        <updated>2021-07-22T02:03:10.159Z</updated>
        <summary type="html"><![CDATA[One of the most important humanitarian responsibility of every individual is
to protect the future of our children. This entails not only protection of
physical welfare but also from ill events that can potentially affect the
mental well-being of a child such as sexual coercion and abuse which, in
worst-case scenarios, can result to lifelong trauma. In this study, we perform
a preliminary investigation of how child sex peddlers spread illegal
pornographic content and target minors for sexual activities on Twitter in the
Philippines using Natural Language Processing techniques. Results of our
studies show frequently used and co-occurring words that traffickers use to
spread content as well as four main roles played by these entities that
contribute to the proliferation of child pornography in the country.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision. (arXiv:2107.09852v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09852</id>
        <link href="http://arxiv.org/abs/2107.09852"/>
        <updated>2021-07-22T02:03:10.136Z</updated>
        <summary type="html"><![CDATA[Recent work has shown success in incorporating pre-trained models like BERT
to improve NLP systems. However, existing pre-trained models lack of causal
knowledge which prevents today's NLP systems from thinking like humans. In this
paper, we investigate the problem of injecting causal knowledge into
pre-trained models. There are two fundamental problems: 1) how to collect a
large-scale causal resource from unstructured texts; 2) how to effectively
inject causal knowledge into pre-trained models. To address these issues, we
propose CausalBERT, which collects the largest scale of causal resource using
precise causal patterns and causal embedding techniques. In addition, we adopt
a regularization-based method to preserve the already learned knowledge with an
extra regularization term while injecting causal knowledge. Extensive
experiments on 7 datasets, including four causal pair classification tasks, two
causal QA tasks and a causal inference task, demonstrate that CausalBERT
captures rich causal knowledge and outperforms all pre-trained models-based
state-of-the-art methods, achieving a new causal inference benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1"&gt;Kuo Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bing Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing Multilingual Word Embeddings: A Case Study of Three Indian Languages. (arXiv:2107.10181v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10181</id>
        <link href="http://arxiv.org/abs/2107.10181"/>
        <updated>2021-07-22T02:03:10.125Z</updated>
        <summary type="html"><![CDATA[In this paper, we advance the current state-of-the-art method for debiasing
monolingual word embeddings so as to generalize well in a multilingual setting.
We consider different methods to quantify bias and different debiasing
approaches for monolingual as well as multilingual settings. We demonstrate the
significance of our bias-mitigation approach on downstream NLP applications.
Our proposed methods establish the state-of-the-art performance for debiasing
multilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu
in addition to English. We believe that our work will open up new opportunities
in building unbiased downstream NLP applications that are inherently dependent
on the quality of the word embeddings used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1"&gt;Srijan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garimella_V/0/1/0/all/0/1"&gt;Vishal Garimella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhane_A/0/1/0/all/0/1"&gt;Ayush Suhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Animesh Mukherjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An artificial intelligence natural language processing pipeline for information extraction in neuroradiology. (arXiv:2107.10021v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10021</id>
        <link href="http://arxiv.org/abs/2107.10021"/>
        <updated>2021-07-22T02:03:10.105Z</updated>
        <summary type="html"><![CDATA[The use of electronic health records in medical research is difficult because
of the unstructured format. Extracting information within reports and
summarising patient presentations in a way amenable to downstream analysis
would be enormously beneficial for operational and clinical research. In this
work we present a natural language processing pipeline for information
extraction of radiological reports in neurology. Our pipeline uses a hybrid
sequence of rule-based and artificial intelligence models to accurately extract
and summarise neurological reports. We train and evaluate a custom language
model on a corpus of 150000 radiological reports from National Hospital for
Neurology and Neurosurgery, London MRI imaging. We also present results for
standard NLP tasks on domain-specific neuroradiology datasets. We show our
pipeline, called `neuroNLP', can reliably extract clinically relevant
information from these reports, enabling downstream modelling of reports and
associated imaging on a heretofore unprecedented scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watkins_H/0/1/0/all/0/1"&gt;Henry Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1"&gt;Robert Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Ashwani Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1"&gt;Parashkev Nachev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. (arXiv:2105.04165v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04165</id>
        <link href="http://arxiv.org/abs/2105.04165"/>
        <updated>2021-07-22T02:03:10.088Z</updated>
        <summary type="html"><![CDATA[Geometry problem solving has attracted much attention in the NLP community
recently. The task is challenging as it requires abstract problem understanding
and symbolic reasoning with axiomatic knowledge. However, current datasets are
either small in scale or not publicly available. Thus, we construct a new
large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with
dense annotation in formal language. We further propose a novel geometry
solving approach with formal language and symbolic reasoning, called
Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the
problem text and diagram into formal language automatically via rule-based text
parsing and neural object detecting, respectively. Unlike implicit learning in
existing methods, Inter-GPS incorporates theorem knowledge as conditional rules
and performs symbolic reasoning step by step. Also, a theorem predictor is
designed to infer the theorem application sequence fed to the symbolic solver
for the more efficient and reasonable searching path. Extensive experiments on
the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves
significant improvements over existing methods. The project with code and data
is available at https://lupantech.github.io/inter-gps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ran Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shibiao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Text Classification via Contrastive Adversarial Training. (arXiv:2107.10137v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10137</id>
        <link href="http://arxiv.org/abs/2107.10137"/>
        <updated>2021-07-22T02:03:10.075Z</updated>
        <summary type="html"><![CDATA[We propose a simple and general method to regularize the fine-tuning of
Transformer-based encoders for text classification tasks. Specifically, during
fine-tuning we generate adversarial examples by perturbing the word embeddings
of the model and perform contrastive learning on clean and adversarial examples
in order to teach the model to learn noise-invariant representations. By
training on both clean and adversarial examples along with the additional
contrastive objective, we observe consistent improvement over standard
fine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned
BERT Large model outperforms BERT Large baseline by 1.7% on average, and our
fine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We
additionally validate our method in different domains using three intent
classification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa
Large baseline by 1-2% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lin Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1"&gt;Chung-Wei Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1"&gt;Avirup Sil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1"&gt;Saloni Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09729</id>
        <link href="http://arxiv.org/abs/2107.09729"/>
        <updated>2021-07-22T02:03:10.063Z</updated>
        <summary type="html"><![CDATA[We combine beam search with the probabilistic pruning technique of nucleus
sampling to create two deterministic nucleus search algorithms for natural
language generation. The first algorithm, p-exact search, locally prunes the
next-token distribution and performs an exact search over the remaining space.
The second algorithm, dynamic beam search, shrinks and expands the beam size
according to the entropy of the candidate's probability distribution. Despite
the probabilistic intuition behind nucleus search, experiments on machine
translation and summarization benchmarks show that both algorithms reach the
same performance levels as standard beam search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1"&gt;Uri Shaham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1"&gt;Omer Levy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Causality Extraction From Natural Language Requirements Using Recursive Neural Tensor Networks. (arXiv:2107.09980v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09980</id>
        <link href="http://arxiv.org/abs/2107.09980"/>
        <updated>2021-07-22T02:03:10.035Z</updated>
        <summary type="html"><![CDATA[[Context:] Causal relations (e.g., If A, then B) are prevalent in functional
requirements. For various applications of AI4RE, e.g., the automatic derivation
of suitable test cases from requirements, automatically extracting such causal
statements are a basic necessity. [Problem:] We lack an approach that is able
to extract causal relations from natural language requirements in fine-grained
form. Specifically, existing approaches do not consider the combinatorics
between causes and effects. They also do not allow to split causes and effects
into more granular text fragments (e.g., variable and condition), making the
extracted relations unsuitable for automatic test case derivation. [Objective &
Contributions:] We address this research gap and make the following
contributions: First, we present the Causality Treebank, which is the first
corpus of fully labeled binary parse trees representing the composition of
1,571 causal requirements. Second, we propose a fine-grained causality
extractor based on Recursive Neural Tensor Networks. Our approach is capable of
recovering the composition of causal statements written in natural language and
achieves a F1 score of 74 % in the evaluation on the Causality Treebank. Third,
we disclose our open data sets as well as our code to foster the discourse on
the automatic extraction of causality in the RE community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1"&gt;Jannik Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springer_T/0/1/0/all/0/1"&gt;Tobias Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1"&gt;Julian Frattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Femmer_H/0/1/0/all/0/1"&gt;Henning Femmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1"&gt;Andreas Vogelsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_D/0/1/0/all/0/1"&gt;Daniel Mendez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Checkovid: A COVID-19 misinformation detection system on Twitter using network and content mining perspectives. (arXiv:2107.09768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09768</id>
        <link href="http://arxiv.org/abs/2107.09768"/>
        <updated>2021-07-22T02:03:10.013Z</updated>
        <summary type="html"><![CDATA[During the COVID-19 pandemic, social media platforms were ideal for
communicating due to social isolation and quarantine. Also, it was the primary
source of misinformation dissemination on a large scale, referred to as the
infodemic. Therefore, automatic debunking misinformation is a crucial problem.
To tackle this problem, we present two COVID-19 related misinformation datasets
on Twitter and propose a misinformation detection system comprising
network-based and content-based processes based on machine learning algorithms
and NLP techniques. In the network-based process, we focus on social
properties, network characteristics, and users. On the other hand, we classify
misinformation using the content of the tweets directly in the content-based
process, which contains text classification models (paragraph-level and
sentence-level) and similarity models. The evaluation results on the
network-based process show the best results for the artificial neural network
model with an F1 score of 88.68%. In the content-based process, our novel
similarity models, which obtained an F1 score of 90.26%, show an improvement in
the misinformation classification results compared to the network-based models.
In addition, in the text classification models, the best result was achieved
using the stacking ensemble-learning model by obtaining an F1 score of 95.18%.
Furthermore, we test our content-based models on the Constraint@AAAI2021
dataset, and by getting an F1 score of 94.38%, we improve the baseline results.
Finally, we develop a fact-checking website called Checkovid that uses each
process to detect misinformative and informative claims in the domain of
COVID-19 from different perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dadgar_S/0/1/0/all/0/1"&gt;Sajad Dadgar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1"&gt;Mehdi Ghatee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CATE: CAusality Tree Extractor from Natural Language Requirements. (arXiv:2107.10023v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.10023</id>
        <link href="http://arxiv.org/abs/2107.10023"/>
        <updated>2021-07-22T02:03:09.971Z</updated>
        <summary type="html"><![CDATA[Causal relations (If A, then B) are prevalent in requirements artifacts.
Automatically extracting causal relations from requirements holds great
potential for various RE activities (e.g., automatic derivation of suitable
test cases). However, we lack an approach capable of extracting causal
relations from natural language with reasonable performance. In this paper, we
present our tool CATE (CAusality Tree Extractor), which is able to parse the
composition of a causal relation as a tree structure. CATE does not only
provide an overview of causes and effects in a sentence, but also reveals their
semantic coherence by translating the causal relation into a binary tree. We
encourage fellow researchers and practitioners to use CATE at
https://causalitytreeextractor.com/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jadallah_N/0/1/0/all/0/1"&gt;Noah Jadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1"&gt;Jannik Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1"&gt;Julian Frattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1"&gt;Andreas Vogelsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provenance, Anonymisation and Data Environments: a Unifying Construction. (arXiv:2107.09966v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.09966</id>
        <link href="http://arxiv.org/abs/2107.09966"/>
        <updated>2021-07-22T02:03:09.935Z</updated>
        <summary type="html"><![CDATA[The Anonymisation Decision-making Framework (ADF) operationalizes the risk
management of data exchange between organizations, referred to as "data
environments". The second edition of ADF has increased its emphasis on modeling
data flows, highlighting a potential new use of provenance information to
support anonymisation decision-making. In this paper, we provide a use case
that showcases this functionality more. Based on this use case, we identify how
provenance information could be utilized within the ADF framework, and identify
a currently un-met requirement which is the modeling of \textit{data
environments}. We show how data environments can be implemented within the W3C
PROV in four different ways. We analyze the costs and benefits of each
approach, and consider another use case as a partial check for completeness. We
then summarize our findings and suggest ways forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarwar_M/0/1/0/all/0/1"&gt;Muhammad Aslam Jarwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1"&gt;Adriane Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elliot_M/0/1/0/all/0/1"&gt;Mark Elliot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raji_F/0/1/0/all/0/1"&gt;Fatemeh Raji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Domain-Agnostic Contrastive Learning. (arXiv:2011.04419v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04419</id>
        <link href="http://arxiv.org/abs/2011.04419"/>
        <updated>2021-07-21T02:01:37.638Z</updated>
        <summary type="html"><![CDATA[Despite recent success, most contrastive self-supervised learning methods are
domain-specific, relying heavily on data augmentation techniques that require
knowledge about a particular domain, such as image cropping and rotation. To
overcome such limitation, we propose a novel domain-agnostic approach to
contrastive learning, named DACL, that is applicable to domains where
invariances, and thus, data augmentation techniques, are not readily available.
Key to our approach is the use of Mixup noise to create similar and dissimilar
examples by mixing data samples differently either at the input or hidden-state
levels. To demonstrate the effectiveness of DACL, we conduct experiments across
various domains such as tabular data, images, and graphs. Our results show that
DACL not only outperforms other domain-agnostic noising methods, such as
Gaussian-noise, but also combines well with domain-specific methods, such as
SimCLR, to improve self-supervised visual representation learning. Finally, we
theoretically analyze our method and show advantages over the Gaussian-noise
based contrastive learning approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vikas Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1"&gt;Minh-Thang Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection. (arXiv:2107.09519v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09519</id>
        <link href="http://arxiv.org/abs/2107.09519"/>
        <updated>2021-07-21T02:01:37.625Z</updated>
        <summary type="html"><![CDATA[Acoustic monitoring for machine fault detection is a recent and expanding
research path that has already provided promising results for industries.
However, it is impossible to collect enough data to learn all types of faults
from a machine. Thus, new algorithms, trained using data from healthy
conditions only, were developed to perform unsupervised anomaly detection. A
key issue in the development of these algorithms is the noise in the signals,
as it impacts the anomaly detection performance. In this work, we propose a
powerful data-driven and quasi non-parametric denoising strategy for spectral
data based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)
decomposition. This method is particularly adapted for machine emitting
stationary sound. We demonstrate in a case study, the Malfunctioning Industrial
Machine Investigation and Inspection (MIMII) baseline, how the use of our
denoising strategy leads to a sensible improvement of the unsupervised anomaly
detection. Such approaches are capable to make sound-based monitoring of
industrial processes more reliable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gaetan_F/0/1/0/all/0/1"&gt;Frusque Gaetan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gabriel_M/0/1/0/all/0/1"&gt;Michau Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Olga_F/0/1/0/all/0/1"&gt;Fink Olga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-07-21T02:01:37.619Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards in
silico experimentation, is to synthesise the imagery itself. Here, we propose
Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence
microscopy imagery of living cells, based on a past experiment. This novel
generative adversarial network synthesises a multi-domain sequence of
consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live
yeast cells in microstructured environments and train on a dataset recorded in
our laboratory. The simulation captures underlying biophysical factors and time
dependencies, such as cell morphology, growth, physical interactions, as well
as the intensity of a fluorescent reporter protein. An immediate application is
to generate additional training and validation data for feature extraction
algorithms or to aid and expedite development of advanced experimental
techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse within Sparse Gaussian Processes using Neighbor Information. (arXiv:2011.05041v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05041</id>
        <link href="http://arxiv.org/abs/2011.05041"/>
        <updated>2021-07-21T02:01:37.612Z</updated>
        <summary type="html"><![CDATA[Approximations to Gaussian processes based on inducing variables, combined
with variational inference techniques, enable state-of-the-art sparse
approaches to infer GPs at scale through mini batch-based learning. In this
work, we address one limitation of sparse GPs, which is due to the challenge in
dealing with a large number of inducing variables without imposing a special
structure on the inducing inputs. In particular, we introduce a novel
hierarchical prior, which imposes sparsity on the set of inducing variables. We
treat our model variationally, and we experimentally show considerable
computational gains compared to standard sparse GPs when sparsity on the
inducing variables is realized considering the nearest inducing inputs of a
random mini-batch of the data. We perform an extensive experimental validation
that demonstrates the effectiveness of our approach compared to the
state-of-the-art. Our approach enables the possibility to use sparse GPs using
a large number of inducing points without incurring a prohibitive computational
cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tran_G/0/1/0/all/0/1"&gt;Gia-Lac Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Milios_D/0/1/0/all/0/1"&gt;Dimitrios Milios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1"&gt;Pietro Michiardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1"&gt;Maurizio Filippone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09609</id>
        <link href="http://arxiv.org/abs/2107.09609"/>
        <updated>2021-07-21T02:01:37.296Z</updated>
        <summary type="html"><![CDATA[Detecting customized moments and highlights from videos given natural
language (NL) user queries is an important but under-studied topic. One of the
challenges in pursuing this direction is the lack of annotated data. To address
this issue, we present the Query-based Video Highlights (QVHighlights) dataset.
It consists of over 10,000 YouTube videos, covering a wide range of topics,
from everyday activities and travel in lifestyle vlog videos to social and
political activities in news videos. Each video in the dataset is annotated
with: (1) a human-written free-form NL query, (2) relevant moments in the video
w.r.t. the query, and (3) five-point scale saliency scores for all
query-relevant clips. This comprehensive annotation enables us to develop and
evaluate systems that detect relevant moments as well as salient highlights for
diverse, flexible user queries. We also present a strong baseline for this
task, Moment-DETR, a transformer encoder-decoder model that views moment
retrieval as a direct set prediction problem, taking extracted video and query
representations as inputs and predicting moment coordinates and saliency scores
end-to-end. While our model does not utilize any human prior, we show that it
performs competitively when compared to well-engineered architectures. With
weakly supervised pretraining using ASR captions, Moment-DETR substantially
outperforms previous methods. Lastly, we present several ablations and
visualizations of Moment-DETR. Data and code is publicly available at
https://github.com/jayleicn/moment_detr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09391</id>
        <link href="http://arxiv.org/abs/2107.09391"/>
        <updated>2021-07-21T02:01:37.289Z</updated>
        <summary type="html"><![CDATA[We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion and Gaussian perturbations improves, while even improving the
performance on clean images slightly without performing any data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1"&gt;Sadaf Gulshad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06054</id>
        <link href="http://arxiv.org/abs/2106.06054"/>
        <updated>2021-07-21T02:01:37.283Z</updated>
        <summary type="html"><![CDATA[In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sumon Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedProto: Federated Prototype Learning over Heterogeneous Devices. (arXiv:2105.00243v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00243</id>
        <link href="http://arxiv.org/abs/2105.00243"/>
        <updated>2021-07-21T02:01:37.276Z</updated>
        <summary type="html"><![CDATA[The heterogeneity across devices usually hinders the optimization convergence
and generalization performance of federated learning (FL) when the aggregation
of devices' knowledge occurs in the gradient space. For example, devices may
differ in terms of data distribution, network latency, input/output space,
and/or model architecture, which can easily lead to the misalignment of their
local gradients. To improve the tolerance to heterogeneity, we propose a novel
federated prototype learning (FedProto) framework in which the devices and
server communicate the class prototypes instead of the gradients. FedProto
aggregates the local prototypes collected from different devices, and then
sends the global prototypes back to all devices to regularize the training of
local models. The training on each device aims to minimize the classification
error on the local data while keeping the resulting local prototypes
sufficiently close to the corresponding global ones. Through experiments, we
propose a benchmark setting tailored for heterogeneous FL, with FedProto
outperforming several recent FL approaches on multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yue Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Guodong Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengqi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ps and Qs: Quantization-aware pruning for efficient low latency neural network inference. (arXiv:2102.11289v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11289</id>
        <link href="http://arxiv.org/abs/2102.11289"/>
        <updated>2021-07-21T02:01:37.257Z</updated>
        <summary type="html"><![CDATA[Efficient machine learning implementations optimized for inference in
hardware have wide-ranging benefits, depending on the application, from lower
inference latency to higher data throughput and reduced energy consumption. Two
popular techniques for reducing computation in neural networks are pruning,
removing insignificant synapses, and quantization, reducing the precision of
the calculations. In this work, we explore the interplay between pruning and
quantization during the training of neural networks for ultra low latency
applications targeting high energy physics use cases. Techniques developed for
this study have potential applications across many other domains. We study
various configurations of pruning during quantization-aware training, which we
term quantization-aware pruning, and the effect of techniques like
regularization, batch normalization, and different pruning schemes on
performance, computational complexity, and information content metrics. We find
that quantization-aware pruning yields more computationally efficient models
than either pruning or quantization alone for our task. Further,
quantization-aware pruning typically performs similar to or better in terms of
computational efficiency compared to other neural architecture search
techniques like Bayesian optimization. Surprisingly, while networks with
different training configurations can have similar performance for the
benchmark application, the information content in the network can vary
significantly, affecting its generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hawks_B/0/1/0/all/0/1"&gt;Benjamin Hawks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_N/0/1/0/all/0/1"&gt;Nicholas J. Fraser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1"&gt;Alessandro Pappalardo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1"&gt;Nhan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umuroglu_Y/0/1/0/all/0/1"&gt;Yaman Umuroglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning with Prototypical Representations. (arXiv:2102.11271v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11271</id>
        <link href="http://arxiv.org/abs/2102.11271"/>
        <updated>2021-07-21T02:01:37.250Z</updated>
        <summary type="html"><![CDATA[Learning effective representations in image-based environments is crucial for
sample efficient Reinforcement Learning (RL). Unfortunately, in RL,
representation learning is confounded with the exploratory experience of the
agent -- learning a useful representation requires diverse data, while
effective exploration is only possible with coherent representations.
Furthermore, we would like to learn representations that not only generalize
across tasks but also accelerate downstream exploration for efficient
task-specific training. To address these challenges we propose Proto-RL, a
self-supervised framework that ties representation learning with exploration
through prototypical representations. These prototypes simultaneously serve as
a summarization of the exploratory experience of an agent as well as a basis
for representing observations. We pre-train these task-agnostic representations
and prototypes on environments without downstream task information. This
enables state-of-the-art downstream policy learning on a set of difficult
continuous control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1"&gt;Denis Yarats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1"&gt;Lerrel Pinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra. (arXiv:2011.04125v5 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04125</id>
        <link href="http://arxiv.org/abs/2011.04125"/>
        <updated>2021-07-21T02:01:37.244Z</updated>
        <summary type="html"><![CDATA[We create classical (non-quantum) dynamic data structures supporting queries
for recommender systems and least-squares regression that are comparable to
their quantum analogues. De-quantizing such algorithms has received a flurry of
attention in recent years; we obtain sharper bounds for these problems. More
significantly, we achieve these improvements by arguing that the previous
quantum-inspired algorithms for these problems are doing leverage or
ridge-leverage score sampling in disguise; these are powerful and standard
techniques in randomized numerical linear algebra. With this recognition, we
are able to employ the large body of work in numerical linear algebra to obtain
algorithms for these problems that are simpler or faster (or both) than
existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1"&gt;Nadiia Chepurko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1"&gt;Kenneth L. Clarkson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1"&gt;Lior Horesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the 2020 US Presidential Election with Twitter. (arXiv:2107.09640v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.09640</id>
        <link href="http://arxiv.org/abs/2107.09640"/>
        <updated>2021-07-21T02:01:37.237Z</updated>
        <summary type="html"><![CDATA[One major sub-domain in the subject of polling public opinion with social
media data is electoral prediction. Electoral prediction utilizing social media
data potentially would significantly affect campaign strategies, complementing
traditional polling methods and providing cheaper polling in real-time. First,
this paper explores past successful methods from research for analysis and
prediction of the 2020 US Presidential Election using Twitter data. Then, this
research proposes a new method for electoral prediction which combines
sentiment, from NLP on the text of tweets, and structural data with aggregate
polling, a time series analysis, and a special focus on Twitter users critical
to the election. Though this method performed worse than its baseline of
polling predictions, it is inconclusive whether this is an accurate method for
predicting elections due to scarcity of data. More research and more data are
needed to accurately measure this method's overall effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caballero_M/0/1/0/all/0/1"&gt;Michael Caballero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14686</id>
        <link href="http://arxiv.org/abs/2105.14686"/>
        <updated>2021-07-21T02:01:37.231Z</updated>
        <summary type="html"><![CDATA[Hyperbolic neural networks have shown great potential for modeling complex
data. However, existing hyperbolic networks are not completely hyperbolic, as
they encode features in a hyperbolic space yet formalize most of their
operations in the tangent space (a Euclidean subspace) at the origin of the
hyperbolic space. This hybrid method greatly limits the modeling ability of
networks. In this paper, we propose a fully hyperbolic framework to build
hyperbolic networks based on the Lorentz model by adapting the Lorentz
transformations (including boost and rotation) to formalize essential
operations of neural networks. Moreover, we also prove that linear
transformation in tangent spaces used by existing hyperbolic networks is a
relaxation of the Lorentz rotation and does not include the boost, implicitly
limiting the capabilities of existing hyperbolic networks. The experimental
results on four NLP tasks show that our method has better performance for
building both shallow and deep networks. Our code will be released to
facilitate follow-up research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weize Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hexu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Buffered Asynchronous Aggregation. (arXiv:2106.06639v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06639</id>
        <link href="http://arxiv.org/abs/2106.06639"/>
        <updated>2021-07-21T02:01:37.215Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) trains a shared model across distributed devices
while keeping the training data on the devices. Most FL schemes are
synchronous: they perform a synchronized aggregation of model updates from
individual devices. Synchronous training can be slow because of late-arriving
devices (stragglers). On the other hand, completely asynchronous training makes
FL less private because of incompatibility with secure aggregation. In this
work, we propose a model aggregation scheme, FedBuff, that combines the best
properties of synchronous and asynchronous FL. Similar to synchronous FL,
FedBuff is compatible with secure aggregation. Similar to asynchronous FL,
FedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and
send updates to the server. The server aggregates client updates in a private
buffer until updates have been received, at which point a server model update
is immediately performed. We provide theoretical convergence guarantees for
FedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x
faster than previous proposals for synchronous FL (e.g., FedAvgM), and up to
2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We
show that FedBuff is robust to different staleness distributions and is more
scalable than synchronous FL techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1"&gt;John Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kshitiz Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Hongyuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1"&gt;Ashkan Yousefpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1"&gt;Mani Malek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huba_D/0/1/0/all/0/1"&gt;Dzmitry Huba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-07-21T02:01:37.209Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated data-driven approach for gap filling in the time series using evolutionary learning. (arXiv:2103.01124v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01124</id>
        <link href="http://arxiv.org/abs/2103.01124"/>
        <updated>2021-07-21T02:01:37.203Z</updated>
        <summary type="html"><![CDATA[In the paper, we propose an adaptive data-driven model-based approach for
filling the gaps in time series. The approach is based on the automated
evolutionary identification of the optimal structure for a composite
data-driven model. It allows adapting the model for the effective gap-filling
in a specific dataset without the involvement of the data scientist. As a case
study, both synthetic and real datasets from different fields (environmental,
economic, etc) are used. The experiments confirm that the proposed approach
allows achieving the higher quality of the gap restoration and improve the
effectiveness of forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1"&gt;Mikhail Sarafanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1"&gt;Nikolay O. Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1"&gt;Anna V. Kalyuzhnaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Precision-Weighted Federated Learning. (arXiv:2107.09627v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09627</id>
        <link href="http://arxiv.org/abs/2107.09627"/>
        <updated>2021-07-21T02:01:37.196Z</updated>
        <summary type="html"><![CDATA[Federated Learning using the Federated Averaging algorithm has shown great
advantages for large-scale applications that rely on collaborative learning,
especially when the training data is either unbalanced or inaccessible due to
privacy constraints. We hypothesize that Federated Averaging underestimates the
full extent of heterogeneity of data when the aggregation is performed. We
propose Precision-weighted Federated Learning a novel algorithm that takes into
account the variance of the stochastic gradients when computing the weighted
average of the parameters of models trained in a Federated Learning setting.
With Precision-weighted Federated Learning, we provide an alternate averaging
scheme that leverages the heterogeneity of the data when it has a large
diversity of features in its composition. Our method was evaluated using
standard image classification datasets with two different data partitioning
strategies (IID/non-IID) to measure the performance and speed of our method in
resource-constrained environments, such as mobile and IoT devices. We obtained
a good balance between computational efficiency and convergence rates with
Precision-weighted Federated Learning. Our performance evaluations show 9%
better predictions with MNIST, 18% with Fashion-MNIST, and 5% with CIFAR-10 in
the non-IID setting. Further reliability evaluations ratify the stability in
our method by reaching a 99% reliability index with IID partitions and 96% with
non-IID partitions. In addition, we obtained a 20x speedup on Fashion-MNIST
with only 10 clients and up to 37x with 100 clients participating in the
aggregation concurrently per communication round. The results indicate that
Precision-weighted Federated Learning is an effective and faster alternative
approach for aggregating private data, especially in domains where data is
highly heterogeneous.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reyes_J/0/1/0/all/0/1"&gt;Jonatan Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorio_L/0/1/0/all/0/1"&gt;Lisa Di Jorio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_Kam_C/0/1/0/all/0/1"&gt;Cecile Low-Kam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersten_Oertel_M/0/1/0/all/0/1"&gt;Marta Kersten-Oertel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and Nonlocal Effects. (arXiv:2105.00304v2 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00304</id>
        <link href="http://arxiv.org/abs/2105.00304"/>
        <updated>2021-07-21T02:01:37.189Z</updated>
        <summary type="html"><![CDATA[Machine-learned force fields (ML-FFs) combine the accuracy of ab initio
methods with the efficiency of conventional force fields. However, current
ML-FFs typically ignore electronic degrees of freedom, such as the total charge
or spin state, and assume chemical locality, which is problematic when
molecules have inconsistent electronic states, or when nonlocal effects play a
significant role. This work introduces SpookyNet, a deep neural network for
constructing ML-FFs with explicit treatment of electronic degrees of freedom
and quantum nonlocality. Chemically meaningful inductive biases and analytical
corrections built into the network architecture allow it to properly model
physical limits. SpookyNet improves upon the current state-of-the-art (or
achieves similar performance) on popular quantum chemistry data sets. Notably,
it is able to generalize across chemical and conformational space and can
leverage the learned chemical insights, e.g. by predicting unknown spin states,
thus helping to close a further important remaining gap for today's machine
learning models in quantum chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Unke_O/0/1/0/all/0/1"&gt;Oliver T. Unke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gastegger_M/0/1/0/all/0/1"&gt;Michael Gastegger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schutt_K/0/1/0/all/0/1"&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sauceda_H/0/1/0/all/0/1"&gt;Huziel E. Sauceda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Federated Learning Framework with Online Laplace Approximation. (arXiv:2102.01936v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01936</id>
        <link href="http://arxiv.org/abs/2102.01936"/>
        <updated>2021-07-21T02:01:37.170Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) allows multiple clients to collaboratively learn a
globally shared model through cycles of model aggregation and local model
training, without the need to share data. Most existing FL methods train local
models separately on different clients, and then simply average their
parameters to obtain a centralized model on the server side. However, these
approaches generally suffer from large aggregation errors and severe local
forgetting, which are particularly bad in heterogeneous data settings. To
tackle these issues, in this paper, we propose a novel FL framework that uses
online Laplace approximation to approximate posteriors on both the client and
server side. On the server side, a multivariate Gaussian product mechanism is
employed to construct and maximize a global posterior, largely reducing the
aggregation errors induced by large discrepancies between local models. On the
client side, a prior loss that uses the global posterior probabilistic
parameters delivered from the server is designed to guide the local training.
Binding such learning constraints from other clients enables our method to
mitigate local forgetting. Finally, we achieve state-of-the-art results on
several benchmarks, clearly demonstrating the advantages of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liangxi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guo-Jun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Bandit Learning Through Heterogeneous Feedback Aggregation. (arXiv:2010.15390v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15390</id>
        <link href="http://arxiv.org/abs/2010.15390"/>
        <updated>2021-07-21T02:01:37.164Z</updated>
        <summary type="html"><![CDATA[In many real-world applications, multiple agents seek to learn how to perform
highly related yet slightly different tasks in an online bandit learning
protocol. We formulate this problem as the $\epsilon$-multi-player multi-armed
bandit problem, in which a set of players concurrently interact with a set of
arms, and for each arm, the reward distributions for all players are similar
but not necessarily identical. We develop an upper confidence bound-based
algorithm, RobustAgg$(\epsilon)$, that adaptively aggregates rewards collected
by different players. In the setting where an upper bound on the pairwise
similarities of reward distributions between players is known, we achieve
instance-dependent regret guarantees that depend on the amenability of
information sharing across players. We complement these upper bounds with
nearly matching lower bounds. In the setting where pairwise similarities are
unknown, we provide a lower bound, as well as an algorithm that trades off
minimax regret guarantees for adaptivity to unknown similarity structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chicheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Manish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riek_L/0/1/0/all/0/1"&gt;Laurel D. Riek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-07-21T02:01:37.157Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1"&gt;Johannes Otterbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Discovery with Multi-Domain LiNGAM for Latent Factors. (arXiv:2009.09176v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09176</id>
        <link href="http://arxiv.org/abs/2009.09176"/>
        <updated>2021-07-21T02:01:37.150Z</updated>
        <summary type="html"><![CDATA[Discovering causal structures among latent factors from observed data is a
particularly challenging problem. Despite some efforts for this problem,
existing methods focus on the single-domain data only. In this paper, we
propose Multi-Domain Linear Non-Gaussian Acyclic Models for Latent Factors
(MD-LiNA), where the causal structure among latent factors of interest is
shared for all domains, and we provide its identification results. The model
enriches the causal representation for multi-domain data. We propose an
integrated two-phase algorithm to estimate the model. In particular, we first
locate the latent factors and estimate the factor loading matrix. Then to
uncover the causal structure among shared latent factors of interest, we derive
a score function based on the characterization of independence relations
between external influences and the dependence relations between multi-domain
latent factors and latent factors of interest. We show that the proposed method
provides locally consistent estimators. Experimental results on both synthetic
and real-world data demonstrate the efficacy and robustness of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1"&gt;Shohei Shimizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1"&gt;Feng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_M/0/1/0/all/0/1"&gt;Michio Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scan Specific Artifact Reduction in K-space (SPARK) Neural Networks Synergize with Physics-based Reconstruction to Accelerate MRI. (arXiv:2104.01188v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01188</id>
        <link href="http://arxiv.org/abs/2104.01188"/>
        <updated>2021-07-21T02:01:37.134Z</updated>
        <summary type="html"><![CDATA[Purpose: To develop a scan-specific model that estimates and corrects k-space
errors made when reconstructing accelerated Magnetic Resonance Imaging (MRI)
data.

Methods: Scan-Specific Artifact Reduction in k-space (SPARK) trains a
convolutional-neural-network to estimate and correct k-space errors made by an
input reconstruction technique by back-propagating from the mean-squared-error
loss between an auto-calibration signal (ACS) and the input technique's
reconstructed ACS. First, SPARK is applied to GRAPPA and demonstrates improved
robustness over other scan-specific models, such as RAKI and residual-RAKI.
Subsequent experiments demonstrate that SPARK synergizes with residual-RAKI to
improve reconstruction performance. SPARK also improves reconstruction quality
when applied to advanced acquisition and reconstruction techniques like 2D
virtual coil (VC-) GRAPPA, 2D LORAKS, 3D GRAPPA without an integrated ACS
region, and 2D/3D wave-encoded images.

Results: SPARK yields 1.5x - 2x RMSE reduction when applied to GRAPPA and
improves robustness to ACS size for various acceleration rates in comparison to
other scan-specific techniques. When applied to advanced reconstruction
techniques such as residual-RAKI, 2D VC-GRAPPA and LORAKS, SPARK achieves up to
20% RMSE improvement. SPARK with 3D GRAPPA also improves performance by ~2x and
perceived image quality without a fully sampled ACS region. Finally, SPARK
synergizes with non-cartesian 2D and 3D wave-encoding imaging by reducing RMSE
between 20-25% and providing qualitative improvements.

Conclusion: SPARK synergizes with physics-based acquisition and
reconstruction techniques to improve accelerated MRI by training scan-specific
models to estimate and correct reconstruction errors in k-space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arefeen_Y/0/1/0/all/0/1"&gt;Yamin Arefeen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Beker_O/0/1/0/all/0/1"&gt;Onur Beker&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jaejin Cho&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Heng Yu&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt; (1 and 5 and 6), &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1"&gt;Berkin Bilgic&lt;/a&gt; (3 and 5 and 7) ((1) Massachusetts Institute of Technology, (2) &amp;#xc9;cole Polytechnique F&amp;#xe9;d&amp;#xe9;rale de Lausanne, (3) Athinoula A. Martinos Center for Biomedical Imaging (4) Tsinghua University, (5) Harvard-MIT Health Sciences and Technology, (6) Institute for Medical Engineering and Science, (7) Harvard Medical School)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-free calibration guarantees for histogram binning without sample splitting. (arXiv:2105.04656v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04656</id>
        <link href="http://arxiv.org/abs/2105.04656"/>
        <updated>2021-07-21T02:01:37.126Z</updated>
        <summary type="html"><![CDATA[We prove calibration guarantees for the popular histogram binning (also
called uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram
binning has displayed strong practical performance, but theoretical guarantees
have only been shown for sample split versions that avoid 'double dipping' the
data. We demonstrate that the statistical cost of sample splitting is
practically significant on a credit default dataset. We then prove calibration
guarantees for the original method that double dips the data, using a certain
Markov property of order statistics. Based on our results, we make practical
recommendations for choosing the number of bins in histogram binning. In our
illustrative simulations, we propose a new tool for assessing calibration --
validity plots -- which provide more information than an ECE estimate. Code for
this work will be made publicly available at
https://github.com/aigen/df-posthoc-calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1"&gt;Chirag Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya K. Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13792</id>
        <link href="http://arxiv.org/abs/2106.13792"/>
        <updated>2021-07-21T02:01:37.120Z</updated>
        <summary type="html"><![CDATA[Although the optimization objectives for learning neural networks are highly
non-convex, gradient-based methods have been wildly successful at learning
neural networks in practice. This juxtaposition has led to a number of recent
studies on provable guarantees for neural networks trained by gradient descent.
Unfortunately, the techniques in these works are often highly specific to the
problem studied in each setting, relying on different assumptions on the
distribution, optimization parameters, and network architectures, making it
difficult to generalize across different settings. In this work, we propose a
unified non-convex optimization framework for the analysis of neural network
training. We introduce the notions of proxy convexity and proxy
Polyak-Lojasiewicz (PL) inequalities, which are satisfied if the original
objective function induces a proxy objective function that is implicitly
minimized when using gradient methods. We show that stochastic gradient descent
(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads
to efficient guarantees for proxy objective functions. We further show that
many existing guarantees for neural networks trained by gradient descent can be
unified through proxy convexity and proxy PL inequalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Topological Dependencies of Recurrent Congestion in Road Networks. (arXiv:2107.09554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09554</id>
        <link href="http://arxiv.org/abs/2107.09554"/>
        <updated>2021-07-21T02:01:37.112Z</updated>
        <summary type="html"><![CDATA[The discovery of spatio-temporal dependencies within urban road networks that
cause Recurrent Congestion (RC) patterns is crucial for numerous real-world
applications, including urban planning and scheduling of public transportation
services. While most existing studies investigate temporal patterns of RC
phenomena, the influence of the road network topology on RC is often
overlooked. This article proposes the ST-Discovery algorithm, a novel
unsupervised spatio-temporal data mining algorithm that facilitates the
effective data-driven discovery of RC dependencies induced by the road network
topology using real-world traffic data. We factor out regularly reoccurring
traffic phenomena, such as rush hours, mainly induced by the daytime, by
modelling and systematically exploiting temporal traffic load outliers. We
present an algorithm that first constructs connected subgraphs of the road
network based on the traffic speed outliers. Second, the algorithm identifies
pairs of subgraphs that indicate spatio-temporal correlations in their traffic
load behaviour to identify topological dependencies within the road network.
Finally, we rank the identified subgraph pairs based on the dependency score
determined by our algorithm. Our experimental results demonstrate that
ST-Discovery can effectively reveal topological dependencies in urban road
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1"&gt;Nicolas Tempelmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feuerhake_U/0/1/0/all/0/1"&gt;Udo Feuerhake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wage_O/0/1/0/all/0/1"&gt;Oskar Wage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1"&gt;Elena Demidova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Parametric Estimation of Manifolds from Noisy Data. (arXiv:2105.04754v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04754</id>
        <link href="http://arxiv.org/abs/2105.04754"/>
        <updated>2021-07-21T02:01:37.106Z</updated>
        <summary type="html"><![CDATA[A common observation in data-driven applications is that high dimensional
data has a low intrinsic dimension, at least locally. In this work, we consider
the problem of estimating a $d$ dimensional sub-manifold of $\mathbb{R}^D$ from
a finite set of noisy samples. Assuming that the data was sampled uniformly
from a tubular neighborhood of $\mathcal{M}\in \mathcal{C}^k$, a compact
manifold without boundary, we present an algorithm that takes a point $r$ from
the tubular neighborhood and outputs $\hat p_n\in \mathbb{R}^D$, and
$\widehat{T_{\hat p_n}\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.
We prove that as the number of samples $n\to\infty$ the point $\hat p_n$
converges to $p\in \mathcal{M}$ and $\widehat{T_{\hat p_n}\mathcal{M}}$
converges to $T_p\mathcal{M}$ (the tangent space at that point) with high
probability. Furthermore, we show that the estimation yields asymptotic rates
of convergence of $n^{-\frac{k}{2k + d}}$ for the point estimation and
$n^{-\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates
are known to be optimal for the case of function estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Aizenbud_Y/0/1/0/all/0/1"&gt;Yariv Aizenbud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sober_B/0/1/0/all/0/1"&gt;Barak Sober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09602</id>
        <link href="http://arxiv.org/abs/2107.09602"/>
        <updated>2021-07-21T02:01:37.098Z</updated>
        <summary type="html"><![CDATA[The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of
lives and has affected all aspects of human life. This paper focuses on the
application of deep learning (DL) models to medical imaging and drug discovery
for managing COVID-19 disease. In this article, we detail various medical
imaging-based studies such as X-rays and computed tomography (CT) images along
with DL methods for classifying COVID-19 affected versus pneumonia. The
applications of DL techniques to medical images are further described in terms
of image localization, segmentation, registration, and classification leading
to COVID-19 detection. The reviews of recent papers indicate that the highest
classification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is
applied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients
and 365 normal people. Furthermore, it can be seen that the best classification
accuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT
image dataset of 7500 samples where COVID-19 patients, lung tumor patients and
normal people are equal in number. Moreover, we illustrate the potential DL
techniques in drug or vaccine discovery in combating the coronavirus. Finally,
we address a number of problems, concerns and future research directions
relevant to DL applications for COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1"&gt;Subrato Bharati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1"&gt;Prajoy Podder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1"&gt;M. Rubaiyat Hossain Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1"&gt;V.B. Surya Prasath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Laplace for Bayesian neural networks. (arXiv:2103.00222v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00222</id>
        <link href="http://arxiv.org/abs/2103.00222"/>
        <updated>2021-07-21T02:01:37.091Z</updated>
        <summary type="html"><![CDATA[We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1"&gt;Ali Unlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stein Latent Optimization for GANs. (arXiv:2106.05319v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05319</id>
        <link href="http://arxiv.org/abs/2106.05319"/>
        <updated>2021-07-21T02:01:37.085Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) with clustered latent spaces can
perform conditional generation in a completely unsupervised manner. However,
the salient attributes of unlabeled data in the real-world are mostly
imbalanced. Existing unsupervised conditional GANs cannot properly cluster the
attributes in their latent spaces because they assume uniform distributions of
the attributes. To address this problem, we theoretically derive Stein latent
optimization that provides reparameterizable gradient estimations of the latent
distribution parameters assuming a Gaussian mixture prior in a continuous
latent space. Structurally, we introduce an encoder network and a novel
contrastive loss to help generated data from a single mixture component to
represent a single attribute. We confirm that the proposed method, named Stein
Latent Optimization for GANs (SLOGAN), successfully learns the balanced or
imbalanced attributes and performs unsupervised tasks such as unsupervised
conditional generation, unconditional generation, and cluster assignment even
in the absence of information of the attributes (e.g. the imbalance ratio).
Moreover, we demonstrate that the attributes to be learned can be manipulated
using a small amount of probe data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1"&gt;Uiwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heeseung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1"&gt;Dahuin Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1"&gt;Hyemi Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyungyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09562</id>
        <link href="http://arxiv.org/abs/2107.09562"/>
        <updated>2021-07-21T02:01:37.052Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) aims to find representations suitable for
zero-shot transfer to a priori unknown test distributions. However, common
evaluation protocols only test a single, fixed data split in which train and
test classes are assigned randomly. More realistic evaluations should consider
a broad spectrum of distribution shifts with potentially varying degree and
difficulty. In this work, we systematically construct train-test splits of
increasing difficulty and present the ooDML benchmark to characterize
generalization under out-of-distribution shifts in DML. ooDML is designed to
probe the generalization performance on much more challenging, diverse
train-to-test distribution shifts. Based on our new benchmark, we conduct a
thorough empirical analysis of state-of-the-art DML methods. We find that while
generalization tends to consistently degrade with difficulty, some methods are
better at retaining performance as the distribution shift increases. Finally,
we propose few-shot DML as an efficient way to consistently improve
generalization in response to unknown test shifts presented in ooDML. Code
available here:
https://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Samarth Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-21T02:01:37.040Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, i.e., it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying identifiability to choose and audit $\epsilon$ in differentially private deep learning. (arXiv:2103.02913v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02913</id>
        <link href="http://arxiv.org/abs/2103.02913"/>
        <updated>2021-07-21T02:01:37.032Z</updated>
        <summary type="html"><![CDATA[Differential privacy allows bounding the influence that training data records
have on a machine learning model. To use differential privacy in machine
learning, data scientists must choose privacy parameters $(\epsilon,\delta)$.
Choosing meaningful privacy parameters is key, since models trained with weak
privacy parameters might result in excessive privacy leakage, while strong
privacy parameters might overly degrade model utility. However, privacy
parameter values are difficult to choose for two main reasons. First, the
theoretical upper bound on privacy loss $(\epsilon,\delta)$ might be loose,
depending on the chosen sensitivity and data distribution of practical
datasets. Second, legal requirements and societal norms for anonymization often
refer to individual identifiability, to which $(\epsilon,\delta)$ are only
indirectly related.

We transform $(\epsilon,\delta)$ to a bound on the Bayesian posterior belief
of the adversary assumed by differential privacy concerning the presence of any
record in the training dataset. The bound holds for multidimensional queries
under composition, and we show that it can be tight in practice. Furthermore,
we derive an identifiability bound, which relates the adversary assumed in
differential privacy to previous work on membership inference adversaries. We
formulate an implementation of this differential privacy adversary that allows
data scientists to audit model training and compute empirical identifiability
scores and empirical $(\epsilon,\delta)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bernau_D/0/1/0/all/0/1"&gt;Daniel Bernau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eibl_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Eibl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grassal_P/0/1/0/all/0/1"&gt;Philip W. Grassal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_H/0/1/0/all/0/1"&gt;Hannah Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1"&gt;Florian Kerschbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Theory of Label Propagation for Subpopulation Shift. (arXiv:2102.11203v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11203</id>
        <link href="http://arxiv.org/abs/2102.11203"/>
        <updated>2021-07-21T02:01:37.021Z</updated>
        <summary type="html"><![CDATA[One of the central problems in machine learning is domain adaptation. Unlike
past theoretical work, we consider a new model for subpopulation shift in the
input or representation space. In this work, we propose a provably effective
framework for domain adaptation based on label propagation. In our analysis, we
use a simple but realistic expansion assumption, proposed in
\citet{wei2021theoretical}. Using a teacher classifier trained on the source
domain, our algorithm not only propagates to the target domain but also
improves upon the teacher. By leveraging existing generalization bounds, we
also obtain end-to-end finite-sample guarantees on the entire algorithm. In
addition, we extend our theoretical framework to a more general setting of
source-to-target transfer based on a third unlabeled dataset, which can be
easily applied in various learning scenarios. Inspired by our theory, we adapt
consistency-based semi-supervised learning methods to domain adaptation
settings and gain significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruiqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Agnostic Learning to Meta-Learn. (arXiv:2012.02684v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02684</id>
        <link href="http://arxiv.org/abs/2012.02684"/>
        <updated>2021-07-21T02:01:37.004Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a learning algorithm that enables a model to
quickly exploit commonalities among related tasks from an unseen task
distribution, before quickly adapting to specific tasks from that same
distribution. We investigate how learning with different task distributions can
first improve adaptability by meta-finetuning on related tasks before improving
goal task generalization with finetuning. Synthetic regression experiments
validate the intuition that learning to meta-learn improves adaptability and
consecutively generalization. Experiments on more complex image classification,
continual regression, and reinforcement learning tasks demonstrate that
learning to meta-learn generally improves task-specific adaptation. The
methodology, setup, and hypotheses in this proposal were positively evaluated
by peer review before conclusive experiments were carried out.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devos_A/0/1/0/all/0/1"&gt;Arnout Devos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1"&gt;Yatin Dandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations. (arXiv:2104.10586v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10586</id>
        <link href="http://arxiv.org/abs/2104.10586"/>
        <updated>2021-07-21T02:01:36.979Z</updated>
        <summary type="html"><![CDATA[To tackle the susceptibility of deep neural networks to examples, the
adversarial training has been proposed which provides a notion of robust
through an inner maximization problem presenting the first-order embedded
within the outer minimization of the training loss. To generalize the
adversarial robustness over different perturbation types, the adversarial
training method has been augmented with the improved inner maximization
presenting a union of multiple perturbations e.g., various $\ell_p$
norm-bounded perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1"&gt;Ryan Goldhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13677</id>
        <link href="http://arxiv.org/abs/2010.13677"/>
        <updated>2021-07-21T02:01:36.971Z</updated>
        <summary type="html"><![CDATA[In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)
decomposition, or robust principal component analysis (PCA), has achieved
stunning performance. However, the selection of the parameters of L+S is
empirical, and the acceleration rate is limited, which are common failings of
iterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many
deep learning approaches have been proposed to address these issues, but few of
them use a low-rank prior. In this paper, a model-based low-rank plus sparse
network, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In
particular, we use an alternating linearized minimization method to solve the
optimization problem with low-rank and sparse regularization. Learned soft
singular value thresholding is introduced to ensure the clear separation of the
L component and S component. Then, the iterative steps are unrolled into a
network in which the regularization parameters are learnable. We prove that the
proposed L+S-Net achieves global convergence under two standard assumptions.
Experiments on retrospective and prospective cardiac cine datasets show that
the proposed model outperforms state-of-the-art CS and existing deep learning
methods and has great potential for extremely high acceleration factors (up to
24x).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1"&gt;Ziwen Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhuo-Xu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Zhilang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1"&gt;Leslie Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dong Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05108</id>
        <link href="http://arxiv.org/abs/2103.05108"/>
        <updated>2021-07-21T02:01:36.929Z</updated>
        <summary type="html"><![CDATA[Understanding the predictions made by Artificial Intelligence (AI) systems is
becoming more and more important as deep learning models are used for
increasingly complex and high-stakes tasks. Saliency mapping - an easily
interpretable visual attribution method - is one important tool for this, but
existing formulations are limited by either computational cost or architectural
constraints. We therefore propose Hierarchical Perturbation, a very fast and
completely model-agnostic method for explaining model predictions with robust
saliency maps. Using standard benchmarks and datasets, we show that our
saliency maps are of competitive or superior quality to those generated by
existing model-agnostic methods - and are over 20X faster to compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning. (arXiv:2107.09645v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.09645</id>
        <link href="http://arxiv.org/abs/2107.09645"/>
        <updated>2021-07-21T02:01:36.923Z</updated>
        <summary type="html"><![CDATA[We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for
visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic
approach that uses data augmentation to learn directly from pixels. We
introduce several improvements that yield state-of-the-art results on the
DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid
locomotion tasks directly from pixel observations, previously unattained by
model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides
significantly better computational footprint compared to prior work, with the
majority of tasks taking just 8 hours to train on a single GPU. Finally, we
publicly release DrQ-v2's implementation to provide RL practitioners with a
strong and computationally efficient baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1"&gt;Denis Yarats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1"&gt;Lerrel Pinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Evolvability ES: Evolving Individuals With a Distribution of Well Performing and Diverse Offspring. (arXiv:2103.10790v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10790</id>
        <link href="http://arxiv.org/abs/2103.10790"/>
        <updated>2021-07-21T02:01:36.890Z</updated>
        <summary type="html"><![CDATA[One of the most important lessons from the success of deep learning is that
learned representations tend to perform much better at any task compared to
representations we design by hand. Yet evolution of evolvability algorithms,
which aim to automatically learn good genetic representations, have received
relatively little attention, perhaps because of the large amount of
computational power they require. The recent method Evolvability ES allows
direct selection for evolvability with little computation. However, it can only
be used to solve problems where evolvability and task performance are aligned.
We propose Quality Evolvability ES, a method that simultaneously optimizes for
task performance and evolvability and without this restriction. Our proposed
approach Quality Evolvability has similar motivation to Quality Diversity
algorithms, but with some important differences. While Quality Diversity aims
to find an archive of diverse and well-performing, but potentially genetically
distant individuals, Quality Evolvability aims to find a single individual with
a diverse and well-performing distribution of offspring. By doing so Quality
Evolvability is forced to discover more evolvable representations. We
demonstrate on robotic locomotion control tasks that Quality Evolvability ES,
similarly to Quality Diversity methods, can learn faster than objective-based
methods and can handle deceptive problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1"&gt;Adam Katona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franks_D/0/1/0/all/0/1"&gt;Daniel W. Franks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1"&gt;James Alfred Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08898</id>
        <link href="http://arxiv.org/abs/2102.08898"/>
        <updated>2021-07-21T02:01:36.863Z</updated>
        <summary type="html"><![CDATA[We develop a novel approach to conformal prediction when the target task has
limited data available for training. Conformal prediction identifies a small
set of promising output candidates in place of a single prediction, with
guarantees that the set contains the correct answer with high probability. When
training data is limited, however, the predicted set can easily become unusably
large. In this work, we obtain substantially tighter prediction sets while
maintaining desirable marginal guarantees by casting conformal prediction as a
meta-learning paradigm over exchangeable collections of auxiliary tasks. Our
conformalization algorithm is simple, fast, and agnostic to the choice of
underlying model, learning algorithm, or dataset. We demonstrate the
effectiveness of this approach across a number of few-shot classification and
regression tasks in natural language processing, computer vision, and
computational chemistry for drug discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1"&gt;Adam Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1"&gt;Tal Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Billion Audio Sounds from GPU-enabled Modular Synthesis. (arXiv:2104.12922v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12922</id>
        <link href="http://arxiv.org/abs/2104.12922"/>
        <updated>2021-07-21T02:01:36.832Z</updated>
        <summary type="html"><![CDATA[We release synth1B1, a multi-modal audio corpus consisting of 1 billion
4-second synthesized sounds, paired with the synthesis parameters used to
generate them. The dataset is 100x larger than any audio dataset in the
literature. We also introduce torchsynth, an open source modular synthesizer
that generates the synth1B1 samples on-the-fly at 16200x faster than real-time
(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth
timbre and subtractive synth pitch. Using these datasets, we demonstrate new
rank-based evaluation criteria for existing audio representations. Finally, we
propose a novel approach to synthesizer hyperparameter optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turian_J/0/1/0/all/0/1"&gt;Joseph Turian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shier_J/0/1/0/all/0/1"&gt;Jordie Shier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzanetakis_G/0/1/0/all/0/1"&gt;George Tzanetakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McNally_K/0/1/0/all/0/1"&gt;Kirk McNally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_M/0/1/0/all/0/1"&gt;Max Henry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Sampling Without Replacement for Sequence Models. (arXiv:2002.09067v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09067</id>
        <link href="http://arxiv.org/abs/2002.09067"/>
        <updated>2021-07-21T02:01:36.825Z</updated>
        <summary type="html"><![CDATA[Sampling is a fundamental technique, and sampling without replacement is
often desirable when duplicate samples are not beneficial. Within machine
learning, sampling is useful for generating diverse outputs from a trained
model. We present an elegant procedure for sampling without replacement from a
broad class of randomized programs, including generative neural models that
construct outputs sequentially. Our procedure is efficient even for
exponentially-large output spaces. Unlike prior work, our approach is
incremental, i.e., samples can be drawn one at a time, allowing for increased
flexibility. We also present a new estimator for computing expectations from
samples drawn without replacement. We show that incremental sampling without
replacement is applicable to many domains, e.g., program synthesis and
combinatorial optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kensen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1"&gt;David Bieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positively Weighted Kernel Quadrature via Subsampling. (arXiv:2107.09597v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.09597</id>
        <link href="http://arxiv.org/abs/2107.09597"/>
        <updated>2021-07-21T02:01:36.719Z</updated>
        <summary type="html"><![CDATA[We study kernel quadrature rules with positive weights for probability
measures on general domains. Our theoretical analysis combines the spectral
properties of the kernel with random sampling of points. This results in
effective algorithms to construct kernel quadrature rules with positive weights
and small worst-case error. Besides additional robustness, our numerical
experiments indicate that this can achieve fast convergence rates that compete
with the optimal bounds in well-known examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hayakawa_S/0/1/0/all/0/1"&gt;Satoshi Hayakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Oberhauser_H/0/1/0/all/0/1"&gt;Harald Oberhauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lyons_T/0/1/0/all/0/1"&gt;Terry Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous network-based drug repurposing for COVID-19. (arXiv:2107.09217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09217</id>
        <link href="http://arxiv.org/abs/2107.09217"/>
        <updated>2021-07-21T02:01:36.701Z</updated>
        <summary type="html"><![CDATA[The Corona Virus Disease 2019 (COVID-19) belongs to human coronaviruses
(HCoVs), which spreads rapidly around the world. Compared with new drug
development, drug repurposing may be the best shortcut for treating COVID-19.
Therefore, we constructed a comprehensive heterogeneous network based on the
HCoVs-related target proteins and use the previously proposed deepDTnet, to
discover potential drug candidates for COVID-19. We obtain high performance in
predicting the possible drugs effective for COVID-19 related proteins. In
summary, this work utilizes a powerful heterogeneous network-based deep
learning method, which may be beneficial to quickly identify candidate
repurposable drugs toward future clinical trials for COVID-19. The code and
data are available at https://github.com/stjin-XMU/HnDR-COVID.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shuting Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiangxiang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Changzhi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangrong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaoliang Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Riemannian Manifold Optimization for Discriminant Subspace Learning. (arXiv:2101.08032v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08032</id>
        <link href="http://arxiv.org/abs/2101.08032"/>
        <updated>2021-07-21T02:01:36.638Z</updated>
        <summary type="html"><![CDATA[Linear discriminant analysis (LDA) is a widely used algorithm in machine
learning to extract a low-dimensional representation of high-dimensional data,
it features to find the orthogonal discriminant projection subspace by using
the Fisher discriminant criterion. However, the traditional Euclidean-based
methods for solving LDA are easily convergent to spurious local minima and
hardly obtain an optimal solution. To address such a problem, in this paper, we
propose a novel algorithm namely Riemannian-based discriminant analysis (RDA)
for subspace learning. In order to obtain an explicit solution, we transform
the traditional Euclidean-based methods to the Riemannian manifold space and
use the trust-region method to learn the discriminant projection subspace. We
compare the proposed algorithm to existing variants of LDA, as well as the
unsupervised tensor decomposition methods on image classification tasks. The
numerical results suggest that RDA achieves state-of-the-art performance in
classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wanguang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Sampling for Minimax Fair Classification. (arXiv:2103.00755v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00755</id>
        <link href="http://arxiv.org/abs/2103.00755"/>
        <updated>2021-07-21T02:01:36.628Z</updated>
        <summary type="html"><![CDATA[Machine learning models trained on uncurated datasets can often end up
adversely affecting inputs belonging to underrepresented groups. To address
this issue, we consider the problem of adaptively constructing training sets
which allow us to learn classifiers that are fair in a minimax sense. We first
propose an adaptive sampling algorithm based on the principle of optimism, and
derive theoretical bounds on its performance. We also propose heuristic
extensions of this algorithm suitable for application to large scale, practical
problems. Next, by deriving algorithm independent lower-bounds for a specific
class of problems, we show that the performance achieved by our adaptive scheme
cannot be improved in general. We then validate the benefits of adaptively
constructing training sets via experiments on synthetic tasks with logistic
regression classifiers, as well as on several real-world tasks using
convolutional neural networks (CNNs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1"&gt;Shubhanshu Shekhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fields_G/0/1/0/all/0/1"&gt;Greg Fields&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1"&gt;Tara Javidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions. (arXiv:2107.09546v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09546</id>
        <link href="http://arxiv.org/abs/2107.09546"/>
        <updated>2021-07-21T02:01:36.619Z</updated>
        <summary type="html"><![CDATA[Machine learning is expected to fuel significant improvements in medical
care. To ensure that fundamental principles such as beneficence, respect for
human autonomy, prevention of harm, justice, privacy, and transparency are
respected, medical machine learning applications must be developed responsibly.
In this paper, we survey the technical challenges involved in creating medical
machine learning systems responsibly and in conformity with existing
regulations, as well as possible solutions to address these challenges. We
begin by providing a brief overview of existing regulations affecting medical
machine learning, showing that properties such as safety, robustness,
reliability, privacy, security, transparency, explainability, and
nondiscrimination are all demanded already by existing law and regulations -
albeit, in many cases, to an uncertain degree. Next, we discuss the underlying
technical challenges, possible ways for addressing them, and their respective
merits and drawbacks. We notice that distribution shift, spurious correlations,
model underspecification, and data scarcity represent severe challenges in the
medical context (and others) that are very difficult to solve with classical
black-box deep neural networks. Important measures that may help to address
these challenges include the use of large and representative datasets and
federated learning as a means to that end, the careful exploitation of domain
knowledge wherever feasible, the use of inherently transparent models,
comprehensive model testing and verification, as well as stakeholder inclusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_E/0/1/0/all/0/1"&gt;Eike Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potdevin_Y/0/1/0/all/0/1"&gt;Yannik Potdevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1"&gt;Esfandiar Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zidowitz_S/0/1/0/all/0/1"&gt;Stephan Zidowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breyer_S/0/1/0/all/0/1"&gt;Sabrina Breyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowotka_D/0/1/0/all/0/1"&gt;Dirk Nowotka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henn_S/0/1/0/all/0/1"&gt;Sandra Henn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechmann_L/0/1/0/all/0/1"&gt;Ludwig Pechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leucker_M/0/1/0/all/0/1"&gt;Martin Leucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostalski_P/0/1/0/all/0/1"&gt;Philipp Rostalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzog_C/0/1/0/all/0/1"&gt;Christian Herzog&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DanHAR: Dual Attention Network For Multimodal Human Activity Recognition Using Wearable Sensors. (arXiv:2006.14435v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14435</id>
        <link href="http://arxiv.org/abs/2006.14435"/>
        <updated>2021-07-21T02:01:36.612Z</updated>
        <summary type="html"><![CDATA[Human activity recognition (HAR) in ubiquitous computing has been beginning
to incorporate attention into the context of deep neural networks (DNNs), in
which the rich sensing data from multimodal sensors such as accelerometer and
gyroscope is used to infer human activities. Recently, two attention methods
are proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term
Memory (LSTM) network, which can capture the dependencies of sensing signals in
both spatial and temporal domains simultaneously. However, recurrent networks
often have a weak feature representing power compared with convolutional neural
networks (CNNs). On the other hand, two attention, i.e., hard attention and
soft attention, are applied in temporal domains via combining with CNN, which
pay more attention to the target activity from a long sequence. However, they
can only tell where to focus and miss channel information, which plays an
important role in deciding what to focus. As a result, they fail to address the
spatial-temporal dependencies of multimodal sensing signals, compared with
attention-based GRU or LSTM. In the paper, we propose a novel dual attention
method called DanHAR, which introduces the framework of blending channel
attention and temporal attention on a CNN, demonstrating superiority in
improving the comprehensibility for multimodal HAR. Extensive experiments on
four public HAR datasets and weakly labeled dataset show that DanHAR achieves
state-of-the-art performance with negligible overhead of parameters.
Furthermore, visualizing analysis is provided to show that our attention can
amplifies more important sensor modalities and timesteps during classification,
which agrees well with human common intuition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wenbin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1"&gt;Qi Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles. (arXiv:2010.13972v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13972</id>
        <link href="http://arxiv.org/abs/2010.13972"/>
        <updated>2021-07-21T02:01:36.593Z</updated>
        <summary type="html"><![CDATA[SHAP (SHapley Additive exPlanation) values provide a game theoretic
interpretation of the predictions of machine learning models based on Shapley
values. While exact calculation of SHAP values is computationally intractable
in general, a recursive polynomial-time algorithm called TreeShap is available
for decision tree models. However, despite its polynomial time complexity,
TreeShap can become a significant bottleneck in practical machine learning
pipelines when applied to large decision tree ensembles. We present
GPUTreeShap, a modified TreeShap algorithm suitable for massively parallel
computation on graphics processing units. Our approach first preprocesses each
decision tree to isolate variable sized sub-problems from the original
recursive algorithm, then solves a bin packing problem, and finally maps
sub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel
execution with specialised hardware instructions. With a single NVIDIA Tesla
V100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of
up to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU
implementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also
experiment with multi-GPU computing using eight V100 GPUs, demonstrating
throughput of 1.2M rows per second -- equivalent CPU-based performance is
estimated to require 6850 CPU cores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1"&gt;Rory Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1"&gt;Eibe Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmes_G/0/1/0/all/0/1"&gt;Geoffrey Holmes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Privacy-preserving Explanations in Medical Image Analysis. (arXiv:2107.09652v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09652</id>
        <link href="http://arxiv.org/abs/2107.09652"/>
        <updated>2021-07-21T02:01:36.586Z</updated>
        <summary type="html"><![CDATA[The use of Deep Learning in the medical field is hindered by the lack of
interpretability. Case-based interpretability strategies can provide intuitive
explanations for deep learning models' decisions, thus, enhancing trust.
However, the resulting explanations threaten patient privacy, motivating the
development of privacy-preserving methods compatible with the specifics of
medical data. In this work, we analyze existing privacy-preserving methods and
their respective capacity to anonymize medical data while preserving
disease-related semantic features. We find that the PPRL-VGAN deep learning
method was the best at preserving the disease-related semantic features while
guaranteeing a high level of privacy among the compared state-of-the-art
methods. Nevertheless, we emphasize the need to improve privacy-preserving
methods for medical imaging, as we identified relevant drawbacks in all
existing privacy-preserving approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montenegro_H/0/1/0/all/0/1"&gt;H. Montenegro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_W/0/1/0/all/0/1"&gt;W. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1"&gt;J. S. Cardoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05675</id>
        <link href="http://arxiv.org/abs/2007.05675"/>
        <updated>2021-07-21T02:01:36.578Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims at rapidly adapting to novel categories with only a
handful of samples at test time, which has been predominantly tackled with the
idea of meta-learning. However, meta-learning approaches essentially learn
across a variety of few-shot tasks and thus still require large-scale training
data with fine-grained supervision to derive a generalized model, thereby
involving prohibitive annotation cost. In this paper, we advance the few-shot
classification paradigm towards a more challenging scenario, i.e.,
cross-granularity few-shot classification, where the model observes only coarse
labels during training while is expected to perform fine-grained classification
during testing. This task largely relieves the annotation cost since
fine-grained labeling usually requires strong domain-specific expertise. To
bridge the cross-granularity gap, we approximate the fine-grained data
distribution by greedy clustering of each coarse-class into pseudo-fine-classes
according to the similarity of image embeddings. We then propose a
meta-embedder that jointly optimizes the visual- and semantic-discrimination,
in both instance-wise and coarse class-wise, to obtain a good feature space for
this coarse-to-fine pseudo-labeling process. Extensive experiments and ablation
studies are conducted to demonstrate the effectiveness and robustness of our
approach on three representative datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Assisted Calibrated Beam Training for Millimeter-Wave Communication Systems. (arXiv:2101.05206v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05206</id>
        <link href="http://arxiv.org/abs/2101.05206"/>
        <updated>2021-07-21T02:01:36.571Z</updated>
        <summary type="html"><![CDATA[Huge overhead of beam training imposes a significant challenge in
millimeter-wave (mmWave) wireless communications. To address this issue, in
this paper, we propose a wide beam based training approach to calibrate the
narrow beam direction according to the channel power leakage. To handle the
complex nonlinear properties of the channel power leakage, deep learning is
utilized to predict the optimal narrow beam directly. Specifically, three deep
learning assisted calibrated beam training schemes are proposed. The first
scheme adopts convolution neural network to implement the prediction based on
the instantaneous received signals of wide beam training. We also perform the
additional narrow beam training based on the predicted probabilities for
further beam direction calibrations. However, the first scheme only depends on
one wide beam training, which lacks the robustness to noise. To tackle this
problem, the second scheme adopts long-short term memory (LSTM) network for
tracking the movement of users and calibrating the beam direction according to
the received signals of prior beam training, in order to enhance the robustness
to noise. To further reduce the overhead of wide beam training, our third
scheme, an adaptive beam training strategy, selects partial wide beams to be
trained based on the prior received signals. Two criteria, namely, optimal
neighboring criterion and maximum probability criterion, are designed for the
selection. Furthermore, to handle mobile scenarios, auxiliary LSTM is
introduced to calibrate the directions of the selected wide beams more
precisely. Simulation results demonstrate that our proposed schemes achieve
significantly higher beamforming gain with smaller beam training overhead
compared with the conventional and existing deep-learning based counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Ke Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1"&gt;Dongxuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hancun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sheng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy Disaggregation using Variational Autoencoders. (arXiv:2103.12177v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12177</id>
        <link href="http://arxiv.org/abs/2103.12177"/>
        <updated>2021-07-21T02:01:36.564Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) is a technique that uses a single sensor
to measure the total power consumption of a building. Using an energy
disaggregation method, the consumption of individual appliances can be
estimated from the aggregate measurement. Recent disaggregation algorithms have
significantly improved the performance of NILM systems. However, the
generalization capability of these methods to different houses as well as the
disaggregation of multi-state appliances are still major challenges. In this
paper we address these issues and propose an energy disaggregation approach
based on the variational autoencoders framework. The probabilistic encoder
makes this approach an efficient model for encoding information relevant to the
reconstruction of the target appliance consumption. In particular, the proposed
model accurately generates more complex load profiles, thus improving the power
signal reconstruction of multi-state appliances. Moreover, its regularized
latent space improves the generalization capabilities of the model across
different houses. The proposed model is compared to state-of-the-art NILM
approaches on the UK-DALE and REFIT datasets, and yields competitive results.
The mean absolute error reduces by 18% on average across all appliances
compared to the state-of-the-art. The F1-Score increases by more than 11%,
showing improvements for the detection of the target appliance in the aggregate
measurement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langevin_A/0/1/0/all/0/1"&gt;Antoine Langevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1"&gt;Marc-Andr&amp;#xe9; Carbonneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheriet_M/0/1/0/all/0/1"&gt;Mohamed Cheriet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagnon_G/0/1/0/all/0/1"&gt;Ghyslain Gagnon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08001</id>
        <link href="http://arxiv.org/abs/2103.08001"/>
        <updated>2021-07-21T02:01:36.542Z</updated>
        <summary type="html"><![CDATA[This article describes research on claim verification carried out using a
multiple GAN-based model. The proposed model consists of three pairs of
generators and discriminators. The generator and discriminator pairs are
responsible for generating synthetic data for supported and refuted claims and
claim labels. A theoretical discussion about the proposed model is provided to
validate the equilibrium state of the model. The proposed model is applied to
the FEVER dataset, and a pre-trained language model is used for the input text
data. The synthetically generated data helps to gain information which helps
the model to perform better than state of the art models and other standard
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1"&gt;Amartya Hatua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Arjun Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1"&gt;Rakesh M. Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Altruistic Behaviours in Reinforcement Learning without External Rewards. (arXiv:2107.09598v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.09598</id>
        <link href="http://arxiv.org/abs/2107.09598"/>
        <updated>2021-07-21T02:01:36.535Z</updated>
        <summary type="html"><![CDATA[Can artificial agents learn to assist others in achieving their goals without
knowing what those goals are? Generic reinforcement learning agents could be
trained to behave altruistically towards others by rewarding them for
altruistic behaviour, i.e., rewarding them for benefiting other agents in a
given situation. Such an approach assumes that other agents' goals are known so
that the altruistic agent can cooperate in achieving those goals. However,
explicit knowledge of other agents' goals is often difficult to acquire. Even
assuming such knowledge to be given, training of altruistic agents would
require manually-tuned external rewards for each new environment. Thus, it is
beneficial to develop agents that do not depend on external supervision and can
learn altruistic behaviour in a task-agnostic manner. Assuming that other
agents rationally pursue their goals, we hypothesize that giving them more
choices will allow them to pursue those goals better. Some concrete examples
include opening a door for others or safeguarding them to pursue their
objectives without interference. We formalize this concept and propose an
altruistic agent that learns to increase the choices another agent has by
maximizing the number of states that the other agent can reach in its future.
We evaluate our approach on three different multi-agent environments where
another agent's success depends on the altruistic agent's behaviour. Finally,
we show that our unsupervised agents can perform comparably to agents
explicitly trained to work cooperatively. In some cases, our agents can even
outperform the supervised ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franzmeyer_T/0/1/0/all/0/1"&gt;Tim Franzmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08909</id>
        <link href="http://arxiv.org/abs/2106.08909"/>
        <updated>2021-07-21T02:01:36.527Z</updated>
        <summary type="html"><![CDATA[Most prior approaches to offline reinforcement learning (RL) have taken an
iterative actor-critic approach involving off-policy evaluation. In this paper
we show that simply doing one step of constrained/regularized policy
improvement using an on-policy Q estimate of the behavior policy performs
surprisingly well. This one-step algorithm beats the previously reported
results of iterative algorithms on a large portion of the D4RL benchmark. The
simple one-step baseline achieves this strong performance without many of the
tricks used by previously proposed iterative algorithms and is more robust to
hyperparameters. We argue that the relatively poor performance of iterative
approaches is a result of the high variance inherent in doing off-policy
evaluation and magnified by the repeated optimization of policies against those
high-variance estimates. In addition, we hypothesize that the strong
performance of the one-step algorithm is due to a combination of favorable
structure in the environment and behavior policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1"&gt;David Brandfonbrener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-07-21T02:01:36.520Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Policy Optimization for Tracking Control Exploiting Future Reference Information. (arXiv:2107.09647v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09647</id>
        <link href="http://arxiv.org/abs/2107.09647"/>
        <updated>2021-07-21T02:01:36.513Z</updated>
        <summary type="html"><![CDATA[In recent years, reinforcement learning (RL) has gained increasing attention
in control engineering. Especially, policy gradient methods are widely used. In
this work, we improve the tracking performance of proximal policy optimization
(PPO) for arbitrary reference signals by incorporating information about future
reference values. Two variants of extending the argument of the actor and the
critic taking future reference values into account are presented. In the first
variant, global future reference values are added to the argument. For the
second variant, a novel kind of residual space with future reference values
applicable to model-free reinforcement learning is introduced. Our approach is
evaluated against a PI controller on a simple drive train model. We expect our
method to generalize to arbitrary references better than previous approaches,
pointing towards the applicability of RL to control real systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_J/0/1/0/all/0/1"&gt;Jana Mayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Westermann_J/0/1/0/all/0/1"&gt;Johannes Westermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muriedas_J/0/1/0/all/0/1"&gt;Juan Pedro Guti&amp;#xe9;rrez H. Muriedas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mettin_U/0/1/0/all/0/1"&gt;Uwe Mettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampe_A/0/1/0/all/0/1"&gt;Alexander Lampe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification. (arXiv:2102.07856v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07856</id>
        <link href="http://arxiv.org/abs/2102.07856"/>
        <updated>2021-07-21T02:01:36.497Z</updated>
        <summary type="html"><![CDATA[Modern machine learning models with high accuracy are often miscalibrated --
the predicted top probability does not reflect the actual accuracy, and tends
to be over-confident. It is commonly believed that such over-confidence is
mainly due to over-parametrization, in particular when the model is large
enough to memorize the training data and maximize the confidence.

In this paper, we show theoretically that over-parametrization is not the
only reason for over-confidence. We prove that logistic regression is
inherently over-confident, in the realizable, under-parametrized setting where
the data is generated from the logistic model, and the sample size is much
larger than the number of parameters. Further, this over-confidence happens for
general well-specified binary classification problems as long as the activation
is symmetric and concave on the positive part. Perhaps surprisingly, we also
show that over-confidence is not always the case -- there exists another
activation function (and a suitable loss function) under which the learned
classifier is under-confident at some probability values. Overall, our theory
provides a precise characterization of calibration in realizable binary
classification, which we verify on simulations and real data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1"&gt;Song Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture Density Network Estimation of Continuous Variable Maximum Likelihood Using Discrete Training Samples. (arXiv:2103.13416v2 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13416</id>
        <link href="http://arxiv.org/abs/2103.13416"/>
        <updated>2021-07-21T02:01:36.490Z</updated>
        <summary type="html"><![CDATA[Mixture Density Networks (MDNs) can be used to generate probability density
functions of model parameters $\boldsymbol{\theta}$ given a set of observables
$\mathbf{x}$. In some applications, training data are available only for
discrete values of a continuous parameter $\boldsymbol{\theta}$. In such
situations a number of performance-limiting issues arise which can result in
biased estimates. We demonstrate the usage of MDNs for parameter estimation,
discuss the origins of the biases, and propose a corrective method for each
issue.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Burton_C/0/1/0/all/0/1"&gt;Charles Burton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stubbs_S/0/1/0/all/0/1"&gt;Spencer Stubbs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Onyisi_P/0/1/0/all/0/1"&gt;Peter Onyisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10314</id>
        <link href="http://arxiv.org/abs/2104.10314"/>
        <updated>2021-07-21T02:01:36.482Z</updated>
        <summary type="html"><![CDATA[Sparse coding is a class of unsupervised methods for learning a sparse
representation of the input data in the form of a linear combination of a
dictionary and a sparse code. This learning framework has led to
state-of-the-art results in various image and video processing tasks. However,
classical methods learn the dictionary and the sparse code based on alternating
optimizations, usually without theoretical guarantees for either optimality or
convergence due to non-convexity of the problem. Recent works on sparse coding
with a complete dictionary provide strong theoretical guarantees thanks to the
development of the non-convex optimization. However, initial non-convex
approaches learn the dictionary in the sparse coding problem sequentially in an
atom-by-atom manner, which leads to a long execution time. More recent works
seek to directly learn the entire dictionary at once, which substantially
reduces the execution time. However, the associated recovery performance is
degraded with a finite number of data samples. In this paper, we propose an
efficient sparse coding scheme with a two-stage optimization. The proposed
scheme leverages the global and local Riemannian geometry of the two-stage
optimization problem and facilitates fast implementation for superb dictionary
recovery performance by a finite number of samples without atom-by-atom
calculation. We further prove that, with high probability, the proposed scheme
can exactly recover any atom in the target dictionary with a finite number of
samples if it is adopted to recover one atom of the dictionary. An application
on wireless sensor data compression is also proposed. Experiments on both
synthetic and real-world data verify the efficiency and effectiveness of the
proposed scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Ye Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1"&gt;Vincent Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Songfu Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-07-21T02:01:36.474Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Tradeoff in Integrated Sensing and Communication: Recognition Accuracy versus Communication Rate. (arXiv:2107.09621v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09621</id>
        <link href="http://arxiv.org/abs/2107.09621"/>
        <updated>2021-07-21T02:01:36.468Z</updated>
        <summary type="html"><![CDATA[Integrated sensing and communication (ISAC) is a promising technology to
improve the band-utilization efficiency via spectrum sharing or hardware
sharing between radar and communication systems. Since a common radio resource
budget is shared by both functionalities, there exists a tradeoff between the
sensing and communication performance. However, this tradeoff curve is
currently unknown in ISAC systems with human motion recognition tasks based on
deep learning. To fill this gap, this paper formulates and solves a
multi-objective optimization problem which simultaneously maximizes the
recognition accuracy and the communication data rate. The key ingredient of
this new formulation is a nonlinear recognition accuracy model with respect to
the wireless resources, where the model is derived from power function
regression of the system performance of the deep spectrogram network. To avoid
cost-expensive data collection procedures, a primitive-based autoregressive
hybrid (PBAH) channel model is developed, which facilitates efficient training
and testing dataset generation for human motion recognition in a virtual
environment. Extensive results demonstrate that the proposed wireless
recognition accuracy and PBAH channel models match the actual experimental data
very well. Moreover, it is found that the accuracy-rate region consists of a
communication saturation zone, a sensing saturation zone, and a
communication-sensing adversarial zone, of which the third zone achieves the
desirable balanced performance for ISAC systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaohui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tony Xiao Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03974</id>
        <link href="http://arxiv.org/abs/2107.03974"/>
        <updated>2021-07-21T02:01:36.449Z</updated>
        <summary type="html"><![CDATA[Meta-reinforcement learning (RL) can meta-train policies that adapt to new
tasks with orders of magnitude less data than standard RL, but meta-training
itself is costly and time-consuming. If we can meta-train on offline data, then
we can reuse the same static dataset, labeled once with rewards for different
tasks, to meta-train policies that adapt to a variety of new tasks at meta-test
time. Although this capability would make meta-RL a practical tool for
real-world use, offline meta-RL presents additional challenges beyond online
meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy
that collects data for adapting, and also meta-trains a policy that quickly
adapts to data from a new task. Since this policy was meta-trained on a fixed,
offline dataset, it might behave unpredictably when adapting to data collected
by the learned exploration strategy, which differs systematically from the
offline data and thus induces distributional shift. We do not want to remove
this distributional shift by simply adopting a conservative exploration
strategy, because learning an exploration strategy enables an agent to collect
better data for faster adaptation. Instead, we propose a hybrid offline meta-RL
algorithm, which uses offline data with rewards to meta-train an adaptive
policy, and then collects additional unsupervised online data, without any
reward labels to bridge this distribution shift. By not requiring reward labels
for online collection, this data can be much cheaper to collect. We compare our
method to prior work on offline meta-RL on simulated robot locomotion and
manipulation tasks and find that using additional unsupervised online data
collection leads to a dramatic improvement in the adaptive capabilities of the
meta-trained policies, matching the performance of fully online meta-RL on a
range of challenging domains that require generalization to new tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1"&gt;Vitchyr H. Pong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Laura Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Catherine Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Representations and Neural Network Estimation of R\'enyi Divergences. (arXiv:2007.03814v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03814</id>
        <link href="http://arxiv.org/abs/2007.03814"/>
        <updated>2021-07-21T02:01:36.442Z</updated>
        <summary type="html"><![CDATA[We derive a new variational formula for the R\'enyi family of divergences,
$R_\alpha(Q\|P)$, between probability measures $Q$ and $P$. Our result
generalizes the classical Donsker-Varadhan variational formula for the
Kullback-Leibler divergence. We further show that this R\'enyi variational
formula holds over a range of function spaces; this leads to a formula for the
optimizer under very weak assumptions and is also key in our development of a
consistency theory for R\'enyi divergence estimators. By applying this theory
to neural-network estimators, we show that if a neural network family satisfies
one of several strengthened versions of the universal approximation property
then the corresponding R\'enyi divergence estimator is consistent. In contrast
to density-estimator based methods, our estimators involve only expectations
under $Q$ and $P$ and hence are more effective in high dimensional systems. We
illustrate this via several numerical examples of neural network estimation in
systems of up to 5000 dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Birrell_J/0/1/0/all/0/1"&gt;Jeremiah Birrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dupuis_P/0/1/0/all/0/1"&gt;Paul Dupuis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1"&gt;Markos A. Katsoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1"&gt;Luc Rey-Bellet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best-of-All-Worlds Bounds for Online Learning with Feedback Graphs. (arXiv:2107.09572v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09572</id>
        <link href="http://arxiv.org/abs/2107.09572"/>
        <updated>2021-07-21T02:01:36.436Z</updated>
        <summary type="html"><![CDATA[We study the online learning with feedback graphs framework introduced by
Mannor and Shamir (2011), in which the feedback received by the online learner
is specified by a graph $G$ over the available actions. We develop an algorithm
that simultaneously achieves regret bounds of the form:
$\smash{\mathcal{O}(\sqrt{\theta(G) T})}$ with adversarial losses;
$\mathcal{O}(\theta(G)\operatorname{polylog}{T})$ with stochastic losses; and
$\mathcal{O}(\theta(G)\operatorname{polylog}{T} + \smash{\sqrt{\theta(G) C})}$
with stochastic losses subject to $C$ adversarial corruptions. Here,
$\theta(G)$ is the clique covering number of the graph $G$. Our algorithm is an
instantiation of Follow-the-Regularized-Leader with a novel regularization that
can be seen as a product of a Tsallis entropy component (inspired by Zimmert
and Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted
stochastic case by Amir et al. (2020)), thus subtly interpolating between the
two forms of entropies. One of our key technical contributions is in
establishing the convexity of this regularizer and controlling its inverse
Hessian, despite its complex product structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erez_L/0/1/0/all/0/1"&gt;Liad Erez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Theory of Convolutional Architectures for Time Series Modelling. (arXiv:2107.09355v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09355</id>
        <link href="http://arxiv.org/abs/2107.09355"/>
        <updated>2021-07-21T02:01:36.429Z</updated>
        <summary type="html"><![CDATA[We study the approximation properties of convolutional architectures applied
to time series modelling, which can be formulated mathematically as a
functional approximation problem. In the recurrent setting, recent results
reveal an intricate connection between approximation efficiency and memory
structures in the data generation process. In this paper, we derive parallel
results for convolutional architectures, with WaveNet being a prime example.
Our results reveal that in this new setting, approximation efficiency is not
only characterised by memory, but also additional fine structures in the target
relationship. This leads to a novel definition of spectrum-based regularity
that measures the complexity of temporal relationships under the convolutional
approximation scheme. These analyses provide a foundation to understand the
differences between architectural choices for time series modelling and can
give theoretically grounded guidance for practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haotian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qianxiao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Problem: Is There an Online Learning Algorithm That Learns Whenever Online Learning Is Possible?. (arXiv:2107.09542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09542</id>
        <link href="http://arxiv.org/abs/2107.09542"/>
        <updated>2021-07-21T02:01:36.422Z</updated>
        <summary type="html"><![CDATA[This open problem asks whether there exists an online learning algorithm for
binary classification that guarantees, for all target concepts, to make a
sublinear number of mistakes, under only the assumption that the (possibly
random) sequence of points X allows that such a learning algorithm can exist
for that sequence. As a secondary problem, it also asks whether a specific
concise condition completely determines whether a given (possibly random)
sequence of points X admits the existence of online learning algorithms
guaranteeing a sublinear number of mistakes for all target concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1"&gt;Steve Hanneke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06212</id>
        <link href="http://arxiv.org/abs/2107.06212"/>
        <updated>2021-07-21T02:01:36.416Z</updated>
        <summary type="html"><![CDATA[Ongoing advancements in the fields of 3D modelling and digital archiving have
led to an outburst in the amount of data stored digitally. Consequently,
several retrieval systems have been developed depending on the type of data
stored in these databases. However, unlike text data or images, performing a
search for 3D models is non-trivial. Among 3D models, retrieving 3D
Engineering/CAD models or mechanical components is even more challenging due to
the presence of holes, volumetric features, presence of sharp edges etc., which
make CAD a domain unto itself. The research work presented in this paper aims
at developing a dataset suitable for building a retrieval system for 3D CAD
models based on deep learning. 3D CAD models from the available CAD databases
are collected, and a dataset of computer-generated sketch data, termed
'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the
components are also added to CADSketchNet. Using the sketch images from this
dataset, the paper also aims at evaluating the performance of various retrieval
system or a search engine for 3D CAD models that accepts a sketch image as the
input query. Many experimental models are constructed and tested on
CADSketchNet. These experiments, along with the model architecture, choice of
similarity metrics are reported along with the search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1"&gt;Shubham Dhayarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1"&gt;Sai Mitheran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1"&gt;V.K. Viekash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting. (arXiv:2105.03620v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03620</id>
        <link href="http://arxiv.org/abs/2105.03620"/>
        <updated>2021-07-21T02:01:36.398Z</updated>
        <summary type="html"><![CDATA[End-to-end text-spotting, which aims to integrate detection and recognition
in a unified framework, has attracted increasing attention due to its
simplicity of the two complimentary tasks. It remains an open problem
especially when processing arbitrarily-shaped text instances. Previous methods
can be roughly categorized into two groups: character-based and
segmentation-based, which often require character-level annotations and/or
complex post-processing due to the unstructured output. Here, we tackle
end-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet
v2). Our main contributions are four-fold: 1) For the first time, we adaptively
fit arbitrarily-shaped text by a parameterized Bezier curve, which, compared
with segmentation-based methods, can not only provide structured output but
also controllable representation. 2) We design a novel BezierAlign layer for
extracting accurate convolution features of a text instance of arbitrary
shapes, significantly improving the precision of recognition over previous
methods. 3) Different from previous methods, which often suffer from complex
post-processing and sensitive hyper-parameters, our ABCNet v2 maintains a
simple pipeline with the only post-processing non-maximum suppression (NMS). 4)
As the performance of text recognition closely depends on feature alignment,
ABCNet v2 further adopts a simple yet effective coordinate convolution to
encode the position of the convolutional filters, which leads to a considerable
improvement with negligible computation overhead. Comprehensive experiments
conducted on various bilingual (English and Chinese) benchmark datasets
demonstrate that ABCNet v2 can achieve state-of-the-art performance while
maintaining very high efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chongyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parametric Scattering Networks. (arXiv:2107.09539v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09539</id>
        <link href="http://arxiv.org/abs/2107.09539"/>
        <updated>2021-07-21T02:01:36.391Z</updated>
        <summary type="html"><![CDATA[The wavelet scattering transform creates geometric invariants and deformation
stability from an initial structured signal. In multiple signal domains it has
been shown to yield more discriminative representations compared to other
non-learned representations, and to outperform learned representations in
certain tasks, particularly on limited labeled data and highly structured
signals. The wavelet filters used in the scattering transform are typically
selected to create a tight frame via a parameterized mother wavelet. Focusing
on Morlet wavelets, we propose to instead adapt the scales, orientations, and
slants of the filters to produce problem-specific parametrizations of the
scattering transform. We show that our learned versions of the scattering
transform yield significant performance gains over the standard scattering
transform in the small sample classification settings, and our empirical
results suggest that tight frames may not always be necessary for scattering
transforms to extract effective representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_S/0/1/0/all/0/1"&gt;Shanel Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1"&gt;Benjamin Th&amp;#xe9;rien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsene_Racicot_L/0/1/0/all/0/1"&gt;Laurent Als&amp;#xe8;ne-Racicot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1"&gt;Michael Eickenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1"&gt;Guy Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Does Cell-Free Massive MIMO Support Multiple Federated Learning Groups?. (arXiv:2107.09577v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09577</id>
        <link href="http://arxiv.org/abs/2107.09577"/>
        <updated>2021-07-21T02:01:36.385Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has been considered as a promising learning framework
for future machine learning systems due to its privacy preservation and
communication efficiency. In beyond-5G/6G systems, it is likely to have
multiple FL groups with different learning purposes. This scenario leads to a
question: How does a wireless network support multiple FL groups? As an answer,
we first propose to use a cell-free massive multiple-input multiple-output
(MIMO) network to guarantee the stable operation of multiple FL processes by
letting the iterations of these FL processes be executed together within a
large-scale coherence time. We then develop a novel scheme that asynchronously
executes the iterations of FL processes under multicasting downlink and
conventional uplink transmission protocols. Finally, we propose a
simple/low-complexity resource allocation algorithm which optimally chooses the
power and computation resources to minimize the execution time of each
iteration of each FL process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Tung T. Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1"&gt;Hien Quoc Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzetta_T/0/1/0/all/0/1"&gt;Thomas L. Marzetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthaiou_M/0/1/0/all/0/1"&gt;Michail Matthaiou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature-Filter: Detecting Adversarial Examples through Filtering off Recessive Features. (arXiv:2107.09502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09502</id>
        <link href="http://arxiv.org/abs/2107.09502"/>
        <updated>2021-07-21T02:01:36.368Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are under threat from adversarial example
attacks. The adversary can easily change the outputs of DNNs by adding small
well-designed perturbations to inputs. Adversarial example detection is a
fundamental work for robust DNNs-based service. Adversarial examples show the
difference between humans and DNNs in image recognition. From a human-centric
perspective, image features could be divided into dominant features that are
comprehensible to humans, and recessive features that are incomprehensible to
humans, yet are exploited by DNNs. In this paper, we reveal that imperceptible
adversarial examples are the product of recessive features misleading neural
networks, and an adversarial attack is essentially a kind of method to enrich
these recessive features in the image. The imperceptibility of the adversarial
examples indicates that the perturbations enrich recessive features, yet hardly
affect dominant features. Therefore, adversarial examples are sensitive to
filtering off recessive features, while benign examples are immune to such
operation. Inspired by this idea, we propose a label-only adversarial detection
approach that is referred to as feature-filter. Feature-filter utilizes
discrete cosine transform to approximately separate recessive features from
dominant features, and gets a mutant image that is filtered off recessive
features. By only comparing DNN's prediction labels on the input and its
mutant, feature-filter can real-time detect imperceptible adversarial examples
at high accuracy and few false positives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yuefeng Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiabao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09957</id>
        <link href="http://arxiv.org/abs/2103.09957"/>
        <updated>2021-07-21T02:01:36.362Z</updated>
        <summary type="html"><![CDATA[A major obstacle to the integration of deep learning models for chest x-ray
interpretation into clinical settings is the lack of understanding of their
failure modes. In this work, we first investigate whether there are patient
subgroups that chest x-ray models are likely to misclassify. We find that
patient age and the radiographic finding of lung lesion, pneumothorax or
support devices are statistically relevant features for predicting
misclassification for some chest x-ray models. Second, we develop
misclassification predictors on chest x-ray models using their outputs and
clinical features. We find that our best performing misclassification
identifier achieves an AUROC close to 0.9 for most diseases. Third, employing
our misclassification identifiers, we develop a corrective algorithm to
selectively flip model predictions that have high likelihood of
misclassification at inference time. We observe F1 improvement on the
prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,
[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and
high-performing chest x-ray models, we are able to derive insights across model
architectures and offer a generalizable framework applicable to other medical
imaging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Emma Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Andy Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1"&gt;Rayan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09362</id>
        <link href="http://arxiv.org/abs/2107.09362"/>
        <updated>2021-07-21T02:01:36.354Z</updated>
        <summary type="html"><![CDATA[Since production-level trained deep neural networks (DNNs) are of a great
business value, protecting such DNN models against copyright infringement and
unauthorized access is in a rising demand. However, conventional model
protection methods focused only the image classification task, and these
protection methods were never applied to semantic segmentation although it has
an increasing number of applications. In this paper, we propose to protect
semantic segmentation models from unauthorized access by utilizing block-wise
transformation with a secret key for the first time. Protected models are
trained by using transformed images. Experiment results show that the proposed
protection method allows rightful users with the correct key to access the
model to full capacity and deteriorate the performance for unauthorized users.
However, protected models slightly drop the segmentation performance compared
to non-protected models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1"&gt;Hiroki Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1"&gt;MaungMaung AprilPyone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNN4IP: Graph Neural Network for Hardware Intellectual Property Piracy Detection. (arXiv:2107.09130v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.09130</id>
        <link href="http://arxiv.org/abs/2107.09130"/>
        <updated>2021-07-21T02:01:36.345Z</updated>
        <summary type="html"><![CDATA[Aggressive time-to-market constraints and enormous hardware design and
fabrication costs have pushed the semiconductor industry toward hardware
Intellectual Properties (IP) core design. However, the globalization of the
integrated circuits (IC) supply chain exposes IP providers to theft and illegal
redistribution of IPs. Watermarking and fingerprinting are proposed to detect
IP piracy. Nevertheless, they come with additional hardware overhead and cannot
guarantee IP security as advanced attacks are reported to remove the watermark,
forge, or bypass it. In this work, we propose a novel methodology, GNN4IP, to
assess similarities between circuits and detect IP piracy. We model the
hardware design as a graph and construct a graph neural network model to learn
its behavior using the comprehensive dataset of register transfer level codes
and gate-level netlists that we have gathered. GNN4IP detects IP piracy with
96% accuracy in our dataset and recognizes the original IP in its obfuscated
version with 100% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasaei_R/0/1/0/all/0/1"&gt;Rozhin Yasaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shih-Yuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1"&gt;Emad Kasaeyan Naeini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild. (arXiv:2107.09510v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09510</id>
        <link href="http://arxiv.org/abs/2107.09510"/>
        <updated>2021-07-21T02:01:36.339Z</updated>
        <summary type="html"><![CDATA[Multimodal wearable physiological data in daily life settings have been used
to estimate self-reported stress labels.However, missing data modalities in
data collection make it challenging to leverage all the collected samples.
Besides, heterogeneous sensor data and labels among individuals add challenges
in building robust stress detection models. In this paper, we proposed a
modality fusion network (MFN) to train models and infer self-reported binary
stress labels under both complete and incomplete modality condition. In
addition, we applied a personalized attention (PA) strategy to leverage
personalized representation along with the generalized one-size-fits-all model.
We evaluated our methods on a multimodal wearable sensor dataset (N=41)
including galvanic skin response (GSR) and electrocardiogram (ECG). Compared to
the baseline method using the samples with complete modalities, the performance
of the MFN improved by 1.6\% in f1-scores. On the other hand, the proposed PA
strategy showed a 2.3\% higher stress detection f1-score and approximately up
to 70\% reduction in personalized model parameter size (9.1 MB) compared to the
previous state-of-the-art transfer learning strategy (29.3 MB).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vaessen_T/0/1/0/all/0/1"&gt;Thomas Vaessen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Myin_Germeys_I/0/1/0/all/0/1"&gt;Inez Myin-Germeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sano_A/0/1/0/all/0/1"&gt;Akane Sano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm Selection on a Meta Level. (arXiv:2107.09414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09414</id>
        <link href="http://arxiv.org/abs/2107.09414"/>
        <updated>2021-07-21T02:01:36.271Z</updated>
        <summary type="html"><![CDATA[The problem of selecting an algorithm that appears most suitable for a
specific instance of an algorithmic problem class, such as the Boolean
satisfiability problem, is called instance-specific algorithm selection. Over
the past decade, the problem has received considerable attention, resulting in
a number of different methods for algorithm selection. Although most of these
methods are based on machine learning, surprisingly little work has been done
on meta learning, that is, on taking advantage of the complementarity of
existing algorithm selection methods in order to combine them into a single
superior algorithm selector. In this paper, we introduce the problem of meta
algorithm selection, which essentially asks for the best way to combine a given
set of algorithm selectors. We present a general methodological framework for
meta algorithm selection as well as several concrete learning methods as
instantiations of this framework, essentially combining ideas of meta learning
and ensemble learning. In an extensive experimental evaluation, we demonstrate
that ensembles of algorithm selectors can significantly outperform single
algorithm selectors and have the potential to form the new state of the art in
algorithm selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1"&gt;Alexander Tornede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehring_L/0/1/0/all/0/1"&gt;Lukas Gehring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1"&gt;Tanja Tornede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1"&gt;Marcel Wever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Real-time Speaker Diarization System Based on Spatial Spectrum. (arXiv:2107.09321v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09321</id>
        <link href="http://arxiv.org/abs/2107.09321"/>
        <updated>2021-07-21T02:01:36.264Z</updated>
        <summary type="html"><![CDATA[In this paper we describe a speaker diarization system that enables
localization and identification of all speakers present in a conversation or
meeting. We propose a novel systematic approach to tackle several long-standing
challenges in speaker diarization tasks: (1) to segment and separate
overlapping speech from two speakers; (2) to estimate the number of speakers
when participants may enter or leave the conversation at any time; (3) to
provide accurate speaker identification on short text-independent utterances;
(4) to track down speakers movement during the conversation; (5) to detect
speaker change incidence real-time. First, a differential directional
microphone array-based approach is exploited to capture the target speakers'
voice in far-field adverse environment. Second, an online speaker-location
joint clustering approach is proposed to keep track of speaker location. Third,
an instant speaker number detector is developed to trigger the mechanism that
separates overlapped speech. The results suggest that our system effectively
incorporates spatial information and achieves significant gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Siqi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weilong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xianliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suo_H/0/1/0/all/0/1"&gt;Hongbin Suo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinwei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhijie Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09099</id>
        <link href="http://arxiv.org/abs/2107.09099"/>
        <updated>2021-07-21T02:01:36.257Z</updated>
        <summary type="html"><![CDATA[Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiushi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Tom Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;H Lilian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xubo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. (arXiv:2107.08982v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08982</id>
        <link href="http://arxiv.org/abs/2107.08982"/>
        <updated>2021-07-21T02:01:36.213Z</updated>
        <summary type="html"><![CDATA[Multi-person pose estimation is an attractive and challenging task. Existing
methods are mostly based on two-stage frameworks, which include top-down and
bottom-up methods. Two-stage methods either suffer from high computational
redundancy for additional person detectors or they need to group keypoints
heuristically after predicting all the instance-agnostic keypoints. The
single-stage paradigm aims to simplify the multi-person pose estimation
pipeline and receives a lot of attention. However, recent single-stage methods
have the limitation of low performance due to the difficulty of regressing
various full-body poses from a single feature vector. Different from previous
solutions that involve complex heuristic designs, we present a simple yet
effective solution by employing instance-aware dynamic networks. Specifically,
we propose an instance-aware module to adaptively adjust (part of) the network
parameters for each instance. Our solution can significantly increase the
capacity and adaptive-ability of the network for recognizing various poses,
while maintaining a compact end-to-end trainable pipeline. Extensive
experiments on the MS-COCO dataset demonstrate that our method achieves
significant improvement over existing single-stage methods, and makes a better
balance of accuracy and efficiency compared to the state-of-the-art two-stage
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dahu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Wenming Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Ye Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive wavelet distillation from neural networks through interpretations. (arXiv:2107.09145v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09145</id>
        <link href="http://arxiv.org/abs/2107.09145"/>
        <updated>2021-07-21T02:01:36.204Z</updated>
        <summary type="html"><![CDATA[Recent deep-learning models have achieved impressive prediction performance,
but often sacrifice interpretability and computational efficiency.
Interpretability is crucial in many disciplines, such as science and medicine,
where models must be carefully vetted or where interpretation is the goal
itself. Moreover, interpretable models are concise and often yield
computational efficiency. Here, we propose adaptive wavelet distillation (AWD),
a method which aims to distill information from a trained neural network into a
wavelet transform. Specifically, AWD penalizes feature attributions of a neural
network in the wavelet domain to learn an effective multi-resolution wavelet
transform. The resulting model is highly predictive, concise, computationally
efficient, and has properties (such as a multi-scale structure) which make it
easy to interpret. In close collaboration with domain experts, we showcase how
AWD addresses challenges in two real-world settings: cosmological parameter
inference and molecular-partner prediction. In both cases, AWD yields a
scientifically interpretable and concise model which gives predictive
performance better than state-of-the-art neural networks. Moreover, AWD
identifies predictive features that are scientifically meaningful in the
context of respective domains. All code and models are released in a
full-fledged package available on Github
(https://github.com/Yu-Group/adaptive-wavelets).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1"&gt;Wooseok Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1"&gt;Chandan Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lanusse_F/0/1/0/all/0/1"&gt;Francois Lanusse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Song_E/0/1/0/all/0/1"&gt;Eli Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dang_S/0/1/0/all/0/1"&gt;Song Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_K/0/1/0/all/0/1"&gt;Kangmin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Upadhyayula_S/0/1/0/all/0/1"&gt;Srigokul Upadhyayula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Clustering-Based Technique for the Acceleration of Deep Convolutional Networks. (arXiv:2107.09095v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09095</id>
        <link href="http://arxiv.org/abs/2107.09095"/>
        <updated>2021-07-21T02:01:36.197Z</updated>
        <summary type="html"><![CDATA[Deep learning and especially the use of Deep Neural Networks (DNNs) provides
impressive results in various regression and classification tasks. However, to
achieve these results, there is a high demand for computing and storing
resources. This becomes problematic when, for instance, real-time, mobile
applications are considered, in which the involved (embedded) devices have
limited resources. A common way of addressing this problem is to transform the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Within the MCA framework, we
propose a clustering-based approach that is able to increase the number of
employed centroids/representatives, while at the same time, have an
acceleration gain compared to conventional, $k$-means based approaches. This is
achieved by imposing a special structure to the employed representatives, which
is enabled by the particularities of the problem at hand. Moreover, the
theoretical acceleration gains are presented and the key system
hyper-parameters that affect that gain, are identified. Extensive evaluation
studies carried out using various state-of-the-art DNN models trained in image
classification, validate the superiority of the proposed method as compared for
its use in MCA tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1"&gt;Erion-Vasilis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1"&gt;Christos Mavrokefalidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S. Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Driver Takeover Time in Conditionally Automated Driving. (arXiv:2107.09545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09545</id>
        <link href="http://arxiv.org/abs/2107.09545"/>
        <updated>2021-07-21T02:01:36.176Z</updated>
        <summary type="html"><![CDATA[It is extremely important to ensure a safe takeover transition in
conditionally automated driving. One of the critical factors that quantifies
the safe takeover transition is takeover time. Previous studies identified the
effects of many factors on takeover time, such as takeover lead time,
non-driving tasks, modalities of the takeover requests (TORs), and scenario
urgency. However, there is a lack of research to predict takeover time by
considering these factors all at the same time. Toward this end, we used
eXtreme Gradient Boosting (XGBoost) to predict the takeover time using a
dataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley
Additive exPlanation) to analyze and explain the effects of the predictors on
takeover time. We identified seven most critical predictors that resulted in
the best prediction performance. Their main effects and interaction effects on
takeover time were examined. The results showed that the proposed approach
provided both good performance and explainability. Our findings have
implications on the design of in-vehicle monitoring and alert systems to
facilitate the interaction between the drivers and the automated vehicle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayoub_J/0/1/0/all/0/1"&gt;Jackie Ayoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1"&gt;Na Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;X. Jessie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04259</id>
        <link href="http://arxiv.org/abs/2107.04259"/>
        <updated>2021-07-21T02:01:36.164Z</updated>
        <summary type="html"><![CDATA[We introduce the Unity Perception package which aims to simplify and
accelerate the process of generating synthetic datasets for computer vision
tasks by offering an easy-to-use and highly customizable toolset. This
open-source package extends the Unity Editor and engine components to generate
perfectly annotated examples for several common computer vision tasks.
Additionally, it offers an extensible Randomization framework that lets the
user quickly construct and configure randomized simulation parameters in order
to introduce variation into the generated datasets. We provide an overview of
the provided tools and how they work, and demonstrate the value of the
generated synthetic datasets by training a 2D object detection model. The model
trained with mostly synthetic data outperforms the model trained using only
real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1"&gt;Steve Borkman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1"&gt;Adam Crespi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1"&gt;Saurav Dhakad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1"&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1"&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1"&gt;You-Cyuan Jhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1"&gt;Mohsen Kamalzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1"&gt;Steven Leal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1"&gt;Pete Parisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1"&gt;Cesar Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1"&gt;Wesley Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1"&gt;Alex Thaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1"&gt;Samuel Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nupur Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09204</id>
        <link href="http://arxiv.org/abs/2107.09204"/>
        <updated>2021-07-21T02:01:36.145Z</updated>
        <summary type="html"><![CDATA[Anomaly detection in images plays a significant role for many applications
across all industries, such as disease diagnosis in healthcare or quality
assurance in manufacturing. Manual inspection of images, when extended over a
monotonously repetitive period of time is very time consuming and can lead to
anomalies being overlooked.Artificial neural networks have proven themselves
very successful on simple, repetitive tasks, in some cases even outperforming
humans. Therefore, in this paper we investigate different methods of deep
learning, including supervised and unsupervised learning, for anomaly detection
applied to a quality assurance use case. We utilize the MVTec anomaly dataset
and develop three different models, a CNN for supervised anomaly detection,
KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly
detection and a DCGAN for generating reconstructed images. By experiments, we
found that KD-CAE performs better on the anomaly datasets compared to CNN and
NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also
implemented a DCGAN for the creation of new training data but due to
computational limitation and lack of extrapolating the mechanics of AnoGAN, we
restricted ourselves just to the generation of GAN based images. We conclude
that unsupervised methods are more powerful for anomaly detection in images,
especially in a setting where only a small amount of anomalous data is
available, or the data is unlabeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1"&gt;Vincent Wilmet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sauraj Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1"&gt;Tabea Redl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kon Sandaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenning Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-21T02:01:36.138Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, i.e., it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EEG-based Cross-Subject Driver Drowsiness Recognition with Interpretable CNN. (arXiv:2107.09507v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09507</id>
        <link href="http://arxiv.org/abs/2107.09507"/>
        <updated>2021-07-21T02:01:36.039Z</updated>
        <summary type="html"><![CDATA[In the context of electroencephalogram (EEG)-based driver drowsiness
recognition, it is still a challenging task to design a calibration-free
system, since there exists a significant variability of EEG signals among
different subjects and recording sessions. As deep learning has received much
research attention in recent years, many efforts have been made to use deep
learning methods for EEG signal recognition. However, existing works mostly
treat deep learning models as blackbox classifiers, while what have been
learned by the models and to which extent they are affected by the noise from
EEG data are still underexplored. In this paper, we develop a novel
convolutional neural network that can explain its decision by highlighting the
local areas of the input sample that contain important information for the
classification. The network has a compact structure for ease of interpretation
and takes advantage of separable convolutions to process the EEG signals in a
spatial-temporal sequence. Results show that the model achieves an average
accuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness
recognition, which is higher than the conventional baseline methods of
53.4%-72.68% and state-of-art deep learning methods of 63.90%-65.61%.
Visualization results show that the model has learned to recognize biologically
explainable features from EEG signals, e.g., Alpha spindles, as strong
indicators of drowsiness across different subjects. In addition, we also
explore reasons behind some wrongly classified samples and how the model is
affected by artifacts and noise in the data. Our work illustrates a promising
direction on using interpretable deep learning models to discover meaning
patterns related to different mental states from complex EEG signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jian Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yisi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zirui Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sourina_O/0/1/0/all/0/1"&gt;Olga Sourina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muller_Wittig_W/0/1/0/all/0/1"&gt;Wolfgang M&amp;#xfc;ller-Wittig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relay-Assisted Cooperative Federated Learning. (arXiv:2107.09518v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09518</id>
        <link href="http://arxiv.org/abs/2107.09518"/>
        <updated>2021-07-21T02:01:36.023Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has recently emerged as a promising technology to
enable artificial intelligence (AI) at the network edge, where distributed
mobile devices collaboratively train a shared AI model under the coordination
of an edge server. To significantly improve the communication efficiency of FL,
over-the-air computation allows a large number of mobile devices to
concurrently upload their local models by exploiting the superposition property
of wireless multi-access channels. Due to wireless channel fading, the model
aggregation error at the edge server is dominated by the weakest channel among
all devices, causing severe straggler issues. In this paper, we propose a
relay-assisted cooperative FL scheme to effectively address the straggler
issue. In particular, we deploy multiple half-duplex relays to cooperatively
assist the devices in uploading the local model updates to the edge server. The
nature of the over-the-air computation poses system objectives and constraints
that are distinct from those in traditional relay communication systems.
Moreover, the strong coupling between the design variables renders the
optimization of such a system challenging. To tackle the issue, we propose an
alternating-optimization-based algorithm to optimize the transceiver and relay
operation with low complexity. Then, we analyze the model aggregation error in
a single-relay case and show that our relay-assisted scheme achieves a smaller
error than the one without relays provided that the relay transmit power and
the relay channel gains are sufficiently large. The analysis provides critical
insights on relay deployment in the implementation of cooperative FL. Extensive
numerical results show that our design achieves faster convergence compared
with state-of-the-art schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zehong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying-Jun Angela Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wearable Health Monitoring System for Older Adults in a Smart Home Environment. (arXiv:2107.09509v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09509</id>
        <link href="http://arxiv.org/abs/2107.09509"/>
        <updated>2021-07-21T02:01:36.014Z</updated>
        <summary type="html"><![CDATA[The advent of IoT has enabled the design of connected and integrated smart
health monitoring systems. These smart health monitoring systems could be
realized in a smart home context to render long-term care to the elderly
population. In this paper, we present the design of a wearable health
monitoring system suitable for older adults in a smart home context. The
proposed system offers solutions to monitor the stress, blood pressure, and
location of an individual within a smart home environment. The stress detection
model proposed in this work uses Electrodermal Activity (EDA),
Photoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart
wristband for detecting physiological stress. The stress detection model is
trained and tested using stress labels obtained from salivary cortisol which is
a clinically established biomarker for physiological stress. A voice-based
prototype is also implemented and the feasibility of the proposed system for
integration in a smart home environment is analyzed by simulating a data
acquisition and streaming scenario. We have also proposed a blood pressure
estimation model using PPG signal and advanced regression techniques for
integration with the stress detection model in the wearable health monitoring
system. Finally, the design of a voice-assisted indoor location system is
proposed for integration with the proposed system within a smart home
environment. The proposed wearable health monitoring system is an important
direction to realize a smart home environment with extensive diagnostic
capabilities so that such a system could be useful for rendering long-term and
personalized care to the aging population in the comfort of their home.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nath_R/0/1/0/all/0/1"&gt;Rajdeep Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thapliyal_H/0/1/0/all/0/1"&gt;Himanshu Thapliyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images. (arXiv:1912.00384v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.00384</id>
        <link href="http://arxiv.org/abs/1912.00384"/>
        <updated>2021-07-21T02:01:35.994Z</updated>
        <summary type="html"><![CDATA[Weakly-supervised object detection attempts to limit the amount of
supervision by dispensing the need for bounding boxes, but still assumes
image-level labels on the entire training set. In this work, we study the
problem of training an object detector from one or few images with image-level
labels and a larger set of completely unlabeled images. This is an extreme case
of semi-supervised learning where the labeled data are not enough to bootstrap
the learning of a detector. Our solution is to train a weakly-supervised
student detector model from image-level pseudo-labels generated on the
unlabeled set by a teacher classifier model, bootstrapped by region-level
similarities to labeled images. Building upon the recent representative
weakly-supervised pipeline PCL, our method can use more unlabeled images to
achieve performance competitive or superior to many recent weakly-supervised
detection solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhaohui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1"&gt;Yannis Avrithis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09648</id>
        <link href="http://arxiv.org/abs/2107.09648"/>
        <updated>2021-07-21T02:01:35.987Z</updated>
        <summary type="html"><![CDATA[Despite being designed for performance rather than cognitive plausibility,
transformer language models have been found to be better at predicting metrics
used to assess human language comprehension than language models with other
architectures, such as recurrent neural networks. Based on how well they
predict the N400, a neural signal associated with processing difficulty, we
propose and provide evidence for one possible explanation - their predictions
are affected by the preceding context in a way analogous to the effect of
semantic facilitation in humans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1"&gt;James A. Michaelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1"&gt;Megan D. Bardolph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1"&gt;Seana Coulson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1"&gt;Benjamin K. Bergen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn2Hop: Learned Optimization on Rough Landscapes. (arXiv:2107.09661v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09661</id>
        <link href="http://arxiv.org/abs/2107.09661"/>
        <updated>2021-07-21T02:01:35.981Z</updated>
        <summary type="html"><![CDATA[Optimization of non-convex loss surfaces containing many local minima remains
a critical problem in a variety of domains, including operations research,
informatics, and material design. Yet, current techniques either require
extremely high iteration counts or a large number of random restarts for good
performance. In this work, we propose adapting recent developments in
meta-learning to these many-minima problems by learning the optimization
algorithm for various loss landscapes. We focus on problems from atomic
structural optimization--finding low energy configurations of many-atom
systems--including widely studied models such as bimetallic clusters and
disordered silicon. We find that our optimizer learns a 'hopping' behavior
which enables efficient exploration and improves the rate of low energy minima
discovery. Finally, our learned optimizers show promising generalization with
efficiency gains on never before seen tasks (e.g. new elements or
compositions). Code will be made available shortly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merchant_A/0/1/0/all/0/1"&gt;Amil Merchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1"&gt;Luke Metz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1"&gt;Sam Schoenholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1"&gt;Ekin Dogus Cubuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09282</id>
        <link href="http://arxiv.org/abs/2107.09282"/>
        <updated>2021-07-21T02:01:35.973Z</updated>
        <summary type="html"><![CDATA[Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most of methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduced a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. Moreover, to boost the
performance, we argue that weak augmentations matter to represent a more
reliable relation, and leverage momentum strategy for practical efficiency.
Experimental results show that our proposed ReSSL significantly outperforms the
previous state-of-the-art algorithms in terms of both performance and training
efficiency. Code is available at \url{https://github.com/KyleZheng1997/ReSSL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge of chaos as a guiding principle for modern neural network training. (arXiv:2107.09437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09437</id>
        <link href="http://arxiv.org/abs/2107.09437"/>
        <updated>2021-07-21T02:01:35.966Z</updated>
        <summary type="html"><![CDATA[The success of deep neural networks in real-world problems has prompted many
attempts to explain their training dynamics and generalization performance, but
more guiding principles for the training of neural networks are still needed.
Motivated by the edge of chaos principle behind the optimal performance of
neural networks, we study the role of various hyperparameters in modern neural
network training algorithms in terms of the order-chaos phase diagram. In
particular, we study a fully analytical feedforward neural network trained on
the widely adopted Fashion-MNIST dataset, and study the dynamics associated
with the hyperparameters in back-propagation during the training process. We
find that for the basic algorithm of stochastic gradient descent with momentum,
in the range around the commonly used hyperparameter values, clear scaling
relations are present with respect to the training time during the ordered
phase in the phase diagram, and the model's optimal generalization power at the
edge of chaos is similar across different training parameter combinations. In
the chaotic phase, the same scaling no longer exists. The scaling allows us to
choose the training parameters to achieve faster training without sacrificing
performance. In addition, we find that the commonly used model regularization
method - weight decay - effectively pushes the model towards the ordered phase
to achieve better performance. Leveraging on this fact and the scaling
relations in the other hyperparameters, we derived a principled guideline for
hyperparameter determination, such that the model can achieve optimal
performance by saturating it at the edge of chaos. Demonstrated on this simple
neural network model and training algorithm, our work improves the
understanding of neural network training dynamics, and can potentially be
extended to guiding principles of more complex model architectures and
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Ling Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Choy Heng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09240</id>
        <link href="http://arxiv.org/abs/2107.09240"/>
        <updated>2021-07-21T02:01:35.947Z</updated>
        <summary type="html"><![CDATA[Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Fu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Probabilistic Inference in Deep Learning: Beyond Marginal Predictions. (arXiv:2107.09224v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09224</id>
        <link href="http://arxiv.org/abs/2107.09224"/>
        <updated>2021-07-21T02:01:35.941Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge for any intelligent system is prediction: given some
inputs $X_1,..,X_\tau$ can you predict outcomes $Y_1,.., Y_\tau$. The KL
divergence $\mathbf{d}_{\mathrm{KL}}$ provides a natural measure of prediction
quality, but the majority of deep learning research looks only at the marginal
predictions per input $X_t$. In this technical report we propose a scoring rule
$\mathbf{d}_{\mathrm{KL}}^\tau$, parameterized by $\tau \in \mathcal{N}$ that
evaluates the joint predictions at $\tau$ inputs simultaneously. We show that
the commonly-used $\tau=1$ can be insufficient to drive good decisions in many
settings of interest. We also show that, as $\tau$ grows, performing well
according to $\mathbf{d}_{\mathrm{KL}}^\tau$ recovers universal guarantees for
any possible decision. Finally, we provide problem-dependent guidance on the
scale of $\tau$ for which our score provides sufficient guarantees for good
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiuyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zheng Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Establishing process-structure linkages using Generative Adversarial Networks. (arXiv:2107.09402v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.09402</id>
        <link href="http://arxiv.org/abs/2107.09402"/>
        <updated>2021-07-21T02:01:35.934Z</updated>
        <summary type="html"><![CDATA[The microstructure of material strongly influences its mechanical properties
and the microstructure itself is influenced by the processing conditions. Thus,
establishing a Process-Structure-Property relationship is a crucial task in
material design and is of interest in many engineering applications. We develop
a GAN (Generative Adversarial Network) to synthesize microstructures based on
given processing conditions. This approach is devoid of feature engineering,
needs little domain awareness, and can be applied to a wide variety of material
systems. Results show that our GAN model can produce high-fidelity multi-phase
microstructures which have a good correlation with the given processing
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Safiuddin_M/0/1/0/all/0/1"&gt;Mohammad Safiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Reddy_C/0/1/0/all/0/1"&gt;CH Likith Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Vasantada_G/0/1/0/all/0/1"&gt;Ganesh Vasantada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Harsha_C/0/1/0/all/0/1"&gt;CHJNS Harsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Gangolu_S/0/1/0/all/0/1"&gt;Srinu Gangolu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Approach to Invariant Deep Neural Networks. (arXiv:2107.09301v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09301</id>
        <link href="http://arxiv.org/abs/2107.09301"/>
        <updated>2021-07-21T02:01:35.929Z</updated>
        <summary type="html"><![CDATA[We propose a novel Bayesian neural network architecture that can learn
invariances from data alone by inferring a posterior distribution over
different weight-sharing schemes. We show that our model outperforms other
non-invariant architectures, when trained on datasets that contain specific
invariances. The same holds true when no data augmentation is performed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mourdoukoutas_N/0/1/0/all/0/1"&gt;Nikolaos Mourdoukoutas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Federici_M/0/1/0/all/0/1"&gt;Marco Federici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pantalos_G/0/1/0/all/0/1"&gt;Georges Pantalos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1"&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVSNet: An End-to-end Speaker Voice Similarity Assessment Model. (arXiv:2107.09392v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.09392</id>
        <link href="http://arxiv.org/abs/2107.09392"/>
        <updated>2021-07-21T02:01:35.922Z</updated>
        <summary type="html"><![CDATA[Neural evaluation metrics derived for numerous speech generation tasks have
recently attracted great attention. In this paper, we propose SVSNet, the first
end-to-end neural network model to assess the speaker voice similarity between
natural speech and synthesized speech. Unlike most neural evaluation metrics
that use hand-crafted features, SVSNet directly takes the raw waveform as input
to more completely utilize speech information for prediction. SVSNet consists
of encoder, co-attention, distance calculation, and prediction modules and is
trained in an end-to-end manner. The experimental results on the Voice
Conversion Challenge 2018 and 2020 (VCC2018 and VCC2020) datasets show that
SVSNet notably outperforms well-known baseline systems in the assessment of
speaker similarity at the utterance and system levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1"&gt;Cheng-Hung Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yu-Huai Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression. (arXiv:2107.09461v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09461</id>
        <link href="http://arxiv.org/abs/2107.09461"/>
        <updated>2021-07-21T02:01:35.903Z</updated>
        <summary type="html"><![CDATA[Due to the high communication cost in distributed and federated learning,
methods relying on compressed communication are becoming increasingly popular.
Besides, the best theoretically and practically performing gradient-type
methods invariably rely on some form of acceleration/momentum to reduce the
number of communications (faster convergence), e.g., Nesterov's accelerated
gradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to
combine the benefits of communication compression and convergence acceleration,
we propose a \emph{compressed and accelerated} gradient method for distributed
optimization, which we call CANITA. Our CANITA achieves the \emph{first
accelerated rate}
$O\bigg(\sqrt{\Big(1+\sqrt{\frac{\omega^3}{n}}\Big)\frac{L}{\epsilon}} +
\omega\big(\frac{1}{\epsilon}\big)^{\frac{1}{3}}\bigg)$, which improves upon
the state-of-the-art non-accelerated rate
$O\left((1+\frac{\omega}{n})\frac{L}{\epsilon} +
\frac{\omega^2+n}{\omega+n}\frac{1}{\epsilon}\right)$ of DIANA (Khaled et al.,
2020b) for distributed general convex problems, where $\epsilon$ is the target
error, $L$ is the smooth parameter of the objective, $n$ is the number of
machines/devices, and $\omega$ is the compression parameter (larger $\omega$
means more compression can be applied, and no compression implies $\omega=0$).
Our results show that as long as the number of devices $n$ is large (often true
in distributed/federated learning), or the compression $\omega$ is not very
high, CANITA achieves the faster convergence rate
$O\Big(\sqrt{\frac{L}{\epsilon}}\Big)$, i.e., the number of communication
rounds is $O\Big(\sqrt{\frac{L}{\epsilon}}\Big)$ (vs.
$O\big(\frac{L}{\epsilon}\big)$ achieved by previous works). As a result,
CANITA enjoys the advantages of both compression (compressed communication in
each round) and acceleration (much fewer communication rounds).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhize Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CREW: Computation Reuse and Efficient Weight Storage for Hardware-accelerated MLPs and RNNs. (arXiv:2107.09408v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.09408</id>
        <link href="http://arxiv.org/abs/2107.09408"/>
        <updated>2021-07-21T02:01:35.897Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have achieved tremendous success for cognitive
applications. The core operation in a DNN is the dot product between quantized
inputs and weights. Prior works exploit the weight/input repetition that arises
due to quantization to avoid redundant computations in Convolutional Neural
Networks (CNNs). However, in this paper we show that their effectiveness is
severely limited when applied to Fully-Connected (FC) layers, which are
commonly used in state-of-the-art DNNs, as it is the case of modern Recurrent
Neural Networks (RNNs) and Transformer models.

To improve energy-efficiency of FC computation we present CREW, a hardware
accelerator that implements Computation Reuse and an Efficient Weight Storage
mechanism to exploit the large number of repeated weights in FC layers. CREW
first performs the multiplications of the unique weights by their respective
inputs and stores the results in an on-chip buffer. The storage requirements
are modest due to the small number of unique weights and the relatively small
size of the input compared to convolutional layers. Next, CREW computes each
output by fetching and adding its required products. To this end, each weight
is replaced offline by an index in the buffer of unique products. Indices are
typically smaller than the quantized weights, since the number of unique
weights for each input tends to be much lower than the range of quantized
weights, which reduces storage and memory bandwidth requirements.

Overall, CREW greatly reduces the number of multiplications and provides
significant savings in model memory footprint and memory bandwidth usage. We
evaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x
speedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,
a state-of-art computation reuse technique, CREW achieves 2.10x speedup and
2.08x energy savings on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1"&gt;Marc Riera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnau_J/0/1/0/all/0/1"&gt;Jose-Maria Arnau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1"&gt;Antonio Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An induction proof of the backpropagation algorithm in matrix notation. (arXiv:2107.09384v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09384</id>
        <link href="http://arxiv.org/abs/2107.09384"/>
        <updated>2021-07-21T02:01:35.890Z</updated>
        <summary type="html"><![CDATA[Backpropagation (BP) is a core component of the contemporary deep learning
incarnation of neural networks. Briefly, BP is an algorithm that exploits the
computational architecture of neural networks to efficiently evaluate the
gradient of a cost function during neural network parameter optimization. The
validity of BP rests on the application of a multivariate chain rule to the
computational architecture of neural networks and their associated objective
functions. Introductions to deep learning theory commonly present the
computational architecture of neural networks in matrix form, but eschew a
parallel formulation and justification of BP in the framework of matrix
differential calculus. This entails several drawbacks for the theory and
didactics of deep learning. In this work, we overcome these limitations by
providing a full induction proof of the BP algorithm in matrix notation.
Specifically, we situate the BP algorithm in the framework of matrix
differential calculus, encompass affine-linear potential functions, prove the
validity of the BP algorithm in inductive form, and exemplify the
implementation of the matrix form BP algorithm in computer code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ostwald_D/0/1/0/all/0/1"&gt;Dirk Ostwald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Usee_F/0/1/0/all/0/1"&gt;Franziska Us&amp;#xe9;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Significant Wave Height Prediction based on Wavelet Graph Neural Network. (arXiv:2107.09483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09483</id>
        <link href="http://arxiv.org/abs/2107.09483"/>
        <updated>2021-07-21T02:01:35.883Z</updated>
        <summary type="html"><![CDATA[Computational intelligence-based ocean characteristics forecasting
applications, such as Significant Wave Height (SWH) prediction, are crucial for
avoiding social and economic loss in coastal cities. Compared to the
traditional empirical-based or numerical-based forecasting models, "soft
computing" approaches, including machine learning and deep learning models,
have shown numerous success in recent years. In this paper, we focus on
enabling the deep learning model to learn both short-term and long-term
spatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural
Network (WGNN) approach is proposed to integrate the advantages of wavelet
transform and graph neural network. Several parallel graph neural networks are
separately trained on wavelet decomposed data, and the reconstruction of each
model's prediction forms the final SWH prediction. Experimental results show
that the proposed WGNN approach outperforms other models, including the
numerical models, the machine learning models, and several deep learning
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Delong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaomin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zewen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wave-Informed Matrix Factorization withGlobal Optimality Guarantees. (arXiv:2107.09144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09144</id>
        <link href="http://arxiv.org/abs/2107.09144"/>
        <updated>2021-07-21T02:01:35.877Z</updated>
        <summary type="html"><![CDATA[With the recent success of representation learning methods, which includes
deep learning as a special case, there has been considerable interest in
developing representation learning techniques that can incorporate known
physical constraints into the learned representation. As one example, in many
applications that involve a signal propagating through physical media (e.g.,
optics, acoustics, fluid dynamics, etc), it is known that the dynamics of the
signal must satisfy constraints imposed by the wave equation. Here we propose a
matrix factorization technique that decomposes such signals into a sum of
components, where each component is regularized to ensure that it satisfies
wave equation constraints. Although our proposed formulation is non-convex, we
prove that our model can be efficiently solved to global optimality in
polynomial time. We demonstrate the benefits of our work by applications in
structural health monitoring, where prior work has attempted to solve this
problem using sparse dictionary learning approaches that do not come with any
theoretical guarantees regarding convergence to global optimality and employ
heuristics to capture desired physical constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tetali_H/0/1/0/all/0/1"&gt;Harsha Vardhan Tetali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_J/0/1/0/all/0/1"&gt;Joel B. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1"&gt;Benjamin D. Haeffele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09305</id>
        <link href="http://arxiv.org/abs/2107.09305"/>
        <updated>2021-07-21T02:01:35.858Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often have a huge number of parameters, which posts
challenges in deployment in application scenarios with limited memory and
computation capacity. Knowledge distillation is one approach to derive compact
models from bigger ones. However, it has been observed that a converged heavy
teacher model is strongly constrained for learning a compact student network
and could make the optimization subject to poor local optima. In this paper, we
propose ProKT, a new model-agnostic method by projecting the supervision
signals of a teacher model into the student's parameter space. Such projection
is implemented by decomposing the training objective into local intermediate
targets with an approximate mirror descent technique. The proposed method could
be less sensitive with the quirks during optimization which could result in a
better local optimum. Experiments on both image and text datasets show that our
proposed ProKT consistently achieves superior performance compared to other
existing knowledge distillation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Wenxian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuxuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bohan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A quantum algorithm for training wide and deep classical neural networks. (arXiv:2107.09200v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.09200</id>
        <link href="http://arxiv.org/abs/2107.09200"/>
        <updated>2021-07-21T02:01:35.851Z</updated>
        <summary type="html"><![CDATA[Given the success of deep learning in classical machine learning, quantum
algorithms for traditional neural network architectures may provide one of the
most promising settings for quantum machine learning. Considering a
fully-connected feedforward neural network, we show that conditions amenable to
classical trainability via gradient descent coincide with those necessary for
efficiently solving quantum linear systems. We propose a quantum algorithm to
approximately train a wide and deep neural network up to $O(1/n)$ error for a
training set of size $n$ by performing sparse matrix inversion in $O(\log n)$
time. To achieve an end-to-end exponential speedup over gradient descent, the
data distribution must permit efficient state preparation and readout. We
numerically demonstrate that the MNIST image dataset satisfies such conditions;
moreover, the quantum algorithm matches the accuracy of the fully-connected
network. Beyond the proven architecture, we provide empirical evidence for
$O(\log n)$ training of a convolutional neural network with pooling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zlokapa_A/0/1/0/all/0/1"&gt;Alexander Zlokapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Neven_H/0/1/0/all/0/1"&gt;Hartmut Neven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lloyd_S/0/1/0/all/0/1"&gt;Seth Lloyd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression. (arXiv:2107.09194v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09194</id>
        <link href="http://arxiv.org/abs/2107.09194"/>
        <updated>2021-07-21T02:01:35.844Z</updated>
        <summary type="html"><![CDATA[Models like LASSO and ridge regression are extensively used in practice due
to their interpretability, ease of use, and strong theoretical guarantees.
Cross-validation (CV) is widely used for hyperparameter tuning in these models,
but do practical optimization methods minimize the true out-of-sample loss? A
recent line of research promises to show that the optimum of the CV loss
matches the optimum of the out-of-sample loss (possibly after simple
corrections). It remains to show how tractable it is to minimize the CV loss.
In the present paper, we show that, in the case of ridge regression, the CV
loss may fail to be quasiconvex and thus may have multiple local optima. We can
guarantee that the CV loss is quasiconvex in at least one case: when the
spectrum of the covariate matrix is nearly flat and the noise in the observed
responses is not too high. More generally, we show that quasiconvexity status
is independent of many properties of the observed data (response norm,
covariate-matrix right singular vectors and singular-value scaling) and has a
complex dependence on the few that remain. We empirically confirm our theory
using simulated experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stephenson_W/0/1/0/all/0/1"&gt;William T. Stephenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frangella_Z/0/1/0/all/0/1"&gt;Zachary Frangella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1"&gt;Tamara Broderick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Credit Card Fraud Detection: A Journey from Research to Production. (arXiv:2107.09323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09323</id>
        <link href="http://arxiv.org/abs/2107.09323"/>
        <updated>2021-07-21T02:01:35.837Z</updated>
        <summary type="html"><![CDATA[The dark face of digital commerce generalization is the increase of fraud
attempts. To prevent any type of attacks, state of the art fraud detection
systems are now embedding Machine Learning (ML) modules. The conception of such
modules is only communicated at the level of research and papers mostly focus
on results for isolated benchmark datasets and metrics. But research is only a
part of the journey, preceded by the right formulation of the business problem
and collection of data, and followed by a practical integration. In this paper,
we give a wider vision of the process, on a case study of transfer learning for
fraud detection, from business to research, and back to business.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siblini_W/0/1/0/all/0/1"&gt;Wissam Siblini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coter_G/0/1/0/all/0/1"&gt;Guillaume Coter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fabry_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Fabry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Guelton_L/0/1/0/all/0/1"&gt;Liyun He-Guelton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oble_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Obl&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebichot_B/0/1/0/all/0/1"&gt;Bertrand Lebichot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgne_Y/0/1/0/all/0/1"&gt;Yann-A&amp;#xeb;l Le Borgne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1"&gt;Gianluca Bontempi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward-Weighted Regression Converges to a Global Optimum. (arXiv:2107.09088v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09088</id>
        <link href="http://arxiv.org/abs/2107.09088"/>
        <updated>2021-07-21T02:01:35.829Z</updated>
        <summary type="html"><![CDATA[Reward-Weighted Regression (RWR) belongs to a family of widely known
iterative Reinforcement Learning algorithms based on the
Expectation-Maximization framework. In this family, learning at each iteration
consists of sampling a batch of trajectories using the current policy and
fitting a new policy to maximize a return-weighted log-likelihood of actions.
Although RWR is known to yield monotonic improvement of the policy under
certain circumstances, whether and under which conditions RWR converges to the
optimal policy have remained open questions. In this paper, we provide for the
first time a proof that RWR converges to a global optimum when no function
approximation is used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Strupl_M/0/1/0/all/0/1"&gt;Miroslav &amp;#x160;trupl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Faccio_F/0/1/0/all/0/1"&gt;Francesco Faccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ashley_D/0/1/0/all/0/1"&gt;Dylan R. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Srivastava_R/0/1/0/all/0/1"&gt;Rupesh Kumar Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidhuber_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Music Tempo Estimation via Neural Networks -- A Comparative Analysis. (arXiv:2107.09208v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09208</id>
        <link href="http://arxiv.org/abs/2107.09208"/>
        <updated>2021-07-21T02:01:35.810Z</updated>
        <summary type="html"><![CDATA[This paper presents a comparative analysis on two artificial neural networks
(with different architectures) for the task of tempo estimation. For this
purpose, it also proposes the modeling, training and evaluation of a B-RNN
(Bidirectional Recurrent Neural Network) model capable of estimating tempo in
bpm (beats per minutes) of musical pieces, without using external auxiliary
modules. An extensive database (12,550 pieces in total) was curated to conduct
a quantitative and qualitative analysis over the experiment. Percussion-only
tracks were also included in the dataset. The performance of the B-RNN is
compared to that of state-of-the-art models. For further comparison, a
state-of-the-art CNN was also retrained with the same datasets used for the
B-RNN training. Evaluation results for each model and datasets are presented
and discussed, as well as observations and ideas for future research. Tempo
estimation was more accurate for the percussion only dataset, suggesting that
the estimation can be more accurate for percussion-only tracks, although
further experiments (with more of such datasets) should be made to gather
stronger evidence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1"&gt;Mila Soares de Oliveira de Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moura_P/0/1/0/all/0/1"&gt;Pedro Nuno de Souza Moura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1"&gt;Jean-Pierre Briot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support Recovery in Universal One-bit Compressed Sensing. (arXiv:2107.09091v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09091</id>
        <link href="http://arxiv.org/abs/2107.09091"/>
        <updated>2021-07-21T02:01:35.804Z</updated>
        <summary type="html"><![CDATA[One-bit compressed sensing (1bCS) is an extreme-quantized signal acquisition
method that has been widely studied in the past decade. In 1bCS, linear samples
of a high dimensional signal are quantized to only one bit per sample (sign of
the measurement). Assuming the original signal vector to be sparse, existing
results either aim to find the support of the vector, or approximate the signal
within an $\epsilon$-ball. The focus of this paper is support recovery, which
often also computationally facilitates approximate signal recovery. A universal
measurement matrix for 1bCS refers to one set of measurements that work for all
sparse signals. With universality, it is known that $\tilde{\Theta}(k^2)$ 1bCS
measurements are necessary and sufficient for support recovery (where $k$
denotes the sparsity). In this work, we show that it is possible to universally
recover the support with a small number of false positives with
$\tilde{O}(k^{3/2})$ measurements. If the dynamic range of the signal vector is
known, then with a different technique, this result can be improved to only
$\tilde{O}(k)$ measurements. Further results on support recovery are also
provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumyabrata Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09356</id>
        <link href="http://arxiv.org/abs/2107.09356"/>
        <updated>2021-07-21T02:01:35.797Z</updated>
        <summary type="html"><![CDATA[Communication between agents in collaborative multi-agent settings is in
general implicit or a direct data stream. This paper considers text-based
natural language as a novel form of communication between multiple agents
trained with reinforcement learning. This could be considered first steps
toward a truly autonomous communication without the need to define a limited
set of instructions, and natural collaboration between humans and robots.
Inspired by the game of Blind Leads, we propose an environment where one agent
uses natural language instructions to guide another through a maze. We test the
ability of reinforcement learning agents to effectively communicate through
discrete word-level symbols and show that the agents are able to sufficiently
communicate through natural language with a limited vocabulary. Although the
communication is not always perfect English, the agents are still able to
navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of
0.61 over randomly generated sequences while maintaining a 100% maze completion
rate. This is a 3.5 times the performance of the random baseline using our
reference set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1"&gt;Kevin Eloff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1"&gt;Herman Engelbrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive/Negative Approximate Multipliers for DNN Accelerators. (arXiv:2107.09366v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.09366</id>
        <link href="http://arxiv.org/abs/2107.09366"/>
        <updated>2021-07-21T02:01:35.789Z</updated>
        <summary type="html"><![CDATA[Recent Deep Neural Networks (DNNs) managed to deliver superhuman accuracy
levels on many AI tasks. Several applications rely more and more on DNNs to
deliver sophisticated services and DNN accelerators are becoming integral
components of modern systems-on-chips. DNNs perform millions of arithmetic
operations per inference and DNN accelerators integrate thousands of
multiply-accumulate units leading to increased energy requirements. Approximate
computing principles are employed to significantly lower the energy consumption
of DNN accelerators at the cost of some accuracy loss. Nevertheless, recent
research demonstrated that complex DNNs are increasingly sensitive to
approximation. Hence, the obtained energy savings are often limited when
targeting tight accuracy constraints. In this work, we present a dynamically
configurable approximate multiplier that supports three operation modes, i.e.,
exact, positive error, and negative error. In addition, we propose a
filter-oriented approximation method to map the weights to the appropriate
modes of the approximate multiplier. Our mapping algorithm balances the
positive with the negative errors due to the approximate multiplications,
aiming at maximizing the energy reduction while minimizing the overall
convolution error. We evaluate our approach on multiple DNNs and datasets
against state-of-the-art approaches, where our method achieves 18.33% energy
gains on average across 7 NNs on 4 different datasets for a maximum accuracy
drop of only 1%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spantidi_O/0/1/0/all/0/1"&gt;Ourania Spantidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zervakis_G/0/1/0/all/0/1"&gt;Georgios Zervakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anagnostopoulos_I/0/1/0/all/0/1"&gt;Iraklis Anagnostopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amrouch_H/0/1/0/all/0/1"&gt;Hussam Amrouch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Henkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09262</id>
        <link href="http://arxiv.org/abs/2107.09262"/>
        <updated>2021-07-21T02:01:35.782Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Sanchita Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1"&gt;John J. Prevost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale graph representation learning with very deep GNNs and self-supervision. (arXiv:2107.09422v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09422</id>
        <link href="http://arxiv.org/abs/2107.09422"/>
        <updated>2021-07-21T02:01:35.764Z</updated>
        <summary type="html"><![CDATA[Effectively and efficiently deploying graph neural networks (GNNs) at scale
remains one of the most challenging aspects of graph representation learning.
Many powerful solutions have only ever been validated on comparatively small
datasets, often with counter-intuitive outcomes -- a barrier which has been
broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered
the OGB-LSC with two large-scale GNNs: a deep transductive node classifier
powered by bootstrapping, and a very deep (up to 50-layer) inductive graph
regressor regularised by denoising objectives. Our models achieved an
award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In
doing so, we demonstrate evidence of scalable self-supervised graph
representation learning, and utility of very deep GNNs -- both very important
open issues. Our code is publicly available at:
https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Addanki_R/0/1/0/all/0/1"&gt;Ravichandra Addanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter W. Battaglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1"&gt;David Budden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deac_A/0/1/0/all/0/1"&gt;Andreea Deac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1"&gt;Jonathan Godwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keck_T/0/1/0/all/0/1"&gt;Thomas Keck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wai Lok Sibon Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1"&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stott_J/0/1/0/all/0/1"&gt;Jacklynn Stott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakoor_S/0/1/0/all/0/1"&gt;Shantanu Thakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating domain knowledge into neural-guided search. (arXiv:2107.09182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09182</id>
        <link href="http://arxiv.org/abs/2107.09182"/>
        <updated>2021-07-21T02:01:35.757Z</updated>
        <summary type="html"><![CDATA[Many AutoML problems involve optimizing discrete objects under a black-box
reward. Neural-guided search provides a flexible means of searching these
combinatorial spaces using an autoregressive recurrent neural network. A major
benefit of this approach is that builds up objects sequentially--this provides
an opportunity to incorporate domain knowledge into the search by directly
modifying the logits emitted during sampling. In this work, we formalize a
framework for incorporating such in situ priors and constraints into
neural-guided search, and provide sufficient conditions for enforcing
constraints. We integrate several priors and constraints from existing works
into this framework, propose several new ones, and demonstrate their efficacy
in informing the task of symbolic regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1"&gt;Brenden K. Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1"&gt;Claudio P. Santiago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1"&gt;Mikel Landajuela Larma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models. (arXiv:2107.09428v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.09428</id>
        <link href="http://arxiv.org/abs/2107.09428"/>
        <updated>2021-07-21T02:01:35.750Z</updated>
        <summary type="html"><![CDATA[Non-autoregressive (NAR) modeling has gained more and more attention in
speech processing. With recent state-of-the-art attention-based automatic
speech recognition (ASR) structure, NAR can realize promising real-time factor
(RTF) improvement with only small degradation of accuracy compared to the
autoregressive (AR) models. However, the recognition inference needs to wait
for the completion of a full speech utterance, which limits their applications
on low latency scenarios. To address this issue, we propose a novel end-to-end
streaming NAR speech recognition system by combining blockwise-attention and
connectionist temporal classification with mask-predict (Mask-CTC) NAR. During
inference, the input audio is separated into small blocks and then processed in
a blockwise streaming way. To address the insertion and deletion error at the
edge of the output of each block, we apply an overlapping decoding strategy
with a dynamic mapping trick that can produce more coherent sentences.
Experimental results show that the proposed method improves online ASR
recognition in low latency conditions compared to vanilla Mask-CTC. Moreover,
it can achieve a much faster inference speed compared to the AR attention-based
models. All of our codes will be publicly available at
https://github.com/espnet/espnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianzi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fujita_Y/0/1/0/all/0/1"&gt;Yuya Fujita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xuankai Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPAL: Offline Preference-Based Apprenticeship Learning. (arXiv:2107.09251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09251</id>
        <link href="http://arxiv.org/abs/2107.09251"/>
        <updated>2021-07-21T02:01:35.743Z</updated>
        <summary type="html"><![CDATA[We study how an offline dataset of prior (possibly random) experience can be
used to address two challenges that autonomous systems face when they endeavor
to learn from, adapt to, and collaborate with humans : (1) identifying the
human's intent and (2) safely optimizing the autonomous system's behavior to
achieve this inferred intent. First, we use the offline dataset to efficiently
infer the human's reward function via pool-based active preference learning.
Second, given this learned reward function, we perform offline reinforcement
learning to optimize a policy based on the inferred human intent. Crucially,
our proposed approach does not require actual physical rollouts or an accurate
simulator for either the reward learning or policy optimization steps, enabling
both safe and efficient apprenticeship learning. We identify and evaluate our
approach on a subset of existing offline RL benchmarks that are well suited for
offline reward learning and also evaluate extensions of these benchmarks which
allow more open-ended behaviors. Our experiments show that offline
preference-based reward learning followed by offline reinforcement learning
enables efficient and high-performing policies, while only requiring small
numbers of preference queries. Videos available at
https://sites.google.com/view/offline-prefs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Daniel Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LENS: Layer Distribution Enabled Neural Architecture Search in Edge-Cloud Hierarchies. (arXiv:2107.09309v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09309</id>
        <link href="http://arxiv.org/abs/2107.09309"/>
        <updated>2021-07-21T02:01:35.736Z</updated>
        <summary type="html"><![CDATA[Edge-Cloud hierarchical systems employing intelligence through Deep Neural
Networks (DNNs) endure the dilemma of workload distribution within them.
Previous solutions proposed to distribute workloads at runtime according to the
state of the surroundings, like the wireless conditions. However, such
conditions are usually overlooked at design time. This paper addresses this
issue for DNN architectural design by presenting a novel methodology, LENS,
which administers multi-objective Neural Architecture Search (NAS) for
two-tiered systems, where the performance objectives are refashioned to
consider the wireless communication parameters. From our experimental search
space, we demonstrate that LENS improves upon the traditional solution's Pareto
set by 76.47% and 75% with respect to the energy and latency metrics,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1"&gt;Mohanad Odema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_N/0/1/0/all/0/1"&gt;Nafiul Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1"&gt;Berken Utku Demirel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Embedding of ReLU Networks and an Analysis of their Identifiability. (arXiv:2107.09370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09370</id>
        <link href="http://arxiv.org/abs/2107.09370"/>
        <updated>2021-07-21T02:01:35.729Z</updated>
        <summary type="html"><![CDATA[Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are
described by a vector of parameters $\theta$, and realized as a piecewise
linear continuous function $R_{\theta}: x \in \mathbb R^{d} \mapsto
R_{\theta}(x) \in \mathbb R^{k}$. Natural scalings and permutations operations
on the parameters $\theta$ leave the realization unchanged, leading to
equivalence classes of parameters that yield the same realization. These
considerations in turn lead to the notion of identifiability -- the ability to
recover (the equivalence class of) $\theta$ from the sole knowledge of its
realization $R_{\theta}$. The overall objective of this paper is to introduce
an embedding for ReLU neural networks of any depth, $\Phi(\theta)$, that is
invariant to scalings and that provides a locally linear parameterization of
the realization of the network. Leveraging these two key properties, we derive
some conditions under which a deep ReLU network is indeed locally identifiable
from the knowledge of the realization on a finite set of samples $x_{i} \in
\mathbb R^{d}$. We study the shallow case in more depth, establishing necessary
and sufficient conditions for the network to be identifiable from a bounded
subset $\mathcal X \subseteq \mathbb R^{d}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1"&gt;Pierre Stock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09106</id>
        <link href="http://arxiv.org/abs/2107.09106"/>
        <updated>2021-07-21T02:01:35.711Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1"&gt;Spencer Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Interest: Large-Scale Visual Analysis of Model Behavior by Measuring Human-AI Alignment. (arXiv:2107.09234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09234</id>
        <link href="http://arxiv.org/abs/2107.09234"/>
        <updated>2021-07-21T02:01:35.704Z</updated>
        <summary type="html"><![CDATA[Saliency methods -- techniques to identify the importance of input features
on a model's output -- are a common first step in understanding neural network
behavior. However, interpreting saliency requires tedious manual inspection to
identify and aggregate patterns in model behavior, resulting in ad hoc or
cherry-picked analysis. To address these concerns, we present Shared Interest:
a set of metrics for comparing saliency with human annotated ground truths. By
providing quantitative descriptors, Shared Interest allows ranking, sorting,
and aggregation of inputs thereby facilitating large-scale systematic analysis
of model behavior. We use Shared Interest to identify eight recurring patterns
in model behavior including focusing on a sufficient subset of ground truth
features or being distracted by contextual features. Working with
representative real-world users, we show how Shared Interest can be used to
rapidly develop or lose trust in a model's reliability, uncover issues that are
missed in manual analyses, and enable interactive probing of model behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1"&gt;Angie Boggust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1"&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1"&gt;Arvind Satyanarayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1"&gt;Hendrik Strobelt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Friction System Performance with Symbolic Regression and Genetic Programming with Factor Variables. (arXiv:2107.09484v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09484</id>
        <link href="http://arxiv.org/abs/2107.09484"/>
        <updated>2021-07-21T02:01:35.697Z</updated>
        <summary type="html"><![CDATA[Friction systems are mechanical systems wherein friction is used for force
transmission (e.g. mechanical braking systems or automatic gearboxes). For
finding optimal and safe design parameters, engineers have to predict friction
system performance. This is especially difficult in real-world applications,
because it is affected by many parameters. We have used symbolic regression and
genetic programming for finding accurate and trustworthy prediction models for
this task. However, it is not straight-forward how nominal variables can be
included. In particular, a one-hot-encoding is unsatisfactory because genetic
programming tends to remove such indicator variables. We have therefore used
so-called factor variables for representing nominal variables in symbolic
regression models. Our results show that GP is able to produce symbolic
regression models for predicting friction performance with predictive accuracy
that is comparable to artificial neural networks. The symbolic regression
models with factor variables are less complex than models using a one-hot
encoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1"&gt;Michael Kommenda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Promberger_A/0/1/0/all/0/1"&gt;Andreas Promberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1"&gt;Falk Nickel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Analysis of Measure-Valued Derivatives for Policy Gradients. (arXiv:2107.09359v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09359</id>
        <link href="http://arxiv.org/abs/2107.09359"/>
        <updated>2021-07-21T02:01:35.690Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning methods for robotics are increasingly successful due
to the constant development of better policy gradient techniques. A precise
(low variance) and accurate (low bias) gradient estimator is crucial to face
increasingly complex tasks. Traditional policy gradient algorithms use the
likelihood-ratio trick, which is known to produce unbiased but high variance
estimates. More modern approaches exploit the reparametrization trick, which
gives lower variance gradient estimates but requires differentiable value
function approximators. In this work, we study a different type of stochastic
gradient estimator: the Measure-Valued Derivative. This estimator is unbiased,
has low variance, and can be used with differentiable and non-differentiable
function approximators. We empirically evaluate this estimator in the
actor-critic policy gradient setting and show that it can reach comparable
performance with methods based on the likelihood-ratio or reparametrization
tricks, both in low and high-dimensional action spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tateo_D/0/1/0/all/0/1"&gt;Davide Tateo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muratore_F/0/1/0/all/0/1"&gt;Fabio Muratore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09391</id>
        <link href="http://arxiv.org/abs/2107.09391"/>
        <updated>2021-07-21T02:01:35.682Z</updated>
        <summary type="html"><![CDATA[We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion and Gaussian perturbations improves, while even improving the
performance on clean images slightly without performing any data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1"&gt;Sadaf Gulshad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09286</id>
        <link href="http://arxiv.org/abs/2107.09286"/>
        <updated>2021-07-21T02:01:35.665Z</updated>
        <summary type="html"><![CDATA[Recent studies show that advanced priors play a major role in deep generative
models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has
achieved impressive results. However, due to the nature of model design, an
exemplar-based model usually requires vast amounts of data to participate in
training, which leads to huge computational complexity. To address this issue,
we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of
VAE with a prior based on Bayesian pseudocoreset. The proposed prior is
conditioned on a small-scale pseudocoreset rather than the whole dataset for
reducing the computational cost and avoiding overfitting. Simultaneously, we
obtain the optimal pseudocoreset via a stochastic optimization algorithm during
VAE training aiming to minimize the Kullback-Leibler divergence between the
prior based on the pseudocoreset and that based on the whole dataset.
Experimental results show that ByPE-VAE can achieve competitive improvements
over the state-of-the-art VAEs in the tasks of density estimation,
representation learning, and generative data augmentation. Particularly, on a
basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE
while almost holding the performance. Code is available at our supplementary
materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingzhong Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lirong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.09480</id>
        <link href="http://arxiv.org/abs/2107.09480"/>
        <updated>2021-07-21T02:01:35.658Z</updated>
        <summary type="html"><![CDATA[Sorted Table Search Procedures are the quintessential query-answering tool,
still very useful, e.g, Search Engines (Google Chrome). Speeding them up, in
small additional space with respect to the table being searched into, is still
a quite significant achievement. Static Learned Indexes have been very
successful in achieving such a speed-up, but leave open a major question: To
what extent one can enjoy the speed-up of Learned Indexes while using constant
or nearly constant additional space. By generalizing the experimental
methodology of a recent benchmarking study on Learned Indexes, we shed light on
this question, by considering two scenarios. The first, quite elementary, i.e.,
textbook code, and the second using advanced Learned Indexing algorithms and
the supporting sophisticated software platforms. Although in both cases one
would expect a positive answer, its achievement is not as simple as it seems.
Indeed, our extensive set of experiments reveal a complex relationship between
query time and model space. The findings regarding this relationship and the
corresponding quantitative estimates, across memory levels, can be of interest
to algorithm designers and of use to practitioners as well. As an essential
part of our research, we introduce two new models that are of interest in their
own right. The first is a constant space model that can be seen as a
generalization of $k$-ary search, while the second is a synoptic {\bf RMI}, in
which we can control model space usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1"&gt;Domenico Amato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1"&gt;Raffaele Giancarlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1"&gt;Giosu&amp;#xe8; Lo Bosco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide and Deep Graph Neural Network with Distributed Online Learning. (arXiv:2107.09203v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09203</id>
        <link href="http://arxiv.org/abs/2107.09203"/>
        <updated>2021-07-21T02:01:35.651Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are naturally distributed architectures for
learning representations from network data. This renders them suitable
candidates for decentralized tasks. In these scenarios, the underlying graph
often changes with time due to link failures or topology variations, creating a
mismatch between the graphs on which GNNs were trained and the ones on which
they are tested. Online learning can be leveraged to retrain GNNs at testing
time to overcome this issue. However, most online algorithms are centralized
and usually offer guarantees only on convex problems, which GNNs rarely lead
to. This paper develops the Wide and Deep GNN (WD-GNN), a novel architecture
that can be updated with distributed online learning mechanisms. The WD-GNN
consists of two components: the wide part is a linear graph filter and the deep
part is a nonlinear GNN. At training time, the joint wide and deep architecture
learns nonlinear representations from data. At testing time, the wide, linear
part is retrained, while the deep, nonlinear one remains fixed. This often
leads to a convex formulation. We further propose a distributed online learning
algorithm that can be implemented in a decentralized setting. We also show the
stability of the WD-GNN to changes of the underlying graph and analyze the
convergence of the proposed online learning procedure. Experiments on movie
recommendation, source localization and robot swarm control corroborate
theoretical findings and show the potential of the WD-GNN for distributed
online learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active operator inference for learning low-dimensional dynamical-system models from noisy data. (arXiv:2107.09256v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09256</id>
        <link href="http://arxiv.org/abs/2107.09256"/>
        <updated>2021-07-21T02:01:35.603Z</updated>
        <summary type="html"><![CDATA[Noise poses a challenge for learning dynamical-system models because already
small variations can distort the dynamics described by trajectory data. This
work builds on operator inference from scientific machine learning to infer
low-dimensional models from high-dimensional state trajectories polluted with
noise. The presented analysis shows that, under certain conditions, the
inferred operators are unbiased estimators of the well-studied projection-based
reduced operators from traditional model reduction. Furthermore, the connection
between operator inference and projection-based model reduction enables
bounding the mean-squared errors of predictions made with the learned models
with respect to traditional reduced models. The analysis also motivates an
active operator inference approach that judiciously samples high-dimensional
trajectories with the aim of achieving a low mean-squared error by reducing the
effect of noise. Numerical experiments with high-dimensional linear and
nonlinear state dynamics demonstrate that predictions obtained with active
operator inference have orders of magnitude lower mean-squared errors than
operator inference with traditional, equidistantly sampled trajectory data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uy_W/0/1/0/all/0/1"&gt;Wayne Isaac Tan Uy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuepeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yuxiao Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1"&gt;Benjamin Peherstorfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion. (arXiv:2107.09133v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09133</id>
        <link href="http://arxiv.org/abs/2107.09133"/>
        <updated>2021-07-21T02:01:35.596Z</updated>
        <summary type="html"><![CDATA[In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). We find empirically that long after
performance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the
structure in the gradient noise, and the Hessian matrix at the end of training
that explains this anomalous diffusion. To build this understanding, we first
derive a continuous-time model for SGD with finite learning rates and batch
sizes as an underdamped Langevin equation. We study this equation in the
setting of linear regression, where we can derive exact, analytic expressions
for the phase space dynamics of the parameters and their instantaneous
velocities from initialization to stationarity. Using the Fokker-Planck
equation, we show that the key ingredient driving these dynamics is not the
original training loss, but rather the combination of a modified loss, which
implicitly regularizes the velocity, and probability currents, which cause
oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on
ImageNet. Through the lens of statistical physics, we uncover a mechanistic
origin for the anomalous limiting dynamics of deep neural networks trained with
SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kunin_D/0/1/0/all/0/1"&gt;Daniel Kunin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagastuy_Brena_J/0/1/0/all/0/1"&gt;Javier Sagastuy-Brena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1"&gt;Lauren Gillespie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Margalit_E/0/1/0/all/0/1"&gt;Eshed Margalit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1"&gt;Hidenori Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1"&gt;Daniel L. K. Yamins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning autonomously identifying the source of errors for agents in a group mission. (arXiv:2107.09232v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.09232</id>
        <link href="http://arxiv.org/abs/2107.09232"/>
        <updated>2021-07-21T02:01:35.577Z</updated>
        <summary type="html"><![CDATA[When agents are swarmed to carry out a mission, there is often a sudden
failure of some of the agents observed from the command base. It is generally
difficult to distinguish whether the failure is caused by actuators
(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) solely by the communication
between the command base and the concerning agent. By making a collision to the
agent by another, we would be able to distinguish which hypothesis is likely:
For $h_a$, we expect to detect corresponding displacements while for $h_a$ we
do not. Such swarm strategies to grasp the situation are preferably to be
generated autonomously by artificial intelligence (AI). Preferable actions
($e.g.$, the collision) for the distinction would be those maximizing the
difference between the expected behaviors for each hypothesis, as a value
function. Such actions exist, however, only very sparsely in the whole
possibilities, for which the conventional search based on gradient methods does
not make sense. Instead, we have successfully applied the reinforcement
learning technique, achieving the maximization of such a sparse value function.
The machine learning actually concluded autonomously the colliding action to
distinguish the hypothesises. Getting recognized an agent with actuator error
by the action, the agents behave as if other ones want to assist the
malfunctioning one to achieve a given mission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Utimula_K/0/1/0/all/0/1"&gt;Keishu Utimula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayaschi_K/0/1/0/all/0/1"&gt;Ken-taro Hayaschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakano_K/0/1/0/all/0/1"&gt;Kousuke Nakano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hongo_K/0/1/0/all/0/1"&gt;Kenta Hongo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maezono_R/0/1/0/all/0/1"&gt;Ryo Maezono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Policy Gradient Method for Safe and Fast Reinforcement Learning: a Neural Tangent Kernel Based Approach. (arXiv:2107.09139v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09139</id>
        <link href="http://arxiv.org/abs/2107.09139"/>
        <updated>2021-07-21T02:01:35.570Z</updated>
        <summary type="html"><![CDATA[This paper presents a constrained policy gradient algorithm. We introduce
constraints for safe learning with the following steps. First, learning is
slowed down (lazy learning) so that the episodic policy change can be computed
with the help of the policy gradient theorem and the neural tangent kernel.
Then, this enables us the evaluation of the policy at arbitrary states too. In
the same spirit, learning can be guided, ensuring safety via augmenting episode
batches with states where the desired action probabilities are prescribed.
Finally, exogenous discounted sum of future rewards (returns) can be computed
at these specific state-action pairs such that the policy network satisfies
constraints. Computing the returns is based on solving a system of linear
equations (equality constraints) or a constrained quadratic program (inequality
constraints). Simulation results suggest that adding constraints (external
information) to the learning can improve learning in terms of speed and safety
reasonably if constraints are appropriately selected. The efficiency of the
constrained learning was demonstrated with a shallow and wide ReLU network in
the Cartpole and Lunar Lander OpenAI gym environments. The main novelty of the
paper is giving a practical use of the neural tangent kernel in reinforcement
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varga_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulcsar_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Kulcs&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1"&gt;Morteza Haghir Chehreghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-to-Sequence Piano Transcription with Transformers. (arXiv:2107.09142v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09142</id>
        <link href="http://arxiv.org/abs/2107.09142"/>
        <updated>2021-07-21T02:01:35.563Z</updated>
        <summary type="html"><![CDATA[Automatic Music Transcription has seen significant progress in recent years
by training custom deep neural networks on large datasets. However, these
models have required extensive domain-specific design of network architectures,
input/output representations, and complex decoding schemes. In this work, we
show that equivalent performance can be achieved using a generic
encoder-decoder Transformer with standard decoding methods. We demonstrate that
the model can learn to translate spectrogram inputs directly to MIDI-like
output events for several transcription tasks. This sequence-to-sequence
approach simplifies transcription by jointly modeling audio features and
language-like output dependencies, thus removing the need for task-specific
architectures. These results point toward possibilities for creating new Music
Information Retrieval models by focusing on dataset creation and labeling
rather than custom model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1"&gt;Curtis Hawthorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1"&gt;Ian Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swavely_R/0/1/0/all/0/1"&gt;Rigel Swavely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1"&gt;Ethan Manilow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1"&gt;Jesse Engel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold. (arXiv:2107.09207v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.09207</id>
        <link href="http://arxiv.org/abs/2107.09207"/>
        <updated>2021-07-21T02:01:35.534Z</updated>
        <summary type="html"><![CDATA[We show that the Riemannian gradient descent algorithm on the low-rank matrix
manifold almost surely escapes some spurious critical points on the boundary of
the manifold. Given that the low-rank matrix manifold is an incomplete set,
this result is the first to overcome this difficulty and partially justify the
global use of the Riemannian gradient descent on the manifold. The spurious
critical points are some rank-deficient matrices that capture only part of the
SVD components of the ground truth. They exhibit very singular behavior and
evade the classical analysis of strict saddle points. We show that using the
dynamical low-rank approximation and a rescaled gradient flow, some of the
spurious critical points can be converted to classical strict saddle points,
which leads to the desired result. Numerical experiments are provided to
support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hou_T/0/1/0/all/0/1"&gt;Thomas Y. Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Selection for Stein Variational Gradient Descent. (arXiv:2107.09338v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09338</id>
        <link href="http://arxiv.org/abs/2107.09338"/>
        <updated>2021-07-21T02:01:35.518Z</updated>
        <summary type="html"><![CDATA[Stein variational gradient descent (SVGD) and its variants have shown
promising successes in approximate inference for complex distributions.
However, their empirical performance depends crucially on the choice of optimal
kernel. Unfortunately, RBF kernel with median heuristics is a common choice in
previous approaches which has been proved sub-optimal. Inspired by the paradigm
of multiple kernel learning, our solution to this issue is using a combination
of multiple kernels to approximate the optimal kernel instead of a single one
which may limit the performance and flexibility. To do so, we extend Kernelized
Stein Discrepancy (KSD) to its multiple kernel view called Multiple Kernelized
Stein Discrepancy (MKSD). Further, we leverage MKSD to construct a general
algorithm based on SVGD, which be called Multiple Kernel SVGD (MK-SVGD).
Besides, we automatically assign a weight to each kernel without any other
parameters. The proposed method not only gets rid of optimal kernel dependence
but also maintains computational effectiveness. Experiments on various tasks
and models show the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingzhong Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Multisets with Large Alphabets. (arXiv:2107.09202v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09202</id>
        <link href="http://arxiv.org/abs/2107.09202"/>
        <updated>2021-07-21T02:01:35.511Z</updated>
        <summary type="html"><![CDATA[Current methods that optimally compress multisets are not suitable for
high-dimensional symbols, as their compute time scales linearly with alphabet
size. Compressing a multiset as an ordered sequence with off-the-shelf codecs
is computationally more efficient, but has a sub-optimal compression rate, as
bits are wasted encoding the order between symbols. We present a method that
can recover those bits, assuming symbols are i.i.d., at the cost of an
additional $\mathcal{O}(|\mathcal{M}|\log M)$ in average time complexity, where
$|\mathcal{M}|$ and $M$ are the total and unique number of symbols in the
multiset. Our method is compatible with any prefix-free code. Experiments show
that, when paired with efficient coders, our method can efficiently compress
high-dimensional sources such as multisets of images and collections of JSON
files.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1"&gt;Daniel Severo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1"&gt;James Townsend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1"&gt;Ashish Khisti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1"&gt;Alireza Makhzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09060</id>
        <link href="http://arxiv.org/abs/2107.09060"/>
        <updated>2021-07-21T02:01:35.437Z</updated>
        <summary type="html"><![CDATA[Physiological motion, such as cardiac and respiratory motion, during Magnetic
Resonance (MR) image acquisition can cause image artifacts. Motion correction
techniques have been proposed to compensate for these types of motion during
thoracic scans, relying on accurate motion estimation from undersampled
motion-resolved reconstruction. A particular interest and challenge lie in the
derivation of reliable non-rigid motion fields from the undersampled
motion-resolved data. Motion estimation is usually formulated in image space
via diffusion, parametric-spline, or optical flow methods. However, image-based
registration can be impaired by remaining aliasing artifacts due to the
undersampled motion-resolved reconstruction. In this work, we describe a
formalism to perform non-rigid registration directly in the sampled Fourier
space, i.e. k-space. We propose a deep-learning based approach to perform fast
and accurate non-rigid registration from the undersampled k-space data. The
basic working principle originates from the Local All-Pass (LAP) technique, a
recently introduced optical flow-based registration. The proposed LAPNet is
compared against traditional and deep learning image-based registrations and
tested on fully-sampled and highly-accelerated (with two undersampling
strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients
with suspected liver or lung metastases and 25 healthy subjects. The proposed
LAPNet provided consistent and superior performance to image-based approaches
throughout different sampling trajectories and acceleration factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1"&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jiazhen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haikun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1"&gt;Gastao Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1"&gt;Christopher Gilliam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1"&gt;Thierry Blu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Botnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1"&gt;Claudia Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09101</id>
        <link href="http://arxiv.org/abs/2107.09101"/>
        <updated>2021-07-21T02:01:35.427Z</updated>
        <summary type="html"><![CDATA[Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount
of interest in the past few decades, while one of the most critical operations
in these systems is the perception of the environment. Deep learning and,
especially, the use of Deep Neural Networks (DNNs) provides impressive results
in analyzing and understanding complex and dynamic scenes from visual data. The
prediction horizons for those perception systems are very short and inference
must often be performed in real time, stressing the need of transforming the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Our goal in this work is to
investigate best practices for appropriately applying novel weight sharing
techniques, optimizing the available variables and the training procedures
towards the significant acceleration of widely adopted DNNs. Extensive
evaluation studies carried out using various state-of-the-art DNN models in
object detection and tracking experiments, provide details about the type of
errors that manifest after the application of weight sharing techniques,
resulting in significant acceleration gains with negligible accuracy losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1"&gt;Stavros Nousias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1"&gt;Erion-Vasilis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1"&gt;Christos Mavrokefalidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S. Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09170</id>
        <link href="http://arxiv.org/abs/2107.09170"/>
        <updated>2021-07-21T02:01:35.400Z</updated>
        <summary type="html"><![CDATA[Current datasets to train social behaviors are usually borrowed from
surveillance applications that capture visual data from a bird's-eye
perspective. This leaves aside precious relationships and visual cues that
could be captured through a first-person view of a scene. In this work, we
propose a strategy to exploit the power of current game engines, such as Unity,
to transform pre-existing bird's-eye view datasets into a first-person view, in
particular, a depth view. Using this strategy, we are able to generate large
volumes of synthetic data that can be used to pre-train a social navigation
model. To test our ideas, we present DeepSocNav, a deep learning based model
that takes advantage of the proposed approach to generate synthetic data.
Furthermore, DeepSocNav includes a self-supervised strategy that is included as
an auxiliary task. This consists of predicting the next depth frame that the
agent will face. Our experiments show the benefits of the proposed model that
is able to outperform relevant baselines in terms of social navigation scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1"&gt;Juan Pablo de Vicente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1"&gt;Alvaro Soto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Learning Quantum Circuits. (arXiv:2107.09078v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.09078</id>
        <link href="http://arxiv.org/abs/2107.09078"/>
        <updated>2021-07-21T02:01:35.387Z</updated>
        <summary type="html"><![CDATA[Quantum computers hold unprecedented potentials for machine learning
applications. Here, we prove that physical quantum circuits are PAC (probably
approximately correct) learnable on a quantum computer via empirical risk
minimization: to learn a quantum circuit with at most $n^c$ gates and each gate
acting on a constant number of qubits, the sample complexity is bounded by
$\tilde{O}(n^{c+1})$. In particular, we explicitly construct a family of
variational quantum circuits with $O(n^{c+1})$ elementary gates arranged in a
fixed pattern, which can represent all physical quantum circuits consisting of
at most $n^c$ elementary gates. Our results provide a valuable guide for
quantum machine learning in both theory and experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cai_H/0/1/0/all/0/1"&gt;Haoyuan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1"&gt;Dong-Ling Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OnlineSTL: Scaling Time Series Decomposition by 100x. (arXiv:2107.09110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09110</id>
        <link href="http://arxiv.org/abs/2107.09110"/>
        <updated>2021-07-21T02:01:35.365Z</updated>
        <summary type="html"><![CDATA[Decomposing a complex time series into trend, seasonality, and remainder
components is an important primitive that facilitates time series anomaly
detection, change point detection and forecasting. Although numerous batch
algorithms are known for time series decomposition, none operate well in an
online scalable setting where high throughput and real-time response are
paramount. In this paper, we propose OnlineSTL, a novel online algorithm for
time series decomposition which solves the scalability problem and is deployed
for real-time metrics monitoring on high resolution, high ingest rate data.
Experiments on different synthetic and real world time series datasets
demonstrate that OnlineSTL achieves orders of magnitude speedups while
maintaining quality of decomposition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Abhinav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sriharsha_R/0/1/0/all/0/1"&gt;Ram Sriharsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1"&gt;Sichen Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving exploration in policy gradient search: Application to symbolic optimization. (arXiv:2107.09158v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09158</id>
        <link href="http://arxiv.org/abs/2107.09158"/>
        <updated>2021-07-21T02:01:35.334Z</updated>
        <summary type="html"><![CDATA[Many machine learning strategies designed to automate mathematical tasks
leverage neural networks to search large combinatorial spaces of mathematical
symbols. In contrast to traditional evolutionary approaches, using a neural
network at the core of the search allows learning higher-level symbolic
patterns, providing an informed direction to guide the search. When no labeled
data is available, such networks can still be trained using reinforcement
learning. However, we demonstrate that this approach can suffer from an early
commitment phenomenon and from initialization bias, both of which limit
exploration. We present two exploration methods to tackle these issues,
building upon ideas of entropy regularization and distribution initialization.
We show that these techniques can improve the performance, increase sample
efficiency, and lower the complexity of solutions for the task of symbolic
regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1"&gt;Mikel Landajuela Larma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1"&gt;Brenden K. Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soo K. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1"&gt;Claudio P. Santiago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glatt_R/0/1/0/all/0/1"&gt;Ruben Glatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundhenk_T/0/1/0/all/0/1"&gt;T. Nathan Mundhenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pettit_J/0/1/0/all/0/1"&gt;Jacob F. Pettit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faissol_D/0/1/0/all/0/1"&gt;Daniel M. Faissol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI in Finance: Challenges, Techniques and Opportunities. (arXiv:2107.09051v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2107.09051</id>
        <link href="http://arxiv.org/abs/2107.09051"/>
        <updated>2021-07-21T02:01:35.316Z</updated>
        <summary type="html"><![CDATA[AI in finance broadly refers to the applications of AI techniques in
financial businesses. This area has been lasting for decades with both classic
and modern AI techniques applied to increasingly broader areas of finance,
economy and society. In contrast to either discussing the problems, aspects and
opportunities of finance that have benefited from specific AI techniques and in
particular some new-generation AI and data science (AIDS) areas or reviewing
the progress of applying specific techniques to resolving certain financial
problems, this review offers a comprehensive and dense roadmap of the
overwhelming challenges, techniques and opportunities of AI research in finance
over the past decades. The landscapes and challenges of financial businesses
and data are firstly outlined, followed by a comprehensive categorization and a
dense overview of the decades of AI research in finance. We then structure and
illustrate the data-driven analytics and learning of financial businesses and
data. The comparison, criticism and discussion of classic vs. modern AI
techniques for finance are followed. Lastly, open issues and opportunities
address future AI-empowered finance and finance-motivated AI research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latency-Memory Optimized Splitting of Convolution Neural Networks for Resource Constrained Edge Devices. (arXiv:2107.09123v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09123</id>
        <link href="http://arxiv.org/abs/2107.09123"/>
        <updated>2021-07-21T02:01:35.299Z</updated>
        <summary type="html"><![CDATA[With the increasing reliance of users on smart devices, bringing essential
computation at the edge has become a crucial requirement for any type of
business. Many such computations utilize Convolution Neural Networks (CNNs) to
perform AI tasks, having high resource and computation requirements, that are
infeasible for edge devices. Splitting the CNN architecture to perform part of
the computation on edge and remaining on the cloud is an area of research that
has seen increasing interest in the field. In this paper, we assert that
running CNNs between an edge device and the cloud is synonymous to solving a
resource-constrained optimization problem that minimizes the latency and
maximizes resource utilization at the edge. We formulate a multi-objective
optimization problem and propose the LMOS algorithm to achieve a Pareto
efficient solution. Experiments done on real-world edge devices show that, LMOS
ensures feasible execution of different CNN models at the edge and also
improves upon existing state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1"&gt;Tanmay Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avaneesh/0/1/0/all/0/1"&gt;Avaneesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1"&gt;Rohit Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shorey_R/0/1/0/all/0/1"&gt;Rajeev Shorey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer. (arXiv:2104.10567v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10567</id>
        <link href="http://arxiv.org/abs/2104.10567"/>
        <updated>2021-07-21T02:01:35.279Z</updated>
        <summary type="html"><![CDATA[In recent years, virtual makeup applications have become more and more
popular. However, it is still challenging to propose a robust makeup transfer
method in the real-world environment. Current makeup transfer methods mostly
work well on good-conditioned clean makeup images, but transferring makeup that
exhibits shadow and occlusion is not satisfying. To alleviate it, we propose a
novel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN
(SOGAN). Given the source and the reference faces, we first fit a 3D face model
and then disentangle the faces into shape and texture. In the texture branch,
we map the texture to the UV space and design a UV texture generator to
transfer the makeup. Since human faces are symmetrical in the UV space, we can
conveniently remove the undesired shadow and occlusion from the reference image
by carefully designing a Flip Attention Module (FAM). After obtaining cleaner
makeup features from the reference image, a Makeup Transfer Module (MTM) is
introduced to perform accurate makeup transfer. The qualitative and
quantitative experiments demonstrate that our SOGAN not only achieves superior
results in shadow and occlusion situations but also performs well in large pose
and expression variations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yueming Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Bo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09118</id>
        <link href="http://arxiv.org/abs/2107.09118"/>
        <updated>2021-07-21T02:01:35.272Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) models have received particular attention in medical
imaging due to their promising pattern recognition capabilities. However, Deep
Neural Networks (DNNs) require a huge amount of data, and because of the lack
of sufficient data in this field, transfer learning can be a great solution.
DNNs used for disease diagnosis meticulously concentrate on improving the
accuracy of predictions without providing a figure about their confidence of
predictions. Knowing how much a DNN model is confident in a computer-aided
diagnosis model is necessary for gaining clinicians' confidence and trust in
DL-based solutions. To address this issue, this work presents three different
methods for quantifying uncertainties for skin cancer detection from images. It
also comprehensively evaluates and compares performance of these DNNs using
novel uncertainty-related metrics. The obtained results reveal that the
predictive uncertainty estimation methods are capable of flagging risky and
erroneous predictions with a high uncertainty estimate. We also demonstrate
that ensemble approaches are more reliable in capturing uncertainties through
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1"&gt;Donya Khaledyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1"&gt;AmirReza Tajally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1"&gt;Reza Sarkhosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1"&gt;Afshar Shamsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1"&gt;Hamzeh Asgharnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Semantic Segmentation in Cityscape via Hyperspectral Image. (arXiv:2012.10122v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10122</id>
        <link href="http://arxiv.org/abs/2012.10122"/>
        <updated>2021-07-21T02:01:35.265Z</updated>
        <summary type="html"><![CDATA[High-resolution hyperspectral images (HSIs) contain the response of each
pixel in different spectral bands, which can be used to effectively distinguish
various objects in complex scenes. While HSI cameras have become low cost,
algorithms based on it have not been well exploited. In this paper, we focus on
a novel topic, weakly-supervised semantic segmentation in cityscape via HSIs.
It is based on the idea that high-resolution HSIs in city scenes contain rich
spectral information, which can be easily associated to semantics without
manual labeling. Therefore, it enables low cost, highly reliable semantic
segmentation in complex scenes. Specifically, in this paper, we theoretically
analyze the HSIs and introduce a weakly-supervised HSI semantic segmentation
framework, which utilizes spectral information to improve the coarse labels to
a finer degree. The experimental results show that our method can obtain highly
competitive labels and even have higher edge fineness than artificial fine
labels in some classes. At the same time, the results also show that the
refined labels can effectively improve the effect of semantic segmentation. The
combination of HSIs and semantic segmentation proves that HSIs have great
potential in high-level visual tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shaodi You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1"&gt;Qiu Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Facial Affect Analysis: A Review of Methods, Applications and Challenges. (arXiv:2103.15599v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15599</id>
        <link href="http://arxiv.org/abs/2103.15599"/>
        <updated>2021-07-21T02:01:35.259Z</updated>
        <summary type="html"><![CDATA[Facial affect analysis (FAA) using visual signals is important in
human-computer interaction. Early methods focus on extracting appearance and
geometry features associated with human affects, while ignoring the latent
semantic information among individual facial changes, leading to limited
performance and generalization. Recent work attempts to establish a graph-based
representation to model these semantic relationships and develop frameworks to
leverage them for various FAA tasks. In this paper, we provide a comprehensive
review of graph-based FAA, including the evolution of algorithms and their
applications. First, the FAA background knowledge is introduced, especially on
the role of the graph. We then discuss approaches that are widely used for
graph-based affective representation in literature and show a trend towards
graph construction. For the relational reasoning in graph-based FAA, existing
studies are categorized according to their usage of traditional methods or deep
models, with a special emphasis on the latest graph neural networks.
Performance comparisons of the state-of-the-art graph-based FAA methods are
also summarized. Finally, we discuss the challenges and potential directions.
As far as we know, this is the first survey of graph-based FAA methods. Our
findings can serve as a reference for future research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinzhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yante Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guoying Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dim but not entirely dark: Extracting the Galactic Center Excess' source-count distribution with neural nets. (arXiv:2107.09070v1 [astro-ph.HE])]]></title>
        <id>http://arxiv.org/abs/2107.09070</id>
        <link href="http://arxiv.org/abs/2107.09070"/>
        <updated>2021-07-21T02:01:35.252Z</updated>
        <summary type="html"><![CDATA[The two leading hypotheses for the Galactic Center Excess (GCE) in the
$\textit{Fermi}$ data are an unresolved population of faint millisecond pulsars
(MSPs) and dark-matter (DM) annihilation. The dichotomy between these
explanations is typically reflected by modeling them as two separate emission
components. However, point-sources (PSs) such as MSPs become statistically
degenerate with smooth Poisson emission in the ultra-faint limit (formally
where each source is expected to contribute much less than one photon on
average), leading to an ambiguity that can render questions such as whether the
emission is PS-like or Poissonian in nature ill-defined. We present a
conceptually new approach that describes the PS and Poisson emission in a
unified manner and only afterwards derives constraints on the Poissonian
component from the so obtained results. For the implementation of this
approach, we leverage deep learning techniques, centered around a neural
network-based method for histogram regression that expresses uncertainties in
terms of quantiles. We demonstrate that our method is robust against a number
of systematics that have plagued previous approaches, in particular DM / PS
misattribution. In the $\textit{Fermi}$ data, we find a faint GCE described by
a median source-count distribution (SCD) peaked at a flux of $\sim4 \times
10^{-11} \ \text{counts} \ \text{cm}^{-2} \ \text{s}^{-1}$ (corresponding to
$\sim3 - 4$ expected counts per PS), which would require $N \sim
\mathcal{O}(10^4)$ sources to explain the entire excess (median value $N =
\text{29,300}$ across the sky). Although faint, this SCD allows us to derive
the constraint $\eta_P \leq 66\%$ for the Poissonian fraction of the GCE flux
$\eta_P$ at 95% confidence, suggesting that a substantial amount of the GCE
flux is due to PSs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+List_F/0/1/0/all/0/1"&gt;Florian List&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rodd_N/0/1/0/all/0/1"&gt;Nicholas L. Rodd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lewis_G/0/1/0/all/0/1"&gt;Geraint F. Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-07-21T02:01:35.229Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards in
silico experimentation, is to synthesise the imagery itself. Here, we propose
Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence
microscopy imagery of living cells, based on a past experiment. This novel
generative adversarial network synthesises a multi-domain sequence of
consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live
yeast cells in microstructured environments and train on a dataset recorded in
our laboratory. The simulation captures underlying biophysical factors and time
dependencies, such as cell morphology, growth, physical interactions, as well
as the intensity of a fluorescent reporter protein. An immediate application is
to generate additional training and validation data for feature extraction
algorithms or to aid and expedite development of advanced experimental
techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-07-21T02:01:35.220Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1"&gt;Johannes Otterbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstruction of the Density Power Spectrum from Quasar Spectra using Machine Learning. (arXiv:2107.09082v1 [astro-ph.CO])]]></title>
        <id>http://arxiv.org/abs/2107.09082</id>
        <link href="http://arxiv.org/abs/2107.09082"/>
        <updated>2021-07-21T02:01:35.213Z</updated>
        <summary type="html"><![CDATA[We describe a novel end-to-end approach using Machine Learning to reconstruct
the power spectrum of cosmological density perturbations at high redshift from
observed quasar spectra. State-of-the-art cosmological simulations of structure
formation are used to generate a large synthetic dataset of line-of-sight
absorption spectra paired with 1-dimensional fluid quantities along the same
line-of-sight, such as the total density of matter and the density of neutral
atomic hydrogen. With this dataset, we build a series of data-driven models to
predict the power spectrum of total matter density. We are able to produce
models which yield reconstruction to accuracy of about 1% for wavelengths $k
\leq 2 h Mpc^{-1}$, while the error increases at larger $k$. We show the size
of data sample required to reach a particular error rate, giving a sense of how
much data is necessary to reach a desired accuracy. This work provides a
foundation for developing methods to analyse very large upcoming datasets with
the next-generation observational facilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Veiga_M/0/1/0/all/0/1"&gt;Maria Han Veiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gnedin_O/0/1/0/all/0/1"&gt;Oleg Y. Gnedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gnedin_N/0/1/0/all/0/1"&gt;Nickolay Y. Gnedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Huan_X/0/1/0/all/0/1"&gt;Xun Huan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02924</id>
        <link href="http://arxiv.org/abs/2012.02924"/>
        <updated>2021-07-21T02:01:35.206Z</updated>
        <summary type="html"><![CDATA[We present iGibson 1.0, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson 1.0 integrates several key features to facilitate
the study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson 1.0
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson 1.0 is open-source, equipped with comprehensive examples and
documentation. For more information, visit our project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1"&gt;Claudia P&amp;#xe9;rez-D&amp;#x27;Arpino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1"&gt;Lyne P. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael E. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation. (arXiv:2101.09321v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09321</id>
        <link href="http://arxiv.org/abs/2101.09321"/>
        <updated>2021-07-21T02:01:35.199Z</updated>
        <summary type="html"><![CDATA[Deep learning techniques for 3D brain vessel image segmentation have not been
as successful as in the segmentation of other organs and tissues. This can be
explained by two factors. First, deep learning techniques tend to show poor
performances at the segmentation of relatively small objects compared to the
size of the full image. Second, due to the complexity of vascular trees and the
small size of vessels, it is challenging to obtain the amount of annotated
training data typically needed by deep learning methods. To address these
problems, we propose a novel annotation-efficient deep learning vessel
segmentation framework. The framework avoids pixel-wise annotations, only
requiring weak patch-level labels to discriminate between vessel and non-vessel
2D patches in the training set, in a setup similar to the CAPTCHAs used to
differentiate humans from bots in web applications. The user-provided weak
annotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels
for vessels and background in each patch, which are used to train a
segmentation network, and 2) to train a classifier network. The classifier
network allows to generate additional weak patch labels, further reducing the
annotation burden, and it acts as a noise filter for poor quality images. We
use this framework for the segmentation of the cerebrovascular tree in
Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The
results show that the framework achieves state-of-the-art accuracy, while
reducing the annotation time by ~77% w.r.t. learning-based segmentation methods
using pixel-wise labels for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_V/0/1/0/all/0/1"&gt;Vien Ngoc Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galati_F/0/1/0/all/0/1"&gt;Francesco Galati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cortese_R/0/1/0/all/0/1"&gt;Rosa Cortese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1"&gt;Giuseppe Di Giacomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marconetto_V/0/1/0/all/0/1"&gt;Viola Marconetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1"&gt;Prateek Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prados_F/0/1/0/all/0/1"&gt;Ferran Prados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuluaga_M/0/1/0/all/0/1"&gt;Maria A. Zuluaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPNNet-2.0 Part I: Finding hidden planets from simulated images of protoplanetary disk gaps. (arXiv:2107.09086v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2107.09086</id>
        <link href="http://arxiv.org/abs/2107.09086"/>
        <updated>2021-07-21T02:01:35.180Z</updated>
        <summary type="html"><![CDATA[The observed sub-structures, like annular gaps, in dust emissions from
protoplanetary disk, are often interpreted as signatures of embedded planets.
Fitting a model of planetary gaps to these observed features using customized
simulations or empirical relations can reveal the characteristics of the hidden
planets. However, customized fitting is often impractical owing to the
increasing sample size and the complexity of disk-planet interaction. In this
paper we introduce the architecture of DPNNet-2.0, second in the series after
DPNNet \citep{aud20}, designed using a Convolutional Neural Network ( CNN, here
specifically ResNet50) for predicting exoplanet masses directly from simulated
images of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally
consists of a multi-input framework that uses both a CNN and multi-layer
perceptron (a class of artificial neural network) for processing image and disk
parameters simultaneously. This enables DPNNet-2.0 to be trained using images
directly, with the added option of considering disk parameters (disk
viscosities, disk temperatures, disk surface density profiles, dust abundances,
and particle Stokes numbers) generated from disk-planet hydrodynamic
simulations as inputs. This work provides the required framework and is the
first step towards the use of computer vision (implementing CNN) to directly
extract mass of an exoplanet from planetary gaps observed in dust-surface
density maps by telescopes such as the Atacama Large (sub-)Millimeter Array.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Auddy_S/0/1/0/all/0/1"&gt;Sayantan Auddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dey_R/0/1/0/all/0/1"&gt;Ramit Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lin_M/0/1/0/all/0/1"&gt;Min-Kai Lin&lt;/a&gt; (ASIAA, NCTS Physics Division), &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hall_C/0/1/0/all/0/1"&gt;Cassandra Hall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active 3D Shape Reconstruction from Vision and Touch. (arXiv:2107.09584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09584</id>
        <link href="http://arxiv.org/abs/2107.09584"/>
        <updated>2021-07-21T02:01:35.174Z</updated>
        <summary type="html"><![CDATA[Humans build 3D understandings of the world through active object
exploration, using jointly their senses of vision and touch. However, in 3D
shape reconstruction, most recent progress has relied on static datasets of
limited sensory data such as RGB images, depth maps or haptic readings, leaving
the active exploration of the shape largely unexplored. In active touch sensing
for 3D reconstruction, the goal is to actively select the tactile readings that
maximize the improvement in shape reconstruction accuracy. However, the
development of deep learning-based active touch models is largely limited by
the lack of frameworks for shape exploration. In this paper, we focus on this
problem and introduce a system composed of: 1) a haptic simulator leveraging
high spatial resolution vision-based tactile sensors for active touching of 3D
objects; 2) a mesh-based 3D shape reconstruction model that relies on tactile
or visuotactile signals; and 3) a set of data-driven solutions with either
tactile or visuotactile priors to guide the shape exploration. Our framework
enables the development of the first fully data-driven solutions to active
touch on top of learned models for object understanding. Our experiments show
the benefits of such solutions in the task of 3D shape understanding where our
models consistently outperform natural baselines. We provide our framework as a
tool to foster future research in this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1"&gt;Edward J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1"&gt;David Meger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1"&gt;Roberto Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1"&gt;Adriana Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1"&gt;Michal Drozdzal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09957</id>
        <link href="http://arxiv.org/abs/2103.09957"/>
        <updated>2021-07-21T02:01:35.167Z</updated>
        <summary type="html"><![CDATA[A major obstacle to the integration of deep learning models for chest x-ray
interpretation into clinical settings is the lack of understanding of their
failure modes. In this work, we first investigate whether there are patient
subgroups that chest x-ray models are likely to misclassify. We find that
patient age and the radiographic finding of lung lesion, pneumothorax or
support devices are statistically relevant features for predicting
misclassification for some chest x-ray models. Second, we develop
misclassification predictors on chest x-ray models using their outputs and
clinical features. We find that our best performing misclassification
identifier achieves an AUROC close to 0.9 for most diseases. Third, employing
our misclassification identifiers, we develop a corrective algorithm to
selectively flip model predictions that have high likelihood of
misclassification at inference time. We observe F1 improvement on the
prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,
[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and
high-performing chest x-ray models, we are able to derive insights across model
architectures and offer a generalizable framework applicable to other medical
imaging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Emma Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Andy Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1"&gt;Rayan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantification of Carbon Sequestration in Urban Forests. (arXiv:2106.00182v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00182</id>
        <link href="http://arxiv.org/abs/2106.00182"/>
        <updated>2021-07-21T02:01:35.159Z</updated>
        <summary type="html"><![CDATA[Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide
from the atmosphere. However, the lack of efficient quantification methods of
carbon stored in trees renders it difficult to track the process. We present an
approach to estimate the carbon storage in trees based on fusing multi-spectral
aerial imagery and LiDAR data to identify tree coverage, geometric shape, and
tree species -- key attributes to carbon storage quantification. We demonstrate
that tree species information and their three-dimensional geometric shapes can
be estimated from aerial imagery in order to determine the tree's biomass.
Specifically, we estimate a total of $52,000$ tons of carbon sequestered in
trees for New York City's borough Manhattan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1"&gt;Levente J. Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1"&gt;Conrad M. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05108</id>
        <link href="http://arxiv.org/abs/2103.05108"/>
        <updated>2021-07-21T02:01:35.152Z</updated>
        <summary type="html"><![CDATA[Understanding the predictions made by Artificial Intelligence (AI) systems is
becoming more and more important as deep learning models are used for
increasingly complex and high-stakes tasks. Saliency mapping - an easily
interpretable visual attribution method - is one important tool for this, but
existing formulations are limited by either computational cost or architectural
constraints. We therefore propose Hierarchical Perturbation, a very fast and
completely model-agnostic method for explaining model predictions with robust
saliency maps. Using standard benchmarks and datasets, we show that our
saliency maps are of competitive or superior quality to those generated by
existing model-agnostic methods - and are over 20X faster to compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Efficient Scene Understanding via Squeeze Reasoning. (arXiv:2011.03308v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03308</id>
        <link href="http://arxiv.org/abs/2011.03308"/>
        <updated>2021-07-21T02:01:35.132Z</updated>
        <summary type="html"><![CDATA[Graph-based convolutional model such as non-local block has shown to be
effective for strengthening the context modeling ability in convolutional
neural networks (CNNs). However, its pixel-wise computational overhead is
prohibitive which renders it unsuitable for high resolution imagery. In this
paper, we explore the efficiency of context graph reasoning and propose a novel
framework called Squeeze Reasoning. Instead of propagating information on the
spatial map, we first learn to squeeze the input feature into a channel-wise
global vector and perform reasoning within the single vector where the
computation cost can be significantly reduced. Specifically, we build the node
graph in the vector where each node represents an abstract semantic concept.
The refined feature within the same semantic category results to be consistent,
which is thus beneficial for downstream tasks. We show that our approach can be
modularized as an end-to-end trained block and can be easily plugged into
existing networks. {Despite its simplicity and being lightweight, the proposed
strategy allows us to establish the considerable results on different semantic
segmentation datasets and shows significant improvements with respect to strong
baselines on various other scene understanding tasks including object
detection, instance segmentation and panoptic segmentation.} Code is available
at \url{https://github.com/lxtGH/SFSegNets}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1"&gt;Ansheng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kuiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.09055</id>
        <link href="http://arxiv.org/abs/2107.09055"/>
        <updated>2021-07-21T02:01:35.125Z</updated>
        <summary type="html"><![CDATA[The stock market has been a popular topic of interest in the recent past. The
growth in the inflation rate has compelled people to invest in the stock and
commodity markets and other areas rather than saving. Further, the ability of
Deep Learning models to make predictions on the time series data has been
proven time and again. Technical analysis on the stock market with the help of
technical indicators has been the most common practice among traders and
investors. One more aspect is the sentiment analysis - the emotion of the
investors that shows the willingness to invest. A variety of techniques have
been used by people around the globe involving basic Machine Learning and
Neural Networks. Ranging from the basic linear regression to the advanced
neural networks people have experimented with all possible techniques to
predict the stock market. It's evident from recent events how news and
headlines affect the stock markets and cryptocurrencies. This paper proposes an
ensemble of state-of-the-art methods for predicting stock prices. Firstly
sentiment analysis of the news and the headlines for the company Apple Inc,
listed on the NASDAQ is performed using a version of BERT, which is a
pre-trained transformer model by Google for Natural Language Processing (NLP).
Afterward, a Generative Adversarial Network (GAN) predicts the stock price for
Apple Inc using the technical indicators, stock indexes of various countries,
some commodities, and historical prices along with the sentiment scores.
Comparison is done with baseline models like - Long Short Term Memory (LSTM),
Gated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving
Average (ARIMA) model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1"&gt;Priyank Sonkiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1"&gt;Vikas Bajpai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Anukriti Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisQA: X-raying Vision and Language Reasoning in Transformers. (arXiv:2104.00926v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00926</id>
        <link href="http://arxiv.org/abs/2104.00926"/>
        <updated>2021-07-21T02:01:35.119Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering systems target answering open-ended textual
questions given input images. They are a testbed for learning high-level
reasoning with a primary use in HCI, for instance assistance for the visually
impaired. Recent research has shown that state-of-the-art models tend to
produce answers exploiting biases and shortcuts in the training data, and
sometimes do not even look at the input image, instead of performing the
required reasoning steps. We present VisQA, a visual analytics tool that
explores this question of reasoning vs. bias exploitation. It exposes the key
element of state-of-the-art neural models -- attention maps in transformers.
Our working hypothesis is that reasoning steps leading to model predictions are
observable from attention distributions, which are particularly useful for
visualization. The design process of VisQA was motivated by well-known bias
examples from the fields of deep learning and vision-language reasoning and
evaluated in two ways. First, as a result of a collaboration of three fields,
machine learning, vision and language reasoning, and data analytics, the work
lead to a better understanding of bias exploitation of neural models for VQA,
which eventually resulted in an impact on its design and training through the
proposition of a method for the transfer of reasoning patterns from an oracle
model. Second, we also report on the design of VisQA, and a goal-oriented
evaluation of VisQA targeting the analysis of a model decision process from
multiple experts, providing evidence that it makes the inner workings of models
accessible to users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaunet_T/0/1/0/all/0/1"&gt;Theo Jaunet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1"&gt;Corentin Kervadec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuillemot_R/0/1/0/all/0/1"&gt;Romain Vuillemot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antipov_G/0/1/0/all/0/1"&gt;Grigory Antipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baccouche_M/0/1/0/all/0/1"&gt;Moez Baccouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1"&gt;Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks. (arXiv:2007.00337v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2007.00337</id>
        <link href="http://arxiv.org/abs/2007.00337"/>
        <updated>2021-07-21T02:01:35.112Z</updated>
        <summary type="html"><![CDATA[Developing secure machine learning models from adversarial examples is
challenging as various methods are continually being developed to generate
adversarial attacks. In this work, we propose an evolutionary approach to
automatically determine Image Processing Techniques Sequence (IPTS) for
detecting malicious inputs. Accordingly, we first used a diverse set of attack
methods including adaptive attack methods (on our defense) to generate
adversarial samples from the clean dataset. A detection framework based on a
genetic algorithm (GA) is developed to find the optimal IPTS, where the
optimality is estimated by different fitness measures such as Euclidean
distance, entropy loss, average histogram, local binary pattern and loss
functions. The "image difference" between the original and processed images is
used to extract the features, which are then fed to a classification scheme in
order to determine whether the input sample is adversarial or clean. This paper
described our methodology and performed experiments using multiple data-sets
tested with several adversarial attacks. For each attack-type and dataset, it
generates unique IPTS. A set of IPTS selected dynamically in testing time which
works as a filter for the adversarial attack. Our empirical experiments
exhibited promising results indicating the approach can efficiently be used as
processing for any AI model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kishor Datta Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1"&gt;Zahid Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_D/0/1/0/all/0/1"&gt;Dipankar Dasgupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09305</id>
        <link href="http://arxiv.org/abs/2107.09305"/>
        <updated>2021-07-21T02:01:35.104Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often have a huge number of parameters, which posts
challenges in deployment in application scenarios with limited memory and
computation capacity. Knowledge distillation is one approach to derive compact
models from bigger ones. However, it has been observed that a converged heavy
teacher model is strongly constrained for learning a compact student network
and could make the optimization subject to poor local optima. In this paper, we
propose ProKT, a new model-agnostic method by projecting the supervision
signals of a teacher model into the student's parameter space. Such projection
is implemented by decomposing the training objective into local intermediate
targets with an approximate mirror descent technique. The proposed method could
be less sensitive with the quirks during optimization which could result in a
better local optimum. Experiments on both image and text datasets show that our
proposed ProKT consistently achieves superior performance compared to other
existing knowledge distillation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Wenxian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuxuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bohan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13677</id>
        <link href="http://arxiv.org/abs/2010.13677"/>
        <updated>2021-07-21T02:01:35.085Z</updated>
        <summary type="html"><![CDATA[In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)
decomposition, or robust principal component analysis (PCA), has achieved
stunning performance. However, the selection of the parameters of L+S is
empirical, and the acceleration rate is limited, which are common failings of
iterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many
deep learning approaches have been proposed to address these issues, but few of
them use a low-rank prior. In this paper, a model-based low-rank plus sparse
network, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In
particular, we use an alternating linearized minimization method to solve the
optimization problem with low-rank and sparse regularization. Learned soft
singular value thresholding is introduced to ensure the clear separation of the
L component and S component. Then, the iterative steps are unrolled into a
network in which the regularization parameters are learnable. We prove that the
proposed L+S-Net achieves global convergence under two standard assumptions.
Experiments on retrospective and prospective cardiac cine datasets show that
the proposed model outperforms state-of-the-art CS and existing deep learning
methods and has great potential for extremely high acceleration factors (up to
24x).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1"&gt;Ziwen Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhuo-Xu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Zhilang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1"&gt;Leslie Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dong Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09562</id>
        <link href="http://arxiv.org/abs/2107.09562"/>
        <updated>2021-07-21T02:01:35.078Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) aims to find representations suitable for
zero-shot transfer to a priori unknown test distributions. However, common
evaluation protocols only test a single, fixed data split in which train and
test classes are assigned randomly. More realistic evaluations should consider
a broad spectrum of distribution shifts with potentially varying degree and
difficulty. In this work, we systematically construct train-test splits of
increasing difficulty and present the ooDML benchmark to characterize
generalization under out-of-distribution shifts in DML. ooDML is designed to
probe the generalization performance on much more challenging, diverse
train-to-test distribution shifts. Based on our new benchmark, we conduct a
thorough empirical analysis of state-of-the-art DML methods. We find that while
generalization tends to consistently degrade with difficulty, some methods are
better at retaining performance as the distribution shift increases. Finally,
we propose few-shot DML as an efficient way to consistently improve
generalization in response to unknown test shifts presented in ooDML. Code
available here:
https://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Samarth Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Visual Navigation. (arXiv:2107.01151v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01151</id>
        <link href="http://arxiv.org/abs/2107.01151"/>
        <updated>2021-07-21T02:01:35.071Z</updated>
        <summary type="html"><![CDATA[As a fundamental problem for Artificial Intelligence, multi-agent system
(MAS) is making rapid progress, mainly driven by multi-agent reinforcement
learning (MARL) techniques. However, previous MARL methods largely focused on
grid-world like or game environments; MAS in visually rich environments has
remained less explored. To narrow this gap and emphasize the crucial role of
perception in MAS, we propose a large-scale 3D dataset, CollaVN, for
multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed
to cooperatively navigate across photo-realistic environments to reach target
locations. Diverse MAVN variants are explored to make our problem more general.
Moreover, a memory-augmented communication framework is proposed. Each agent is
equipped with a private, external memory to persistently store communication
information. This allows agents to make better use of their past communication
information, enabling more efficient collaboration and robust long-term
planning. In our experiments, several baselines and evaluation metrics are
designed. We also empirically verify the efficacy of our proposed MARL approach
across different MAVN task settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenguan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor Pruning for Object Detection. (arXiv:2104.00432v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00432</id>
        <link href="http://arxiv.org/abs/2104.00432"/>
        <updated>2021-07-21T02:01:35.063Z</updated>
        <summary type="html"><![CDATA[This paper proposes anchor pruning for object detection in one-stage
anchor-based detectors. While pruning techniques are widely used to reduce the
computational cost of convolutional neural networks, they tend to focus on
optimizing the backbone networks where often most computations are. In this
work we demonstrate an additional pruning technique, specifically for object
detection: anchor pruning. With more efficient backbone networks and a growing
trend of deploying object detectors on embedded systems where post-processing
steps such as non-maximum suppression can be a bottleneck, the impact of the
anchors used in the detection head is becoming increasingly more important. In
this work, we show that many anchors in the object detection head can be
removed without any loss in accuracy. With additional retraining, anchor
pruning can even lead to improved accuracy. Extensive experiments on SSD and MS
COCO show that the detection head can be made up to 44% more efficient while
simultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL
VOC show the general effectiveness of our approach. We also introduce
`overanchorized' models that can be used together with anchor pruning to
eliminate hyperparameters related to the initial shape of anchors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonnaerens_M/0/1/0/all/0/1"&gt;Maxim Bonnaerens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1"&gt;Matthias Freiberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1"&gt;Joni Dambre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00329</id>
        <link href="http://arxiv.org/abs/2106.00329"/>
        <updated>2021-07-21T02:01:35.054Z</updated>
        <summary type="html"><![CDATA[Rigid registration of partial observations is a fundamental problem in
various applied fields. In computer graphics, special attention has been given
to the registration between two partial point clouds generated by scanning
devices. State-of-the-art registration techniques still struggle when the
overlap region between the two point clouds is small, and completely fail if
there is no overlap between the scan pairs. In this paper, we present a
learning-based technique that alleviates this problem, and allows registration
between point clouds, presented in arbitrary poses, and having little or even
no overlap, a setting that has been referred to as tele-registration. Our
technique is based on a novel neural network design that learns a prior of a
class of shapes and can complete a partial shape. The key idea is combining the
registration and completion tasks in a way that reinforces each other. In
particular, we simultaneously train the registration network and completion
network using two coupled flows, one that register-and-complete, and one that
complete-and-register, and encourage the two flows to produce a consistent
result. We show that, compared with each separate flow, this two-flow training
leads to robust and reliable tele-registration, and hence to a better point
cloud prediction that completes the registered scans. It is also worth
mentioning that each of the components in our neural network outperforms
state-of-the-art methods in both completion and registration. We further
analyze our network with several ablation studies and demonstrate its
performance on a large number of partial point clouds, both synthetic and
real-world, that have only small or no overlap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zihao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1"&gt;Zimu Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ruizhen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hui Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception. (arXiv:2103.02850v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02850</id>
        <link href="http://arxiv.org/abs/2103.02850"/>
        <updated>2021-07-21T02:01:35.036Z</updated>
        <summary type="html"><![CDATA[Distortion quantification of point clouds plays a stealth, yet vital role in
a wide range of human and machine perception tasks. For human perception tasks,
a distortion quantification can substitute subjective experiments to guide 3D
visualization; while for machine perception tasks, a distortion quantification
can work as a loss function to guide the training of deep neural networks for
unsupervised learning tasks. To handle a variety of demands in many
applications, a distortion quantification needs to be distortion discriminable,
differentiable, and have a low computational complexity. Currently, however,
there is a lack of a general distortion quantification that can satisfy all
three conditions. To fill this gap, this work proposes multiscale potential
energy discrepancy (MPED), a distortion quantification to measure point cloud
geometry and color difference. By evaluating at various neighborhood sizes, the
proposed MPED achieves global-local tradeoffs, capturing distortion in a
multiscale fashion. Extensive experimental studies validate MPED's superiority
for both human and machine perception tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yiling Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;M. Salman Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhan Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09600</id>
        <link href="http://arxiv.org/abs/2107.09600"/>
        <updated>2021-07-21T02:01:35.029Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt
a segmentation model trained on the labeled source domain to the unlabeled
target domain. Existing methods try to learn domain invariant features while
suffering from large domain gaps that make it difficult to correctly align
discrepant features, especially in the initial training phase. To address this
issue, we propose a novel Dual Soft-Paste (DSP) method in this paper.
Specifically, DSP selects some classes from a source domain image using a
long-tail class first sampling strategy and softly pastes the corresponding
image patch on both the source and target training images with a fusion weight.
Technically, we adopt the mean teacher framework for domain adaptation, where
the pasted source and target images go through the student network while the
original target image goes through the teacher network. Output-level alignment
is carried out by aligning the probability maps of the target fused image from
both networks using a weighted cross-entropy loss. In addition, feature-level
alignment is carried out by aligning the feature maps of the source and target
images from student network using a weighted maximum mean discrepancy loss. DSP
facilitates the model learning domain-invariant features from the intermediate
domains, leading to faster convergence and better performance. Experiments on
two challenging benchmarks demonstrate the superiority of DSP over
state-of-the-art methods. Code is available at
\url{https://github.com/GaoLii/DSP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Li Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lefei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Fusion Transformer. (arXiv:2107.09011v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09011</id>
        <link href="http://arxiv.org/abs/2107.09011"/>
        <updated>2021-07-21T02:01:35.018Z</updated>
        <summary type="html"><![CDATA[In image fusion, images obtained from different sensors are fused to generate
a single image with enhanced information. In recent years, state-of-the-art
methods have adopted Convolution Neural Networks (CNNs) to encode meaningful
features for image fusion. Specifically, CNN-based methods perform image fusion
by fusing local features. However, they do not consider long-range dependencies
that are present in the image. Transformer-based models are designed to
overcome this by modeling the long-range dependencies with the help of
self-attention mechanism. This motivates us to propose a novel Image Fusion
Transformer (IFT) where we develop a transformer-based multi-scale fusion
strategy that attends to both local and long-range information (or global
context). The proposed method follows a two-stage training approach. In the
first stage, we train an auto-encoder to extract deep features at multiple
scales. In the second stage, multi-scale features are fused using a
Spatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of
a CNN and a transformer branch which capture local and long-range features,
respectively. Extensive experiments on multiple benchmark datasets show that
the proposed method performs better than many competitive fusion algorithms.
Furthermore, we show the effectiveness of the proposed ST fusion strategy with
an ablation analysis. The source code is available at:
https://github.com/Vibashan/Image-Fusion-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1"&gt;Vibashan VS&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1"&gt;Poojan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09602</id>
        <link href="http://arxiv.org/abs/2107.09602"/>
        <updated>2021-07-21T02:01:35.002Z</updated>
        <summary type="html"><![CDATA[The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of
lives and has affected all aspects of human life. This paper focuses on the
application of deep learning (DL) models to medical imaging and drug discovery
for managing COVID-19 disease. In this article, we detail various medical
imaging-based studies such as X-rays and computed tomography (CT) images along
with DL methods for classifying COVID-19 affected versus pneumonia. The
applications of DL techniques to medical images are further described in terms
of image localization, segmentation, registration, and classification leading
to COVID-19 detection. The reviews of recent papers indicate that the highest
classification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is
applied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients
and 365 normal people. Furthermore, it can be seen that the best classification
accuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT
image dataset of 7500 samples where COVID-19 patients, lung tumor patients and
normal people are equal in number. Moreover, we illustrate the potential DL
techniques in drug or vaccine discovery in combating the coronavirus. Finally,
we address a number of problems, concerns and future research directions
relevant to DL applications for COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1"&gt;Subrato Bharati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1"&gt;Prajoy Podder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1"&gt;M. Rubaiyat Hossain Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1"&gt;V.B. Surya Prasath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer. (arXiv:2107.03008v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03008</id>
        <link href="http://arxiv.org/abs/2107.03008"/>
        <updated>2021-07-21T02:01:34.994Z</updated>
        <summary type="html"><![CDATA[Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain
by utilizing transferable information learned from the available source domain
and a few labeled target data. However, source data is not always accessible in
practical scenarios, which restricts the application of SSDA in real world
circumstances. In this paper, we propose a novel task named Semi-supervised
Source Hypothesis Transfer (SSHT), which performs domain adaptation based on
source trained model, to generalize well in target domain with a few
supervisions. In SSHT, we are facing two challenges: (1) The insufficient
labeled target data may result in target features near the decision boundary,
with the increased risk of mis-classification; (2) The data are usually
imbalanced in source domain, so the model trained with these data is biased.
The biased model is prone to categorize samples of minority categories into
majority ones, resulting in low prediction diversity. To tackle the above
issues, we propose Consistency and Diversity Learning (CDL), a simple but
effective framework for SSHT by facilitating prediction consistency between two
randomly augmented unlabeled data and maintaining the prediction diversity when
adapting model to target domain. Encouraging consistency regularization brings
difficulty to memorize the few labeled target data and thus enhances the
generalization ability of the learned model. We further integrate Batch
Nuclear-norm Maximization into our method to enhance the discriminability and
diversity. Experimental results show that our method outperforms existing SSDA
methods and unsupervised model adaptation methods on DomainNet, Office-Home and
Office-31 datasets. The code is available at
https://github.com/Wang-xd1899/SSHT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaodong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-Heterogeneous and Adaptive Segmentation from Multi-Source and Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation. (arXiv:2005.13201v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13201</id>
        <link href="http://arxiv.org/abs/2005.13201"/>
        <updated>2021-07-21T02:01:34.976Z</updated>
        <summary type="html"><![CDATA[In medical imaging, organ/pathology segmentation models trained on current
publicly available and fully-annotated datasets usually do not well-represent
the heterogeneous modalities, phases, pathologies, and clinical scenarios
encountered in real environments. On the other hand, there are tremendous
amounts of unlabelled patient imaging scans stored by many modern clinical
centers. In this work, we present a novel segmentation strategy,
co-heterogenous and adaptive segmentation (CHASe), which only requires a small
labeled cohort of single phase imaging data to adapt to any unlabeled cohort of
heterogenous multi-phase data with possibly new clinical scenarios and
pathologies. To do this, we propose a versatile framework that fuses appearance
based semi-supervision, mask based adversarial domain adaptation, and
pseudo-labeling. We also introduce co-heterogeneous training, which is a novel
integration of co-training and hetero modality learning. We have evaluated
CHASe using a clinically comprehensive and challenging dataset of multi-phase
computed tomography (CT) imaging studies (1147 patients and 4577 3D volumes).
Compared to previous state-of-the-art baselines, CHASe can further improve
pathological liver mask Dice-Sorensen coefficients by ranges of $4.2\% \sim
9.4\%$, depending on the phase combinations: e.g., from $84.6\%$ to $94.0\%$ on
non-contrast CTs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Raju_A/0/1/0/all/0/1"&gt;Ashwin Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chi-Tung Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yunakai Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jinzheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Le Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1"&gt;ChienHuang Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1"&gt;Adam P Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09282</id>
        <link href="http://arxiv.org/abs/2107.09282"/>
        <updated>2021-07-21T02:01:34.957Z</updated>
        <summary type="html"><![CDATA[Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most of methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduced a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. Moreover, to boost the
performance, we argue that weak augmentations matter to represent a more
reliable relation, and leverage momentum strategy for practical efficiency.
Experimental results show that our proposed ReSSL significantly outperforms the
previous state-of-the-art algorithms in terms of both performance and training
efficiency. Code is available at \url{https://github.com/KyleZheng1997/ReSSL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05675</id>
        <link href="http://arxiv.org/abs/2007.05675"/>
        <updated>2021-07-21T02:01:34.947Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims at rapidly adapting to novel categories with only a
handful of samples at test time, which has been predominantly tackled with the
idea of meta-learning. However, meta-learning approaches essentially learn
across a variety of few-shot tasks and thus still require large-scale training
data with fine-grained supervision to derive a generalized model, thereby
involving prohibitive annotation cost. In this paper, we advance the few-shot
classification paradigm towards a more challenging scenario, i.e.,
cross-granularity few-shot classification, where the model observes only coarse
labels during training while is expected to perform fine-grained classification
during testing. This task largely relieves the annotation cost since
fine-grained labeling usually requires strong domain-specific expertise. To
bridge the cross-granularity gap, we approximate the fine-grained data
distribution by greedy clustering of each coarse-class into pseudo-fine-classes
according to the similarity of image embeddings. We then propose a
meta-embedder that jointly optimizes the visual- and semantic-discrimination,
in both instance-wise and coarse class-wise, to obtain a good feature space for
this coarse-to-fine pseudo-labeling process. Extensive experiments and ablation
studies are conducted to demonstrate the effectiveness and robustness of our
approach on three representative datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saliency for free: Saliency prediction as a side-effect of object recognition. (arXiv:2107.09628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09628</id>
        <link href="http://arxiv.org/abs/2107.09628"/>
        <updated>2021-07-21T02:01:34.925Z</updated>
        <summary type="html"><![CDATA[Saliency is the perceptual capacity of our visual system to focus our
attention (i.e. gaze) on relevant objects. Neural networks for saliency
estimation require ground truth saliency maps for training which are usually
achieved via eyetracking experiments. In the current paper, we demonstrate that
saliency maps can be generated as a side-effect of training an object
recognition deep neural network that is endowed with a saliency branch. Such a
network does not require any ground-truth saliency maps for training.Extensive
experiments carried out on both real and synthetic saliency datasets
demonstrate that our approach is able to generate accurate saliency maps,
achieving competitive results on both synthetic and real datasets when compared
to methods that do require ground truth data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Figueroa_Flores_C/0/1/0/all/0/1"&gt;Carola Figueroa-Flores&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berga_D/0/1/0/all/0/1"&gt;David Berga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van der Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1"&gt;Bogdan Raducanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05214</id>
        <link href="http://arxiv.org/abs/2107.05214"/>
        <updated>2021-07-21T02:01:34.907Z</updated>
        <summary type="html"><![CDATA[Table structure recognition is an essential part for making machines
understand tables. Its main task is to recognize the internal structure of a
table. However, due to the complexity and diversity in their structure and
style, it is very difficult to parse the tabular data into the structured
format which machines can understand easily, especially for complex tables. In
this paper, we introduce Split, Embed and Merge (SEM), an accurate table
structure recognizer. Our model takes table images as input and can correctly
recognize the structure of tables, whether they are simple or a complex tables.
SEM is mainly composed of three parts, splitter, embedder and merger. In the
first stage, we apply the splitter to predict the potential regions of the
table row (column) separators, and obtain the fine grid structure of the table.
In the second stage, by taking a full consideration of the textual information
in the table, we fuse the output features for each table grid from both vision
and language modalities. Moreover, we achieve a higher precision in our
experiments through adding additional semantic features. Finally, we process
the merging of these basic table grids in a self-regression manner. The
correspondent merging results is learned through the attention mechanism. In
our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR
dataset which outperforms other methods by a large margin. We also won the
first place in the complex table and third place in all tables in ICDAR 2021
Competition on Scientific Literature Parsing, Task-B. Extensive experiments on
other publicly available datasets demonstrate that our model achieves
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianshu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jun Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARID: A Comprehensive Study on Recognizing Actions in the Dark and A New Benchmark Dataset. (arXiv:2006.03876v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03876</id>
        <link href="http://arxiv.org/abs/2006.03876"/>
        <updated>2021-07-21T02:01:34.849Z</updated>
        <summary type="html"><![CDATA[The task of action recognition in dark videos is useful in various scenarios,
e.g., night surveillance and self-driving at night. Though progress has been
made in the action recognition task for videos in normal illumination, few have
studied action recognition in the dark. This is partly due to the lack of
sufficient datasets for such a task. In this paper, we explored the task of
action recognition in dark videos. We bridge the gap of the lack of data for
this task by collecting a new dataset: the Action Recognition in the Dark
(ARID) dataset. It consists of over 3,780 video clips with 11 action
categories. To the best of our knowledge, it is the first dataset focused on
human actions in dark videos. To gain further understandings of our ARID
dataset, we analyze the ARID dataset in detail and exhibited its necessity over
synthetic dark videos. Additionally, we benchmarked the performance of several
current action recognition models on our dataset and explored potential methods
for increasing their performances. Our results show that current action
recognition models and frame enhancement methods may not be effective solutions
for the task of action recognition in dark videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuecong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Haozhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1"&gt;Kezhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianxiong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1"&gt;Simon See&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06212</id>
        <link href="http://arxiv.org/abs/2107.06212"/>
        <updated>2021-07-21T02:01:34.842Z</updated>
        <summary type="html"><![CDATA[Ongoing advancements in the fields of 3D modelling and digital archiving have
led to an outburst in the amount of data stored digitally. Consequently,
several retrieval systems have been developed depending on the type of data
stored in these databases. However, unlike text data or images, performing a
search for 3D models is non-trivial. Among 3D models, retrieving 3D
Engineering/CAD models or mechanical components is even more challenging due to
the presence of holes, volumetric features, presence of sharp edges etc., which
make CAD a domain unto itself. The research work presented in this paper aims
at developing a dataset suitable for building a retrieval system for 3D CAD
models based on deep learning. 3D CAD models from the available CAD databases
are collected, and a dataset of computer-generated sketch data, termed
'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the
components are also added to CADSketchNet. Using the sketch images from this
dataset, the paper also aims at evaluating the performance of various retrieval
system or a search engine for 3D CAD models that accepts a sketch image as the
input query. Many experimental models are constructed and tested on
CADSketchNet. These experiments, along with the model architecture, choice of
similarity metrics are reported along with the search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1"&gt;Shubham Dhayarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1"&gt;Sai Mitheran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1"&gt;V.K. Viekash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Segmentation and Volume Measurement of Intracranial Carotid Artery Calcification on Non-Contrast CT. (arXiv:2107.09442v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09442</id>
        <link href="http://arxiv.org/abs/2107.09442"/>
        <updated>2021-07-21T02:01:34.809Z</updated>
        <summary type="html"><![CDATA[Purpose: To evaluate a fully-automated deep-learning-based method for
assessment of intracranial carotid artery calcification (ICAC). Methods: Two
observers manually delineated ICAC in non-contrast CT scans of 2,319
participants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study,
prospectively collected between 2003 and 2006. These data were used to
retrospectively develop and validate a deep-learning-based method for automated
ICAC delineation and volume measurement. To evaluate the method, we compared
manual and automatic assessment (computed using ten-fold cross-validation) with
respect to 1) the agreement with an independent observer's assessment
(available in a random subset of 47 scans); 2) the accuracy in delineating ICAC
as judged via blinded visual comparison by an expert; 3) the association with
first stroke incidence from the scan date until 2012. All method performance
metrics were computed using 10-fold cross-validation. Results: The automated
delineation of ICAC reached sensitivity of 83.8% and positive predictive value
(PPV) of 88%. The intraclass correlation between automatic and manual ICAC
volume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset).
Measured between the assessments of independent observers, sensitivity was
73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95;
computed in the 47-scan subset). In the blinded visual comparisons, automatic
delineations were more accurate than manual ones (p-value = 0.01). The
association of ICAC volume with incident stroke was similarly strong for both
automated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured
volumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed
model was capable of automated segmentation and volume quantification of ICAC
with accuracy comparable to human experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bortsova_G/0/1/0/all/0/1"&gt;Gerda Bortsova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bos_D/0/1/0/all/0/1"&gt;Daniel Bos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1"&gt;Florian Dubost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vernooij_M/0/1/0/all/0/1"&gt;Meike W. Vernooij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ikram_M/0/1/0/all/0/1"&gt;M. Kamran Ikram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tulder_G/0/1/0/all/0/1"&gt;Gijs van Tulder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bruijne_M/0/1/0/all/0/1"&gt;Marleen de Bruijne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification. (arXiv:2107.09289v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09289</id>
        <link href="http://arxiv.org/abs/2107.09289"/>
        <updated>2021-07-21T02:01:34.802Z</updated>
        <summary type="html"><![CDATA[Cell detection is an essential task in cell image analysis. Recent deep
learning-based detection methods have achieved very promising results. In
general, these methods require exhaustively annotating the cells in an entire
image. If some of the cells are not annotated (imperfect annotation), the
detection performance significantly degrades due to noisy labels. This often
occurs in real collaborations with biologists and even in public data-sets. Our
proposed method takes a pseudo labeling approach for cell detection from
imperfect annotated data. A detection convolutional neural network (CNN)
trained using such missing labeled data often produces over-detection. We treat
partially labeled cells as positive samples and the detected positions except
for the labeled cell as unlabeled samples. Then we select reliable pseudo
labels from unlabeled data using recent machine learning techniques;
positive-and-unlabeled (PU) learning and P-classification. Experiments using
microscopy images for five different conditions demonstrate the effectiveness
of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1"&gt;Kazuma Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daiki_S/0/1/0/all/0/1"&gt;Suehiro Daiki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazuya_N/0/1/0/all/0/1"&gt;Nishimura Kazuya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryoma_B/0/1/0/all/0/1"&gt;Bise Ryoma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition. (arXiv:2107.09126v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09126</id>
        <link href="http://arxiv.org/abs/2107.09126"/>
        <updated>2021-07-21T02:01:34.606Z</updated>
        <summary type="html"><![CDATA[The modern open internet contains billions of public images of human faces
across the web, especially on social media websites used by half the world's
population. In this context, Face Recognition (FR) systems have the potential
to match faces to specific names and identities, creating glaring privacy
concerns. Adversarial attacks are a promising way to grant users privacy from
FR systems by disrupting their capability to recognize faces. Yet, such attacks
can be perceptible to human observers, especially under the more challenging
black-box threat model. In the literature, the justification for the
imperceptibility of such attacks hinges on bounding metrics such as $\ell_p$
norms. However, there is not much research on how these norms match up with
human perception. Through examining and measuring both the effectiveness of
recent black-box attacks in the face recognition setting and their
corresponding human perceptibility through survey data, we demonstrate the
trade-offs in perceptibility that occur as attacks become more aggressive. We
also show how the $\ell_2$ norm and other metrics do not correlate with human
perceptibility in a linear fashion, thus making these norms suboptimal at
measuring adversarial attack perceptibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spetter_Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Spetter-Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1"&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminator-Free Generative Adversarial Attack. (arXiv:2107.09225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09225</id>
        <link href="http://arxiv.org/abs/2107.09225"/>
        <updated>2021-07-21T02:01:34.599Z</updated>
        <summary type="html"><![CDATA[The Deep Neural Networks are vulnerable toadversarial exam-ples(Figure 1),
making the DNNs-based systems collapsed byadding the inconspicuous
perturbations to the images. Most of the existing works for adversarial attack
are gradient-based and suf-fer from the latency efficiencies and the load on
GPU memory. Thegenerative-based adversarial attacks can get rid of this
limitation,and some relative works propose the approaches based on GAN.However,
suffering from the difficulty of the convergence of train-ing a GAN, the
adversarial examples have either bad attack abilityor bad visual quality. In
this work, we find that the discriminatorcould be not necessary for
generative-based adversarial attack, andpropose theSymmetric Saliency-based
Auto-Encoder (SSAE)to generate the perturbations, which is composed of the
saliencymap module and the angle-norm disentanglement of the featuresmodule.
The advantage of our proposed method lies in that it is notdepending on
discriminator, and uses the generative saliency map to pay more attention to
label-relevant regions. The extensive exper-iments among the various tasks,
datasets, and models demonstratethat the adversarial examples generated by SSAE
not only make thewidely-used models collapse, but also achieves good visual
quality.The code is available at https://github.com/BravoLu/SSAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shaohao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1"&gt;Yuqiao Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Ke Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wei-Shi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09262</id>
        <link href="http://arxiv.org/abs/2107.09262"/>
        <updated>2021-07-21T02:01:34.592Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Sanchita Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1"&gt;John J. Prevost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online Proctoring. (arXiv:2107.09373v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.09373</id>
        <link href="http://arxiv.org/abs/2107.09373"/>
        <updated>2021-07-21T02:01:34.586Z</updated>
        <summary type="html"><![CDATA[Online proctoring has become a necessity in online teaching. Video-based
crowd-sourced online proctoring solutions are being used, where an exam-taking
student's video is monitored by third parties, leading to privacy concerns. In
this paper, we propose a privacy-preserving online proctoring system. The
proposed image-hashing-based system can detect the student's excessive face and
body movement (i.e., anomalies) that is resulted when the student tries to
cheat in the exam. The detection can be done even if the student's face is
blurred or masked in video frames. Experiment with an in-house dataset shows
the usability of the proposed system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaqub_W/0/1/0/all/0/1"&gt;Waheeb Yaqub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1"&gt;Manoranjan Mohanty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suleiman_B/0/1/0/all/0/1"&gt;Basem Suleiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using Vessel Image Reconstruction. (arXiv:2107.09372v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09372</id>
        <link href="http://arxiv.org/abs/2107.09372"/>
        <updated>2021-07-21T02:01:34.567Z</updated>
        <summary type="html"><![CDATA[This paper investigates the problem of domain adaptation for diabetic
retinopathy (DR) grading. We learn invariant target-domain features by defining
a novel self-supervised task based on retinal vessel image reconstructions,
inspired by medical domain knowledge. Then, a benchmark of current
state-of-the-art unsupervised domain adaptation methods on the DR problem is
provided. It can be shown that our approach outperforms existing domain
adaption strategies. Furthermore, when utilizing entire training data in the
target domain, we are able to compete with several state-of-the-art approaches
in final classification accuracy just by applying standard network
architectures and using image-level labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duy M. H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Truong T. N. Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Ngoc T. T. Than&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prange_A/0/1/0/all/0/1"&gt;Alexander Prange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1"&gt;Daniel Sonntag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Landmarks Localization using Cascaded Neural Networks. (arXiv:1805.01760v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.01760</id>
        <link href="http://arxiv.org/abs/1805.01760"/>
        <updated>2021-07-21T02:01:34.559Z</updated>
        <summary type="html"><![CDATA[The accurate localization of facial landmarks is at the core of face analysis
tasks, such as face recognition and facial expression analysis, to name a few.
In this work, we propose a novel localization approach based on a deep learning
architecture that utilizes cascaded subnetworks with convolutional neural
network units. The cascaded units of the first subnetwork estimate
heatmap-based encodings of the landmarks locations, while the cascaded units of
the second subnetwork receive as input the output of the corresponding heatmap
estimation units, and refine them through regression. The proposed scheme is
experimentally shown to compare favorably with contemporary state-of-the-art
schemes, especially when applied to images depicting challenging localization
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahpod_S/0/1/0/all/0/1"&gt;Shahar Mahpod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Rig Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maiorana_E/0/1/0/all/0/1"&gt;Emanuele Maiorana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yosi Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campisi_P/0/1/0/all/0/1"&gt;Patrizio Campisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZstGAN: An Adversarial Approach for Unsupervised Zero-Shot Image-to-Image Translation. (arXiv:1906.00184v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00184</id>
        <link href="http://arxiv.org/abs/1906.00184"/>
        <updated>2021-07-21T02:01:34.552Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation models have shown remarkable ability on
transferring images among different domains. Most of existing work follows the
setting that the source domain and target domain keep the same at training and
inference phases, which cannot be generalized to the scenarios for translating
an image from an unseen domain to another unseen domain. In this work, we
propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem,
which aims to learn a model that can translate samples from image domains that
are not observed during training. Accordingly, we propose a framework called
ZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model
each domain with domain-specific feature distribution that is semantically
consistent on vision and attribute modalities. Then the domain-invariant
features are disentangled with an shared encoder for image generation. We carry
out extensive experiments on CUB and FLO datasets, and the results demonstrate
the effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows
significant accuracy improvements over state-of-the-art zero-shot learning
methods on CUB and FLO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianxin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingce Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuqin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Gender and Racial Disparities in Image Recognition Models. (arXiv:2107.09211v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09211</id>
        <link href="http://arxiv.org/abs/2107.09211"/>
        <updated>2021-07-21T02:01:34.545Z</updated>
        <summary type="html"><![CDATA[Large scale image classification models trained on top of popular datasets
such as Imagenet have shown to have a distributional skew which leads to
disparities in prediction accuracies across different subsections of population
demographics. A lot of approaches have been made to solve for this
distributional skew using methods that alter the model pre, post and during
training. We investigate one such approach - which uses a multi-label softmax
loss with cross-entropy as the loss function instead of a binary cross-entropy
on a multi-label classification problem on the Inclusive Images dataset which
is a subset of the OpenImages V6 dataset. We use the MR2 dataset, which
contains images of people with self-identified gender and race attributes to
evaluate the fairness in the model outcomes and try to interpret the mistakes
by looking at model activations and suggest possible fixes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahadev_R/0/1/0/all/0/1"&gt;Rohan Mahadev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarti_A/0/1/0/all/0/1"&gt;Anindya Chakravarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular Visual Analysis for Electronic Line Calling of Tennis Games. (arXiv:2107.09255v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09255</id>
        <link href="http://arxiv.org/abs/2107.09255"/>
        <updated>2021-07-21T02:01:34.539Z</updated>
        <summary type="html"><![CDATA[Electronic Line Calling is an auxiliary referee system used for tennis
matches based on binocular vision technology. While ELC has been widely used,
there are still many problems, such as complex installation and maintenance,
high cost and etc. We propose a monocular vision technology based ELC method.
The method has the following steps. First, locate the tennis ball's trajectory.
We propose a multistage tennis ball positioning approach combining background
subtraction and color area filtering. Then we propose a bouncing point
prediction method by minimizing the fitting loss of the uncertain point.
Finally, we find out whether the bouncing point of the ball is out of bounds or
not according to the relative position between the bouncing point and the court
side line in the two dimensional image. We collected and tagged 394 samples
with an accuracy rate of 99.4%, and 81.8% of the 11 samples with bouncing
points.The experimental results show that our method is feasible to judge if a
ball is out of the court with monocular vision and significantly reduce complex
installation and costs of ELC system with binocular vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shaobo Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models. (arXiv:2107.09313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09313</id>
        <link href="http://arxiv.org/abs/2107.09313"/>
        <updated>2021-07-21T02:01:34.521Z</updated>
        <summary type="html"><![CDATA[For successful scene text recognition (STR) models, synthetic text image
generators have alleviated the lack of annotated text images from the real
world. Specifically, they generate multiple text images with diverse
backgrounds, font styles, and text shapes and enable STR models to learn visual
patterns that might not be accessible from manually annotated data. In this
paper, we introduce a new synthetic text image generator, SynthTIGER, by
analyzing techniques used for text image synthesis and integrating effective
ones under a single algorithm. Moreover, we propose two techniques that
alleviate the long-tail problem in length and character distributions of
training data. In our experiments, SynthTIGER achieves better STR performance
than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).
Our ablation study demonstrates the benefits of using sub-components of
SynthTIGER and the guideline on generating synthetic text images for STR
models. Our implementation is publicly available at
https://github.com/clovaai/synthtiger.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1"&gt;Moonbin Yim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoonsik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Han-Cheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse Experts with Self-Supervision. (arXiv:2107.09249v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09249</id>
        <link href="http://arxiv.org/abs/2107.09249"/>
        <updated>2021-07-21T02:01:34.514Z</updated>
        <summary type="html"><![CDATA[Existing long-tailed recognition methods, aiming to train class-balance
models from long-tailed data, generally assume the models would be evaluated on
the uniform test class distribution. However, the practical test class
distribution often violates such an assumption (e.g., being long-tailed or even
inversely long-tailed), which would lead existing methods to fail in real-world
applications. In this work, we study a more practical task setting, called
test-agnostic long-tailed recognition, where the training class distribution is
long-tailed while the test class distribution is unknown and can be skewed
arbitrarily. In addition to the issue of class imbalance, this task poses
another challenge: the class distribution shift between the training and test
samples is unidentified. To address this task, we propose a new method, called
Test-time Aggregating Diverse Experts (TADE), that presents two solution
strategies: (1) a novel skill-diverse expert learning strategy that trains
diverse experts to excel at handling different test distributions from a single
long-tailed training distribution; (2) a novel test-time expert aggregation
strategy that leverages self-supervision to aggregate multiple experts for
handling various test distributions. Moreover, we theoretically show that our
method has provable ability to simulate unknown test class distributions.
Promising results on both vanilla and test-agnostic long-tailed recognition
verify the effectiveness of TADE. Code is available at
https://github.com/Vanint/TADE-AgnosticLT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09405</id>
        <link href="http://arxiv.org/abs/2107.09405"/>
        <updated>2021-07-21T02:01:34.507Z</updated>
        <summary type="html"><![CDATA[We propose a Deep learning-based weak label learning method for analysing
whole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not
requiring pixel-level or tile-level annotations using Self-supervised
pre-training and heterogeneity-aware deep Multiple Instance LEarning
(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination
deficiency (HRD) and microsatellite instability (MSI) prediction. We utilize
contrastive self-supervised learning to pre-train a feature extractor on
histopathology tiles of cancer tissue. Additionally, we use variability-aware
deep multiple instance learning to learn the tile feature aggregation function
while modeling tumor heterogeneity. Compared to state-of-the-art genomic label
classification methods, DeepSMILE improves classification performance for HRD
from $70.43\pm4.10\%$ to $83.79\pm1.25\%$ AUC and MSI from $78.56\pm6.24\%$ to
$90.32\pm3.58\%$ AUC in a multi-center breast and colorectal cancer dataset,
respectively. These improvements suggest we can improve genomic label
classification performance without collecting larger datasets. In the future,
this may reduce the need for expensive genome sequencing techniques, provide
personalized therapy recommendations based on widely available WSIs of cancer
tissue, and improve patient care with quicker treatment decisions - also in
medical centers without access to genome sequencing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1"&gt;Yoni Schirris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1"&gt;Iris Nederlof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1"&gt;Hugo Mark Horlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1"&gt;Jonas Teuwen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09362</id>
        <link href="http://arxiv.org/abs/2107.09362"/>
        <updated>2021-07-21T02:01:34.500Z</updated>
        <summary type="html"><![CDATA[Since production-level trained deep neural networks (DNNs) are of a great
business value, protecting such DNN models against copyright infringement and
unauthorized access is in a rising demand. However, conventional model
protection methods focused only the image classification task, and these
protection methods were never applied to semantic segmentation although it has
an increasing number of applications. In this paper, we propose to protect
semantic segmentation models from unauthorized access by utilizing block-wise
transformation with a secret key for the first time. Protected models are
trained by using transformed images. Experiment results show that the proposed
protection method allows rightful users with the correct key to access the
model to full capacity and deteriorate the performance for unauthorized users.
However, protected models slightly drop the segmentation performance compared
to non-protected models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1"&gt;Hiroki Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1"&gt;MaungMaung AprilPyone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank. (arXiv:2107.09427v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09427</id>
        <link href="http://arxiv.org/abs/2107.09427"/>
        <updated>2021-07-21T02:01:34.493Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GAN) have demonstrated the potential to
recover realistic details for single image super-resolution (SISR). To further
improve the visual quality of super-resolved results, PIRM2018-SR Challenge
employed perceptual metrics to assess the perceptual quality, such as PI, NIQE,
and Ma. However, existing methods cannot directly optimize these
indifferentiable perceptual metrics, which are shown to be highly correlated
with human ratings. To address the problem, we propose Super-Resolution
Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator
in the direction of different perceptual metrics. Specifically, we first train
a Ranker which can learn the behaviour of perceptual metrics and then introduce
a novel rank-content loss to optimize the perceptual quality. The most
appealing part is that the proposed method can combine the strengths of
different SR methods to generate better results. Furthermore, we extend our
method to multiple Rankers to provide multi-dimension constraints for the
generator. Extensive experiments show that RankSRGAN achieves visually pleasing
results and reaches state-of-the-art performance in perceptual metrics and
quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09244</id>
        <link href="http://arxiv.org/abs/2107.09244"/>
        <updated>2021-07-21T02:01:34.485Z</updated>
        <summary type="html"><![CDATA[Collecting large-scale annotated satellite imagery datasets is essential for
deep-learning-based global building change surveillance. In particular, the
scroll imaging mode of optical satellites enables larger observation ranges and
shorter revisit periods, facilitating efficient global surveillance. However,
the images in recent satellite change detection datasets are mainly captured at
near-nadir viewing angles. In this paper, we introduce S2Looking, a building
change detection dataset that contains large-scale side-looking satellite
images captured at varying off-nadir angles. Our S2Looking dataset consists of
5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel)
of rural areas throughout the world and more than 65,920 annotated change
instances. We provide two label maps to separately indicate the newly built and
demolished building regions for each sample in the dataset. We establish a
benchmark task based on this dataset, i.e., identifying the pixel-level
building changes in the bi-temporal images. We test several state-of-the-art
methods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset.
The experimental results show that recent change detection methods exhibit much
poorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking
dataset presents three main challenges: 1) large viewing angle changes, 2)
large illumination variances and 3) various complex scene characteristics
encountered in rural areas. Our proposed dataset may promote the development of
algorithms for satellite image change detection and registration under
conditions of large off-nadir angles. The dataset is available at
https://github.com/AnonymousForACMMM/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1"&gt;Donghai Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1"&gt;Jiabao Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Ao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1"&gt;Shouye Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bitao Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locality-aware Channel-wise Dropout for Occluded Face Recognition. (arXiv:2107.09270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09270</id>
        <link href="http://arxiv.org/abs/2107.09270"/>
        <updated>2021-07-21T02:01:34.462Z</updated>
        <summary type="html"><![CDATA[Face recognition remains a challenging task in unconstrained scenarios,
especially when faces are partially occluded. To improve the robustness against
occlusion, augmenting the training images with artificial occlusions has been
proved as a useful approach. However, these artificial occlusions are commonly
generated by adding a black rectangle or several object templates including
sunglasses, scarfs and phones, which cannot well simulate the realistic
occlusions. In this paper, based on the argument that the occlusion essentially
damages a group of neurons, we propose a novel and elegant occlusion-simulation
method via dropping the activations of a group of neurons in some elaborately
selected channel. Specifically, we first employ a spatial regularization to
encourage each feature channel to respond to local and different face regions.
In this way, the activations affected by an occlusion in a local region are
more likely to be located in a single feature channel. Then, the locality-aware
channel-wise dropout (LCD) is designed to simulate the occlusion by dropping
out the entire feature channel. Furthermore, by randomly dropping out several
feature channels, our method can well simulate the occlusion of larger area.
The proposed LCD can encourage its succeeding layers to minimize the
intra-class feature variance caused by occlusions, thus leading to improved
robustness against occlusion. In addition, we design an auxiliary spatial
attention module by learning a channel-wise attention vector to reweight the
feature channels, which improves the contributions of non-occluded regions.
Extensive experiments on various benchmarks show that the proposed method
outperforms state-of-the-art methods with a remarkable improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mingjie He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09240</id>
        <link href="http://arxiv.org/abs/2107.09240"/>
        <updated>2021-07-21T02:01:34.453Z</updated>
        <summary type="html"><![CDATA[Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Fu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-07-21T02:01:34.445Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography. (arXiv:2107.09287v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09287</id>
        <link href="http://arxiv.org/abs/2107.09287"/>
        <updated>2021-07-21T02:01:34.418Z</updated>
        <summary type="html"><![CDATA[Data hiding is the process of embedding information into a noise-tolerant
signal such as a piece of audio, video, or image. Digital watermarking is a
form of data hiding where identifying data is robustly embedded so that it can
resist tampering and be used to identify the original owners of the media.
Steganography, another form of data hiding, embeds data for the purpose of
secure and secret communication. This survey summarises recent developments in
deep learning techniques for data hiding for the purposes of watermarking and
steganography, categorising them based on model architectures and noise
injection methods. The objective functions, evaluation metrics, and datasets
used for training these data hiding models are comprehensively summarised.
Finally, we propose and discuss possible future directions for research into
deep data hiding techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byrnes_O/0/1/0/all/0/1"&gt;Olivia Byrnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+La_W/0/1/0/all/0/1"&gt;Wendy La&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Congbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Minhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09101</id>
        <link href="http://arxiv.org/abs/2107.09101"/>
        <updated>2021-07-21T02:01:34.411Z</updated>
        <summary type="html"><![CDATA[Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount
of interest in the past few decades, while one of the most critical operations
in these systems is the perception of the environment. Deep learning and,
especially, the use of Deep Neural Networks (DNNs) provides impressive results
in analyzing and understanding complex and dynamic scenes from visual data. The
prediction horizons for those perception systems are very short and inference
must often be performed in real time, stressing the need of transforming the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Our goal in this work is to
investigate best practices for appropriately applying novel weight sharing
techniques, optimizing the available variables and the training procedures
towards the significant acceleration of widely adopted DNNs. Extensive
evaluation studies carried out using various state-of-the-art DNN models in
object detection and tracking experiments, provide details about the type of
errors that manifest after the application of weight sharing techniques,
resulting in significant acceleration gains with negligible accuracy losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1"&gt;Stavros Nousias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1"&gt;Erion-Vasilis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1"&gt;Christos Mavrokefalidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S. Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthSeg: Domain Randomisation for Segmentation of Brain MRI Scans of any Contrast and Resolution. (arXiv:2107.09559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09559</id>
        <link href="http://arxiv.org/abs/2107.09559"/>
        <updated>2021-07-21T02:01:34.386Z</updated>
        <summary type="html"><![CDATA[Despite advances in data augmentation and transfer learning, convolutional
neural networks (CNNs) have difficulties generalising to unseen target domains.
When applied to segmentation of brain MRI scans, CNNs are highly sensitive to
changes in resolution and contrast: even within the same MR modality, decreases
in performance can be observed across datasets. We introduce SynthSeg, the
first segmentation CNN agnostic to brain MRI scans of any contrast and
resolution. SynthSeg is trained with synthetic data sampled from a generative
model inspired by Bayesian segmentation. Crucially, we adopt a \textit{domain
randomisation} strategy where we fully randomise the generation parameters to
maximise the variability of the training data. Consequently, SynthSeg can
segment preprocessed and unpreprocessed real scans of any target domain,
without retraining or fine-tuning. Because SynthSeg only requires segmentations
to be trained (no images), it can learn from label maps obtained automatically
from existing datasets of different populations (e.g., with atrophy and
lesions), thus achieving robustness to a wide range of morphological
variability. We demonstrate SynthSeg on 5,500 scans of 6 modalities and 10
resolutions, where it exhibits unparalleled generalisation compared to
supervised CNNs, test time adaptation, and Bayesian segmentation. The code and
trained model are available at https://github.com/BBillot/SynthSeg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1"&gt;Benjamin Billot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1"&gt;Douglas N. Greve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Puonti_O/0/1/0/all/0/1"&gt;Oula Puonti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thielscher_A/0/1/0/all/0/1"&gt;Axel Thielscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1"&gt;Koen Van Leemput&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1"&gt;Bruce Fischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1"&gt;Juan Eugenio Iglesias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09204</id>
        <link href="http://arxiv.org/abs/2107.09204"/>
        <updated>2021-07-21T02:01:34.379Z</updated>
        <summary type="html"><![CDATA[Anomaly detection in images plays a significant role for many applications
across all industries, such as disease diagnosis in healthcare or quality
assurance in manufacturing. Manual inspection of images, when extended over a
monotonously repetitive period of time is very time consuming and can lead to
anomalies being overlooked.Artificial neural networks have proven themselves
very successful on simple, repetitive tasks, in some cases even outperforming
humans. Therefore, in this paper we investigate different methods of deep
learning, including supervised and unsupervised learning, for anomaly detection
applied to a quality assurance use case. We utilize the MVTec anomaly dataset
and develop three different models, a CNN for supervised anomaly detection,
KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly
detection and a DCGAN for generating reconstructed images. By experiments, we
found that KD-CAE performs better on the anomaly datasets compared to CNN and
NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also
implemented a DCGAN for the creation of new training data but due to
computational limitation and lack of extrapolating the mechanics of AnoGAN, we
restricted ourselves just to the generation of GAN based images. We conclude
that unsupervised methods are more powerful for anomaly detection in images,
especially in a setting where only a small amount of anomalous data is
available, or the data is unlabeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1"&gt;Vincent Wilmet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sauraj Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1"&gt;Tabea Redl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kon Sandaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenning Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Large-scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts. (arXiv:2103.11528v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11528</id>
        <link href="http://arxiv.org/abs/2103.11528"/>
        <updated>2021-07-21T02:01:34.365Z</updated>
        <summary type="html"><![CDATA[In recent years, Vietnam witnesses the mass development of social network
users on different social platforms such as Facebook, Youtube, Instagram, and
Tiktok. On social medias, hate speech has become a critical problem for social
network users. To solve this problem, we introduce the ViHSD - a
human-annotated dataset for automatically detecting hate speech on the social
network. This dataset contains over 30,000 comments, each comment in the
dataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, we
introduce the data creation process for annotating and evaluating the quality
of the dataset. Finally, we evaluated the dataset by deep learning models and
transformer models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos. (arXiv:2107.09504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09504</id>
        <link href="http://arxiv.org/abs/2107.09504"/>
        <updated>2021-07-21T02:01:34.357Z</updated>
        <summary type="html"><![CDATA[Anticipating human actions is an important task that needs to be addressed
for the development of reliable intelligent agents, such as self-driving cars
or robot assistants. While the ability to make future predictions with high
accuracy is crucial for designing the anticipation approaches, the speed at
which the inference is performed is not less important. Methods that are
accurate but not sufficiently fast would introduce a high latency into the
decision process. Thus, this will increase the reaction time of the underlying
system. This poses a problem for domains such as autonomous driving, where the
reaction time is crucial. In this work, we propose a simple and effective
multi-modal architecture based on temporal convolutions. Our approach stacks a
hierarchy of temporal convolutional layers and does not rely on recurrent
layers to ensure a fast prediction. We further introduce a multi-modal fusion
mechanism that captures the pairwise interactions between RGB, flow, and object
modalities. Results on two large-scale datasets of egocentric videos,
EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves
comparable performance to the state-of-the-art approaches while being
significantly faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zatsarynna_O/0/1/0/all/0/1"&gt;Olga Zatsarynna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1"&gt;Yazan Abu Farha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09293</id>
        <link href="http://arxiv.org/abs/2107.09293"/>
        <updated>2021-07-21T02:01:34.351Z</updated>
        <summary type="html"><![CDATA[We propose an audio-driven talking-head method to generate photo-realistic
talking-head videos from a single reference image. In this work, we tackle two
key challenges: (i) producing natural head motions that match speech prosody,
and (ii) maintaining the appearance of a speaker in a large head motion while
stabilizing the non-face regions. We first design a head pose predictor by
modeling rigid 6D head movements with a motion-aware recurrent neural network
(RNN). In this way, the predicted head poses act as the low-frequency holistic
movements of a talking head, thus allowing our latter network to focus on
detailed facial movement generation. To depict the entire image motions arising
from audio, we exploit a keypoint based dense motion field representation.
Then, we develop a motion field generator to produce the dense motion fields
from input audio, head poses, and a reference image. As this keypoint based
representation models the motions of facial regions, head, and backgrounds
integrally, our method can better constrain the spatial and temporal
consistency of the generated videos. Finally, an image generation network is
employed to render photo-realistic talking-head videos from the estimated
keypoint based motion fields and the input reference image. Extensive
experiments demonstrate that our method produces videos with plausible head
motions, synchronized facial expressions, and stable backgrounds and
outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lincheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting few-shot classification with view-learnable contrastive learning. (arXiv:2107.09242v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09242</id>
        <link href="http://arxiv.org/abs/2107.09242"/>
        <updated>2021-07-21T02:01:34.331Z</updated>
        <summary type="html"><![CDATA[The goal of few-shot classification is to classify new categories with few
labeled examples within each class. Nowadays, the excellent performance in
handling few-shot classification problems is shown by metric-based
meta-learning methods. However, it is very hard for previous methods to
discriminate the fine-grained sub-categories in the embedding space without
fine-grained labels. This may lead to unsatisfactory generalization to
fine-grained subcategories, and thus affects model interpretation. To tackle
this problem, we introduce the contrastive loss into few-shot classification
for learning latent fine-grained structure in the embedding space. Furthermore,
to overcome the drawbacks of random image transformation used in current
contrastive learning in producing noisy and inaccurate image pairs (i.e.,
views), we develop a learning-to-learn algorithm to automatically generate
different views of the same image. Extensive experiments on standard few-shot
learning benchmarks demonstrate the superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Liangjian Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Critic Guided Segmentation of Rewarding Objects in First-Person Views. (arXiv:2107.09540v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09540</id>
        <link href="http://arxiv.org/abs/2107.09540"/>
        <updated>2021-07-21T02:01:34.324Z</updated>
        <summary type="html"><![CDATA[This work discusses a learning approach to mask rewarding objects in images
using sparse reward signals from an imitation learning dataset. For that, we
train an Hourglass network using only feedback from a critic model. The
Hourglass network learns to produce a mask to decrease the critic's score of a
high score image and increase the critic's score of a low score image by
swapping the masked areas between these two images. We trained the model on an
imitation learning dataset from the NeurIPS 2020 MineRL Competition Track,
where our model learned to mask rewarding objects in a complex interactive 3D
environment with a sparse reward signal. This approach was part of the 1st
place winning solution in this competition. Video demonstration and code:
https://rebrand.ly/critic-guided-segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1"&gt;Andrew Melnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harter_A/0/1/0/all/0/1"&gt;Augustin Harter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Limberg_C/0/1/0/all/0/1"&gt;Christian Limberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1"&gt;Krishan Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suenderhauf_N/0/1/0/all/0/1"&gt;Niko Suenderhauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1"&gt;Helge Ritter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues. (arXiv:2107.09237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09237</id>
        <link href="http://arxiv.org/abs/2107.09237"/>
        <updated>2021-07-21T02:01:34.317Z</updated>
        <summary type="html"><![CDATA[Near infrared (NIR) imaging has been widely applied in low-light imaging
scenarios; however, it is difficult for human and algorithms to perceive the
real scene in the colorless NIR domain. While Generative Adversarial Network
(GAN) has been widely employed in various image colorization tasks, it is
challenging for a direct mapping mechanism, such as a conventional GAN, to
transform an image from the NIR to the RGB domain with correct semantic
reasoning, well-preserved textures, and vivid color combinations concurrently.
In this work, we propose a novel Attention-based NIR image colorization
framework via Adaptive Fusion of Semantic and Texture clues, aiming at
achieving these goals within the same framework. The tasks of texture transfer
and semantic reasoning are carried out in two separate network blocks.
Specifically, the Texture Transfer Block (TTB) aims at extracting texture
features from the NIR image's Laplacian component and transferring them for
subsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic
clues and maps the NIR pixel values to the RGB domain. Finally, a Fusion
Attention Block (FAB) is proposed to adaptively fuse the features from the two
branches and generate an optimized colorization result. In order to enhance the
network's learning capacity in semantic reasoning as well as mapping precision
in texture transfer, we have proposed the Residual Coordinate Attention Block
(RCAB), which incorporates coordinate attention into a residual learning
framework, enabling the network to capture long-range dependencies along the
channel direction and meanwhile precise positional information can be preserved
along spatial directions. RCAB is also incorporated into FAB to facilitate
accurate texture alignment during fusion. Both quantitative and qualitative
evaluations show that the proposed method outperforms state-of-the-art NIR
image colorization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingxing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zaifeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09009</id>
        <link href="http://arxiv.org/abs/2106.09009"/>
        <updated>2021-07-21T02:01:34.309Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) spoken language understanding (SLU) systems predict
utterance semantics directly from speech using a single model. Previous work in
this area has focused on targeted tasks in fixed domains, where the output
semantic structure is assumed a priori and the input speech is of limited
complexity. In this work we present our approach to developing an E2E model for
generalized SLU in commercial voice assistants (VAs). We propose a fully
differentiable, transformer-based, hierarchical system that can be pretrained
at both the ASR and NLU levels. This is then fine-tuned on both transcription
and semantic classification losses to handle a diverse set of intent and
argument combinations. This leads to an SLU system that achieves significant
improvements over baselines on a complex internal generalized VA dataset with a
43% improvement in accuracy, while still meeting the 99% accuracy benchmark on
the popular Fluent Speech Commands dataset. We further evaluate our model on a
hard test set, exclusively containing slot arguments unseen in training, and
demonstrate a nearly 20% improvement, showing the efficacy of our approach in
truly demanding VA scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1"&gt;Michael Saxon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Samridhi Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1"&gt;Joseph P. McKenna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09170</id>
        <link href="http://arxiv.org/abs/2107.09170"/>
        <updated>2021-07-21T02:01:34.291Z</updated>
        <summary type="html"><![CDATA[Current datasets to train social behaviors are usually borrowed from
surveillance applications that capture visual data from a bird's-eye
perspective. This leaves aside precious relationships and visual cues that
could be captured through a first-person view of a scene. In this work, we
propose a strategy to exploit the power of current game engines, such as Unity,
to transform pre-existing bird's-eye view datasets into a first-person view, in
particular, a depth view. Using this strategy, we are able to generate large
volumes of synthetic data that can be used to pre-train a social navigation
model. To test our ideas, we present DeepSocNav, a deep learning based model
that takes advantage of the proposed approach to generate synthetic data.
Furthermore, DeepSocNav includes a self-supervised strategy that is included as
an auxiliary task. This consists of predicting the next depth frame that the
agent will face. Our experiments show the benefits of the proposed model that
is able to outperform relevant baselines in terms of social navigation scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1"&gt;Juan Pablo de Vicente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1"&gt;Alvaro Soto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression. (arXiv:2107.09179v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09179</id>
        <link href="http://arxiv.org/abs/2107.09179"/>
        <updated>2021-07-21T02:01:34.082Z</updated>
        <summary type="html"><![CDATA[State-of-the-art 2D image compression schemes rely on the power of
convolutional neural networks (CNNs). Although CNNs offer promising
perspectives for 2D image compression, extending such models to omnidirectional
images is not straightforward. First, omnidirectional images have specific
spatial and statistical properties that can not be fully captured by current
CNN models. Second, basic mathematical operations composing a CNN architecture,
e.g., translation and sampling, are not well-defined on the sphere. In this
paper, we study the learning of representation models for omnidirectional
images and propose to use the properties of HEALPix uniform sampling of the
sphere to redefine the mathematical tools used in deep learning models for
omnidirectional images. In particular, we: i) propose the definition of a new
convolution operation on the sphere that keeps the high expressiveness and the
low complexity of a classical 2D convolution; ii) adapt standard CNN techniques
such as stride, iterative aggregation, and pixel shuffling to the spherical
domain; and then iii) apply our new framework to the task of omnidirectional
image compression. Our experiments show that our proposed on-the-sphere
solution leads to a better compression gain that can save 13.7% of the bit rate
compared to similar learned models applied to equirectangular images. Also,
compared to learning models based on graph convolutional networks, our solution
supports more expressive filters that can preserve high frequencies and provide
a better perceptual quality of the compressed images. Such results demonstrate
the efficiency of the proposed framework, which opens new research venues for
other omnidirectional vision tasks to be effectively implemented on the sphere
manifold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bidgoli_N/0/1/0/all/0/1"&gt;Navid Mahmoudian Bidgoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Azevedo_R/0/1/0/all/0/1"&gt;Roberto G. de A. Azevedo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maugey_T/0/1/0/all/0/1"&gt;Thomas Maugey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roumy_A/0/1/0/all/0/1"&gt;Aline Roumy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality and Complexity Assessment of Learning-Based Image Compression Solutions. (arXiv:2107.09136v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09136</id>
        <link href="http://arxiv.org/abs/2107.09136"/>
        <updated>2021-07-21T02:01:33.936Z</updated>
        <summary type="html"><![CDATA[This work presents an analysis of state-of-the-art learning-based image
compression techniques. We compare 8 models available in the Tensorflow
Compression package in terms of visual quality metrics and processing time,
using the KODAK data set. The results are compared with the Better Portable
Graphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the
lowest execution times compared with the fastest learning-based model, with a
speedup of 1.46x in compression and 30x in decompression. However, the
learning-based models achieved improvements over JPEG2000 in terms of quality,
specially for lower bitrates. Our findings also show that BPG is more efficient
in terms of PSNR, but the learning models are better for other quality metrics,
and sometimes even faster. The results indicate that learning-based techniques
are promising solutions towards a future mainstream compression method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dick_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Dick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abreu_B/0/1/0/all/0/1"&gt;Brunno Abreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grellert_M/0/1/0/all/0/1"&gt;Mateus Grellert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bampi_S/0/1/0/all/0/1"&gt;Sergio Bampi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08001</id>
        <link href="http://arxiv.org/abs/2103.08001"/>
        <updated>2021-07-21T02:01:33.929Z</updated>
        <summary type="html"><![CDATA[This article describes research on claim verification carried out using a
multiple GAN-based model. The proposed model consists of three pairs of
generators and discriminators. The generator and discriminator pairs are
responsible for generating synthetic data for supported and refuted claims and
claim labels. A theoretical discussion about the proposed model is provided to
validate the equilibrium state of the model. The proposed model is applied to
the FEVER dataset, and a pre-trained language model is used for the input text
data. The synthetically generated data helps to gain information which helps
the model to perform better than state of the art models and other standard
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1"&gt;Amartya Hatua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Arjun Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1"&gt;Rakesh M. Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09106</id>
        <link href="http://arxiv.org/abs/2107.09106"/>
        <updated>2021-07-21T02:01:33.922Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1"&gt;Spencer Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward a Knowledge Discovery Framework for Data Science Job Market in the United States. (arXiv:2106.11077v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11077</id>
        <link href="http://arxiv.org/abs/2106.11077"/>
        <updated>2021-07-21T02:01:33.913Z</updated>
        <summary type="html"><![CDATA[The growth of the data science field requires better tools to understand such
a fast-paced growing domain. Moreover, individuals from different backgrounds
became interested in following a career as data scientists. Therefore,
providing a quantitative guide for individuals and organizations to understand
the skills required in the job market would be crucial. This paper introduces a
framework to analyze the job market for data science-related jobs within the US
while providing an interface to access insights in this market. The proposed
framework includes three sub-modules allowing continuous data collection,
information extraction, and a web-based dashboard visualization to investigate
the spatial and temporal distribution of data science-related jobs and skills.
The result of this work shows important skills for the main branches of data
science jobs and attempts to provide a skill-based definition of these data
science branches. The current version of this application is deployed on the
web and allows individuals and institutes to investigate skills required for
data science positions through the industry lens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidarysafa_M/0/1/0/all/0/1"&gt;Mojtaba Heidarysafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowsari_K/0/1/0/all/0/1"&gt;Kamran Kowsari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashiri_M/0/1/0/all/0/1"&gt;Masoud Bashiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Donald E. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional module for heart localization and segmentation in MRI. (arXiv:2107.09134v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09134</id>
        <link href="http://arxiv.org/abs/2107.09134"/>
        <updated>2021-07-21T02:01:33.906Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance imaging (MRI) is a widely known medical imaging technique
used to assess the heart function. Deep learning (DL) models perform several
tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation,
estimation, and detection of diseases. Many DL models based on convolutional
neural networks (CNN) were improved by detecting regions-of-interest (ROI)
either automatically or by hand. In this paper we describe Visual-Motion-Focus
(VMF), a module that detects the heart motion in the 4D MRI sequence, and
highlights ROIs by focusing a Radial Basis Function (RBF) on the estimated
motion field. We experimented and evaluated VMF on three CMR datasets,
observing that the proposed ROIs cover 99.7% of data labels (Recall score),
improved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI
extraction, and improved the overall training speed by 2.5 times (+150%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1"&gt;Daniel Lima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graves_C/0/1/0/all/0/1"&gt;Catharine Graves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1"&gt;Marco Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brandoli_B/0/1/0/all/0/1"&gt;Bruno Brandoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jose_J/0/1/0/all/0/1"&gt;Jose Rodrigues-Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09060</id>
        <link href="http://arxiv.org/abs/2107.09060"/>
        <updated>2021-07-21T02:01:33.895Z</updated>
        <summary type="html"><![CDATA[Physiological motion, such as cardiac and respiratory motion, during Magnetic
Resonance (MR) image acquisition can cause image artifacts. Motion correction
techniques have been proposed to compensate for these types of motion during
thoracic scans, relying on accurate motion estimation from undersampled
motion-resolved reconstruction. A particular interest and challenge lie in the
derivation of reliable non-rigid motion fields from the undersampled
motion-resolved data. Motion estimation is usually formulated in image space
via diffusion, parametric-spline, or optical flow methods. However, image-based
registration can be impaired by remaining aliasing artifacts due to the
undersampled motion-resolved reconstruction. In this work, we describe a
formalism to perform non-rigid registration directly in the sampled Fourier
space, i.e. k-space. We propose a deep-learning based approach to perform fast
and accurate non-rigid registration from the undersampled k-space data. The
basic working principle originates from the Local All-Pass (LAP) technique, a
recently introduced optical flow-based registration. The proposed LAPNet is
compared against traditional and deep learning image-based registrations and
tested on fully-sampled and highly-accelerated (with two undersampling
strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients
with suspected liver or lung metastases and 25 healthy subjects. The proposed
LAPNet provided consistent and superior performance to image-based approaches
throughout different sampling trajectories and acceleration factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1"&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jiazhen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haikun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1"&gt;Gastao Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1"&gt;Christopher Gilliam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1"&gt;Thierry Blu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Botnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1"&gt;Claudia Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09356</id>
        <link href="http://arxiv.org/abs/2107.09356"/>
        <updated>2021-07-21T02:01:33.868Z</updated>
        <summary type="html"><![CDATA[Communication between agents in collaborative multi-agent settings is in
general implicit or a direct data stream. This paper considers text-based
natural language as a novel form of communication between multiple agents
trained with reinforcement learning. This could be considered first steps
toward a truly autonomous communication without the need to define a limited
set of instructions, and natural collaboration between humans and robots.
Inspired by the game of Blind Leads, we propose an environment where one agent
uses natural language instructions to guide another through a maze. We test the
ability of reinforcement learning agents to effectively communicate through
discrete word-level symbols and show that the agents are able to sufficiently
communicate through natural language with a limited vocabulary. Although the
communication is not always perfect English, the agents are still able to
navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of
0.61 over randomly generated sequences while maintaining a 100% maze completion
rate. This is a 3.5 times the performance of the random baseline using our
reference set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1"&gt;Kevin Eloff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1"&gt;Herman Engelbrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09118</id>
        <link href="http://arxiv.org/abs/2107.09118"/>
        <updated>2021-07-21T02:01:33.852Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) models have received particular attention in medical
imaging due to their promising pattern recognition capabilities. However, Deep
Neural Networks (DNNs) require a huge amount of data, and because of the lack
of sufficient data in this field, transfer learning can be a great solution.
DNNs used for disease diagnosis meticulously concentrate on improving the
accuracy of predictions without providing a figure about their confidence of
predictions. Knowing how much a DNN model is confident in a computer-aided
diagnosis model is necessary for gaining clinicians' confidence and trust in
DL-based solutions. To address this issue, this work presents three different
methods for quantifying uncertainties for skin cancer detection from images. It
also comprehensively evaluates and compares performance of these DNNs using
novel uncertainty-related metrics. The obtained results reveal that the
predictive uncertainty estimation methods are capable of flagging risky and
erroneous predictions with a high uncertainty estimate. We also demonstrate
that ensemble approaches are more reliable in capturing uncertainties through
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1"&gt;Donya Khaledyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1"&gt;AmirReza Tajally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1"&gt;Reza Sarkhosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1"&gt;Afshar Shamsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1"&gt;Hamzeh Asgharnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Parameters? No Thanks!. (arXiv:2107.09622v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09622</id>
        <link href="http://arxiv.org/abs/2107.09622"/>
        <updated>2021-07-21T02:01:33.837Z</updated>
        <summary type="html"><![CDATA[This work studies the long-standing problems of model capacity and negative
interference in multilingual neural machine translation MNMT. We use network
pruning techniques and observe that pruning 50-70% of the parameters from a
trained MNMT model results only in a 0.29-1.98 drop in the BLEU score.
Suggesting that there exist large redundancies even in MNMT models. These
observations motivate us to use the redundant parameters and counter the
interference problem efficiently. We propose a novel adaptation strategy, where
we iteratively prune and retrain the redundant parameters of an MNMT to improve
bilingual representations while retaining the multilinguality. Negative
interference severely affects high resource languages, and our method
alleviates it without any additional adapter modules. Hence, we call it
parameter-free adaptation strategy, paving way for the efficient adaptation of
MNMT. We demonstrate the effectiveness of our method on a 9 language MNMT
trained on TED talks, and report an average improvement of +1.36 BLEU on high
resource pairs. Code will be released here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zeeshan Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akella_K/0/1/0/all/0/1"&gt;Kartheek Akella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C V Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Sensor-invariant Embedding of Satellite Data: A Case Study for Lake Ice Monitoring. (arXiv:2107.09092v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09092</id>
        <link href="http://arxiv.org/abs/2107.09092"/>
        <updated>2021-07-21T02:01:33.823Z</updated>
        <summary type="html"><![CDATA[Fusing satellite imagery acquired with different sensors has been a
long-standing challenge of Earth observation, particularly across different
modalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we
explore the joint analysis of imagery from different sensors in the light of
representation learning: we propose to learn a joint, sensor-invariant
embedding (feature representation) within a deep neural network. Our
application problem is the monitoring of lake ice on Alpine lakes. To reach the
temporal resolution requirement of the Swiss Global Climate Observing System
(GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra
MODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains
and between the sensor resolutions make this a challenging instance of the
sensor fusion problem. Our approach can be classified as a feature-level fusion
that is learnt in a data-driven manner. The proposed network architecture has
separate encoding branches for each image sensor, which feed into a single
latent embedding. I.e., a common feature representation shared by all inputs,
such that subsequent processing steps deliver comparable output irrespective of
which sort of input image was used. By fusing satellite data, we map lake ice
at a temporal resolution of <1.5 days. The network produces spatially explicit
lake ice maps with pixel-wise accuracies >91.3% (respectively, mIoU scores
>60.7%) and generalises well across different lakes and winters. Moreover, it
sets a new state-of-the-art for determining the important ice-on and ice-off
dates for the target lakes, in many cases meeting the GCOS requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1"&gt;Manu Tom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuchang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1"&gt;Emmanuel Baltsavias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A keyword-driven approach to science. (arXiv:2106.14610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14610</id>
        <link href="http://arxiv.org/abs/2106.14610"/>
        <updated>2021-07-21T02:01:33.805Z</updated>
        <summary type="html"><![CDATA[To a good extent, words can be understood as corresponding to patterns or
categories that appeared in order to represent concepts and structures that are
particularly important or useful in a given time and space. Words are
characterized by not being completely general nor specific, in the sense that
the same word can be instantiated or related to several different contexts,
depending on specific situations. Indeed, the way in which words are
instantiated and associated represents a particularly interesting aspect that
can substantially help to better understand the context in which they are
employed. Scientific words are no exception to that. In the present work, we
approach the associations between a set of particularly relevant words in the
sense of being not only frequently used in several areas, but also representing
concepts that are currently related to some of the main standing challenges in
science. More specifically, the study reported here takes into account the
words "prediction", "model", "optimization", "complex", "entropy", "random",
"deterministic", "pattern", and "database". In order to complement the
analysis, we also obtain a network representing the relationship between the
adopted areas. Many interesting results were found. First and foremost, several
of the words were observed to have markedly distinct associations in different
areas. Biology was found to be related to computer science, sharing
associations with databases. Furthermore, for most of the cases, the words
"complex", "model", and "prediction" were observed to have several strong
associations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1"&gt;Henrique Ferraz de Arruda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1"&gt;Luciano da Fontoura Costa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08898</id>
        <link href="http://arxiv.org/abs/2102.08898"/>
        <updated>2021-07-21T02:01:33.797Z</updated>
        <summary type="html"><![CDATA[We develop a novel approach to conformal prediction when the target task has
limited data available for training. Conformal prediction identifies a small
set of promising output candidates in place of a single prediction, with
guarantees that the set contains the correct answer with high probability. When
training data is limited, however, the predicted set can easily become unusably
large. In this work, we obtain substantially tighter prediction sets while
maintaining desirable marginal guarantees by casting conformal prediction as a
meta-learning paradigm over exchangeable collections of auxiliary tasks. Our
conformalization algorithm is simple, fast, and agnostic to the choice of
underlying model, learning algorithm, or dataset. We demonstrate the
effectiveness of this approach across a number of few-shot classification and
regression tasks in natural language processing, computer vision, and
computational chemistry for drug discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1"&gt;Adam Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1"&gt;Tal Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11820</id>
        <link href="http://arxiv.org/abs/2012.11820"/>
        <updated>2021-07-21T02:01:33.787Z</updated>
        <summary type="html"><![CDATA[We address the problem of recognizing emotion cause in conversations, define
two novel sub-tasks of this problem, and provide a corresponding dialogue-level
dataset, along with strong Transformer-based baselines. The dataset is
available at https://github.com/declare-lab/RECCON.

Introduction: Recognizing the cause behind emotions in text is a fundamental
yet under-explored area of research in NLP. Advances in this area hold the
potential to improve interpretability and performance in affect-based models.
Identifying emotion causes at the utterance level in conversations is
particularly challenging due to the intermingling dynamics among the
interlocutors.

Method: We introduce the task of Recognizing Emotion Cause in CONversations
with an accompanying dataset named \RECCONDA, containing over 1,000 dialogues
and 10,000 utterance cause-effect pairs. Furthermore, we define different cause
types based on the source of the causes, and establish strong Transformer-based
baselines to address two different sub-tasks on this dataset: causal span
extraction and causal emotion entailment.

Result: Our Transformer-based baselines, which leverage contextual
pre-trained embeddings, such as RoBERTa, outperform the state-of-the-art
emotion cause extraction approaches

Conclusion: We introduce a new task highly relevant for (explainable)
emotion-aware artificial intelligence: recognizing emotion cause in
conversations, provide a new highly challenging publicly available
dialogue-level dataset for this task, and give strong baseline results on this
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1"&gt;Rishabh Bhardwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1"&gt;Samson Yu Bai Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Romila Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhinaba Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1"&gt;Niyati Chhaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering. (arXiv:2012.14610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14610</id>
        <link href="http://arxiv.org/abs/2012.14610"/>
        <updated>2021-07-21T02:01:33.780Z</updated>
        <summary type="html"><![CDATA[We study open-domain question answering with structured, unstructured and
semi-structured knowledge sources, including text, tables, lists and knowledge
bases. Departing from prior work, we propose a unifying approach that
homogenizes all sources by reducing them to text and applies the
retriever-reader model which has so far been limited to text sources only. Our
approach greatly improves the results on knowledge-base QA tasks by 11 points,
compared to latest graph-based methods. More importantly, we demonstrate that
our unified knowledge (UniK-QA) model is a simple and yet effective way to
combine heterogeneous sources of knowledge, advancing the state-of-the-art
results on two popular question answering benchmarks, NaturalQuestions and
WebQuestions, by 3.5 and 2.6 points, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1"&gt;Barlas Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1"&gt;Vladimir Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1"&gt;Stan Peshterliev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1"&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1"&gt;Michael Schlichtkrull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sonal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1"&gt;Scott Yih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical Embeddings. (arXiv:2104.01131v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01131</id>
        <link href="http://arxiv.org/abs/2104.01131"/>
        <updated>2021-07-21T02:01:33.773Z</updated>
        <summary type="html"><![CDATA[Social media plays a pivotal role in disseminating news globally and acts as
a platform for people to express their opinions on various topics. A wide
variety of views accompanies COVID-19 vaccination drives across the globe,
often colored by emotions, which change along with rising cases, approval of
vaccines, and multiple factors discussed online. This study aims at analyzing
the temporal evolution of different Emotion categories: Hesitation, Rage,
Sorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine
Rollout, Misinformation, Health Effects, and Inequities as lexical categories
created from Tweets belonging to five countries with vital vaccine roll-out
programs, namely, India, United States of America, Brazil, United Kingdom, and
Australia. We extracted a corpus of nearly 1.8 million Twitter posts related to
COVID-19 vaccination. Using cosine distance from selected seed words, we
expanded the vocabulary of each category and tracked the longitudinal change in
their strength from June 2020 to April 2021. We used community detection
algorithms to find modules in positive correlation networks. Our findings
suggest that tweets expressing hesitancy towards vaccines contain the highest
mentions of health-related effects in all countries. Our results indicated that
the patterns of hesitancy were variable across geographies and can help us
learn targeted interventions. We also observed a significant change in the
linear trends of categories like hesitation and contentment before and after
approval of vaccines. Negative emotions like rage and sorrow gained the highest
importance in the alluvial diagram. They formed a significant module with all
the influencing factors in April 2021, when India observed the second wave of
COVID-19 cases. The relationship between Emotions and Influencing Factors was
found to be variable across the countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_H/0/1/0/all/0/1"&gt;Harshita Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishtha_A/0/1/0/all/0/1"&gt;Aniket Vashishtha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1"&gt;Ridam Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashima/0/1/0/all/0/1"&gt;Ashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1"&gt;Ananya Tyagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1"&gt;Tavpritesh Sethi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-07-21T02:01:33.752Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The advent and fall of a vocabulary learning bias from communicative efficiency. (arXiv:2105.11519v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11519</id>
        <link href="http://arxiv.org/abs/2105.11519"/>
        <updated>2021-07-21T02:01:33.745Z</updated>
        <summary type="html"><![CDATA[Biosemiosis is a process of choice-making between simultaneously alternative
options. It is well-known that, when sufficiently young children encounter a
new word, they tend to interpret it as pointing to a meaning that does not have
a word yet in their lexicon rather than to a meaning that already has a word
attached. In previous research, the strategy was shown to be optimal from an
information theoretic standpoint. In that framework, interpretation is
hypothesized to be driven by the minimization of a cost function: the option of
least communication cost is chosen. However, the information theoretic model
employed in that research neither explains the weakening of that vocabulary
learning bias in older children or polylinguals nor reproduces Zipf's
meaning-frequency law, namely the non-linear relationship between the number of
meanings of a word and its frequency. Here we consider a generalization of the
model that is channeled to reproduce that law. The analysis of the new model
reveals regions of the phase space where the bias disappears consistently with
the weakening or loss of the bias in older children or polylinguals. The model
is abstract enough to support future research on other levels of life that are
relevant to biosemiotics. In the deep learning era, the model is a transparent
low-dimensional tool for future experimental research and illustrates the
predictive power of a theoretical framework originally designed to shed light
on the origins of Zipf's rank-frequency law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carrera_Casado_D/0/1/0/all/0/1"&gt;David Carrera-Casado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1"&gt;Ramon Ferrer-i-Cancho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14686</id>
        <link href="http://arxiv.org/abs/2105.14686"/>
        <updated>2021-07-21T02:01:33.737Z</updated>
        <summary type="html"><![CDATA[Hyperbolic neural networks have shown great potential for modeling complex
data. However, existing hyperbolic networks are not completely hyperbolic, as
they encode features in a hyperbolic space yet formalize most of their
operations in the tangent space (a Euclidean subspace) at the origin of the
hyperbolic space. This hybrid method greatly limits the modeling ability of
networks. In this paper, we propose a fully hyperbolic framework to build
hyperbolic networks based on the Lorentz model by adapting the Lorentz
transformations (including boost and rotation) to formalize essential
operations of neural networks. Moreover, we also prove that linear
transformation in tangent spaces used by existing hyperbolic networks is a
relaxation of the Lorentz rotation and does not include the boost, implicitly
limiting the capabilities of existing hyperbolic networks. The experimental
results on four NLP tasks show that our method has better performance for
building both shallow and deep networks. Our code will be released to
facilitate follow-up research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weize Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hexu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09648</id>
        <link href="http://arxiv.org/abs/2107.09648"/>
        <updated>2021-07-21T02:01:33.727Z</updated>
        <summary type="html"><![CDATA[Despite being designed for performance rather than cognitive plausibility,
transformer language models have been found to be better at predicting metrics
used to assess human language comprehension than language models with other
architectures, such as recurrent neural networks. Based on how well they
predict the N400, a neural signal associated with processing difficulty, we
propose and provide evidence for one possible explanation - their predictions
are affected by the preceding context in a way analogous to the effect of
semantic facilitation in humans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1"&gt;James A. Michaelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1"&gt;Megan D. Bardolph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1"&gt;Seana Coulson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1"&gt;Benjamin K. Bergen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning ULMFiT and Self-Distillation with Calibration for Medical Dialogue System. (arXiv:2107.09625v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09625</id>
        <link href="http://arxiv.org/abs/2107.09625"/>
        <updated>2021-07-21T02:01:33.710Z</updated>
        <summary type="html"><![CDATA[A medical dialogue system is essential for healthcare service as providing
primary clinical advice and diagnoses. It has been gradually adopted and
practiced in medical organizations in the form of a conversational bot, largely
due to the advancement of NLP. In recent years, the introduction of
state-of-the-art deep learning models and transfer learning techniques like
Universal Language Model Fine Tuning (ULMFiT) and Knowledge Distillation (KD)
largely contributes to the performance of NLP tasks. However, some deep neural
networks are poorly calibrated and wrongly estimate the uncertainty. Hence the
model is not trustworthy, especially in sensitive medical decision-making
systems and safety tasks. In this paper, we investigate the well-calibrated
model for ULMFiT and self-distillation (SD) in a medical dialogue system. The
calibrated ULMFiT (CULMFiT) is obtained by incorporating label smoothing (LS),
a commonly used regularization technique to achieve a well-calibrated model.
Moreover, we apply the technique to recalibrate the confidence score called
temperature scaling (TS) with KD to observe its correlation with network
calibration. To further understand the relation between SD and calibration, we
use both fixed and optimal temperatures to fine-tune the whole model. All
experiments are conducted on the consultation backpain dataset collected by
experts then further validated using a large publicly medial dialogue corpus.
We empirically show that our proposed methodologies outperform conventional
methods in terms of accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ao_S/0/1/0/all/0/1"&gt;Shuang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_X/0/1/0/all/0/1"&gt;Xeno Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StreamBlocks: A compiler for heterogeneous dataflow computing (technical report). (arXiv:2107.09333v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.09333</id>
        <link href="http://arxiv.org/abs/2107.09333"/>
        <updated>2021-07-21T02:01:33.687Z</updated>
        <summary type="html"><![CDATA[To increase performance and efficiency, systems use FPGAs as reconfigurable
accelerators. A key challenge in designing these systems is partitioning
computation between processors and an FPGA. An appropriate division of labor
may be difficult to predict in advance and require experiments and
measurements. When an investigation requires rewriting part of the system in a
new language or with a new programming model, its high cost can retard the
study of different configurations. A single-language system with an appropriate
programming model and compiler that targets both platforms simplifies this
exploration to a simple recompile with new compiler directives.

This work introduces StreamBlocks, an open-source compiler and runtime that
uses the CAL dataflow programming language to partition computations across
heterogeneous (CPU/accelerator) platforms. Because of the dataflow model's
semantics and the CAL language, StreamBlocks can exploit both thread
parallelism in multi-core CPUs and the inherent parallelism of FPGAs.
StreamBlocks supports exploring the design space with a profile-guided tool
that helps identify the best hardware-software partitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bezati_E/0/1/0/all/0/1"&gt;Endri Bezati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emami_M/0/1/0/all/0/1"&gt;Mahyar Emami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janneck_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Janneck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larus_J/0/1/0/all/0/1"&gt;James Larus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09609</id>
        <link href="http://arxiv.org/abs/2107.09609"/>
        <updated>2021-07-21T02:01:33.677Z</updated>
        <summary type="html"><![CDATA[Detecting customized moments and highlights from videos given natural
language (NL) user queries is an important but under-studied topic. One of the
challenges in pursuing this direction is the lack of annotated data. To address
this issue, we present the Query-based Video Highlights (QVHighlights) dataset.
It consists of over 10,000 YouTube videos, covering a wide range of topics,
from everyday activities and travel in lifestyle vlog videos to social and
political activities in news videos. Each video in the dataset is annotated
with: (1) a human-written free-form NL query, (2) relevant moments in the video
w.r.t. the query, and (3) five-point scale saliency scores for all
query-relevant clips. This comprehensive annotation enables us to develop and
evaluate systems that detect relevant moments as well as salient highlights for
diverse, flexible user queries. We also present a strong baseline for this
task, Moment-DETR, a transformer encoder-decoder model that views moment
retrieval as a direct set prediction problem, taking extracted video and query
representations as inputs and predicting moment coordinates and saliency scores
end-to-end. While our model does not utilize any human prior, we show that it
performs competitively when compared to well-engineered architectures. With
weakly supervised pretraining using ASR captions, Moment-DETR substantially
outperforms previous methods. Lastly, we present several ablations and
visualizations of Moment-DETR. Data and code is publicly available at
https://github.com/jayleicn/moment_detr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into Summarization Techniques for IoT Data Discovery Routing. (arXiv:2107.09558v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.09558</id>
        <link href="http://arxiv.org/abs/2107.09558"/>
        <updated>2021-07-21T02:01:33.669Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the IoT data discovery data objects to specific
nodes in the network. They are very problem in very large and growing scale
networks. Specifically, we investigate in depth the routing table summarization
techniques to support effective and space-efficient IoT data discovery routing.
Novel summarization algorithms, including alphabetical based, hash based, and
meaning based summarization and their corresponding coding schemes are
proposed. The issue of potentially misleading routing due to summarization is
also investigated. Subsequently, we analyze the strategy of when to summarize
in order to balance the tradeoff especially in handling MAA based lookups.
between the routing table compression rate and the chance of Unstructured
discovery routing approaches, such as [4] [5], causing misleading routing. For
experimental study, we have collected 100K IoT data streams from various IoT
databases as the input dataset. Experimental results show that our
summarization solution can reduce the routing table size by 20 to 30 folds with
2-5% increase in latency when compared with similar peer-to-peer discovery
routing algorithms without summarization. Also, our approach outperforms DHT
based approaches by 2 to 6 folds in terms of latency and traffic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hieu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1"&gt;I-Ling Yen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1"&gt;Farokh Bastani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoningKnife: Joint Entity Mention Detection and Typing for Nested NER via prior Boundary Knowledge. (arXiv:2107.09429v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09429</id>
        <link href="http://arxiv.org/abs/2107.09429"/>
        <updated>2021-07-21T02:01:33.660Z</updated>
        <summary type="html"><![CDATA[While named entity recognition (NER) is a key task in natural language
processing, most approaches only target flat entities, ignoring nested
structures which are common in many scenarios. Most existing nested NER methods
traverse all sub-sequences which is both expensive and inefficient, and also
don't well consider boundary knowledge which is significant for nested
entities. In this paper, we propose a joint entity mention detection and typing
model via prior boundary knowledge (BoningKnife) to better handle nested NER
extraction and recognition tasks. BoningKnife consists of two modules,
MentionTagger and TypeClassifier. MentionTagger better leverages boundary
knowledge beyond just entity start/end to improve the handling of nesting
levels and longer spans, while generating high quality mention candidates.
TypeClassifier utilizes a two-level attention mechanism to decouple different
nested level representations and better distinguish entity types. We jointly
train both modules sharing a common representation and a new dual-info
attention layer, which leads to improved representation focus on entity-related
information. Experiments over different datasets show that our approach
outperforms previous state of the art methods and achieves 86.41, 85.46, and
94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huiqiang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weile Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengxi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1"&gt;B&amp;#xf6;rje F. Karlsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seed Words Based Data Selection for Language Model Adaptation. (arXiv:2107.09433v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09433</id>
        <link href="http://arxiv.org/abs/2107.09433"/>
        <updated>2021-07-21T02:01:33.651Z</updated>
        <summary type="html"><![CDATA[We address the problem of language model customization in applications where
the ASR component needs to manage domain-specific terminology; although current
state-of-the-art speech recognition technology provides excellent results for
generic domains, the adaptation to specialized dictionaries or glossaries is
still an open issue. In this work we present an approach for automatically
selecting sentences, from a text corpus, that match, both semantically and
morphologically, a glossary of terms (words or composite words) furnished by
the user. The final goal is to rapidly adapt the language model of an hybrid
ASR system with a limited amount of in-domain text data in order to
successfully cope with the linguistic domain at hand; the vocabulary of the
baseline model is expanded and tailored, reducing the resulting OOV rate. Data
selection strategies based on shallow morphological seeds and semantic
similarity viaword2vec are introduced and discussed; the experimental setting
consists in a simultaneous interpreting scenario, where ASRs in three languages
are designed to recognize the domain-specific terms (i.e. dentistry). Results
using different metrics (OOV rate, WER, precision and recall) show the
effectiveness of the proposed techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1"&gt;Roberto Gretter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1"&gt;Marco Matassoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1"&gt;Daniele Falavigna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Prosody Modeling for ASR+TTS based Voice Conversion. (arXiv:2107.09477v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09477</id>
        <link href="http://arxiv.org/abs/2107.09477"/>
        <updated>2021-07-21T02:01:33.628Z</updated>
        <summary type="html"><![CDATA[In voice conversion (VC), an approach showing promising results in the latest
voice conversion challenge (VCC) 2020 is to first use an automatic speech
recognition (ASR) model to transcribe the source speech into the underlying
linguistic contents; these are then used as input by a text-to-speech (TTS)
system to generate the converted speech. Such a paradigm, referred to as
ASR+TTS, overlooks the modeling of prosody, which plays an important role in
speech naturalness and conversion similarity. Although some researchers have
considered transferring prosodic clues from the source speech, there arises a
speaker mismatch during training and conversion. To address this issue, in this
work, we propose to directly predict prosody from the linguistic representation
in a target-speaker-dependent manner, referred to as target text prediction
(TTP). We evaluate both methods on the VCC2020 benchmark and consider different
linguistic representations. The results demonstrate the effectiveness of TTP in
both objective and subjective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1"&gt;Tomoki Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset. (arXiv:2107.09556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09556</id>
        <link href="http://arxiv.org/abs/2107.09556"/>
        <updated>2021-07-21T02:01:33.596Z</updated>
        <summary type="html"><![CDATA[We present a new dataset of Wikipedia articles each paired with a knowledge
graph, to facilitate the research in conditional text generation, graph
generation and graph representation learning. Existing graph-text paired
datasets typically contain small graphs and short text (1 or few sentences),
thus limiting the capabilities of the models that can be learned on the data.
Our new dataset WikiGraphs is collected by pairing each Wikipedia article from
the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph
from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy
to benchmark against other state-of-the-art text generative models that are
capable of generating long paragraphs of coherent text. Both the graphs and the
text data are of significantly larger scale compared to prior graph-text paired
datasets. We present baseline graph neural network and transformer model
results on our dataset for 3 tasks: graph -> text generation, graph -> text
retrieval and text -> graph retrieval. We show that better conditioning on the
graph provides gains in generation and retrieval quality but there is still
large room for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Luyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aslan_O/0/1/0/all/0/1"&gt;Ozlem Aslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09332</id>
        <link href="http://arxiv.org/abs/2107.09332"/>
        <updated>2021-07-21T02:01:33.587Z</updated>
        <summary type="html"><![CDATA[The sentence-level relation extraction mainly aims to classify the relation
between two entities in a sentence. The sentence-level relation extraction
corpus is often containing data of difficulty for the model to infer or noise
data. In this paper, we propose a curriculum learning-based relation extraction
model that split data by difficulty and utilize it for learning. In the
experiments with the representative sentence-level relation extraction
datasets, TACRED and Re-TACRED, the proposed method showed good performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongsik Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Harksoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paraphrasing via Ranking Many Candidates. (arXiv:2107.09274v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09274</id>
        <link href="http://arxiv.org/abs/2107.09274"/>
        <updated>2021-07-21T02:01:33.251Z</updated>
        <summary type="html"><![CDATA[We present a simple and effective way to generate a variety of paraphrases
and find a good quality paraphrase among them. As in previous studies, it is
difficult to ensure that one generation method always generates the best
paraphrase in various domains. Therefore, we focus on finding the best
candidate from multiple candidates, rather than assuming that there is only one
combination of generative models and decoding options. Our approach shows that
it is easy to apply in various domains and has sufficiently good performance
compared to previous methods. In addition, our approach can be used for data
agumentation that extends the downstream corpus, showing that it can help
improve performance in English and Korean datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joosung Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09262</id>
        <link href="http://arxiv.org/abs/2107.09262"/>
        <updated>2021-07-21T02:01:33.208Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Sanchita Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1"&gt;John J. Prevost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown FoV Viewing Probabilities in Wireless Networks. (arXiv:2107.09491v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.09491</id>
        <link href="http://arxiv.org/abs/2107.09491"/>
        <updated>2021-07-21T02:01:33.193Z</updated>
        <summary type="html"><![CDATA[This paper investigates adaptive streaming of one or multiple tiled 360
videos from a multi-antenna base station (BS) to one or multiple single-antenna
users, respectively, in a multi-carrier wireless system. We aim to maximize the
video quality while keeping rebuffering time small via encoding rate adaptation
at each group of pictures (GOP) and transmission adaptation at each
(transmission) slot. To capture the impact of field-of-view (FoV) prediction,
we consider three cases of FoV viewing probability distributions, i.e.,
perfect, imperfect, and unknown FoV viewing probability distributions, and use
the average total utility, worst average total utility, and worst total utility
as the respective performance metrics. In the single-user scenario, we optimize
the encoding rates of the tiles, encoding rates of the FoVs, and transmission
beamforming vectors for all subcarriers to maximize the total utility in each
case. In the multi-user scenario, we adopt rate splitting with successive
decoding and optimize the encoding rates of the tiles, encoding rates of the
FoVs, rates of the common and private messages, and transmission beamforming
vectors for all subcarriers to maximize the total utility in each case. Then,
we separate the challenging optimization problem into multiple tractable
problems in each scenario. In the single-user scenario, we obtain a globally
optimal solution of each problem using transformation techniques and the
Karush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a
KKT point of each problem using the concave-convex procedure (CCCP). Finally,
numerical results demonstrate that the proposed solutions achieve notable gains
over existing schemes in all three cases. To the best of our knowledge, this is
the first work revealing the impact of FoV prediction on the performance of
adaptive streaming of tiled 360 videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lingzhi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Ying Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sheng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09293</id>
        <link href="http://arxiv.org/abs/2107.09293"/>
        <updated>2021-07-21T02:01:33.170Z</updated>
        <summary type="html"><![CDATA[We propose an audio-driven talking-head method to generate photo-realistic
talking-head videos from a single reference image. In this work, we tackle two
key challenges: (i) producing natural head motions that match speech prosody,
and (ii) maintaining the appearance of a speaker in a large head motion while
stabilizing the non-face regions. We first design a head pose predictor by
modeling rigid 6D head movements with a motion-aware recurrent neural network
(RNN). In this way, the predicted head poses act as the low-frequency holistic
movements of a talking head, thus allowing our latter network to focus on
detailed facial movement generation. To depict the entire image motions arising
from audio, we exploit a keypoint based dense motion field representation.
Then, we develop a motion field generator to produce the dense motion fields
from input audio, head poses, and a reference image. As this keypoint based
representation models the motions of facial regions, head, and backgrounds
integrally, our method can better constrain the spatial and temporal
consistency of the generated videos. Finally, an image generation network is
employed to render photo-realistic talking-head videos from the estimated
keypoint based motion fields and the input reference image. Extensive
experiments demonstrate that our method produces videos with plausible head
motions, synchronized facial expressions, and stable backgrounds and
outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lincheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09278</id>
        <link href="http://arxiv.org/abs/2107.09278"/>
        <updated>2021-07-21T02:01:33.158Z</updated>
        <summary type="html"><![CDATA[Transcripts generated by automatic speech recognition (ASR) systems for
spoken documents lack structural annotations such as paragraphs, significantly
reducing their readability. Automatically predicting paragraph segmentation for
spoken documents may both improve readability and downstream NLP performance
such as summarization and machine reading comprehension. We propose a sequence
model with self-adaptive sliding window for accurate and efficient paragraph
segmentation. We also propose an approach to exploit phonetic information,
which significantly improves robustness of spoken document segmentation to ASR
errors. Evaluations are conducted on the English Wiki-727K document
segmentation benchmark, a Chinese Wikipedia-based document segmentation dataset
we created, and an in-house Chinese spoken document dataset. Our proposed model
outperforms the state-of-the-art (SOTA) model based on the same BERT-Base,
increasing segmentation F1 on the English benchmark by 4.2 points and on
Chinese datasets by 4.3-10.1 points, while reducing inference time to less than
1/6 of inference time of the current SOTA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yali Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.09055</id>
        <link href="http://arxiv.org/abs/2107.09055"/>
        <updated>2021-07-21T02:01:33.143Z</updated>
        <summary type="html"><![CDATA[The stock market has been a popular topic of interest in the recent past. The
growth in the inflation rate has compelled people to invest in the stock and
commodity markets and other areas rather than saving. Further, the ability of
Deep Learning models to make predictions on the time series data has been
proven time and again. Technical analysis on the stock market with the help of
technical indicators has been the most common practice among traders and
investors. One more aspect is the sentiment analysis - the emotion of the
investors that shows the willingness to invest. A variety of techniques have
been used by people around the globe involving basic Machine Learning and
Neural Networks. Ranging from the basic linear regression to the advanced
neural networks people have experimented with all possible techniques to
predict the stock market. It's evident from recent events how news and
headlines affect the stock markets and cryptocurrencies. This paper proposes an
ensemble of state-of-the-art methods for predicting stock prices. Firstly
sentiment analysis of the news and the headlines for the company Apple Inc,
listed on the NASDAQ is performed using a version of BERT, which is a
pre-trained transformer model by Google for Natural Language Processing (NLP).
Afterward, a Generative Adversarial Network (GAN) predicts the stock price for
Apple Inc using the technical indicators, stock indexes of various countries,
some commodities, and historical prices along with the sentiment scores.
Comparison is done with baseline models like - Long Short Term Memory (LSTM),
Gated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving
Average (ARIMA) model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1"&gt;Priyank Sonkiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1"&gt;Vikas Bajpai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Anukriti Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09106</id>
        <link href="http://arxiv.org/abs/2107.09106"/>
        <updated>2021-07-21T02:01:33.123Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1"&gt;Spencer Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and Isometric Conditions. (arXiv:2107.09186v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09186</id>
        <link href="http://arxiv.org/abs/2107.09186"/>
        <updated>2021-07-21T02:01:33.080Z</updated>
        <summary type="html"><![CDATA[Typically, a linearly orthogonal transformation mapping is learned by
aligning static type-level embeddings to build a shared semantic space. In view
of the analysis that contextual embeddings contain richer semantic features, we
investigate a context-aware and dictionary-free mapping approach by leveraging
parallel corpora. We illustrate that our contextual embedding space mapping
significantly outperforms previous multilingual word embedding methods on the
bilingual dictionary induction (BDI) task by providing a higher degree of
isomorphism. To improve the quality of mapping, we also explore sense-level
embeddings that are split from type-level representations, which can align
spaces in a finer resolution and yield more precise mapping. Moreover, we
reveal that contextual embedding spaces suffer from their natural properties --
anisotropy and anisometry. To mitigate these two problems, we introduce the
iterative normalization algorithm as an imperative preprocessing step. Our
findings unfold the tight relationship between isotropy, isometry, and
isomorphism in normalized contextual embedding spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haoran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1"&gt;Philipp Koehn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09240</id>
        <link href="http://arxiv.org/abs/2107.09240"/>
        <updated>2021-07-21T02:01:33.069Z</updated>
        <summary type="html"><![CDATA[Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Fu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06252</id>
        <link href="http://arxiv.org/abs/2107.06252"/>
        <updated>2021-07-21T02:01:33.048Z</updated>
        <summary type="html"><![CDATA[Dance and music typically go hand in hand. The complexities in dance, music,
and their synchronisation make them fascinating to study from a computational
creativity perspective. While several works have looked at generating dance for
a given music, automatically generating music for a given dance remains
under-explored. This capability could have several creative expression and
entertainment applications. We present some early explorations in this
direction. We present a search-based offline approach that generates music
after processing the entire dance video and an online approach that uses a deep
neural network to generate music on-the-fly as the video proceeds. We compare
these approaches to a strong heuristic baseline via human studies and present
our findings. We have integrated our online approach in a live demo! A video of
the demo can be found here:
https://sites.google.com/view/dance2music/live-demo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1"&gt;Gunjan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning. (arXiv:2107.09285v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09285</id>
        <link href="http://arxiv.org/abs/2107.09285"/>
        <updated>2021-07-21T02:01:33.033Z</updated>
        <summary type="html"><![CDATA[Although virtual agents are increasingly situated in environments where
natural language is the most effective mode of interaction with humans, these
exchanges are rarely used as an opportunity for learning. Leveraging language
interactions effectively requires addressing limitations in the two most common
approaches to language grounding: semantic parsers built on top of fixed object
categories are precise but inflexible and end-to-end models are maximally
expressive, but fickle and opaque. Our goal is to develop a system that
balances the strengths of each approach so that users can teach agents new
instructions that generalize broadly from a single example. We introduce the
idea of neural abstructions: a set of constraints on the inference procedure of
a label-conditioned generative model that can affect the meaning of the label
in context. Starting from a core programming language that operates over
abstructions, users can define increasingly complex mappings from natural
language to actions. We show that with this method a user population is able to
build a semantic parser for an open-ended house modification task in Minecraft.
The semantic parser that results is both flexible and expressive: the
percentage of utterances sourced from redefinitions increases steadily over the
course of 191 total exchanges, achieving a final value of 28%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burns_K/0/1/0/all/0/1"&gt;Kaylee Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1"&gt;Christopher D. Manning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09099</id>
        <link href="http://arxiv.org/abs/2107.09099"/>
        <updated>2021-07-21T02:01:33.001Z</updated>
        <summary type="html"><![CDATA[Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiushi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Tom Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;H Lilian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xubo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12537</id>
        <link href="http://arxiv.org/abs/2010.12537"/>
        <updated>2021-07-21T02:01:32.983Z</updated>
        <summary type="html"><![CDATA[Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiruo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ran Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.09480</id>
        <link href="http://arxiv.org/abs/2107.09480"/>
        <updated>2021-07-21T02:01:32.669Z</updated>
        <summary type="html"><![CDATA[Sorted Table Search Procedures are the quintessential query-answering tool,
still very useful, e.g, Search Engines (Google Chrome). Speeding them up, in
small additional space with respect to the table being searched into, is still
a quite significant achievement. Static Learned Indexes have been very
successful in achieving such a speed-up, but leave open a major question: To
what extent one can enjoy the speed-up of Learned Indexes while using constant
or nearly constant additional space. By generalizing the experimental
methodology of a recent benchmarking study on Learned Indexes, we shed light on
this question, by considering two scenarios. The first, quite elementary, i.e.,
textbook code, and the second using advanced Learned Indexing algorithms and
the supporting sophisticated software platforms. Although in both cases one
would expect a positive answer, its achievement is not as simple as it seems.
Indeed, our extensive set of experiments reveal a complex relationship between
query time and model space. The findings regarding this relationship and the
corresponding quantitative estimates, across memory levels, can be of interest
to algorithm designers and of use to practitioners as well. As an essential
part of our research, we introduce two new models that are of interest in their
own right. The first is a constant space model that can be seen as a
generalization of $k$-ary search, while the second is a synoptic {\bf RMI}, in
which we can control model space usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1"&gt;Domenico Amato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1"&gt;Raffaele Giancarlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1"&gt;Giosu&amp;#xe8; Lo Bosco&lt;/a&gt;</name>
        </author>
    </entry>
</feed>