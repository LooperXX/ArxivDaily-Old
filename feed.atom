<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-10T00:27:39.819Z</updated>
    <generator>osmosfeed 1.10.2</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Launchpad: A Programming Model for Distributed Machine Learning Research. (arXiv:2106.04516v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.04516</id>
        <link href="http://arxiv.org/abs/2106.04516"/>
        <updated>2021-06-10T00:27:39.761Z</updated>
        <summary type="html"><![CDATA[A major driver behind the success of modern machine learning algorithms has
been their ability to process ever-larger amounts of data. As a result, the use
of distributed systems in both research and production has become increasingly
prevalent as a means to scale to this growing data. At the same time, however,
distributing the learning process can drastically complicate the implementation
of even simple algorithms. This is especially problematic as many machine
learning practitioners are not well-versed in the design of distributed
systems, let alone those that have complicated communication topologies. In
this work we introduce Launchpad, a programming model that simplifies the
process of defining and launching distributed systems that is specifically
tailored towards a machine learning audience. We describe our framework, its
design philosophy and implementation, and give a number of examples of common
learning algorithms whose designs are greatly simplified by this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barth_Maron_G/0/1/0/all/0/1"&gt;Gabriel Barth-Maron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1"&gt;Piotr Sta&amp;#x144;czyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1"&gt;Matthew Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroiss_M/0/1/0/all/0/1"&gt;Manuel Kroiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pope_A/0/1/0/all/0/1"&gt;Aedan Pope&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rrustemi_A/0/1/0/all/0/1"&gt;Alban Rrustemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretic Measures for Fairness-aware Feature Selection. (arXiv:2106.00772v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00772</id>
        <link href="http://arxiv.org/abs/2106.00772"/>
        <updated>2021-06-09T22:43:50.610Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms are increasingly used for consequential decision
making regarding individuals based on their relevant features. Features that
are relevant for accurate decisions may however lead to either explicit or
implicit forms of discrimination against unprivileged groups, such as those of
certain race or gender. This happens due to existing biases in the training
data, which are often replicated or even exacerbated by the learning algorithm.
Identifying and measuring these biases at the data level is a challenging
problem due to the interdependence among the features, and the decision
outcome. In this work, we develop a framework for fairness-aware feature
selection which takes into account the correlation among the features and the
decision outcome, and is based on information theoretic measures for the
accuracy and discriminatory impacts of features. In particular, we first
propose information theoretic measures which quantify the impact of different
subsets of features on the accuracy and discrimination of the decision
outcomes. We then deduce the marginal impact of each feature using Shapley
value function; a solution concept in cooperative game theory used to estimate
marginal contributions of players in a coalitional game. Finally, we design a
fairness utility score for each feature (for feature selection) which
quantifies how this feature influences accurate as well as nondiscriminatory
decisions. Our framework depends on the joint statistics of the data rather
than a particular classifier design. We examine our proposed framework on real
and synthetic data to evaluate its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1"&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nafea_M/0/1/0/all/0/1"&gt;Mohamed Nafea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1"&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1"&gt;Negar Kiyavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Active Surface Models. (arXiv:2011.08826v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08826</id>
        <link href="http://arxiv.org/abs/2011.08826"/>
        <updated>2021-06-09T22:43:50.598Z</updated>
        <summary type="html"><![CDATA[Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1"&gt;Graham Knott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalizable Approach to Learning Optimizers. (arXiv:2106.00958v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00958</id>
        <link href="http://arxiv.org/abs/2106.00958"/>
        <updated>2021-06-09T22:43:50.588Z</updated>
        <summary type="html"><![CDATA[A core issue with learning to optimize neural networks has been the lack of
generalization to real world problems. To address this, we describe a system
designed from a generalization-first perspective, learning to update optimizer
hyperparameters instead of model parameters directly using novel features,
actions, and a reward function. This system outperforms Adam at all neural
network tasks including on modalities not seen during training. We achieve 2x
speedups on ImageNet, and a 2.5x speedup on a language modeling task using over
5 orders of magnitude more compute than the training tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1"&gt;Diogo Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1"&gt;Clemens Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1"&gt;Wojciech Zaremba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Bias Metrics Do Not Correlate with Application Bias. (arXiv:2012.15859v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15859</id>
        <link href="http://arxiv.org/abs/2012.15859"/>
        <updated>2021-06-09T22:43:50.385Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) systems learn harmful societal biases that
cause them to amplify inequality as they are deployed in more and more
situations. To guide efforts at debiasing these systems, the NLP community
relies on a variety of metrics that quantify bias in models. Some of these
metrics are intrinsic, measuring bias in word embedding spaces, and some are
extrinsic, measuring bias in downstream tasks that the word embeddings enable.
Do these intrinsic and extrinsic metrics correlate with each other? We compare
intrinsic and extrinsic metrics across hundreds of trained models covering
different tasks and experimental conditions. Our results show no reliable
correlation between these metrics that holds in all scenarios across tasks and
languages. We urge researchers working on debiasing to focus on extrinsic
measures of bias, and to make using these measures more feasible via creation
of new challenge sets and annotated test data. To aid this effort, we release
code, a new intrinsic metric, and an annotated test set focused on gender bias
in hate speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1"&gt;Seraphina Goldfarb-Tarrant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchant_R/0/1/0/all/0/1"&gt;Rebecca Marchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1"&gt;Ricardo Mu&amp;#xf1;oz Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandya_M/0/1/0/all/0/1"&gt;Mugdha Pandya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1"&gt;Adam Lopez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Active Surface Models. (arXiv:2011.08826v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08826</id>
        <link href="http://arxiv.org/abs/2011.08826"/>
        <updated>2021-06-09T22:43:50.359Z</updated>
        <summary type="html"><![CDATA[Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1"&gt;Graham Knott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic. (arXiv:2101.01785v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01785</id>
        <link href="http://arxiv.org/abs/2101.01785"/>
        <updated>2021-06-09T22:43:50.339Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (LMs) are currently integral to many natural
language processing systems. Although multilingual LMs were also introduced to
serve many languages, these have limitations such as being costly at inference
time and the size and diversity of non-English data involved in their
pre-training. We remedy these issues for a collection of diverse Arabic
varieties by introducing two powerful deep bidirectional transformer-based
models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a
new benchmark for multi-dialectal Arabic language understanding evaluation.
ARLUE is built using 42 datasets targeting six different task clusters,
allowing us to offer a series of standardized experiments under rich
conditions. When fine-tuned on ARLUE, our models collectively achieve new
state-of-the-art results across the majority of tasks (37 out of 48
classification tasks, on the 42 datasets). Our best model acquires the highest
ARLUE score (77.40) across all six task clusters, outperforming all other
models including XLM-R Large (~ 3.4 x larger size). Our models are publicly
available at https://github.com/UBC-NLP/marbert and ARLUE will be released
through the same repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1"&gt;AbdelRahim Elmadany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1"&gt;El Moatez Billah Nagoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Sparse Attention more Interpretable?. (arXiv:2106.01087v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01087</id>
        <link href="http://arxiv.org/abs/2106.01087"/>
        <updated>2021-06-09T22:43:50.188Z</updated>
        <summary type="html"><![CDATA[Sparse attention has been claimed to increase model interpretability under
the assumption that it highlights influential inputs. Yet the attention
distribution is typically over representations internal to the model rather
than the inputs themselves, suggesting this assumption may not have merit. We
build on the recent work exploring the interpretability of attention; we design
a set of experiments to help us understand how sparsity affects our ability to
use attention as an explainability tool. On three text classification tasks, we
verify that only a weak relationship between inputs and co-indexed intermediate
representations exists -- under sparse attention and otherwise. Further, we do
not find any plausible mappings from sparse attention distributions to a sparse
set of influential inputs through other avenues. Rather, we observe in this
setting that inducing sparsity may make it less plausible that attention can be
used as a tool for understanding model behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazov_S/0/1/0/all/0/1"&gt;Stefan Lazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (arXiv:2104.14528v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14528</id>
        <link href="http://arxiv.org/abs/2104.14528"/>
        <updated>2021-06-09T02:01:52.019Z</updated>
        <summary type="html"><![CDATA[Existing deep learning methods for diagnosis of gastric cancer commonly use
convolutional neural network. Recently, the Visual Transformer has attracted
great attention because of its performance and efficiency, but its applications
are mostly in the field of computer vision. In this paper, a multi-scale visual
transformer model, referred to as GasHis-Transformer, is proposed for Gastric
Histopathological Image Classification (GHIC), which enables the automatic
classification of microscopic gastric images into abnormal and normal cases.
The GasHis-Transformer model consists of two key modules: A global information
module and a local information module to extract histopathological features
effectively. In our experiments, a public hematoxylin and eosin (H&E) stained
gastric histopathological dataset with 280 abnormal and normal images are
divided into training, validation and test sets by a ratio of 1 : 1 : 2. The
GasHis-Transformer model is applied to estimate precision, recall, F1-score and
accuracy on the test set of gastric histopathological dataset as 98.0%, 100.0%,
96.0% and 98.0%, respectively. Furthermore, a critical study is conducted to
evaluate the robustness of GasHis-Transformer, where ten different noises
including four adversarial attack and six conventional image noises are added.
In addition, a clinically meaningful study is executed to test the
gastrointestinal cancer identification performance of GasHis-Transformer with
620 abnormal images and achieves 96.8% accuracy. Finally, a comparative study
is performed to test the generalizability with both H&E and immunohistochemical
stained images on a lymphoma image dataset and a breast cancer dataset,
producing comparable F1-scores (85.6% and 82.8%) and accuracies (83.9% and
89.4%), respectively. In conclusion, GasHisTransformer demonstrates high
classification performance and shows its significant potential in the GHIC
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yudong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yueyang Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph Clustering. (arXiv:2105.05320v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05320</id>
        <link href="http://arxiv.org/abs/2105.05320"/>
        <updated>2021-06-09T02:01:52.013Z</updated>
        <summary type="html"><![CDATA[Recently, there has been considerable research interest in graph clustering
aimed at data partition using the graph information. However, one limitation of
the most of graph-based methods is that they assume the graph structure to
operate is fixed and reliable. And there are inevitably some edges in the graph
that are not conducive to graph clustering, which we call spurious edges. This
paper is the first attempt to employ graph pooling technique for node
clustering and we propose a novel dual graph embedding network (DGEN), which is
designed as a two-step graph encoder connected by a graph pooling layer to
learn the graph embedding. In our model, it is assumed that if a node and its
nearest neighboring node are close to the same clustering center, this node is
an informative node and this edge can be considered as a cluster-friendly edge.
Based on this assumption, the neighbor cluster pooling (NCPool) is devised to
select the most informative subset of nodes and the corresponding edges based
on the distance of nodes and their nearest neighbors to the cluster centers.
This can effectively alleviate the impact of the spurious edges on the
clustering. Finally, to obtain the clustering assignment of all nodes, a
classifier is trained using the clustering results of the selected nodes.
Experiments on five benchmark graph datasets demonstrate the superiority of the
proposed method over state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dongxia Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyTr^2: Unbiased Image Style Transfer with Transformers. (arXiv:2105.14576v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14576</id>
        <link href="http://arxiv.org/abs/2105.14576"/>
        <updated>2021-06-09T02:01:52.007Z</updated>
        <summary type="html"><![CDATA[The goal of image style transfer is to render an image with artistic features
guided by a style reference while maintaining the original content. Due to the
locality and spatial invariance in CNNs, it is difficult to extract and
maintain the global information of input images. Therefore, traditional neural
style transfer methods are usually biased and content leak can be observed by
running several times of the style transfer process with the same reference
style image. To address this critical issue, we take long-range dependencies of
input images into account for unbiased style transfer by proposing a
transformer-based approach, namely StyTr^2. In contrast with visual
transformers for other vision tasks, our StyTr^2 contains two different
transformer encoders to generate domain-specific sequences for content and
style, respectively. Following the encoders, a multi-layer transformer decoder
is adopted to stylize the content sequence according to the style sequence. In
addition, we analyze the deficiency of existing positional encoding methods and
propose the content-aware positional encoding (CAPE) which is scale-invariant
and more suitable for image style transfer task. Qualitative and quantitative
experiments demonstrate the effectiveness of the proposed StyTr^2 compared to
state-of-the-art CNN-based and flow-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yingying Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weiming Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08604</id>
        <link href="http://arxiv.org/abs/2102.08604"/>
        <updated>2021-06-09T02:01:52.002Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) methods aim to achieve generalizability to an
unseen target domain by using only training data from the source domains.
Although a variety of DG methods have been proposed, a recent study shows that
under a fair evaluation protocol, called DomainBed, the simple empirical risk
minimization (ERM) approach works comparable to or even outperforms previous
methods. Unfortunately, simply solving ERM on a complex, non-convex loss
function can easily lead to sub-optimal generalizability by seeking sharp
minima. In this paper, we theoretically show that finding flat minima results
in a smaller domain generalization gap. We also propose a simple yet effective
method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.
SWAD finds flatter minima and suffers less from overfitting than does the
vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.
SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,
VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large
margins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with
conventional generalization methods, such as data augmentation and consistency
regularization methods, to verify that the remarkable performance improvements
are originated from by seeking flat minima, not from better in-domain
generalizability. Last but not least, SWAD is readily adaptable to existing DG
methods without modification; the combination of SWAD and an existing DG method
further improves DG performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1"&gt;Junbum Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Han-Cheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04274</id>
        <link href="http://arxiv.org/abs/2106.04274"/>
        <updated>2021-06-09T02:01:51.996Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is still a challenging problem despite the large
amount of work that has been done in this field. Generally, most methods
directly use neural networks and ignore certain constraints (e.g., reprojection
constraints and joint angle and bone length constraints). This paper proposes a
weakly supervised GAN-based model for 3D human pose estimation that considers
3D information along with 2D information simultaneously, in which a
reprojection network is employed to learn the mapping of the distribution from
3D poses to 2D poses. In particular, we train the reprojection network and the
generative adversarial network synchronously. Furthermore, inspired by the
typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,
which is added into the discriminator's input to impose joint angle and bone
length constraints. The experimental results on Human3.6M show that our method
outperforms state-of-the-art methods by approximately 5.1\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interaction-GCN: A Graph Convolutional Network based framework for social interaction recognition in egocentric videos. (arXiv:2104.14007v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14007</id>
        <link href="http://arxiv.org/abs/2104.14007"/>
        <updated>2021-06-09T02:01:51.979Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a new framework to categorize social interactions in
egocentric videos, we named InteractionGCN. Our method extracts patterns of
relational and non-relational cues at the frame level and uses them to build a
relational graph from which the interactional context at the frame level is
estimated via a Graph Convolutional Network based approach. Then it propagates
this context over time, together with first-person motion information, through
a Gated Recurrent Unit architecture. Ablation studies and experimental
evaluation on two publicly available datasets validate the proposed approach
and establish state of the art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Felicioni_S/0/1/0/all/0/1"&gt;Simone Felicioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1"&gt;Mariella Dimiccoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Shifted Contrastive Loss for Anomaly Detection. (arXiv:2106.03844v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.03844</id>
        <link href="http://arxiv.org/abs/2106.03844"/>
        <updated>2021-06-09T02:01:51.967Z</updated>
        <summary type="html"><![CDATA[Deep anomaly detection methods learn representations that separate between
normal and anomalous samples. Very effective representations are obtained when
powerful externally trained feature extractors (e.g. ResNets pre-trained on
ImageNet) are fine-tuned on the training data which consists of normal samples
and no anomalies. However, this is a difficult task that can suffer from
catastrophic collapse, i.e. it is prone to learning trivial and non-specific
features. In this paper, we propose a new loss function which can overcome
failure modes of both center-loss and contrastive-loss methods. Furthermore, we
combine it with a confidence-invariant angular center loss, which replaces the
Euclidean distance used in previous work, that was sensitive to prediction
confidence. Our improvements yield a new anomaly detection approach, based on
$\textit{Mean-Shifted Contrastive Loss}$, which is both more accurate and less
sensitive to catastrophic collapse than previous methods. Our method achieves
state-of-the-art anomaly detection performance on multiple benchmarks including
$97.5\%$ ROC-AUC on the CIFAR-10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiss_T/0/1/0/all/0/1"&gt;Tal Reiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NaturalProofs: Mathematical Theorem Proving in Natural Language. (arXiv:2104.01112v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01112</id>
        <link href="http://arxiv.org/abs/2104.01112"/>
        <updated>2021-06-09T02:01:51.890Z</updated>
        <summary type="html"><![CDATA[Understanding and creating mathematics using natural mathematical language -
the mixture of symbolic and natural language used by humans - is a challenging
and important problem for driving progress in machine learning. As a step in
this direction, we develop NaturalProofs, a multi-domain corpus of mathematical
statements and their proofs, written in natural mathematical language.
NaturalProofs unifies broad coverage, deep coverage, and low-resource
mathematical sources, allowing for evaluating both in-distribution and
zero-shot generalization. Using NaturalProofs, we benchmark strong neural
methods on mathematical reference retrieval and generation tasks which test a
system's ability to determine key results that appear in a proof. Large-scale
sequence models show promise compared to classical information retrieval
methods, yet their performance and out-of-domain generalization leave
substantial room for improvement. NaturalProofs opens many avenues for research
on challenging mathematical tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1"&gt;Ronan Le Bras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Best Arm Identification Under a Deadline. (arXiv:2106.03221v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03221</id>
        <link href="http://arxiv.org/abs/2106.03221"/>
        <updated>2021-06-09T02:01:51.875Z</updated>
        <summary type="html"><![CDATA[We study $(\epsilon, \delta)$-PAC best arm identification, where a
decision-maker must identify an $\epsilon$-optimal arm with probability at
least $1 - \delta$, while minimizing the number of arm pulls (samples). Most of
the work on this topic is in the sequential setting, where there is no
constraint on the time taken to identify such an arm; this allows the
decision-maker to pull one arm at a time. In this work, the decision-maker is
given a deadline of $T$ rounds, where, on each round, it can adaptively choose
which arms to pull and how many times to pull them; this distinguishes the
number of decisions made (i.e., time or number of rounds) from the number of
samples acquired (cost). Such situations occur in clinical trials, where one
may need to identify a promising treatment under a deadline while minimizing
the number of test subjects, or in simulation-based studies run on the cloud,
where we can elastically scale up or down the number of virtual machines to
conduct as many experiments as we wish, but need to pay for the resource-time
used. As the decision-maker can only make $T$ decisions, she may need to pull
some arms excessively relative to a sequential algorithm in order to perform
well on all possible problems. We formalize this added difficulty with two
hardness results that indicate that unlike sequential settings, the ability to
adapt to the problem difficulty is constrained by the finite deadline. We
propose Elastic Batch Racing (EBR), a novel algorithm for this setting and
bound its sample complexity, showing that EBR is optimal with respect to both
hardness results. We present simulations evaluating EBR in this setting, where
it outperforms baselines by several orders of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1"&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Whitening Batch Normalization. (arXiv:2106.04413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04413</id>
        <link href="http://arxiv.org/abs/2106.04413"/>
        <updated>2021-06-09T02:01:51.869Z</updated>
        <summary type="html"><![CDATA[Batch Normalization (BN) is a popular technique for training Deep Neural
Networks (DNNs). BN uses scaling and shifting to normalize activations of
mini-batches to accelerate convergence and improve generalization. The recently
proposed Iterative Normalization (IterNorm) method improves these properties by
whitening the activations iteratively using Newton's method. However, since
Newton's method initializes the whitening matrix independently at each training
step, no information is shared between consecutive steps. In this work, instead
of exact computation of whitening matrix at each time step, we estimate it
gradually during training in an online fashion, using our proposed Stochastic
Whitening Batch Normalization (SWBN) algorithm. We show that while SWBN
improves the convergence rate and generalization of DNNs, its computational
overhead is less than that of IterNorm. Due to the high efficiency of the
proposed method, it can be easily employed in most DNN architectures with a
large number of layers. We provide comprehensive experiments and comparisons
between BN, IterNorm, and SWBN layers to demonstrate the effectiveness of the
proposed technique in conventional (many-shot) image classification and
few-shot classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nezhadarya_E/0/1/0/all/0/1"&gt;Ehsan Nezhadarya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fashandi_H/0/1/0/all/0/1"&gt;Homa Fashandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_D/0/1/0/all/0/1"&gt;Darin Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-09T02:01:51.864Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering. (arXiv:2106.03798v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03798</id>
        <link href="http://arxiv.org/abs/2106.03798"/>
        <updated>2021-06-09T02:01:51.859Z</updated>
        <summary type="html"><![CDATA[We introduce DoubleField, a novel representation combining the merits of both
surface field and radiance field for high-fidelity human rendering. Within
DoubleField, the surface field and radiance field are associated together by a
shared feature embedding and a surface-guided sampling strategy. In this way,
DoubleField has a continuous but disentangled learning space for geometry and
appearance modeling, which supports fast training, inference, and finetuning.
To achieve high-fidelity free-viewpoint rendering, DoubleField is further
augmented to leverage ultra-high-resolution inputs, where a view-to-view
transformer and a transfer learning scheme are introduced for more efficient
learning and finetuning from sparse-view inputs at original resolutions. The
efficacy of DoubleField is validated by the quantitative evaluations on several
datasets and the qualitative results in a real-world sparse multi-view system,
showing its superior capability for photo-realistic free-viewpoint human
rendering. For code and demo video, please refer to our project page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1"&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;He Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yanpei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PolypGen: A multi-center polyp detection and segmentation dataset for generalisability assessment. (arXiv:2106.04463v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04463</id>
        <link href="http://arxiv.org/abs/2106.04463"/>
        <updated>2021-06-09T02:01:51.853Z</updated>
        <summary type="html"><![CDATA[Polyps in the colon are widely known as cancer precursors identified by
colonoscopy either related to diagnostic work-up for symptoms, colorectal
cancer screening or systematic surveillance of certain diseases. Whilst most
polyps are benign, the number, size and the surface structure of the polyp are
tightly linked to the risk of colon cancer. There exists a high missed
detection rate and incomplete removal of colon polyps due to the variable
nature, difficulties to delineate the abnormality, high recurrence rates and
the anatomical topography of the colon. In the past, several methods have been
built to automate polyp detection and segmentation. However, the key issue of
most methods is that they have not been tested rigorously on a large
multi-center purpose-built dataset. Thus, these methods may not generalise to
different population datasets as they overfit to a specific population and
endoscopic surveillance. To this extent, we have curated a dataset from 6
different centers incorporating more than 300 patients. The dataset includes
both single frame and sequence data with 3446 annotated polyp labels with
precise delineation of polyp boundaries verified by six senior
gastroenterologists. To our knowledge, this is the most comprehensive detection
and pixel-level segmentation dataset curated by a team of computational
scientists and expert gastroenterologists. This dataset has been originated as
the part of the Endocv2021 challenge aimed at addressing generalisability in
polyp detection and segmentation. In this paper, we provide comprehensive
insight into data construction and annotation strategies, annotation quality
assurance and technical validation for our extended EndoCV2021 dataset which we
refer to as PolypGen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghatwary_N/0/1/0/all/0/1"&gt;Noha Ghatwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Realdon_S/0/1/0/all/0/1"&gt;Stefano Realdon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cannizzaro_R/0/1/0/all/0/1"&gt;Renato Cannizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salem_O/0/1/0/all/0/1"&gt;Osama E. Salem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lamarque_D/0/1/0/all/0/1"&gt;Dominique Lamarque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Daul_C/0/1/0/all/0/1"&gt;Christian Daul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anonsen_K/0/1/0/all/0/1"&gt;Kim V. Anonsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael A. Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1"&gt;Thomas de Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+East_J/0/1/0/all/0/1"&gt;James E. East&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DisTop: Discovering a Topological representation to learn diverse and rewarding skills. (arXiv:2106.03853v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03853</id>
        <link href="http://arxiv.org/abs/2106.03853"/>
        <updated>2021-06-09T02:01:51.847Z</updated>
        <summary type="html"><![CDATA[The optimal way for a deep reinforcement learning (DRL) agent to explore is
to learn a set of skills that achieves a uniform distribution of states.
Following this,we introduce DisTop, a new model that simultaneously learns
diverse skills and focuses on improving rewarding skills. DisTop progressively
builds a discrete topology of the environment using an unsupervised contrastive
loss, a growing network and a goal-conditioned policy. Using this topology, a
state-independent hierarchical policy can select where the agent has to keep
discovering skills in the state space. In turn, the newly visited states allows
an improved learnt representation and the learning loop continues. Our
experiments emphasize that DisTop is agnostic to the ground state
representation and that the agent can discover the topology of its environment
whether the states are high-dimensional binary data, images, or proprioceptive
inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks
with state-of-the-art algorithms on both single-task dense rewards and diverse
skill discovery. By combining these two aspects, we showthat DisTop achieves
state-of-the-art performance in comparison with hierarchical reinforcement
learning (HRL) when rewards are sparse. We believe DisTop opens new
perspectives by showing that bottom-up skill discovery combined with
representation learning can unlock the exploration challenge in DRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1"&gt;Arthur Aubret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+matignon_L/0/1/0/all/0/1"&gt;Laetitia matignon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassas_S/0/1/0/all/0/1"&gt;Salima Hassas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practical Credit Assignment for Deep Reinforcement Learning. (arXiv:2106.04499v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04499</id>
        <link href="http://arxiv.org/abs/2106.04499"/>
        <updated>2021-06-09T02:01:51.840Z</updated>
        <summary type="html"><![CDATA[Credit assignment is a fundamental problem in reinforcement learning, the
problem of measuring an action's influence on future rewards. Improvements in
credit assignment methods have the potential to boost the performance of RL
algorithms on many tasks, but thus far have not seen widespread adoption.
Recently, a family of methods called Hindsight Credit Assignment (HCA) was
proposed, which explicitly assign credit to actions in hindsight based on the
probability of the action having led to an observed outcome. This approach is
appealing as a means to more efficient data usage, but remains a largely
theoretical idea applicable to a limited set of tabular RL tasks, and it is
unclear how to extend HCA to Deep RL environments. In this work, we explore the
use of HCA-style credit in a deep RL context. We first describe the limitations
of existing HCA algorithms in deep RL, then propose several
theoretically-justified modifications to overcome them. Based on this
exploration, we present a new algorithm, Credit-Constrained Advantage
Actor-Critic (C2A2C), which ignores policy updates for actions which don't
affect future outcomes based on credit in hindsight, while updating the policy
as normal for those that do. We find that C2A2C outperforms Advantage
Actor-Critic (A2C) on the Arcade Learning Environment (ALE) benchmark, showing
broad improvements over A2C and motivating further work on credit-constrained
update rules for deep RL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alipov_V/0/1/0/all/0/1"&gt;Vyacheslav Alipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simmons_Edler_R/0/1/0/all/0/1"&gt;Riley Simmons-Edler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putintsev_N/0/1/0/all/0/1"&gt;Nikita Putintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinin_P/0/1/0/all/0/1"&gt;Pavel Kalinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04408</id>
        <link href="http://arxiv.org/abs/2011.04408"/>
        <updated>2021-06-09T02:01:51.835Z</updated>
        <summary type="html"><![CDATA[Changing environments poses a great challenge on the outdoor visual
perception and scene understanding for robust long-term autonomous driving and
mobile robots, where depth-auxiliary geometric information plays an essential
role to the robustness under challenging scenes. Although monocular depth
prediction has been well studied recently, there are few work focusing on the
depth prediction across multiple environmental conditions, e.g. changing
illumination and seasons, owing to the lack of such a real-world dataset and
benchmark. In this work, a new cross-season monocular depth prediction dataset
SeasonDepth (available on https://seasondepth.github.io) is derived from CMU
Visual Localization dataset through structure from motion. To benchmark the
depth estimation performance under different environments, we investigate
representative and recent state-of-the-art open-source supervised,
self-supervised and domain adaptation depth prediction methods from KITTI
benchmark using several newly-formulated metrics. Through extensive
experimental evaluation on the proposed dataset without fine-tuning, the
influence of multiple environments on performance and robustness is analyzed
both qualitatively and quantitatively, showing that the long-term monocular
depth prediction is far from solved. We further give promising solutions
especially with stereo geometry and multi-task sequential self-supervised
training to enhance the robustness to changing environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baoquan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Loss Networks. (arXiv:2106.03722v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03722</id>
        <link href="http://arxiv.org/abs/2106.03722"/>
        <updated>2021-06-09T02:01:51.829Z</updated>
        <summary type="html"><![CDATA[A novel model called error loss network (ELN) is proposed to build an error
loss function for supervised learning. The ELN is in structure similar to a
radial basis function (RBF) neural network, but its input is an error sample
and output is a loss corresponding to that error sample. That means the
nonlinear input-output mapper of ELN creates an error loss function. The
proposed ELN provides a unified model for a large class of error loss
functions, which includes some information theoretic learning (ITL) loss
functions as special cases. The activation function, weight parameters and
network size of the ELN can be predetermined or learned from the error samples.
On this basis, we propose a new machine learning paradigm where the learning
process is divided into two stages: first, learning a loss function using an
ELN; second, using the learned loss function to continue to perform the
learning. Experimental results are presented to demonstrate the desirable
performance of the new method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Badong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yunfei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengju Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04484</id>
        <link href="http://arxiv.org/abs/2106.04484"/>
        <updated>2021-06-09T02:01:51.810Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;Daniel Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The best of both worlds: stochastic and adversarial episodic MDPs with unknown transition. (arXiv:2106.04117v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04117</id>
        <link href="http://arxiv.org/abs/2106.04117"/>
        <updated>2021-06-09T02:01:51.800Z</updated>
        <summary type="html"><![CDATA[We consider the best-of-both-worlds problem for learning an episodic Markov
Decision Process through $T$ episodes, with the goal of achieving
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret when the losses are adversarial and
simultaneously $\mathcal{O}(\text{polylog}(T))$ regret when the losses are
(almost) stochastic. Recent work by [Jin and Luo, 2020] achieves this goal when
the fixed transition is known, and leaves the case of unknown transition as a
major open question. In this work, we resolve this open problem by using the
same Follow-the-Regularized-Leader ($\text{FTRL}$) framework together with a
set of new techniques. Specifically, we first propose a loss-shifting trick in
the $\text{FTRL}$ analysis, which greatly simplifies the approach of [Jin and
Luo, 2020] and already improves their results for the known transition case.
Then, we extend this idea to the unknown transition case and develop a novel
analysis which upper bounds the transition estimation error by (a fraction of)
the regret itself in the stochastic setting, a key property to ensure
$\mathcal{O}(\text{polylog}(T))$ regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1"&gt;Tiancheng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions. (arXiv:2106.04165v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04165</id>
        <link href="http://arxiv.org/abs/2106.04165"/>
        <updated>2021-06-09T02:01:51.793Z</updated>
        <summary type="html"><![CDATA[Effective control and prediction of dynamical systems often require
appropriate handling of continuous-time and discrete, event-triggered
processes. Stochastic hybrid systems (SHSs), common across engineering domains,
provide a formalism for dynamical systems subject to discrete, possibly
stochastic, state jumps and multi-modal continuous-time flows. Despite the
versatility and importance of SHSs across applications, a general procedure for
the explicit learning of both discrete events and multi-mode continuous
dynamics remains an open problem. This work introduces Neural Hybrid Automata
(NHAs), a recipe for learning SHS dynamics without a priori knowledge on the
number of modes and inter-modal transition dynamics. NHAs provide a systematic
inference method based on normalizing flows, neural differential equations and
self-supervision. We showcase NHAs on several tasks, including mode recovery
and flow learning in systems with stochastic transitions, and end-to-end
learning of hierarchical robot controllers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1"&gt;Luca Scimeca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Animesh Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Sparse Subspace Representation: A Unified Framework for Subspace Clustering, Constrained Clustering, and Active Learning. (arXiv:2106.04330v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04330</id>
        <link href="http://arxiv.org/abs/2106.04330"/>
        <updated>2021-06-09T02:01:51.771Z</updated>
        <summary type="html"><![CDATA[Spectral-based subspace clustering methods have proved successful in many
challenging applications such as gene sequencing, image recognition, and motion
segmentation. In this work, we first propose a novel spectral-based subspace
clustering algorithm that seeks to represent each point as a sparse convex
combination of a few nearby points. We then extend the algorithm to constrained
clustering and active learning settings. Our motivation for developing such a
framework stems from the fact that typically either a small amount of labelled
data is available in advance; or it is possible to label some points at a cost.
The latter scenario is typically encountered in the process of validating a
cluster assignment. Extensive experiments on simulated and real data sets show
that the proposed approach is effective and competitive with state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hankui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pavlidis_N/0/1/0/all/0/1"&gt;Nicos G. Pavlidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonsmooth Implicit Differentiation for Machine Learning and Optimization. (arXiv:2106.04350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04350</id>
        <link href="http://arxiv.org/abs/2106.04350"/>
        <updated>2021-06-09T02:01:51.760Z</updated>
        <summary type="html"><![CDATA[In view of training increasingly complex learning architectures, we establish
a nonsmooth implicit function theorem with an operational calculus. Our result
applies to most practical problems (i.e., definable problems) provided that a
nonsmooth form of the classical invertibility condition is fulfilled. This
approach allows for formal subdifferentiation: for instance, replacing
derivatives by Clarke Jacobians in the usual differentiation formulas is fully
justified for a wide class of nonsmooth problems. Moreover this calculus is
entirely compatible with algorithmic differentiation (e.g., backpropagation).
We provide several applications such as training deep equilibrium networks,
training neural nets with conic optimization layers, or hyperparameter-tuning
for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we
present numerical experiments showcasing the extremely pathological gradient
dynamics one can encounter when applying implicit algorithmic differentiation
without any hypothesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt; (TSE), &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tam Le&lt;/a&gt; (TSE), &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt; (IRIT), &lt;a href="http://arxiv.org/find/cs/1/au:+Silveti_Falls_A/0/1/0/all/0/1"&gt;Antonio Silveti-Falls&lt;/a&gt; (TSE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Robust Experts (MoRE): A Flexible Defense Against Multiple Perturbations. (arXiv:2104.10586v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10586</id>
        <link href="http://arxiv.org/abs/2104.10586"/>
        <updated>2021-06-09T02:01:51.747Z</updated>
        <summary type="html"><![CDATA[To tackle the susceptibility of deep neural networks to adversarial examples,
the adversarial training has been proposed which provides a notion of security
through an inner maximization problem presenting the first-order adversaries
embedded within the outer minimization of the training loss. To generalize the
adversarial robustness over different perturbation types, the adversarial
training method has been augmented with the improved inner maximization
presenting a union of multiple perturbations e.g., various $\ell_p$
norm-bounded perturbations. However, the improved inner maximization only
enjoys limited flexibility in terms of the allowable perturbation types. In
this work, through a gating mechanism, we assemble a set of expert networks,
each one either adversarially trained to deal with a particular perturbation
type or normally trained for boosting accuracy on clean data. The gating module
assigns weights dynamically to each expert to achieve superior accuracy under
various data types e.g., adversarial examples, adverse weather perturbations,
and clean input. In order to deal with the obfuscated gradients issue, the
training of the gating module is conducted together with fine-tuning of the
last fully connected layers of expert networks through adversarial training
approach. Using extensive experiments, we show that our Mixture of Robust
Experts (MoRE) approach enables flexible integration of a broad range of robust
experts with superior performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1"&gt;Ryan Goldhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Expressive Power of Self-Attention Matrices. (arXiv:2106.03764v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03764</id>
        <link href="http://arxiv.org/abs/2106.03764"/>
        <updated>2021-06-09T02:01:51.734Z</updated>
        <summary type="html"><![CDATA[Transformer networks are able to capture patterns in data coming from many
domains (text, images, videos, proteins, etc.) with little or no change to
architecture components. We perform a theoretical analysis of the core
component responsible for signal propagation between elements, i.e. the
self-attention matrix. In practice, this matrix typically exhibits two
properties: (1) it is sparse, meaning that each token only attends to a small
subset of other tokens; and (2) it changes dynamically depending on the input
to the module. With these considerations in mind, we ask the following
question: Can a fixed self-attention module approximate arbitrary sparse
patterns depending on the input? How small is the hidden size $d$ required for
such approximation? We make progress in answering this question and show that
the self-attention matrix can provably approximate sparse matrices, where
sparsity is in terms of a bounded number of nonzero elements in each row and
column. While the parameters of self-attention are fixed, various sparse
matrices can be approximated by only modifying the inputs. Our proof is based
on the random projection technique and uses the seminal Johnson-Lindenstrauss
lemma. Our proof is constructive, enabling us to propose an algorithm for
finding adaptive inputs and fixed self-attention parameters in order to
approximate a given matrix. In particular, we show that, in order to
approximate any sparse matrix up to a given precision defined in terms of
preserving matrix element ratios, $d$ grows only logarithmically with the
sequence length $L$ (i.e. $d = O(\log L)$).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks. (arXiv:2106.04469v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04469</id>
        <link href="http://arxiv.org/abs/2106.04469"/>
        <updated>2021-06-09T02:01:51.729Z</updated>
        <summary type="html"><![CDATA[We consider the task of minimizing the sum of smooth and strongly convex
functions stored in a decentralized manner across the nodes of a communication
network whose links are allowed to change in time. We solve two fundamental
problems for this task. First, we establish the first lower bounds on the
number of decentralized communication rounds and the number of local
computations required to find an $\epsilon$-accurate solution. Second, we
design two optimal algorithms that attain these lower bounds: (i) a variant of
the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a
multi-consensus subroutine, which is optimal in the case when access to the
dual gradients is assumed, and (ii) a novel algorithm, called ADOM+, which is
optimal in the case when access to the primal gradients is assumed. We
corroborate the theoretical efficiency of these algorithms by performing an
experimental comparison with existing state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovalev_D/0/1/0/all/0/1"&gt;Dmitry Kovalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasanov_E/0/1/0/all/0/1"&gt;Elnur Gasanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing Natural Language Processing in A Natural Way: An NLP toolkit based on object-oriented knowledge base and multi-level grammar base. (arXiv:2105.05227v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05227</id>
        <link href="http://arxiv.org/abs/2105.05227"/>
        <updated>2021-06-09T02:01:51.713Z</updated>
        <summary type="html"><![CDATA[We introduce an NLP toolkit based on object-oriented knowledge base and
multi-level grammar base. This toolkit focuses on semantic parsing, it also has
abilities to discover new knowledge and grammar automatically, new discovered
knowledge and grammar will be identified by human, and will be used to update
the knowledge base and grammar base. This process can be iterated many times to
improve the toolkit continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Generalization of Bilevel Programming in Hyperparameter Optimization. (arXiv:2106.04188v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04188</id>
        <link href="http://arxiv.org/abs/2106.04188"/>
        <updated>2021-06-09T02:01:51.708Z</updated>
        <summary type="html"><![CDATA[Recently, the (gradient-based) bilevel programming framework is widely used
in hyperparameter optimization and has achieved excellent performance
empirically. Previous theoretical work mainly focuses on its optimization
properties, while leaving the analysis on generalization largely open. This
paper attempts to address the issue by presenting an expectation bound w.r.t.
the validation set based on uniform stability. Our results can explain some
mysterious behaviours of the bilevel programming in practice, for instance,
overfitting to the validation set. We also present an expectation bound for the
classical cross-validation algorithm. Our results suggest that gradient-based
algorithms can be better than cross-validation under certain conditions in a
theoretical perspective. Furthermore, we prove that regularization terms in
both the outer and inner levels can relieve the overfitting problem in
gradient-based algorithms. In experiments on feature learning and data
reweighting for noisy labels, we corroborate our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1"&gt;Fan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guoqiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongxuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconciling Rewards with Predictive State Representations. (arXiv:2106.03926v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.03926</id>
        <link href="http://arxiv.org/abs/2106.03926"/>
        <updated>2021-06-09T02:01:51.703Z</updated>
        <summary type="html"><![CDATA[Predictive state representations (PSRs) are models of controlled non-Markov
observation sequences which exhibit the same generative process governing POMDP
observations without relying on an underlying latent state. In that respect, a
PSR is indistinguishable from the corresponding POMDP. However, PSRs
notoriously ignore the notion of rewards, which undermines the general utility
of PSR models for control, planning, or reinforcement learning. Therefore, we
describe a sufficient and necessary accuracy condition which determines whether
a PSR is able to accurately model POMDP rewards, we show that rewards can be
approximated even when the accuracy condition is not satisfied, and we find
that a non-trivial number of POMDPs taken from a well-known third-party
repository do not satisfy the accuracy condition. We propose reward-predictive
state representations (R-PSRs), a generalization of PSRs which accurately
models both observations and rewards, and develop value iteration for R-PSRs.
We show that there is a mismatch between optimal POMDP policies and the optimal
PSR policies derived from approximate rewards. On the other hand, optimal R-PSR
policies perfectly match optimal POMDP policies, reconfirming R-PSRs as
accurate state-less generative models of observations and rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisero_A/0/1/0/all/0/1"&gt;Andrea Baisero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1"&gt;Christopher Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Householder-Absolute Neural Layers For High Variability and Deep Trainability. (arXiv:2106.04088v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04088</id>
        <link href="http://arxiv.org/abs/2106.04088"/>
        <updated>2021-06-09T02:01:51.697Z</updated>
        <summary type="html"><![CDATA[We propose a new architecture for artificial neural networks called
Householder-absolute neural layers, or Han-layers for short, that use
Householder reflectors as weight matrices and the absolute-value function for
activation. Han-layers, functioning as fully connected layers, are motivated by
recent results on neural-network variability and are designed to increase
activation ratio and reduce the chance of Collapse to Constants. Neural
networks constructed chiefly from Han-layers are called HanNets. By
construction, HanNets enjoy a theoretical guarantee that vanishing or exploding
gradient never occurs. We conduct several proof-of-concept experiments. Some
surprising results obtained on styled test problems suggest that, under certain
conditions, HanNets exhibit an unusual ability to produce nearly perfect
solutions unattainable by fully connected networks. Experiments on regression
datasets show that HanNets can significantly reduce the number of model
parameters while maintaining or improving the level of generalization accuracy.
In addition, by adding a few Han-layers into the pre-classification FC-layer of
a convolutional neural network, we are able to quickly improve a
state-of-the-art result on CIFAR10 dataset. These proof-of-concept results are
sufficient to necessitate further studies on HanNets to understand their
capacities and limits, and to exploit their potentials in real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yueyao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StutterNet: Stuttering Detection Using Time Delay Neural Network. (arXiv:2105.05599v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05599</id>
        <link href="http://arxiv.org/abs/2105.05599"/>
        <updated>2021-06-09T02:01:51.692Z</updated>
        <summary type="html"><![CDATA[This paper introduces StutterNet, a novel deep learning based stuttering
detection capable of detecting and identifying various types of disfluencies.
Most of the existing work in this domain uses automatic speech recognition
(ASR) combined with language models for stuttering detection. Compared to the
existing work, which depends on the ASR module, our method relies solely on the
acoustic signal. We use a time-delay neural network (TDNN) suitable for
capturing contextual aspects of the disfluent utterances. We evaluate our
system on the UCLASS stuttering dataset consisting of more than 100 speakers.
Our method achieves promising results and outperforms the state-of-the-art
residual neural network based method. The number of trainable parameters of the
proposed method is also substantially less due to the parameter sharing scheme
of TDNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sheikh_S/0/1/0/all/0/1"&gt;Shakeel A. Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hirsch_F/0/1/0/all/0/1"&gt;Fabrice Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouni_S/0/1/0/all/0/1"&gt;Slim Ouni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bangla Natural Language Processing: A Comprehensive Review of Classical, Machine Learning, and Deep Learning Based Methods. (arXiv:2105.14875v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14875</id>
        <link href="http://arxiv.org/abs/2105.14875"/>
        <updated>2021-06-09T02:01:51.675Z</updated>
        <summary type="html"><![CDATA[The Bangla language is the seventh most spoken language, with 265 million
native and non-native speakers worldwide. However, English is the predominant
language for online resources and technical knowledge, journals, and
documentation. Consequently, many Bangla-speaking people, who have limited
command of English, face hurdles to utilize English resources. To bridge the
gap between limited support and increasing demand, researchers conducted many
experiments and developed valuable tools and techniques to create and process
Bangla language materials. Many efforts are also ongoing to make it easy to use
the Bangla language in the online and technical domains. There are some review
papers to understand the past, previous, and future Bangla Natural Language
Processing (BNLP) trends. The studies are mainly concentrated on the specific
domains of BNLP, such as sentiment analysis, speech recognition, optical
character recognition, and text summarization. There is an apparent scarcity of
resources that contain a comprehensive study of the recent BNLP tools and
methods. Therefore, in this paper, we present a thorough review of 71 BNLP
research papers and categorize them into 11 categories, namely Information
Extraction, Machine Translation, Named Entity Recognition, Parsing, Parts of
Speech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake
Detection, Text Summarization, Word Sense Disambiguation, and Speech Processing
and Recognition. We study articles published between 1999 to 2021, and 50% of
the papers were published after 2015. We discuss Classical, Machine Learning
and Deep Learning approaches with different datasets while addressing the
limitations and current and future trends of the BNLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1"&gt;Ovishake Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Mohtasim Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;MD. Nazrul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1"&gt;Jakaria Rabbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;MD. Kamrul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baz_M/0/1/0/all/0/1"&gt;Mohammed Baz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masud_M/0/1/0/all/0/1"&gt;Mehedi Masud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1"&gt;Md. Abdul Awal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1"&gt;Awal Ahmed Fime&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Md. Tahmid Hasan Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1"&gt;Delowar Sikder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1"&gt;MD. Akil Raihan Iftee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-09T02:01:51.669Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers
through reducing their costs of trial and error for discovering the optimal
advertising strategies is crucial for the long-term prosperity of online
advertising. To achieve this goal, the advertising platform needs to identify
the advertisers' marketing objectives, and then recommend the corresponding
strategies to fulfill this objective. In this work, we first deploy a prototype
of strategy recommender system on Taobao display advertising platform,
recommending bid prices and targeted users to advertisers. We further augment
this prototype system by directly revealing the advertising performance, and
then infer the advertisers' marketing objectives through their adoptions of
different recommending advertising performance. We use the techniques from
context bandit to jointly learn the advertisers' marketing objectives and the
recommending strategies. Online evaluations show that the designed advertising
strategy recommender system can optimize the advertisers' advertising
performance and increase the platform's revenue. Simulation experiments based
on Taobao online bidding data show that the designed contextual bandit
algorithm can effectively optimize the strategy adoption rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Channel Dimensions for Efficient Model Design. (arXiv:2007.00992v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00992</id>
        <link href="http://arxiv.org/abs/2007.00992"/>
        <updated>2021-06-09T02:01:51.663Z</updated>
        <summary type="html"><![CDATA[Designing an efficient model within the limited computational cost is
challenging. We argue the accuracy of a lightweight model has been further
limited by the design convention: a stage-wise configuration of the channel
dimensions, which looks like a piecewise linear function of the network stage.
In this paper, we study an effective channel dimension configuration towards
better performance than the convention. To this end, we empirically study how
to design a single layer properly by analyzing the rank of the output feature.
We then investigate the channel configuration of a model by searching network
architectures concerning the channel configuration under the computational cost
restriction. Based on the investigation, we propose a simple yet effective
channel configuration that can be parameterized by the layer index. As a
result, our proposed model following the channel parameterization achieves
remarkable performance on ImageNet classification and transfer learning tasks
including COCO object detection, COCO instance segmentation, and fine-grained
classifications. Code and ImageNet pretrained models are available at
https://github.com/clovaai/rexnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1"&gt;Byeongho Heo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;YoungJoon Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Medkit-Learn(ing) Environment: Medical Decision Modelling through Simulation. (arXiv:2106.04240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04240</id>
        <link href="http://arxiv.org/abs/2106.04240"/>
        <updated>2021-06-09T02:01:51.657Z</updated>
        <summary type="html"><![CDATA[Understanding decision-making in clinical environments is of paramount
importance if we are to bring the strengths of machine learning to ultimately
improve patient outcomes. Several factors including the availability of public
data, the intrinsically offline nature of the problem, and the complexity of
human decision making, has meant that the mainstream development of algorithms
is often geared towards optimal performance in tasks that do not necessarily
translate well into the medical regime; often overlooking more niche issues
commonly associated with the area. We therefore present a new benchmarking
suite designed specifically for medical sequential decision making: the
Medkit-Learn(ing) Environment, a publicly available Python package providing
simple and easy access to high-fidelity synthetic medical data. While providing
a standardised way to compare algorithms in a realistic medical setting we
employ a generating process that disentangles the policy and environment
dynamics to allow for a range of customisations, thus enabling systematic
evaluation of algorithms' robustness against specific challenges prevalent in
healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Alex J. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bica_I/0/1/0/all/0/1"&gt;Ioana Bica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1"&gt;Alihan Huyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1"&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable agent communication from scratch(with a generic visual processor emerging on the side). (arXiv:2106.04258v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04258</id>
        <link href="http://arxiv.org/abs/2106.04258"/>
        <updated>2021-06-09T02:01:51.652Z</updated>
        <summary type="html"><![CDATA[As deep networks begin to be deployed as autonomous agents, the issue of how
they can communicate with each other becomes important. Here, we train two deep
nets from scratch to perform realistic referent identification through
unsupervised emergent communication. We show that the largely interpretable
emergent protocol allows the nets to successfully communicate even about object
types they did not see at training time. The visual representations induced as
a by-product of our training regime, moreover, show comparable quality, when
re-used as generic visual features, to a recent self-supervised learning model.
Our results provide concrete evidence of the viability of (interpretable)
emergent deep net communication in a more realistic scenario than previously
considered, as well as establishing an intriguing link between this field and
self-supervised visual learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1"&gt;Roberto Dess&amp;#xec;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1"&gt;Eugene Kharitonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1"&gt;Marco Baroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. (arXiv:2106.03969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03969</id>
        <link href="http://arxiv.org/abs/2106.03969"/>
        <updated>2021-06-09T02:01:51.635Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boix_Adsera_E/0/1/0/all/0/1"&gt;Enric Boix-Adsera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1"&gt;Guy Bresler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Data Augmentation Do We Need for Deep-Learning-Based Finance?. (arXiv:2106.04114v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04114</id>
        <link href="http://arxiv.org/abs/2106.04114"/>
        <updated>2021-06-09T02:01:51.629Z</updated>
        <summary type="html"><![CDATA[The main task we consider is portfolio construction in a speculative market,
a fundamental problem in modern finance. While various empirical works now
exist to explore deep learning in finance, the theory side is almost
non-existent. In this work, we focus on developing a theoretical framework for
understanding the use of data augmentation for deep-learning-based approaches
to quantitative finance. The proposed theory clarifies the role and necessity
of data augmentation for finance; moreover, our theory motivates a simple
algorithm of injecting a random noise of strength $\sqrt{|r_{t-1}|}$ to the
observed return $r_{t}$. This algorithm is shown to work well in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minami_K/0/1/0/all/0/1"&gt;Kentaro Minami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imajo_K/0/1/0/all/0/1"&gt;Kentaro Imajo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E(n) Equivariant Normalizing Flows. (arXiv:2105.09016v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09016</id>
        <link href="http://arxiv.org/abs/2105.09016"/>
        <updated>2021-06-09T02:01:51.623Z</updated>
        <summary type="html"><![CDATA[This paper introduces a generative model equivariant to Euclidean symmetries:
E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the
discriminative E(n) graph neural networks and integrate them as a differential
equation to obtain an invertible equivariant function: a continuous-time
normalizing flow. We demonstrate that E-NFs considerably outperform baselines
and existing methods from the literature on particle systems such as DW4 and
LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our
knowledge, this is the first flow that jointly generates molecule features and
positions in 3D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Satorras_V/0/1/0/all/0/1"&gt;Victor Garcia Satorras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1"&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuchs_F/0/1/0/all/0/1"&gt;Fabian B. Fuchs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1"&gt;Ingmar Posner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes Multimodal Learning Better than Single (Provably). (arXiv:2106.04538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04538</id>
        <link href="http://arxiv.org/abs/2106.04538"/>
        <updated>2021-06-09T02:01:51.618Z</updated>
        <summary type="html"><![CDATA[The world provides us with data of multiple modalities. Intuitively, models
fusingdata from different modalities outperform unimodal models, since more
informationis aggregated. Recently, joining the success of deep learning, there
is an influentialline of work on deep multimodal learning, which has remarkable
empirical resultson various applications. However, theoretical justifications
in this field are notablylacking.Can multimodal provably perform better than
unimodal? In this paper, we answer this question under a most popular
multimodal learningframework, which firstly encodes features from different
modalities into a commonlatent space and seamlessly maps the latent
representations into the task space. Weprove that learning with multiple
modalities achieves a smaller population risk thanonly using its subset of
modalities. The main intuition is that the former has moreaccurate estimate of
the latent space representation. To the best of our knowledge,this is the first
theoretical treatment to capture important qualitative phenomenaobserved in
real multimodal applications. Combining with experiment results, weshow that
multimodal learning does possess an appealing formal guarantee.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chenzhuang Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentive Mechanism for Privacy-Preserving Federated Learning. (arXiv:2106.04384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04384</id>
        <link href="http://arxiv.org/abs/2106.04384"/>
        <updated>2021-06-09T02:01:51.613Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is an emerging paradigm for machine learning, in
which data owners can collaboratively train a model by sharing gradients
instead of their raw data. Two fundamental research problems in FL are
incentive mechanism and privacy protection. The former focuses on how to
incentivize data owners to participate in FL. The latter studies how to protect
data owners' privacy while maintaining high utility of trained models. However,
incentive mechanism and privacy protection in FL have been studied separately
and no work solves both problems at the same time. In this work, we address the
two problems simultaneously by an FL-Market that incentivizes data owners'
participation by providing appropriate payments and privacy protection.
FL-Market enables data owners to obtain compensation according to their privacy
loss quantified by local differential privacy (LDP). Our insight is that, by
meeting data owners' personalized privacy preferences and providing appropriate
payments, we can (1) incentivize privacy risk-tolerant data owners to set
larger privacy parameters (i.e., gradients with less noise) and (2) provide
preferred privacy protection for privacy risk-averse data owners. To achieve
this, we design a personalized LDP-based FL framework with a deep
learning-empowered auction mechanism for incentivizing trading gradients with
less noise and optimal aggregation mechanisms for model updates. Our
experiments verify the effectiveness of the proposed framework and mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1"&gt;Masatoshi Yoshikawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Policy Comparison under Limited Historical Agent-Environment Interactions. (arXiv:2106.03934v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03934</id>
        <link href="http://arxiv.org/abs/2106.03934"/>
        <updated>2021-06-09T02:01:51.606Z</updated>
        <summary type="html"><![CDATA[We address the challenge of policy evaluation in real-world applications of
reinforcement learning systems where the available historical data is limited
due to ethical, practical, or security considerations. This constrained
distribution of data samples often leads to biased policy evaluation estimates.
To remedy this, we propose that instead of policy evaluation, one should
perform policy comparison, i.e. to rank the policies of interest in terms of
their value based on available historical data. In addition we present the
Limited Data Estimator (LDE) as a simple method for evaluating and comparing
policies from a small number of interactions with the environment. According to
our theoretical analysis, the LDE is shown to be statistically reliable on
policy comparison tasks under mild assumptions on the distribution of the
historical data. Additionally, our numerical experiments compare the LDE to
other policy evaluation methods on the task of policy ranking and demonstrate
its advantage in various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dereventsov_A/0/1/0/all/0/1"&gt;Anton Dereventsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daws_J/0/1/0/all/0/1"&gt;Joseph D. Daws Jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webster_C/0/1/0/all/0/1"&gt;Clayton Webster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Online Learning for Dynamic k-Clustering. (arXiv:2106.04336v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04336</id>
        <link href="http://arxiv.org/abs/2106.04336"/>
        <updated>2021-06-09T02:01:51.590Z</updated>
        <summary type="html"><![CDATA[We study dynamic clustering problems from the perspective of online learning.
We consider an online learning problem, called \textit{Dynamic $k$-Clustering},
in which $k$ centers are maintained in a metric space over time (centers may
change positions) such as a dynamically changing set of $r$ clients is served
in the best possible way. The connection cost at round $t$ is given by the
\textit{$p$-norm} of the vector consisting of the distance of each client to
its closest center at round $t$, for some $p\geq 1$ or $p = \infty$. We present
a \textit{$\Theta\left( \min(k,r) \right)$-regret} polynomial-time online
learning algorithm and show that, under some well-established computational
complexity conjectures, \textit{constant-regret} cannot be achieved in
polynomial-time. In addition to the efficient solution of Dynamic
$k$-Clustering, our work contributes to the long line of research on
combinatorial online learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1"&gt;Dimitris Fotakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skoulakis_S/0/1/0/all/0/1"&gt;Stratis Skoulakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04024</id>
        <link href="http://arxiv.org/abs/2106.04024"/>
        <updated>2021-06-09T02:01:51.583Z</updated>
        <summary type="html"><![CDATA[We develop a framework for comparing data manifolds, aimed, in particular,
towards the evaluation of deep generative models. We describe a novel tool,
Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional
space, tracks multiscale topology spacial discrepancies between manifolds on
which the distributions are concentrated. Based on the Cross-Barcode, we
introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it
to assess the performance of deep generative models in various domains: images,
3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,
CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate
that the MTop-Divergence accurately detects various degrees of mode-dropping,
intra-mode collapse, mode invention, and image disturbance. Our algorithm
scales well (essentially linearly) with the increase of the dimension of the
ambient high-dimensional space. It is one of the first TDA-based practical
methodologies that can be applied universally to datasets of different sizes
and dimensions, including the ones on which the most recent GANs in the visual
domain are trained. The proposed method is domain agnostic and does not rely on
pre-trained networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1"&gt;Serguei Barannikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1"&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1"&gt;Grigorii Sotnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimbach_E/0/1/0/all/0/1"&gt;Ekaterina Trimbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Anomalous Event Sequences with Temporal Point Processes. (arXiv:2106.04465v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04465</id>
        <link href="http://arxiv.org/abs/2106.04465"/>
        <updated>2021-06-09T02:01:51.578Z</updated>
        <summary type="html"><![CDATA[Automatically detecting anomalies in event data can provide substantial value
in domains such as healthcare, DevOps, and information security. In this paper,
we frame the problem of detecting anomalous continuous-time event sequences as
out-of-distribution (OoD) detection for temporal point processes (TPPs). First,
we show how this problem can be approached using goodness-of-fit (GoF) tests.
We then demonstrate the limitations of popular GoF statistics for TPPs and
propose a new test that addresses these shortcomings. The proposed method can
be combined with various TPP models, such as neural TPPs, and is easy to
implement. In our experiments, we show that the proposed statistic excels at
both traditional GoF testing, as well as at detecting anomalies in simulated
and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shchur_O/0/1/0/all/0/1"&gt;Oleksandr Shchur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkmen_A/0/1/0/all/0/1"&gt;Ali Caner T&amp;#xfc;rkmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Januschowski_T/0/1/0/all/0/1"&gt;Tim Januschowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1"&gt;Jan Gasthaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video. (arXiv:2103.08834v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08834</id>
        <link href="http://arxiv.org/abs/2103.08834"/>
        <updated>2021-06-09T02:01:51.572Z</updated>
        <summary type="html"><![CDATA[This paper addresses fast semantic segmentation on video.Video segmentation
often calls for real-time, or even fasterthan real-time, processing. One common
recipe for conserving computation arising from feature extraction is to
propagate features of few selected keyframes. However, recent advances in fast
image segmentation make these solutions less attractive. To leverage fast image
segmentation for furthering video segmentation, we propose a simple yet
efficient propagation framework. Specifically, we perform lightweight flow
estimation in 1/8-downscaled image space for temporal warping in segmentation
outpace space. Moreover, we introduce a guided spatially-varying convolution
for fusing segmentations derived from the previous and current frames, to
mitigate propagation error and enable lightweight feature extraction on
non-keyframes. Experimental results on Cityscapes and CamVid show that our
scheme achieves the state-of-the-art accuracy-throughput trade-off on video
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Shih-Po Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si-Cun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Hsiao Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of data-splits on generalization: Identifying COVID-19 from cough and context. (arXiv:2106.03851v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.03851</id>
        <link href="http://arxiv.org/abs/2106.03851"/>
        <updated>2021-06-09T02:01:51.567Z</updated>
        <summary type="html"><![CDATA[Rapidly scaling screening, testing and quarantine has shown to be an
effective strategy to combat the COVID-19 pandemic. We consider the application
of deep learning techniques to distinguish individuals with COVID from
non-COVID by using data acquirable from a phone. Using cough and context
(symptoms and meta-data) represent such a promising approach. Several
independent works in this direction have shown promising results. However, none
of them report performance across clinically relevant data splits.
Specifically, the performance where the development and test sets are split in
time (retrospective validation) and across sites (broad validation). Although
there is meaningful generalization across these splits the performance
significantly varies (up to 0.1 AUC score). In addition, we study the
performance of symptomatic and asymptomatic individuals across these three
splits. Finally, we show that our model focuses on meaningful features of the
input, cough bouts for cough and relevant symptoms for context. The code and
checkpoints are available at https://github.com/WadhwaniAI/cough-against-covid]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1"&gt;Makkunda Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_N/0/1/0/all/0/1"&gt;Nikhil Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_J/0/1/0/all/0/1"&gt;Jigar Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagad_P/0/1/0/all/0/1"&gt;Piyush Bagad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalmia_A/0/1/0/all/0/1"&gt;Aman Dalmia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhamare_P/0/1/0/all/0/1"&gt;Parag Bhamare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahale_A/0/1/0/all/0/1"&gt;Amrita Mahale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1"&gt;Saurabh Rane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1"&gt;Neeraj Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panicker_R/0/1/0/all/0/1"&gt;Rahul Panicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning. (arXiv:2106.04480v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04480</id>
        <link href="http://arxiv.org/abs/2106.04480"/>
        <updated>2021-06-09T02:01:51.552Z</updated>
        <summary type="html"><![CDATA[We propose to learn to distinguish reversible from irreversible actions for
better informed decision-making in Reinforcement Learning (RL). From
theoretical considerations, we show that approximate reversibility can be
learned through a simple surrogate task: ranking randomly sampled trajectory
events in chronological order. Intuitively, pairs of events that are always
observed in the same order are likely to be separated by an irreversible
sequence of actions. Conveniently, learning the temporal order of events can be
done in a fully self-supervised way, which we use to estimate the reversibility
of actions from experience, without any priors. We propose two different
strategies that incorporate reversibility in RL agents, one strategy for
exploration (RAE) and one strategy for control (RAC). We demonstrate the
potential of reversibility-aware agents in several environments, including the
challenging Sokoban game. In synthetic tasks, we show that we can learn control
policies that never fail and reduce to zero the side-effects of interactions,
even without access to the reward function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1"&gt;Nathan Grinsztajn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferret_J/0/1/0/all/0/1"&gt;Johan Ferret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1"&gt;Philippe Preux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-06-09T02:01:51.547Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive transfer learning. (arXiv:2106.04455v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04455</id>
        <link href="http://arxiv.org/abs/2106.04455"/>
        <updated>2021-06-09T02:01:51.541Z</updated>
        <summary type="html"><![CDATA[In transfer learning, we wish to make inference about a target population
when we have access to data both from the distribution itself, and from a
different but related source distribution. We introduce a flexible framework
for transfer learning in the context of binary classification, allowing for
covariate-dependent relationships between the source and target distributions
that are not required to preserve the Bayes decision boundary. Our main
contributions are to derive the minimax optimal rates of convergence (up to
poly-logarithmic factors) in this problem, and show that the optimal rate can
be achieved by an algorithm that adapts to key aspects of the unknown transfer
relationship, as well as the smoothness and tail parameters of our
distributional classes. This optimal rate turns out to have several regimes,
depending on the interplay between the relative sample sizes and the strength
of the transfer relationship, and our algorithm achieves optimality by careful,
decision tree-based calibration of local nearest-neighbour procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Reeve_H/0/1/0/all/0/1"&gt;Henry W. J. Reeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cannings_T/0/1/0/all/0/1"&gt;Timothy I. Cannings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Samworth_R/0/1/0/all/0/1"&gt;Richard J. Samworth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Elastic Lottery Ticket Hypothesis. (arXiv:2103.16547v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16547</id>
        <link href="http://arxiv.org/abs/2103.16547"/>
        <updated>2021-06-09T02:01:51.536Z</updated>
        <summary type="html"><![CDATA[Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse
trainable subnetworks, or winning tickets, of training, which can be trained in
isolation to achieve similar or even better performance compared to the full
models. Despite many efforts being made, the most effective method to identify
such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is
computationally expensive and has to be run thoroughly for every different
network. A natural question that comes in is: can we "transform" the winning
ticket found in one network to another with a different architecture, yielding
a winning ticket for the latter at the beginning, without re-doing the
expensive IMP? Answering this question is not only practically relevant for
efficient "once-for-all" winning ticket finding, but also theoretically
appealing for uncovering inherently scalable sparse patterns in networks. We
conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety
of strategies to tweak the winning tickets found from different networks of the
same model family (e.g., ResNets). Based on these results, we articulate the
Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or
dropping) and re-ordering layers for one network, its corresponding winning
ticket could be stretched (or squeezed) into a subnetwork for another deeper
(or shallower) network from the same family, whose performance is nearly the
same competitive as the latter's winning ticket directly found by IMP. We have
also thoroughly compared E-LTH with pruning-at-initialization and dynamic
sparse training methods, and discuss the generalizability of E-LTH to different
model families, layer types, or across datasets. Code is available at
https://github.com/VITA-Group/ElasticLTH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying $\ell_\infty$ Adversarial Training to the Union of Perturbation Models. (arXiv:2105.14710v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14710</id>
        <link href="http://arxiv.org/abs/2105.14710"/>
        <updated>2021-06-09T02:01:51.520Z</updated>
        <summary type="html"><![CDATA[Classical adversarial training (AT) frameworks are designed to achieve high
adversarial accuracy against a single attack type, typically $\ell_\infty$
norm-bounded perturbations. Recent extensions in AT have focused on defending
against the union of multiple perturbations but this benefit is obtained at the
expense of a significant (up to $10\times$) increase in training complexity
over single-attack $\ell_\infty$ AT. In this work, we expand the capabilities
of widely popular single-attack $\ell_\infty$ AT frameworks to provide
robustness to the union of ($\ell_\infty, \ell_2, \ell_1$) perturbations while
preserving their training efficiency. Our technique, referred to as Shaped
Noise Augmented Processing (SNAP), exploits a well-established byproduct of
single-attack AT frameworks -- the reduction in the curvature of the decision
boundary of networks. SNAP prepends a given deep net with a shaped noise
augmentation layer whose distribution is learned along with network parameters
using any standard single-attack AT. As a result, SNAP enhances adversarial
accuracy of ResNet-18 on CIFAR-10 against the union of ($\ell_\infty, \ell_2,
\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)
single-attack $\ell_\infty$ AT frameworks, and, for the first time, establishes
a benchmark for ResNet-50 and ResNet-101 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1"&gt;Ameya D. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuttle_M/0/1/0/all/0/1"&gt;Michael Tuttle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1"&gt;Naresh R. Shanbhag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Feature Learning for Manipulation with Contrastive Domain Randomization. (arXiv:2103.11144v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11144</id>
        <link href="http://arxiv.org/abs/2103.11144"/>
        <updated>2021-06-09T02:01:51.515Z</updated>
        <summary type="html"><![CDATA[Robotic tasks such as manipulation with visual inputs require image features
that capture the physical properties of the scene, e.g., the position and
configuration of objects. Recently, it has been suggested to learn such
features in an unsupervised manner from simulated, self-supervised, robot
interaction; the idea being that high-level physical properties are well
captured by modern physical simulators, and their representation from visual
inputs may transfer well to the real world. In particular, learning methods
based on noise contrastive estimation have shown promising results. To
robustify the simulation-to-real transfer, domain randomization (DR) was
suggested for learning features that are invariant to irrelevant visual
properties such as textures or lighting. In this work, however, we show that a
naive application of DR to unsupervised learning based on contrastive
estimation does not promote invariance, as the loss function maximizes mutual
information between the features and both the relevant and irrelevant visual
properties. We propose a simple modification of the contrastive loss to fix
this, exploiting the fact that we can control the simulated randomization of
visual properties. Our approach learns physical features that are significantly
more robust to visual domain variation, as we demonstrate using both rigid and
non-rigid objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabinovitz_C/0/1/0/all/0/1"&gt;Carmel Rabinovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grupen_N/0/1/0/all/0/1"&gt;Niko Grupen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1"&gt;Aviv Tamar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling. (arXiv:2102.13156v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13156</id>
        <link href="http://arxiv.org/abs/2102.13156"/>
        <updated>2021-06-09T02:01:51.509Z</updated>
        <summary type="html"><![CDATA[Integrating physics models within machine learning models holds considerable
promise toward learning robust models with improved interpretability and
abilities to extrapolate. In this work, we focus on the integration of
incomplete physics models into deep generative models. In particular, we
introduce an architecture of variational autoencoders (VAEs) in which a part of
the latent space is grounded by physics. A key technical challenge is to strike
a balance between the incomplete physics and trainable components such as
neural networks for ensuring that the physics part is used in a meaningful
manner. To this end, we propose a regularized learning method that controls the
effect of the trainable components and preserves the semantics of the
physics-based latent variables as intended. We not only demonstrate generative
performance improvements over a set of synthetic and real-world datasets, but
we also show that we learn robust models that can consistently extrapolate
beyond the training distribution in a meaningful manner. Moreover, we show that
we can control the generative process in an interpretable manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1"&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1"&gt;Alexandros Kalousis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deeply-Debiased Off-Policy Interval Estimation. (arXiv:2105.04646v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04646</id>
        <link href="http://arxiv.org/abs/2105.04646"/>
        <updated>2021-06-09T02:01:51.504Z</updated>
        <summary type="html"><![CDATA[Off-policy evaluation learns a target policy's value with a historical
dataset generated by a different behavior policy. In addition to a point
estimate, many applications would benefit significantly from having a
confidence interval (CI) that quantifies the uncertainty of the point estimate.
In this paper, we propose a novel deeply-debiasing procedure to construct an
efficient, robust, and flexible CI on a target policy's value. Our method is
justified by theoretical results and numerical experiments. A Python
implementation of the proposed procedure is available at
https://github.com/RunzheStat/D2OPE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengchun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wan_R/0/1/0/all/0/1"&gt;Runzhe Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing. (arXiv:2105.08285v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08285</id>
        <link href="http://arxiv.org/abs/2105.08285"/>
        <updated>2021-06-09T02:01:51.499Z</updated>
        <summary type="html"><![CDATA[We present the first provable Least-Squares Value Iteration (LSVI) algorithms
that have runtime complexity sublinear in the number of actions. We formulate
the value function estimation procedure in value iteration as an approximate
maximum inner product search problem and propose a locality sensitive hashing
(LSH) [Indyk and Motwani STOC'98, Andoni and Razenshteyn STOC'15, Andoni,
Laarhoven, Razenshteyn and Waingarten SODA'17] type data structure to solve
this problem with sublinear time complexity. Moreover, we build the connections
between the theory of approximate maximum inner product search and the regret
analysis of reinforcement learning. We prove that, with our choice of
approximation factor, our Sublinear LSVI algorithms maintain the same regret as
the original LSVI algorithms while reducing the runtime complexity to sublinear
in the number of actions. To the best of our knowledge, this is the first work
that combines LSH with reinforcement learning resulting in provable
improvements. We hope that our novel way of combining data-structures and
iterative algorithm will open the door for further study into cost reduction in
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhaozhuo Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Learning in Online Queuing Systems. (arXiv:2106.04228v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04228</id>
        <link href="http://arxiv.org/abs/2106.04228"/>
        <updated>2021-06-09T02:01:51.484Z</updated>
        <summary type="html"><![CDATA[Motivated by packet routing in computer networks, online queuing systems are
composed of queues receiving packets at different rates. Repeatedly, they send
packets to servers, each of them treating only at most one packet at a time. In
the centralized case, the number of accumulated packets remains bounded (i.e.,
the system is \textit{stable}) as long as the ratio between service rates and
arrival rates is larger than $1$. In the decentralized case, individual
no-regret strategies ensures stability when this ratio is larger than $2$. Yet,
myopically minimizing regret disregards the long term effects due to the
carryover of packets to further rounds. On the other hand, minimizing long term
costs leads to stable Nash equilibria as soon as the ratio exceeds
$\frac{e}{e-1}$. Stability with decentralized learning strategies with a ratio
below $2$ was a major remaining question. We first argue that for ratios up to
$2$, cooperation is required for stability of learning strategies, as selfish
minimization of policy regret, a \textit{patient} notion of regret, might
indeed still be unstable in this case. We therefore consider cooperative queues
and propose the first learning decentralized algorithm guaranteeing stability
of the system as long as the ratio of rates is larger than $1$, thus reaching
performances comparable to centralized strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sentenac_F/0/1/0/all/0/1"&gt;Flore Sentenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boursier_E/0/1/0/all/0/1"&gt;Etienne Boursier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cyberbullying Detection Using Deep Neural Network from Social Media Comments in Bangla Language. (arXiv:2106.04506v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04506</id>
        <link href="http://arxiv.org/abs/2106.04506"/>
        <updated>2021-06-09T02:01:51.468Z</updated>
        <summary type="html"><![CDATA[Cyberbullying or Online harassment detection on social media for various
major languages is currently being given a good amount of focus by researchers
worldwide. Being the seventh most speaking language in the world and increasing
usage of online platform among the Bengali speaking people urge to find
effective detection technique to handle the online harassment. In this paper,
we have proposed binary and multiclass classification model using hybrid neural
network for bully expression detection in Bengali language. We have used 44,001
users comments from popular public Facebook pages, which fall into five classes
- Non-bully, Sexual, Threat, Troll and Religious. We have examined the
performance of our proposed models from different perspective. Our binary
classification model gives 87.91% accuracy, whereas introducing ensemble
technique after neural network for multiclass classification, we got 85%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Md Faisal Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_Z/0/1/0/all/0/1"&gt;Zalish Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biash_Z/0/1/0/all/0/1"&gt;Zarin Tasnim Biash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryen_A/0/1/0/all/0/1"&gt;Ahmed Ann Noor Ryen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_A/0/1/0/all/0/1"&gt;Arman Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashraf_F/0/1/0/all/0/1"&gt;Faisal Bin Ashraf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Struggle with Academic Plagiarism: Approaches based on Semantic Similarity. (arXiv:2106.04404v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04404</id>
        <link href="http://arxiv.org/abs/2106.04404"/>
        <updated>2021-06-09T02:01:51.443Z</updated>
        <summary type="html"><![CDATA[Academic plagiarism is a serious problem nowadays. Due to the existence of
inexhaustible sources of digital information, today it is easier to plagiarize
more than ever before. The good thing is that plagiarism detection techniques
have improved and are powerful enough to detect attempts of plagiarism in
education. We are now witnessing efficient plagiarism detection software in
action, such as Turnitin, iThenticate or SafeAssign. In the introduction we
explore software that is used within the Croatian academic community for
plagiarism detection in universities and/or in scientific journals. The
question is: is this enough? Current software has proven to be successful,
however the problem of identifying paraphrasing or obfuscation plagiarism
remains unresolved. In this paper we present a report of how semantic
similarity measures can be used in the plagiarism detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Property-Aware Robot Object Manipulation: a Generative Approach. (arXiv:2106.04385v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04385</id>
        <link href="http://arxiv.org/abs/2106.04385"/>
        <updated>2021-06-09T02:01:51.315Z</updated>
        <summary type="html"><![CDATA[When transporting an object, we unconsciously adapt our movement to its
properties, for instance by slowing down when the item is fragile. The most
relevant features of an object are immediately revealed to a human observer by
the way the handling occurs, without any need for verbal description. It would
greatly facilitate collaboration to enable humanoid robots to perform movements
that convey similar intuitive cues to the observers. In this work, we focus on
how to generate robot motion adapted to the hidden properties of the
manipulated objects, such as their weight and fragility. We explore the
possibility of leveraging Generative Adversarial Networks to synthesize new
actions coherent with the properties of the object. The use of a generative
approach allows us to create new and consistent motion patterns, without the
need of collecting a large number of recorded human-led demonstrations.
Besides, the informative content of the actions is preserved. Our results show
that Generative Adversarial Nets can be a powerful tool for the generation of
novel and meaningful transportation actions, which result effectively modulated
as a function of the object weight and the carefulness required in its
handling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garello_L/0/1/0/all/0/1"&gt;Luca Garello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lastrico_L/0/1/0/all/0/1"&gt;Linda Lastrico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rea_F/0/1/0/all/0/1"&gt;Francesco Rea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mastrogiovanni_F/0/1/0/all/0/1"&gt;Fulvio Mastrogiovanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noceti_N/0/1/0/all/0/1"&gt;Nicoletta Noceti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sciutti_A/0/1/0/all/0/1"&gt;Alessandra Sciutti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Generation of Machine Learning Synthetic Data Using ROS. (arXiv:2106.04547v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04547</id>
        <link href="http://arxiv.org/abs/2106.04547"/>
        <updated>2021-06-09T02:01:51.309Z</updated>
        <summary type="html"><![CDATA[Data labeling is a time intensive process. As such, many data scientists use
various tools to aid in the data generation and labeling process. While these
tools help automate labeling, many still require user interaction throughout
the process. Additionally, most target only a few network frameworks. Any
researchers exploring multiple frameworks must find additional tools orwrite
conversion scripts. This paper presents an automated tool for generating
synthetic data in arbitrary network formats. It uses Robot Operating System
(ROS) and Gazebo, which are common tools in the robotics community. Through ROS
paradigms, it allows extensive user customization of the simulation environment
and data generation process. Additionally, a plugin-like framework allows the
development of arbitrary data format writers without the need to change the
main body of code. Using this tool, the authors were able to generate an
arbitrarily large image dataset for three unique training formats using
approximately 15 min of user setup time and a variable amount of hands-off run
time, depending on the dataset size. The source code for this data generation
tool is available at https://github.com/Navy-RISE-Lab/nn_data_collection]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hart_K/0/1/0/all/0/1"&gt;Kyle M. Hart&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_A/0/1/0/all/0/1"&gt;Ari B. Goodman&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+OShea_R/0/1/0/all/0/1"&gt;Ryan P. O&amp;#x27;Shea&lt;/a&gt; (1) ((1) Naval Air Warfare Center - Aircraft Division - Lakehurst)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Control with Graph Neural Networks. (arXiv:2012.14906v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14906</id>
        <link href="http://arxiv.org/abs/2012.14906"/>
        <updated>2021-06-09T02:01:51.166Z</updated>
        <summary type="html"><![CDATA[Dynamical systems consisting of a set of autonomous agents face the challenge
of having to accomplish a global task, relying only on local information. While
centralized controllers are readily available, they face limitations in terms
of scalability and implementation, as they do not respect the distributed
information structure imposed by the network system of agents. Given the
difficulties in finding optimal decentralized controllers, we propose a novel
framework using graph neural networks (GNNs) to \emph{learn} these controllers.
GNNs are well-suited for the task since they are naturally distributed
architectures and exhibit good scalability and transferability properties. The
problems of flocking and multi-agent path planning are explored to illustrate
the potential of GNNs in learning decentralized controllers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingbiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstaya_E/0/1/0/all/0/1"&gt;Ekaterina Tolstaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1"&gt;Amanda Prorok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Deep Inverse Rosenblatt Transports. (arXiv:2106.04170v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04170</id>
        <link href="http://arxiv.org/abs/2106.04170"/>
        <updated>2021-06-09T02:01:51.160Z</updated>
        <summary type="html"><![CDATA[We present a novel offline-online method to mitigate the computational burden
of the characterization of conditional beliefs in statistical learning. In the
offline phase, the proposed method learns the joint law of the belief random
variables and the observational random variables in the tensor-train (TT)
format. In the online phase, it utilizes the resulting order-preserving
conditional transport map to issue real-time characterization of the
conditional beliefs given new observed information. Compared with the
state-of-the-art normalizing flows techniques, the proposed method relies on
function approximation and is equipped with thorough performance analysis. This
also allows us to further extend the capability of transport maps in
challenging problems with high-dimensional observations and high-dimensional
belief variables. On the one hand, we present novel heuristics to reorder
and/or reparametrize the variables to enhance the approximation power of TT. On
the other, we integrate the TT-based transport maps and the parameter
reordering/reparametrization into layered compositions to further improve the
performance of the resulting transport maps. We demonstrate the efficiency of
the proposed method on various statistical learning tasks in ordinary
differential equations (ODEs) and partial differential equations (PDEs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cui_T/0/1/0/all/0/1"&gt;Tiangang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dolgov_S/0/1/0/all/0/1"&gt;Sergey Dolgov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zahm_O/0/1/0/all/0/1"&gt;Olivier Zahm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Loss Surfaces of Neural Networks with General Activation Functions. (arXiv:2004.03959v3 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03959</id>
        <link href="http://arxiv.org/abs/2004.03959"/>
        <updated>2021-06-09T02:01:51.146Z</updated>
        <summary type="html"><![CDATA[The loss surfaces of deep neural networks have been the subject of several
studies, theoretical and experimental, over the last few years. One strand of
work considers the complexity, in the sense of local optima, of high
dimensional random functions with the aim of informing how local optimisation
methods may perform in such complicated settings. Prior work of Choromanska et
al (2015) established a direct link between the training loss surfaces of deep
multi-layer perceptron networks and spherical multi-spin glass models under
some very strong assumptions on the network and its data. In this work, we test
the validity of this approach by removing the undesirable restriction to ReLU
activation functions. In doing so, we chart a new path through the spin glass
complexity calculations using supersymmetric methods in Random Matrix Theory
which may prove useful in other contexts. Our results shed new light on both
the strengths and the weaknesses of spin glass models in this context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Baskerville_N/0/1/0/all/0/1"&gt;Nicholas P. Baskerville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Keating_J/0/1/0/all/0/1"&gt;Jonathan P. Keating&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mezzadri_F/0/1/0/all/0/1"&gt;Francesco Mezzadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Najnudel_J/0/1/0/all/0/1"&gt;Joseph Najnudel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic Neural Networks with Inductive Biases Capture Epistemic and Aleatoric Uncertainty. (arXiv:2102.11582v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11582</id>
        <link href="http://arxiv.org/abs/2102.11582"/>
        <updated>2021-06-09T02:01:51.140Z</updated>
        <summary type="html"><![CDATA[We show that a single softmax neural net with minimal changes can beat the
uncertainty predictions of Deep Ensembles and other more complex
single-forward-pass uncertainty approaches. Standard softmax neural nets suffer
from feature collapse and extrapolate arbitrarily for OoD points. This results
in arbitrary softmax entropies for OoD points which can have high entropy, low,
or anything in between, thus cannot capture epistemic uncertainty reliably. We
prove that this failure lies at the core of "why" Deep Ensemble Uncertainty
works well. Instead of using softmax entropy, we show that with appropriate
inductive biases softmax neural nets trained with maximum likelihood reliably
capture epistemic uncertainty through their feature-space density. This density
is obtained using simple Gaussian Discriminant Analysis, but it cannot
represent aleatoric uncertainty reliably. We show that it is necessary to
combine feature-space density with softmax entropy to disentangle uncertainties
well. We evaluate the epistemic uncertainty quality on active learning and OoD
detection, achieving SOTA ~98 AUROC on CIFAR-10 vs SVHN without fine-tuning on
OoD data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukhoti_J/0/1/0/all/0/1"&gt;Jishnu Mukhoti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A low discrepancy sequence on graphs. (arXiv:2010.04227v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04227</id>
        <link href="http://arxiv.org/abs/2010.04227"/>
        <updated>2021-06-09T02:01:51.135Z</updated>
        <summary type="html"><![CDATA[Many applications such as election forecasting, environmental monitoring,
health policy, and graph based machine learning require taking expectation of
functions defined on the vertices of a graph. We describe a construction of a
sampling scheme analogous to the so called Leja points in complex potential
theory that can be proved to give low discrepancy estimates for the
approximation of the expected value by the impirical expected value based on
these points. In contrast to classical potential theory where the kernel is
fixed and the equilibrium distribution depends upon the kernel, we fix a
probability distribution and construct a kernel (which represents the graph
structure) for which the equilibrium distribution is the given probability
distribution. Our estimates do not depend upon the size of the graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;A. Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1"&gt;H. N. Mhaskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the benefits of defining vicinal distributions in latent space. (arXiv:2003.06566v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06566</id>
        <link href="http://arxiv.org/abs/2003.06566"/>
        <updated>2021-06-09T02:01:51.129Z</updated>
        <summary type="html"><![CDATA[The vicinal risk minimization (VRM) principle is an empirical risk
minimization (ERM) variant that replaces Dirac masses with vicinal functions.
There is strong numerical and theoretical evidence showing that VRM outperforms
ERM in terms of generalization if appropriate vicinal functions are chosen.
Mixup Training (MT), a popular choice of vicinal distribution, improves the
generalization performance of models by introducing globally linear behavior in
between training examples. Apart from generalization, recent works have shown
that mixup trained models are relatively robust to input
perturbations/corruptions and at the same time are calibrated better than their
non-mixup counterparts. In this work, we investigate the benefits of defining
these vicinal distributions like mixup in latent space of generative models
rather than in input space itself. We propose a new approach - \textit{VarMixup
(Variational Mixup)} - to better sample mixup images by using the latent
manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and
Tiny-ImageNet demonstrate that models trained by performing mixup in the latent
manifold learned by VAEs are inherently more robust to various input
corruptions/perturbations, are significantly better calibrated, and exhibit
more local-linear loss landscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1"&gt;Puneet Mangla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vedant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1"&gt;Shreyas Jayant Havaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications. (arXiv:1703.01610v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1703.01610</id>
        <link href="http://arxiv.org/abs/1703.01610"/>
        <updated>2021-06-09T02:01:51.124Z</updated>
        <summary type="html"><![CDATA[We study combinatorial multi-armed bandit with probabilistically triggered
arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior
CMAB-T studies where the regret bounds contain a possibly exponentially large
factor of $1/p^*$, where $p^*$ is the minimum positive probability that an arm
is triggered by any action. We address this issue by introducing a triggering
probability modulated (TPM) bounded smoothness condition into the general
CMAB-T framework, and show that many applications such as influence
maximization bandit and combinatorial cascading bandit satisfy this TPM
condition. As a result, we completely remove the factor of $1/p^*$ from the
regret bounds, achieving significantly better regret bounds for influence
maximization and cascading bandits than before. Finally, we provide lower bound
results showing that the factor $1/p^*$ is unavoidable for general CMAB-T
problems, suggesting that the TPM condition is crucial in removing this factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qinshi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Assumptions in Bayesian Optimisation. (arXiv:2012.03826v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03826</id>
        <link href="http://arxiv.org/abs/2012.03826"/>
        <updated>2021-06-09T02:01:51.108Z</updated>
        <summary type="html"><![CDATA[Inspired by the increasing desire to efficiently tune machine learning
hyper-parameters, in this work we rigorously analyse conventional and
non-conventional assumptions inherent to Bayesian optimisation. Across an
extensive set of experiments we conclude that: 1) the majority of
hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity,
2) multi-objective acquisition ensembles with Pareto-front solutions
significantly improve queried configurations, and 3) robust acquisition
maximisation affords empirical advantages relative to its non-robust
counterparts. We hope these findings may serve as guiding principles, both for
practitioners and for further research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cowen_Rivers_A/0/1/0/all/0/1"&gt;Alexander I. Cowen-Rivers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1"&gt;Wenlong Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tutunov_R/0/1/0/all/0/1"&gt;Rasul Tutunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1"&gt;Antoine Grosnit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jianye_H/0/1/0/all/0/1"&gt;Hao Jianye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ammar_H/0/1/0/all/0/1"&gt;Haitham Bou Ammar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12002</id>
        <link href="http://arxiv.org/abs/2105.12002"/>
        <updated>2021-06-09T02:01:51.102Z</updated>
        <summary type="html"><![CDATA[The Lottery Ticket Hypothesis suggests that an over-parametrized network
consists of ``lottery tickets'', and training a certain collection of them
(i.e., a subnetwork) can match the performance of the full model. In this
paper, we study such a collection of tickets, which is referred to as ``winning
tickets'', in extremely over-parametrized models, e.g., pre-trained language
models. We observe that at certain compression ratios, the generalization
performance of the winning tickets can not only match but also exceed that of
the full model. In particular, we observe a phase transition phenomenon: As the
compression ratio increases, generalization performance of the winning tickets
first improves then deteriorates after a certain threshold. We refer to the
tickets on the threshold as ``super tickets''. We further show that the phase
transition is task and model dependent -- as the model size becomes larger and
the training data set becomes smaller, the transition becomes more pronounced.
Our experiments on the GLUE benchmark show that the super tickets improve
single task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on
BERT-large, in terms of task-average score. We also demonstrate that adaptively
sharing the super tickets across tasks benefits multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Simiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00351</id>
        <link href="http://arxiv.org/abs/2105.00351"/>
        <updated>2021-06-09T02:01:51.070Z</updated>
        <summary type="html"><![CDATA[Topological data analysis, including persistent homology, has undergone
significant development in recent years. However, one outstanding challenge is
to build a coherent statistical inference procedure on persistent diagrams. The
paired dependent data structure, as birth and death in persistent diagrams,
adds additional complexity to the development. In this paper, we present a new
lattice path representation for persistent diagrams. A new exact statistical
inference procedure is developed for lattice paths via combinatorial
enumerations. The proposed lattice path method is applied to the topological
characterization of the protein structures of COVID-19 viruse. We demonstrate
that there are topological changes during the conformation change of spike
proteins that are needed to initiate the infection of host cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1"&gt;Moo K. Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Through Regularization for Learning to Rank. (arXiv:2102.05996v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05996</id>
        <link href="http://arxiv.org/abs/2102.05996"/>
        <updated>2021-06-09T02:01:51.064Z</updated>
        <summary type="html"><![CDATA[Given the abundance of applications of ranking in recent years, addressing
fairness concerns around automated ranking systems becomes necessary for
increasing the trust among end-users. Previous work on fair ranking has mostly
focused on application-specific fairness notions, often tailored to online
advertising, and it rarely considers learning as part of the process. In this
work, we show how to transfer numerous fairness notions from binary
classification to a learning to rank setting. Our formalism allows us to design
methods for incorporating fairness objectives with provable generalization
guarantees. An extensive experimental evaluation shows that our method can
improve ranking fairness substantially with no or only little loss of model
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1"&gt;Nikola Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications. (arXiv:2103.04244v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04244</id>
        <link href="http://arxiv.org/abs/2103.04244"/>
        <updated>2021-06-09T02:01:51.058Z</updated>
        <summary type="html"><![CDATA[There has been a growing interest in model-agnostic methods that can make
deep learning models more transparent and explainable to a user. Some
researchers recently argued that for a machine to achieve a certain degree of
human-level explainability, this machine needs to provide human causally
understandable explanations, also known as causability. A specific class of
algorithms that have the potential to provide causability are counterfactuals.
This paper presents an in-depth systematic review of the diverse existing body
of literature on counterfactuals and causability for explainable artificial
intelligence. We performed an LDA topic modelling analysis under a PRISMA
framework to find the most relevant literature articles. This analysis resulted
in a novel taxonomy that considers the grounding theories of the surveyed
algorithms, together with their underlying properties and applications in
real-world data. This research suggests that current model-agnostic
counterfactual algorithms for explainable AI are not grounded on a causal
theoretical formalism and, consequently, cannot promote causability to a human
decision-maker. Our findings suggest that the explanations derived from major
algorithms in the literature provide spurious correlations rather than
cause/effects relationships, leading to sub-optimal, erroneous or even biased
explanations. This paper also advances the literature with new directions and
challenges on promoting causability in model-agnostic approaches for
explainable artificial intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Liang Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1"&gt;Catarina Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruza_P/0/1/0/all/0/1"&gt;Peter Bruza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Chun Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorge_J/0/1/0/all/0/1"&gt;Joaquim Jorge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Universal Law of Robustness via Isoperimetry. (arXiv:2105.12806v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12806</id>
        <link href="http://arxiv.org/abs/2105.12806"/>
        <updated>2021-06-09T02:01:51.041Z</updated>
        <summary type="html"><![CDATA[Classically, data interpolation with a parametrized model class is possible
as long as the number of parameters is larger than the number of equations to
be satisfied. A puzzling phenomenon in deep learning is that models are trained
with many more parameters than what this classical theory would suggest. We
propose a theoretical explanation for this phenomenon. We prove that for a
broad class of data distributions and model classes, overparametrization is
necessary if one wants to interpolate the data smoothly. Namely we show that
smooth interpolation requires $d$ times more parameters than mere
interpolation, where $d$ is the ambient data dimension. We prove this universal
law of robustness for any smoothly parametrized function class with polynomial
size weights, and any covariate distribution verifying isoperimetry. In the
case of two-layers neural networks and Gaussian covariates, this law was
conjectured in prior work by Bubeck, Li and Nagaraj. We also give an
interpretation of our result as an improved generalization bound for model
classes consisting of smooth functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Bubeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Greedy-Step Bellman Optimality Equation for Efficient Value Propagation. (arXiv:2102.11717v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11717</id>
        <link href="http://arxiv.org/abs/2102.11717"/>
        <updated>2021-06-09T02:01:51.036Z</updated>
        <summary type="html"><![CDATA[Efficiently propagating credit to responsible actions is a central and
challenging task in reinforcement learning. To accelerate information
propagation, this paper presents a new method that bridges a highway that
allows unimpeded information to flow across long horizons. The key to our
method is a newly proposed Bellman equation, called Greedy-Step Bellman
Optimality Equation, through which the high-credit information can fast
propagate across a long horizon. We theoretically show that the solution of the
new equation is exactly the optimal value function and the corresponding
operator converges faster than the classical operator. Besides, it leads to a
new multi-step off-policy algorithm, which is capable of safely utilizing any
off-policy data collected by the arbitrary policy. Experiments reveal that the
proposed method is reliable, easy to implement. Moreover, without employing
additional components of Rainbow except Double DQN, our method achieves
competitive performance with Rainbow on the benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiaoyang Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Policy Gradient against Strong Data Corruption. (arXiv:2102.05800v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05800</id>
        <link href="http://arxiv.org/abs/2102.05800"/>
        <updated>2021-06-09T02:01:51.030Z</updated>
        <summary type="html"><![CDATA[We study the problem of robust reinforcement learning under adversarial
corruption on both rewards and transitions. Our attack model assumes an
\textit{adaptive} adversary who can arbitrarily corrupt the reward and
transition at every step within an episode, for at most $\epsilon$-fraction of
the learning episodes. Our attack model is strictly stronger than those
considered in prior works. Our first result shows that no algorithm can find a
better than $O(\epsilon)$-optimal policy under our attack model. Next, we show
that surprisingly the natural policy gradient (NPG) method retains a natural
robustness property if the reward corruption is bounded, and can find an
$O(\sqrt{\epsilon})$-optimal policy. Consequently, we develop a Filtered Policy
Gradient (FPG) algorithm that can tolerate even unbounded reward corruption and
can find an $O(\epsilon^{1/4})$-optimal policy. We emphasize that FPG is the
first that can achieve a meaningful learning guarantee when a constant fraction
of episodes are corrupted. Complimentary to the theoretical results, we show
that a neural implementation of FPG achieves strong robust learning performance
on the MuJoCo continuous control benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuezhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiding Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Local Pseudorandom Generators to Hardness of Learning. (arXiv:2101.08303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08303</id>
        <link href="http://arxiv.org/abs/2101.08303"/>
        <updated>2021-06-09T02:01:51.024Z</updated>
        <summary type="html"><![CDATA[We prove hardness-of-learning results under a well-studied assumption on the
existence of local pseudorandom generators. As we show, this assumption allows
us to surpass the current state of the art, and prove hardness of various basic
problems, with no hardness results to date.

Our results include: hardness of learning shallow ReLU neural networks under
the Gaussian distribution and other distributions; hardness of learning
intersections of $\omega(1)$ halfspaces, DNF formulas with $\omega(1)$ terms,
and ReLU networks with $\omega(1)$ hidden neurons; hardness of weakly learning
deterministic finite automata under the uniform distribution; hardness of
weakly learning depth-$3$ Boolean circuits under the uniform distribution, as
well as distribution-specific hardness results for learning DNF formulas and
intersections of halfspaces. We also establish lower bounds on the complexity
of learning intersections of a constant number of halfspaces, and ReLU networks
with a constant number of hidden neurons. Moreover, our results imply the
hardness of virtually all improper PAC-learning problems (both
distribution-free and distribution-specific) that were previously shown hard
under other assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1"&gt;Amit Daniely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1"&gt;Gal Vardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing. (arXiv:2106.04502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04502</id>
        <link href="http://arxiv.org/abs/2106.04502"/>
        <updated>2021-06-09T02:01:51.018Z</updated>
        <summary type="html"><![CDATA[Tuning hyperparameters is a crucial but arduous part of the machine learning
pipeline. Hyperparameter optimization is even more challenging in federated
learning, where models are learned over a distributed network of heterogeneous
devices; here, the need to keep data on device and perform local training makes
it difficult to efficiently train and evaluate configurations. In this work, we
investigate the problem of federated hyperparameter tuning. We first identify
key challenges and show how standard approaches may be adapted to form
baselines for the federated setting. Then, by making a novel connection to the
neural architecture search technique of weight-sharing, we introduce a new
method, FedEx, to accelerate federated hyperparameter tuning that is applicable
to widely-used federated optimization methods such as FedAvg and recent
variants. Theoretically, we show that a FedEx variant correctly tunes the
on-device learning rate in the setting of online convex optimization across
devices. Empirically, we show that FedEx can outperform natural baselines for
federated hyperparameter tuning by several percentage points on the
Shakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using
the same training budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1"&gt;Mikhail Khodak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1"&gt;Renbo Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liam Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1"&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Regularization in ReLU Networks with the Square Loss. (arXiv:2012.05156v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05156</id>
        <link href="http://arxiv.org/abs/2012.05156"/>
        <updated>2021-06-09T02:01:51.002Z</updated>
        <summary type="html"><![CDATA[Understanding the implicit regularization (or implicit bias) of gradient
descent has recently been a very active research area. However, the implicit
regularization in nonlinear neural networks is still poorly understood,
especially for regression losses such as the square loss. Perhaps surprisingly,
we prove that even for a single ReLU neuron, it is impossible to characterize
the implicit regularization with the square loss by any explicit function of
the model parameters (although on the positive side, we show it can be
characterized approximately). For one hidden-layer networks, we prove a similar
result, where in general it is impossible to characterize implicit
regularization properties in this manner, except for the "balancedness"
property identified in Du et al. [2018]. Our results suggest that a more
general framework than the one considered so far may be needed to understand
implicit regularization for nonlinear predictors, and provides some clues on
what this framework should be.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1"&gt;Gal Vardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-MLP: Node Classification without Message Passing in Graph. (arXiv:2106.04051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04051</id>
        <link href="http://arxiv.org/abs/2106.04051"/>
        <updated>2021-06-09T02:01:50.997Z</updated>
        <summary type="html"><![CDATA[Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing
with non-Euclidean structural data. Both spatial-based and spectral-based GNNs
are relying on adjacency matrix to guide message passing among neighbors during
feature aggregation. Recent works have mainly focused on powerful message
passing modules, however, in this paper, we show that none of the message
passing modules is necessary. Instead, we propose a pure
multilayer-perceptron-based framework, Graph-MLP with the supervision signal
leveraging graph structure, which is sufficient for learning discriminative
node representation. In model-level, Graph-MLP only includes multi-layer
perceptrons, activation function, and layer normalization. In the loss level,
we design a neighboring contrastive (NContrast) loss to bridge the gap between
GNNs and MLPs by utilizing the adjacency information implicitly. This design
allows our model to be lighter and more robust when facing large-scale graph
data and corrupted adjacency information. Extensive experiments prove that even
without adjacency information in testing phase, our framework can still reach
comparable and even superior performance against the state-of-the-art models in
the graph node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1"&gt;Haoxuan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhecan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust R-Peak Detection in Low-Quality Holter ECGs using 1D Convolutional Neural Network. (arXiv:2101.01666v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01666</id>
        <link href="http://arxiv.org/abs/2101.01666"/>
        <updated>2021-06-09T02:01:50.991Z</updated>
        <summary type="html"><![CDATA[Noise and low quality of ECG signals acquired from Holter or wearable devices
deteriorate the accuracy and robustness of R-peak detection algorithms. This
paper presents a generic and robust system for R-peak detection in Holter ECG
signals. While many proposed algorithms have successfully addressed the problem
of ECG R-peak detection, there is still a notable gap in the performance of
these detectors on such low-quality ECG records. Therefore, in this study, a
novel implementation of the 1D Convolutional Neural Network (CNN) is used
integrated with a verification model to reduce the number of false alarms. This
CNN architecture consists of an encoder block and a corresponding decoder block
followed by a sample-wise classification layer to construct the 1D segmentation
map of R- peaks from the input ECG signal. Once the proposed model has been
trained, it can solely be used to detect R-peaks possibly in a single channel
ECG data stream quickly and accurately, or alternatively, such a solution can
be conveniently employed for real-time monitoring on a lightweight portable
device. The model is tested on two open-access ECG databases: The China
Physiological Signal Challenge (2020) database (CPSC-DB) with more than one
million beats, and the commonly used MIT-BIH Arrhythmia Database (MIT-DB).
Experimental results demonstrate that the proposed systematic approach achieves
99.30% F1-score, 99.69% recall, and 98.91% precision in CPSC-DB, which is the
best R-peak detection performance ever achieved. Compared to all competing
methods, the proposed approach can reduce the false-positives and
false-negatives in Holter ECG signals by more than 54% and 82%, respectively.
Results also demonstrate similar or better performance than most competing
algorithms on MIT-DB with 99.83% F1-score, 99.85% recall, and 99.82% precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zahid_M/0/1/0/all/0/1"&gt;Muhammad Uzair Zahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ince_T/0/1/0/all/0/1"&gt;Turker Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Devecioglu_O/0/1/0/all/0/1"&gt;Ozer Can Devecioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tahir_A/0/1/0/all/0/1"&gt;Anas Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive Optimizers by Exploiting Strong Convexity. (arXiv:2104.13790v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13790</id>
        <link href="http://arxiv.org/abs/2104.13790"/>
        <updated>2021-06-09T02:01:50.986Z</updated>
        <summary type="html"><![CDATA[AdaBelief, one of the current best optimizers, demonstrates superior
generalization ability compared to the popular Adam algorithm by viewing the
exponential moving average of observed gradients. AdaBelief is theoretically
appealing in that it has a data-dependent $O(\sqrt{T})$ regret bound when
objective functions are convex, where $T$ is a time horizon. It remains however
an open problem whether the convergence rate can be further improved without
sacrificing its generalization ability. %on how to exploit strong convexity to
further improve the convergence rate of AdaBelief. To this end, we make a first
attempt in this work and design a novel optimization algorithm called
FastAdaBelief that aims to exploit its strong convexity in order to achieve an
even faster convergence rate. In particular, by adjusting the step size that
better considers strong convexity and prevents fluctuation, our proposed
FastAdaBelief demonstrates excellent generalization ability as well as superior
convergence. As an important theoretical contribution, we prove that
FastAdaBelief attains a data-dependant $O(\log T)$ regret bound, which is
substantially lower than AdaBelief. On the empirical side, we validate our
theoretical analysis with extensive experiments in both scenarios of strong and
non-strong convexity on three popular baseline models. Experimental results are
very encouraging: FastAdaBelief converges the quickest in comparison to all
mainstream algorithms while maintaining an excellent generalization ability, in
cases of both strong or non-strong convexity. FastAdaBelief is thus posited as
a new benchmark model for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yangfan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Cheng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Amir Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08248</id>
        <link href="http://arxiv.org/abs/2102.08248"/>
        <updated>2021-06-09T02:01:50.980Z</updated>
        <summary type="html"><![CDATA[Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1"&gt;Jakob D. Havtorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1"&gt;Lars Maal&amp;#xf8;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Vision Transformers. (arXiv:2106.04560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04560</id>
        <link href="http://arxiv.org/abs/2106.04560"/>
        <updated>2021-06-09T02:01:50.974Z</updated>
        <summary type="html"><![CDATA[Attention-based neural networks such as the Vision Transformer (ViT) have
recently attained state-of-the-art results on many computer vision benchmarks.
Scale is a primary ingredient in attaining excellent results, therefore,
understanding a model's scaling properties is a key to designing future
generations effectively. While the laws for scaling Transformer language models
have been studied, it is unknown how Vision Transformers scale. To address
this, we scale ViT models and data, both up and down, and characterize the
relationships between error rate, data, and compute. Along the way, we refine
the architecture and training of ViT, reducing memory consumption and
increasing accuracy the resulting models. As a result, we successfully train a
ViT model with two billion parameters, which attains a new state-of-the-art on
ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot
learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10
examples per class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Geometry and Density: Path Distances on High-Dimensional Data. (arXiv:2012.09385v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09385</id>
        <link href="http://arxiv.org/abs/2012.09385"/>
        <updated>2021-06-09T02:01:50.958Z</updated>
        <summary type="html"><![CDATA[New geometric and computational analyses of power-weighted shortest-path
distances (PWSPDs) are presented. By illuminating the way these metrics balance
density and geometry in the underlying data, we clarify their key parameters
and discuss how they may be chosen in practice. Comparisons are made with
related data-driven metrics, which illustrate the broader role of density in
kernel-based unsupervised and semi-supervised machine learning.
Computationally, we relate PWSPDs on complete weighted graphs to their
analogues on weighted nearest neighbor graphs, providing high probability
guarantees on their equivalence that are near-optimal. Connections with
percolation theory are developed to establish estimates on the bias and
variance of PWSPDs in the finite sample setting. The theoretical results are
bolstered by illustrative experiments, demonstrating the versatility of PWSPDs
for a wide range of data settings. Throughout the paper, our results require
only that the underlying data is sampled from a low-dimensional manifold, and
depend crucially on the intrinsic dimension of this manifold, rather than its
ambient dimension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Little_A/0/1/0/all/0/1"&gt;Anna Little&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McKenzie_D/0/1/0/all/0/1"&gt;Daniel McKenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murphy_J/0/1/0/all/0/1"&gt;James Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoequivariant Network Search via Group Decomposition. (arXiv:2104.04848v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04848</id>
        <link href="http://arxiv.org/abs/2104.04848"/>
        <updated>2021-06-09T02:01:50.952Z</updated>
        <summary type="html"><![CDATA[Recent works show that group equivariance as an inductive bias improves
neural network performance for both classification and generation. However,
designing group-equivariant neural networks is challenging when the group of
interest is large and is unknown. Moreover, inducing equivariance can
significantly reduce the number of independent parameters in a network with
fixed feature size, affecting its overall performance. We address these
problems by proving a new group-theoretic result in the context of equivariant
neural networks that shows that a network is equivariant to a large group if
and only if it is equivariant to smaller groups from which it is constructed.
Using this result, we design a novel fast group equivariant construction
algorithm, and a deep Q-learning-based search algorithm in a reduced search
space, yielding what we call autoequivariant networks (AENs). AENs find the
right balance between equivariance and network size when tested on new
benchmark datasets, G-MNIST and G-Fashion-MNIST, obtained via group
transformations on MNIST and Fashion-MNIST respectively that we release.
Extending these results to group convolutional neural networks, where we
optimize between equivariances, augmentations, and network sizes, we find group
equivariance to be the most dominating factor in all high-performing GCNNs on
several datasets like CIFAR10, SVHN, RotMNIST, ASL, EMNIST, and KMNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Sourya Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magesh_A/0/1/0/all/0/1"&gt;Akshayaa Magesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_H/0/1/0/all/0/1"&gt;Harshit Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1"&gt;Lav R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-06-09T02:01:50.946Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-paced ensemble learning for speech and audio classification. (arXiv:2103.11988v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11988</id>
        <link href="http://arxiv.org/abs/2103.11988"/>
        <updated>2021-06-09T02:01:50.931Z</updated>
        <summary type="html"><![CDATA[Combining multiple machine learning models into an ensemble is known to
provide superior performance levels compared to the individual components
forming the ensemble. This is because models can complement each other in
taking better decisions. Instead of just combining the models, we propose a
self-paced ensemble learning scheme in which models learn from each other over
several iterations. During the self-paced learning process based on
pseudo-labeling, in addition to improving the individual models, our ensemble
also gains knowledge about the target domain. To demonstrate the generality of
our self-paced ensemble learning (SPEL) scheme, we conduct experiments on three
audio tasks. Our empirical results indicate that SPEL significantly outperforms
the baseline ensemble models. We also show that applying self-paced learning on
individual models is less effective, illustrating the idea that models in the
ensemble actually learn from each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1"&gt;Nicolae-Catalin Ristea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch-wise++ Perturbation for Adversarial Targeted Attacks. (arXiv:2012.15503v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15503</id>
        <link href="http://arxiv.org/abs/2012.15503"/>
        <updated>2021-06-09T02:01:50.926Z</updated>
        <summary type="html"><![CDATA[Although great progress has been made on adversarial attacks for deep neural
networks (DNNs), their transferability is still unsatisfactory, especially for
targeted attacks. There are two problems behind that have been long overlooked:
1) the conventional setting of $T$ iterations with the step size of
$\epsilon/T$ to comply with the $\epsilon$-constraint. In this case, most of
the pixels are allowed to add very small noise, much less than $\epsilon$; and
2) usually manipulating pixel-wise noise. However, features of a pixel
extracted by DNNs are influenced by its surrounding regions, and different DNNs
generally focus on different discriminative regions in recognition. To tackle
these issues, our previous work proposes a patch-wise iterative method (PIM)
aimed at crafting adversarial examples with high transferability. Specifically,
we introduce an amplification factor to the step size in each iteration, and
one pixel's overall gradient overflowing the $\epsilon$-constraint is properly
assigned to its surrounding regions by a project kernel. But targeted attacks
aim to push the adversarial examples into the territory of a specific class,
and the amplification factor may lead to underfitting. Thus, we introduce the
temperature and propose a patch-wise++ iterative method (PIM++) to further
improve transferability without significantly sacrificing the performance of
the white-box attack. Our method can be generally integrated to any
gradient-based attack methods. Compared with the current state-of-the-art
attack methods, we significantly improve the success rate by 33.1\% for defense
models and 31.4\% for normally trained models on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled Zero-Touch 6G Networks: Model-Free DRL Approach. (arXiv:2103.03817v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03817</id>
        <link href="http://arxiv.org/abs/2103.03817"/>
        <updated>2021-06-09T02:01:50.919Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Zero-Touch, deep reinforcement learning
(DRL)-based Proactive Failure Recovery framework called ZT-PFR for stateful
network function virtualization (NFV)-enabled networks. To this end, we
formulate a resource-efficient optimization problem minimizing the network cost
function including resource cost and wrong decision penalty. As a solution, we
propose state-of-the-art DRL-based methods such as soft-actor-critic (SAC) and
proximal-policy-optimization (PPO). In addition, to train and test our DRL
agents, we propose a novel impending failure model. Moreover, to keep network
status information at an acceptable freshness level for appropriate
decision-making, we apply the concept of age of information to strike a balance
between the event and scheduling-based monitoring. Several key systems and DRL
algorithm design insights for ZT-PFR are drawn from our analysis and simulation
results. For example, we use a hybrid neural network, consisting long
short-term memory layers in the DRL agents]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shaghaghi_A/0/1/0/all/0/1"&gt;Amirhossein Shaghaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zakeri_A/0/1/0/all/0/1"&gt;Abolfazl Zakeri&lt;/a&gt; (Student Member, IEEE), &lt;a href="http://arxiv.org/find/eess/1/au:+Mokari_N/0/1/0/all/0/1"&gt;Nader Mokari&lt;/a&gt; (Senior Member, IEEE), &lt;a href="http://arxiv.org/find/eess/1/au:+Javan_M/0/1/0/all/0/1"&gt;Mohammad Reza Javan&lt;/a&gt; (Senior Member, IEEE), &lt;a href="http://arxiv.org/find/eess/1/au:+Behdadfar_M/0/1/0/all/0/1"&gt;Mohammad Behdadfar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1"&gt;Eduard A Jorswieck&lt;/a&gt; (Fellow, IEEE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many-Speakers Single Channel Speech Separation with Optimal Permutation Training. (arXiv:2104.08955v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08955</id>
        <link href="http://arxiv.org/abs/2104.08955"/>
        <updated>2021-06-09T02:01:50.907Z</updated>
        <summary type="html"><![CDATA[Single channel speech separation has experienced great progress in the last
few years. However, training neural speech separation for a large number of
speakers (e.g., more than 10 speakers) is out of reach for the current methods,
which rely on the Permutation Invariant Loss (PIT). In this work, we present a
permutation invariant training that employs the Hungarian algorithm in order to
train with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in
comparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified
architecture that can handle the increased number of speakers. Our approach
separates up to $20$ speakers and improves the previous results for large $C$
by a wide margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dovrat_S/0/1/0/all/0/1"&gt;Shaked Dovrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1"&gt;Eliya Nachmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Too-Good-to-be-True Prior to Reduce Shortcut Reliance. (arXiv:2102.06406v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06406</id>
        <link href="http://arxiv.org/abs/2102.06406"/>
        <updated>2021-06-09T02:01:50.901Z</updated>
        <summary type="html"><![CDATA[Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep networks often fail to generalize to
out-of-distribution (o.o.d.) samples. One cause for this shortcoming is that
modern architectures tend to rely on "shortcuts" - superficial features that
correlate with categories without capturing deeper invariants that hold across
contexts. Real-world concepts often possess a complex structure that can vary
superficially across contexts, which can make the most intuitive and promising
solutions in one context not generalize to others. One potential way to improve
o.o.d. generalization is to assume simple solutions are unlikely to be valid
across contexts and avoid them, which we refer to as the too-good-to-be-true
prior. A low-capacity network (LCN) with a shallow architecture should only be
able to learn surface relationships, including shortcuts. We find that LCNs can
serve as shortcut detectors. Furthermore, an LCN's predictions can be used in a
two-stage approach to encourage a high-capacity network (HCN) to rely on deeper
invariant features that should generalize broadly. In particular, items that
the LCN can master are downweighted when training the HCN. Using a modified
version of the CIFAR-10 dataset in which we introduced shortcuts, we found that
the two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated
o.o.d. generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dagaev_N/0/1/0/all/0/1"&gt;Nikolay Dagaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D. Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barry_D/0/1/0/all/0/1"&gt;Daniel N. Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1"&gt;Kaustubh R. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C. Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PILOT: Introducing Transformers for Probabilistic Sound Event Localization. (arXiv:2106.03903v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.03903</id>
        <link href="http://arxiv.org/abs/2106.03903"/>
        <updated>2021-06-09T02:01:50.895Z</updated>
        <summary type="html"><![CDATA[Sound event localization aims at estimating the positions of sound sources in
the environment with respect to an acoustic receiver (e.g. a microphone array).
Recent advances in this domain most prominently focused on utilizing deep
recurrent neural networks. Inspired by the success of transformer architectures
as a suitable alternative to classical recurrent neural networks, this paper
introduces a novel transformer-based sound event localization framework, where
temporal dependencies in the received multi-channel audio signals are captured
via self-attention mechanisms. Additionally, the estimated sound event
positions are represented as multivariate Gaussian variables, yielding an
additional notion of uncertainty, which many previously proposed deep
learning-based systems designed for this application do not provide. The
framework is evaluated on three publicly available multi-source sound event
localization datasets and compared against state-of-the-art methods in terms of
localization error and event detection accuracy. It outperforms all competing
systems on all datasets with statistical significant differences in
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schymura_C/0/1/0/all/0/1"&gt;Christopher Schymura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonninghoff_B/0/1/0/all/0/1"&gt;Benedikt B&amp;#xf6;nninghoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochiai_T/0/1/0/all/0/1"&gt;Tsubasa Ochiai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delcroix_M/0/1/0/all/0/1"&gt;Marc Delcroix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinoshita_K/0/1/0/all/0/1"&gt;Keisuke Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakatani_T/0/1/0/all/0/1"&gt;Tomohiro Nakatani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araki_S/0/1/0/all/0/1"&gt;Shoko Araki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1"&gt;Dorothea Kolossa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video. (arXiv:2103.08834v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08834</id>
        <link href="http://arxiv.org/abs/2103.08834"/>
        <updated>2021-06-09T02:01:50.889Z</updated>
        <summary type="html"><![CDATA[This paper addresses fast semantic segmentation on video.Video segmentation
often calls for real-time, or even fasterthan real-time, processing. One common
recipe for conserving computation arising from feature extraction is to
propagate features of few selected keyframes. However, recent advances in fast
image segmentation make these solutions less attractive. To leverage fast image
segmentation for furthering video segmentation, we propose a simple yet
efficient propagation framework. Specifically, we perform lightweight flow
estimation in 1/8-downscaled image space for temporal warping in segmentation
outpace space. Moreover, we introduce a guided spatially-varying convolution
for fusing segmentations derived from the previous and current frames, to
mitigate propagation error and enable lightweight feature extraction on
non-keyframes. Experimental results on Cityscapes and CamVid show that our
scheme achieves the state-of-the-art accuracy-throughput trade-off on video
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Shih-Po Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si-Cun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Hsiao Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Reinforcement Learning with a Nonparametric Off-Policy Policy Gradient. (arXiv:2010.14771v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14771</id>
        <link href="http://arxiv.org/abs/2010.14771"/>
        <updated>2021-06-09T02:01:50.884Z</updated>
        <summary type="html"><![CDATA[Off-policy Reinforcement Learning (RL) holds the promise of better data
efficiency as it allows sample reuse and potentially enables safe interaction
with the environment. Current off-policy policy gradient methods either suffer
from high bias or high variance, delivering often unreliable estimates. The
price of inefficiency becomes evident in real-world scenarios such as
interaction-driven robot learning, where the success of RL has been rather
limited, and a very high sample cost hinders straightforward application. In
this paper, we propose a nonparametric Bellman equation, which can be solved in
closed form. The solution is differentiable w.r.t the policy parameters and
gives access to an estimation of the policy gradient. In this way, we avoid the
high variance of importance sampling approaches, and the high bias of
semi-gradient methods. We empirically analyze the quality of our gradient
estimate against state-of-the-art methods, and show that it outperforms the
baselines in terms of sample efficiency on classical control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tosatto_S/0/1/0/all/0/1"&gt;Samuele Tosatto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Whitening Batch Normalization. (arXiv:2106.04413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04413</id>
        <link href="http://arxiv.org/abs/2106.04413"/>
        <updated>2021-06-09T02:01:50.859Z</updated>
        <summary type="html"><![CDATA[Batch Normalization (BN) is a popular technique for training Deep Neural
Networks (DNNs). BN uses scaling and shifting to normalize activations of
mini-batches to accelerate convergence and improve generalization. The recently
proposed Iterative Normalization (IterNorm) method improves these properties by
whitening the activations iteratively using Newton's method. However, since
Newton's method initializes the whitening matrix independently at each training
step, no information is shared between consecutive steps. In this work, instead
of exact computation of whitening matrix at each time step, we estimate it
gradually during training in an online fashion, using our proposed Stochastic
Whitening Batch Normalization (SWBN) algorithm. We show that while SWBN
improves the convergence rate and generalization of DNNs, its computational
overhead is less than that of IterNorm. Due to the high efficiency of the
proposed method, it can be easily employed in most DNN architectures with a
large number of layers. We provide comprehensive experiments and comparisons
between BN, IterNorm, and SWBN layers to demonstrate the effectiveness of the
proposed technique in conventional (many-shot) image classification and
few-shot classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nezhadarya_E/0/1/0/all/0/1"&gt;Ehsan Nezhadarya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fashandi_H/0/1/0/all/0/1"&gt;Homa Fashandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_D/0/1/0/all/0/1"&gt;Darin Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of marine floating plastic using Sentinel-2 imagery and machine learning models. (arXiv:2106.03694v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03694</id>
        <link href="http://arxiv.org/abs/2106.03694"/>
        <updated>2021-06-09T02:01:50.835Z</updated>
        <summary type="html"><![CDATA[The increasing level of marine plastic pollution poses severe threats to the
marine ecosystem and biodiversity. The present study attempted to explore the
full functionality of open Sentinel satellite data and ML models for detecting
and classifying floating plastic debris in Mytilene (Greece), Limassol
(Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support
Vector Machine (SVM) and Random Forest (RF) were utilized to carry out the
classification analysis. In-situ plastic location data was collected from the
control experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the
same was considered for training the models. Both remote sensing bands and
spectral indices were used for developing the ML models. A spectral signature
profile for plastic was created for discriminating the floating plastic from
other marine debris. A newly developed index, kernel Normalized Difference
Vegetation Index (kNDVI), was incorporated into the modelling to examine its
contribution to model performances. Both SVM and RF were performed well in five
models and test case combinations. Among the two ML models, the highest
performance was measured for the RF. The inclusion of kNDVI was found effective
and increased the model performances, reflected by high balanced accuracy
measured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using
the best-performed model, an automated floating plastic detection system was
developed and tested in Calabria and Beirut. For both sites, the trained model
had detected the floating plastic with ~99% accuracy. Among the six predictors,
the FDI was found the most important variable for detecting marine floating
plastic. These findings collectively suggest that high-resolution remote
sensing imagery and the automated ML models can be an effective alternative for
the cost-effective detection of marine floating plastic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1"&gt;Srikanta Sannigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_B/0/1/0/all/0/1"&gt;Bidroha Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Arunima Sarkar Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilla_F/0/1/0/all/0/1"&gt;Francesco Pilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games. (arXiv:2102.11494v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11494</id>
        <link href="http://arxiv.org/abs/2102.11494"/>
        <updated>2021-06-09T02:01:50.821Z</updated>
        <summary type="html"><![CDATA[Real world applications such as economics and policy making often involve
solving multi-agent games with two unique features: (1) The agents are
inherently asymmetric and partitioned into leaders and followers; (2) The
agents have different reward functions, thus the game is general-sum. The
majority of existing results in this field focuses on either symmetric solution
concepts (e.g. Nash equilibrium) or zero-sum games. It remains vastly open how
to learn the Stackelberg equilibrium -- an asymmetric analog of the Nash
equilibrium -- in general-sum games efficiently from samples.

This paper initiates the theoretical study of sample-efficient learning of
the Stackelberg equilibrium, in the bandit feedback setting where we only
observe noisy samples of the reward. We consider three representative
two-player general-sum games: bandit games, bandit-reinforcement learning
(bandit-RL) games, and linear bandit games. In all these games, we identify a
fundamental gap between the exact value of the Stackelberg equilibrium and its
estimated version using finitely many noisy samples, which can not be closed
information-theoretically regardless of the algorithm. We then establish sharp
positive results on sample-efficient learning of Stackelberg equilibrium with
value optimal up to the gap identified above, with matching lower bounds in the
dependency on the gap, error tolerance, and the size of the action spaces.
Overall, our results unveil unique challenges in learning Stackelberg
equilibria under noisy bandit feedback, which we hope could shed light on
future research on this topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes. (arXiv:2102.12894v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12894</id>
        <link href="http://arxiv.org/abs/2102.12894"/>
        <updated>2021-06-09T02:01:50.804Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are notorious for making more mistakes for the
classes that have substantially fewer samples than the others during training.
Such class imbalance is ubiquitous in clinical applications and very crucial to
handle because the classes with fewer samples most often correspond to critical
cases (e.g., cancer) where misclassifications can have severe consequences. Not
to miss such cases, binary classifiers need to be operated at high True
Positive Rates (TPR) by setting a higher threshold but this comes at the cost
of very high False Positive Rates (FPR) for problems with class imbalance.
Existing methods for learning under class imbalance most often do not take this
into account. We argue that prediction accuracy should be improved by
emphasizing reducing FPRs at high TPRs for problems where misclassification of
the positive, i.e., critical, class samples are associated with higher cost. To
this end, we pose the training of a DNN for binary classification as a
constrained optimization problem and introduce a novel constraint that can be
used with existing loss functions to enforce maximal area under the ROC curve
(AUC) through prioritizing FPR reduction at high TPR. We solve the resulting
constrained optimization problem using an Augmented Lagrangian method (ALM).
Going beyond binary, we also propose two possible extensions of the proposed
constraint for multi-class classification problems. We present experimental
results for image-based binary and multi-class classification applications
using an in-house medical imaging dataset, CIFAR10, and CIFAR100. Our results
demonstrate that the proposed method improves the baselines in majority of the
cases by attaining higher accuracy on critical classes while reducing the
misclassification rate for the non-critical class samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sangalli_S/0/1/0/all/0/1"&gt;Sara Sangalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1"&gt;Ertunc Erdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoetker_A/0/1/0/all/0/1"&gt;Andreas Hoetker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donati_O/0/1/0/all/0/1"&gt;Olivio Donati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1"&gt;Ender Konukoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration. (arXiv:2105.06411v1 [cs.RO] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.06411</id>
        <link href="http://arxiv.org/abs/2105.06411"/>
        <updated>2021-06-09T02:01:50.790Z</updated>
        <summary type="html"><![CDATA[We introduce a simple new method for visual imitation learning, which allows
a novel robot manipulation task to be learned from a single human
demonstration, without requiring any prior knowledge of the object being
interacted with. Our method models imitation learning as a state estimation
problem, with the state defined as the end-effector's pose at the point where
object interaction begins, as observed from the demonstration. By modelling a
manipulation task as a coarse, approach trajectory followed by a fine,
interaction trajectory, this state estimator can be trained in a
self-supervised manner, by automatically moving the end-effector's camera
around the object. At test time, the end-effector is moved to the estimated
state through a linear path, at which point the demonstration's end-effector
velocities are simply repeated, enabling convenient acquisition of a complex
interaction trajectory without actually needing to explicitly learn a policy.
Real-world experiments on 8 everyday tasks show that our method can learn a
diverse range of skills from just a single human demonstration, whilst also
yielding a stable and interpretable controller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential- and Parallel- Constrained Max-value Entropy Search via Information Lower Bound. (arXiv:2102.09788v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09788</id>
        <link href="http://arxiv.org/abs/2102.09788"/>
        <updated>2021-06-09T02:01:50.783Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) is known as a powerful tool for optimizing an
unknown, expensive function through querying the function values sequentially.
On the other hand, in many practical problems, additional unknown constraints
also need to be considered. In this paper, we propose an information-theoretic
approach called Constrained Max-value Entropy Search via Information lower
BOund (CMES-IBO) for the constrained BO (CBO). Although information-theoretic
methods have been studied in CBO literature, they have not revealed any
relation between their acquisition functions and the original mutual
information. In contrast, our acquisition function is an unbiased consistent
estimator of a lower bound of mutual information. We show that our CMES-IBO has
several advantageous properties such as non-negativity, estimation error bounds
of the acquisition function, and well-definedness of the criterion, none of
which have been shown for the existing information-theoretic CBO. Furthermore,
by using conditional mutual information, we extend CMES-IBO to the parallel
setting in which multiple queries can be issued simultaneously. We demonstrate
the effectiveness of CMES-IBO by several benchmark functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takeno_S/0/1/0/all/0/1"&gt;Shion Takeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamura_T/0/1/0/all/0/1"&gt;Tomoyuki Tamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shitara_K/0/1/0/all/0/1"&gt;Kazuki Shitara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karasuyama_M/0/1/0/all/0/1"&gt;Masayuki Karasuyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Shifted Contrastive Loss for Anomaly Detection. (arXiv:2106.03844v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.03844</id>
        <link href="http://arxiv.org/abs/2106.03844"/>
        <updated>2021-06-09T02:01:50.764Z</updated>
        <summary type="html"><![CDATA[Deep anomaly detection methods learn representations that separate between
normal and anomalous samples. Very effective representations are obtained when
powerful externally trained feature extractors (e.g. ResNets pre-trained on
ImageNet) are fine-tuned on the training data which consists of normal samples
and no anomalies. However, this is a difficult task that can suffer from
catastrophic collapse, i.e. it is prone to learning trivial and non-specific
features. In this paper, we propose a new loss function which can overcome
failure modes of both center-loss and contrastive-loss methods. Furthermore, we
combine it with a confidence-invariant angular center loss, which replaces the
Euclidean distance used in previous work, that was sensitive to prediction
confidence. Our improvements yield a new anomaly detection approach, based on
$\textit{Mean-Shifted Contrastive Loss}$, which is both more accurate and less
sensitive to catastrophic collapse than previous methods. Our method achieves
state-of-the-art anomaly detection performance on multiple benchmarks including
$97.5\%$ ROC-AUC on the CIFAR-10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiss_T/0/1/0/all/0/1"&gt;Tal Reiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data. (arXiv:2005.00792v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00792</id>
        <link href="http://arxiv.org/abs/2005.00792"/>
        <updated>2021-06-09T02:01:50.758Z</updated>
        <summary type="html"><![CDATA[Event forecasting is a challenging, yet important task, as humans seek to
constantly plan for the future. Existing automated forecasting studies rely
mostly on structured data, such as time-series or event-based knowledge graphs,
to help predict future events. In this work, we aim to formulate a task,
construct a dataset, and provide benchmarks for developing methods for event
forecasting with large volumes of unstructured text data. To simulate the
forecasting scenario on temporal news documents, we formulate the problem as a
restricted-domain, multiple-choice, question-answering (QA) task. Unlike
existing QA tasks, our task limits accessible information, and thus a model has
to make a forecasting judgement. To showcase the usefulness of this task
formulation, we introduce ForecastQA, a question-answering dataset consisting
of 10,392 event forecasting questions, which have been collected and verified
via crowdsourcing efforts. We present our experiments on ForecastQA using
BERT-based models and find that our best model achieves 60.1% accuracy on the
dataset, which still lags behind human performance by about 19%. We hope
ForecastQA will support future research efforts in bridging this gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Woojeong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1"&gt;Rahul Khanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Suji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dong-Ho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1"&gt;Fred Morstatter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Thompson Sampling using Sparse Gaussian Process Models. (arXiv:2006.05356v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05356</id>
        <link href="http://arxiv.org/abs/2006.05356"/>
        <updated>2021-06-09T02:01:50.752Z</updated>
        <summary type="html"><![CDATA[Thompson Sampling (TS) from Gaussian Process (GP) models is a powerful tool
for the optimization of black-box functions. Although TS enjoys strong
theoretical guarantees and convincing empirical performance, it incurs a large
computational overhead that scales polynomially with the optimization budget.
Recently, scalable TS methods based on sparse GP models have been proposed to
increase the scope of TS, enabling its application to problems that are
sufficiently multi-modal, noisy or combinatorial to require more than a few
hundred evaluations to be solved. However, the approximation error introduced
by sparse GPs invalidates all existing regret bounds. In this work, we perform
a theoretical and empirical analysis of scalable TS. We provide theoretical
guarantees and show that the drastic reduction in computational complexity of
scalable TS can be enjoyed without loss in the regret performance over the
standard TS. These conceptual claims are validated for practical
implementations of scalable TS on synthetic benchmarks and as part of a
real-world high-throughput molecular design task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vakili_S/0/1/0/all/0/1"&gt;Sattar Vakili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moss_H/0/1/0/all/0/1"&gt;Henry Moss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Artemev_A/0/1/0/all/0/1"&gt;Artem Artemev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dutordoir_V/0/1/0/all/0/1"&gt;Vincent Dutordoir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Picheny_V/0/1/0/all/0/1"&gt;Victor Picheny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04270</id>
        <link href="http://arxiv.org/abs/2102.04270"/>
        <updated>2021-06-09T02:01:50.736Z</updated>
        <summary type="html"><![CDATA[The ever-growing computational demands of increasingly complex machine
learning models frequently necessitate the use of powerful cloud-based
infrastructure for their training. Binary neural networks are known to be
promising candidates for on-device inference due to their extreme compute and
memory savings over higher-precision alternatives. However, their existing
training methods require the concurrent storage of high-precision activations
for all layers, generally making learning on memory-constrained devices
infeasible. In this paper, we demonstrate that the backward propagation
operations needed for binary neural network training are strongly robust to
quantization, thereby making on-the-edge learning with modern models a
practical proposition. We introduce a low-cost binary neural network training
strategy exhibiting sizable memory footprint and energy reductions while
inducing little to no accuracy loss vs Courbariaux & Bengio's standard
approach. These resource decreases are primarily enabled through the retention
of activations exclusively in binary format. Against the latter algorithm, our
drop-in replacement sees coincident memory requirement and energy consumption
drops of 2--6$\times$, while reaching similar test accuracy in comparable time,
across a range of small-scale models trained to classify popular datasets. We
also demonstrate from-scratch ImageNet training of binarized ResNet-18,
achieving a 3.12$\times$ memory reduction. Such savings will allow for
unnecessary cloud offloading to be avoided, reducing latency, increasing energy
efficiency and safeguarding privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Erwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James J. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moro_D/0/1/0/all/0/1"&gt;Daniele Moro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zielinski_P/0/1/0/all/0/1"&gt;Piotr Zielinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1"&gt;Claudionor Coelho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Satrajit Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1"&gt;Peter Y. K. Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinides_G/0/1/0/all/0/1"&gt;George A. Constantinides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoCL: Contrastive Learning on Molecular Graphs with Multi-level Domain Knowledge. (arXiv:2106.04509v1 [physics.bio-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04509</id>
        <link href="http://arxiv.org/abs/2106.04509"/>
        <updated>2021-06-09T02:01:50.730Z</updated>
        <summary type="html"><![CDATA[Recent years have seen a rapid growth of utilizing graph neural networks
(GNNs) in the biomedical domain for tackling drug-related problems. However,
like any other deep architectures, GNNs are data hungry. While requiring labels
in real world is often expensive, pretraining GNNs in an unsupervised manner
has been actively explored. Among them, graph contrastive learning, by
maximizing the mutual information between paired graph augmentations, has been
shown to be effective on various downstream tasks. However, the current graph
contrastive learning framework has two limitations. First, the augmentations
are designed for general graphs and thus may not be suitable or powerful enough
for certain domains. Second, the contrastive scheme only learns representations
that are invariant to local perturbations and thus does not consider the global
structure of the dataset, which may also be useful for downstream tasks.
Therefore, in this paper, we study graph contrastive learning in the context of
biomedical domain, where molecular graphs are present. We propose a novel
framework called MoCL, which utilizes domain knowledge at both local- and
global-level to assist representation learning. The local-level domain
knowledge guides the augmentation process such that variation is introduced
without changing graph semantics. The global-level knowledge encodes the
similarity information between graphs in the entire dataset and helps to learn
representations with richer semantics. The entire model is learned through a
double contrast objective. We evaluate MoCL on various molecular datasets under
both linear and semi-supervised settings and results show that MoCL achieves
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mengying Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xing_J/0/1/0/all/0/1"&gt;Jing Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Worst-Case Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:2010.12163v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12163</id>
        <link href="http://arxiv.org/abs/2010.12163"/>
        <updated>2021-06-09T02:01:50.723Z</updated>
        <summary type="html"><![CDATA[This paper studies regret minimization with randomized value functions in
reinforcement learning. In tabular finite-horizon Markov Decision Processes, we
introduce a clipping variant of one classical Thompson Sampling (TS)-like
algorithm, randomized least-squares value iteration (RLSVI). Our
$\tilde{\mathrm{O}}(H^2S\sqrt{AT})$ high-probability worst-case regret bound
improves the previous sharpest worst-case regret bounds for RLSVI and matches
the existing state-of-the-art worst-case TS-based regret bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Priyank Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinglin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Softmax Policy Gradient Methods Can Take Exponential Time to Converge. (arXiv:2102.11270v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11270</id>
        <link href="http://arxiv.org/abs/2102.11270"/>
        <updated>2021-06-09T02:01:50.723Z</updated>
        <summary type="html"><![CDATA[The softmax policy gradient (PG) method, which performs gradient ascent under
softmax policy parameterization, is arguably one of the de facto
implementations of policy optimization in modern reinforcement learning. For
$\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs),
remarkable progress has recently been achieved towards establishing global
convergence of softmax PG methods in finding a near-optimal policy. However,
prior results fall short of delineating clear dependencies of convergence rates
on salient parameters such as the cardinality of the state space $\mathcal{S}$
and the effective horizon $\frac{1}{1-\gamma}$, both of which could be
excessively large. In this paper, we deliver a pessimistic message regarding
the iteration complexity of softmax PG methods, despite assuming access to
exact gradient computation. Specifically, we demonstrate that the softmax PG
method with stepsize $\eta$ can take \[

\frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}}
~\text{iterations} \] to converge, even in the presence of a benign policy
initialization and an initial state distribution amenable to exploration (so
that the distribution mismatch coefficient is not exceedingly large). This is
accomplished by characterizing the algorithmic dynamics over a
carefully-constructed MDP containing only three actions. Our exponential lower
bound hints at the necessity of carefully adjusting update rules or enforcing
proper regularization in accelerating PG methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yuting Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yuejie Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuantao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation and Learning with Deep Convolutional Models: a Kernel Perspective. (arXiv:2102.10032v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10032</id>
        <link href="http://arxiv.org/abs/2102.10032"/>
        <updated>2021-06-09T02:01:50.713Z</updated>
        <summary type="html"><![CDATA[The empirical success of deep convolutional networks on tasks involving
high-dimensional data such as images or audio suggests that they can
efficiently approximate certain functions that are well-suited for such tasks.
In this paper, we study this through the lens of kernel methods, by considering
simple hierarchical kernels with two or three convolution and pooling layers,
inspired by convolutional kernel networks. These achieve good empirical
performance on standard vision datasets, while providing a simple enough
description of the functional space to shed light on their inductive bias. We
show that the RKHS consists of additive models of interaction terms between
patches, and that its norm encourages structured spatial similarities between
these terms through pooling layers. We then provide generalization bounds which
illustrate how pooling yields improved sample complexity guarantees when the
target function presents such regularities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Image Reconstruction using Deep Generative Models. (arXiv:2012.04567v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04567</id>
        <link href="http://arxiv.org/abs/2012.04567"/>
        <updated>2021-06-09T02:01:50.701Z</updated>
        <summary type="html"><![CDATA[Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method,
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We keep the weights of the generator model fixed, and
reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)
estimate over the input latent vector that generated the reconstructed image.
We further use variational inference to approximate the posterior distribution
over the latent vectors, from which we sample multiple solutions. We
demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from
the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III
and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.
Across all three datasets and without any dataset-specific hyperparameter
tuning, our simple approach yields performance competitive with current
task-specific state-of-the-art methods on super-resolution and in-painting,
while being more generalisable and without requiring any training. Our source
code and pre-trained models are available online:
https://razvanmarinescu.github.io/brgm/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Razvan V Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1"&gt;Daniel Moyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSGP-CUDA -- a CUDA framework for Geometric Semantic Genetic Programming. (arXiv:2106.04034v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.04034</id>
        <link href="http://arxiv.org/abs/2106.04034"/>
        <updated>2021-06-09T02:01:50.700Z</updated>
        <summary type="html"><![CDATA[Geometric Semantic Genetic Programming (GSGP) is a state-of-the-art machine
learning method based on evolutionary computation. GSGP performs search
operations directly at the level of program semantics, which can be done more
efficiently then operating at the syntax level like most GP systems. Efficient
implementations of GSGP in C++ exploit this fact, but not to its full
potential. This paper presents GSGP-CUDA, the first CUDA implementation of GSGP
and the most efficient, which exploits the intrinsic parallelism of GSGP using
GPUs. Results show speedups greater than 1,000X relative to the
state-of-the-art sequential implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trujillo_L/0/1/0/all/0/1"&gt;Leonardo Trujillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Contreras_J/0/1/0/all/0/1"&gt;Jose Manuel Mu&amp;#xf1;oz Contreras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1"&gt;Daniel E Hernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castelli_M/0/1/0/all/0/1"&gt;Mauro Castelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1"&gt;Juan J Tapia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Mediated, Object-Centric Representation Learning. (arXiv:2012.15814v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15814</id>
        <link href="http://arxiv.org/abs/2012.15814"/>
        <updated>2021-06-09T02:01:50.700Z</updated>
        <summary type="html"><![CDATA[We present Language-mediated, Object-centric Representation Learning (LORL),
a paradigm for learning disentangled, object-centric scene representations from
vision and language. LORL builds upon recent advances in unsupervised object
discovery and segmentation, notably MONet and Slot Attention. While these
algorithms learn an object-centric representation just by reconstructing the
input image, LORL enables them to further learn to associate the learned
representations to concepts, i.e., words for object categories, properties, and
spatial relationships, from language input. These object-centric concepts
derived from language facilitate the learning of object-centric
representations. LORL can be integrated with various unsupervised object
discovery algorithms that are language-agnostic. Experiments show that the
integration of LORL consistently improves the performance of unsupervised
object discovery methods on two datasets via the help of language. We also show
that concepts learned by LORL, in conjunction with object discovery methods,
aid downstream tasks such as referring expression comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recombine and Resample Data for Compositional Generalization. (arXiv:2010.03706v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03706</id>
        <link href="http://arxiv.org/abs/2010.03706"/>
        <updated>2021-06-09T02:01:50.689Z</updated>
        <summary type="html"><![CDATA[Flexible neural sequence models outperform grammar- and automaton-based
counterparts on a variety of tasks. However, neural models perform poorly in
settings requiring compositional generalization beyond the training data --
particularly to rare or unseen subsequences. Past work has found symbolic
scaffolding (e.g. grammars or automata) essential in these settings. We
describe R&R, a learned data augmentation scheme that enables a large category
of compositional generalizations without appeal to latent symbolic structure.
R&R has two components: recombination of original training examples via a
prototype-based generative model and resampling of generated examples to
encourage extrapolation. Training an ordinary neural sequence model on a
dataset augmented with recombined and resampled examples significantly improves
generalization in two language processing problems -- instruction following
(SCAN) and morphological analysis (SIGMORPHON 2018) -- where R&R enables
learning of new constructions and tenses from as few as eight initial examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1"&gt;Afra Feyza Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08604</id>
        <link href="http://arxiv.org/abs/2102.08604"/>
        <updated>2021-06-09T02:01:50.686Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) methods aim to achieve generalizability to an
unseen target domain by using only training data from the source domains.
Although a variety of DG methods have been proposed, a recent study shows that
under a fair evaluation protocol, called DomainBed, the simple empirical risk
minimization (ERM) approach works comparable to or even outperforms previous
methods. Unfortunately, simply solving ERM on a complex, non-convex loss
function can easily lead to sub-optimal generalizability by seeking sharp
minima. In this paper, we theoretically show that finding flat minima results
in a smaller domain generalization gap. We also propose a simple yet effective
method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.
SWAD finds flatter minima and suffers less from overfitting than does the
vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.
SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,
VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large
margins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with
conventional generalization methods, such as data augmentation and consistency
regularization methods, to verify that the remarkable performance improvements
are originated from by seeking flat minima, not from better in-domain
generalizability. Last but not least, SWAD is readily adaptable to existing DG
methods without modification; the combination of SWAD and an existing DG method
further improves DG performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1"&gt;Junbum Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Han-Cheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic selection of clustering algorithms using supervised graph embedding. (arXiv:2011.08225v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08225</id>
        <link href="http://arxiv.org/abs/2011.08225"/>
        <updated>2021-06-09T02:01:50.685Z</updated>
        <summary type="html"><![CDATA[The widespread adoption of machine learning (ML) techniques and the extensive
expertise required to apply them have led to increased interest in automated ML
solutions that reduce the need for human intervention. One of the main
challenges in applying ML to previously unseen problems is algorithm selection
- the identification of high-performing algorithm(s) for a given dataset, task,
and evaluation measure. This study addresses the algorithm selection challenge
for data clustering, a fundamental task in data mining that is aimed at
grouping similar objects. We present MARCO-GE, a novel meta-learning approach
for the automated recommendation of clustering algorithms. MARCO-GE first
transforms datasets into graphs and then utilizes a graph convolutional neural
network technique to extract their latent representation. Using the embedding
representations obtained, MARCO-GE trains a ranking meta-model capable of
accurately recommending top-performing algorithms for a new dataset and
clustering evaluation measure. Extensive evaluation on 210 datasets, 13
clustering algorithms, and 10 clustering measures demonstrates the
effectiveness of our approach and its superiority in terms of predictive and
generalization performance over state-of-the-art clustering meta-learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Shapira_N/0/1/0/all/0/1"&gt;Noy Cohen-Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1"&gt;Lior Rokach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Elastic Lottery Ticket Hypothesis. (arXiv:2103.16547v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16547</id>
        <link href="http://arxiv.org/abs/2103.16547"/>
        <updated>2021-06-09T02:01:50.684Z</updated>
        <summary type="html"><![CDATA[Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse
trainable subnetworks, or winning tickets, of training, which can be trained in
isolation to achieve similar or even better performance compared to the full
models. Despite many efforts being made, the most effective method to identify
such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is
computationally expensive and has to be run thoroughly for every different
network. A natural question that comes in is: can we "transform" the winning
ticket found in one network to another with a different architecture, yielding
a winning ticket for the latter at the beginning, without re-doing the
expensive IMP? Answering this question is not only practically relevant for
efficient "once-for-all" winning ticket finding, but also theoretically
appealing for uncovering inherently scalable sparse patterns in networks. We
conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety
of strategies to tweak the winning tickets found from different networks of the
same model family (e.g., ResNets). Based on these results, we articulate the
Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or
dropping) and re-ordering layers for one network, its corresponding winning
ticket could be stretched (or squeezed) into a subnetwork for another deeper
(or shallower) network from the same family, whose performance is nearly the
same competitive as the latter's winning ticket directly found by IMP. We have
also thoroughly compared E-LTH with pruning-at-initialization and dynamic
sparse training methods, and discuss the generalizability of E-LTH to different
model families, layer types, or across datasets. Code is available at
https://github.com/VITA-Group/ElasticLTH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Reordering for Modeling Latent Alignments in Sequence Transduction. (arXiv:2106.03257v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03257</id>
        <link href="http://arxiv.org/abs/2106.03257"/>
        <updated>2021-06-09T02:01:50.683Z</updated>
        <summary type="html"><![CDATA[Despite success in many domains, neural models struggle in settings where
train and test examples are drawn from different distributions. In particular,
in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail
to generalize systematically, i.e., interpret sentences representing novel
combinations of concepts (e.g., text segments) seen in training. Traditional
grammar formalisms excel in such settings by implicitly encoding alignments
between input and output segments, but are hard to scale and maintain. Instead
of engineering a grammar, we directly model segment-to-segment alignments as
discrete structured latent variables within a neural seq2seq model. To
efficiently explore the large space of alignments, we introduce a reorder-first
align-later framework whose central component is a neural reordering module
producing {\it separable} permutations. We present an efficient dynamic
programming algorithm performing exact marginal inference of separable
permutations, and, thus, enabling end-to-end differentiable training of our
model. The resulting seq2seq model exhibits better systematic generalization
than standard models on synthetic problems and NLP tasks (i.e., semantic
parsing and machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Anomaly Detectors: Context Matters. (arXiv:2012.06260v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06260</id>
        <link href="http://arxiv.org/abs/2012.06260"/>
        <updated>2021-06-09T02:01:50.678Z</updated>
        <summary type="html"><![CDATA[Deep generative models are challenging the classical methods in the field of
anomaly detection nowadays. Every new method provides evidence of outperforming
its predecessors, often with contradictory results. The objective of this
comparison is twofold: to compare anomaly detection methods of various
paradigms with focus on deep generative models, and identification of sources
of variability that can yield different results. The methods were compared on
popular tabular and image datasets. We identified the main sources of
variability to be experimental conditions: i) the type data set (tabular or
image) and the nature of anomalies (statistical or semantic), and ii) strategy
of selection of hyperparameters, especially the number of available anomalies
in the validation set. Different methods perform the best in different
contexts, i.e. combination of experimental conditions together with
computational time. This explains the variability of the previous results and
highlights the importance of careful specification of the context in the
publication of a new method. All our code and results are available for
download.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skvara_V/0/1/0/all/0/1"&gt;V&amp;#xed;t &amp;#x160;kv&amp;#xe1;ra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franc%5Cr%7Bu%7D_J/0/1/0/all/0/1"&gt;Jan Franc&amp;#x16f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorek_M/0/1/0/all/0/1"&gt;Mat&amp;#x11b;j Zorek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1"&gt;Tom&amp;#xe1;&amp;#x161; Pevn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smidl_V/0/1/0/all/0/1"&gt;V&amp;#xe1;clav &amp;#x160;m&amp;#xed;dl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NWT: Towards natural audio-to-video generation with representation learning. (arXiv:2106.04283v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04283</id>
        <link href="http://arxiv.org/abs/2106.04283"/>
        <updated>2021-06-09T02:01:50.667Z</updated>
        <summary type="html"><![CDATA[In this work we introduce NWT, an expressive speech-to-video model. Unlike
approaches that use domain-specific intermediate representations such as pose
keypoints, NWT learns its own latent representations, with minimal assumptions
about the audio and video content. To this end, we propose a novel discrete
variational autoencoder with adversarial loss, dVAE-Adv, which learns a new
discrete latent representation we call Memcodes. Memcodes are straightforward
to implement, require no additional loss terms, are stable to train compared
with other approaches, and show evidence of interpretability. To predict on the
Memcode space, we use an autoregressive encoder-decoder model conditioned on
audio. Additionally, our model can control latent attributes in the generated
video that are not annotated in the data. We train NWT on clips from HBO's Last
Week Tonight with John Oliver. NWT consistently scores above other approaches
in Mean Opinion Score (MOS) on tests of overall video naturalness, facial
naturalness and expressiveness, and lipsync quality. This work sets a strong
baseline for generalized audio-to-video synthesis. Samples are available at
https://next-week-tonight.github.io/NWT/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mama_R/0/1/0/all/0/1"&gt;Rayhane Mama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyndel_M/0/1/0/all/0/1"&gt;Marc S. Tyndel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadhim_H/0/1/0/all/0/1"&gt;Hashiam Kadhim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_C/0/1/0/all/0/1"&gt;Cole Clifford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thurairatnam_R/0/1/0/all/0/1"&gt;Ragavan Thurairatnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Learning for Knowledge Distillation. (arXiv:2106.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04570</id>
        <link href="http://arxiv.org/abs/2106.04570"/>
        <updated>2021-06-09T02:01:50.666Z</updated>
        <summary type="html"><![CDATA[We present Meta Learning for Knowledge Distillation (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models. The code is available at
https://github.com/JetRunner/MetaDistil]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Medical Image Alignment with Curriculum Learning. (arXiv:2102.10438v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10438</id>
        <link href="http://arxiv.org/abs/2102.10438"/>
        <updated>2021-06-09T02:01:50.665Z</updated>
        <summary type="html"><![CDATA[We explore different curriculum learning methods for training convolutional
neural networks on the task of deformable pairwise 3D medical image
registration. To the best of our knowledge, we are the first to attempt to
improve performance by training medical image registration models using
curriculum learning, starting from an easy training setup in the first training
stages, and gradually increasing the complexity of the setup. On the one hand,
we consider two existing curriculum learning approaches, namely curriculum
dropout and curriculum by smoothing. On the other hand, we propose a novel and
simple strategy to achieve curriculum, namely to use purposely blurred images
at the beginning, then gradually transit to sharper images in the later
training stages. Our experiments with an underlying state-of-the-art deep
learning model show that curriculum learning can lead to superior results
compared to conventional training. Additionally, we show that curriculum by
input blur has the best accuracy versus speed trade-off among the compared
curriculum learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burduja_M/0/1/0/all/0/1"&gt;Mihail Burduja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Too-Good-to-be-True Prior to Reduce Shortcut Reliance. (arXiv:2102.06406v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06406</id>
        <link href="http://arxiv.org/abs/2102.06406"/>
        <updated>2021-06-09T02:01:50.660Z</updated>
        <summary type="html"><![CDATA[Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep networks often fail to generalize to
out-of-distribution (o.o.d.) samples. One cause for this shortcoming is that
modern architectures tend to rely on "shortcuts" - superficial features that
correlate with categories without capturing deeper invariants that hold across
contexts. Real-world concepts often possess a complex structure that can vary
superficially across contexts, which can make the most intuitive and promising
solutions in one context not generalize to others. One potential way to improve
o.o.d. generalization is to assume simple solutions are unlikely to be valid
across contexts and avoid them, which we refer to as the too-good-to-be-true
prior. A low-capacity network (LCN) with a shallow architecture should only be
able to learn surface relationships, including shortcuts. We find that LCNs can
serve as shortcut detectors. Furthermore, an LCN's predictions can be used in a
two-stage approach to encourage a high-capacity network (HCN) to rely on deeper
invariant features that should generalize broadly. In particular, items that
the LCN can master are downweighted when training the HCN. Using a modified
version of the CIFAR-10 dataset in which we introduced shortcuts, we found that
the two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated
o.o.d. generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dagaev_N/0/1/0/all/0/1"&gt;Nikolay Dagaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D. Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barry_D/0/1/0/all/0/1"&gt;Daniel N. Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1"&gt;Kaustubh R. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C. Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-aware Spatiotemporal Modules with Auxiliary Tasks for Meta-Learning. (arXiv:2006.08831v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08831</id>
        <link href="http://arxiv.org/abs/2006.08831"/>
        <updated>2021-06-09T02:01:50.650Z</updated>
        <summary type="html"><![CDATA[Modeling the dynamics of real-world physical systems is critical for
spatiotemporal prediction tasks, but challenging when data is limited. The
scarcity of real-world data and the difficulty in reproducing the data
distribution hinder directly applying meta-learning techniques. Although the
knowledge of governing partial differential equations (PDE) of data can be
helpful for the fast adaptation to few observations, it is mostly infeasible to
exactly find the equation for observations in real-world physical systems. In
this work, we propose a framework, physics-aware meta-learning with auxiliary
tasks, whose spatial modules incorporate PDE-independent knowledge and temporal
modules utilize the generalized features from the spatial modules to be adapted
to the limited data, respectively. The framework is inspired by a local
conservation law expressed mathematically as a continuity equation and does not
require the exact form of governing equation to model the spatiotemporal
observations. The proposed method mitigates the need for a large number of
real-world tasks for meta-learning by leveraging spatial information in
simulated data to meta-initialize the spatial modules. We apply the proposed
framework to both synthetic and real-world spatiotemporal prediction tasks and
demonstrate its superior performance with limited observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sungyong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chuizheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1"&gt;Sirisha Rambhatla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Representation Learning for Efficient Medical Image Analysis. (arXiv:2006.11223v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11223</id>
        <link href="http://arxiv.org/abs/2006.11223"/>
        <updated>2021-06-09T02:01:50.644Z</updated>
        <summary type="html"><![CDATA[Medical image analysis typically includes several tasks such as enhancement,
segmentation, and classification. Traditionally, these tasks are implemented
using separate deep learning models for separate tasks, which is not efficient
because it involves unnecessary training repetitions, demands greater
computational resources, and requires a relatively large amount of labeled
data. In this paper, we propose a multi-task training approach for medical
image analysis, where individual tasks are fine-tuned simultaneously through
relevant knowledge transfer using a unified modality-specific feature
representation (UMS-Rep). We explore different fine-tuning strategies to
demonstrate the impact of the strategy on the performance of target medical
image tasks. We experiment with different visual tasks (e.g., image denoising,
segmentation, and classification) to highlight the advantages offered with our
approach for two imaging modalities, chest X-ray and Doppler echocardiography.
Our results demonstrate that the proposed approach reduces the overall demand
for computational resources and improves target task generalization and
performance. Further, our results prove that the performance of target tasks in
medical images is highly influenced by the utilized fine-tuning strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1"&gt;Sivaramakrishnan Rajaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1"&gt;Sameer Antani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future. (arXiv:2106.04420v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04420</id>
        <link href="http://arxiv.org/abs/2106.04420"/>
        <updated>2021-06-09T02:01:50.639Z</updated>
        <summary type="html"><![CDATA[In real-time forecasting in public health, data collection is a non-trivial
and demanding task. Often after initially released, it undergoes several
revisions later (maybe due to human or technical constraints) - as a result, it
may take weeks until the data reaches to a stable value. This so-called
'backfill' phenomenon and its effect on model performance has been barely
studied in the prior literature. In this paper, we introduce the multi-variate
backfill problem using COVID-19 as the motivating example. We construct a
detailed dataset composed of relevant signals over the past year of the
pandemic. We then systematically characterize several patterns in backfill
dynamics and leverage our observations for formulating a novel problem and
neural framework Back2Future that aims to refines a given model's predictions
in real-time. Our extensive experiments demonstrate that our method refines the
performance of top models for COVID-19 forecasting, in contrast to non-trivial
baselines, yielding 18% improvement over baselines, enabling us obtain a new
SOTA performance. In addition, we show that our model improves model evaluation
too; hence policy-makers can better understand the true accuracy of forecasting
models in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1"&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1"&gt;B. Aditya Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Fairness of Causal Algorithmic Recourse. (arXiv:2010.06529v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06529</id>
        <link href="http://arxiv.org/abs/2010.06529"/>
        <updated>2021-06-09T02:01:50.623Z</updated>
        <summary type="html"><![CDATA[Algorithmic fairness is typically studied from the perspective of
predictions. Instead, here we investigate fairness from the perspective of
recourse actions suggested to individuals to remedy an unfavourable
classification. We propose two new fairness criteria at the group and
individual level, which -- unlike prior work on equalising the average
group-wise distance from the decision boundary -- explicitly account for causal
relationships between features, thereby capturing downstream effects of
recourse actions performed in the physical world. We explore how our criteria
relate to others, such as counterfactual fairness, and show that fairness of
recourse is complementary to fairness of prediction. We study theoretically and
empirically how to enforce fair causal recourse by altering the classifier and
perform a case study on the Adult dataset. Finally, we discuss whether fairness
violations in the data generating process revealed by our criteria may be
better addressed by societal interventions as opposed to constraints on the
classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1"&gt;Amir-Hossein Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1"&gt;Umang Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1"&gt;Isabel Valera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accommodating Picky Customers: Regret Bound and Exploration Complexity for Multi-Objective Reinforcement Learning. (arXiv:2011.13034v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13034</id>
        <link href="http://arxiv.org/abs/2011.13034"/>
        <updated>2021-06-09T02:01:50.617Z</updated>
        <summary type="html"><![CDATA[In this paper we consider multi-objective reinforcement learning where the
objectives are balanced using preferences. In practice, the preferences are
often given in an adversarial manner, e.g., customers can be picky in many
applications. We formalize this problem as an episodic learning problem on a
Markov decision process, where transitions are unknown and a reward function is
the inner product of a preference vector with pre-specified multi-objective
reward functions. We consider two settings. In the online setting, the agent
receives a (adversarial) preference every episode and proposes policies to
interact with the environment. We provide a model-based algorithm that achieves
a nearly minimax optimal regret bound
$\widetilde{\mathcal{O}}\bigl(\sqrt{\min\{d,S\}\cdot H^2 SAK}\bigr)$, where $d$
is the number of objectives, $S$ is the number of states, $A$ is the number of
actions, $H$ is the length of the horizon, and $K$ is the number of episodes.
Furthermore, we consider preference-free exploration, i.e., the agent first
interacts with the environment without specifying any preference and then is
able to accommodate arbitrary preference vector up to $\epsilon$ error. Our
proposed algorithm is provably efficient with a nearly optimal trajectory
complexity $\widetilde{\mathcal{O}}\bigl({\min\{d,S\}\cdot H^3
SA}/{\epsilon^2}\bigr)$. This result partly resolves an open problem raised by
\citet{jin2020reward}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingfeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1"&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin F. Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-09T02:01:50.552Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Feature Distillation for Visual Recognition. (arXiv:2106.04411v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04411</id>
        <link href="http://arxiv.org/abs/2106.04411"/>
        <updated>2021-06-09T02:01:50.546Z</updated>
        <summary type="html"><![CDATA[Fairness is becoming an increasingly crucial issue for computer vision,
especially in the human-related decision systems. However, achieving
algorithmic fairness, which makes a model produce indiscriminative outcomes
against protected groups, is still an unresolved problem. In this paper, we
devise a systematic approach which reduces algorithmic biases via feature
distillation for visual recognition tasks, dubbed as MMD-based Fair
Distillation (MFD). While the distillation technique has been widely used in
general to improve the prediction accuracy, to the best of our knowledge, there
has been no explicit work that also tries to improve fairness via distillation.
Furthermore, We give a theoretical justification of our MFD on the effect of
knowledge distillation and fairness. Throughout the extensive experiments, we
show our MFD significantly mitigates the bias against specific minorities
without any loss of the accuracy on both synthetic and real-world face
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Sangwon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donggyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taeeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. (arXiv:2102.12855v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12855</id>
        <link href="http://arxiv.org/abs/2102.12855"/>
        <updated>2021-06-09T02:01:50.431Z</updated>
        <summary type="html"><![CDATA[This paper investigates the motion planning of autonomous dynamical systems
modeled by Markov decision processes (MDP) with unknown transition
probabilities over continuous state and action spaces. Linear temporal logic
(LTL) is used to specify high-level tasks over infinite horizon, which can be
converted into a limit deterministic generalized B\"uchi automaton (LDGBA) with
several accepting sets. The novelty is to design an embedded product MDP
(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous
tracking-frontier function to record unvisited accepting sets of the automaton,
and to facilitate the satisfaction of the accepting conditions. The proposed
LDGBA-based reward shaping and discounting schemes for the model-free
reinforcement learning (RL) only depend on the EP-MDP states and can overcome
the issues of sparse rewards. Rigorous analysis shows that any RL method that
optimizes the expected discounted return is guaranteed to find an optimal
policy whose traces maximize the satisfaction probability. A modular deep
deterministic policy gradient (DDPG) is then developed to generate such
policies over continuous state and action spaces. The performance of our
framework is evaluated via an array of OpenAI gym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1"&gt;Mingyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanbeig_M/0/1/0/all/0/1"&gt;Mohammadhosein Hasanbeig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1"&gt;Shaoping Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1"&gt;Zhen Kan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01391</id>
        <link href="http://arxiv.org/abs/2103.01391"/>
        <updated>2021-06-09T02:01:50.403Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the dynamics of temporal difference learning with
neural network-based value function approximation over a general state space,
namely, \emph{Neural TD learning}. We consider two practically used algorithms,
projection-free and max-norm regularized Neural TD learning, and establish the
first convergence bounds for these algorithms. An interesting observation from
our results is that max-norm regularization can dramatically improve the
performance of TD learning algorithms, both in terms of sample complexity and
overparameterization. In particular, we prove that max-norm regularization
appears to be more effective than $\ell_2$-regularization, again both in terms
of sample complexity and overparameterization. The results in this work rely on
a novel Lyapunov drift analysis of the network parameters as a stopped and
controlled random process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1"&gt;Siddhartha Satpathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Concise yet Effective model for Non-Aligned Incomplete Multi-view and Missing Multi-label Learning. (arXiv:2005.00976v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00976</id>
        <link href="http://arxiv.org/abs/2005.00976"/>
        <updated>2021-06-09T02:01:50.377Z</updated>
        <summary type="html"><![CDATA[In reality, learning from multi-view multi-label data inevitably confronts
three challenges: missing labels, incomplete views, and non-aligned views.
Existing methods mainly concern the first two and commonly need multiple
assumptions to attack them, making even state-of-the-arts involve at least two
explicit hyper-parameters such that model selection is quite difficult. More
roughly, they will fail in handling the third challenge, let alone addressing
the three jointly. In this paper, we aim at meeting these under the least
assumption by building a concise yet effective model with just one
hyper-parameter. To ease insufficiency of available labels, we exploit not only
the consensus of multiple views but also the global and local structures hidden
among multiple labels. Specifically, we introduce an indicator matrix to tackle
the first two challenges in a regression form while aligning the same
individual labels and all labels of different views in a common label space to
battle the third challenge. In aligning, we characterize the global and local
structures of multiple labels to be high-rank and low-rank, respectively.
Subsequently, an efficient algorithm with linear time complexity in the number
of samples is established. Finally, even without view-alignment, our method
substantially outperforms state-of-the-arts with view-alignment on five real
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Songcan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning. (arXiv:1912.02631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02631</id>
        <link href="http://arxiv.org/abs/1912.02631"/>
        <updated>2021-06-09T02:01:50.352Z</updated>
        <summary type="html"><![CDATA[Machine learning has started to be deployed in fields such as healthcare and
finance, which propelled the need for and growth of privacy-preserving machine
learning (PPML). We propose an actively secure four-party protocol (4PC), and a
framework for PPML, showcasing its applications on four of the most
widely-known machine learning algorithms -- Linear Regression, Logistic
Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC
protocol tolerating at most one malicious corruption is practically efficient
as compared to the existing works. We use the protocol to build an efficient
mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and
Garbled worlds. Our framework operates in the offline-online paradigm over
rings and is instantiated in an outsourced setting for machine learning. Also,
we propose conversions especially relevant to privacy-preserving machine
learning. The highlights of our framework include using a minimal number of
expensive circuits overall as compared to ABY3. This can be seen in our
technique for truncation, which does not affect the online cost of
multiplication and removes the need for any circuits in the offline phase. Our
B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and
$\mathbf{18} \times$ in the communication complexity. The practicality of our
framework is argued through improvements in the benchmarking of the
aforementioned algorithms when compared with ABY3. All the protocols are
implemented over a 64-bit ring in both LAN and WAN settings. Our improvements
go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$
for the prediction phase when observed over LAN and WAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_H/0/1/0/all/0/1"&gt;Harsh Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rachuri_R/0/1/0/all/0/1"&gt;Rahul Rachuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ajith Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference for Network Regression Models with Community Structure. (arXiv:2106.04271v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.04271</id>
        <link href="http://arxiv.org/abs/2106.04271"/>
        <updated>2021-06-09T02:01:50.340Z</updated>
        <summary type="html"><![CDATA[Network regression models, where the outcome comprises the valued edge in a
network and the predictors are actor or dyad-level covariates, are used
extensively in the social and biological sciences. Valid inference relies on
accurately modeling the residual dependencies among the relations. Frequently
homogeneity assumptions are placed on the errors which are commonly incorrect
and ignore critical, natural clustering of the actors. In this work, we present
a novel regression modeling framework that models the errors as resulting from
a community-based dependence structure and exploits the subsequent
exchangeability properties of the error distribution to obtain parsimonious
standard errors for regression parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pan_M/0/1/0/all/0/1"&gt;Mengjie Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1"&gt;Tyler H. McCormick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fosdick_B/0/1/0/all/0/1"&gt;Bailey K. Fosdick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Uncanny Similarity of Recurrence and Depth. (arXiv:2102.11011v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11011</id>
        <link href="http://arxiv.org/abs/2102.11011"/>
        <updated>2021-06-09T02:01:50.335Z</updated>
        <summary type="html"><![CDATA[It is widely believed that deep neural networks contain layer specialization,
wherein networks extract hierarchical features representing edges and patterns
in shallow layers and complete objects in deeper layers. Unlike common
feed-forward models that have distinct filters at each layer, recurrent
networks reuse the same parameters at various depths. In this work, we observe
that recurrent models exhibit the same hierarchical behaviors and the same
performance benefits as depth despite reusing the same filters at every
recurrence. By training models of various feed-forward and recurrent
architectures on several datasets for image classification as well as maze
solving, we show that recurrent networks have the ability to closely emulate
the behavior of non-recurrent deep models, often doing so with far fewer
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arjun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_A/0/1/0/all/0/1"&gt;Amin Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obtaining Better Static Word Embeddings Using Contextual Embedding Models. (arXiv:2106.04302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04302</id>
        <link href="http://arxiv.org/abs/2106.04302"/>
        <updated>2021-06-09T02:01:50.334Z</updated>
        <summary type="html"><![CDATA[The advent of contextual word embeddings -- representations of words which
incorporate semantic and syntactic information from their context -- has led to
tremendous improvements on a wide variety of NLP tasks. However, recent
contextual models have prohibitively high computational cost in many use-cases
and are often hard to interpret. In this work, we demonstrate that our proposed
distillation method, which is a simple extension of CBOW-based training, allows
to significantly improve computational efficiency of NLP applications, while
outperforming the quality of existing static embeddings trained from scratch as
well as those distilled from previously proposed methods. As a side-effect, our
approach also allows a fair comparison of both contextual and static embeddings
via standard lexical evaluation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prakhar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Possibility in Algorithmic Fairness: Can Calibration and Equal Error Rates Be Reconciled?. (arXiv:2002.07676v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07676</id>
        <link href="http://arxiv.org/abs/2002.07676"/>
        <updated>2021-06-09T02:01:50.328Z</updated>
        <summary type="html"><![CDATA[Decision makers increasingly rely on algorithmic risk scores to determine
access to binary treatments including bail, loans, and medical interventions.
In these settings, we reconcile two fairness criteria that were previously
shown to be in conflict: calibration and error rate equality. In particular, we
derive necessary and sufficient conditions for the existence of calibrated
scores that yield classifications achieving equal error rates at any given
group-blind threshold. We then present an algorithm that searches for the most
accurate score subject to both calibration and minimal error rate disparity.
Applied to the COMPAS criminal risk assessment tool, we show that our method
can eliminate error disparities while maintaining calibration. In a separate
application to credit lending, we compare our procedure to the omission of
sensitive features and show that it raises both profit and the probability that
creditworthy individuals receive loans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Claire Lazar Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijaykumar_S/0/1/0/all/0/1"&gt;Suhas Vijaykumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When in Doubt: Neural Non-Parametric Uncertainty Quantification for Epidemic Forecasting. (arXiv:2106.03904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03904</id>
        <link href="http://arxiv.org/abs/2106.03904"/>
        <updated>2021-06-09T02:01:50.304Z</updated>
        <summary type="html"><![CDATA[Accurate and trustworthy epidemic forecasting is an important problem that
has impact on public health planning and disease mitigation. Most existing
epidemic forecasting models disregard uncertainty quantification, resulting in
mis-calibrated predictions. Recent works in deep neural models for
uncertainty-aware time-series forecasting also have several limitations; e.g.
it is difficult to specify meaningful priors in Bayesian NNs, while methods
like deep ensembling are computationally expensive in practice. In this paper,
we fill this important gap. We model the forecasting task as a probabilistic
generative process and propose a functional neural process model called EPIFNP,
which directly models the probability density of the forecast value. EPIFNP
leverages a dynamic stochastic correlation graph to model the correlations
between sequences in a non-parametric way, and designs different stochastic
latent variables to capture functional uncertainty from different perspectives.
Our extensive experiments in a real-time flu forecasting setting show that
EPIFNP significantly outperforms previous state-of-the-art models in both
accuracy and calibration metrics, up to 2.5x in accuracy and 2.4x in
calibration. Additionally, due to properties of its generative process,EPIFNP
learns the relations between the current season and similar patterns of
historical seasons,enabling interpretable forecasts. Beyond epidemic
forecasting, the EPIFNP can be of independent interest for advancing principled
uncertainty quantification in deep sequential models for predictive analytics]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1"&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingkai Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1"&gt;B. Aditya Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust R-Peak Detection in Low-Quality Holter ECGs using 1D Convolutional Neural Network. (arXiv:2101.01666v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01666</id>
        <link href="http://arxiv.org/abs/2101.01666"/>
        <updated>2021-06-09T02:01:50.295Z</updated>
        <summary type="html"><![CDATA[Noise and low quality of ECG signals acquired from Holter or wearable devices
deteriorate the accuracy and robustness of R-peak detection algorithms. This
paper presents a generic and robust system for R-peak detection in Holter ECG
signals. While many proposed algorithms have successfully addressed the problem
of ECG R-peak detection, there is still a notable gap in the performance of
these detectors on such low-quality ECG records. Therefore, in this study, a
novel implementation of the 1D Convolutional Neural Network (CNN) is used
integrated with a verification model to reduce the number of false alarms. This
CNN architecture consists of an encoder block and a corresponding decoder block
followed by a sample-wise classification layer to construct the 1D segmentation
map of R- peaks from the input ECG signal. Once the proposed model has been
trained, it can solely be used to detect R-peaks possibly in a single channel
ECG data stream quickly and accurately, or alternatively, such a solution can
be conveniently employed for real-time monitoring on a lightweight portable
device. The model is tested on two open-access ECG databases: The China
Physiological Signal Challenge (2020) database (CPSC-DB) with more than one
million beats, and the commonly used MIT-BIH Arrhythmia Database (MIT-DB).
Experimental results demonstrate that the proposed systematic approach achieves
99.30% F1-score, 99.69% recall, and 98.91% precision in CPSC-DB, which is the
best R-peak detection performance ever achieved. Compared to all competing
methods, the proposed approach can reduce the false-positives and
false-negatives in Holter ECG signals by more than 54% and 82%, respectively.
Results also demonstrate similar or better performance than most competing
algorithms on MIT-DB with 99.83% F1-score, 99.85% recall, and 99.82% precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zahid_M/0/1/0/all/0/1"&gt;Muhammad Uzair Zahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ince_T/0/1/0/all/0/1"&gt;Turker Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Devecioglu_O/0/1/0/all/0/1"&gt;Ozer Can Devecioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tahir_A/0/1/0/all/0/1"&gt;Anas Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data. (arXiv:2101.03735v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03735</id>
        <link href="http://arxiv.org/abs/2101.03735"/>
        <updated>2021-06-09T02:01:50.295Z</updated>
        <summary type="html"><![CDATA[In the biopharmaceutical manufacturing, fermentation process plays a critical
role impacting on productivity and profit. Since biotherapeutics are
manufactured in living cells whose biological mechanisms are complex and have
highly variable outputs, in this paper, we introduce a model-based
reinforcement learning framework accounting for model risk to support
bioprocess online learning and guide the optimal reliable customized stopping
policy for fermentation process. Specifically, built on the dynamic mechanisms
of protein and impurity generation, we first construct a probabilistic model
characterizing the impact of underlying bioprocess stochastic uncertainty on
impurity and protein growth rates. Since biopharmaceutical manufacturing often
has very limited batch data during the development and early stage of
production, we derive the posterior distribution quantifying the process model
risk, and further develop the Bayesian rule based knowledge update to support
bioprocess online learning. With the prediction risk accounting for both
bioprocess stochastic uncertainty and model risk, the proposed reinforcement
learning framework can provide the optimal and reliable decision making. We
conduct the structural analysis of optimal policy and study the impact of model
risk on the policy selection. We can show that it asymptotically converges to
the optimal policy obtained under perfect information of underlying stochastic
process. Our case studies demonstrate that the proposed framework can greatly
improve the biomanufacturing industrial practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martagan_T/0/1/0/all/0/1"&gt;Tugce Martagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akcay_A/0/1/0/all/0/1"&gt;Alp Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ravenstein_B/0/1/0/all/0/1"&gt;Bram van Ravenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks. (arXiv:2106.04292v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04292</id>
        <link href="http://arxiv.org/abs/2106.04292"/>
        <updated>2021-06-09T02:01:50.291Z</updated>
        <summary type="html"><![CDATA[Hypergraph offers a framework to depict the multilateral relationships in
real-world complex data. Predicting higher-order relationships, i.e hyperedge,
becomes a fundamental problem for the full understanding of complicated
interactions. The development of graph neural network (GNN) has greatly
advanced the analysis of ordinary graphs with pair-wise relations. However,
these methods could not be easily extended to the case of hypergraph. In this
paper, we generalize the challenges of GNN in representing higher-order data in
principle, which are edge- and node-level ambiguities. To overcome the
challenges, we present \textbf{SNALS} that utilizes bipartite graph neural
network with structural features to collectively tackle the two ambiguity
issues. SNALS captures the joint interactions of a hyperedge by its local
environment, which is retrieved by collecting the spectrum information of their
connections. As a result, SNALS achieves nearly 30% performance increase
compared with most recent GNN-based models. In addition, we applied SNALS to
predict genetic higher-order interactions on 3D genome organization data. SNALS
showed consistently high prediction accuracy across different chromosomes, and
generated novel findings on 4-way gene interaction, which is further validated
by existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Changlin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Muhan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Wei Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Sha Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the stability properties of Gated Recurrent Units neural networks. (arXiv:2011.06806v4 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06806</id>
        <link href="http://arxiv.org/abs/2011.06806"/>
        <updated>2021-06-09T02:01:50.291Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is to provide sufficient conditions for guaranteeing
the Input-to-State Stability (ISS) and the Incremental Input-to-State Stability
({\delta}ISS) of Gated Recurrent Units (GRUs) neural networks. These
conditions, devised for both single-layer and multi-layer architectures,
consist of nonlinear inequalities on network's weights. They can be employed to
check the stability of trained networks, or can be enforced as constraints
during the training procedure of a GRU. The resulting training procedure is
tested on a Quadruple Tank nonlinear benchmark system, showing satisfactory
modeling performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Farina_M/0/1/0/all/0/1"&gt;Marcello Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cooperative Stochastic Multi-agent Multi-armed Bandits Robust to Adversarial Corruptions. (arXiv:2106.04207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04207</id>
        <link href="http://arxiv.org/abs/2106.04207"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[We study the problem of stochastic bandits with adversarial corruptions in
the cooperative multi-agent setting, where $V$ agents interact with a common
$K$-armed bandit problem, and each pair of agents can communicate with each
other to expedite the learning process. In the problem, the rewards are
independently sampled from distributions across all agents and rounds, but they
may be corrupted by an adversary. Our goal is to minimize both the overall
regret and communication cost across all agents. We first show that an additive
term of corruption is unavoidable for any algorithm in this problem. Then, we
propose a new algorithm that is agnostic to the level of corruption. Our
algorithm not only achieves near-optimal regret in the stochastic setting, but
also obtains a regret with an additive term of corruption in the corrupted
setting, while maintaining efficient communication. The algorithm is also
applicable for the single-agent corruption problem, and achieves a high
probability regret that removes the multiplicative dependence of $K$ on
corruption level. Our result of the single-agent case resolves an open question
from Gupta et al. [2019].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dapeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Generation for Adaptive Education. (arXiv:2106.04262v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04262</id>
        <link href="http://arxiv.org/abs/2106.04262"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[Intelligent and adaptive online education systems aim to make high-quality
education available for a diverse range of students. However, existing systems
usually depend on a pool of hand-made questions, limiting how fine-grained and
open-ended they can be in adapting to individual students. We explore targeted
question generation as a controllable sequence generation task. We first show
how to fine-tune pre-trained language models for deep knowledge tracing
(LM-KT). This model accurately predicts the probability of a student answering
a question correctly, and generalizes to questions not seen in training. We
then use LM-KT to specify the objective and data for training a model to
generate questions conditioned on the student and target difficulty. Our
results show we succeed at generating novel, well-calibrated language
translation questions for second language learners from a real online education
platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1"&gt;Megha Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-Aware Query Auto-completion using Extreme Multi-label Ranking. (arXiv:2012.07654v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07654</id>
        <link href="http://arxiv.org/abs/2012.07654"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[Query auto-completion (QAC) is a fundamental feature in search engines where
the task is to suggest plausible completions of a prefix typed in the search
bar. Previous queries in the user session can provide useful context for the
user's intent and can be leveraged to suggest auto-completions that are more
relevant while adhering to the user's prefix. Such session-aware QACs can be
generated by recent sequence-to-sequence deep learning models; however, these
generative approaches often do not meet the stringent latency requirements of
responding to each user keystroke. Moreover, these generative approaches pose
the risk of showing nonsensical queries.

In this paper, we provide a solution to this problem: we take the novel
approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)
problem where the input is the previous query in the session and the user's
current prefix, while the output space is the set of tens of millions of
queries entered by users in the recent past. We adapt a popular XMR algorithm
for this purpose by proposing several modifications to the key steps in the
algorithm. The proposed modifications yield a 10x improvement in terms of Mean
Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs
dataset. We are able to maintain an inference latency of less than 10 ms while
still using session context. When compared against baseline models of
acceptable latency, we observed a 33% improvement in MRR for short prefixes of
up to 3 characters. Moreover, our model yielded a statistically significant
improvement of 2.81% over a production QAC system in terms of suggestion
acceptance rate, when deployed on the search bar of an online shopping store as
part of an A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nishant Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Daniel N. Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deciding What to Learn: A Rate-Distortion Approach. (arXiv:2101.06197v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06197</id>
        <link href="http://arxiv.org/abs/2101.06197"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[Agents that learn to select optimal actions represent a prominent focus of
the sequential decision-making literature. In the face of a complex environment
or constraints on time and resources, however, aiming to synthesize such an
optimal policy can become infeasible. These scenarios give rise to an important
trade-off between the information an agent must acquire to learn and the
sub-optimality of the resulting policy. While an agent designer has a
preference for how this trade-off is resolved, existing approaches further
require that the designer translate these preferences into a fixed learning
target for the agent. In this work, leveraging rate-distortion theory, we
automate this process such that the designer need only express their
preferences via a single hyperparameter and the agent is endowed with the
ability to compute its own learning targets that best achieve the desired
trade-off. We establish a general bound on expected discounted regret for an
agent that decides what to learn in this manner along with computational
experiments that illustrate the expressiveness of designer preferences and even
show improvements over Thompson sampling in identifying an optimal policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1"&gt;Dilip Arumugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Privacy-Preserving Text Classification based on Secure Multiparty Computation. (arXiv:2101.07365v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07365</id>
        <link href="http://arxiv.org/abs/2101.07365"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[We propose a privacy-preserving Naive Bayes classifier and apply it to the
problem of private text classification. In this setting, a party (Alice) holds
a text message, while another party (Bob) holds a classifier. At the end of the
protocol, Alice will only learn the result of the classifier applied to her
text input and Bob learns nothing. Our solution is based on Secure Multiparty
Computation (SMC). Our Rust implementation provides a fast and secure solution
for the classification of unstructured text. Applying our solution to the case
of spam detection (the solution is generic, and can be used in any other
scenario in which the Naive Bayes classifier can be employed), we can classify
an SMS as spam or ham in less than 340ms in the case where the dictionary size
of Bob's model includes all words (n = 5200) and Alice's SMS has at most m =
160 unigrams. In the case with n = 369 and m = 8 (the average of a spam SMS in
the database), our solution takes only 21ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Resende_A/0/1/0/all/0/1"&gt;Amanda Resende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Railsback_D/0/1/0/all/0/1"&gt;Davis Railsback&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1"&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1"&gt;Anderson C. A. Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aranha_D/0/1/0/all/0/1"&gt;Diego F. Aranha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustNav: Towards Benchmarking Robustness in Embodied Navigation. (arXiv:2106.04531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04531</id>
        <link href="http://arxiv.org/abs/2106.04531"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[As an attempt towards assessing the robustness of embodied navigation agents,
we propose RobustNav, a framework to quantify the performance of embodied
navigation agents when exposed to a wide variety of visual - affecting RGB
inputs - and dynamics - affecting transition dynamics - corruptions. Most
recent efforts in visual navigation have typically focused on generalizing to
novel target environments with similar appearance and dynamics characteristics.
With RobustNav, we find that some standard embodied navigation agents
significantly underperform (or fail) in the presence of visual or dynamics
corruptions. We systematically analyze the kind of idiosyncrasies that emerge
in the behavior of such agents when operating under corruptions. Finally, for
visual corruptions in RobustNav, we show that while standard techniques to
improve robustness such as data-augmentation and self-supervised adaptation
offer some zero-shot resistance and improvements in navigation performance,
there is still a long way to go in terms of recovering lost performance
relative to clean "non-corrupt" settings, warranting more research in this
direction. Our code is available at https://github.com/allenai/robustnav]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1"&gt;Prithvijit Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;Judy Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1"&gt;Aniruddha Kembhavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03932</id>
        <link href="http://arxiv.org/abs/2106.03932"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1"&gt;Okan K&amp;#xf6;p&amp;#xfc;kl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1"&gt;Maja Taseska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NWT: Towards natural audio-to-video generation with representation learning. (arXiv:2106.04283v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04283</id>
        <link href="http://arxiv.org/abs/2106.04283"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[In this work we introduce NWT, an expressive speech-to-video model. Unlike
approaches that use domain-specific intermediate representations such as pose
keypoints, NWT learns its own latent representations, with minimal assumptions
about the audio and video content. To this end, we propose a novel discrete
variational autoencoder with adversarial loss, dVAE-Adv, which learns a new
discrete latent representation we call Memcodes. Memcodes are straightforward
to implement, require no additional loss terms, are stable to train compared
with other approaches, and show evidence of interpretability. To predict on the
Memcode space, we use an autoregressive encoder-decoder model conditioned on
audio. Additionally, our model can control latent attributes in the generated
video that are not annotated in the data. We train NWT on clips from HBO's Last
Week Tonight with John Oliver. NWT consistently scores above other approaches
in Mean Opinion Score (MOS) on tests of overall video naturalness, facial
naturalness and expressiveness, and lipsync quality. This work sets a strong
baseline for generalized audio-to-video synthesis. Samples are available at
https://next-week-tonight.github.io/NWT/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mama_R/0/1/0/all/0/1"&gt;Rayhane Mama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyndel_M/0/1/0/all/0/1"&gt;Marc S. Tyndel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadhim_H/0/1/0/all/0/1"&gt;Hashiam Kadhim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_C/0/1/0/all/0/1"&gt;Cole Clifford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thurairatnam_R/0/1/0/all/0/1"&gt;Ragavan Thurairatnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Fairness in Classification with a Model-Agnostic Multi-Objective Algorithm. (arXiv:2009.04441v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04441</id>
        <link href="http://arxiv.org/abs/2009.04441"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[The goal of fairness in classification is to learn a classifier that does not
discriminate against groups of individuals based on sensitive attributes, such
as race and gender. One approach to designing fair algorithms is to use
relaxations of fairness notions as regularization terms or in a constrained
optimization problem. We observe that the hyperbolic tangent function can
approximate the indicator function. We leverage this property to define a
differentiable relaxation that approximates fairness notions provably better
than existing relaxations. In addition, we propose a model-agnostic
multi-objective architecture that can simultaneously optimize for multiple
fairness notions and multiple sensitive attributes and supports all statistical
parity-based notions of fairness. We use our relaxation with the
multi-objective architecture to learn fair classifiers. Experiments on public
datasets show that our method suffers a significantly lower loss of accuracy
than current debiasing algorithms relative to the unconstrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padh_K/0/1/0/all/0/1"&gt;Kirtan Padh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. (arXiv:2009.07526v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07526</id>
        <link href="http://arxiv.org/abs/2009.07526"/>
        <updated>2021-06-09T02:01:50.288Z</updated>
        <summary type="html"><![CDATA[Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stochastic Subgradient Method for Distributionally Robust Non-Convex Learning. (arXiv:2006.04873v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04873</id>
        <link href="http://arxiv.org/abs/2006.04873"/>
        <updated>2021-06-09T02:01:50.280Z</updated>
        <summary type="html"><![CDATA[We consider a distributionally robust formulation of stochastic optimization
problems arising in statistical learning, where robustness is with respect to
uncertainty in the underlying data distribution. Our formulation builds on
risk-averse optimization techniques and the theory of coherent risk measures.
It uses semi-deviation risk for quantifying uncertainty, allowing us to compute
solutions that are robust against perturbations in the population data
distribution. We consider a large family of loss functions that can be
non-convex and non-smooth and develop an efficient stochastic subgradient
method. We prove that it converges to a point satisfying the optimality
conditions. To our knowledge, this is the first method with rigorous
convergence guarantees in the context of non-convex non-smooth distributionally
robust stochastic optimization. Our method can achieve any desired level of
robustness with little extra computational cost compared to population risk
minimization. We also illustrate the performance of our algorithm on real
datasets arising in convex and non-convex supervised learning problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert G&amp;#xfc;rb&amp;#xfc;zbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ruszczynski_A/0/1/0/all/0/1"&gt;Andrzej Ruszczy&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Landi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Multiple Noisy Partial Labelers. (arXiv:2106.04530v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04530</id>
        <link href="http://arxiv.org/abs/2106.04530"/>
        <updated>2021-06-09T02:01:50.279Z</updated>
        <summary type="html"><![CDATA[Programmatic weak supervision creates models without hand-labeled training
data by combining the outputs of noisy, user-written rules and other heuristic
labelers. Existing frameworks make the restrictive assumption that labelers
output a single class label. Enabling users to create partial labelers that
output subsets of possible class labels would greatly expand the expressivity
of programmatic weak supervision. We introduce this capability by defining a
probabilistic generative model that can estimate the underlying accuracies of
multiple noisy partial labelers without ground truth labels. We prove that this
class of models is generically identifiable up to label swapping under mild
conditions. We also show how to scale up learning to 100k examples in one
minute, a 300X speed up compared to a naive implementation. We evaluate our
framework on three text classification and six object classification tasks. On
text tasks, adding partial labels increases average accuracy by 9.6 percentage
points. On image tasks, we show that partial labels allow us to approach some
zero-shot object classification problems with programmatic weak supervision by
using class attributes as partial labelers. Our framework is able to achieve
accuracy comparable to recent embedding-based zero-shot learning methods using
only pre-trained attribute detectors]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Peilin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1"&gt;Tiffany Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1"&gt;Stephen H. Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Representation Learning for Efficient Medical Image Analysis. (arXiv:2006.11223v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11223</id>
        <link href="http://arxiv.org/abs/2006.11223"/>
        <updated>2021-06-09T02:01:50.278Z</updated>
        <summary type="html"><![CDATA[Medical image analysis typically includes several tasks such as enhancement,
segmentation, and classification. Traditionally, these tasks are implemented
using separate deep learning models for separate tasks, which is not efficient
because it involves unnecessary training repetitions, demands greater
computational resources, and requires a relatively large amount of labeled
data. In this paper, we propose a multi-task training approach for medical
image analysis, where individual tasks are fine-tuned simultaneously through
relevant knowledge transfer using a unified modality-specific feature
representation (UMS-Rep). We explore different fine-tuning strategies to
demonstrate the impact of the strategy on the performance of target medical
image tasks. We experiment with different visual tasks (e.g., image denoising,
segmentation, and classification) to highlight the advantages offered with our
approach for two imaging modalities, chest X-ray and Doppler echocardiography.
Our results demonstrate that the proposed approach reduces the overall demand
for computational resources and improves target task generalization and
performance. Further, our results prove that the performance of target tasks in
medical images is highly influenced by the utilized fine-tuning strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1"&gt;Sivaramakrishnan Rajaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1"&gt;Sameer Antani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Graph Transformers with Spectral Attention. (arXiv:2106.03893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03893</id>
        <link href="http://arxiv.org/abs/2106.03893"/>
        <updated>2021-06-09T02:01:50.277Z</updated>
        <summary type="html"><![CDATA[In recent years, the Transformer architecture has proven to be very
successful in sequence processing, but its application to other data
structures, such as graphs, has remained limited due to the difficulty of
properly defining positions. Here, we present the $\textit{Spectral Attention
Network}$ (SAN), which uses a learned positional encoding (LPE) that can take
advantage of the full Laplacian spectrum to learn the position of each node in
a given graph. This LPE is then added to the node features of the graph and
passed to a fully-connected Transformer. By leveraging the full spectrum of the
Laplacian, our model is theoretically powerful in distinguishing graphs, and
can better detect similar sub-structures from their resonance. Further, by
fully connecting the graph, the Transformer does not suffer from
over-squashing, an information bottleneck of most GNNs, and enables better
modeling of physical phenomenons such as heat transfer and electric
interaction. When tested empirically on a set of 4 standard datasets, our model
performs on par or better than state-of-the-art GNNs, and outperforms any
attention-based model by a wide margin, becoming the first fully-connected
architecture to perform well on graph benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kreuzer_D/0/1/0/all/0/1"&gt;Devin Kreuzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beaini_D/0/1/0/all/0/1"&gt;Dominique Beaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Letourneau_V/0/1/0/all/0/1"&gt;Vincent L&amp;#xe9;tourneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tossou_P/0/1/0/all/0/1"&gt;Prudencio Tossou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-09T02:01:50.276Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system under the team name
Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We
also participate in the textual-only subtask of the same language pair for
which we use mBART, a pretrained multilingual sequence-to-sequence model. For
multimodal translation, we propose to enhance the textual input by bringing the
visual information to a textual domain by extracting object tags from the
image. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the multimodal task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning. (arXiv:2106.03921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03921</id>
        <link href="http://arxiv.org/abs/2106.03921"/>
        <updated>2021-06-09T02:01:50.276Z</updated>
        <summary type="html"><![CDATA[Imagine you are in a supermarket. You have two bananas in your basket and
want to buy four apples. How many fruits do you have in total? This seemingly
straightforward question can be challenging for data-driven language models,
even if trained at scale. However, we would expect such generic language models
to possess some mathematical abilities in addition to typical linguistic
competence. Towards this goal, we investigate if a commonly used language
model, BERT, possesses such mathematical abilities and, if so, to what degree.
For that, we fine-tune BERT on a popular dataset for word math problems,
AQuA-RAT, and conduct several tests to understand learned representations
better. Since we teach models trained on natural language to do formal
mathematics, we hypothesize that such models would benefit from training on
semi-formal steps that explain how math results are derived. To better
accommodate such training, we also propose new pretext tasks for learning
mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or
NROP). With this new model, we achieve significantly better outcomes than
data-driven baselines and even on-par with more tailored models. We also show
how to reduce positional bias in such models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piekos_P/0/1/0/all/0/1"&gt;Piotr Pi&amp;#x119;kos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1"&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Momentum in Temporal Difference Learning. (arXiv:2106.03955v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03955</id>
        <link href="http://arxiv.org/abs/2106.03955"/>
        <updated>2021-06-09T02:01:50.275Z</updated>
        <summary type="html"><![CDATA[A common optimization tool used in deep reinforcement learning is momentum,
which consists in accumulating and discounting past gradients, reapplying them
at each iteration. We argue that, unlike in supervised learning, momentum in
Temporal Difference (TD) learning accumulates gradients that become doubly
stale: not only does the gradient of the loss change due to parameter updates,
the loss itself changes due to bootstrapping. We first show that this
phenomenon exists, and then propose a first-order correction term to momentum.
We show that this correction term improves sample efficiency in policy
evaluation by correcting target value drift. An important insight of this work
is that deep RL methods are not always best served by directly importing
techniques from the supervised setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virtual Screening of Pharmaceutical Compounds with hERG Inhibitory Activity (Cardiotoxicity) using Ensemble Learning. (arXiv:2106.04377v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.04377</id>
        <link href="http://arxiv.org/abs/2106.04377"/>
        <updated>2021-06-09T02:01:50.271Z</updated>
        <summary type="html"><![CDATA[In silico prediction of cardiotoxicity with high sensitivity and specificity
for potential drug molecules can be of immense value. Hence, building machine
learning classification models, based on some features extracted from the
molecular structure of drugs, which are capable of efficiently predicting
cardiotoxicity is critical. In this paper, we consider the application of
various machine learning approaches, and then propose an ensemble classifier
for the prediction of molecular activity on a Drug Discovery Hackathon (DDH)
(1st reference) dataset. We have used only 2-D descriptors of SMILE notations
for our prediction. Our ensemble classification uses 5 classifiers (2 Random
Forest Classifiers, 2 Support Vector Machines and a Dense Neural Network) and
uses Max-Voting technique and Weighted-Average technique for final decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Aditya Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bhavsar_A/0/1/0/all/0/1"&gt;Arnav Bhavsar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation. (arXiv:2106.04151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04151</id>
        <link href="http://arxiv.org/abs/2106.04151"/>
        <updated>2021-06-09T02:01:50.266Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned
from a well-labeled source domain to an unlabeled target domain. Recently,
adversarial domain adaptation with two distinct classifiers (bi-classifier) has
been introduced into UDA which is effective to align distributions between
different domains. Previous bi-classifier adversarial learning methods only
focus on the similarity between the outputs of two distinct classifiers.
However, the similarity of the outputs cannot guarantee the accuracy of target
samples, i.e., target samples may match to wrong categories even if the
discrepancy between two classifiers is small. To challenge this issue, in this
paper, we propose a cross-domain gradient discrepancy minimization (CGDM)
method which explicitly minimizes the discrepancy of gradients generated by
source samples and target samples. Specifically, the gradient gives a cue for
the semantic information of target samples so it can be used as a good
supervision to improve the accuracy of target samples. In order to compute the
gradient signal of target samples, we further obtain target pseudo labels
through a clustering-based self-supervised learning. Extensive experiments on
three widely used UDA datasets show that our method surpasses many previous
state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hongzu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear MPC for Offset-Free Tracking of systems learned by GRU Neural Networks. (arXiv:2103.02383v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02383</id>
        <link href="http://arxiv.org/abs/2103.02383"/>
        <updated>2021-06-09T02:01:50.260Z</updated>
        <summary type="html"><![CDATA[The use of Recurrent Neural Networks (RNNs) for system identification has
recently gathered increasing attention, thanks to their black-box modeling
capabilities.Albeit RNNs have been fruitfully adopted in many applications,
only few works are devoted to provide rigorous theoretical foundations that
justify their use for control purposes. The aim of this paper is to describe
how stable Gated Recurrent Units (GRUs), a particular RNN architecture, can be
trained and employed in a Nonlinear MPC framework to perform offset-free
tracking of constant references with guaranteed closed-loop stability. The
proposed approach is tested on a pH neutralization process benchmark, showing
remarkable performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_C/0/1/0/all/0/1"&gt;Caio Fabio Oliveira da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Federated Learning in the Presence of Arbitrary Device Unavailability. (arXiv:2106.04159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04159</id>
        <link href="http://arxiv.org/abs/2106.04159"/>
        <updated>2021-06-09T02:01:50.254Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) coordinates with numerous heterogeneous devices to
collaboratively train a shared model while preserving user privacy. Despite its
multiple advantages, FL faces new challenges. One challenge arises when devices
drop out of the training process beyond the control of the central server. In
this case, the convergence of popular FL algorithms such as FedAvg is severely
influenced by the straggling devices. To tackle this challenge, we study
federated learning algorithms under arbitrary device unavailability and propose
an algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our
algorithm efficiently avoids excessive latency induced by inactive devices, and
corrects the gradient bias using the memorized latest updates from the devices.
We prove that MIFA achieves minimax optimal convergence rates on non-i.i.d.
data for both strongly convex and non-convex smooth functions. We also provide
an explicit characterization of the improvement over baseline algorithms
through a case study, and validate the results by numerical experiments on
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xinran Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-dataset Pretraining: A Unified Model for Semantic Segmentation. (arXiv:2106.04121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04121</id>
        <link href="http://arxiv.org/abs/2106.04121"/>
        <updated>2021-06-09T02:01:50.234Z</updated>
        <summary type="html"><![CDATA[Collecting annotated data for semantic segmentation is time-consuming and
hard to scale up. In this paper, we for the first time propose a unified
framework, termed as Multi-Dataset Pretraining, to take full advantage of the
fragmented annotations of different datasets. The highlight is that the
annotations from different domains can be efficiently reused and consistently
boost performance for each specific domain. This is achieved by first
pretraining the network via the proposed pixel-to-prototype contrastive loss
over multiple datasets regardless of their taxonomy labels, and followed by
fine-tuning the pretrained model over specific dataset as usual. In order to
better model the relationship among images and classes from different datasets,
we extend the pixel level embeddings via cross dataset mixing and propose a
pixel-to-class sparse coding strategy that explicitly models the pixel-class
similarity over the manifold embedding space. In this way, we are able to
increase intra-class compactness and inter-class separability, as well as
considering inter-class similarity across different datasets for better
transferability. Experiments conducted on several benchmarks demonstrate its
superior performance. Notably, MDP consistently outperforms the pretrained
models over ImageNet by a considerable margin, while only using less than 10%
samples for pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bowen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haohang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04140</id>
        <link href="http://arxiv.org/abs/2106.04140"/>
        <updated>2021-06-09T02:01:50.211Z</updated>
        <summary type="html"><![CDATA[Keyword spotting is an important research field because it plays a key role
in device wake-up and user interaction on smart devices. However, it is
challenging to minimize errors while operating efficiently in devices with
limited resources such as mobile phones. We present a broadcasted residual
learning method to achieve high accuracy with small model size and
computational load. Our method configures most of the residual functions as 1D
temporal convolution while still allows 2D convolution together using a
broadcasted-residual connection that expands temporal output to
frequency-temporal dimension. This residual mapping enables the network to
effectively represent useful audio features with much less computation than
conventional convolutional neural networks. We also propose a novel network
architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted
residual learning and describe how to scale up the model according to the
target device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%
top-1 accuracy on Google speech command datasets v1 and v2, respectively, and
consistently outperform previous approaches, using fewer computations and
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byeonggeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Simyung Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1"&gt;Dooyong Sung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Normalization Orthogonalizes Representations in Deep Random Networks. (arXiv:2106.03970v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.03970</id>
        <link href="http://arxiv.org/abs/2106.03970"/>
        <updated>2021-06-09T02:01:50.210Z</updated>
        <summary type="html"><![CDATA[This paper underlines a subtle property of batch-normalization (BN):
Successive batch normalizations with random linear transformations make hidden
representations increasingly orthogonal across layers of a deep neural network.
We establish a non-asymptotic characterization of the interplay between depth,
width, and the orthogonality of deep representations. More precisely, under a
mild assumption, we prove that the deviation of the representations from
orthogonality rapidly decays with depth up to a term inversely proportional to
the network width. This result has two main implications: 1) Theoretically, as
the depth grows, the distribution of the representation -- after the linear
layers -- contracts to a Wasserstein-2 ball around an isotropic Gaussian
distribution. Furthermore, the radius of this Wasserstein ball shrinks with the
width of the network. 2) In practice, the orthogonality of the representations
directly influences the performance of stochastic gradient descent (SGD). When
representations are initially aligned, we observe SGD wastes many iterations to
orthogonalize representations before the classification. Nevertheless, we
experimentally show that starting optimization from orthogonal representations
is sufficient to accelerate SGD, with no need for BN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Daneshmand_H/0/1/0/all/0/1"&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Joudaki_A/0/1/0/all/0/1"&gt;Amir Joudaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04569</id>
        <link href="http://arxiv.org/abs/2106.04569"/>
        <updated>2021-06-09T02:01:50.208Z</updated>
        <summary type="html"><![CDATA[Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this model in a face recognition
scenario. We are the first to show that weaknesses of models trained on real
data can be discovered using simulated samples. Using our proposed method, we
can find adversarial synthetic faces that fool contemporary face recognition
models. This demonstrates the fact that these models have weaknesses that are
not measured by commonly used validation datasets. We hypothesize that this
type of adversarial examples are not isolated, but usually lie in connected
components in the latent space of the simulator. We present a method to find
these adversarial regions as opposed to the typical adversarial points found in
the adversarial example literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1"&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Weichao Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1"&gt;Stan Sclaroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks. (arXiv:2101.06475v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06475</id>
        <link href="http://arxiv.org/abs/2101.06475"/>
        <updated>2021-06-09T02:01:50.208Z</updated>
        <summary type="html"><![CDATA[In contrast to traditional weight optimization in a continuous space, we
demonstrate the existence of effective random networks whose weights are never
updated. By selecting a weight among a fixed set of random values for each
individual connection, our method uncovers combinations of random weights that
match the performance of traditionally-trained networks of the same capacity.
We refer to our networks as "slot machines" where each reel (connection)
contains a fixed set of symbols (random values). Our backpropagation algorithm
"spins" the reels to seek "winning" combinations, i.e., selections of random
weight values that minimize the given loss. Quite surprisingly, we find that
allocating just a few random values to each connection (e.g., 8 values per
connection) yields highly competitive combinations despite being dramatically
more constrained compared to traditionally learned weights. Moreover,
finetuning these combinations often improves performance over the trained
baselines. A randomly initialized VGG-19 with 8 values per connection contains
a combination that achieves 91% test accuracy on CIFAR-10. Our method also
achieves an impressive performance of 98.2% on MNIST for neural networks
containing only random weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aladago_M/0/1/0/all/0/1"&gt;Maxwell Mbabilla Aladago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Distributional Treatment Effect with Kernel Conditional Mean Embeddings and U-Statistic Regression. (arXiv:2102.08208v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08208</id>
        <link href="http://arxiv.org/abs/2102.08208"/>
        <updated>2021-06-09T02:01:50.207Z</updated>
        <summary type="html"><![CDATA[We propose to analyse the conditional distributional treatment effect
(CoDiTE), which, in contrast to the more common conditional average treatment
effect (CATE), is designed to encode a treatment's distributional aspects
beyond the mean. We first introduce a formal definition of the CoDiTE
associated with a distance function between probability measures. Then we
discuss the CoDiTE associated with the maximum mean discrepancy via kernel
conditional mean embeddings, which, coupled with a hypothesis test, tells us
whether there is any conditional distributional effect of the treatment.
Finally, we investigate what kind of conditional distributional effect the
treatment has, both in an exploratory manner via the conditional witness
function, and in a quantitative manner via U-statistic regression, generalising
the CATE to higher-order moments. Experiments on synthetic, semi-synthetic and
real datasets demonstrate the merits of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Park_J/0/1/0/all/0/1"&gt;Junhyung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight. (arXiv:2106.04263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04263</id>
        <link href="http://arxiv.org/abs/2106.04263"/>
        <updated>2021-06-09T02:01:50.205Z</updated>
        <summary type="html"><![CDATA[Vision Transformer (ViT) attains state-of-the-art performance in visual
recognition, and the variant, Local Vision Transformer, makes further
improvements. The major component in Local Vision Transformer, local attention,
performs the attention separately over small local windows. We rephrase local
attention as a channel-wise locally-connected layer and analyze it from two
network regularization manners, sparse connectivity and weight sharing, as well
as weight computation. Sparse connectivity: there is no connection across
channels, and each position is connected to the positions within a small local
window. Weight sharing: the connection weights for one position are shared
across channels or within each group of channels. Dynamic weight: the
connection weights are dynamically predicted according to each image instance.
We point out that local attention resembles depth-wise convolution and its
dynamic version in sparse connectivity. The main difference lies in weight
sharing - depth-wise convolution shares connection weights (kernel weights)
across spatial positions. We empirically observe that the models based on
depth-wise convolution and the dynamic variant with lower computation
complexity perform on-par with or sometimes slightly better than Swin
Transformer, an instance of Local Vision Transformer, for ImageNet
classification, COCO object detection and ADE semantic segmentation. These
observations suggest that Local Vision Transformer takes advantage of two
regularization forms and dynamic weight to increase the network capacity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zejia Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Machine Learning with Plausible Deniability. (arXiv:2106.04267v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04267</id>
        <link href="http://arxiv.org/abs/2106.04267"/>
        <updated>2021-06-09T02:01:50.205Z</updated>
        <summary type="html"><![CDATA[We study the question of how well machine learning (ML) models trained on a
certain data set provide privacy for the training data, or equivalently,
whether it is possible to reverse-engineer the training data from a given ML
model. While this is easy to answer negatively in the most general case, it is
interesting to note that the protection extends over non-recoverability towards
plausible deniability: Given an ML model $f$, we show that one can take a set
of purely random training data, and from this define a suitable ``learning
rule'' that will produce a ML model that is exactly $f$. Thus, any speculation
about which data has been used to train $f$ is deniable upon the claim that any
other data could have led to the same results. We corroborate our theoretical
finding with practical examples, and open source implementations of how to find
the learning rules for a chosen set of raining data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rass_S/0/1/0/all/0/1"&gt;Stefan Rass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konig_S/0/1/0/all/0/1"&gt;Sandra K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachter_J/0/1/0/all/0/1"&gt;Jasmin Wachter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_M/0/1/0/all/0/1"&gt;Manuel Egger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hobisch_M/0/1/0/all/0/1"&gt;Manuel Hobisch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08199</id>
        <link href="http://arxiv.org/abs/2007.08199"/>
        <updated>2021-06-09T02:01:50.204Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved remarkable success in numerous domains with help
from large amounts of big data. However, the quality of data labels is a
concern because of the lack of high-quality labels in many real-world
scenarios. As noisy labels severely degrade the generalization performance of
deep neural networks, learning from noisy labels (robust training) is becoming
an important task in modern deep learning applications. In this survey, we
first describe the problem of learning with label noise from a supervised
learning perspective. Next, we provide a comprehensive review of 57
state-of-the-art robust training methods, all of which are categorized into
five groups according to their methodological difference, followed by a
systematic comparison of six properties used to evaluate their superiority.
Subsequently, we perform an in-depth analysis of noise rate estimation and
summarize the typically used evaluation methodology, including public noisy
datasets and evaluation metrics. Finally, we present several promising research
directions that can serve as a guideline for future studies. All the contents
will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NISQ Algorithm for Semidefinite Programming. (arXiv:2106.03891v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.03891</id>
        <link href="http://arxiv.org/abs/2106.03891"/>
        <updated>2021-06-09T02:01:50.203Z</updated>
        <summary type="html"><![CDATA[Semidefinite Programming (SDP) is a class of convex optimization programs
with vast applications in control theory, quantum information, combinatorial
optimization and operational research. Noisy intermediate-scale quantum (NISQ)
algorithms aim to make an efficient use of the current generation of quantum
hardware. However, optimizing variational quantum algorithms is a challenge as
it is an NP-hard problem that in general requires an exponential time to solve
and can contain many far from optimal local minima. Here, we present a current
term NISQ algorithm for SDP. The classical optimization program of our NISQ
solver is another SDP over a smaller dimensional ansatz space. We harness the
SDP based formulation of the Hamiltonian ground state problem to design a NISQ
eigensolver. Unlike variational quantum eigensolvers, the classical
optimization program of our eigensolver is convex, can be solved in polynomial
time with the number of ansatz parameters and every local minimum is a global
minimum. Further, we demonstrate the potential of our NISQ SDP solver by
finding the largest eigenvalue of up to $2^{1000}$ dimensional matrices and
solving graph problems related to quantum contextuality. We also discuss NISQ
algorithms for rank-constrained SDPs. Our work extends the application of NISQ
computers onto one of the most successful algorithmic frameworks of the past
few decades.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bharti_K/0/1/0/all/0/1"&gt;Kishor Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Haug_T/0/1/0/all/0/1"&gt;Tobias Haug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vedral_V/0/1/0/all/0/1"&gt;Vlatko Vedral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kwek_L/0/1/0/all/0/1"&gt;Leong-Chuan Kwek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast rates in structured prediction. (arXiv:2102.00760v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00760</id>
        <link href="http://arxiv.org/abs/2102.00760"/>
        <updated>2021-06-09T02:01:50.201Z</updated>
        <summary type="html"><![CDATA[Discrete supervised learning problems such as classification are often
tackled by introducing a continuous surrogate problem akin to regression.
Bounding the original error, between estimate and solution, by the surrogate
error endows discrete problems with convergence rates already shown for
continuous instances. Yet, current approaches do not leverage the fact that
discrete problems are essentially predicting a discrete output when continuous
problems are predicting a continuous value. In this paper, we tackle this issue
for general structured prediction problems, opening the way to "super fast"
rates, that is, convergence rates for the excess risk faster than $n^{-1}$,
where $n$ is the number of observations, with even exponential rates with the
strongest assumptions. We first illustrate it for predictors based on nearest
neighbors, generalizing rates known for binary classification to any discrete
problem within the framework of structured prediction. We then consider kernel
ridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast
rates, depending on a parameter characterizing the hardness of the problem,
thus allowing, under smoothness assumptions, to bypass the curse of
dimensionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Bin Packing with Predictions. (arXiv:2102.03311v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03311</id>
        <link href="http://arxiv.org/abs/2102.03311"/>
        <updated>2021-06-09T02:01:50.201Z</updated>
        <summary type="html"><![CDATA[Bin packing is a classic optimization problem with a wide range of
applications from load balancing in networks to supply chain management. In
this work we study the online variant of the problem, in which a sequence of
items of various sizes must be placed into a minimum number of bins of uniform
capacity. The online algorithm is enhanced with a (potentially erroneous)
prediction concerning the frequency of item sizes in the sequence. We design
and analyze online algorithms with efficient tradeoffs between consistency
(i.e., the competitive ratio assuming no prediction error) and robustness
(i.e., the competitive ratio under adversarial error), and whose performance
degrades gently as a function of the prediction error. This is the first
theoretical study of online bin packing in the realistic setting of erroneous
predictions, as well as the first experimental study in the setting in which
the input is generated according to both static and evolving distributions.
Previous work on this problem has only addressed the extreme cases with respect
to the prediction error, has relied on overly powerful and error-free
prediction oracles, and has focused on experimental evaluation based on static
input distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_S/0/1/0/all/0/1"&gt;Spyros Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamali_S/0/1/0/all/0/1"&gt;Shahin Kamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shadkami_K/0/1/0/all/0/1"&gt;Kimia Shadkami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-Human Performance in Online Low-latency Recognition of Conversational Speech. (arXiv:2010.03449v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03449</id>
        <link href="http://arxiv.org/abs/2010.03449"/>
        <updated>2021-06-09T02:01:50.200Z</updated>
        <summary type="html"><![CDATA[Achieving super-human performance in recognizing human speech has been a goal
for several decades, as researchers have worked on increasingly challenging
tasks. In the 1990's it was discovered, that conversational speech between two
humans turns out to be considerably more difficult than read speech as
hesitations, disfluencies, false starts and sloppy articulation complicate
acoustic processing and require robust handling of acoustic, lexical and
language context, jointly. Early attempts with statistical models could only
reach error rates over 50% and far from human performance (WER of around 5.5%).
Neural hybrid models and recent attention-based encoder-decoder models have
considerably improved performance as such contexts can now be learned in an
integral fashion. However, processing such contexts requires an entire
utterance presentation and thus introduces unwanted delays before a recognition
result can be output. In this paper, we address performance as well as latency.
We present results for a system that can achieve super-human performance (at a
WER of 5.0%, over the Switchboard conversational benchmark) at a word based
latency of only 1 second behind a speaker's speech. The system uses multiple
attention-based encoder-decoder networks integrated within a novel low latency
incremental inference approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thai-Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stueker_S/0/1/0/all/0/1"&gt;Sebastian Stueker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alex Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the use of automatically generated synthetic image datasets for benchmarking face recognition. (arXiv:2106.04215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04215</id>
        <link href="http://arxiv.org/abs/2106.04215"/>
        <updated>2021-06-09T02:01:50.198Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale face datasets has been key in the progress of
face recognition. However, due to licensing issues or copyright infringement,
some datasets are not available anymore (e.g. MS-Celeb-1M). Recent advances in
Generative Adversarial Networks (GANs), to synthesize realistic face images,
provide a pathway to replace real datasets by synthetic datasets, both to train
and benchmark face recognition (FR) systems. The work presented in this paper
provides a study on benchmarking FR systems using a synthetic dataset. First,
we introduce the proposed methodology to generate a synthetic dataset, without
the need for human intervention, by exploiting the latent structure of a
StyleGAN2 model with multiple controlled factors of variation. Then, we confirm
that (i) the generated synthetic identities are not data subjects from the
GAN's training dataset, which is verified on a synthetic dataset with 10K+
identities; (ii) benchmarking results on the synthetic dataset are a good
substitution, often providing error rates and system ranking similar to the
benchmarking on the real dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colbois_L/0/1/0/all/0/1"&gt;Laurent Colbois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1"&gt;Tiago de Freitas Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Marcel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning. (arXiv:2106.04015v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04015</id>
        <link href="http://arxiv.org/abs/2106.04015"/>
        <updated>2021-06-09T02:01:50.198Z</updated>
        <summary type="html"><![CDATA[High-quality estimates of uncertainty and robustness are crucial for numerous
real-world applications, especially for deep learning which underlies many
deployed ML systems. The ability to compare techniques for improving these
estimates is therefore very important for research and practice alike. Yet,
competitive comparisons of methods are often lacking due to a range of reasons,
including: compute availability for extensive tuning, incorporation of
sufficiently many baselines, and concrete documentation for reproducibility. In
this paper we introduce Uncertainty Baselines: high-quality implementations of
standard and state-of-the-art deep learning methods on a variety of tasks. As
of this writing, the collection spans 19 methods across 9 tasks, each with at
least 5 metrics. Each baseline is a self-contained experiment pipeline with
easily reusable and extendable components. Our goal is to provide immediate
starting points for experimentation with new methods or applications.
Additionally we provide model checkpoints, experiment outputs as Python
notebooks, and leaderboards for comparing results. Code available at
https://github.com/google/uncertainty-baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nado_Z/0/1/0/all/0/1"&gt;Zachary Nado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1"&gt;Mark Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1"&gt;Josip Djolonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusenberry_M/0/1/0/all/0/1"&gt;Michael W. Dusenberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1"&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filos_A/0/1/0/all/0/1"&gt;Angelos Filos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1"&gt;Marton Havasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jerfel_G/0/1/0/all/0/1"&gt;Ghassen Jerfel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mariet_Z/0/1/0/all/0/1"&gt;Zelda Mariet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1"&gt;Jeremy Nixon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhy_S/0/1/0/all/0/1"&gt;Shreyas Padhy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1"&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yeming Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1"&gt;Florian Wenzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1"&gt;Kevin Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sculley_D/0/1/0/all/0/1"&gt;D. Sculley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1"&gt;Jasper Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph Clustering. (arXiv:2105.05320v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05320</id>
        <link href="http://arxiv.org/abs/2105.05320"/>
        <updated>2021-06-09T02:01:50.198Z</updated>
        <summary type="html"><![CDATA[Recently, there has been considerable research interest in graph clustering
aimed at data partition using the graph information. However, one limitation of
the most of graph-based methods is that they assume the graph structure to
operate is fixed and reliable. And there are inevitably some edges in the graph
that are not conducive to graph clustering, which we call spurious edges. This
paper is the first attempt to employ graph pooling technique for node
clustering and we propose a novel dual graph embedding network (DGEN), which is
designed as a two-step graph encoder connected by a graph pooling layer to
learn the graph embedding. In our model, it is assumed that if a node and its
nearest neighboring node are close to the same clustering center, this node is
an informative node and this edge can be considered as a cluster-friendly edge.
Based on this assumption, the neighbor cluster pooling (NCPool) is devised to
select the most informative subset of nodes and the corresponding edges based
on the distance of nodes and their nearest neighbors to the cluster centers.
This can effectively alleviate the impact of the spurious edges on the
clustering. Finally, to obtain the clustering assignment of all nodes, a
classifier is trained using the clustering results of the selected nodes.
Experiments on five benchmark graph datasets demonstrate the superiority of the
proposed method over state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dongxia Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotating spiders and reflecting dogs: a class conditional approach to learning data augmentation distributions. (arXiv:2106.04009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04009</id>
        <link href="http://arxiv.org/abs/2106.04009"/>
        <updated>2021-06-09T02:01:50.197Z</updated>
        <summary type="html"><![CDATA[Building invariance to non-meaningful transformations is essential to
building efficient and generalizable machine learning models. In practice, the
most common way to learn invariance is through data augmentation. There has
been recent interest in the development of methods that learn distributions on
augmentation transformations from the training data itself. While such
approaches are beneficial since they are responsive to the data, they ignore
the fact that in many situations the range of transformations to which a model
needs to be invariant changes depending on the particular class input belongs
to. For example, if a model needs to be able to predict whether an image
contains a starfish or a dog, we may want to apply random rotations to starfish
images during training (since these do not have a preferred orientation), but
we would not want to do this to images of dogs. In this work we introduce a
method by which we can learn class conditional distributions on augmentation
transformations. We give a number of examples where our methods learn different
non-meaningful transformations depending on class and further show how our
method can be used as a tool to probe the symmetries intrinsic to a potentially
complex dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahan_S/0/1/0/all/0/1"&gt;Scott Mahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doster_T/0/1/0/all/0/1"&gt;Tim Doster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LEADS: Learning Dynamical Systems that Generalize Across Environments. (arXiv:2106.04546v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04546</id>
        <link href="http://arxiv.org/abs/2106.04546"/>
        <updated>2021-06-09T02:01:50.197Z</updated>
        <summary type="html"><![CDATA[When modeling dynamical systems from real-world data samples, the
distribution of data often changes according to the environment in which they
are captured, and the dynamics of the system itself vary from one environment
to another. Generalizing across environments thus challenges the conventional
frameworks. The classical settings suggest either considering data as i.i.d.
and learning a single model to cover all situations or learning
environment-specific models. Both are sub-optimal: the former disregards the
discrepancies between environments leading to biased solutions, while the
latter does not exploit their potential commonalities and is prone to scarcity
problems. We propose LEADS, a novel framework that leverages the commonalities
and discrepancies among known environments to improve model generalization.
This is achieved with a tailored training formulation aiming at capturing
common dynamics within a shared model while additional terms capture
environment-specific dynamics. We ground our approach in theory, exhibiting a
decrease in sample complexity with our approach and corroborate these results
empirically, instantiating it for linear dynamics. Moreover, we concretize this
framework for neural networks and evaluate it experimentally on representative
families of nonlinear dynamics. We show that this new setting can exploit
knowledge extracted from environment-dependent data and improves generalization
for both known and novel environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yuan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ibrahim Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1"&gt;Emmanuel de B&amp;#xe9;zenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baskiotis_N/0/1/0/all/0/1"&gt;Nicolas Baskiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directional Bias Amplification. (arXiv:2102.12594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12594</id>
        <link href="http://arxiv.org/abs/2102.12594"/>
        <updated>2021-06-09T02:01:50.197Z</updated>
        <summary type="html"><![CDATA[Mitigating bias in machine learning systems requires refining our
understanding of bias propagation pathways: from societal structures to
large-scale data to trained models to impact on society. In this work, we focus
on one aspect of the problem, namely bias amplification: the tendency of models
to amplify the biases present in the data they are trained on. A metric for
measuring bias amplification was introduced in the seminal work by Zhao et al.
(2017); however, as we demonstrate, this metric suffers from a number of
shortcomings including conflating different types of bias amplification and
failing to account for varying base rates of protected attributes. We introduce
and analyze a new, decoupled metric for measuring bias amplification,
$\text{BiasAmp}_{\rightarrow}$ (Directional Bias Amplification). We thoroughly
analyze and discuss both the technical assumptions and normative implications
of this metric. We provide suggestions about its measurement by cautioning
against predicting sensitive attributes, encouraging the use of confidence
intervals due to fluctuations in the fairness of models across runs, and
discussing the limitations of what this metric captures. Throughout this paper,
we work to provide an interrogative look at the technical measurement of bias
amplification, guided by our normative ideas of what we want it to encompass.
Code is located at https://github.com/princetonvisualai/directional-bias-amp]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Angelina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Sparse Training for Deep Reinforcement Learning. (arXiv:2106.04217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04217</id>
        <link href="http://arxiv.org/abs/2106.04217"/>
        <updated>2021-06-09T02:01:50.196Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning has achieved significant success in many
decision-making tasks in various fields. However, it requires a large training
time of dense neural networks to obtain a good performance. This hinders its
applicability on low-resource devices where memory and computation are strictly
constrained. In a step towards enabling deep reinforcement learning agents to
be applied to low-resource devices, in this work, we propose for the first time
to dynamically train deep reinforcement learning agents with sparse neural
networks from scratch. We adopt the evolution principles of dynamic sparse
training in the reinforcement learning paradigm and introduce a training
algorithm that optimizes the sparse topology and the weight values jointly to
dynamically fit the incoming data. Our approach is easy to be integrated into
existing deep reinforcement learning algorithms and has many favorable
advantages. First, it allows for significant compression of the network size
which reduces the memory and computation costs substantially. This would
accelerate not only the agent inference but also its training process. Second,
it speeds up the agent learning process and allows for reducing the number of
required training steps. Third, it can achieve higher performance than training
the dense counterpart network. We evaluate our approach on OpenAI gym
continuous control tasks. The experimental results show the effectiveness of
our approach in achieving higher performance than one of the state-of-art
baselines with a 50\% reduction in the network size and floating-point
operations (FLOPs). Moreover, our proposed approach can reach the same
performance achieved by the dense network with a 40-50\% reduction in the
number of training steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1"&gt;Ghada Sokar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1"&gt;Elena Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPRNet: Hierarchical Point Regression for Whole-Body Human Pose Estimation. (arXiv:2106.04269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04269</id>
        <link href="http://arxiv.org/abs/2106.04269"/>
        <updated>2021-06-09T02:01:50.195Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new bottom-up one-stage method for whole-body
pose estimation, which we name "hierarchical point regression," or HPRNet for
short, referring to the network that implements this method. To handle the
scale variance among different body parts, we build a hierarchical point
representation of body parts and jointly regress them. Unlike the existing
two-stage methods, our method predicts whole-body pose in a constant time
independent of the number of people in an image. On the COCO WholeBody dataset,
HPRNet significantly outperforms all previous bottom-up methods on the keypoint
detection of all whole-body parts (i.e. body, foot, face and hand); it also
achieves state-of-the-art results in the face (75.4 AP) and hand (50.4 AP)
keypoint detection. Code and models are available at
https://github.com/nerminsamet/HPRNet.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samet_N/0/1/0/all/0/1"&gt;Nermin Samet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph-level Representation Learning with Local and Global Structure. (arXiv:2106.04113v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04113</id>
        <link href="http://arxiv.org/abs/2106.04113"/>
        <updated>2021-06-09T02:01:50.195Z</updated>
        <summary type="html"><![CDATA[This paper studies unsupervised/self-supervised whole-graph representation
learning, which is critical in many tasks such as molecule properties
prediction in drug and material discovery. Existing methods mainly focus on
preserving the local similarity structure between different graph instances but
fail to discover the global semantic structure of the entire data set. In this
paper, we propose a unified framework called Local-instance and Global-semantic
Learning (GraphLoG) for self-supervised whole-graph representation learning.
Specifically, besides preserving the local similarities, GraphLoG introduces
the hierarchical prototypes to capture the global semantic clusters. An
efficient online expectation-maximization (EM) algorithm is further developed
for learning the model. We evaluate GraphLoG by pre-training it on massive
unlabeled graphs followed by fine-tuning on downstream tasks. Extensive
experiments on both chemical and biological benchmark data sets demonstrate the
effectiveness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minghao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-series Imputation of Temporally-occluded Multiagent Trajectories. (arXiv:2106.04219v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04219</id>
        <link href="http://arxiv.org/abs/2106.04219"/>
        <updated>2021-06-09T02:01:50.193Z</updated>
        <summary type="html"><![CDATA[In multiagent environments, several decision-making individuals interact
while adhering to the dynamics constraints imposed by the environment. These
interactions, combined with the potential stochasticity of the agents'
decision-making processes, make such systems complex and interesting to study
from a dynamical perspective. Significant research has been conducted on
learning models for forward-direction estimation of agent behaviors, for
example, pedestrian predictions used for collision-avoidance in self-driving
cars. However, in many settings, only sporadic observations of agents may be
available in a given trajectory sequence. For instance, in football, subsets of
players may come in and out of view of broadcast video footage, while
unobserved players continue to interact off-screen. In this paper, we study the
problem of multiagent time-series imputation, where available past and future
observations of subsets of agents are used to estimate missing observations for
other agents. Our approach, called the Graph Imputer, uses forward- and
backward-information in combination with graph networks and variational
autoencoders to enable learning of a distribution of imputed trajectories. We
evaluate our approach on a dataset of football matches, using a projective
camera module to train and evaluate our model for the off-screen player state
estimation setting. We illustrate that our method outperforms several
state-of-the-art approaches, including those hand-crafted for football.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Omidshafiei_S/0/1/0/all/0/1"&gt;Shayegan Omidshafiei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennes_D/0/1/0/all/0/1"&gt;Daniel Hennes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garnelo_M/0/1/0/all/0/1"&gt;Marta Garnelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarassov_E/0/1/0/all/0/1"&gt;Eugene Tarassov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Connor_J/0/1/0/all/0/1"&gt;Jerome T. Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1"&gt;Paul Muller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_I/0/1/0/all/0/1"&gt;Ian Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spearman_W/0/1/0/all/0/1"&gt;William Spearman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1"&gt;Karl Tuyls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Mixture Density Networks. (arXiv:2012.03085v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03085</id>
        <link href="http://arxiv.org/abs/2012.03085"/>
        <updated>2021-06-09T02:01:50.193Z</updated>
        <summary type="html"><![CDATA[We introduce the Graph Mixture Density Networks, a new family of machine
learning models that can fit multimodal output distributions conditioned on
graphs of arbitrary topology. By combining ideas from mixture models and graph
representation learning, we address a broader class of challenging conditional
density estimation problems that rely on structured data. In this respect, we
evaluate our method on a new benchmark application that leverages random graphs
for stochastic epidemic simulations. We show a significant improvement in the
likelihood of epidemic outcomes when taking into account both multimodality and
structure. The empirical analysis is complemented by two real-world regression
tasks showing the effectiveness of our approach in modeling the output
prediction uncertainty. Graph Mixture Density Networks open appealing research
opportunities in the study of structure-dependent phenomena that exhibit
non-trivial conditional output distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1"&gt;Federico Errica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1"&gt;Alessio Micheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XIRL: Cross-embodiment Inverse Reinforcement Learning. (arXiv:2106.03911v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.03911</id>
        <link href="http://arxiv.org/abs/2106.03911"/>
        <updated>2021-06-09T02:01:50.192Z</updated>
        <summary type="html"><![CDATA[We investigate the visual cross-embodiment imitation setting, in which agents
learn policies from videos of other agents (such as humans) demonstrating the
same task, but with stark differences in their embodiments -- shape, actions,
end-effector dynamics, etc. In this work, we demonstrate that it is possible to
automatically discover and learn vision-based reward functions from
cross-embodiment demonstration videos that are robust to these differences.
Specifically, we present a self-supervised method for Cross-embodiment Inverse
Reinforcement Learning (XIRL) that leverages temporal cycle-consistency
constraints to learn deep visual embeddings that capture task progression from
offline videos of demonstrations across multiple expert agents, each performing
the same task differently due to embodiment differences. Prior to our work,
producing rewards from self-supervised embeddings has typically required
alignment with a reference trajectory, which may be difficult to acquire. We
show empirically that if the embeddings are aware of task-progress, simply
taking the negative distance between the current state and goal state in the
learned embedding space is useful as a reward for training policies with
reinforcement learning. We find our learned reward function not only works for
embodiments seen during training, but also generalizes to entirely new
embodiments. We also find that XIRL policies are more sample efficient than
baselines, and in some cases exceed the sample efficiency of the same agent
trained with ground truth sparse rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakka_K/0/1/0/all/0/1"&gt;Kevin Zakka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Andy Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1"&gt;Pete Florence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1"&gt;Jonathan Tompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1"&gt;Jeannette Bohg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1"&gt;Debidatta Dwibedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BIGDML: Towards Exact Machine Learning Force Fields for Materials. (arXiv:2106.04229v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.04229</id>
        <link href="http://arxiv.org/abs/2106.04229"/>
        <updated>2021-06-09T02:01:50.192Z</updated>
        <summary type="html"><![CDATA[Machine-learning force fields (MLFF) should be accurate, computationally and
data efficient, and applicable to molecules, materials, and interfaces thereof.
Currently, MLFFs often introduce tradeoffs that restrict their practical
applicability to small subsets of chemical space or require exhaustive datasets
for training. Here, we introduce the Bravais-Inspired Gradient-Domain Machine
Learning (BIGDML) approach and demonstrate its ability to construct reliable
force fields using a training set with just 10-200 geometries for materials
including pristine and defect-containing 2D and 3D semiconductors and metals,
as well as chemisorbed and physisorbed atomic and molecular adsorbates on
surfaces. The BIGDML model employs the full relevant symmetry group for a given
material, does not assume artificial atom types or localization of atomic
interactions and exhibits high data efficiency and state-of-the-art energy
accuracies (errors substantially below 1 meV per atom) for an extended set of
materials. Extensive path-integral molecular dynamics carried out with BIGDML
models demonstrate the counterintuitive localization of benzene--graphene
dynamics induced by nuclear quantum effects and allow to rationalize the
Arrhenius behavior of hydrogen diffusion coefficient in a Pd crystal for a wide
range of temperatures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sauceda_H/0/1/0/all/0/1"&gt;Huziel E. Sauceda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Galvez_Gonzalez_L/0/1/0/all/0/1"&gt;Luis E. G&amp;#xe1;lvez-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Paz_Borbon_L/0/1/0/all/0/1"&gt;Lauro Oliver Paz-Borb&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Tkatchenko_A/0/1/0/all/0/1"&gt;Alexandre Tkatchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Robust Detection of Out-of-distribution Data (almost) for free. (arXiv:2106.04260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04260</id>
        <link href="http://arxiv.org/abs/2106.04260"/>
        <updated>2021-06-09T02:01:50.192Z</updated>
        <summary type="html"><![CDATA[When applying machine learning in safety-critical systems, a reliable
assessment of the uncertainy of a classifier is required. However, deep neural
networks are known to produce highly overconfident predictions on
out-of-distribution (OOD) data and even if trained to be non-confident on OOD
data one can still adversarially manipulate OOD data so that the classifer
again assigns high confidence to the manipulated samples. In this paper we
propose a novel method where from first principles we combine a certifiable OOD
detector with a standard classifier into an OOD aware classifier. In this way
we achieve the best of two worlds: certifiably adversarially robust OOD
detection, even for OOD samples close to the in-distribution, without loss in
prediction accuracy and close to state-of-the-art OOD detection performance for
non-manipulated OOD data. Moreover, due to the particular construction our
classifier provably avoids the asymptotic overconfidence problem of standard
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meinke_A/0/1/0/all/0/1"&gt;Alexander Meinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bitterwolf_J/0/1/0/all/0/1"&gt;Julian Bitterwolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Sampling in POMDPs with Lipschitz Bandits for Motion Planning in Continuous Spaces. (arXiv:2106.04206v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04206</id>
        <link href="http://arxiv.org/abs/2106.04206"/>
        <updated>2021-06-09T02:01:50.191Z</updated>
        <summary type="html"><![CDATA[Decision making under uncertainty can be framed as a partially observable
Markov decision process (POMDP). Finding exact solutions of POMDPs is generally
computationally intractable, but the solution can be approximated by
sampling-based approaches. These sampling-based POMDP solvers rely on
multi-armed bandit (MAB) heuristics, which assume the outcomes of different
actions to be uncorrelated. In some applications, like motion planning in
continuous spaces, similar actions yield similar outcomes. In this paper, we
utilize variants of MAB heuristics that make Lipschitz continuity assumptions
on the outcomes of actions to improve the efficiency of sampling-based planning
approaches. We demonstrate the effectiveness of this approach in the context of
motion planning for automated driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tas_O/0/1/0/all/0/1"&gt;&amp;#xd6;mer &amp;#x15e;ahin Ta&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauser_F/0/1/0/all/0/1"&gt;Felix Hauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lauer_M/0/1/0/all/0/1"&gt;Martin Lauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Byakto Speech: Real-time long speech synthesis with convolutional neural network: Transfer learning from English to Bangla. (arXiv:2106.03937v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.03937</id>
        <link href="http://arxiv.org/abs/2106.03937"/>
        <updated>2021-06-09T02:01:50.184Z</updated>
        <summary type="html"><![CDATA[Speech synthesis is one of the challenging tasks to automate by deep
learning, also being a low-resource language there are very few attempts at
Bangla speech synthesis. Most of the existing works can't work with anything
other than simple Bangla characters script, very short sentences, etc. This
work attempts to solve these problems by introducing Byakta, the first-ever
open-source deep learning-based bilingual (Bangla and English) text to a speech
synthesis system. A speech recognition model-based automated scoring metric was
also proposed to evaluate the performance of a TTS model. We also introduce a
test benchmark dataset for Bangla speech synthesis models for evaluating speech
quality. The TTS is available at https://github.com/zabir-nabil/bangla-tts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nazi_Z/0/1/0/all/0/1"&gt;Zabir Al Nazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huda_S/0/1/0/all/0/1"&gt;Sayed Mohammed Tasmimul Huda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. (arXiv:2106.04392v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04392</id>
        <link href="http://arxiv.org/abs/2106.04392"/>
        <updated>2021-06-09T02:01:50.184Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yihong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Ying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Songtao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qingjiang Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MindReader: Recommendation over Knowledge Graph Entities with Explicit User Ratings. (arXiv:2106.04209v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04209</id>
        <link href="http://arxiv.org/abs/2106.04209"/>
        <updated>2021-06-09T02:01:50.183Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KGs) have been integrated in several models of
recommendation to augment the informational value of an item by means of its
related entities in the graph. Yet, existing datasets only provide explicit
ratings on items and no information is provided about user opinions of other
(non-recommendable) entities. To overcome this limitation, we introduce a new
dataset, called the MindReader, providing explicit user ratings both for items
and for KG entities. In this first version, the MindReader dataset provides
more than 102 thousands explicit ratings collected from 1,174 real users on
both items and entities from a KG in the movie domain. This dataset has been
collected through an online interview application that we also release open
source. As a demonstration of the importance of this new dataset, we present a
comparative study of the effect of the inclusion of ratings on non-item KG
entities in a variety of state-of-the-art recommendation models. In particular,
we show that most models, whether designed specifically for graph data or not,
see improvements in recommendation quality when trained on explicit non-item
ratings. Moreover, for some models, we show that non-item ratings can
effectively replace item ratings without loss of recommendation quality. This
finding, thanks also to an observed greater familiarity of users towards common
KG entities than towards long-tail items, motivates the use of KG entities for
both warm and cold-start recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brams_A/0/1/0/all/0/1"&gt;Anders H. Brams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobsen_A/0/1/0/all/0/1"&gt;Anders L. Jakobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jendal_T/0/1/0/all/0/1"&gt;Theis E. Jendal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lissandrini_M/0/1/0/all/0/1"&gt;Matteo Lissandrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolog_P/0/1/0/all/0/1"&gt;Peter Dolog&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hose_K/0/1/0/all/0/1"&gt;Katja Hose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning. (arXiv:2008.03606v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03606</id>
        <link href="http://arxiv.org/abs/2008.03606"/>
        <updated>2021-06-09T02:01:50.183Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a challenging setting for optimization due to the
heterogeneity of the data across different clients which gives rise to the
client drift phenomenon. In fact, obtaining an algorithm for FL which is
uniformly better than simple centralized training has been a major open problem
thus far. In this work, we propose a general algorithmic framework, Mime, which
i) mitigates client drift and ii) adapts arbitrary centralized optimization
algorithms such as momentum and Adam to the cross-device federated learning
setting. Mime uses a combination of control-variates and server-level
statistics (e.g. momentum) at every client-update step to ensure that each
local update mimics that of the centralized method run on iid data. We prove a
reduction result showing that Mime can translate the convergence of a generic
algorithm in the centralized setting into convergence in the federated setting.
Further, we show that when combined with momentum based variance reduction,
Mime is provably faster than any centralized method--the first such result. We
also perform a thorough experimental exploration of Mime's performance on real
world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1"&gt;Satyen Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank J. Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation. (arXiv:2106.04403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04403</id>
        <link href="http://arxiv.org/abs/2106.04403"/>
        <updated>2021-06-09T02:01:50.182Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have brought significant progress in visual
grounding tasks such as language-guided video object segmentation. However,
collecting large datasets for these tasks is expensive in terms of annotation
time, which represents a bottleneck. To this end, we propose a novel method,
namely SynthRef, for generating synthetic referring expressions for target
objects in an image (or video frame), and we also present and disseminate the
first large-scale dataset with synthetic referring expressions for video object
segmentation. Our experiments demonstrate that by training with our synthetic
referring expressions one can improve the ability of a model to generalize
across different datasets, without any additional annotation cost. Moreover,
our formulation allows its application to any object detection or segmentation
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazakos_I/0/1/0/all/0/1"&gt;Ioannis Kazakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellver_M/0/1/0/all/0/1"&gt;Miriam Bellver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1"&gt;Carina Silberer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking the Limits of Message Passing Graph Neural Networks. (arXiv:2106.04319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04319</id>
        <link href="http://arxiv.org/abs/2106.04319"/>
        <updated>2021-06-09T02:01:50.182Z</updated>
        <summary type="html"><![CDATA[Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear
complexity with respect to the number of nodes when applied to sparse graphs,
they have been widely implemented and still raise a lot of interest even though
their theoretical expressive power is limited to the first order
Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph
convolution supports are designed in spectral-domain by a non-linear custom
function of eigenvalues and masked with an arbitrary large receptive field, the
MPNN is theoretically more powerful than the 1-WL test and experimentally as
powerful as a 3-WL existing models, while remaining spatially localized.
Moreover, by designing custom filter functions, outputs can have various
frequency components that allow the convolution process to learn different
relationships between a given input graph signal and its associated properties.
So far, the best 3-WL equivalent graph neural networks have a computational
complexity in $\mathcal{O}(n^3)$ with memory usage in $\mathcal{O}(n^2)$,
consider non-local update mechanism and do not provide the spectral richness of
output profile. The proposed method overcomes all these aforementioned problems
and reaches state-of-the-art results in many downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balcilar_M/0/1/0/all/0/1"&gt;Muhammet Balcilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heroux_P/0/1/0/all/0/1"&gt;Pierre H&amp;#xe9;roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gauzere_B/0/1/0/all/0/1"&gt;Benoit Ga&amp;#xfc;z&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1"&gt;Pascal Vasseur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honeine_P/0/1/0/all/0/1"&gt;Paul Honeine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Hierarchical Learning Based Network Traffic Analytics. (arXiv:2106.03850v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03850</id>
        <link href="http://arxiv.org/abs/2106.03850"/>
        <updated>2021-06-09T02:01:50.181Z</updated>
        <summary type="html"><![CDATA[Classifying network traffic is the basis for important network applications.
Prior research in this area has faced challenges on the availability of
representative datasets, and many of the results cannot be readily reproduced.
Such a problem is exacerbated by emerging data-driven machine learning based
approaches. To address this issue, we present(N et)2databasewith three open
datasets containing nearly 1.3M labeled flows in total, with a comprehensive
list of flow features, for there search community1. We focus on broad aspects
in network traffic analysis, including both malware detection and application
classification. As we continue to grow them, we expect the datasets to serve as
a common ground for AI driven, reproducible research on network flow analytics.
We release the datasets publicly and also introduce a Multi-Task Hierarchical
Learning (MTHL)model to perform all tasks in a single model. Our results show
that MTHL is capable of accurately performing multiple tasks with hierarchical
labeling with a dramatic reduction in training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barut_O/0/1/0/all/0/1"&gt;Onur Barut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weigang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peilong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Out-of-Distribution Detection with Mixup Outlier Exposure. (arXiv:2106.03917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03917</id>
        <link href="http://arxiv.org/abs/2106.03917"/>
        <updated>2021-06-09T02:01:50.181Z</updated>
        <summary type="html"><![CDATA[Enabling out-of-distribution (OOD) detection for DNNs is critical for their
safe and reliable operation in the "open world". Unfortunately, current works
in both methodology and evaluation focus on rather contrived detection
problems, and only consider a coarse level of granularity w.r.t.: 1) the
in-distribution (ID) classes, and 2) the OOD data's "closeness" to the ID data.
We posit that such settings may be poor approximations of many real-world tasks
that are naturally fine-grained (e.g., bird species classification), and thus
the reported detection abilities may be over-estimates. Differently, in this
work we make granularity a top priority and focus on fine-grained OOD
detection. We start by carefully constructing five novel fine-grained test
environments in which existing methods are shown to have difficulties. We then
propose a new DNN training algorithm, Mixup Outlier Exposure (MixupOE), which
leverages an outlier distribution and principles from vicinal risk
minimization. Finally, we perform extensive experiments and analyses in our
custom test environments and demonstrate that MixupOE can consistently improve
fine-grained detection performance, establishing a strong baseline in these
more realistic and challenging OOD detection settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1"&gt;Nathan Inkawhich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiran Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Active Learning for Bayesian Decision-Making. (arXiv:2106.04193v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04193</id>
        <link href="http://arxiv.org/abs/2106.04193"/>
        <updated>2021-06-09T02:01:50.180Z</updated>
        <summary type="html"><![CDATA[Active learning is usually applied to acquire labels of informative data
points in supervised learning, to maximize accuracy in a sample-efficient way.
However, maximizing the accuracy is not the end goal when the results are used
for decision-making, for example in personalized medicine or economics. We
argue that when acquiring samples sequentially, separating learning and
decision-making is sub-optimal, and we introduce a novel active learning
strategy which takes the down-the-line decision problem into account.
Specifically, we introduce a novel active learning criterion which maximizes
the expected information gain on the posterior distribution of the optimal
decision. We compare our decision-making-aware active learning strategy to
existing alternatives on both simulated and real data, and show improved
performance in decision-making accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Filstroff_L/0/1/0/all/0/1"&gt;Louis Filstroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sundin_I/0/1/0/all/0/1"&gt;Iiris Sundin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikkola_P/0/1/0/all/0/1"&gt;Petrus Mikkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tiulpin_A/0/1/0/all/0/1"&gt;Aleksei Tiulpin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kylmaoja_J/0/1/0/all/0/1"&gt;Juuso Kylm&amp;#xe4;oja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TENGraD: Time-Efficient Natural Gradient Descent with Exact Fisher-Block Inversion. (arXiv:2106.03947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03947</id>
        <link href="http://arxiv.org/abs/2106.03947"/>
        <updated>2021-06-09T02:01:50.177Z</updated>
        <summary type="html"><![CDATA[This work proposes a time-efficient Natural Gradient Descent method, called
TENGraD, with linear convergence guarantees. Computing the inverse of the
neural network's Fisher information matrix is expensive in NGD because the
Fisher matrix is large. Approximate NGD methods such as KFAC attempt to improve
NGD's running time and practical application by reducing the Fisher matrix
inversion cost with approximation. However, the approximations do not reduce
the overall time significantly and lead to less accurate parameter updates and
loss of curvature information. TENGraD improves the time efficiency of NGD by
computing Fisher block inverses with a computationally efficient covariance
factorization and reuse method. It computes the inverse of each block exactly
using the Woodbury matrix identity to preserve curvature information while
admitting (linear) fast convergence rates. Our experiments on image
classification tasks for state-of-the-art deep neural architecture on CIFAR-10,
CIFAR-100, and Fashion-MNIST show that TENGraD significantly outperforms
state-of-the-art NGD methods and often stochastic gradient descent in
wall-clock time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soori_S/0/1/0/all/0/1"&gt;Saeed Soori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Can_B/0/1/0/all/0/1"&gt;Bugra Can&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1"&gt;Baourun Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert G&amp;#xfc;rb&amp;#xfc;zbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehnavi_M/0/1/0/all/0/1"&gt;Maryam Mehri Dehnavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information. (arXiv:2106.00559v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00559</id>
        <link href="http://arxiv.org/abs/2106.00559"/>
        <updated>2021-06-09T02:01:50.176Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of road users is of vital importance for the
development of trajectory prediction systems. In this context, the latest
advances have focused on recurrent structures, establishing the social
interaction between the agents involved in the scene. More recently, simpler
structures have also been introduced for predicting pedestrian trajectories,
based on Transformer Networks, and using positional information. They allow the
individual modelling of each agent's trajectory separately without any complex
interaction terms. Our model exploits these simple structures by adding
augmented data (position and heading), and adapting their use to the problem of
vehicle trajectory prediction in urban scenarios in prediction horizons up to 5
seconds. In addition, a cross-performance analysis is performed between
different types of scenarios, including highways, intersections and
roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our
model achieves state-of-the-art results and proves to be flexible and adaptable
to different types of urban contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quintanar_A/0/1/0/all/0/1"&gt;A. Quintanar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Llorca_D/0/1/0/all/0/1"&gt;D. Fern&amp;#xe1;ndez-Llorca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_I/0/1/0/all/0/1"&gt;I. Parra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izquierdo_R/0/1/0/all/0/1"&gt;R. Izquierdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotelo_M/0/1/0/all/0/1"&gt;M. A. Sotelo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Theoretical Framework of Out-of-Distribution Generalization. (arXiv:2106.04496v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04496</id>
        <link href="http://arxiv.org/abs/2106.04496"/>
        <updated>2021-06-09T02:01:50.176Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution (OOD) data, or domain generalization,
is one of the central problems in modern machine learning. Recently, there is a
surge of attempts to propose algorithms for OOD that mainly build upon the idea
of extracting invariant features. Although intuitively reasonable, theoretical
understanding of what kind of invariance can guarantee OOD generalization is
still limited, and generalization to arbitrary out-of-distribution is clearly
impossible. In this work, we take the first step towards rigorous and
quantitative definitions of 1) what is OOD; and 2) what does it mean by saying
an OOD problem is learnable. We also introduce a new concept of expansion
function, which characterizes to what extent the variance is amplified in the
test domains over the training domains, and therefore give a quantitative
meaning of invariant features. Based on these, we prove OOD generalization
error bounds. It turns out that OOD generalization largely depends on the
expansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),
any OOD learning algorithm without a model selection module is incomplete. Our
theory naturally induces a model selection criterion. Extensive experiments on
benchmark OOD datasets demonstrate that our model selection criterion has a
significant advantage over baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Haotian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chuanlong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Spatio-Temporal Bilinear Network with Monte Carlo Dropout for Landmark-based Facial Expression Recognition with Uncertainty Estimation. (arXiv:2106.04332v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04332</id>
        <link href="http://arxiv.org/abs/2106.04332"/>
        <updated>2021-06-09T02:01:50.173Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been widely used for feature learning in facial
expression recognition systems. However, small datasets and large intra-class
variability can lead to overfitting. In this paper, we propose a method which
learns an optimized compact network topology for real-time facial expression
recognition utilizing localized facial landmark features. Our method employs a
spatio-temporal bilinear layer as backbone to capture the motion of facial
landmarks during the execution of a facial expression effectively. Besides, it
takes advantage of Monte Carlo Dropout to capture the model's uncertainty which
is of great importance to analyze and treat uncertain cases. The performance of
our method is evaluated on three widely used datasets and it is comparable to
that of video-based state-of-the-art methods while it has much less complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_N/0/1/0/all/0/1"&gt;Negar Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coresets for Classification -- Simplified and Strengthened. (arXiv:2106.04254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04254</id>
        <link href="http://arxiv.org/abs/2106.04254"/>
        <updated>2021-06-09T02:01:50.173Z</updated>
        <summary type="html"><![CDATA[We give relative error coresets for training linear classifiers with a broad
class of loss functions, including the logistic loss and hinge loss. Our
construction achieves $(1\pm \epsilon)$ relative error with $\tilde O(d \cdot
\mu_y(X)^2/\epsilon^2)$ points, where $\mu_y(X)$ is a natural complexity
measure of the data matrix $X \in \mathbb{R}^{n \times d}$ and label vector $y
\in \{-1,1\}^n$, introduced in by Munteanu et al. 2018. Our result is based on
subsampling data points with probabilities proportional to their $\ell_1$
$Lewis$ $weights$. It significantly improves on existing theoretical bounds and
performs well in practice, outperforming uniform subsampling along with other
importance sampling methods. Our sampling distribution does not depend on the
labels, so can be used for active learning. It also does not depend on the
specific loss function, so a single coreset can be used in multiple training
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Tung Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anup B. Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representation Learning for Hand Shape Estimation. (arXiv:2106.04324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04324</id>
        <link href="http://arxiv.org/abs/2106.04324"/>
        <updated>2021-06-09T02:01:50.172Z</updated>
        <summary type="html"><![CDATA[This work presents improvements in monocular hand shape estimation by
building on top of recent advances in unsupervised learning. We extend momentum
contrastive learning and contribute a structured collection of hand images,
well suited for visual representation learning, which we call HanCo. We find
that the representation learned by established contrastive learning methods can
be improved significantly by exploiting advanced background removal techniques
and multi-view information. These allow us to generate more diverse instance
pairs than those obtained by augmentations commonly used in exemplar based
approaches. Our method leads to a more suitable representation for the hand
shape estimation task and shows a 4.7% reduction in mesh error and a 3.6%
improvement in F-score compared to an ImageNet pretrained baseline. We make our
benchmark dataset publicly available, to encourage further research into this
direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_C/0/1/0/all/0/1"&gt;Christian Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1"&gt;Max Argus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention. (arXiv:2106.04471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04471</id>
        <link href="http://arxiv.org/abs/2106.04471"/>
        <updated>2021-06-09T02:01:50.171Z</updated>
        <summary type="html"><![CDATA[Early prediction of cerebral palsy is essential as it leads to early
treatment and monitoring. Deep learning has shown promising results in
biomedical engineering thanks to its capacity of modelling complicated data
with its non-linear architecture. However, due to their complex structure, deep
learning models are generally not interpretable by humans, making it difficult
for clinicians to rely on the findings. In this paper, we propose a channel
attention module for deep learning models to predict cerebral palsy from
infants' body movements, which highlights the key features (i.e. body joints)
the model identifies as important, thereby indicating why certain diagnostic
results are found. To highlight the capacity of the deep network in modelling
input features, we utilize raw joint positions instead of hand-crafted
features. We validate our system with a real-world infant movement dataset. Our
proposed channel attention module enables the visualization of the vital joints
to this disease that the network considers. Our system achieves 91.67%
accuracy, suppressing other state-of-the-art deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Manli Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1"&gt;Qianhui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1"&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1"&gt;Howard Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04169</id>
        <link href="http://arxiv.org/abs/2106.04169"/>
        <updated>2021-06-09T02:01:50.157Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very low
black-box transferability even for large ViT models. However, we show that this
phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance the transferability of existing attacks by introducing two novel
strategies specific to the architecture of ViT models. (i) Self-Ensemble: We
propose a method to find multiple discriminative pathways by dissecting a
single ViT model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens. An
adversarial attack, when applied to such refined tokens within the ensemble of
classifiers found in a single vision transformer, has significantly higher
transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter Inference with Bifurcation Diagrams. (arXiv:2106.04243v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04243</id>
        <link href="http://arxiv.org/abs/2106.04243"/>
        <updated>2021-06-09T02:01:50.151Z</updated>
        <summary type="html"><![CDATA[Estimation of parameters in differential equation models can be achieved by
applying learning algorithms to quantitative time-series data. However,
sometimes it is only possible to measure qualitative changes of a system in
response to a controlled condition. In dynamical systems theory, such change
points are known as \textit{bifurcations} and lie on a function of the
controlled condition called the \textit{bifurcation diagram}. In this work, we
propose a gradient-based semi-supervised approach for inferring the parameters
of differential equations that produce a user-specified bifurcation diagram.
The cost function contains a supervised error term that is minimal when the
model bifurcations match the specified targets and an unsupervised bifurcation
measure which has gradients that push optimisers towards bifurcating parameter
regimes. The gradients can be computed without the need to differentiate
through the operations of the solver that was used to compute the diagram. We
demonstrate parameter inference with minimal models which explore the space of
saddle-node and pitchfork diagrams and the genetic toggle switch from synthetic
biology. Furthermore, the cost landscape allows us to organise models in terms
of topological and geometric equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szep_G/0/1/0/all/0/1"&gt;Gregory Szep&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalchau_N/0/1/0/all/0/1"&gt;Neil Dalchau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Csikasz_Nagy_A/0/1/0/all/0/1"&gt;Attila Csikasz-Nagy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Limited Memory Neural-Linear Bandits with Likelihood Matching. (arXiv:2102.03799v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03799</id>
        <link href="http://arxiv.org/abs/2102.03799"/>
        <updated>2021-06-09T02:01:50.144Z</updated>
        <summary type="html"><![CDATA[We study neural-linear bandits for solving problems where {\em both}
exploration and representation learning play an important role. Neural-linear
bandits harnesses the representation power of Deep Neural Networks (DNNs) and
combines it with efficient exploration mechanisms by leveraging uncertainty
estimation of the model, designed for linear contextual bandits on top of the
last hidden layer. In order to mitigate the problem of representation change
during the process, new uncertainty estimations are computed using stored data
from an unlimited buffer. Nevertheless, when the amount of stored data is
limited, a phenomenon called catastrophic forgetting emerges. To alleviate
this, we propose a likelihood matching algorithm that is resilient to
catastrophic forgetting and is completely online. We applied our algorithm,
Limited Memory Neural-Linear with Likelihood Matching (NeuralLinear-LiM2) on a
variety of datasets and observed that our algorithm achieves comparable
performance to the unlimited memory approach while exhibits resilience to
catastrophic forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nabati_O/0/1/0/all/0/1"&gt;Ofir Nabati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1"&gt;Tom Zahavy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White Paper Assistance: A Step Forward Beyond the Shortcut Learning. (arXiv:2106.04178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04178</id>
        <link href="http://arxiv.org/abs/2106.04178"/>
        <updated>2021-06-09T02:01:50.138Z</updated>
        <summary type="html"><![CDATA[The promising performances of CNNs often overshadow the need to examine
whether they are doing in the way we are actually interested. We show through
experiments that even over-parameterized models would still solve a dataset by
recklessly leveraging spurious correlations, or so-called 'shortcuts'. To
combat with this unintended propensity, we borrow the idea of printer test page
and propose a novel approach called White Paper Assistance. Our proposed method
involves the white paper to detect the extent to which the model has preference
for certain characterized patterns and alleviates it by forcing the model to
make a random guess on the white paper. We show the consistent accuracy
improvements that are manifest in various architectures, datasets and
combinations with other techniques. Experiments have also demonstrated the
versatility of our approach on fine-grained recognition, imbalanced
classification and robustness to corruptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianshu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiali Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras. (arXiv:2106.04477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04477</id>
        <link href="http://arxiv.org/abs/2106.04477"/>
        <updated>2021-06-09T02:01:50.137Z</updated>
        <summary type="html"><![CDATA[Synthesizing novel views of dynamic humans from stationary monocular cameras
is a popular scenario. This is particularly attractive as it does not require
static scenes, controlled environments, or specialized hardware. In contrast to
techniques that exploit multi-view observations to constrain the modeling,
given a single fixed viewpoint only, the problem of modeling the dynamic scene
is significantly more under-constrained and ill-posed. In this paper, we
introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that
models the dynamic scene using a 4D continuous time-variant function. The
proposed representation is learned by an optimization which models a dynamic
scene that minimizes the error of rendering all observation images. At the
heart of our work lies a novel optimization formulation, which is constrained
by a motion consensus regularization on the motion flow. We extensively
evaluate MoCo-Flow on several datasets that contain human motions of varying
complexity, and compare, both qualitatively and quantitatively, to several
baseline methods and variants of our methods. Pretrained model, code, and data
will be released for research purposes upon paper acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuelin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoquan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08248</id>
        <link href="http://arxiv.org/abs/2102.08248"/>
        <updated>2021-06-09T02:01:50.133Z</updated>
        <summary type="html"><![CDATA[Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1"&gt;Jakob D. Havtorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1"&gt;Lars Maal&amp;#xf8;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Runtime-Based Computational Performance Predictor for Deep Neural Network Training. (arXiv:2102.00527v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00527</id>
        <link href="http://arxiv.org/abs/2102.00527"/>
        <updated>2021-06-09T02:01:50.128Z</updated>
        <summary type="html"><![CDATA[Deep learning researchers and practitioners usually leverage GPUs to help
train their deep neural networks (DNNs) faster. However, choosing which GPU to
use is challenging both because (i) there are many options, and (ii) users
grapple with competing concerns: maximizing compute performance while
minimizing costs. In this work, we present a new practical technique to help
users make informed and cost-efficient GPU selections: make performance
predictions with the help of a GPU that the user already has. Our technique
exploits the observation that, because DNN training consists of repetitive
compute steps, predicting the execution time of a single iteration is usually
enough to characterize the performance of an entire training process. We make
predictions by scaling the execution time of each operation in a training
iteration from one GPU to another using either (i) wave scaling, a technique
based on a GPU's execution model, or (ii) pre-trained multilayer perceptrons.
We implement our technique into a Python library called Habitat and find that
it makes accurate iteration execution time predictions (with an average error
of 11.8%) on ResNet-50, Inception v3, the Transformer, GNMT, and DCGAN across
six different GPU architectures. Habitat supports PyTorch, is easy to use, and
is open source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Geoffrey X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yubo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golikov_P/0/1/0/all/0/1"&gt;Pavel Golikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1"&gt;Gennady Pekhimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-dataset Pretraining: A Unified Model for Semantic Segmentation. (arXiv:2106.04121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04121</id>
        <link href="http://arxiv.org/abs/2106.04121"/>
        <updated>2021-06-09T02:01:50.112Z</updated>
        <summary type="html"><![CDATA[Collecting annotated data for semantic segmentation is time-consuming and
hard to scale up. In this paper, we for the first time propose a unified
framework, termed as Multi-Dataset Pretraining, to take full advantage of the
fragmented annotations of different datasets. The highlight is that the
annotations from different domains can be efficiently reused and consistently
boost performance for each specific domain. This is achieved by first
pretraining the network via the proposed pixel-to-prototype contrastive loss
over multiple datasets regardless of their taxonomy labels, and followed by
fine-tuning the pretrained model over specific dataset as usual. In order to
better model the relationship among images and classes from different datasets,
we extend the pixel level embeddings via cross dataset mixing and propose a
pixel-to-class sparse coding strategy that explicitly models the pixel-class
similarity over the manifold embedding space. In this way, we are able to
increase intra-class compactness and inter-class separability, as well as
considering inter-class similarity across different datasets for better
transferability. Experiments conducted on several benchmarks demonstrate its
superior performance. Notably, MDP consistently outperforms the pretrained
models over ImageNet by a considerable margin, while only using less than 10%
samples for pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bowen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haohang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation. (arXiv:2106.04096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04096</id>
        <link href="http://arxiv.org/abs/2106.04096"/>
        <updated>2021-06-09T02:01:50.106Z</updated>
        <summary type="html"><![CDATA[Natural policy gradient (NPG) methods with function approximation achieve
impressive empirical success in reinforcement learning problems with large
state-action spaces. However, theoretical understanding of their convergence
behaviors remains limited in the function approximation setting. In this paper,
we perform a finite-time analysis of NPG with linear function approximation and
softmax parameterization, and prove for the first time that widely used entropy
regularization method, which encourages exploration, leads to linear
convergence rate. We adopt a Lyapunov drift analysis to prove the convergence
results and explain the effectiveness of entropy regularization in improving
the convergence rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for White Blood Cells Nucleus. (arXiv:2101.11654v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11654</id>
        <link href="http://arxiv.org/abs/2101.11654"/>
        <updated>2021-06-09T02:01:50.101Z</updated>
        <summary type="html"><![CDATA[The nucleus of white blood cells (WBCs) plays a significant role in their
detection and classification. Appropriate feature extraction of the nucleus is
necessary to fit a suitable artificial intelligence model to classify WBCs.
Therefore, designing a method is needed to segment the nucleus accurately.
There should be a comparison between the ground truths distinguished by a
hematologist and the detected nuclei to evaluate the performance of the nucleus
segmentation method accurately. It is a time-consuming and tedious task for
experts to establish the ground truth manually. This paper presents an
intelligent open-source software called Easy-GT to create the ground truth of
WBCs' nucleus faster and easier. This software first detects the nucleus by
employing a new Otsu's thresholding-based method with a dice similarity
coefficient (DSC) of 95.42 %; the hematologist can then create a more accurate
ground truth, using the designed buttons to modify the threshold value. This
software can speed up ground truth's forming process more than six times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kouzehkanan_Z/0/1/0/all/0/1"&gt;Zahra Mousavi Kouzehkanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tavakoli_S/0/1/0/all/0/1"&gt;Sajad Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alipanah_A/0/1/0/all/0/1"&gt;Arezoo Alipanah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Deep Q-Network for Autonomous Vehicles at Unsignalized Intersection. (arXiv:2106.04561v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04561</id>
        <link href="http://arxiv.org/abs/2106.04561"/>
        <updated>2021-06-09T02:01:50.096Z</updated>
        <summary type="html"><![CDATA[We propose a safe DRL approach for autonomous vehicle (AV) navigation through
crowds of pedestrians while making a left turn at an unsignalized intersection.
Our method uses two long-short term memory (LSTM) models that are trained to
generate the perceived state of the environment and the future trajectories of
pedestrians given noisy observations of their movement. A future collision
prediction algorithm based on the future trajectories of the ego vehicle and
pedestrians is used to mask unsafe actions if the system predicts a collision.
The performance of our approach is evaluated in two experiments using the
high-fidelity CARLA simulation environment. The first experiment tests the
performance of our method at intersections that are similar to the training
intersection and the second experiment tests our method at intersections with a
different topology. For both experiments, our methods do not result in a
collision with a pedestrian while still navigating the intersection at a
reasonable speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtari_K/0/1/0/all/0/1"&gt;Kasra Mokhtari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Alan R. Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI and Adoption of Financial Algorithmic Advisors: an Experimental Study. (arXiv:2101.02555v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02555</id>
        <link href="http://arxiv.org/abs/2101.02555"/>
        <updated>2021-06-09T02:01:50.090Z</updated>
        <summary type="html"><![CDATA[We study whether receiving advice from either a human or algorithmic advisor,
accompanied by five types of Local and Global explanation labelings, has an
effect on the readiness to adopt, willingness to pay, and trust in a financial
AI consultant. We compare the differences over time and in various key
situations using a unique experimental framework where participants play a
web-based game with real monetary consequences. We observed that accuracy-based
explanations of the model in initial phases leads to higher adoption rates.
When the performance of the model is immaculate, there is less importance
associated with the kind of explanation for adoption. Using more elaborate
feature-based or accuracy-based explanations helps substantially in reducing
the adoption drop upon model failure. Furthermore, using an autopilot increases
adoption significantly. Participants assigned to the AI-labeled advice with
explanations were willing to pay more for the advice than the AI-labeled advice
with a No-explanation alternative. These results add to the literature on the
importance of XAI for algorithmic adoption and trust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1"&gt;Daniel Ben David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resheff_Y/0/1/0/all/0/1"&gt;Yehezkel S. Resheff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tron_T/0/1/0/all/0/1"&gt;Talia Tron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Description and Discussion on DCASE 2021 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring under Domain Shifted Conditions. (arXiv:2106.04492v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.04492</id>
        <link href="http://arxiv.org/abs/2106.04492"/>
        <updated>2021-06-09T02:01:50.076Z</updated>
        <summary type="html"><![CDATA[We present the task description and discussion on the results of the DCASE
2021 Challenge Task 2. Last year, we organized unsupervised anomalous sound
detection (ASD) task; identifying whether the given sound is normal or
anomalous without anomalous training data. In this year, we organize an
advanced unsupervised ASD task under domain-shift conditions which focuses on
the inevitable problem for the practical use of ASD systems. The main challenge
of this task is to detect unknown anomalous sounds where the acoustic
characteristics of the training and testing samples are different, i.e.
domain-shifted. This problem is frequently occurs due to changes in seasons,
manufactured products, and/or environmental noise. After the challenge
submission deadline, we will add challenge results and analysis of the
submissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1"&gt;Yohei Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Imoto_K/0/1/0/all/0/1"&gt;Keisuke Imoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koizumi_Y/0/1/0/all/0/1"&gt;Yuma Koizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harada_N/0/1/0/all/0/1"&gt;Noboru Harada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niizumi_D/0/1/0/all/0/1"&gt;Daisuke Niizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dohi_K/0/1/0/all/0/1"&gt;Kota Dohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanabe_R/0/1/0/all/0/1"&gt;Ryo Tanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Purohit_H/0/1/0/all/0/1"&gt;Harsh Purohit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Endo_T/0/1/0/all/0/1"&gt;Takashi Endo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\ell_0$-based Sparse Canonical Correlation Analysis. (arXiv:2010.05620v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05620</id>
        <link href="http://arxiv.org/abs/2010.05620"/>
        <updated>2021-06-09T02:01:50.070Z</updated>
        <summary type="html"><![CDATA[Canonical Correlation Analysis (CCA) models are powerful for studying the
associations between two sets of variables. The canonically correlated
representations, termed \textit{canonical variates} are widely used in
unsupervised learning to analyze unlabeled multi-modal registered datasets.
Despite their success, CCA models may break (or overfit) if the number of
variables in either of the modalities exceeds the number of samples. Moreover,
often a significant fraction of the variables measures modality-specific
information, and thus removing them is beneficial for identifying the
\textit{canonically correlated variates}. Here, we propose $\ell_0$-CCA, a
method for learning correlated representations based on sparse subsets of
variables from two observed modalities. Sparsity is obtained by multiplying the
input variables by stochastic gates, whose parameters are learned together with
the CCA weights via an $\ell_0$-regularized correlation loss. We further
propose $\ell_0$-Deep CCA for solving the problem of non-linear sparse CCA by
modeling the correlated representations using deep nets. We demonstrate the
efficacy of the method using several synthetic and real examples. Most notably,
by gating nuisance input variables, our approach improves the extracted
representations compared to other linear, non-linear and sparse CCA-based
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindenbaum_O/0/1/0/all/0/1"&gt;Ofir Lindenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salhov_M/0/1/0/all/0/1"&gt;Moshe Salhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Averbuch_A/0/1/0/all/0/1"&gt;Amir Averbuch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kluger_Y/0/1/0/all/0/1"&gt;Yuval Kluger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04156</id>
        <link href="http://arxiv.org/abs/2106.04156"/>
        <updated>2021-06-09T02:01:50.064Z</updated>
        <summary type="html"><![CDATA[Recent works in self-supervised learning have advanced the state-of-the-art
by relying on the contrastive learning paradigm, which learns representations
by pushing positive pairs, or similar examples from the same class, closer
together while keeping negative pairs far apart. Despite the empirical
successes, theoretical foundations are limited -- prior analyses assume
conditional independence of the positive pairs given the same class label, but
recent empirical applications use heavily correlated positive pairs (i.e., data
augmentations of the same image). Our work analyzes contrastive learning
without assuming conditional independence of positive pairs using a novel
concept of the augmentation graph on data. Edges in this graph connect
augmentations of the same data, and ground-truth classes naturally form
connected sub-graphs. We propose a loss that performs spectral decomposition on
the population augmentation graph and can be succinctly written as a
contrastive learning objective on neural net representations. Minimizing this
objective leads to features with provable accuracy guarantees under linear
probe evaluation. By standard generalization bounds, these accuracy guarantees
also hold when minimizing the training contrastive loss. Empirically, the
features learned by our objective can match or outperform several strong
baselines on benchmark vision datasets. In all, this work provides the first
provable analysis for contrastive learning where guarantees for linear probe
evaluation can apply to realistic empirical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1"&gt;Jeff Z. HaoChen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors. (arXiv:2005.07519v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07519</id>
        <link href="http://arxiv.org/abs/2005.07519"/>
        <updated>2021-06-09T02:01:50.058Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML), especially deep learning (DL) techniques have been
increasingly used in anomaly-based network intrusion detection systems (NIDS).
However, ML/DL has shown to be extremely vulnerable to adversarial attacks,
especially in such security-sensitive systems. Many adversarial attacks have
been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately,
existing attacks mostly focused on feature-space and/or white-box attacks,
which make impractical assumptions in real-world scenarios, leaving the study
on practical gray/black-box attacks largely unexplored.

To bridge this gap, we conduct the first systematic study of the
gray/black-box traffic-space adversarial attacks to evaluate the robustness of
ML-based NIDSs. Our work outperforms previous ones in the following aspects:
(i) practical-the proposed attack can automatically mutate original traffic
with extremely limited knowledge and affordable overhead while preserving its
functionality; (ii) generic-the proposed attack is effective for evaluating the
robustness of various NIDSs using diverse ML/DL models and non-payload-based
features; (iii) explainable-we propose an explanation method for the fragile
robustness of ML-based NIDSs. Based on this, we also propose a defense scheme
against adversarial attacks to improve system robustness. We extensively
evaluate the robustness of various NIDSs using diverse feature sets and ML/DL
models. Experimental results show our attack is effective (e.g., >97% evasion
rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable
execution cost and the proposed defense method can effectively mitigate such
attacks (evasion rate is reduced by >50% in most cases).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongqi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Ying Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiahai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shuqiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xingang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xia Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Heavy-Tail Phenomenon in SGD. (arXiv:2006.04740v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04740</id>
        <link href="http://arxiv.org/abs/2006.04740"/>
        <updated>2021-06-09T02:01:50.053Z</updated>
        <summary type="html"><![CDATA[In recent years, various notions of capacity and complexity have been
proposed for characterizing the generalization properties of stochastic
gradient descent (SGD) in deep learning. Some of the popular notions that
correlate well with the performance on unseen data are (i) the `flatness' of
the local minimum found by SGD, which is related to the eigenvalues of the
Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which
essentially controls the magnitude of the stochastic gradient noise, and (iii)
the `tail-index', which measures the heaviness of the tails of the network
weights at convergence. In this paper, we argue that these three seemingly
unrelated perspectives for generalization are deeply linked to each other. We
claim that depending on the structure of the Hessian of the loss at the
minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD
iterates will converge to a \emph{heavy-tailed} stationary distribution. We
rigorously prove this claim in the setting of quadratic optimization: we show
that even in a simple linear regression problem with independent and
identically distributed data whose distribution has finite moments of all
order, the iterates can be heavy-tailed with infinite variance. We further
characterize the behavior of the tails with respect to algorithm parameters,
the dimension, and the curvature. We then translate our results into insights
about the behavior of SGD in deep learning. We support our theory with
experiments conducted on synthetic data, fully connected, and convolutional
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert Gurbuzbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut Simsekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingjiong Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding (Generalized) Label Smoothing whenLearning with Noisy Labels. (arXiv:2106.04149v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04149</id>
        <link href="http://arxiv.org/abs/2106.04149"/>
        <updated>2021-06-09T02:01:50.047Z</updated>
        <summary type="html"><![CDATA[Label smoothing (LS) is an arising learning paradigm that uses the positively
weighted average of both the hard training labels and uniformly distributed
soft labels. It was shown that LS serves as a regularizer for training data
with hard labels and therefore improves the generalization of the model. Later
it was reported LS even helps with improving robustness when learning with
noisy labels. However, we observe that the advantage of LS vanishes when we
operate in a high label noise regime. Puzzled by the observation, we proceeded
to discover that several proposed learning-with-noisy-labels solutions in the
literature instead relate more closely to negative label smoothing (NLS), which
defines as using a negative weight to combine the hard and soft labels! We show
that NLS functions substantially differently from LS in their achieved model
confidence. To differentiate the two cases, we will call LS the positive label
smoothing (PLS), and this paper unifies PLS and NLS into generalized label
smoothing (GLS). We provide understandings for the properties of GLS when
learning with noisy labels. Among other established properties, we
theoretically show NLS is considered more beneficial when the label noise rates
are high. We provide experimental results to support our findings too.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hangyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention. (arXiv:2106.04471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04471</id>
        <link href="http://arxiv.org/abs/2106.04471"/>
        <updated>2021-06-09T02:01:50.029Z</updated>
        <summary type="html"><![CDATA[Early prediction of cerebral palsy is essential as it leads to early
treatment and monitoring. Deep learning has shown promising results in
biomedical engineering thanks to its capacity of modelling complicated data
with its non-linear architecture. However, due to their complex structure, deep
learning models are generally not interpretable by humans, making it difficult
for clinicians to rely on the findings. In this paper, we propose a channel
attention module for deep learning models to predict cerebral palsy from
infants' body movements, which highlights the key features (i.e. body joints)
the model identifies as important, thereby indicating why certain diagnostic
results are found. To highlight the capacity of the deep network in modelling
input features, we utilize raw joint positions instead of hand-crafted
features. We validate our system with a real-world infant movement dataset. Our
proposed channel attention module enables the visualization of the vital joints
to this disease that the network considers. Our system achieves 91.67%
accuracy, suppressing other state-of-the-art deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Manli Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1"&gt;Qianhui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1"&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1"&gt;Howard Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Generalization despite Distribution Shift via Minimum Discriminating Information. (arXiv:2106.04443v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04443</id>
        <link href="http://arxiv.org/abs/2106.04443"/>
        <updated>2021-06-09T02:01:50.024Z</updated>
        <summary type="html"><![CDATA[Training models that perform well under distribution shifts is a central
challenge in machine learning. In this paper, we introduce a modeling framework
where, in addition to training data, we have partial structural knowledge of
the shifted test distribution. We employ the principle of minimum
discriminating information to embed the available prior knowledge, and use
distributionally robust optimization to account for uncertainty due to the
limited samples. By leveraging large deviation results, we obtain explicit
generalization bounds with respect to the unknown shifted distribution. Lastly,
we demonstrate the versatility of our framework by demonstrating it on two
rather distinct applications: (1) training classifiers on systematically biased
data and (2) off-policy evaluation in Markov Decision Processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Tobias Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-09T02:01:50.019Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks. (arXiv:2106.04537v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04537</id>
        <link href="http://arxiv.org/abs/2106.04537"/>
        <updated>2021-06-09T02:01:50.013Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are powerful machines for visual pattern recognition,
but reasoning tasks that are easy for humans may still be difficult for neural
models. Humans possess the ability to extrapolate reasoning strategies learned
on simple problems to solve harder examples, often by thinking for longer. For
example, a person who has learned to solve small mazes can easily extend the
very same search techniques to solve much larger mazes by spending more time.
In computers, this behavior is often achieved through the use of algorithms,
which scale to arbitrarily hard problem instances at the cost of more
computation. In contrast, the sequential computing budget of feed-forward
neural networks is limited by their depth, and networks trained on simple
problems have no way of extending their reasoning to accommodate harder
problems. In this work, we show that recurrent networks trained to solve simple
problems with few recurrent steps can indeed solve much more complex problems
simply by performing additional recurrences during inference. We demonstrate
this algorithmic behavior of recurrent networks on prefix sum computation,
mazes, and chess. In all three domains, networks trained on simple problem
instances are able to extend their reasoning abilities at test time simply by
"thinking for longer."]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1"&gt;Eitan Borgnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arjun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vishkin_U/0/1/0/all/0/1"&gt;Uzi Vishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Giving Commands to a Self-Driving Car: How to Deal with Uncertain Situations?. (arXiv:2106.04232v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04232</id>
        <link href="http://arxiv.org/abs/2106.04232"/>
        <updated>2021-06-09T02:01:50.008Z</updated>
        <summary type="html"><![CDATA[Current technology for autonomous cars primarily focuses on getting the
passenger from point A to B. Nevertheless, it has been shown that passengers
are afraid of taking a ride in self-driving cars. One way to alleviate this
problem is by allowing the passenger to give natural language commands to the
car. However, the car can misunderstand the issued command or the visual
surroundings which could lead to uncertain situations. It is desirable that the
self-driving car detects these situations and interacts with the passenger to
solve them. This paper proposes a model that detects uncertain situations when
a command is given and finds the visual objects causing it. Optionally, a
question generated by the system describing the uncertain objects is included.
We argue that if the car could explain the objects in a human-like way,
passengers could gain more confidence in the car's abilities. Thus, we
investigate how to (1) detect uncertain situations and their underlying causes,
and (2) how to generate clarifying questions for the passenger. When evaluating
on the Talk2Car dataset, we show that the proposed model, \acrfull{pipeline},
improves \gls{m:ambiguous-absolute-increase} in terms of $IoU_{.5}$ compared to
not using \gls{pipeline}. Furthermore, we designed a referring expression
generator (REG) \acrfull{reg_model} tailored to a self-driving car setting
which yields a relative improvement of \gls{m:meteor-relative} METEOR and
\gls{m:rouge-relative} ROUGE-l compared with state-of-the-art REG models, and
is three times faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deruyttere_T/0/1/0/all/0/1"&gt;Thierry Deruyttere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milewski_V/0/1/0/all/0/1"&gt;Victor Milewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What training reveals about neural network complexity. (arXiv:2106.04186v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04186</id>
        <link href="http://arxiv.org/abs/2106.04186"/>
        <updated>2021-06-09T02:01:49.992Z</updated>
        <summary type="html"><![CDATA[This work explores the hypothesis that the complexity of the function a deep
neural network (NN) is learning can be deduced by how fast its weights change
during training. Our analysis provides evidence for this supposition by
relating the network's distribution of Lipschitz constants (i.e., the norm of
the gradient at different regions of the input space) during different training
intervals with the behavior of the stochastic training procedure. We first
observe that the average Lipschitz constant close to the training data affects
various aspects of the parameter trajectory, with more complex networks having
a longer trajectory, bigger variance, and often veering further from their
initialization. We then show that NNs whose biases are trained more steadily
have bounded complexity even in regions of the input space that are far from
any training point. Finally, we find that steady training with Dropout implies
a training- and data-dependent generalization bound that grows
poly-logarithmically with the number of parameters. Overall, our results
support the hypothesis that good training behavior can be a useful bias towards
good generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poiitis_M/0/1/0/all/0/1"&gt;Marinos Poiitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04306</id>
        <link href="http://arxiv.org/abs/2106.04306"/>
        <updated>2021-06-09T02:01:49.981Z</updated>
        <summary type="html"><![CDATA[While classic control theory offers state of the art solutions in many
problem scenarios, it is often desired to improve beyond the structure of such
solutions and surpass their limitations. To this end, \emph{\gls{rpl}} offers a
formulation to improve existing controllers with reinforcement learning (RL) by
learning an additive "residual" to the output of a given controller. However,
the applicability of such an approach highly depends on the structure of the
controller. Often, internal feedback signals of the controller limit an RL
algorithm to adequately change the policy and, hence, learn the task. We
propose a new formulation that addresses these limitations by also modifying
the feedback signals to the controller with an RL policy and show superior
performance of our approach on a contact-rich peg-insertion task under position
and orientation uncertainty. In addition, we use a recent impedance control
architecture as control framework and show the difficulties of standard RPL.
Furthermore, we introduce an adaptive curriculum for the given task to
gradually increase the task difficulty in terms of position and orientation
uncertainty. A video showing the results can be found at
https://youtu.be/SAZm_Krze7U .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1"&gt;Alireza Ranjbar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1"&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1"&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Subspaces in GANs. (arXiv:2106.04488v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04488</id>
        <link href="http://arxiv.org/abs/2106.04488"/>
        <updated>2021-06-09T02:01:49.979Z</updated>
        <summary type="html"><![CDATA[The latent space of a Generative Adversarial Network (GAN) has been shown to
encode rich semantics within some subspaces. To identify these subspaces,
researchers typically analyze the statistical information from a collection of
synthesized data, and the identified subspaces tend to control image attributes
globally (i.e., manipulating an attribute causes the change of an entire
image). By contrast, this work introduces low-rank subspaces that enable more
precise control of GAN generation. Concretely, given an arbitrary image and a
region of interest (e.g., eyes of face images), we manage to relate the latent
space to the image region with the Jacobian matrix and then use low-rank
factorization to discover steerable latent subspaces. There are three
distinguishable strengths of our approach that can be aptly called LowRankGAN.
First, compared to analytic algorithms in prior work, our low-rank
factorization of Jacobians is able to find the low-dimensional representation
of attribute manifold, making image editing more precise and controllable.
Second, low-rank factorization naturally yields a null space of attributes such
that moving the latent code within it only affects the outer region of
interest. Therefore, local image editing can be simply achieved by projecting
an attribute vector into the null space without relying on a spatial mask as
existing methods do. Third, our method can robustly work with a local region
from one image for analysis yet well generalize to other images, making it much
easy to use in practice. Extensive experiments on state-of-the-art GAN models
(including StyleGAN2 and BigGAN) trained on various datasets demonstrate the
effectiveness of our LowRankGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiapeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruili Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yujun Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Deli Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staircase Attention for Recurrent Processing of Sequences. (arXiv:2106.04279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04279</id>
        <link href="http://arxiv.org/abs/2106.04279"/>
        <updated>2021-06-09T02:01:49.973Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have become a standard tool for sequence modeling tasks,
in particular by stacking self-attention layers over the entire input sequence
as in the Transformer architecture. In this work we introduce a novel attention
procedure called staircase attention that, unlike self-attention, operates
across the sequence (in time) recurrently processing the input by adding
another step of processing. A step in the staircase comprises of backward
tokens (encoding the sequence so far seen) and forward tokens (ingesting a new
part of the sequence), or an extreme Ladder version with a forward step of zero
that simply repeats the Transformer on each step of the ladder, sharing the
weights. We thus describe a family of such models that can trade off
performance and compute, by either increasing the amount of recurrence through
time, the amount of sequential processing via recurrence in depth, or both.
Staircase attention is shown to be able to solve tasks that involve tracking
that conventional Transformers cannot, due to this recurrence. Further, it is
shown to provide improved modeling power for the same size model (number of
parameters) compared to self-attentive Transformers on large language modeling
and dialogue tasks, yielding significant perplexity gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Hybrid Model for Identity Document Classification. (arXiv:2106.04345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04345</id>
        <link href="http://arxiv.org/abs/2106.04345"/>
        <updated>2021-06-09T02:01:49.968Z</updated>
        <summary type="html"><![CDATA[Digitization, i.e., the process of converting information into a digital
format, may provide various opportunities (e.g., increase in productivity,
disaster recovery, and environmentally friendly solutions) and challenges for
businesses. In this context, one of the main challenges would be to accurately
classify numerous scanned documents uploaded every day by customers as usual
business processes. For example, processes in banking (e.g., applying for
loans) or the Government Registry of BDM (Births, Deaths, and Marriages)
applications may involve uploading several documents such as a driver's license
and passport. There are not many studies available to address the challenge as
an application of image classification. Although some studies are available
which used various methods, a more accurate model is still required. The
current study has proposed a robust fusion model to define the type of identity
documents accurately. The proposed approach is based on two different methods
in which images are classified based on their visual features and text
features. A novel model based on statistics and regression has been proposed to
calculate the confidence level for the feature-based classifier. A fuzzy-mean
fusion model has been proposed to combine the classifier results based on their
confidence score. The proposed approach has been implemented using Python and
experimentally validated on synthetic and real-world datasets. The performance
of the proposed model is evaluated using the Receiver Operating Characteristic
(ROC) curve analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khandan_N/0/1/0/all/0/1"&gt;Nouna Khandan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting. (arXiv:2106.04148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04148</id>
        <link href="http://arxiv.org/abs/2106.04148"/>
        <updated>2021-06-09T02:01:49.962Z</updated>
        <summary type="html"><![CDATA[Time series forecasting is a relevant task that is performed in several
real-world scenarios such as product sales analysis and prediction of energy
demand. Given their accuracy performance, currently, Recurrent Neural Networks
(RNNs) are the models of choice for this task. Despite their success in time
series forecasting, less attention has been paid to make the RNNs trustworthy.
For example, RNNs can not naturally provide an uncertainty measure to their
predictions. This could be extremely useful in practice in several cases e.g.
to detect when a prediction might be completely wrong due to an unusual pattern
in the time series. Whittle Sum-Product Networks (WSPNs), prominent deep
tractable probabilistic circuits (PCs) for time series, can assist an RNN with
providing meaningful probabilities as uncertainty measure. With this aim, we
propose RECOWN, a novel architecture that employs RNNs and a discriminant
variant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a
Log-Likelihood Ratio Score as better estimation of uncertainty that is tailored
to time series and Whittle likelihoods. In our experiments, we show that
RECOWNs are accurate and trustworthy time series predictors, able to "know when
they do not know".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoma_N/0/1/0/all/0/1"&gt;Nils Thoma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1"&gt;Fabrizio Ventola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Robust Detection of Out-of-distribution Data (almost) for free. (arXiv:2106.04260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04260</id>
        <link href="http://arxiv.org/abs/2106.04260"/>
        <updated>2021-06-09T02:01:49.929Z</updated>
        <summary type="html"><![CDATA[When applying machine learning in safety-critical systems, a reliable
assessment of the uncertainy of a classifier is required. However, deep neural
networks are known to produce highly overconfident predictions on
out-of-distribution (OOD) data and even if trained to be non-confident on OOD
data one can still adversarially manipulate OOD data so that the classifer
again assigns high confidence to the manipulated samples. In this paper we
propose a novel method where from first principles we combine a certifiable OOD
detector with a standard classifier into an OOD aware classifier. In this way
we achieve the best of two worlds: certifiably adversarially robust OOD
detection, even for OOD samples close to the in-distribution, without loss in
prediction accuracy and close to state-of-the-art OOD detection performance for
non-manipulated OOD data. Moreover, due to the particular construction our
classifier provably avoids the asymptotic overconfidence problem of standard
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meinke_A/0/1/0/all/0/1"&gt;Alexander Meinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bitterwolf_J/0/1/0/all/0/1"&gt;Julian Bitterwolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-Start Users. (arXiv:2005.12979v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12979</id>
        <link href="http://arxiv.org/abs/2005.12979"/>
        <updated>2021-06-09T02:01:49.921Z</updated>
        <summary type="html"><![CDATA[Static recommendation methods like collaborative filtering suffer from the
inherent limitation of performing real-time personalization for cold-start
users. Online recommendation, e.g., multi-armed bandit approach, addresses this
limitation by interactively exploring user preference online and pursuing the
exploration-exploitation (EE) trade-off. However, existing bandit-based methods
model recommendation actions homogeneously. Specifically, they only consider
the items as the arms, being incapable of handling the item attributes, which
naturally provide interpretable information of user's current demands and can
effectively filter out undesired items. In this work, we consider the
conversational recommendation for cold-start users, where a system can both ask
the attributes from and recommend items to a user interactively. This important
scenario was studied in a recent work. However, it employs a hand-crafted
function to decide when to ask attributes or make recommendations. Such
separate modeling of attributes and items makes the effectiveness of the system
highly rely on the choice of the hand-crafted function, thus introducing
fragility to the system. To address this limitation, we seamlessly unify
attributes and items in the same arm space and achieve their EE trade-offs
automatically using the framework of Thompson Sampling. Our Conversational
Thompson Sampling (ConTS) model holistically solves all questions in
conversational recommendation by choosing the arm with the maximal reward to
play. Extensive experiments on three benchmark datasets show that ConTS
outperforms the state-of-the-art methods Conversational UCB (ConUCB) and
Estimation-Action-Reflection model in both metrics of success rate and average
number of conversation turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shijun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: A privacy-respecting Android malware classifier using Federated Learning. (arXiv:2007.08319v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08319</id>
        <link href="http://arxiv.org/abs/2007.08319"/>
        <updated>2021-06-09T02:01:49.913Z</updated>
        <summary type="html"><![CDATA[In this paper we present LiM ("Less is More"), a malware classification
framework that leverages Federated Learning to detect and classify malicious
apps in a privacy-respecting manner. Information about newly installed apps is
kept locally on users' devices, so that the provider cannot infer which apps
were installed by users. At the same time, input from all users is taken into
account in the federated learning process and they all benefit from better
classification performance. A key challenge of this setting is that users do
not have access to the ground truth (i.e. they cannot correctly identify
whether an app is malicious). To tackle this, LiM uses a safe semi-supervised
ensemble that maximizes classification accuracy with respect to a baseline
classifier trained by the service provider (i.e. the cloud). We implement LiM
and show that the cloud server has F1 score of 95%, while clients have perfect
recall with only 1 false positive in >100 apps, using a dataset of 25K clean
apps and 25K malicious apps, 200 users and 50 rounds of federation.
Furthermore, we conduct a security analysis and demonstrate that LiM is robust
against both poisoning attacks by adversaries who control half of the clients,
and inference attacks performed by an honest-but-curious cloud server. Further
experiments with MaMaDroid's dataset confirm resistance against poisoning
attacks and a performance improvement due to the federation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galvez_R/0/1/0/all/0/1"&gt;Rafa G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moonsamy_V/0/1/0/all/0/1"&gt;Veelasha Moonsamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_C/0/1/0/all/0/1"&gt;Claudia Diaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Isometric Gaussian Process Latent Variable Model for Dissimilarity Data. (arXiv:2006.11741v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11741</id>
        <link href="http://arxiv.org/abs/2006.11741"/>
        <updated>2021-06-09T02:01:49.907Z</updated>
        <summary type="html"><![CDATA[We present a probabilistic model where the latent variable respects both the
distances and the topology of the modeled data. The model leverages the
Riemannian geometry of the generated manifold to endow the latent space with a
well-defined stochastic distance measure, which is modeled locally as Nakagami
distributions. These stochastic distances are sought to be as similar as
possible to observed distances along a neighborhood graph through a censoring
process. The model is inferred by variational inference based on observations
of pairwise distances. We demonstrate how the new model can encode invariances
in the learned manifolds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jorgensen_M/0/1/0/all/0/1"&gt;Martin J&amp;#xf8;rgensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Multiple Shooting Layers. (arXiv:2106.03885v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03885</id>
        <link href="http://arxiv.org/abs/2106.03885"/>
        <updated>2021-06-09T02:01:49.902Z</updated>
        <summary type="html"><![CDATA[We detail a novel class of implicit neural models. Leveraging time-parallel
methods for differential equations, Multiple Shooting Layers (MSLs) seek
solutions of initial value problems via parallelizable root-finding algorithms.
MSLs broadly serve as drop-in replacements for neural ordinary differential
equations (Neural ODEs) with improved efficiency in number of function
evaluations (NFEs) and wall-clock inference time. We develop the algorithmic
framework of MSLs, analyzing the different choices of solution methods from a
theoretical and computational perspective. MSLs are showcased in long horizon
optimal control of ODEs and PDEs and as latent models for sequence generation.
Finally, we investigate the speedups obtained through application of MSL
inference in neural controlled differential equations (Neural CDEs) for time
series classification of medical data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonoda_S/0/1/0/all/0/1"&gt;Sho Sonoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suicidal Ideation and Mental Disorder Detection with Attentive Relation Networks. (arXiv:2004.07601v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07601</id>
        <link href="http://arxiv.org/abs/2004.07601"/>
        <updated>2021-06-09T02:01:49.887Z</updated>
        <summary type="html"><![CDATA[Mental health is a critical issue in modern society, and mental disorders
could sometimes turn to suicidal ideation without effective treatment. Early
detection of mental disorders and suicidal ideation from social content
provides a potential way for effective social intervention. However,
classifying suicidal ideation and other mental disorders is challenging as they
share similar patterns in language usage and sentimental polarity. This paper
enhances text representation with lexicon-based sentiment scores and latent
topics and proposes using relation networks to detect suicidal ideation and
mental disorders with related risk indicators. The relation module is further
equipped with the attention mechanism to prioritize more critical relational
features. Through experiments on three real-world datasets, our model
outperforms most of its counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaoxiong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in North Carolina. (arXiv:2106.04515v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04515</id>
        <link href="http://arxiv.org/abs/2106.04515"/>
        <updated>2021-06-09T02:01:49.882Z</updated>
        <summary type="html"><![CDATA[Coronavirus disease (COVID-19) pandemic has changed various aspects of
people's lives and behaviors. At this stage, there are no other ways to control
the natural progression of the disease than adopting mitigation strategies such
as wearing masks, watching distance, and washing hands. Moreover, at this time
of social distancing, social media plays a key role in connecting people and
providing a platform for expressing their feelings. In this study, we tap into
social media to surveil the uptake of mitigation and detection strategies, and
capture issues and concerns about the pandemic. In particular, we explore the
research question, "how much can be learned regarding the public uptake of
mitigation strategies and concerns about COVID-19 pandemic by using natural
language processing on Reddit posts?" After extracting COVID-related posts from
the four largest subreddit communities of North Carolina over six months, we
performed NLP-based preprocessing to clean the noisy data. We employed a custom
Named-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)
method for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',
and 'testing' are the most prevalent named-entities for "Personal Protective
Equipment", "symptoms", and "testing" categories, respectively. We also
observed that the most discussed topics are related to testing, masks, and
employment. The mitigation measures are the most prevalent theme of discussion
across all subreddits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitfield_C/0/1/0/all/0/1"&gt;Christopher Whitfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_M/0/1/0/all/0/1"&gt;Mohad Anwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Ranked Recall: Collision Safety Metric for Object Detection Systems in Autonomous Vehicles. (arXiv:2106.04146v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04146</id>
        <link href="http://arxiv.org/abs/2106.04146"/>
        <updated>2021-06-09T02:01:49.876Z</updated>
        <summary type="html"><![CDATA[Commonly used metrics for evaluation of object detection systems (precision,
recall, mAP) do not give complete information about their suitability of use in
safety critical tasks, like obstacle detection for collision avoidance in
Autonomous Vehicles (AV). This work introduces the Risk Ranked Recall ($R^3$)
metrics for object detection systems. The $R^3$ metrics categorize objects
within three ranks. Ranks are assigned based on an objective cyber-physical
model for the risk of collision. Recall is measured for each rank.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Ayoosh Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jayati Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verucchi_M/0/1/0/all/0/1"&gt;Micaela Verucchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caccamo_M/0/1/0/all/0/1"&gt;Marco Caccamo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1"&gt;Lui Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Riemannian Manifolds for Geodesic Motion Skills. (arXiv:2106.04315v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04315</id>
        <link href="http://arxiv.org/abs/2106.04315"/>
        <updated>2021-06-09T02:01:49.870Z</updated>
        <summary type="html"><![CDATA[For robots to work alongside humans and perform in unstructured environments,
they must learn new motion skills and adapt them to unseen situations on the
fly. This demands learning models that capture relevant motion patterns, while
offering enough flexibility to adapt the encoded skills to new requirements,
such as dynamic obstacle avoidance. We introduce a Riemannian manifold
perspective on this problem, and propose to learn a Riemannian manifold from
human demonstrations on which geodesics are natural motion skills. We realize
this with a variational autoencoder (VAE) over the space of position and
orientations of the robot end-effector. Geodesic motion skills let a robot plan
movements from and to arbitrary points on the data manifold. They also provide
a straightforward method to avoid obstacles by redefining the ambient metric in
an online fashion. Moreover, geodesics naturally exploit the manifold resulting
from multiple--mode tasks to design motions that were not explicitly
demonstrated previously. We test our learning framework using a 7-DoF robotic
manipulator, where the robot satisfactorily learns and reproduces realistic
skills featuring elaborated motion patterns, avoids previously unseen
obstacles, and generates novel movements in multiple-mode settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beik_Mohammadi_H/0/1/0/all/0/1"&gt;Hadi Beik-Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1"&gt;Georgios Arvanitidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozo_L/0/1/0/all/0/1"&gt;Leonel Rozo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Lov\'asz Embeddings for Proposal-free Panoptic Segmentation. (arXiv:2106.04555v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04555</id>
        <link href="http://arxiv.org/abs/2106.04555"/>
        <updated>2021-06-09T02:01:49.864Z</updated>
        <summary type="html"><![CDATA[Panoptic segmentation brings together two separate tasks: instance and
semantic segmentation. Although they are related, unifying them faces an
apparent paradox: how to learn simultaneously instance-specific and
category-specific (i.e. instance-agnostic) representations jointly. Hence,
state-of-the-art panoptic segmentation methods use complex models with a
distinct stream for each task. In contrast, we propose Hierarchical Lov\'asz
Embeddings, per pixel feature vectors that simultaneously encode instance- and
category-level discriminative information. We use a hierarchical Lov\'asz hinge
loss to learn a low-dimensional embedding space structured into a unified
semantic and instance hierarchy without requiring separate network branches or
object proposals. Besides modeling instances precisely in a proposal-free
manner, our Hierarchical Lov\'asz Embeddings generalize to categories by using
a simple Nearest-Class-Mean classifier, including for non-instance "stuff"
classes where instance segmentation methods are not applicable. Our simple
model achieves state-of-the-art results compared to existing proposal-free
panoptic segmentation methods on Cityscapes, COCO, and Mapillary Vistas.
Furthermore, our model demonstrates temporal stability between video frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kerola_T/0/1/0/all/0/1"&gt;Tommi Kerola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanehira_A/0/1/0/all/0/1"&gt;Atsushi Kanehira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudo_Y/0/1/0/all/0/1"&gt;Yasunori Kudo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallet_A/0/1/0/all/0/1"&gt;Alexis Vallet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Muddling Label Regularization: Deep Learning for Tabular Datasets. (arXiv:2106.04462v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04462</id>
        <link href="http://arxiv.org/abs/2106.04462"/>
        <updated>2021-06-09T02:01:49.857Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is considered the state-of-the-art in computer vision,
speech recognition and natural language processing. Until recently, it was also
widely accepted that DL is irrelevant for learning tasks on tabular data,
especially in the small sample regime where ensemble methods are acknowledged
as the gold standard. We present a new end-to-end differentiable method to
train a standard FFNN. Our method, \textbf{Muddling labels for Regularization}
(\texttt{MLR}), penalizes memorization through the generation of uninformative
labels and the application of a differentiable close-form regularization scheme
on the last hidden layer during training. \texttt{MLR} outperforms classical NN
and the gold standard (GBDT, RF) for regression and classification tasks on
several datasets from the UCI database and Kaggle covering a large range of
sample sizes and feature to sample ratios. Researchers and practitioners can
use \texttt{MLR} on its own as an off-the-shelf \DL{} solution or integrate it
into the most advanced ML pipelines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lounici_K/0/1/0/all/0/1"&gt;Karim Lounici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meziani_K/0/1/0/all/0/1"&gt;Katia Meziani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riu_B/0/1/0/all/0/1"&gt;Benjamin Riu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-09T02:01:49.842Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Based Attention Through Internal Gating. (arXiv:2106.04540v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.04540</id>
        <link href="http://arxiv.org/abs/2106.04540"/>
        <updated>2021-06-09T02:01:49.837Z</updated>
        <summary type="html"><![CDATA[Object-based attention is a key component of the visual system, relevant for
perception, learning, and memory. Neurons tuned to features of attended objects
tend to be more active than those associated with non-attended objects. There
is a rich set of models of this phenomenon in computational neuroscience.
However, there is currently a divide between models that successfully match
physiological data but can only deal with extremely simple problems and models
of attention used in computer vision. For example, attention in the brain is
known to depend on top-down processing, whereas self-attention in deep learning
does not. Here, we propose an artificial neural network model of object-based
attention that captures the way in which attention is both top-down and
recurrent. Our attention model works well both on simple test stimuli, such as
those using images of handwritten digits, and on more complex stimuli, such as
natural images drawn from the COCO dataset. We find that our model replicates a
range of findings from neuroscience, including attention-invariant tuning,
inhibition of return, and attention-mediated scaling of activity. Understanding
object based attention is both computationally interesting and a key problem
for computational neuroscience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jordan Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Benjamin_A/0/1/0/all/0/1"&gt;Ari S. Benjamin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kording_K/0/1/0/all/0/1"&gt;Konrad P. Kording&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Image Reconstruction using Deep Generative Models. (arXiv:2012.04567v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04567</id>
        <link href="http://arxiv.org/abs/2012.04567"/>
        <updated>2021-06-09T02:01:49.831Z</updated>
        <summary type="html"><![CDATA[Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method,
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We keep the weights of the generator model fixed, and
reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)
estimate over the input latent vector that generated the reconstructed image.
We further use variational inference to approximate the posterior distribution
over the latent vectors, from which we sample multiple solutions. We
demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from
the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III
and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.
Across all three datasets and without any dataset-specific hyperparameter
tuning, our simple approach yields performance competitive with current
task-specific state-of-the-art methods on super-resolution and in-painting,
while being more generalisable and without requiring any training. Our source
code and pre-trained models are available online:
https://razvanmarinescu.github.io/brgm/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Razvan V Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1"&gt;Daniel Moyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Value-network Based Approach for Multi-Driver Order Dispatching. (arXiv:2106.04493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04493</id>
        <link href="http://arxiv.org/abs/2106.04493"/>
        <updated>2021-06-09T02:01:49.825Z</updated>
        <summary type="html"><![CDATA[Recent works on ride-sharing order dispatching have highlighted the
importance of taking into account both the spatial and temporal dynamics in the
dispatching process for improving the transportation system efficiency. At the
same time, deep reinforcement learning has advanced to the point where it
achieves superhuman performance in a number of fields. In this work, we propose
a deep reinforcement learning based solution for order dispatching and we
conduct large scale online A/B tests on DiDi's ride-dispatching platform to
show that the proposed method achieves significant improvement on both total
driver income and user experience related metrics. In particular, we model the
ride dispatching problem as a Semi Markov Decision Process to account for the
temporal aspect of the dispatching actions. To improve the stability of the
value iteration with nonlinear function approximators like neural networks, we
propose Cerebellar Value Networks (CVNet) with a novel distributed state
representation layer. We further derive a regularized policy evaluation scheme
for CVNet that penalizes large Lipschitz constant of the value network for
additional robustness against adversarial perturbation and noises. Finally, we
adapt various transfer learning methods to CVNet for increased learning
adaptability and efficiency across multiple cities. We conduct extensive
offline simulations based on real dispatching data as well as online AB tests
through the DiDi's platform. Results show that CVNet consistently outperforms
other recently proposed dispatching methods. We finally show that the
performance can be further improved through the efficient use of transfer
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaodong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yintai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Markov State Abstractions for Deep Reinforcement Learning. (arXiv:2106.04379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04379</id>
        <link href="http://arxiv.org/abs/2106.04379"/>
        <updated>2021-06-09T02:01:49.808Z</updated>
        <summary type="html"><![CDATA[The fundamental assumption of reinforcement learning in Markov decision
processes (MDPs) is that the relevant decision process is, in fact, Markov.
However, when MDPs have rich observations, agents typically learn by way of an
abstract state representation, and such representations are not guaranteed to
preserve the Markov property. We introduce a novel set of conditions and prove
that they are sufficient for learning a Markov abstract state representation.
We then describe a practical training procedure that combines inverse model
estimation and temporal contrastive learning to learn an abstraction that
approximately satisfies these conditions. Our novel training objective is
compatible with both online and offline training: it does not require a reward
signal, but agents can capitalize on reward information when available. We
empirically evaluate our approach on a visual gridworld domain and a set of
continuous control benchmarks. Our approach learns representations that capture
the underlying structure of the domain and lead to improved sample efficiency
over state-of-the-art deep reinforcement learning with visual features -- often
matching or exceeding the performance achieved with hand-designed compact state
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1"&gt;Cameron Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_N/0/1/0/all/0/1"&gt;Neev Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottesman_O/0/1/0/all/0/1"&gt;Omer Gottesman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1"&gt;George Konidaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Hybrid Model for Identity Document Classification. (arXiv:2106.04345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04345</id>
        <link href="http://arxiv.org/abs/2106.04345"/>
        <updated>2021-06-09T02:01:49.802Z</updated>
        <summary type="html"><![CDATA[Digitization, i.e., the process of converting information into a digital
format, may provide various opportunities (e.g., increase in productivity,
disaster recovery, and environmentally friendly solutions) and challenges for
businesses. In this context, one of the main challenges would be to accurately
classify numerous scanned documents uploaded every day by customers as usual
business processes. For example, processes in banking (e.g., applying for
loans) or the Government Registry of BDM (Births, Deaths, and Marriages)
applications may involve uploading several documents such as a driver's license
and passport. There are not many studies available to address the challenge as
an application of image classification. Although some studies are available
which used various methods, a more accurate model is still required. The
current study has proposed a robust fusion model to define the type of identity
documents accurately. The proposed approach is based on two different methods
in which images are classified based on their visual features and text
features. A novel model based on statistics and regression has been proposed to
calculate the confidence level for the feature-based classifier. A fuzzy-mean
fusion model has been proposed to combine the classifier results based on their
confidence score. The proposed approach has been implemented using Python and
experimentally validated on synthetic and real-world datasets. The performance
of the proposed model is evaluated using the Receiver Operating Characteristic
(ROC) curve analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khandan_N/0/1/0/all/0/1"&gt;Nouna Khandan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. (arXiv:2106.04527v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04527</id>
        <link href="http://arxiv.org/abs/2106.04527"/>
        <updated>2021-06-09T02:01:49.796Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has received a lot of recent attention as it
alleviates the need for large amounts of labelled data which can often be
expensive, requires expert knowledge and be time consuming to collect. Recent
developments in deep semi-supervised classification have reached unprecedented
performance and the gap between supervised and semi-supervised learning is
ever-decreasing. This improvement in performance has been based on the
inclusion of numerous technical tricks, strong augmentation techniques and
costly optimisation schemes with multi-term loss functions. We propose a new
framework, LaplaceNet, for deep semi-supervised classification that has a
greatly reduced model complexity. We utilise a hybrid energy-neural network
where graph based pseudo-labels, generated by minimising the graphical
Laplacian, are used to iteratively improve a neural-network backbone. Our model
outperforms state-of-the-art methods for deep semi-supervised classification,
over several benchmark datasets. Furthermore, we consider the application of
strong-augmentations to neural networks theoretically and justify the use of a
multi-sampling approach for semi-supervised learning. We demonstrate, through
rigorous experimentation, that a multi-sampling augmentation approach improves
generalisation and reduces the sensitivity of the network to augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1"&gt;Philip Sellars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1"&gt;Angelica I. Aviles-Rivero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Specific Causal Discovery for Categorical Data Using Staged Trees. (arXiv:2106.04416v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.04416</id>
        <link href="http://arxiv.org/abs/2106.04416"/>
        <updated>2021-06-09T02:01:49.790Z</updated>
        <summary type="html"><![CDATA[Causal discovery algorithms aims at untangling complex causal relationships
using observational data only. Here, we introduce new causal discovery
algorithms based on staged tree models, which can represent complex and
non-symmetric causal effects. To demonstrate the efficacy of our algorithms, we
introduce a new distance, inspired by the widely used structural interventional
distance, to quantify the closeness between two staged trees in terms of their
corresponding causal inference statements. A simulation study highlights the
efficacy of staged trees in uncovering complex, asymmetric causal relationship
from data and a real-world data application illustrates their use in a
practical causal analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Leonelli_M/0/1/0/all/0/1"&gt;Manuele Leonelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Varando_G/0/1/0/all/0/1"&gt;Gherardo Varando&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of marine floating plastic using Sentinel-2 imagery and machine learning models. (arXiv:2106.03694v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03694</id>
        <link href="http://arxiv.org/abs/2106.03694"/>
        <updated>2021-06-09T02:01:49.783Z</updated>
        <summary type="html"><![CDATA[The increasing level of marine plastic pollution poses severe threats to the
marine ecosystem and biodiversity. The present study attempted to explore the
full functionality of open Sentinel satellite data and ML models for detecting
and classifying floating plastic debris in Mytilene (Greece), Limassol
(Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support
Vector Machine (SVM) and Random Forest (RF) were utilized to carry out the
classification analysis. In-situ plastic location data was collected from the
control experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the
same was considered for training the models. Both remote sensing bands and
spectral indices were used for developing the ML models. A spectral signature
profile for plastic was created for discriminating the floating plastic from
other marine debris. A newly developed index, kernel Normalized Difference
Vegetation Index (kNDVI), was incorporated into the modelling to examine its
contribution to model performances. Both SVM and RF were performed well in five
models and test case combinations. Among the two ML models, the highest
performance was measured for the RF. The inclusion of kNDVI was found effective
and increased the model performances, reflected by high balanced accuracy
measured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using
the best-performed model, an automated floating plastic detection system was
developed and tested in Calabria and Beirut. For both sites, the trained model
had detected the floating plastic with ~99% accuracy. Among the six predictors,
the FDI was found the most important variable for detecting marine floating
plastic. These findings collectively suggest that high-resolution remote
sensing imagery and the automated ML models can be an effective alternative for
the cost-effective detection of marine floating plastic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1"&gt;Srikanta Sannigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_B/0/1/0/all/0/1"&gt;Bidroha Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Arunima Sarkar Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilla_F/0/1/0/all/0/1"&gt;Francesco Pilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cheap and Good? Simple and Effective Data Augmentation for Low Resource Machine Reading. (arXiv:2106.04134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04134</id>
        <link href="http://arxiv.org/abs/2106.04134"/>
        <updated>2021-06-09T02:01:49.777Z</updated>
        <summary type="html"><![CDATA[We propose a simple and effective strategy for data augmentation for
low-resource machine reading comprehension (MRC). Our approach first pretrains
the answer extraction components of a MRC system on the augmented data that
contains approximate context of the correct answers, before training it on the
exact answer spans. The approximate context helps the QA method components in
narrowing the location of the answers. We demonstrate that our simple strategy
substantially improves both document retrieval and answer extraction
performance by providing larger context of the answers and additional training
data. In particular, our method significantly improves the performance of BERT
based retriever (15.12\%), and answer extractor (4.33\% F1) on TechQA, a
complex, low-resource MRC task. Further, our data augmentation strategy yields
significant improvements of up to 3.9\% exact match (EM) and 2.7\% F1 for
answer extraction on PolicyQA, another practical but moderate sized QA dataset
that also contains long answer spans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1"&gt;Hoang Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1"&gt;Vikas Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1"&gt;Mihai Surdeanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A critical look at the current train/test split in machine learning. (arXiv:2106.04525v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04525</id>
        <link href="http://arxiv.org/abs/2106.04525"/>
        <updated>2021-06-09T02:01:49.771Z</updated>
        <summary type="html"><![CDATA[The randomized or cross-validated split of training and testing sets has been
adopted as the gold standard of machine learning for decades. The establishment
of these split protocols are based on two assumptions: (i)-fixing the dataset
to be eternally static so we could evaluate different machine learning
algorithms or models; (ii)-there is a complete set of annotated data available
to researchers or industrial practitioners. However, in this article, we intend
to take a closer and critical look at the split protocol itself and point out
its weakness and limitation, especially for industrial applications. In many
real-world problems, we must acknowledge that there are numerous situations
where assumption (ii) does not hold. For instance, for interdisciplinary
applications like drug discovery, it often requires real lab experiments to
annotate data which poses huge costs in both time and financial considerations.
In other words, it can be very difficult or even impossible to satisfy
assumption (ii). In this article, we intend to access this problem and
reiterate the paradigm of active learning, and investigate its potential on
solving problems under unconventional train/test split protocols. We further
propose a new adaptive active learning architecture (AAL) which involves an
adaptation policy, in comparison with the traditional active learning that only
unidirectionally adds data points to the training pool. We primarily justify
our points by extensively investigating an interdisciplinary drug-protein
binding problem. We additionally evaluate AAL on more conventional machine
learning benchmarking datasets like CIFAR-10 to demonstrate the
generalizability and efficacy of the new framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jimin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jake Zhao&lt;/a&gt; (Junbo)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketch-Based Streaming Anomaly Detection in Dynamic Graphs. (arXiv:2106.04486v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.04486</id>
        <link href="http://arxiv.org/abs/2106.04486"/>
        <updated>2021-06-09T02:01:49.765Z</updated>
        <summary type="html"><![CDATA[Given a stream of graph edges from a dynamic graph, how can we assign anomaly
scores to edges and subgraphs in an online manner, for the purpose of detecting
unusual behavior, using constant time and memory? For example, in intrusion
detection, existing work seeks to detect either anomalous edges or anomalous
subgraphs, but not both. In this paper, we first extend the count-min sketch
data structure to a higher-order sketch. This higher-order sketch has the
useful property of preserving the dense subgraph structure (dense subgraphs in
the input turn into dense submatrices in the data structure). We then propose
four online algorithms that utilize this enhanced data structure, which (a)
detect both edge and graph anomalies; (b) process each edge and graph in
constant memory and constant update time per newly arriving edge, and; (c)
outperform state-of-the-art baselines on four real-world datasets. Our method
is the first streaming approach that incorporates dense subgraph search to
detect graph anomalies in constant memory and time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1"&gt;Mohit Wadhwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Robustness of Neural Networks through Fourier Stabilization. (arXiv:2106.04435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04435</id>
        <link href="http://arxiv.org/abs/2106.04435"/>
        <updated>2021-06-09T02:01:49.728Z</updated>
        <summary type="html"><![CDATA[Despite the considerable success of neural networks in security settings such
as malware detection, such models have proved vulnerable to evasion attacks, in
which attackers make slight changes to inputs (e.g., malware) to bypass
detection. We propose a novel approach, \emph{Fourier stabilization}, for
designing evasion-robust neural networks with binary inputs. This approach,
which is complementary to other forms of defense, replaces the weights of
individual neurons with robust analogs derived using Fourier analytic tools.
The choice of which neurons to stabilize in a neural network is then a
combinatorial optimization problem, and we propose several methods for
approximately solving it. We provide a formal bound on the per-neuron drop in
accuracy due to Fourier stabilization, and experimentally demonstrate the
effectiveness of the proposed approach in boosting robustness of neural
networks in several detection settings. Moreover, we show that our approach
effectively composes with adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raviv_N/0/1/0/all/0/1"&gt;Netanel Raviv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelley_A/0/1/0/all/0/1"&gt;Aidan Kelley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Michael Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1"&gt;Yevgeny Vorobeychik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Inverse Modeling Method based on Generative Adversarial Network. (arXiv:2106.04197v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04197</id>
        <link href="http://arxiv.org/abs/2106.04197"/>
        <updated>2021-06-09T02:01:49.714Z</updated>
        <summary type="html"><![CDATA[Seismic inverse modeling is a common method in reservoir prediction and it
plays a vital role in the exploration and development of oil and gas.
Conventional seismic inversion method is difficult to combine with complicated
and abstract knowledge on geological mode and its uncertainty is difficult to
be assessed. The paper proposes an inversion modeling method based on GAN
consistent with geology, well logs, seismic data. GAN is a the most promising
generation model algorithm that extracts spatial structure and abstract
features of training images. The trained GAN can reproduce the models with
specific mode. In our test, 1000 models were generated in 1 second. Based on
the trained GAN after assessment, the optimal result of models can be
calculated through Bayesian inversion frame. Results show that inversion models
conform to observation data and have a low uncertainty under the premise of
fast generation. This seismic inverse modeling method increases the efficiency
and quality of inversion iteration. It is worthy of studying and applying in
fusion of seismic data and geological knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_Y/0/1/0/all/0/1"&gt;YanShu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hou_J/0/1/0/all/0/1"&gt;JiaGen Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lixin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction. (arXiv:2106.04362v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.04362</id>
        <link href="http://arxiv.org/abs/2106.04362"/>
        <updated>2021-06-09T02:01:49.700Z</updated>
        <summary type="html"><![CDATA[How and where proteins interface with one another can ultimately impact the
proteins' functions along with a range of other biological processes. As such,
precise computational methods for protein interface prediction (PIP) come
highly sought after as they could yield significant advances in drug discovery
and design as well as protein function analysis. However, the traditional
benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a
paltry 230 complexes for training, validating, and testing different machine
learning algorithms. In this work, we expand on a dataset recently introduced
for this task, the Database of Interacting Protein Structures (DIPS), to
present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for
geometric deep learning of protein interfaces. The previous version of DIPS
contains only the Cartesian coordinates and types of the atoms comprising a
given protein complex, whereas DIPS-Plus now includes a plethora of new
residue-level features including protrusion indices, half-sphere amino acid
compositions, and new profile hidden Markov model (HMM)-based sequence features
for each amino acid, giving researchers a large, well-curated feature bank for
training protein interface prediction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Morehead_A/0/1/0/all/0/1"&gt;Alex Morehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sedova_A/0/1/0/all/0/1"&gt;Ada Sedova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jianlin Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chasing Sparsity in Vision Transformers:An End-to-End Exploration. (arXiv:2106.04533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04533</id>
        <link href="http://arxiv.org/abs/2106.04533"/>
        <updated>2021-06-09T02:01:49.693Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViTs) have recently received explosive popularity, but
their enormous model sizes and training costs remain daunting. Conventional
post-training pruning often incurs higher training budgets. In contrast, this
paper aims to trim down both the training memory overhead and the inference
complexity, without scarifying the achievable accuracy. We launch and report
the first-of-its-kind comprehensive exploration, on taking a unified approach
of integrating sparsity in ViTs "from end to end". Specifically, instead of
training full ViTs, we dynamically extract and train sparse subnetworks, while
sticking to a fixed small parameter budget. Our approach jointly optimizes
model parameters and explores connectivity throughout training, ending up with
one sparse network as the final output. The approach is seamlessly extended
from unstructured to structured sparsity, the latter by considering to guide
the prune-and-grow of self-attention heads inside ViTs. For additional
efficiency gains, we further co-explore data and architecture sparsity, by
plugging in a novel learnable token selector to adaptively determine the
currently most vital patches. Extensive results validate the effectiveness of
our proposals on ImageNet with diverse ViT backbones. For instance, at 40%
structured sparsity, our sparsified DeiT-Base can achieve 0.42% accuracy gain,
at 33.13% and 24.70% running time} savings, compared to its dense counterpart.
Perhaps most surprisingly, we find that the proposed sparse (co-)training can
even improve the ViT accuracy rather than compromising it, making sparsity a
tantalizing "free lunch". For example, our sparsified DeiT-Small at 5%, 50%
sparsity for (data, architecture), improves 0.28% top-1 accuracy and meanwhile
enjoys 49.32% FLOPs and 4.40% running time savings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation. (arXiv:2106.04151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04151</id>
        <link href="http://arxiv.org/abs/2106.04151"/>
        <updated>2021-06-09T02:01:49.686Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned
from a well-labeled source domain to an unlabeled target domain. Recently,
adversarial domain adaptation with two distinct classifiers (bi-classifier) has
been introduced into UDA which is effective to align distributions between
different domains. Previous bi-classifier adversarial learning methods only
focus on the similarity between the outputs of two distinct classifiers.
However, the similarity of the outputs cannot guarantee the accuracy of target
samples, i.e., target samples may match to wrong categories even if the
discrepancy between two classifiers is small. To challenge this issue, in this
paper, we propose a cross-domain gradient discrepancy minimization (CGDM)
method which explicitly minimizes the discrepancy of gradients generated by
source samples and target samples. Specifically, the gradient gives a cue for
the semantic information of target samples so it can be used as a good
supervision to improve the accuracy of target samples. In order to compute the
gradient signal of target samples, we further obtain target pseudo labels
through a clustering-based self-supervised learning. Extensive experiments on
three widely used UDA datasets show that our method surpasses many previous
state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hongzu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation. (arXiv:2106.04399v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04399</id>
        <link href="http://arxiv.org/abs/2106.04399"/>
        <updated>2021-06-09T02:01:49.670Z</updated>
        <summary type="html"><![CDATA[This paper is about the problem of learning a stochastic policy for
generating an object (like a molecular graph) from a sequence of actions, such
that the probability of generating an object is proportional to a given
positive reward for that object. Whereas standard return maximization tends to
converge to a single return-maximizing sequence, there are cases where we would
like to sample a diverse set of high-return solutions. These arise, for
example, in black-box function optimization when few rounds are possible, each
with large batches of queries, where the batches should be diverse, e.g., in
the design of new molecules. One can also see this as a problem of
approximately converting an energy function to a generative distribution. While
MCMC methods can achieve that, they are expensive and generally only perform
local exploration. Instead, training a generative policy amortizes the cost of
search during training and yields to fast generation. Using insights from
Temporal Difference learning, we propose GFlowNet, based on a view of the
generative process as a flow network, making it possible to handle the tricky
case where different trajectories can yield the same final state, e.g., there
are many ways to sequentially add atoms to generate some molecular graph. We
cast the set of trajectories as a flow and convert the flow consistency
equations into a learning objective, akin to the casting of the Bellman
equations into Temporal Difference methods. We prove that any global minimum of
the proposed objectives yields a policy which samples from the desired
distribution, and demonstrate the improved performance and diversity of
GFlowNet on a simple domain where there are many modes to the reward function,
and on a molecule synthesis task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Moksh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korablyov_M/0/1/0/all/0/1"&gt;Maksym Korablyov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-Assisted Analysis of Biomedical Images. (arXiv:2106.04381v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04381</id>
        <link href="http://arxiv.org/abs/2106.04381"/>
        <updated>2021-06-09T02:01:49.664Z</updated>
        <summary type="html"><![CDATA[Nowadays, the amount of heterogeneous biomedical data is increasing more and
more thanks to novel sensing techniques and high-throughput technologies. In
reference to biomedical image analysis, the advances in image acquisition
modalities and high-throughput imaging experiments are creating new challenges.
This huge information ensemble could overwhelm the analytic capabilities needed
by physicians in their daily decision-making tasks as well as by biologists
investigating complex biochemical systems. In particular, quantitative imaging
methods convey scientifically and clinically relevant information in
prediction, prognosis or treatment response assessment, by also considering
radiomics approaches. Therefore, the computational analysis of medical and
biological images plays a key role in radiology and laboratory applications. In
this regard, frameworks based on advanced Machine Learning and Computational
Intelligence can significantly improve traditional Image Processing and Pattern
Recognition approaches. However, conventional Artificial Intelligence
techniques must be tailored to address the unique challenges concerning
biomedical imaging data. This thesis aims at proposing novel and advanced
computer-assisted methods for biomedical image analysis, also as an instrument
in the development of Clinical Decision Support Systems, by always keeping in
mind the clinical feasibility of the developed solutions. In conclusion, the
ultimate goal of these research studies is to gain clinically and biologically
useful insights that can guide differential diagnosis and therapies, leading
towards biomedical data integration for personalized medicine. As a matter of
fact, the proposed computer-assisted bioimage analysis methods can be
beneficial for the definition of imaging biomarkers, as well as for
quantitative medicine and biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3KG: Contrastive Learning of 12-Lead Electrocardiograms using Physiologically-Inspired Augmentations. (arXiv:2106.04452v1 [physics.med-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04452</id>
        <link href="http://arxiv.org/abs/2106.04452"/>
        <updated>2021-06-09T02:01:49.658Z</updated>
        <summary type="html"><![CDATA[Self-supervised contrastive learning approaches leverage modality-specific
context or invariances to pretrain models using unlabeled data. While
contrastive learning has demonstrated promising on results in the image domain,
there has been limited work on determining how to exploit modality-specific
invariances in biosignals such as the electrocardiogram. In this work, we
propose 3KG, a method to generate positive pairs for contrastive learning using
physiologically-inspired 3D augmentations of the 12-lead electrocardiogram. We
evaluate representation quality by fine-tuning a linear layer for the
downstream task of 24-class diagnosis on the PhysioNet 2020 challenge training
data, and find that models trained with physiologically-inspired augmentations
both outperform and complement standard time-series augmentations. Our best
performing strategy, which incorporates spatial rotation, spatial scaling, and
time masking, achieves a performance increase of 0.16, .086, and .046 in mean
AUROC over a randomly initialized baseline at 1%, 10%, and 100% label fractions
respectively. Additionally, we show that the strength of spatial augmentations
does not significantly affect the quality of the learned representations.
Finally, we investigate the clinical relevance of how physiologically-inspired
augmentations affect the performance of our classifier on different disease
subgroupings. As expert annotations are often expensive and scarce for medical
contexts, our approach highlights the potential of machine learning to tackle
medical problems with large quantities of unlabeled biosignal data by
exploiting their unique biological properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gopal_B/0/1/0/all/0/1"&gt;Bryan Gopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Han_R/0/1/0/all/0/1"&gt;Ryan W. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Raghupathi_G/0/1/0/all/0/1"&gt;Gautham Raghupathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tison_G/0/1/0/all/0/1"&gt;Geoffrey H. Tison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating NODE with Pre-trained Neural Differential Operator for Learning Dynamics. (arXiv:2106.04166v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04166</id>
        <link href="http://arxiv.org/abs/2106.04166"/>
        <updated>2021-06-09T02:01:49.653Z</updated>
        <summary type="html"><![CDATA[Learning dynamics governed by differential equations is crucial for
predicting and controlling the systems in science and engineering. Neural
Ordinary Differential Equation (NODE), a deep learning model integrated with
differential equations, learns the dynamics directly from the samples on the
trajectory and shows great promise in the scientific field. However, the
training of NODE highly depends on the numerical solver, which can amplify
numerical noise and be unstable, especially for ill-conditioned dynamical
systems. In this paper, to reduce the reliance on the numerical solver, we
propose to enhance the supervised signal in learning dynamics. Specifically,
beyond learning directly from the trajectory samples, we pre-train a neural
differential operator (NDO) to output an estimation of the derivatives to serve
as an additional supervised signal. The NDO is pre-trained on a class of
symbolic functions, and it learns the mapping between the trajectory samples of
these functions to their derivatives. We provide theoretical guarantee on that
the output of NDO can well approximate the ground truth derivatives by proper
tuning the complexity of the library. To leverage both the trajectory signal
and the estimated derivatives from NDO, we propose an algorithm called
NDO-NODE, in which the loss function contains two terms: the fitness on the
true trajectory samples and the fitness on the estimated derivatives that are
output by the pre-trained NDO. Experiments on various of dynamics show that our
proposed NDO-NODE can consistently improve the forecasting accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shiqi Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lijun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhi-Ming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-Assisted Analysis of Biomedical Images. (arXiv:2106.04381v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04381</id>
        <link href="http://arxiv.org/abs/2106.04381"/>
        <updated>2021-06-09T02:01:49.645Z</updated>
        <summary type="html"><![CDATA[Nowadays, the amount of heterogeneous biomedical data is increasing more and
more thanks to novel sensing techniques and high-throughput technologies. In
reference to biomedical image analysis, the advances in image acquisition
modalities and high-throughput imaging experiments are creating new challenges.
This huge information ensemble could overwhelm the analytic capabilities needed
by physicians in their daily decision-making tasks as well as by biologists
investigating complex biochemical systems. In particular, quantitative imaging
methods convey scientifically and clinically relevant information in
prediction, prognosis or treatment response assessment, by also considering
radiomics approaches. Therefore, the computational analysis of medical and
biological images plays a key role in radiology and laboratory applications. In
this regard, frameworks based on advanced Machine Learning and Computational
Intelligence can significantly improve traditional Image Processing and Pattern
Recognition approaches. However, conventional Artificial Intelligence
techniques must be tailored to address the unique challenges concerning
biomedical imaging data. This thesis aims at proposing novel and advanced
computer-assisted methods for biomedical image analysis, also as an instrument
in the development of Clinical Decision Support Systems, by always keeping in
mind the clinical feasibility of the developed solutions. In conclusion, the
ultimate goal of these research studies is to gain clinically and biologically
useful insights that can guide differential diagnosis and therapies, leading
towards biomedical data integration for personalized medicine. As a matter of
fact, the proposed computer-assisted bioimage analysis methods can be
beneficial for the definition of imaging biomarkers, as well as for
quantitative medicine and biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Forest classifier for EEG-based seizure prediction. (arXiv:2106.04510v1 [physics.med-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04510</id>
        <link href="http://arxiv.org/abs/2106.04510"/>
        <updated>2021-06-09T02:01:49.640Z</updated>
        <summary type="html"><![CDATA[Epileptic seizure prediction has gained considerable interest in the
computational Epilepsy research community. This paper presents a Machine
Learning based method for epileptic seizure prediction which outperforms
state-of-the art methods. We compute a probability for a given epoch, of being
pre-ictal against interictal using the Random Forest classifier and introduce
new concepts to enhance the robustness of the algorithm to false alarms. We
assessed our method on 20 patients of the benchmark scalp EEG CHB-MIT dataset
for a seizure prediction horizon (SPH) of 5 minutes and a seizure occurrence
period (SOP) of 30 minutes. Our approach achieves a sensitivity of 82.07 % and
a low false positive rate (FPR) of 0.0799 /h. We also tested our approach on
intracranial EEG recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Messaoud_R/0/1/0/all/0/1"&gt;Remy Ben Messaoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chavez_M/0/1/0/all/0/1"&gt;Mario Chavez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient training for future video generation based on hierarchical disentangled representation of latent variables. (arXiv:2106.03502v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03502</id>
        <link href="http://arxiv.org/abs/2106.03502"/>
        <updated>2021-06-09T02:01:49.621Z</updated>
        <summary type="html"><![CDATA[Generating videos predicting the future of a given sequence has been an area
of active research in recent years. However, an essential problem remains
unsolved: most of the methods require large computational cost and memory usage
for training. In this paper, we propose a novel method for generating future
prediction videos with less memory usage than the conventional methods. This is
a critical stepping stone in the path towards generating videos with high image
quality, similar to that of generated images in the latest works in the field
of image generation. We achieve high-efficiency by training our method in two
stages: (1) image reconstruction to encode video frames into latent variables,
and (2) latent variable prediction to generate the future sequence. Our method
achieves an efficient compression of video into low-dimensional latent
variables by decomposing each frame according to its hierarchical structure.
That is, we consider that video can be separated into background and foreground
objects, and that each object holds time-varying and time-independent
information independently. Our experiments show that the proposed method can
efficiently generate future prediction videos, even for complex datasets that
cannot be handled by previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fushishita_N/0/1/0/all/0/1"&gt;Naoya Fushishita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1"&gt;Antonio Tejero-de-Pablos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1"&gt;Yusuke Mukuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymmetrical Bi-RNN for pedestrian trajectory encoding. (arXiv:2106.04419v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04419</id>
        <link href="http://arxiv.org/abs/2106.04419"/>
        <updated>2021-06-09T02:01:49.615Z</updated>
        <summary type="html"><![CDATA[Pedestrian motion behavior involves a combination of individual goals and
social interactions with other agents. In this article, we present a
non-symmetrical bidirectional recurrent neural network architecture called
U-RNN as a sequence encoder and evaluate its relevance to replace LSTMs for
various forecasting models. Experimental results on the Trajnet++ benchmark
show that the U-LSTM variant can yield better results regarding every available
metric (ADE, FDE, Collision rate) than common LSTMs sequence encoders for a
variety of approaches and interaction modules.

Our implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is
available at:
github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozenberg_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Rozenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1"&gt;Joseph Gesnouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1"&gt;Fabien Moutarde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing Unrecognizable Faces for Face Recognition. (arXiv:2106.04112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04112</id>
        <link href="http://arxiv.org/abs/2106.04112"/>
        <updated>2021-06-09T02:01:49.596Z</updated>
        <summary type="html"><![CDATA[The common implementation of face recognition systems as a cascade of a
detection stage and a recognition or verification stage can cause problems
beyond failures of the detector. When the detector succeeds, it can detect
faces that cannot be recognized, no matter how capable the recognition system.
Recognizability, a latent variable, should therefore be factored into the
design and implementation of face recognition systems. We propose a measure of
recognizability of a face image that leverages a key empirical observation: an
embedding of face images, implemented by a deep neural network trained using
mostly recognizable identities, induces a partition of the hypersphere whereby
unrecognizable identities cluster together. This occurs regardless of the
phenomenon that causes a face to be unrecognizable, it be optical or motion
blur, partial occlusion, spatial quantization, poor illumination. Therefore, we
use the distance from such an "unrecognizable identity" as a measure of
recognizability, and incorporate it in the design of the over-all system. We
show that accounting for recognizability reduces error rate of single-image
face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification
benchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based
recognition on the IJB-C benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Siqi Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closed-Form Analytical Results for Maximum Entropy Reinforcement Learning. (arXiv:2106.03931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03931</id>
        <link href="http://arxiv.org/abs/2106.03931"/>
        <updated>2021-06-09T02:01:49.590Z</updated>
        <summary type="html"><![CDATA[We introduce a mapping between Maximum Entropy Reinforcement Learning (MaxEnt
RL) and Markovian processes conditioned on rare events. In the long time limit,
this mapping allows us to derive analytical expressions for the optimal policy,
dynamics and initial state distributions for the general case of stochastic
dynamics in MaxEnt RL. We find that soft-$\mathcal{Q}$ functions in MaxEnt RL
can be obtained from the Perron-Frobenius eigenvalue and the corresponding left
eigenvector of a regular, non-negative matrix derived from the underlying
Markov Decision Process (MDP). The results derived lead to novel algorithms for
model-based and model-free MaxEnt RL, which we validate by numerical
simulations. The mapping established in this work opens further avenues for the
application of novel analytical and computational approaches to problems in
MaxEnt RL. We make our code available at:
https://github.com/argearriojas/maxent-rl-mdp-scripts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arriojas_A/0/1/0/all/0/1"&gt;Argenis Arriojas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1"&gt;Stas Tiomkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1"&gt;Rahul V. Kulkarni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Molecular Deep Generative Models with Topological Data Analysis Representations. (arXiv:2106.04464v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04464</id>
        <link href="http://arxiv.org/abs/2106.04464"/>
        <updated>2021-06-09T02:01:49.584Z</updated>
        <summary type="html"><![CDATA[Deep generative models have emerged as a powerful tool for learning
informative molecular representations and designing novel molecules with
desired properties, with applications in drug discovery and material design.
Deep generative auto-encoders defined over molecular SMILES strings have been a
popular choice for that purpose. However, capturing salient molecular
properties like quantum-chemical energies remains challenging and requires
sophisticated neural net models of molecular graphs or geometry-based
information. As a simpler and more efficient alternative, we present a SMILES
Variational Auto-Encoder (VAE) augmented with topological data analysis (TDA)
representations of molecules, known as persistence images. Our experiments show
that this TDA augmentation enables a SMILES VAE to capture the complex relation
between 3D geometry and electronic properties, and allows generation of novel,
diverse, and valid molecules with geometric features consistent with the
training data, which exhibit a varying range of global electronic structural
properties, such as a small HOMO-LUMO gap - a critical property for designing
organic solar cells. We demonstrate that our TDA augmentation yields better
success in downstream tasks compared to models trained without these
representations and can assist in targeted molecule discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoffman_S/0/1/0/all/0/1"&gt;Samuel Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Tree Search Configuration: Cutting Planes and Beyond. (arXiv:2106.04033v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04033</id>
        <link href="http://arxiv.org/abs/2106.04033"/>
        <updated>2021-06-09T02:01:49.568Z</updated>
        <summary type="html"><![CDATA[Cutting-plane methods have enabled remarkable successes in integer
programming over the last few decades. State-of-the-art solvers integrate a
myriad of cutting-plane techniques to speed up the underlying tree-search
algorithm used to find optimal solutions. In this paper we prove the first
guarantees for learning high-performing cut-selection policies tailored to the
instance distribution at hand using samples. We first bound the sample
complexity of learning cutting planes from the canonical family of
Chv\'atal-Gomory cuts. Our bounds handle any number of waves of any number of
cuts and are fine tuned to the magnitudes of the constraint coefficients. Next,
we prove sample complexity bounds for more sophisticated cut selection policies
that use a combination of scoring rules to choose from a family of cuts.
Finally, beyond the realm of cutting planes for integer programming, we develop
a general abstraction of tree search that captures key components such as node
selection and variable selection. For this abstraction, we bound the sample
complexity of learning a good policy for building the search tree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1"&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1"&gt;Siddharth Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1"&gt;Tuomas Sandholm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitercik_E/0/1/0/all/0/1"&gt;Ellen Vitercik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discover the Unknown Biased Attribute of an Image Classifier. (arXiv:2104.14556v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14556</id>
        <link href="http://arxiv.org/abs/2104.14556"/>
        <updated>2021-06-09T02:01:49.563Z</updated>
        <summary type="html"><![CDATA[Recent works find that AI algorithms learn biases from data. Therefore, it is
urgent and vital to identify biases in AI algorithms. However, the previous
bias identification pipeline overly relies on human experts to conjecture
potential biases (e.g., gender), which may neglect other underlying biases not
realized by humans. To help human experts better find the AI algorithms'
biases, we study a new problem in this work -- for a classifier that predicts a
target attribute of the input image, discover its unknown biased attribute.

To solve this challenging problem, we use a hyperplane in the generative
model's latent space to represent an image attribute; thus, the original
problem is transformed to optimizing the hyperplane's normal vector and offset.
We propose a novel total-variation loss within this framework as the objective
function and a new orthogonalization penalty as a constraint. The latter
prevents trivial solutions in which the discovered biased attribute is
identical with the target or one of the known-biased attributes. Extensive
experiments on both disentanglement datasets and real-world datasets show that
our method can discover biased attributes and achieve better disentanglement
w.r.t. target attributes. Furthermore, the qualitative results show that our
method can discover unnoticeable biased attributes for various object and scene
classifiers, proving our method's generalizability for detecting biased
attributes in diverse domains of images. The code is available at
https://git.io/J3kMh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Randomness of Input Data Spaces is an A Priori Predictor for Generalization. (arXiv:2106.04181v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04181</id>
        <link href="http://arxiv.org/abs/2106.04181"/>
        <updated>2021-06-09T02:01:49.558Z</updated>
        <summary type="html"><![CDATA[Over-parameterized models can perfectly learn various types of data
distributions, however, generalization error is usually lower for real data in
comparison to artificial data. This suggests that the properties of data
distributions have an impact on generalization capability. This work focuses on
the search space defined by the input data and assumes that the correlation
between labels of neighboring input values influences generalization. If
correlation is low, the randomness of the input data space is high leading to
high generalization error. We suggest to measure the randomness of an input
data space using Maurer's universal. Results for synthetic classification tasks
and common image classification benchmarks (MNIST, CIFAR10, and Microsoft's
cats vs. dogs data set) find a high correlation between the randomness of input
data spaces and the generalization error of deep neural networks for binary
classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Briesch_M/0/1/0/all/0/1"&gt;Martin Briesch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sobania_D/0/1/0/all/0/1"&gt;Dominik Sobania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothlauf_F/0/1/0/all/0/1"&gt;Franz Rothlauf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced Few-Shot Acquisition Function Learning for Bayesian Optimization. (arXiv:2106.04335v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04335</id>
        <link href="http://arxiv.org/abs/2106.04335"/>
        <updated>2021-06-09T02:01:49.552Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) conventionally relies on handcrafted acquisition
functions (AFs) to sequentially determine the sample points. However, it has
been widely observed in practice that the best-performing AF in terms of regret
can vary significantly under different types of black-box functions. It has
remained a challenge to design one AF that can attain the best performance over
a wide variety of black-box functions. This paper aims to attack this challenge
through the perspective of reinforced few-shot AF learning (FSAF).
Specifically, we first connect the notion of AFs with Q-functions and view a
deep Q-network (DQN) as a surrogate differentiable AF. While it serves as a
natural idea to combine DQN and an existing few-shot learning method, we
identify that such a direct combination does not perform well due to severe
overfitting, which is particularly critical in BO due to the need of a
versatile sampling policy. To address this, we present a Bayesian variant of
DQN with the following three features: (i) It learns a distribution of
Q-networks as AFs based on the Kullback-Leibler regularization framework. This
inherently provides the uncertainty required in sampling for BO and mitigates
overfitting. (ii) For the prior of the Bayesian DQN, we propose to use a demo
policy induced by an off-the-shelf AF for better training stability. (iii) On
the meta-level, we leverage the meta-loss of Bayesian model-agnostic
meta-learning, which serves as a natural companion to the proposed FSAF.
Moreover, with the proper design of the Q-networks, FSAF is general-purpose in
that it is agnostic to the dimension and the cardinality of the input domain.
Through extensive experiments, we demonstrate that the FSAF achieves comparable
or better regrets than the state-of-the-art benchmarks on a wide variety of
synthetic and real-world test functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_B/0/1/0/all/0/1"&gt;Bing-Jing Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_P/0/1/0/all/0/1"&gt;Ping-Chun Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Fast Kernel Transform. (arXiv:2106.04487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04487</id>
        <link href="http://arxiv.org/abs/2106.04487"/>
        <updated>2021-06-09T02:01:49.541Z</updated>
        <summary type="html"><![CDATA[Kernel methods are a highly effective and widely used collection of modern
machine learning algorithms. A fundamental limitation of virtually all such
methods are computations involving the kernel matrix that naively scale
quadratically (e.g., constructing the kernel matrix and matrix-vector
multiplication) or cubically (solving linear systems) with the size of the data
set $N.$ We propose the Fast Kernel Transform (FKT), a general algorithm to
compute matrix-vector multiplications (MVMs) for datasets in moderate
dimensions with quasilinear complexity. Typically, analytically grounded fast
multiplication methods require specialized development for specific kernels. In
contrast, our scheme is based on auto-differentiation and automated symbolic
computations that leverage the analytical structure of the underlying kernel.
This allows the FKT to be easily applied to a broad class of kernels, including
Gaussian, Matern, and Rational Quadratic covariance functions and physically
motivated Green's functions, including those of the Laplace and Helmholtz
equations. Furthermore, the FKT maintains a high, quantifiable, and
controllable level of accuracy -- properties that many acceleration methods
lack. We illustrate the efficacy and versatility of the FKT by providing timing
and accuracy benchmarks and by applying it to scale the stochastic neighborhood
embedding (t-SNE) and Gaussian processes to large real-world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryan_J/0/1/0/all/0/1"&gt;John Paul Ryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ament_S/0/1/0/all/0/1"&gt;Sebastian Ament&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damle_A/0/1/0/all/0/1"&gt;Anil Damle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Neural Collaborative Filtering. (arXiv:2106.04405v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04405</id>
        <link href="http://arxiv.org/abs/2106.04405"/>
        <updated>2021-06-09T02:01:49.524Z</updated>
        <summary type="html"><![CDATA[In this work, we present a federated version of the state-of-the-art Neural
Collaborative Filtering (NCF) approach for item recommendations. The system,
named FedNCF, allows learning without requiring users to expose or transmit
their raw data. Experimental validation shows that FedNCF achieves comparable
recommendation quality to the original NCF system. Although federated learning
(FL) enables learning without raw data transmission, recent attacks showed that
FL alone does not eliminate privacy concerns. To overcome this challenge, we
integrate a privacy-preserving enhancement with a secure aggregation scheme
that satisfies the security requirements against an honest-but-curious (HBC)
entity, without affecting the quality of the original model. Finally, we
discuss the peculiarities observed in the application of FL in a collaborative
filtering (CF) task as well as we evaluate the privacy-preserving mechanism in
terms of computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perifanis_V/0/1/0/all/0/1"&gt;Vasileios Perifanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efraimidis_P/0/1/0/all/0/1"&gt;Pavlos S. Efraimidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbalanced Optimal Transport through Non-negative Penalized Linear Regression. (arXiv:2106.04145v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04145</id>
        <link href="http://arxiv.org/abs/2106.04145"/>
        <updated>2021-06-09T02:01:49.519Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of Unbalanced Optimal Transport (UOT) in
which the marginal conditions are relaxed (using weighted penalties in lieu of
equality) and no additional regularization is enforced on the OT plan. In this
context, we show that the corresponding optimization problem can be
reformulated as a non-negative penalized linear regression problem. This
reformulation allows us to propose novel algorithms inspired from inverse
problems and nonnegative matrix factorization. In particular, we consider
majorization-minimization which leads in our setting to efficient
multiplicative updates for a variety of penalties. Furthermore, we derive for
the first time an efficient algorithm to compute the regularization path of UOT
with quadratic penalties. The proposed algorithm provides a continuity of
piece-wise linear OT plans converging to the solution of balanced OT
(corresponding to infinite penalty weights). We perform several numerical
experiments on simulated and real data illustrating the new algorithms, and
provide a detailed discussion about more sophisticated optimization tools that
can further be used to solve OT problems thanks to our reformulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chapel_L/0/1/0/all/0/1"&gt;Laetitia Chapel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haoran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for Reinforcement Learning. (arXiv:2106.04152v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04152</id>
        <link href="http://arxiv.org/abs/2106.04152"/>
        <updated>2021-06-09T02:01:49.513Z</updated>
        <summary type="html"><![CDATA[Learning good feature representations is important for deep reinforcement
learning (RL). However, with limited experience, RL often suffers from data
inefficiency for training. For un-experienced or less-experienced trajectories
(i.e., state-action sequences), the lack of data limits the use of them for
better feature learning. In this work, we propose a novel method, dubbed
PlayVirtual, which augments cycle-consistent virtual trajectories to enhance
the data efficiency for RL feature representation learning. Specifically,
PlayVirtual predicts future states based on the current state and action by a
dynamics model and then predicts the previous states by a backward dynamics
model, which forms a trajectory cycle. Based on this, we augment the actions to
generate a large amount of virtual state-action trajectories. Being free of
groudtruth state supervision, we enforce a trajectory to meet the cycle
consistency constraint, which can significantly enhance the data efficiency. We
validate the effectiveness of our designs on the Atari and DeepMind Control
Suite benchmarks. Our method outperforms the current state-of-the-art methods
by a large margin on both benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mingxiao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Machine Unlearning. (arXiv:2106.04378v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04378</id>
        <link href="http://arxiv.org/abs/2106.04378"/>
        <updated>2021-06-09T02:01:49.462Z</updated>
        <summary type="html"><![CDATA[Data deletion algorithms aim to remove the influence of deleted data points
from trained models at a cheaper computational cost than fully retraining those
models. However, for sequences of deletions, most prior work in the non-convex
setting gives valid guarantees only for sequences that are chosen independently
of the models that are published. If people choose to delete their data as a
function of the published models (because they don't like what the models
reveal about them, for example), then the update sequence is adaptive. In this
paper, we give a general reduction from deletion guarantees against adaptive
sequences to deletion guarantees against non-adaptive sequences, using
differential privacy and its connection to max information. Combined with ideas
from prior work which give guarantees for non-adaptive deletion sequences, this
leads to extremely flexible algorithms able to handle arbitrary model classes
and training methodologies, giving strong provable deletion guarantees for
adaptive deletion sequences. We show in theory how prior work for non-convex
models fails against adaptive deletion sequences, and use this intuition to
design a practical attack against the SISA algorithm of Bourtoule et al. [2021]
on CIFAR-10, MNIST, Fashion-MNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Varun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1"&gt;Christopher Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1"&gt;Seth Neel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifi_Malvajerdi_S/0/1/0/all/0/1"&gt;Saeed Sharifi-Malvajerdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waites_C/0/1/0/all/0/1"&gt;Chris Waites&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03907</id>
        <link href="http://arxiv.org/abs/2106.03907"/>
        <updated>2021-06-09T02:01:49.277Z</updated>
        <summary type="html"><![CDATA[Proxy causal learning (PCL) is a method for estimating the causal effect of
treatments on outcomes in the presence of unobserved confounding, using proxies
(structured side information) for the confounder. This is achieved via
two-stage regression: in the first stage, we model relations among the
treatment and proxies; in the second stage, we use this model to learn the
effect of treatment on the outcome, given the context provided by the proxies.
PCL guarantees recovery of the true causal effect, subject to identifiability
conditions. We propose a novel method for PCL, the deep feature proxy variable
method (DFPV), to address the case where the proxies, treatments, and outcomes
are high-dimensional and have nonlinear complex relationships, as represented
by deep neural network features. We show that DFPV outperforms recent
state-of-the-art PCL methods on challenging synthetic benchmarks, including
settings involving high dimensional image data. Furthermore, we show that PCL
can be applied to off-policy evaluation for the confounded bandit problem, in
which DFPV also exhibits competitive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanagawa_H/0/1/0/all/0/1"&gt;Heishiro Kanagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralFusion: Online Depth Fusion in Latent Space. (arXiv:2011.14791v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14791</id>
        <link href="http://arxiv.org/abs/2011.14791"/>
        <updated>2021-06-09T02:01:49.264Z</updated>
        <summary type="html"><![CDATA[We present a novel online depth map fusion approach that learns depth map
aggregation in a latent feature space. While previous fusion methods use an
explicit scene representation like signed distance functions (SDFs), we propose
a learned feature representation for the fusion. The key idea is a separation
between the scene representation used for the fusion and the output scene
representation, via an additional translator network. Our neural network
architecture consists of two main parts: a depth and feature fusion
sub-network, which is followed by a translator sub-network to produce the final
surface representation (e.g. TSDF) for visualization or other tasks. Our
approach is an online process, handles high noise levels, and is particularly
able to deal with gross outliers common for photometric stereo-based depth
maps. Experiments on real and synthetic data demonstrate improved results
compared to the state of the art, especially in challenging scenarios with
large amounts of noise and outliers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weder_S/0/1/0/all/0/1"&gt;Silvan Weder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonberger_J/0/1/0/all/0/1"&gt;Johannes L. Sch&amp;#xf6;nberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1"&gt;Martin R. Oswald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Distillation: A Self-Supervised Learning Framework for Optical Flow Estimation. (arXiv:2106.04195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04195</id>
        <link href="http://arxiv.org/abs/2106.04195"/>
        <updated>2021-06-09T02:01:49.262Z</updated>
        <summary type="html"><![CDATA[We present DistillFlow, a knowledge distillation approach to learning optical
flow. DistillFlow trains multiple teacher models and a student model, where
challenging transformations are applied to the input of the student model to
generate hallucinated occlusions as well as less confident predictions. Then, a
self-supervised learning framework is constructed: confident predictions from
teacher models are served as annotations to guide the student model to learn
optical flow for those less confident predictions. The self-supervised learning
framework enables us to effectively learn optical flow from unlabeled data, not
only for non-occluded pixels, but also for occluded pixels. DistillFlow
achieves state-of-the-art unsupervised learning performance on both KITTI and
Sintel datasets. Our self-supervised pre-trained model also provides an
excellent initialization for supervised fine-tuning, suggesting an alternate
training paradigm in contrast to current supervised learning methods that
highly rely on pre-training on synthetic data. At the time of writing, our
fine-tuned models ranked 1st among all monocular methods on the KITTI 2015
benchmark, and outperform all published methods on the Sintel Final benchmark.
More importantly, we demonstrate the generalization capability of DistillFlow
in three aspects: framework generalization, correspondence generalization and
cross-dataset generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengpeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael R. Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling. (arXiv:2104.05778v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05778</id>
        <link href="http://arxiv.org/abs/2104.05778"/>
        <updated>2021-06-09T02:01:49.261Z</updated>
        <summary type="html"><![CDATA[This paper explores an efficient solution for Space-time Super-Resolution,
aiming to generate High-resolution Slow-motion videos from Low Resolution and
Low Frame rate videos. A simplistic solution is the sequential running of Video
Super Resolution and Video Frame interpolation models. However, this type of
solutions are memory inefficient, have high inference time, and could not make
the proper use of space-time relation property. To this extent, we first
interpolate in LR space using quadratic modeling. Input LR frames are
super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps
and blending mask which are used to synthesize LR interpolated frame is reused
in HR space using bilinear upsampling. This leads to a coarse estimate of HR
intermediate frame which often contains artifacts along motion boundaries. We
use a refinement network to improve the quality of HR intermediate frame via
residual learning. Our model is lightweight and performs better than current
state-of-the-art models in REDS STSR Validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Saikat Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nisarg A. Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anurag Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPANet: Generalized Permutationless Set Assignment for Particle Physics using Symmetry Preserving Attention. (arXiv:2106.03898v1 [hep-ex])]]></title>
        <id>http://arxiv.org/abs/2106.03898</id>
        <link href="http://arxiv.org/abs/2106.03898"/>
        <updated>2021-06-09T02:01:49.246Z</updated>
        <summary type="html"><![CDATA[The creation of unstable heavy particles at the Large Hadron Collider is the
most direct way to address some of the deepest open questions in physics.
Collisions typically produce variable-size sets of observed particles which
have inherent ambiguities complicating the assignment of observed particles to
the decay products of the heavy particles. Current strategies for tackling
these challenges in the physics community ignore the physical symmetries of the
decay products and consider all possible assignment permutations and do not
scale to complex configurations. Attention based deep learning methods for
sequence modelling have achieved state-of-the-art performance in natural
language processing, but they lack built-in mechanisms to deal with the unique
symmetries found in physical set-assignment problems. We introduce a novel
method for constructing symmetry-preserving attention networks which reflect
the problem's natural invariances to efficiently find assignments without
evaluating all permutations. This general approach is applicable to arbitrarily
complex configurations and significantly outperforms current methods, improving
reconstruction efficiency between 19\% - 35\% on typical benchmark problems
while decreasing inference time by two to five orders of magnitude on the most
complex events, making many important and previously intractable cases
tractable.

A full code repository containing a general library, the specific
configuration used, and a complete dataset release, are avaiable at
https://github.com/Alexanders101/SPANet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ex/1/au:+Shmakov_A/0/1/0/all/0/1"&gt;Alexander Shmakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Fenton_M/0/1/0/all/0/1"&gt;Michael James Fenton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ho_T/0/1/0/all/0/1"&gt;Ta-Wei Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Hsu_S/0/1/0/all/0/1"&gt;Shih-Chieh Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Whiteson_D/0/1/0/all/0/1"&gt;Daniel Whiteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On $w$-mixtures: Finite convex combinations of prescribed component distributions. (arXiv:1708.00568v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1708.00568</id>
        <link href="http://arxiv.org/abs/1708.00568"/>
        <updated>2021-06-09T02:01:49.225Z</updated>
        <summary type="html"><![CDATA[We consider the space of $w$-mixtures which is defined as the set of finite
statistical mixtures sharing the same prescribed component distributions closed
under convex combinations. The information geometry induced by the Bregman
generator set to the Shannon negentropy on this space yields a dually flat
space called the mixture family manifold. We show how the Kullback-Leibler (KL)
divergence can be recovered from the corresponding Bregman divergence for the
negentropy generator: That is, the KL divergence between two $w$-mixtures
amounts to a Bregman Divergence (BD) induced by the Shannon negentropy
generator. Thus the KL divergence between two Gaussian Mixture Models (GMMs)
sharing the same Gaussian components is equivalent to a Bregman divergence.
This KL-BD equivalence on a mixture family manifold implies that we can perform
optimal KL-averaging aggregation of $w$-mixtures without information loss. More
generally, we prove that the statistical skew Jensen-Shannon divergence between
$w$-mixtures is equivalent to a skew Jensen divergence between their
corresponding parameters. Finally, we state several properties, divergence
identities, and inequalities relating to $w$-mixtures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1"&gt;Richard Nock&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occode: an end-to-end machine learning pipeline for transcription of historical population censuses. (arXiv:2106.03996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03996</id>
        <link href="http://arxiv.org/abs/2106.03996"/>
        <updated>2021-06-09T02:01:49.197Z</updated>
        <summary type="html"><![CDATA[Machine learning approaches achieve high accuracy for text recognition and
are therefore increasingly used for the transcription of handwritten historical
sources. However, using machine learning in production requires a streamlined
end-to-end machine learning pipeline that scales to the dataset size, and a
model that achieves high accuracy with few manual transcriptions. In addition,
the correctness of the model results must be verified. This paper describes our
lessons learned developing, tuning, and using the Occode end-to-end machine
learning pipeline for transcribing 7,3 million rows with handwritten occupation
codes in the Norwegian 1950 population census. We achieve an accuracy of 97%
for the automatically transcribed codes, and we send 3% of the codes for manual
verification. We verify that the occupation code distribution found in our
result matches the distribution found in our training data which should be
representative for the census as a whole. We believe our approach and lessons
learned are useful for other transcription projects that plan to use machine
learning in production. The source code is available at:
https://github.com/uit-hdl/rhd-codes]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pedersen_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rn-Richard Pedersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holsbo_E/0/1/0/all/0/1"&gt;Einar Holsb&amp;#xf8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_T/0/1/0/all/0/1"&gt;Trygve Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shvetsov_N/0/1/0/all/0/1"&gt;Nikita Shvetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravn_J/0/1/0/all/0/1"&gt;Johan Ravn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommerseth_H/0/1/0/all/0/1"&gt;Hilde Leikny Sommerseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bongo_L/0/1/0/all/0/1"&gt;Lars Ailo Bongo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speedy Performance Estimation for Neural Architecture Search. (arXiv:2006.04492v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04492</id>
        <link href="http://arxiv.org/abs/2006.04492"/>
        <updated>2021-06-09T02:01:49.141Z</updated>
        <summary type="html"><![CDATA[Reliable yet efficient evaluation of generalisation performance of a proposed
architecture is crucial to the success of neural architecture search (NAS).
Traditional approaches face a variety of limitations: training each
architecture to completion is prohibitively expensive, early stopped validation
accuracy may correlate poorly with fully trained performance, and model-based
estimators require large training sets. We instead propose to estimate the
final test performance based on a simple measure of training speed. Our
estimator is theoretically motivated by the connection between generalisation
and training speed, and is also inspired by the reformulation of a PAC-Bayes
bound under the Bayesian setting. Our model-free estimator is simple,
efficient, and cheap to implement, and does not require hyperparameter-tuning
or surrogate training before deployment. We demonstrate on various NAS search
spaces that our estimator consistently outperforms other alternatives in
achieving better correlation with the true test performance rankings. We
further show that our estimator can be easily incorporated into both
query-based and one-shot NAS methods to improve the speed or quality of the
search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1"&gt;Binxin Ru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lyle_C/0/1/0/all/0/1"&gt;Clare Lyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schut_L/0/1/0/all/0/1"&gt;Lisa Schut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fil_M/0/1/0/all/0/1"&gt;Miroslav Fil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1"&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-output Gaussian Processes for Uncertainty-aware Recommender Systems. (arXiv:2106.04221v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04221</id>
        <link href="http://arxiv.org/abs/2106.04221"/>
        <updated>2021-06-09T02:01:49.089Z</updated>
        <summary type="html"><![CDATA[Recommender systems are often designed based on a collaborative filtering
approach, where user preferences are predicted by modelling interactions
between users and items. Many common approaches to solve the collaborative
filtering task are based on learning representations of users and items,
including simple matrix factorization, Gaussian process latent variable models,
and neural-network based embeddings. While matrix factorization approaches fail
to model nonlinear relations, neural networks can potentially capture such
complex relations with unprecedented predictive power and are highly scalable.
However, neither of them is able to model predictive uncertainties. In
contrast, Gaussian Process based models can generate a predictive distribution,
but cannot scale to large amounts of data. In this manuscript, we propose a
novel approach combining the representation learning paradigm of collaborative
filtering with multi-output Gaussian processes in a joint framework to generate
uncertainty-aware recommendations. We introduce an efficient strategy for model
training and inference, resulting in a model that scales to very large and
sparse datasets and achieves competitive performance in terms of classical
metrics quantifying the reconstruction error. In addition to accurately
predicting user preferences, our model also provides meaningful uncertainty
estimates about that prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinchong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Statistical Arbitrage. (arXiv:2106.04028v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04028</id>
        <link href="http://arxiv.org/abs/2106.04028"/>
        <updated>2021-06-09T02:01:49.045Z</updated>
        <summary type="html"><![CDATA[Statistical arbitrage identifies and exploits temporal price differences
between similar assets. We propose a unifying conceptual framework for
statistical arbitrage and develop a novel deep learning solution, which finds
commonality and time-series patterns from large panels in a data-driven and
flexible way. First, we construct arbitrage portfolios of similar assets as
residual portfolios from conditional latent asset pricing factors. Second, we
extract the time series signals of these residual portfolios with one of the
most powerful machine learning time-series solutions, a convolutional
transformer. Last, we use these signals to form an optimal trading policy, that
maximizes risk-adjusted returns under constraints. We conduct a comprehensive
empirical comparison study with daily large cap U.S. stocks. Our optimal
trading strategy obtains a consistently high out-of-sample Sharpe ratio and
substantially outperforms all benchmark approaches. It is orthogonal to common
risk factors, and exploits asymmetric local trend and reversion patterns. Our
strategies remain profitable after taking into account trading frictions and
costs. Our findings suggest a high compensation for arbitrageurs to enforce the
law of one price.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guijarro_Ordonez_J/0/1/0/all/0/1"&gt;Jorge Guijarro-Ordonez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelger_M/0/1/0/all/0/1"&gt;Markus Pelger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanotti_G/0/1/0/all/0/1"&gt;Greg Zanotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Widening Access to Applied Machine Learning with TinyML. (arXiv:2106.04008v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04008</id>
        <link href="http://arxiv.org/abs/2106.04008"/>
        <updated>2021-06-09T02:01:49.039Z</updated>
        <summary type="html"><![CDATA[Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Vijay Janapa Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plancher_B/0/1/0/all/0/1"&gt;Brian Plancher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_S/0/1/0/all/0/1"&gt;Susan Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moroney_L/0/1/0/all/0/1"&gt;Laurence Moroney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1"&gt;Pete Warden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anant Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1"&gt;Colby Banbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banzi_M/0/1/0/all/0/1"&gt;Massimo Banzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_M/0/1/0/all/0/1"&gt;Matthew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_B/0/1/0/all/0/1"&gt;Benjamin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1"&gt;Sharad Chitlangia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_R/0/1/0/all/0/1"&gt;Radhika Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grafman_S/0/1/0/all/0/1"&gt;Sarah Grafman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaeger_R/0/1/0/all/0/1"&gt;Rupert Jaeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1"&gt;Srivatsan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1"&gt;Maximilian Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leiker_D/0/1/0/all/0/1"&gt;Daniel Leiker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mann_C/0/1/0/all/0/1"&gt;Cara Mann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_M/0/1/0/all/0/1"&gt;Mark Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajak_D/0/1/0/all/0/1"&gt;Dominic Pajak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramaprasad_D/0/1/0/all/0/1"&gt;Dhilan Ramaprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;J. Evan Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_M/0/1/0/all/0/1"&gt;Matthew Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tingley_D/0/1/0/all/0/1"&gt;Dustin Tingley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs. (arXiv:2106.04110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04110</id>
        <link href="http://arxiv.org/abs/2106.04110"/>
        <updated>2021-06-09T02:01:49.028Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) in the infinite width/channel limit have received
much attention recently, as they provide a clear analytical window to deep
learning via mappings to Gaussian Processes (GPs). Despite its theoretical
appeal, this viewpoint lacks a crucial ingredient of deep learning in finite
DNNs, laying at the heart of their success -- feature learning. Here we
consider DNNs trained with noisy gradient descent on a large training set and
derive a self consistent Gaussian Process theory accounting for strong
finite-DNN and feature learning effects. Applying this to a toy model of a
two-layer linear convolutional neural network (CNN) shows good agreement with
experiments. We further identify, both analytical and numerically, a sharp
transition between a feature learning regime and a lazy learning regime in this
model. Strong finite-DNN effects are also derived for a non-linear two-layer
fully connected network. Our self consistent theory provides a rich and
versatile analytical framework for studying feature learning and other non-lazy
effects in finite DNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naveh_G/0/1/0/all/0/1"&gt;Gadi Naveh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ringel_Z/0/1/0/all/0/1"&gt;Zohar Ringel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Label Cleaning with Example-based Explanations. (arXiv:2106.03922v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03922</id>
        <link href="http://arxiv.org/abs/2106.03922"/>
        <updated>2021-06-09T02:01:49.017Z</updated>
        <summary type="html"><![CDATA[We tackle sequential learning under label noise in applications where a human
supervisor can be queried to relabel suspicious examples. Existing approaches
are flawed, in that they only relabel incoming examples that look
``suspicious'' to the model. As a consequence, those mislabeled examples that
elude (or don't undergo) this cleaning step end up tainting the training data
and the model with no further chance of being cleaned. We propose Cincer, a
novel approach that cleans both new and past data by identifying pairs of
mutually incompatible examples. Whenever it detects a suspicious example,
Cincer identifies a counter-example in the training set that -- according to
the model -- is maximally incompatible with the suspicious example, and asks
the annotator to relabel either or both examples, resolving this possible
inconsistency. The counter-examples are chosen to be maximally incompatible, so
to serve as explanations of the model' suspicion, and highly influential, so to
convey as much information as possible if relabeled. Cincer achieves this by
leveraging an efficient and robust approximation of influence functions based
on the Fisher information matrix (FIM). Our extensive empirical evaluation
shows that clarifying the reasons behind the model's suspicions by cleaning the
counter-examples helps acquiring substantially better data and models,
especially when paired with our FIM approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1"&gt;Stefano Teso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontempelli_A/0/1/0/all/0/1"&gt;Andrea Bontempelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1"&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1"&gt;Andrea Passerini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Curriculum Learning. (arXiv:2106.04072v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04072</id>
        <link href="http://arxiv.org/abs/2106.04072"/>
        <updated>2021-06-09T02:01:49.010Z</updated>
        <summary type="html"><![CDATA[When faced with learning challenging new tasks, humans often follow sequences
of steps that allow them to incrementally build up the necessary skills for
performing these new tasks. However, in machine learning, models are most often
trained to solve the target tasks directly.Inspired by human learning, we
propose a novel curriculum learning approach which decomposes challenging tasks
into sequences of easier intermediate goals that are used to pre-train a model
before tackling the target task. We focus on classification tasks, and design
the intermediate tasks using an automatically constructed label hierarchy. We
train the model at each level of the hierarchy, from coarse labels to fine
labels, transferring acquired knowledge across these levels. For instance, the
model will first learn to distinguish animals from objects, and then use this
acquired knowledge when learning to classify among more fine-grained classes
such as cat, dog, car, and truck. Most existing curriculum learning algorithms
for supervised learning consist of scheduling the order in which the training
examples are presented to the model. In contrast, our approach focuses on the
output space of the model. We evaluate our method on several established
datasets and show significant performance gains especially on classification
problems with many labels. We also evaluate on a new synthetic dataset which
allows us to study multiple aspects of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stretcu_O/0/1/0/all/0/1"&gt;Otilia Stretcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platanios_E/0/1/0/all/0/1"&gt;Emmanouil Antonios Platanios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1"&gt;Tom M. Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1"&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Generation of Sequential Counterfactual Explanations for Black-box Models. (arXiv:2106.03962v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03962</id>
        <link href="http://arxiv.org/abs/2106.03962"/>
        <updated>2021-06-09T02:01:48.994Z</updated>
        <summary type="html"><![CDATA[Explainable machine learning (ML) has gained traction in recent years due to
the increasing adoption of ML-based systems in many sectors. Counterfactual
explanations (CFEs) provide ``what if'' feedback of the form ``if an input
datapoint were $x'$ instead of $x$, then an ML-based system's output would be
$y'$ instead of $y$.'' CFEs are attractive due to their actionable feedback,
amenability to existing legal frameworks, and fidelity to the underlying ML
model. Yet, current CFE approaches are single shot -- that is, they assume $x$
can change to $x'$ in a single time period. We propose a novel
stochastic-control-based approach that generates sequential CFEs, that is, CFEs
that allow $x$ to move stochastically and sequentially across intermediate
states to a final state $x'$. Our approach is model agnostic and black box.
Furthermore, calculation of CFEs is amortized such that once trained, it
applies to multiple datapoints without the need for re-optimization. In
addition to these primary characteristics, our approach admits optional
desiderata such as adherence to the data manifold, respect for causal
relations, and sparsity -- identified by past research as desirable properties
of CFEs. We evaluate our approach using three real-world datasets and show
successful generation of sequential CFEs that respect other counterfactual
desiderata.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sahil Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1"&gt;Keegan Hines&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John P. Dickerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Descent and Other Interpolation Phenomena in GANs. (arXiv:2106.04003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04003</id>
        <link href="http://arxiv.org/abs/2106.04003"/>
        <updated>2021-06-09T02:01:48.989Z</updated>
        <summary type="html"><![CDATA[We study overparameterization in generative adversarial networks (GANs) that
can interpolate the training data. We show that overparameterization can
improve generalization performance and accelerate the training process. We
study the generalization error as a function of latent space dimension and
identify two main behaviors, depending on the learning setting. First, we show
that overparameterized generative models that learn distributions by minimizing
a metric or $f$-divergence do not exhibit double descent in generalization
errors; specifically, all the interpolating solutions achieve the same
generalization error. Second, we develop a new pseudo-supervised learning
approach for GANs where the training utilizes pairs of fabricated (noise)
inputs in conjunction with real output samples. Our pseudo-supervised setting
exhibits double descent (and in some cases, triple descent) of generalization
errors. We combine pseudo-supervision with overparameterization (i.e., overly
large latent space dimension) to accelerate training while performing better,
or close to, the generalization performance without pseudo-supervision. While
our analysis focuses mostly on linear GANs, we also apply important insights
for improving generalization of nonlinear, multilayer GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luzi_L/0/1/0/all/0/1"&gt;Lorenzo Luzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1"&gt;Yehuda Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03953</id>
        <link href="http://arxiv.org/abs/2106.03953"/>
        <updated>2021-06-09T02:01:48.982Z</updated>
        <summary type="html"><![CDATA[Summarization has usually relied on gold standard summaries to train
extractive or abstractive models. Social media brings a hurdle to summarization
techniques since it requires addressing a multi-document multi-author approach.
We address this challenging task by introducing a novel method that generates
abstractive summaries of online news discussions. Our method extends a
BERT-based architecture, including an attention encoding that fed comments'
likes during the training stage. To train our model, we define a task which
consists of reconstructing high impact comments based on popularity (likes).
Accordingly, our model learns to summarize online discussions based on their
most relevant comments. Our novel approach provides a summary that represents
the most relevant aspects of a news item that users comment on, incorporating
the social context as a source of information to summarize texts in online
social networks. Our model is evaluated using ROUGE scores between the
generated summary and each comment on the thread. Our model, including the
social attention encoding, significantly outperforms both extractive and
abstractive summarization methods based on such evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1"&gt;Ignacio Tampe Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Marcelo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos Milios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention. (arXiv:2106.04133v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04133</id>
        <link href="http://arxiv.org/abs/2106.04133"/>
        <updated>2021-06-09T02:01:48.970Z</updated>
        <summary type="html"><![CDATA[Emotion recognition from speech is a challenging task. Re-cent advances in
deep learning have led bi-directional recur-rent neural network (Bi-RNN) and
attention mechanism as astandard method for speech emotion recognition,
extractingand attending multi-modal features - audio and text, and thenfusing
them for downstream emotion classification tasks. Inthis paper, we propose a
simple yet efficient neural networkarchitecture to exploit both acoustic and
lexical informationfrom speech. The proposed framework using multi-scale
con-volutional layers (MSCNN) to obtain both audio and text hid-den
representations. Then, a statistical pooling unit (SPU)is used to further
extract the features in each modality. Be-sides, an attention module can be
built on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the
perfor-mance. Extensive experiments show that the proposed modeloutperforms
previous state-of-the-art methods on IEMOCAPdataset with four emotion
categories (i.e., angry, happy, sadand neutral) in both weighted accuracy (WA)
and unweightedaccuracy (UA), with an improvement of 5.0% and 5.2% respectively
under the ASR setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zixuan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shengfeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunfeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04569</id>
        <link href="http://arxiv.org/abs/2106.04569"/>
        <updated>2021-06-09T02:01:48.963Z</updated>
        <summary type="html"><![CDATA[Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this model in a face recognition
scenario. We are the first to show that weaknesses of models trained on real
data can be discovered using simulated samples. Using our proposed method, we
can find adversarial synthetic faces that fool contemporary face recognition
models. This demonstrates the fact that these models have weaknesses that are
not measured by commonly used validation datasets. We hypothesize that this
type of adversarial examples are not isolated, but usually lie in connected
components in the latent space of the simulator. We present a method to find
these adversarial regions as opposed to the typical adversarial points found in
the adversarial example literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1"&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Weichao Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1"&gt;Stan Sclaroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoPtosis. (arXiv:2106.03905v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.03905</id>
        <link href="http://arxiv.org/abs/2106.03905"/>
        <updated>2021-06-09T02:01:48.945Z</updated>
        <summary type="html"><![CDATA[Blepharoptosis, or ptosis as it is more commonly referred to, is a condition
of the eyelid where the upper eyelid droops. The current diagnosis for ptosis
involves cumbersome manual measurements that are time-consuming and prone to
human error. In this paper, we present AutoPtosis, an artificial intelligence
based system with interpretable results for rapid diagnosis of ptosis. We
utilize a diverse dataset collected at the University of Illinois Hospital and
Health to successfully develop a robust deep learning model for prediction and
also develop a clinically inspired model that calculates the marginal reflex
distance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician
verified data that had an equal class balance. The proposed algorithm can help
in the rapid and timely diagnosis of ptosis, significantly reduce the burden on
the healthcare system, and save the patients and clinics valuable resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Aleem_A/0/1/0/all/0/1"&gt;Abdullah Aleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nallabothula_M/0/1/0/all/0/1"&gt;Manoj Prabhakar Nallabothula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Setabutr_P/0/1/0/all/0/1"&gt;Pete Setabutr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hallak_J/0/1/0/all/0/1"&gt;Joelle A. Hallak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_D/0/1/0/all/0/1"&gt;Darvin Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-09T02:01:48.939Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width Limit at Initialization. (arXiv:2106.04013v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04013</id>
        <link href="http://arxiv.org/abs/2106.04013"/>
        <updated>2021-06-09T02:01:48.932Z</updated>
        <summary type="html"><![CDATA[Theoretical results show that neural networks can be approximated by Gaussian
processes in the infinite-width limit. However, for fully connected networks,
it has been previously shown that for any fixed network width, $n$, the
Gaussian approximation gets worse as the network depth, $d$, increases. Given
that modern networks are deep, this raises the question of how well modern
architectures, like ResNets, are captured by the infinite-width limit. To
provide a better approximation, we study ReLU ResNets in the
infinite-depth-and-width limit, where both depth and width tend to infinity as
their ratio, $d/n$, remains constant. In contrast to the Gaussian
infinite-width limit, we show theoretically that the network exhibits
log-Gaussian behaviour at initialization in the infinite-depth-and-width limit,
with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we
demonstrate that even basic properties of standard ResNet architectures are
poorly captured by the Gaussian limit, but remarkably well captured by our
log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at
initialization are hypoactivated: fewer than half of the ReLUs are activated.
Additionally, we calculate the interlayer correlations, which have the effect
of exponentially increasing the variance of the network output. Based on our
analysis, we introduce Balanced ResNets, a simple architecture modification,
which eliminates hypoactivation and interlayer correlations and is more
amenable to theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1"&gt;Mufan Bill Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nica_M/0/1/0/all/0/1"&gt;Mihai Nica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEAR: A Simple Lightweight Method to Rank Architectures. (arXiv:2106.04010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04010</id>
        <link href="http://arxiv.org/abs/2106.04010"/>
        <updated>2021-06-09T02:01:48.923Z</updated>
        <summary type="html"><![CDATA[The fundamental problem in Neural Architecture Search (NAS) is to efficiently
find high-performing architectures from a given search space. We propose a
simple but powerful method which we call FEAR, for ranking architectures in any
search space. FEAR leverages the viewpoint that neural networks are powerful
non-linear feature extractors. First, we train different architectures in the
search space to the same training or validation error. Then, we compare the
usefulness of the features extracted by each architecture. We do so with a
quick training keeping most of the architecture frozen. This gives fast
estimates of the relative performance. We validate FEAR on Natsbench topology
search space on three different datasets against competing baselines and show
strong ranking correlation especially compared to recently proposed zero-cost
methods. FEAR particularly excels at ranking high-performance architectures in
the search space. When used in the inner loop of discrete search algorithms
like random search, FEAR can cut down the search time by approximately 2.4X
without losing accuracy. We additionally empirically study very recently
proposed zero-cost measures for ranking and find that they breakdown in ranking
performance as training proceeds and also that data-agnostic ranking scores
which ignore the dataset do not generalize across dissimilar datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1"&gt;Debadeepta Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shital Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;Sebastien Bubeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon Learning for Few-Shot Neural Sequence Modeling. (arXiv:2106.03993v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03993</id>
        <link href="http://arxiv.org/abs/2106.03993"/>
        <updated>2021-06-09T02:01:48.917Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence transduction is the core problem in language processing
applications as diverse as semantic parsing, machine translation, and
instruction following. The neural network models that provide the dominant
solution to these problems are brittle, especially in low-resource settings:
they fail to generalize correctly or systematically from small datasets. Past
work has shown that many failures of systematic generalization arise from
neural models' inability to disentangle lexical phenomena from syntactic ones.
To address this, we augment neural decoders with a lexical translation
mechanism that generalizes existing copy mechanisms to incorporate learned,
decontextualized, token-level translation rules. We describe how to initialize
this mechanism using a variety of lexicon learning algorithms, and show that it
improves systematic generalization on a diverse set of sequence modeling tasks
drawn from cognitive science, formal semantics, and machine translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Dimension Estimation. (arXiv:2106.04018v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04018</id>
        <link href="http://arxiv.org/abs/2106.04018"/>
        <updated>2021-06-09T02:01:48.900Z</updated>
        <summary type="html"><![CDATA[It has long been thought that high-dimensional data encountered in many
practical machine learning tasks have low-dimensional structure, i.e., the
manifold hypothesis holds. A natural question, thus, is to estimate the
intrinsic dimension of a given population distribution from a finite sample. We
introduce a new estimator of the intrinsic dimension and provide finite sample,
non-asymptotic guarantees. We then apply our techniques to get new sample
complexity bounds for Generative Adversarial Networks (GANs) depending only on
the intrinsic dimension of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Block_A/0/1/0/all/0/1"&gt;Adam Block&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zeyu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Polyanskiy_Y/0/1/0/all/0/1"&gt;Yury Polyanskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rakhlin_A/0/1/0/all/0/1"&gt;Alexander Rakhlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the role of feedback in visual processing: a predictive coding perspective. (arXiv:2106.04225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04225</id>
        <link href="http://arxiv.org/abs/2106.04225"/>
        <updated>2021-06-09T02:01:48.879Z</updated>
        <summary type="html"><![CDATA[Brain-inspired machine learning is gaining increasing consideration,
particularly in computer vision. Several studies investigated the inclusion of
top-down feedback connections in convolutional networks; however, it remains
unclear how and when these connections are functionally helpful. Here we
address this question in the context of object recognition under noisy
conditions. We consider deep convolutional networks (CNNs) as models of
feed-forward visual processing and implement Predictive Coding (PC) dynamics
through feedback connections (predictive feedback) trained for reconstruction
or classification of clean images. To directly assess the computational role of
predictive feedback in various experimental situations, we optimize and
interpret the hyper-parameters controlling the network's recurrent dynamics.
That is, we let the optimization process determine whether top-down connections
and predictive coding dynamics are functionally beneficial. Across different
model depths and architectures (3-layer CNN, ResNet18, and EfficientNetB0) and
against various types of noise (CIFAR100-C), we find that the network
increasingly relies on top-down predictions as the noise level increases; in
deeper networks, this effect is most prominent at lower layers. In addition,
the accuracy of the network implementing PC dynamics significantly increases
over time-steps, compared to its equivalent forward network. All in all, our
results provide novel insights relevant to Neuroscience by confirming the
computational role of feedback connections in sensory systems, and to Machine
Learning by revealing how these can improve the robustness of current vision
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1"&gt;Milad Mozafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Person Re-Identification with a Locally Aware Transformer. (arXiv:2106.03720v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03720</id>
        <link href="http://arxiv.org/abs/2106.03720"/>
        <updated>2021-06-09T02:01:48.879Z</updated>
        <summary type="html"><![CDATA[Person Re-Identification is an important problem in computer vision-based
surveillance applications, in which the same person is attempted to be
identified from surveillance photographs in a variety of nearby zones. At
present, the majority of Person re-ID techniques are based on Convolutional
Neural Networks (CNNs), but Vision Transformers are beginning to displace pure
CNNs for a variety of object recognition tasks. The primary output of a vision
transformer is a global classification token, but vision transformers also
yield local tokens which contain additional information about local regions of
the image. Techniques to make use of these local tokens to improve
classification accuracy are an active area of research. We propose a novel
Locally Aware Transformer (LA-Transformer) that employs a Parts-based
Convolution Baseline (PCB)-inspired strategy for aggregating globally enhanced
local classification tokens into an ensemble of $\sqrt{N}$ classifiers, where
$N$ is the number of patches. An additional novelty is that we incorporate
blockwise fine-tuning which further improves re-ID accuracy. LA-Transformer
with blockwise fine-tuning achieves rank-1 accuracy of $98.27 \%$ with standard
deviation of $0.13$ on the Market-1501 and $98.7\%$ with standard deviation of
$0.2$ on the CUHK03 dataset respectively, outperforming all other
state-of-the-art published methods at the time of writing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1"&gt;Charu Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapil_S/0/1/0/all/0/1"&gt;Siddhant R. Kapil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_D/0/1/0/all/0/1"&gt;David Chapman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Social Welfare While Preserving Autonomy via a Pareto Mediator. (arXiv:2106.03927v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.03927</id>
        <link href="http://arxiv.org/abs/2106.03927"/>
        <updated>2021-06-09T02:01:48.874Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms often make decisions on behalf of agents with
varied and sometimes conflicting interests. In domains where agents can choose
to take their own action or delegate their action to a central mediator, an
open question is how mediators should take actions on behalf of delegating
agents. The main existing approach uses delegating agents to punish
non-delegating agents in an attempt to get all agents to delegate, which tends
to be costly for all. We introduce a Pareto Mediator which aims to improve
outcomes for delegating agents without making any of them worse off. Our
experiments in random normal form games, a restaurant recommendation game, and
a reinforcement learning sequential social dilemma show that the Pareto
Mediator greatly increases social welfare. Also, even when the Pareto Mediator
is based on an incorrect model of agent utility, performance gracefully
degrades to the pre-intervention level, due to the individual autonomy
preserved by the voluntary mediator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1"&gt;Stephen McAleer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1"&gt;John Lanier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1"&gt;Michael Dennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1"&gt;Roy Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Method Based on NARX models and Machine Learning for Pattern Recognition. (arXiv:2106.04021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04021</id>
        <link href="http://arxiv.org/abs/2106.04021"/>
        <updated>2021-06-09T02:01:48.868Z</updated>
        <summary type="html"><![CDATA[This work presents a novel technique that integrates the methodologies of
machine learning and system identification to solve multiclass problems. Such
an approach allows to extract and select sets of representative features with
reduced dimensionality, as well as predicts categorical outputs. The efficiency
of the method was tested by running case studies investigated in machine
learning, obtaining better absolute results when compared with classical
classification algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1"&gt;P. H. O. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerqueira_A/0/1/0/all/0/1"&gt;A. S. Cerqueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nepomuceno_E/0/1/0/all/0/1"&gt;E. G. Nepomuceno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Vision Transformers. (arXiv:2106.04560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04560</id>
        <link href="http://arxiv.org/abs/2106.04560"/>
        <updated>2021-06-09T02:01:48.863Z</updated>
        <summary type="html"><![CDATA[Attention-based neural networks such as the Vision Transformer (ViT) have
recently attained state-of-the-art results on many computer vision benchmarks.
Scale is a primary ingredient in attaining excellent results, therefore,
understanding a model's scaling properties is a key to designing future
generations effectively. While the laws for scaling Transformer language models
have been studied, it is unknown how Vision Transformers scale. To address
this, we scale ViT models and data, both up and down, and characterize the
relationships between error rate, data, and compute. Along the way, we refine
the architecture and training of ViT, reducing memory consumption and
increasing accuracy the resulting models. As a result, we successfully train a
ViT model with two billion parameters, which attains a new state-of-the-art on
ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot
learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10
examples per class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Learning for Knowledge Distillation. (arXiv:2106.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04570</id>
        <link href="http://arxiv.org/abs/2106.04570"/>
        <updated>2021-06-09T02:01:48.833Z</updated>
        <summary type="html"><![CDATA[We present Meta Learning for Knowledge Distillation (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models. The code is available at
https://github.com/JetRunner/MetaDistil]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Mediated, Object-Centric Representation Learning. (arXiv:2012.15814v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15814</id>
        <link href="http://arxiv.org/abs/2012.15814"/>
        <updated>2021-06-09T02:01:48.822Z</updated>
        <summary type="html"><![CDATA[We present Language-mediated, Object-centric Representation Learning (LORL),
a paradigm for learning disentangled, object-centric scene representations from
vision and language. LORL builds upon recent advances in unsupervised object
discovery and segmentation, notably MONet and Slot Attention. While these
algorithms learn an object-centric representation just by reconstructing the
input image, LORL enables them to further learn to associate the learned
representations to concepts, i.e., words for object categories, properties, and
spatial relationships, from language input. These object-centric concepts
derived from language facilitate the learning of object-centric
representations. LORL can be integrated with various unsupervised object
discovery algorithms that are language-agnostic. Experiments show that the
integration of LORL consistently improves the performance of unsupervised
object discovery methods on two datasets via the help of language. We also show
that concepts learned by LORL, in conjunction with object discovery methods,
aid downstream tasks such as referring expression comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[f-CNN$^{\text{x}}$: A Toolflow for Mapping Multi-CNN Applications on FPGAs. (arXiv:1805.10174v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.10174</id>
        <link href="http://arxiv.org/abs/1805.10174"/>
        <updated>2021-06-09T02:01:48.816Z</updated>
        <summary type="html"><![CDATA[The predictive power of Convolutional Neural Networks (CNNs) has been an
integral factor for emerging latency-sensitive applications, such as autonomous
drones and vehicles. Such systems employ multiple CNNs, each one trained for a
particular task. The efficient mapping of multiple CNNs on a single FPGA device
is a challenging task as the allocation of compute resources and external
memory bandwidth needs to be optimised at design time. This paper proposes
f-CNN$^{\text{x}}$, an automated toolflow for the optimised mapping of multiple
CNNs on FPGAs, comprising a novel multi-CNN hardware architecture together with
an automated design space exploration method that considers the user-specified
performance requirements for each model to allocate compute resources and
generate a synthesisable accelerator. Moreover, f-CNN$^{\text{x}}$ employs a
novel scheduling algorithm that alleviates the limitations of the memory
bandwidth contention between CNNs and sustains the high utilisation of the
architecture. Experimental evaluation shows that f-CNN$^{\text{x}}$'s designs
outperform contention-unaware FPGA mappings by up to 50% and deliver up to 6.8x
higher performance-per-Watt over highly optimised GPU designs for multi-CNN
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1"&gt;Christos-Savvas Bouganis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04550</id>
        <link href="http://arxiv.org/abs/2106.04550"/>
        <updated>2021-06-09T02:01:48.797Z</updated>
        <summary type="html"><![CDATA[Unsupervised pretraining has recently proven beneficial for computer vision
tasks, including object detection. However, previous self-supervised approaches
are not designed to handle a key aspect of detection: localizing objects. Here,
we present DETReg, an unsupervised pretraining approach for object DEtection
with TRansformers using Region priors. Motivated by the two tasks underlying
object detection: localization and categorization, we combine two complementary
signals for self-supervision. For an object localization signal, we use pseudo
ground truth object bounding boxes from an off-the-shelf unsupervised region
proposal method, Selective Search, which does not require training data and can
detect objects at a high recall rate and very low precision. The categorization
signal comes from an object embedding loss that encourages invariant object
representations, from which the object category can be inferred. We show how to
combine these two signals to train the Deformable DETR detection architecture
from large amounts of unlabeled data. DETReg improves the performance over
competitive baselines and previous self-supervised methods on standard
benchmarks like MS COCO and PASCAL VOC. DETReg also outperforms previous
supervised and unsupervised baseline approaches on low-data regime when trained
with only 1%, 2%, 5%, and 10% of the labeled data on MS COCO. For code and
pretrained models, visit the project page at https://amirbar.net/detreg]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1"&gt;Amir Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantorov_V/0/1/0/all/0/1"&gt;Vadim Kantorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1"&gt;Colorado J Reed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1"&gt;Roei Herzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Based Attention Through Internal Gating. (arXiv:2106.04540v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.04540</id>
        <link href="http://arxiv.org/abs/2106.04540"/>
        <updated>2021-06-09T02:01:48.791Z</updated>
        <summary type="html"><![CDATA[Object-based attention is a key component of the visual system, relevant for
perception, learning, and memory. Neurons tuned to features of attended objects
tend to be more active than those associated with non-attended objects. There
is a rich set of models of this phenomenon in computational neuroscience.
However, there is currently a divide between models that successfully match
physiological data but can only deal with extremely simple problems and models
of attention used in computer vision. For example, attention in the brain is
known to depend on top-down processing, whereas self-attention in deep learning
does not. Here, we propose an artificial neural network model of object-based
attention that captures the way in which attention is both top-down and
recurrent. Our attention model works well both on simple test stimuli, such as
those using images of handwritten digits, and on more complex stimuli, such as
natural images drawn from the COCO dataset. We find that our model replicates a
range of findings from neuroscience, including attention-invariant tuning,
inhibition of return, and attention-mediated scaling of activity. Understanding
object based attention is both computationally interesting and a key problem
for computational neuroscience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jordan Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Benjamin_A/0/1/0/all/0/1"&gt;Ari S. Benjamin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kording_K/0/1/0/all/0/1"&gt;Konrad P. Kording&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MViT: Mask Vision Transformer for Facial Expression Recognition in the wild. (arXiv:2106.04520v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04520</id>
        <link href="http://arxiv.org/abs/2106.04520"/>
        <updated>2021-06-09T02:01:48.775Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition (FER) in the wild is an extremely challenging
task in computer vision due to variant backgrounds, low-quality facial images,
and the subjectiveness of annotators. These uncertainties make it difficult for
neural networks to learn robust features on limited-scale datasets. Moreover,
the networks can be easily distributed by the above factors and perform
incorrect decisions. Recently, vision transformer (ViT) and data-efficient
image transformers (DeiT) present their significant performance in traditional
classification tasks. The self-attention mechanism makes transformers obtain a
global receptive field in the first layer which dramatically enhances the
feature extraction capability. In this work, we first propose a novel pure
transformer-based mask vision transformer (MViT) for FER in the wild, which
consists of two modules: a transformer-based mask generation network (MGN) to
generate a mask that can filter out complex backgrounds and occlusion of face
images, and a dynamic relabeling module to rectify incorrect labels in FER
datasets in the wild. Extensive experimental results demonstrate that our MViT
outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with
89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable
result on AffectNet-8 with 61.40%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hanting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1"&gt;Mingzhe Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Feng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Conditional Flow Model for Learning the Super-Resolution Space. (arXiv:2106.04428v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04428</id>
        <link href="http://arxiv.org/abs/2106.04428"/>
        <updated>2021-06-09T02:01:48.769Z</updated>
        <summary type="html"><![CDATA[Fundamentally, super-resolution is ill-posed problem because a low-resolution
image can be obtained from many high-resolution images. Recent studies for
super-resolution cannot create diverse super-resolution images. Although SRFlow
tried to account for ill-posed nature of the super-resolution by predicting
multiple high-resolution images given a low-resolution image, there is room to
improve the diversity and visual quality. In this paper, we propose Noise
Conditional flow model for Super-Resolution, NCSR, which increases the visual
quality and diversity of images through noise conditional layer. To learn more
diverse data distribution, we add noise to training data. However, low-quality
images are resulted from adding noise. We propose the noise conditional layer
to overcome this phenomenon. The noise conditional layer makes our model
generate more diverse images with higher visual quality than other works.
Furthermore, we show that this layer can overcome data distribution mismatch, a
problem that arises in normalizing flow models. With these benefits, NCSR
outperforms baseline in diversity and visual quality and achieves better visual
quality than traditional GAN-based models. We also get outperformed scores at
NTIRE 2021 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Younggeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_D/0/1/0/all/0/1"&gt;Donghee Son&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08199</id>
        <link href="http://arxiv.org/abs/2007.08199"/>
        <updated>2021-06-09T02:01:48.764Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved remarkable success in numerous domains with help
from large amounts of big data. However, the quality of data labels is a
concern because of the lack of high-quality labels in many real-world
scenarios. As noisy labels severely degrade the generalization performance of
deep neural networks, learning from noisy labels (robust training) is becoming
an important task in modern deep learning applications. In this survey, we
first describe the problem of learning with label noise from a supervised
learning perspective. Next, we provide a comprehensive review of 57
state-of-the-art robust training methods, all of which are categorized into
five groups according to their methodological difference, followed by a
systematic comparison of six properties used to evaluate their superiority.
Subsequently, we perform an in-depth analysis of noise rate estimation and
summarize the typically used evaluation methodology, including public noisy
datasets and evaluation metrics. Finally, we present several promising research
directions that can serve as a guideline for future studies. All the contents
will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The PREVENTION Challenge: How Good Are Humans Predicting Lane Changes?. (arXiv:2009.05331v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05331</id>
        <link href="http://arxiv.org/abs/2009.05331"/>
        <updated>2021-06-09T02:01:48.758Z</updated>
        <summary type="html"><![CDATA[While driving on highways, every driver tries to be aware of the behavior of
surrounding vehicles, including possible emergency braking, evasive maneuvers
trying to avoid obstacles, unexpected lane changes, or other emergencies that
could lead to an accident. In this paper, human's ability to predict lane
changes in highway scenarios is analyzed through the use of video sequences
extracted from the PREVENTION dataset, a database focused on the development of
research on vehicle intention and trajectory prediction. Thus, users had to
indicate the moment at which they considered that a lane change maneuver was
taking place in a target vehicle, subsequently indicating its direction: left
or right. The results retrieved have been carefully analyzed and compared to
ground truth labels, evaluating statistical models to understand whether humans
can actually predict. The study has revealed that most participants are unable
to anticipate lane-change maneuvers, detecting them after they have started.
These results might serve as a baseline for AI's prediction ability evaluation,
grading if those systems can outperform human skills by analyzing hidden cues
that seem unnoticed, improving the detection time, and even anticipating
maneuvers in some cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quintanar_A/0/1/0/all/0/1"&gt;A. Quintanar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izquierdo_R/0/1/0/all/0/1"&gt;R. Izquierdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_I/0/1/0/all/0/1"&gt;I. Parra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Llorca_D/0/1/0/all/0/1"&gt;D. Fern&amp;#xe1;ndez-Llorca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotelo_M/0/1/0/all/0/1"&gt;M. A. Sotelo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Efficient Instance Generation from Instance Discrimination. (arXiv:2106.04566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04566</id>
        <link href="http://arxiv.org/abs/2106.04566"/>
        <updated>2021-06-09T02:01:48.752Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have significantly advanced image
synthesis, however, the synthesis quality drops significantly given a limited
amount of training data. To improve the data efficiency of GAN training, prior
work typically employs data augmentation to mitigate the overfitting of the
discriminator yet still learn the discriminator with a bi-classification (i.e.,
real vs. fake) task. In this work, we propose a data-efficient Instance
Generation (InsGen) method based on instance discrimination. Concretely,
besides differentiating the real domain from the fake domain, the discriminator
is required to distinguish every individual image, no matter it comes from the
training set or from the generator. In this way, the discriminator can benefit
from the infinite synthesized samples for training, alleviating the overfitting
problem caused by insufficient training data. A noise perturbation strategy is
further introduced to improve its discriminative power. Meanwhile, the learned
instance discrimination capability from the discriminator is in turn exploited
to encourage the generator for diverse generation. Extensive experiments
demonstrate the effectiveness of our method on a variety of datasets and
training settings. Noticeably, on the setting of 2K training images from the
FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID
improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Ceyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yujun Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04484</id>
        <link href="http://arxiv.org/abs/2106.04484"/>
        <updated>2021-06-09T02:01:48.737Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;Daniel Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the benefits of defining vicinal distributions in latent space. (arXiv:2003.06566v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06566</id>
        <link href="http://arxiv.org/abs/2003.06566"/>
        <updated>2021-06-09T02:01:48.714Z</updated>
        <summary type="html"><![CDATA[The vicinal risk minimization (VRM) principle is an empirical risk
minimization (ERM) variant that replaces Dirac masses with vicinal functions.
There is strong numerical and theoretical evidence showing that VRM outperforms
ERM in terms of generalization if appropriate vicinal functions are chosen.
Mixup Training (MT), a popular choice of vicinal distribution, improves the
generalization performance of models by introducing globally linear behavior in
between training examples. Apart from generalization, recent works have shown
that mixup trained models are relatively robust to input
perturbations/corruptions and at the same time are calibrated better than their
non-mixup counterparts. In this work, we investigate the benefits of defining
these vicinal distributions like mixup in latent space of generative models
rather than in input space itself. We propose a new approach - \textit{VarMixup
(Variational Mixup)} - to better sample mixup images by using the latent
manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and
Tiny-ImageNet demonstrate that models trained by performing mixup in the latent
manifold learned by VAEs are inherently more robust to various input
corruptions/perturbations, are significantly better calibrated, and exhibit
more local-linear loss landscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1"&gt;Puneet Mangla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vedant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1"&gt;Shreyas Jayant Havaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04427</id>
        <link href="http://arxiv.org/abs/2106.04427"/>
        <updated>2021-06-09T02:01:48.707Z</updated>
        <summary type="html"><![CDATA[It has been demonstrated many times that the behavior of the human visual
system is connected to the statistics of natural images. Since machine learning
relies on the statistics of training data as well, the above connection has
interesting implications when using perceptual distances (which mimic the
behavior of the human visual system) as a loss function. In this paper, we aim
to unravel the non-trivial relationship between the probability distribution of
the data, perceptual distances, and unsupervised machine learning. To this end,
we show that perceptual sensitivity is correlated with the probability of an
image in its close neighborhood. We also explore the relation between distances
induced by autoencoders and the probability distribution of the data used for
training them, as well as how these induced distances are correlated with human
perception. Finally, we discuss why perceptual distances might not lead to
noticeable gains in performance over standard Euclidean distances in common
image processing tasks except when data is scarce and the perceptual distance
provides regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1"&gt;Alexander Hepburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1"&gt;Valero Laparra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1"&gt;Johannes Ball&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Malo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PolypGen: A multi-center polyp detection and segmentation dataset for generalisability assessment. (arXiv:2106.04463v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04463</id>
        <link href="http://arxiv.org/abs/2106.04463"/>
        <updated>2021-06-09T02:01:48.702Z</updated>
        <summary type="html"><![CDATA[Polyps in the colon are widely known as cancer precursors identified by
colonoscopy either related to diagnostic work-up for symptoms, colorectal
cancer screening or systematic surveillance of certain diseases. Whilst most
polyps are benign, the number, size and the surface structure of the polyp are
tightly linked to the risk of colon cancer. There exists a high missed
detection rate and incomplete removal of colon polyps due to the variable
nature, difficulties to delineate the abnormality, high recurrence rates and
the anatomical topography of the colon. In the past, several methods have been
built to automate polyp detection and segmentation. However, the key issue of
most methods is that they have not been tested rigorously on a large
multi-center purpose-built dataset. Thus, these methods may not generalise to
different population datasets as they overfit to a specific population and
endoscopic surveillance. To this extent, we have curated a dataset from 6
different centers incorporating more than 300 patients. The dataset includes
both single frame and sequence data with 3446 annotated polyp labels with
precise delineation of polyp boundaries verified by six senior
gastroenterologists. To our knowledge, this is the most comprehensive detection
and pixel-level segmentation dataset curated by a team of computational
scientists and expert gastroenterologists. This dataset has been originated as
the part of the Endocv2021 challenge aimed at addressing generalisability in
polyp detection and segmentation. In this paper, we provide comprehensive
insight into data construction and annotation strategies, annotation quality
assurance and technical validation for our extended EndoCV2021 dataset which we
refer to as PolypGen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghatwary_N/0/1/0/all/0/1"&gt;Noha Ghatwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Realdon_S/0/1/0/all/0/1"&gt;Stefano Realdon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cannizzaro_R/0/1/0/all/0/1"&gt;Renato Cannizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salem_O/0/1/0/all/0/1"&gt;Osama E. Salem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lamarque_D/0/1/0/all/0/1"&gt;Dominique Lamarque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Daul_C/0/1/0/all/0/1"&gt;Christian Daul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anonsen_K/0/1/0/all/0/1"&gt;Kim V. Anonsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael A. Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1"&gt;Thomas de Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+East_J/0/1/0/all/0/1"&gt;James E. East&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grapevine Winter Pruning Automation: On Potential Pruning Points Detection through 2D Plant Modeling using Grapevine Segmentation. (arXiv:2106.04208v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04208</id>
        <link href="http://arxiv.org/abs/2106.04208"/>
        <updated>2021-06-09T02:01:48.691Z</updated>
        <summary type="html"><![CDATA[Grapevine winter pruning is a complex task, that requires skilled workers to
execute it correctly. The complexity of this task is also the reason why it is
time consuming. Considering that this operation takes about 80-120 hours/ha to
be completed, and therefore is even more crucial in large-size vineyards, an
automated system can help to speed up the process. To this end, this paper
presents a novel multidisciplinary approach that tackles this challenging task
by performing object segmentation on grapevine images, used to create a
representative model of the grapevine plants. Second, a set of potential
pruning points is generated from this plant representation. We will describe
(a) a methodology for data acquisition and annotation, (b) a neural network
fine-tuning for grapevine segmentation, (c) an image processing based method
for creating the representative model of grapevines, starting from the inferred
segmentation and (d) potential pruning points detection and localization, based
on the plant model which is a simplification of the grapevine structure. With
this approach, we are able to identify a significant set of potential pruning
points on the canes, that can be used, with further selection, to derive the
final set of the real pruning points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_M/0/1/0/all/0/1"&gt;Miguel Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaldaferri_A/0/1/0/all/0/1"&gt;Antonello Scaldaferri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_T/0/1/0/all/0/1"&gt;Tao Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatti_M/0/1/0/all/0/1"&gt;Matteo Gatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poni_S/0/1/0/all/0/1"&gt;Stefano Poni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semini_C/0/1/0/all/0/1"&gt;Claudio Semini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caldwell_D/0/1/0/all/0/1"&gt;Darwin Caldwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image2Point: 3D Point-Cloud Understanding with Pretrained 2D ConvNets. (arXiv:2106.04180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04180</id>
        <link href="http://arxiv.org/abs/2106.04180"/>
        <updated>2021-06-09T02:01:48.663Z</updated>
        <summary type="html"><![CDATA[3D point-clouds and 2D images are different visual representations of the
physical world. While human vision can understand both representations,
computer vision models designed for 2D image and 3D point-cloud understanding
are quite different. Our paper investigates the potential for transferability
between these two representations by empirically investigating whether this
approach works, what factors affect the transfer performance, and how to make
it work even better. We discovered that we can indeed use the same neural net
model architectures to understand both images and point-clouds. Moreover, we
can transfer pretrained weights from image models to point-cloud models with
minimal effort. Specifically, based on a 2D ConvNet pretrained on an image
dataset, we can transfer the image model to a point-cloud model by
\textit{inflating} 2D convolutional filters to 3D then finetuning its input,
output, and optionally normalization layers. The transferred model can achieve
competitive performance on 3D point-cloud classification, indoor and driving
scene segmentation, even beating a wide range of point-cloud models that adopt
task-specific architectures and use a variety of tricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shijia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1"&gt;Bohan Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bichen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1"&gt;Wei Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1"&gt;Peter Vajda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSRNet: Cascaded Selective Resolution Network for Real-time Semantic Segmentation. (arXiv:2106.04400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04400</id>
        <link href="http://arxiv.org/abs/2106.04400"/>
        <updated>2021-06-09T02:01:48.641Z</updated>
        <summary type="html"><![CDATA[Real-time semantic segmentation has received considerable attention due to
growing demands in many practical applications, such as autonomous vehicles,
robotics, etc. Existing real-time segmentation approaches often utilize feature
fusion to improve segmentation accuracy. However, they fail to fully consider
the feature information at different resolutions and the receptive fields of
the networks are relatively limited, thereby compromising the performance. To
tackle this problem, we propose a light Cascaded Selective Resolution Network
(CSRNet) to improve the performance of real-time segmentation through multiple
context information embedding and enhanced feature aggregation. The proposed
network builds a three-stage segmentation system, which integrates feature
information from low resolution to high resolution and achieves feature
refinement progressively. CSRNet contains two critical modules: the Shorted
Pyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The
SPFM is a computationally efficient module to incorporate the global context
information and significantly enlarge the receptive field at each stage. The
SRM is designed to fuse multi-resolution feature maps with various receptive
fields, which assigns soft channel attentions across the feature maps and helps
to remedy the problem caused by multi-scale objects. Comprehensive experiments
on two well-known datasets demonstrate that the proposed CSRNet effectively
improves the performance for real-time segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jingjing Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1"&gt;Lai-Man Po&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wing-Yin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xian_P/0/1/0/all/0/1"&gt;Pengfei Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1"&gt;Weifeng Ou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-frame sequence generator of 4D human body motion. (arXiv:2106.04387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04387</id>
        <link href="http://arxiv.org/abs/2106.04387"/>
        <updated>2021-06-09T02:01:48.582Z</updated>
        <summary type="html"><![CDATA[We examine the problem of generating temporally and spatially dense 4D human
body motion. On the one hand generative modeling has been extensively studied
as a per time-frame static fitting problem for dense 3D models such as mesh
representations, where the temporal aspect is left out of the generative model.
On the other hand, temporal generative models exist for sparse human models
such as marker-based capture representations, but have not to our knowledge
been extended to dense 3D shapes. We propose to bridge this gap with a
generative auto-encoder-based framework, which encodes morphology, global
locomotion including translation and rotation, and multi-frame temporal motion
as a single latent space vector. To assess its generalization and factorization
abilities, we train our model on a cyclic locomotion subset of AMASS,
leveraging the dense surface models it provides for an extensive set of motion
captures. Our results validate the ability of the model to reconstruct 4D
sequences of human locomotions within a low error bound, and the meaningfulness
of latent space interpolation between latent vectors representing different
multi-frame sequences and locomotion types. We also illustrate the benefits of
the approach for 4D human motion prediction of future frames from initial human
locomotion frames, showing promising abilities of our model to learn realistic
spatio-temporal features of human motion. We show that our model allows for
data completion of both spatially and temporally sparse data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_M/0/1/0/all/0/1"&gt;Marsot Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanie_W/0/1/0/all/0/1"&gt;Wuhrer Stefanie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jean_Sebastien_F/0/1/0/all/0/1"&gt;Franco Jean-Sebastien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stephane_D/0/1/0/all/0/1"&gt;Durocher Stephane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04169</id>
        <link href="http://arxiv.org/abs/2106.04169"/>
        <updated>2021-06-09T02:01:48.566Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very low
black-box transferability even for large ViT models. However, we show that this
phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance the transferability of existing attacks by introducing two novel
strategies specific to the architecture of ViT models. (i) Self-Ensemble: We
propose a method to find multiple discriminative pathways by dissecting a
single ViT model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens. An
adversarial attack, when applied to such refined tokens within the ensemble of
classifiers found in a single vision transformer, has significantly higher
transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Concise yet Effective model for Non-Aligned Incomplete Multi-view and Missing Multi-label Learning. (arXiv:2005.00976v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00976</id>
        <link href="http://arxiv.org/abs/2005.00976"/>
        <updated>2021-06-09T02:01:48.559Z</updated>
        <summary type="html"><![CDATA[In reality, learning from multi-view multi-label data inevitably confronts
three challenges: missing labels, incomplete views, and non-aligned views.
Existing methods mainly concern the first two and commonly need multiple
assumptions to attack them, making even state-of-the-arts involve at least two
explicit hyper-parameters such that model selection is quite difficult. More
roughly, they will fail in handling the third challenge, let alone addressing
the three jointly. In this paper, we aim at meeting these under the least
assumption by building a concise yet effective model with just one
hyper-parameter. To ease insufficiency of available labels, we exploit not only
the consensus of multiple views but also the global and local structures hidden
among multiple labels. Specifically, we introduce an indicator matrix to tackle
the first two challenges in a regression form while aligning the same
individual labels and all labels of different views in a common label space to
battle the third challenge. In aligning, we characterize the global and local
structures of multiple labels to be high-rank and low-rank, respectively.
Subsequently, an efficient algorithm with linear time complexity in the number
of samples is established. Finally, even without view-alignment, our method
substantially outperforms state-of-the-arts with view-alignment on five real
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Songcan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SDGMNet: Statistic-based Dynamic Gradient Modulation for Local Descriptor Learning. (arXiv:2106.04434v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04434</id>
        <link href="http://arxiv.org/abs/2106.04434"/>
        <updated>2021-06-09T02:01:48.553Z</updated>
        <summary type="html"><![CDATA[Modifications on triplet loss that rescale the back-propagated gradients of
special pairs have made significant progress on local descriptor learning.
However, current gradient modulation strategies are mainly static so that they
would suffer from changes of training phases or datasets. In this paper, we
propose a dynamic gradient modulation, named SDGMNet, to improve triplet loss
for local descriptor learning. The core of our method is formulating modulation
functions with statistical characteristics which are estimated dynamically.
Firstly, we perform deep analysis on back propagation of general triplet-based
loss and introduce included angle for distance measure. On this basis,
auto-focus modulation is employed to moderate the impact of statistically
uncommon individual pairs in stochastic gradient descent optimization;
probabilistic margin cuts off the gradients of proportional Siamese pairs that
are believed to reach the optimum; power adjustment balances the total weights
of negative pairs and positive pairs. Extensive experiments demonstrate that
our novel descriptor surpasses previous state-of-the-arts on standard
benchmarks including patch verification, matching and retrieval tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yuxin Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative adversarial network with object detector discriminator for enhanced defect detection on ultrasonic B-scans. (arXiv:2106.04281v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04281</id>
        <link href="http://arxiv.org/abs/2106.04281"/>
        <updated>2021-06-09T02:01:48.546Z</updated>
        <summary type="html"><![CDATA[Non-destructive testing is a set of techniques for defect detection in
materials. While the set of imaging techniques are manifold, ultrasonic imaging
is the one used the most. The analysis is mainly performed by human inspectors
manually analyzing recorded images. The low number of defects in real
ultrasonic inspections and legal issues considering data from such inspections
make it difficult to obtain proper results from automatic ultrasonic image
(B-scan) analysis. In this paper, we present a novel deep learning Generative
Adversarial Network model for generating ultrasonic B-scans with defects in
distinct locations. Furthermore, we show that generated B-scans can be used for
synthetic data augmentation, and can improve the performance of deep
convolutional neural object detection networks. Our novel method is
demonstrated on a dataset of almost 4000 B-scans with more than 6000 annotated
defects. Defect detection performance when training on real data yielded
average precision of 71%. By training only on generated data the results
increased to 72.1%, and by mixing generated and real data we achieve 75.7%
average precision. We believe that synthetic data generation can generalize to
other challenges with limited datasets and could be used for training human
personnel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Posilovic_L/0/1/0/all/0/1"&gt;Luka Posilovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Medak_D/0/1/0/all/0/1"&gt;Duje Medak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Subasic_M/0/1/0/all/0/1"&gt;Marko Subasic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Budimir_M/0/1/0/all/0/1"&gt;Marko Budimir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Loncaric_S/0/1/0/all/0/1"&gt;Sven Loncaric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation and ABCD rule extraction for skin tumors classification. (arXiv:2106.04372v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04372</id>
        <link href="http://arxiv.org/abs/2106.04372"/>
        <updated>2021-06-09T02:01:48.537Z</updated>
        <summary type="html"><![CDATA[During the last years, computer vision-based diagnosis systems have been
widely used in several hospitals and dermatology clinics, aiming at the early
detection of malignant melanoma tumor, which is among the most frequent types
of skin cancer. In this work, we present an automated diagnosis system based on
the ABCD rule used in clinical diagnosis in order to discriminate benign from
malignant skin lesions. First, to reduce the influence of small structures, a
preprocessing step based on morphological and fast marching schemes is used. In
the second step, an unsupervised approach for lesion segmentation is proposed.
Iterative thresholding is applied to initialize level set automatically. As the
detection of an automated border is an important step for the correctness of
subsequent phases in the computerized melanoma recognition systems, we compare
its accuracy with growcut and mean shift algorithms, and discuss how these
results may influence in the following steps: the feature extraction and the
final lesion classification. Relying on visual diagnosis four features:
Asymmetry (A), Border (B), Color (C) and Diversity (D) are computed and used to
construct a classification module based on artificial neural network for the
recognition of malignant melanoma. This framework has been tested on a
dermoscopic database [16] of 320 images. The classification results show an
increasing true detection rate and a decreasing false positive rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Messadi_M/0/1/0/all/0/1"&gt;Mahammed Messadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cherifi_H/0/1/0/all/0/1"&gt;Hocine Cherifi&lt;/a&gt; (Le2i), &lt;a href="http://arxiv.org/find/cs/1/au:+Bessaid_A/0/1/0/all/0/1"&gt;Abdelhafid Bessaid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Action Localization without Knowing Boundaries. (arXiv:2106.04150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04150</id>
        <link href="http://arxiv.org/abs/2106.04150"/>
        <updated>2021-06-09T02:01:48.531Z</updated>
        <summary type="html"><![CDATA[Learning to localize actions in long, cluttered, and untrimmed videos is a
hard task, that in the literature has typically been addressed assuming the
availability of large amounts of annotated training samples for each class --
either in a fully-supervised setting, where action boundaries are known, or in
a weakly-supervised setting, where only class labels are known for each video.
In this paper, we go a step further and show that it is possible to learn to
localize actions in untrimmed videos when a) only one/few trimmed examples of
the target action are available at test time, and b) when a large collection of
videos with only class label annotation (some trimmed and some weakly annotated
untrimmed ones) are available for training; with no overlap between the classes
used during training and testing. To do so, we propose a network that learns to
estimate Temporal Similarity Matrices (TSMs) that model a fine-grained
similarity pattern between pairs of videos (trimmed or untrimmed), and uses
them to generate Temporal Class Activation Maps (TCAMs) for seen or unseen
classes. The TCAMs serve as temporal attention mechanisms to extract
video-level representations of untrimmed videos, and to temporally localize
actions at test time. To the best of our knowledge, we are the first to propose
a weakly-supervised, one/few-shot action localization network that can be
trained in an end-to-end fashion. Experimental results on THUMOS14 and
ActivityNet1.2 datasets, show that our method achieves performance comparable
or better to state-of-the-art fully-supervised, few-shot learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Ting-Ting Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1"&gt;Fan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04144</id>
        <link href="http://arxiv.org/abs/2106.04144"/>
        <updated>2021-06-09T02:01:48.514Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks may perform poorly when the test and train data
are from different domains. While this problem can be mitigated by using the
target domain data to align the source and target domain feature
representations, the target domain data may be unavailable due to privacy
concerns. Consequently, there is a need for methods that generalize well
without access to target domain data during training. In this work, we propose
an adversarial hallucination approach, which combines a class-wise
hallucination module and a semantic segmentation module. Since the segmentation
performance varies across different classes, we design a semantic-conditioned
style hallucination layer to adaptively stylize each class. The classwise
stylization parameters are generated from the semantic knowledge in the
segmentation probability maps of the source domain image. Both modules compete
adversarially, with the hallucination module generating increasingly
'difficult' style images to challenge the segmentation module. In response, the
segmentation module improves its performance as it is trained with generated
samples at an appropriate class-wise difficulty level. Experiments on state of
the art domain adaptation work demonstrate the efficacy of our proposed method
when no target domain data are available for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1"&gt;Gabriel Tjio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Left Ventricle Contouring in Cardiac Images Based on Deep Reinforcement Learning. (arXiv:2106.04127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04127</id>
        <link href="http://arxiv.org/abs/2106.04127"/>
        <updated>2021-06-09T02:01:48.508Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation is one of the important tasks of computer-aided
diagnosis in medical image analysis. Since most medical images have the
characteristics of blurred boundaries and uneven intensity distribution,
through existing segmentation methods, the discontinuity within the target area
and the discontinuity of the target boundary are likely to lead to rough or
even erroneous boundary delineation. In this paper, we propose a new iterative
refined interactive segmentation method for medical images based on agent
reinforcement learning, which focuses on the problem of target segmentation
boundaries. We model the dynamic process of drawing the target contour in a
certain order as a Markov Decision Process (MDP) based on a deep reinforcement
learning method. In the dynamic process of continuous interaction between the
agent and the image, the agent tracks the boundary point by point in order
within a limited length range until the contour of the target is completely
drawn. In this process, the agent can quickly improve the segmentation
performance by exploring an interactive policy in the image. The method we
proposed is simple and effective. At the same time, we evaluate our method on
the cardiac MRI scan data set. Experimental results show that our method has a
better segmentation effect on the left ventricle in a small number of medical
image data sets, especially in terms of segmentation boundaries, this method is
better than existing methods. Based on our proposed method, the dynamic
generation process of the predicted contour trajectory of the left ventricle
will be displayed online at https://github.com/H1997ym/LV-contour-trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1"&gt;Sixing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yameng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shufang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EnMcGAN: Adversarial Ensemble Learning for 3D Complete Renal Structures Segmentation. (arXiv:2106.04130v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04130</id>
        <link href="http://arxiv.org/abs/2106.04130"/>
        <updated>2021-06-09T02:01:48.502Z</updated>
        <summary type="html"><![CDATA[3D complete renal structures(CRS) segmentation targets on segmenting the
kidneys, tumors, renal arteries and veins in one inference. Once successful, it
will provide preoperative plans and intraoperative guidance for laparoscopic
partial nephrectomy(LPN), playing a key role in the renal cancer treatment.
However, no success has been reported in 3D CRS segmentation due to the complex
shapes of renal structures, low contrast and large anatomical variation. In
this study, we utilize the adversarial ensemble learning and propose Ensemble
Multi-condition GAN(EnMcGAN) for 3D CRS segmentation for the first time. Its
contribution is three-fold. 1)Inspired by windowing, we propose the
multi-windowing committee which divides CTA image into multiple narrow windows
with different window centers and widths enhancing the contrast for salient
boundaries and soft tissues. And then, it builds an ensemble segmentation model
on these narrow windows to fuse the segmentation superiorities and improve
whole segmentation quality. 2)We propose the multi-condition GAN which equips
the segmentation model with multiple discriminators to encourage the segmented
structures meeting their real shape conditions, thus improving the shape
feature extraction ability. 3)We propose the adversarial weighted ensemble
module which uses the trained discriminators to evaluate the quality of
segmented structures, and normalizes these evaluation scores for the ensemble
weights directed at the input image, thus enhancing the ensemble results. 122
patients are enrolled in this study and the mean Dice coefficient of the renal
structures achieves 84.6%. Extensive experiments with promising results on
renal structures reveal powerful segmentation accuracy and great clinical
significance in renal cancer treatment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuting He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rongjun Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiaoming Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guanyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Youyong Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1"&gt;Huazhong Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coatrieux_J/0/1/0/all/0/1"&gt;Jean-Louis Coatrieux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Deformation Estimation via Multi-Objective Optimization. (arXiv:2106.04139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04139</id>
        <link href="http://arxiv.org/abs/2106.04139"/>
        <updated>2021-06-09T02:01:48.495Z</updated>
        <summary type="html"><![CDATA[The free-form deformation model can represent a wide range of non-rigid
deformations by manipulating a control point lattice over the image. However,
due to a large number of parameters, it is challenging to fit the free-form
deformation model directly to the deformed image for deformation estimation
because of the complexity of the fitness landscape. In this paper, we cast the
registration task as a multi-objective optimization problem (MOP) according to
the fact that regions affected by each control point overlap with each other.
Specifically, by partitioning the template image into several regions and
measuring the similarity of each region independently, multiple objectives are
built and deformation estimation can thus be realized by solving the MOP with
off-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a
coarse-to-fine strategy is realized by image pyramid combined with control
point mesh subdivision. Specifically, the optimized candidate solutions of the
current image level are inherited by the next level, which increases the
ability to deal with large deformation. Also, a post-processing procedure is
proposed to generate a single output utilizing the Pareto optimal solutions.
Comparative experiments on both synthetic and real-world images show the
effectiveness and usefulness of our deformation estimation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakane_T/0/1/0/all/0/1"&gt;Takumi Nakane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuequan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haoran Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization. (arXiv:2106.04185v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04185</id>
        <link href="http://arxiv.org/abs/2106.04185"/>
        <updated>2021-06-09T02:01:48.481Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a video-based learning framework for animating
personalized 3D talking faces from audio. We introduce two training-time data
normalizations that significantly improve data sample efficiency. First, we
isolate and represent faces in a normalized space that decouples 3D geometry,
head pose, and texture. This decomposes the prediction problem into regressions
over the 3D face shape and the corresponding 2D texture atlas. Second, we
leverage facial symmetry and approximate albedo constancy of skin to isolate
and remove spatio-temporal lighting variations. Together, these normalizations
allow simple networks to generate high fidelity lip-sync videos under novel
ambient illumination while training with just a single speaker-specific video.
Further, to stabilize temporal dynamics, we introduce an auto-regressive
approach that conditions the model on its previous visual state. Human ratings
and objective metrics demonstrate that our method outperforms contemporary
state-of-the-art audio-driven video reenactment benchmarks in terms of realism,
lip-sync and visual quality scores. We illustrate several applications enabled
by our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1"&gt;Avisek Lahiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwatra_V/0/1/0/all/0/1"&gt;Vivek Kwatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frueh_C/0/1/0/all/0/1"&gt;Christian Frueh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1"&gt;John Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bregler_C/0/1/0/all/0/1"&gt;Chris Bregler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highly accurate digital traffic recording as a basis for future mobility research: Methods and concepts of the research project HDV-Mess. (arXiv:2106.04175v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04175</id>
        <link href="http://arxiv.org/abs/2106.04175"/>
        <updated>2021-06-09T02:01:48.472Z</updated>
        <summary type="html"><![CDATA[The research project HDV-Mess aims at a currently missing, but very crucial
component for addressing important challenges in the field of connected and
automated driving on public roads. The goal is to record traffic events at
various relevant locations with high accuracy and to collect real traffic data
as a basis for the development and validation of current and future sensor
technologies as well as automated driving functions. For this purpose, it is
necessary to develop a concept for a mobile modular system of measuring
stations for highly accurate traffic data acquisition, which enables a
temporary installation of a sensor and communication infrastructure at
different locations. Within this paper, we first discuss the project goals
before we present our traffic detection concept using mobile modular
intelligent transport systems stations (ITS-Ss). We then explain the approaches
for data processing of sensor raw data to refined trajectories, data
communication, and data validation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1"&gt;Laurent Kloeker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomsen_F/0/1/0/all/0/1"&gt;Fabian Thomsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1"&gt;Lutz Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trettner_P/0/1/0/all/0/1"&gt;Philip Trettner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsner_T/0/1/0/all/0/1"&gt;Tim Elsner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nehring_Wirxel_J/0/1/0/all/0/1"&gt;Julius Nehring-Wirxel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_K/0/1/0/all/0/1"&gt;Kersten Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1"&gt;Leif Kobbelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoesch_M/0/1/0/all/0/1"&gt;Michael Hoesch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. (arXiv:2106.04128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04128</id>
        <link href="http://arxiv.org/abs/2106.04128"/>
        <updated>2021-06-09T02:01:48.467Z</updated>
        <summary type="html"><![CDATA[We study the task of conversational fashion image retrieval via multiturn
natural language feedback. Most previous studies are based on single-turn
settings. Existing models on multiturn conversational fashion image retrieval
have limitations, such as employing traditional models, and leading to
ineffective performance. We propose a novel framework that can effectively
handle conversational fashion image retrieval with multiturn natural language
feedback texts. One characteristic of the framework is that it searches for
candidate images based on exploitation of the encoded reference image and
feedback text information together with the conversation history. Furthermore,
the image fashion attribute information is leveraged via a mutual attention
strategy. Since there is no existing fashion dataset suitable for the multiturn
setting of our task, we derive a large-scale multiturn fashion dataset via
additional manual annotation efforts on an existing single-turn dataset. The
experiments show that our proposed model significantly outperforms existing
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yifei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Transformer Networks for Semantic ImageSegmentation. (arXiv:2106.04108v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04108</id>
        <link href="http://arxiv.org/abs/2106.04108"/>
        <updated>2021-06-09T02:01:48.460Z</updated>
        <summary type="html"><![CDATA[Transformers have shown impressive performance in various natural language
processing and computer vision tasks, due to the capability of modeling
long-range dependencies. Recent progress has demonstrated to combine such
transformers with CNN-based semantic image segmentation models is very
promising. However, it is not well studied yet on how well a pure transformer
based approach can achieve for image segmentation. In this work, we explore a
novel framework for semantic image segmentation, which is encoder-decoder based
Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid
Group Transformer (PGT) as the encoder for progressively learning hierarchical
features, while reducing the computation complexity of the standard visual
transformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse
semantic-level and spatial-level information from multiple levels of the PGT
encoder for semantic image segmentation. Surprisingly, this simple baseline can
achieve new state-of-the-art results on multiple challenging semantic
segmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The
source code will be released upon the publication of this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sitong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fangjian Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1"&gt;Shengwei Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Contextualized Word Embeddings. (arXiv:2010.12684v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12684</id>
        <link href="http://arxiv.org/abs/2010.12684"/>
        <updated>2021-06-09T02:01:48.454Z</updated>
        <summary type="html"><![CDATA[Static word embeddings that represent words by a single vector cannot capture
the variability of word meaning in different linguistic and extralinguistic
contexts. Building on prior work on contextualized and dynamic word embeddings,
we introduce dynamic contextualized word embeddings that represent words as a
function of both linguistic and extralinguistic context. Based on a pretrained
language model (PLM), dynamic contextualized word embeddings model time and
social space jointly, which makes them attractive for a range of NLP tasks
involving semantic variability. We highlight potential application scenarios by
means of qualitative and quantitative analyses on four English datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1"&gt;Valentin Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1"&gt;Janet B. Pierrehumbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design of Low-Artifact Interpolation Kernels by Means of Computer Algebra. (arXiv:2106.04104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04104</id>
        <link href="http://arxiv.org/abs/2106.04104"/>
        <updated>2021-06-09T02:01:48.449Z</updated>
        <summary type="html"><![CDATA[We present a number of new piecewise-polynomial kernels for image
interpolation. The kernels are constructed by optimizing a measure of
interpolation quality based on the magnitude of anisotropic artifacts. The
kernel design process is performed symbolically using Mathematica computer
algebra system. Experimental evaluation involving 14 image quality assessment
methods demonstrates that our results compare favorably with the existing
linear interpolators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karpov_P/0/1/0/all/0/1"&gt;Peter Karpov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Generic Hierarchical Human Motion Prior using VAEs. (arXiv:2106.04004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04004</id>
        <link href="http://arxiv.org/abs/2106.04004"/>
        <updated>2021-06-09T02:01:48.443Z</updated>
        <summary type="html"><![CDATA[A deep generative model that describes human motions can benefit a wide range
of fundamental computer vision and graphics tasks, such as providing robustness
to video-based human pose estimation, predicting complete body movements for
motion capture systems during occlusions, and assisting key frame animation
with plausible movements. In this paper, we present a method for learning
complex human motions independent of specific tasks using a combined global and
local latent space to facilitate coarse and fine-grained modeling.
Specifically, we propose a hierarchical motion variational autoencoder (HM-VAE)
that consists of a 2-level hierarchical latent space. While the global latent
space captures the overall global body motion, the local latent space enables
to capture the refined poses of the different body parts. We demonstrate the
effectiveness of our hierarchical motion variational autoencoder in a variety
of tasks including video-based human pose estimation, motion completion from
partial observations, and motion synthesis from sparse key-frames. Even though,
our model has not been trained for any of these tasks specifically, it provides
superior performance than task-specific alternatives. Our general-purpose human
motion prior model can fix corrupted human body animations and generate
complete movements from incomplete observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaman Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1"&gt;Ruben Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1"&gt;Duygu Ceylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhengfei Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yajie Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation. (arXiv:2106.04054v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04054</id>
        <link href="http://arxiv.org/abs/2106.04054"/>
        <updated>2021-06-09T02:01:48.436Z</updated>
        <summary type="html"><![CDATA[Weakly supervised semantic segmentation is receiving great attention due to
its low human annotation cost. In this paper, we aim to tackle bounding box
supervised semantic segmentation, i.e., training accurate semantic segmentation
models using bounding box annotations as supervision. To this end, we propose
Affinity Attention Graph Neural Network ($A^2$GNN). Following previous
practices, we first generate pseudo semantic-aware seeds, which are then formed
into semantic graphs based on our newly proposed affinity Convolutional Neural
Network (CNN). Then the built graphs are input to our $A^2$GNN, in which an
affinity attention layer is designed to acquire the short- and long- distance
information from soft graph edges to accurately propagate semantic labels from
the confident seeds to the unlabeled pixels. However, to guarantee the
precision of the seeds, we only adopt a limited number of confident pixel seed
labels for $A^2$GNN, which may lead to insufficient supervision for training.
To alleviate this issue, we further introduce a new loss function and a
consistency-checking mechanism to leverage the bounding box constraint, so that
more reliable guidance can be included for the model optimization. Experiments
show that our approach achieves new state-of-the-art performances on Pascal VOC
2012 datasets (val: 76.5\%, test: 75.2\%). More importantly, our approach can
be readily applied to bounding box supervised instance segmentation task or
other weakly supervised semantic segmentation tasks, with state-of-the-art or
comparable performance among almot all weakly supervised tasks on PASCAL VOC or
COCO dataset. Our source code will be available at
https://github.com/zbf1991/A2GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jianbo Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation. (arXiv:2106.04067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04067</id>
        <link href="http://arxiv.org/abs/2106.04067"/>
        <updated>2021-06-09T02:01:48.391Z</updated>
        <summary type="html"><![CDATA[Cross-resolution image alignment is a key problem in multiscale gigapixel
photography, which requires to estimate homography matrix using images with
large resolution gap. Existing deep homography methods concatenate the input
images or features, neglecting the explicit formulation of correspondences
between them, which leads to degraded accuracy in cross-resolution challenges.
In this paper, we consider the cross-resolution homography estimation as a
multimodal problem, and propose a local transformer network embedded within a
multiscale structure to explicitly learn correspondences between the multimodal
inputs, namely, input images with different resolutions. The proposed local
transformer adopts a local attention map specifically for each position in the
feature. By combining the local transformer with the multiscale structure, the
network is able to capture long-short range correspondences efficiently and
accurately. Experiments on both the MS-COCO dataset and the real-captured
cross-resolution dataset show that the proposed network outperforms existing
state-of-the-art feature-based and deep-learning-based homography estimation
methods, and is able to accurately align images under $10\times$ resolution
gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1"&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuemei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1"&gt;Lu Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoPtosis. (arXiv:2106.03905v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.03905</id>
        <link href="http://arxiv.org/abs/2106.03905"/>
        <updated>2021-06-09T02:01:48.373Z</updated>
        <summary type="html"><![CDATA[Blepharoptosis, or ptosis as it is more commonly referred to, is a condition
of the eyelid where the upper eyelid droops. The current diagnosis for ptosis
involves cumbersome manual measurements that are time-consuming and prone to
human error. In this paper, we present AutoPtosis, an artificial intelligence
based system with interpretable results for rapid diagnosis of ptosis. We
utilize a diverse dataset collected at the University of Illinois Hospital and
Health to successfully develop a robust deep learning model for prediction and
also develop a clinically inspired model that calculates the marginal reflex
distance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician
verified data that had an equal class balance. The proposed algorithm can help
in the rapid and timely diagnosis of ptosis, significantly reduce the burden on
the healthcare system, and save the patients and clinics valuable resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Aleem_A/0/1/0/all/0/1"&gt;Abdullah Aleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nallabothula_M/0/1/0/all/0/1"&gt;Manoj Prabhakar Nallabothula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Setabutr_P/0/1/0/all/0/1"&gt;Pete Setabutr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hallak_J/0/1/0/all/0/1"&gt;Joelle A. Hallak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_D/0/1/0/all/0/1"&gt;Darvin Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schema2QA: High-Quality and Low-Cost Q&A Agents for the Structured Web. (arXiv:2001.05609v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05609</id>
        <link href="http://arxiv.org/abs/2001.05609"/>
        <updated>2021-06-09T02:01:48.354Z</updated>
        <summary type="html"><![CDATA[Building a question-answering agent currently requires large annotated
datasets, which are prohibitively expensive. This paper proposes Schema2QA, an
open-source toolkit that can generate a Q&A system from a database schema
augmented with a few annotations for each field. The key concept is to cover
the space of possible compound queries on the database with a large number of
in-domain questions synthesized with the help of a corpus of generic query
templates. The synthesized data and a small paraphrase set are used to train a
novel neural network based on the BERT pretrained model. We use Schema2QA to
generate Q&A systems for five Schema.org domains, restaurants, people, movies,
books and music, and obtain an overall accuracy between 64% and 75% on
crowdsourced questions for these domains. Once annotations and paraphrases are
obtained for a Schema.org schema, no additional manual effort is needed to
create a Q&A agent for any website that uses the same schema. Furthermore, we
demonstrate that learning can be transferred from the restaurant to the hotel
domain, obtaining a 64% accuracy on crowdsourced questions with no manual
effort. Schema2QA achieves an accuracy of 60% on popular restaurant questions
that can be answered using Schema.org. Its performance is comparable to Google
Assistant, 7% lower than Siri, and 15% higher than Alexa. It outperforms all
these assistants by at least 18% on more complex, long-tail questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Silei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1"&gt;Giovanni Campagna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1"&gt;Monica S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding. (arXiv:2106.04053v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04053</id>
        <link href="http://arxiv.org/abs/2106.04053"/>
        <updated>2021-06-09T02:01:48.348Z</updated>
        <summary type="html"><![CDATA[In this paper, we are tackling the weakly-supervised referring expression
grounding task, for the localization of a referent object in an image according
to a query sentence, where the mapping between image regions and queries are
not available during the training stage. In traditional methods, an object
region that best matches the referring expression is picked out, and then the
query sentence is reconstructed from the selected region, where the
reconstruction difference serves as the loss for back-propagation. The existing
methods, however, conduct both the matching and the reconstruction
approximately as they ignore the fact that the matching correctness is unknown.
To overcome this limitation, a discriminative triad is designed here as the
basis to the solution, through which a query can be converted into one or
multiple discriminative triads in a very scalable way. Based on the
discriminative triad, we further propose the triad-level matching and
reconstruction modules which are lightweight yet effective for the
weakly-supervised training, making it three times lighter and faster than the
previous state-of-the-art methods. One important merit of our work is its
superior performance despite the simple and neat design. Specifically, the
proposed method achieves a new state-of-the-art accuracy when evaluated on
RefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is
4.17%, 4.08% and 7.8% higher than the previous one, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingjie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulermas_J/0/1/0/all/0/1"&gt;John Y. Goulermas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAiRE in DialDoc21: Data Augmentation for Information-Seeking Dialogue System. (arXiv:2106.03530v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03530</id>
        <link href="http://arxiv.org/abs/2106.03530"/>
        <updated>2021-06-09T02:01:48.343Z</updated>
        <summary type="html"><![CDATA[Information-seeking dialogue systems, including knowledge identification and
response generation, aim to respond to users with fluent, coherent, and
informative responses based on users' needs, which. To tackle this challenge,
we utilize data augmentation methods and several training techniques with the
pre-trained language models to learn a general pattern of the task and thus
achieve promising performance. In DialDoc21 competition, our system achieved
74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU
score in subtask 2. Empirical analysis is provided to explain the effectiveness
of our approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1"&gt;Andrea Madotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zihan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PANDORA Talks: Personality and Demographics on Reddit. (arXiv:2004.04460v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04460</id>
        <link href="http://arxiv.org/abs/2004.04460"/>
        <updated>2021-06-09T02:01:48.326Z</updated>
        <summary type="html"><![CDATA[Personality and demographics are important variables in social sciences,
while in NLP they can aid in interpretability and removal of societal biases.
However, datasets with both personality and demographic labels are scarce. To
address this, we present PANDORA, the first large-scale dataset of Reddit
comments labeled with three personality models (including the well-established
Big 5 model) and demographics (age, gender, and location) for more than 10k
users. We showcase the usefulness of this dataset on three experiments, where
we leverage the more readily available data from other personality models to
predict the Big 5 traits, analyze gender classification biases arising from
psycho-demographic variables, and carry out a confirmatory and exploratory
analysis based on psychological theories. Finally, we present benchmark
prediction models for all personality and demographic variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gjurkovic_M/0/1/0/all/0/1"&gt;Matej Gjurkovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karan_M/0/1/0/all/0/1"&gt;Mladen Karan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vukojevic_I/0/1/0/all/0/1"&gt;Iva Vukojevi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1"&gt;Mihaela Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recombine and Resample Data for Compositional Generalization. (arXiv:2010.03706v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03706</id>
        <link href="http://arxiv.org/abs/2010.03706"/>
        <updated>2021-06-09T02:01:48.320Z</updated>
        <summary type="html"><![CDATA[Flexible neural sequence models outperform grammar- and automaton-based
counterparts on a variety of tasks. However, neural models perform poorly in
settings requiring compositional generalization beyond the training data --
particularly to rare or unseen subsequences. Past work has found symbolic
scaffolding (e.g. grammars or automata) essential in these settings. We
describe R&R, a learned data augmentation scheme that enables a large category
of compositional generalizations without appeal to latent symbolic structure.
R&R has two components: recombination of original training examples via a
prototype-based generative model and resampling of generated examples to
encourage extrapolation. Training an ordinary neural sequence model on a
dataset augmented with recombined and resampled examples significantly improves
generalization in two language processing problems -- instruction following
(SCAN) and morphological analysis (SIGMORPHON 2018) -- where R&R enables
learning of new constructions and tenses from as few as eight initial examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1"&gt;Afra Feyza Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting the Unknown from Long Math Problems. (arXiv:2103.12048v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12048</id>
        <link href="http://arxiv.org/abs/2103.12048"/>
        <updated>2021-06-09T02:01:48.314Z</updated>
        <summary type="html"><![CDATA[In problem solving, understanding the problem that one seeks to solve is an
essential initial step. In this paper, we propose computational methods for
facilitating problem understanding through the task of recognizing the unknown
in specifications of long Math problems. We focus on the topic of Probability.
Our experimental results show that learning models yield strong results on the
task, a promising first step towards human interpretable, modular approaches to
understanding long Math problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakashole_N/0/1/0/all/0/1"&gt;Ndapa Nakashole&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating label suggestions for opinion mining in German Covid-19 social media. (arXiv:2105.12980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12980</id>
        <link href="http://arxiv.org/abs/2105.12980"/>
        <updated>2021-06-09T02:01:48.308Z</updated>
        <summary type="html"><![CDATA[This work investigates the use of interactively updated label suggestions to
improve upon the efficiency of gathering annotations on the task of opinion
mining in German Covid-19 social media data. We develop guidelines to conduct a
controlled annotation study with social science students and find that
suggestions from a model trained on a small, expert-annotated dataset already
lead to a substantial improvement - in terms of inter-annotator agreement(+.14
Fleiss' $\kappa$) and annotation quality - compared to students that do not
receive any label suggestions. We further find that label suggestions from
interactively trained models do not lead to an improvement over suggestions
from a static model. Nonetheless, our analysis of suggestion bias shows that
annotators remain capable of reflecting upon the suggested label in general.
Finally, we confirm the quality of the annotated data in transfer learning
experiments between different annotator groups. To facilitate further research
in opinion mining on social media data, we release our collected data
consisting of 200 expert and 2,785 student annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1"&gt;Tilman Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Ung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1"&gt;Christina Viehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_M/0/1/0/all/0/1"&gt;Marcus Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quiring_O/0/1/0/all/0/1"&gt;Oliver Quiring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel View Video Prediction Using a Dual Representation. (arXiv:2106.03956v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03956</id>
        <link href="http://arxiv.org/abs/2106.03956"/>
        <updated>2021-06-09T02:01:48.302Z</updated>
        <summary type="html"><![CDATA[We address the problem of novel view video prediction; given a set of input
video clips from a single/multiple views, our network is able to predict the
video from a novel view. The proposed approach does not require any priors and
is able to predict the video from wider angular distances, upto 45 degree, as
compared to the recent studies predicting small variations in viewpoint.
Moreover, our method relies only onRGB frames to learn a dual representation
which is used to generate the video from a novel viewpoint. The dual
representation encompasses a view-dependent and a global representation which
incorporates complementary details to enable novel view video prediction. We
demonstrate the effectiveness of our framework on two real world datasets:
NTU-RGB+D and CMU Panoptic. A comparison with the State-of-the-art novel view
video prediction methods shows an improvement of 26.1% in SSIM, 13.6% in PSNR,
and 60% inFVD scores without using explicit priors from target views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shiraz_S/0/1/0/all/0/1"&gt;Sarah Shiraz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regmi_K/0/1/0/all/0/1"&gt;Krishna Regmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1"&gt;Shruti Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1"&gt;Yogesh S. Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Volumetric Image Segmentation with Deformed Templates. (arXiv:2106.03987v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03987</id>
        <link href="http://arxiv.org/abs/2106.03987"/>
        <updated>2021-06-09T02:01:48.297Z</updated>
        <summary type="html"><![CDATA[There are many approaches that use weak-supervision to train networks to
segment 2D images. By contrast, existing 3D approaches rely on full-supervision
of a subset of 2D slices of the 3D image volume. In this paper, we propose an
approach that is truly weakly-supervised in the sense that we only need to
provide a sparse set of 3D point on the surface of target objects, an easy task
that can be quickly done. We use the 3D points to deform a 3D template so that
it roughly matches the target object outlines and we introduce an architecture
that exploits the supervision provided by coarse template to train a network to
find accurate boundaries.

We evaluate the performance of our approach on Computed Tomography (CT),
Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets.
We will show that it outperforms a more traditional approach to
weak-supervision in 3D at a reduced supervision cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Reordering for Modeling Latent Alignments in Sequence Transduction. (arXiv:2106.03257v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03257</id>
        <link href="http://arxiv.org/abs/2106.03257"/>
        <updated>2021-06-09T02:01:48.280Z</updated>
        <summary type="html"><![CDATA[Despite success in many domains, neural models struggle in settings where
train and test examples are drawn from different distributions. In particular,
in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail
to generalize systematically, i.e., interpret sentences representing novel
combinations of concepts (e.g., text segments) seen in training. Traditional
grammar formalisms excel in such settings by implicitly encoding alignments
between input and output segments, but are hard to scale and maintain. Instead
of engineering a grammar, we directly model segment-to-segment alignments as
discrete structured latent variables within a neural seq2seq model. To
efficiently explore the large space of alignments, we introduce a reorder-first
align-later framework whose central component is a neural reordering module
producing {\it separable} permutations. We present an efficient dynamic
programming algorithm performing exact marginal inference of separable
permutations, and, thus, enabling end-to-end differentiable training of our
model. The resulting seq2seq model exhibits better systematic generalization
than standard models on synthetic problems and NLP tasks (i.e., semantic
parsing and machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translate, then Parse! A strong baseline for Cross-Lingual AMR Parsing. (arXiv:2106.04565v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04565</id>
        <link href="http://arxiv.org/abs/2106.04565"/>
        <updated>2021-06-09T02:01:48.275Z</updated>
        <summary type="html"><![CDATA[In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers
develop models that project sentences from various languages onto their AMRs to
capture their essential semantic structures: given a sentence in any language,
we aim to capture its core semantic content through concepts connected by
manifold types of semantic relations. Methods typically leverage large silver
training data to learn a single model that is able to project non-English
sentences to AMRs. However, we find that a simple baseline tends to be
over-looked: translating the sentences to English and projecting their AMR with
a monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this
simple two-step base-line, and enhance it with a strong NMT system and a strong
AMR parser. Our experiments show that T+P outperforms a recent state-of-the-art
system across all tested languages: German, Italian, Spanish and Mandarin with
+14.6, +12.6, +14.3 and +16.0 Smatch points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uhrig_S/0/1/0/all/0/1"&gt;Sarah Uhrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Y/0/1/0/all/0/1"&gt;Yoalli Rezepka Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1"&gt;Juri Opitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational AutoEncoder for Reference based Image Super-Resolution. (arXiv:2106.04090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04090</id>
        <link href="http://arxiv.org/abs/2106.04090"/>
        <updated>2021-06-09T02:01:48.268Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel reference based image super-resolution
approach via Variational AutoEncoder (RefVAE). Existing state-of-the-art
methods mainly focus on single image super-resolution which cannot perform well
on large upsampling factors, e.g., 8$\times$. We propose a reference based
image super-resolution, for which any arbitrary image can act as a reference
for super-resolution. Even using random map or low-resolution image itself, the
proposed RefVAE can transfer the knowledge from the reference to the
super-resolved images. Depending upon different references, the proposed method
can generate different versions of super-resolved images from a hidden
super-resolution space. Besides using different datasets for some standard
evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space
challenge and have provided results of the randomness evaluation of our
approach. Compared to other state-of-the-art methods, our approach achieves
higher diverse scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi-Song Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siu_W/0/1/0/all/0/1"&gt;Wan-Chi Siu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li-Wen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpaceMeshLab: Spatial Context Memoization and Meshgrid Atrous Convolution Consensus for Semantic Segmentation. (arXiv:2106.04025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04025</id>
        <link href="http://arxiv.org/abs/2106.04025"/>
        <updated>2021-06-09T02:01:48.263Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation networks adopt transfer learning from image
classification networks which occurs a shortage of spatial context information.
For this reason, we propose Spatial Context Memoization (SpaM), a bypassing
branch for spatial context by retaining the input dimension and constantly
communicating its spatial context and rich semantic information mutually with
the backbone network. Multi-scale context information for semantic segmentation
is crucial for dealing with diverse sizes and shapes of target objects in the
given scene. Conventional multi-scale context scheme adopts multiple effective
receptive fields by multiple dilation rates or pooling operations, but often
suffer from misalignment problem with respect to the target pixel. To this end,
we propose Meshgrid Atrous Convolution Consensus (MetroCon^2) which brings
multi-scale scheme into fine-grained multi-scale object context using
convolutions with meshgrid-like scattered dilation rates. SpaceMeshLab
(ResNet-101 + SpaM + MetroCon^2) achieves 82.0% mIoU in Cityscapes test and
53.5% mIoU on Pascal-Context validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinseong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daijin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-MLP: Node Classification without Message Passing in Graph. (arXiv:2106.04051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04051</id>
        <link href="http://arxiv.org/abs/2106.04051"/>
        <updated>2021-06-09T02:01:48.257Z</updated>
        <summary type="html"><![CDATA[Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing
with non-Euclidean structural data. Both spatial-based and spectral-based GNNs
are relying on adjacency matrix to guide message passing among neighbors during
feature aggregation. Recent works have mainly focused on powerful message
passing modules, however, in this paper, we show that none of the message
passing modules is necessary. Instead, we propose a pure
multilayer-perceptron-based framework, Graph-MLP with the supervision signal
leveraging graph structure, which is sufficient for learning discriminative
node representation. In model-level, Graph-MLP only includes multi-layer
perceptrons, activation function, and layer normalization. In the loss level,
we design a neighboring contrastive (NContrast) loss to bridge the gap between
GNNs and MLPs by utilizing the adjacency information implicitly. This design
allows our model to be lighter and more robust when facing large-scale graph
data and corrupted adjacency information. Extensive experiments prove that even
without adjacency information in testing phase, our framework can still reach
comparable and even superior performance against the state-of-the-art models in
the graph node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1"&gt;Haoxuan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhecan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer. (arXiv:2106.04095v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04095</id>
        <link href="http://arxiv.org/abs/2106.04095"/>
        <updated>2021-06-09T02:01:48.241Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (Re-ID) is a challenging task as persons
are frequently occluded by various obstacles or other persons, especially in
the crowd scenario. To address these issues, we propose a novel end-to-end
Part-Aware Transformer (PAT) for occluded person Re-ID through diverse part
discovery via a transformer encoderdecoder architecture, including a pixel
context based transformer encoder and a part prototype based transformer
decoder. The proposed PAT model enjoys several merits. First, to the best of
our knowledge, this is the first work to exploit the transformer
encoder-decoder architecture for occluded person Re-ID in a unified deep model.
Second, to learn part prototypes well with only identity labels, we design two
effective mechanisms including part diversity and part discriminability.
Consequently, we can achieve diverse part discovery for occluded person Re-ID
in a weakly supervised manner. Extensive experimental results on six
challenging benchmarks for three tasks (occluded, partial and holistic Re-ID)
demonstrate that our proposed PAT performs favorably against stat-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yulin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianzhu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Salvage of Supervision in Weakly Supervised Detection. (arXiv:2106.04073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04073</id>
        <link href="http://arxiv.org/abs/2106.04073"/>
        <updated>2021-06-09T02:01:48.235Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object detection (WSOD) has recently attracted much
attention. However, the method, performance and speed gaps between WSOD and
fully supervised detection prevent WSOD from being applied in real-world tasks.
To bridge the gaps, this paper proposes a new framework, Salvage of Supervision
(SoS), with the key idea being to harness every potentially useful supervisory
signal in WSOD: the weak image-level labels, the pseudo-labels, and the power
of semi-supervised object detection. This paper shows that each type of
supervisory signal brings in notable improvements, outperforms existing WSOD
methods (which mainly use only the weak labels) by large margins. The proposed
SoS-WSOD method achieves 64.4 $m\text{AP}_{50}$ on VOC2007, 61.9
$m\text{AP}_{50}$ on VOC2012 and 16.4 $m\text{AP}_{50:95}$ on MS-COCO, and also
has fast inference speed. Ablations and visualization further verify the
effectiveness of SoS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sui_L/0/1/0/all/0/1"&gt;Lin Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen-Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Learning for Knowledge Distillation. (arXiv:2106.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04570</id>
        <link href="http://arxiv.org/abs/2106.04570"/>
        <updated>2021-06-09T02:01:48.229Z</updated>
        <summary type="html"><![CDATA[We present Meta Learning for Knowledge Distillation (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models. The code is available at
https://github.com/JetRunner/MetaDistil]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Structure-from-Motion through Tightly-Coupled Depth and Egomotion Networks. (arXiv:2106.04007v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04007</id>
        <link href="http://arxiv.org/abs/2106.04007"/>
        <updated>2021-06-09T02:01:48.222Z</updated>
        <summary type="html"><![CDATA[Much recent literature has formulated structure-from-motion (SfM) as a
self-supervised learning problem where the goal is to jointly learn neural
network models of depth and egomotion through view synthesis. Herein, we
address the open problem of how to optimally couple the depth and egomotion
network components. Toward this end, we introduce several notions of coupling,
categorize existing approaches, and present a novel tightly-coupled approach
that leverages the interdependence of depth and egomotion at training and at
inference time. Our approach uses iterative view synthesis to recursively
update the egomotion network input, permitting contextual information to be
passed between the components without explicit weight sharing. Through
substantial experiments, we demonstrate that our approach promotes consistency
between the depth and egomotion predictions at test time, improves
generalization on new data, and leads to state-of-the-art accuracy on indoor
and outdoor depth and egomotion evaluation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1"&gt;Brandon Wagstaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peretroukhin_V/0/1/0/all/0/1"&gt;Valentin Peretroukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jonathan Kelly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge. (arXiv:2104.02704v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02704</id>
        <link href="http://arxiv.org/abs/2104.02704"/>
        <updated>2021-06-09T02:01:48.216Z</updated>
        <summary type="html"><![CDATA[Cant is important for understanding advertising, comedies and dog-whistle
politics. However, computational research on cant is hindered by a lack of
available datasets. In this paper, we propose a large and diverse Chinese
dataset for creating and understanding cant from a computational linguistics
perspective. We formulate a task for cant understanding and provide both
quantitative and qualitative analysis for tested word embedding similarity and
pretrained language models. Experiments suggest that such a task requires deep
language understanding, common sense, and world knowledge and thus can be a
good testbed for pretrained language models and help models perform better on
other tasks. The code is available at https://github.com/JetRunner/dogwhistle.
The data and leaderboard are available at
https://competitions.codalab.org/competitions/30451.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-09T02:01:48.210Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system under the team name
Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We
also participate in the textual-only subtask of the same language pair for
which we use mBART, a pretrained multilingual sequence-to-sequence model. For
multimodal translation, we propose to enhance the textual input by bringing the
visual information to a textual domain by extracting object tags from the
image. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the multimodal task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning. (arXiv:2105.03791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03791</id>
        <link href="http://arxiv.org/abs/2105.03791"/>
        <updated>2021-06-09T02:01:48.193Z</updated>
        <summary type="html"><![CDATA[Transfer learning has become the dominant paradigm for many natural language
processing tasks. In addition to models being pretrained on large datasets,
they can be further trained on intermediate (supervised) tasks that are similar
to the target task. For small Natural Language Inference (NLI) datasets,
language modelling is typically followed by pretraining on a large (labelled)
NLI dataset before fine-tuning with each NLI subtask. In this work, we explore
Gradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used
Multi-Layer Perceptron (MLP) classification head. GBDTs have desirable
properties such as good performance on dense, numerical features and are
effective where the ratio of the number of samples w.r.t the number of features
is low. We then introduce FreeGBDT, a method of fitting a GBDT head on the
features computed during fine-tuning to increase performance without additional
computation by the neural network. We demonstrate the effectiveness of our
method on several NLI datasets using a strong baseline model (RoBERTa-large
with MNLI pretraining). The FreeGBDT shows a consistent improvement over the
MLP classification head.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minixhofer_B/0/1/0/all/0/1"&gt;Benjamin Minixhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1"&gt;Milan Gritta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1"&gt;Ignacio Iacobacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subject-Independent Brain-Computer Interface for Decoding High-Level Visual Imagery Tasks. (arXiv:2106.04026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04026</id>
        <link href="http://arxiv.org/abs/2106.04026"/>
        <updated>2021-06-09T02:01:48.187Z</updated>
        <summary type="html"><![CDATA[Brain-computer interface (BCI) is used for communication between humans and
devices by recognizing status and intention of humans. Communication between
humans and a drone using electroencephalogram (EEG) signals is one of the most
challenging issues in the BCI domain. In particular, the control of drone
swarms (the direction and formation) has more advantages compared to the
control of a drone. The visual imagery (VI) paradigm is that subjects visually
imagine specific objects or scenes. Reduction of the variability among EEG
signals of subjects is essential for practical BCI-based systems. In this
study, we proposed the subepoch-wise feature encoder (SEFE) to improve the
performances in the subject-independent tasks by using the VI dataset. This
study is the first attempt to demonstrate the possibility of generalization
among subjects in the VI-based BCI. We used the leave-one-subject-out
cross-validation for evaluating the performances. We obtained higher
performances when including our proposed module than excluding our proposed
module. The DeepConvNet with SEFE showed the highest performance of 0.72 among
six different decoding models. Hence, we demonstrated the feasibility of
decoding the VI dataset in the subject-independent task with robust
performances by using our proposed module.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dong-Kyun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Ji-Hoon Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03932</id>
        <link href="http://arxiv.org/abs/2106.03932"/>
        <updated>2021-06-09T02:01:48.181Z</updated>
        <summary type="html"><![CDATA[Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1"&gt;Okan K&amp;#xf6;p&amp;#xfc;kl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1"&gt;Maja Taseska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bh\=a$\unicode{x1E63}$\=acitra: Visualising the dialect geography of South Asia. (arXiv:2105.14082v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14082</id>
        <link href="http://arxiv.org/abs/2105.14082"/>
        <updated>2021-06-09T02:01:48.176Z</updated>
        <summary type="html"><![CDATA[We present Bh\=a$\unicode{x1E63}$\=acitra, a dialect mapping system for South
Asia built on a database of linguistic studies of languages of the region
annotated for topic and location data. We analyse language coverage and look
towards applications to typology by visualising example datasets. The
application is not only meant to be useful for feature mapping, but also serves
as a new kind of interactive bibliography for linguists of South Asian
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Aryaman Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farris_A/0/1/0/all/0/1"&gt;Adam Farris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+R_G/0/1/0/all/0/1"&gt;Gopalakrishnan R&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Samopriya Basu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEAR: A Simple Lightweight Method to Rank Architectures. (arXiv:2106.04010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04010</id>
        <link href="http://arxiv.org/abs/2106.04010"/>
        <updated>2021-06-09T02:01:48.160Z</updated>
        <summary type="html"><![CDATA[The fundamental problem in Neural Architecture Search (NAS) is to efficiently
find high-performing architectures from a given search space. We propose a
simple but powerful method which we call FEAR, for ranking architectures in any
search space. FEAR leverages the viewpoint that neural networks are powerful
non-linear feature extractors. First, we train different architectures in the
search space to the same training or validation error. Then, we compare the
usefulness of the features extracted by each architecture. We do so with a
quick training keeping most of the architecture frozen. This gives fast
estimates of the relative performance. We validate FEAR on Natsbench topology
search space on three different datasets against competing baselines and show
strong ranking correlation especially compared to recently proposed zero-cost
methods. FEAR particularly excels at ranking high-performance architectures in
the search space. When used in the inner loop of discrete search algorithms
like random search, FEAR can cut down the search time by approximately 2.4X
without losing accuracy. We additionally empirically study very recently
proposed zero-cost measures for ranking and find that they breakdown in ranking
performance as training proceeds and also that data-agnostic ranking scores
which ignore the dataset do not generalize across dissimilar datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1"&gt;Debadeepta Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shital Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;Sebastien Bubeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Mediated, Object-Centric Representation Learning. (arXiv:2012.15814v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15814</id>
        <link href="http://arxiv.org/abs/2012.15814"/>
        <updated>2021-06-09T02:01:48.144Z</updated>
        <summary type="html"><![CDATA[We present Language-mediated, Object-centric Representation Learning (LORL),
a paradigm for learning disentangled, object-centric scene representations from
vision and language. LORL builds upon recent advances in unsupervised object
discovery and segmentation, notably MONet and Slot Attention. While these
algorithms learn an object-centric representation just by reconstructing the
input image, LORL enables them to further learn to associate the learned
representations to concepts, i.e., words for object categories, properties, and
spatial relationships, from language input. These object-centric concepts
derived from language facilitate the learning of object-centric
representations. LORL can be integrated with various unsupervised object
discovery algorithms that are language-agnostic. Experiments show that the
integration of LORL consistently improves the performance of unsupervised
object discovery methods on two datasets via the help of language. We also show
that concepts learned by LORL, in conjunction with object discovery methods,
aid downstream tasks such as referring expression comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04066</id>
        <link href="http://arxiv.org/abs/2106.04066"/>
        <updated>2021-06-09T02:01:48.139Z</updated>
        <summary type="html"><![CDATA[Deep Generative Models (DGMs) are known for their superior capability in
generating realistic data. Extending purely data-driven approaches, recent
specialized DGMs may satisfy additional controllable requirements such as
embedding a traffic sign in a driving scene, by manipulating patterns
\textit{implicitly} in the neuron or feature level. In this paper, we introduce
a novel method to incorporate domain knowledge \textit{explicitly} in the
generation process to achieve semantically controllable scene generation. We
categorize our knowledge into two types to be consistent with the composition
of natural scenes, where the first type represents the property of objects and
the second type represents the relationship among objects. We then propose a
tree-structured generative model to learn complex scene representation, whose
nodes and edges are naturally corresponding to the two types of knowledge
respectively. Knowledge can be explicitly integrated to enable semantically
controllable scene generation by imposing semantic rules on properties of nodes
and edges in the tree structure. We construct a synthetic example to illustrate
the controllability and explainability of our method in a clean setting. We
further extend the synthetic example to realistic autonomous vehicle driving
environments and conduct extensive experiments to show that our method
efficiently identifies adversarial traffic scenes against different
state-of-the-art 3D point cloud segmentation models satisfying the traffic
rules specified as the explicit knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenhao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1"&gt;Kim Ji Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04024</id>
        <link href="http://arxiv.org/abs/2106.04024"/>
        <updated>2021-06-09T02:01:48.133Z</updated>
        <summary type="html"><![CDATA[We develop a framework for comparing data manifolds, aimed, in particular,
towards the evaluation of deep generative models. We describe a novel tool,
Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional
space, tracks multiscale topology spacial discrepancies between manifolds on
which the distributions are concentrated. Based on the Cross-Barcode, we
introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it
to assess the performance of deep generative models in various domains: images,
3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,
CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate
that the MTop-Divergence accurately detects various degrees of mode-dropping,
intra-mode collapse, mode invention, and image disturbance. Our algorithm
scales well (essentially linearly) with the increase of the dimension of the
ambient high-dimensional space. It is one of the first TDA-based practical
methodologies that can be applied universally to datasets of different sizes
and dimensions, including the ones on which the most recent GANs in the visual
domain are trained. The proposed method is domain agnostic and does not rely on
pre-trained networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1"&gt;Serguei Barannikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1"&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1"&gt;Grigorii Sotnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimbach_E/0/1/0/all/0/1"&gt;Ekaterina Trimbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bangla Natural Language Processing: A Comprehensive Review of Classical, Machine Learning, and Deep Learning Based Methods. (arXiv:2105.14875v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14875</id>
        <link href="http://arxiv.org/abs/2105.14875"/>
        <updated>2021-06-09T02:01:48.127Z</updated>
        <summary type="html"><![CDATA[The Bangla language is the seventh most spoken language, with 265 million
native and non-native speakers worldwide. However, English is the predominant
language for online resources and technical knowledge, journals, and
documentation. Consequently, many Bangla-speaking people, who have limited
command of English, face hurdles to utilize English resources. To bridge the
gap between limited support and increasing demand, researchers conducted many
experiments and developed valuable tools and techniques to create and process
Bangla language materials. Many efforts are also ongoing to make it easy to use
the Bangla language in the online and technical domains. There are some review
papers to understand the past, previous, and future Bangla Natural Language
Processing (BNLP) trends. The studies are mainly concentrated on the specific
domains of BNLP, such as sentiment analysis, speech recognition, optical
character recognition, and text summarization. There is an apparent scarcity of
resources that contain a comprehensive study of the recent BNLP tools and
methods. Therefore, in this paper, we present a thorough review of 71 BNLP
research papers and categorize them into 11 categories, namely Information
Extraction, Machine Translation, Named Entity Recognition, Parsing, Parts of
Speech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake
Detection, Text Summarization, Word Sense Disambiguation, and Speech Processing
and Recognition. We study articles published between 1999 to 2021, and 50% of
the papers were published after 2015. We discuss Classical, Machine Learning
and Deep Learning approaches with different datasets while addressing the
limitations and current and future trends of the BNLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1"&gt;Ovishake Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Mohtasim Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;MD. Nazrul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1"&gt;Jakaria Rabbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;MD. Kamrul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baz_M/0/1/0/all/0/1"&gt;Mohammed Baz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masud_M/0/1/0/all/0/1"&gt;Mehedi Masud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1"&gt;Md. Abdul Awal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1"&gt;Awal Ahmed Fime&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Md. Tahmid Hasan Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1"&gt;Delowar Sikder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1"&gt;MD. Akil Raihan Iftee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I-BERT: Integer-only BERT Quantization. (arXiv:2101.01321v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01321</id>
        <link href="http://arxiv.org/abs/2101.01321"/>
        <updated>2021-06-09T02:01:48.121Z</updated>
        <summary type="html"><![CDATA[Transformer based models, like BERT and RoBERTa, have achieved
state-of-the-art results in many Natural Language Processing tasks. However,
their memory footprint, inference latency, and power consumption are
prohibitive efficient inference at the edge, and even at the data center. While
quantization can be a viable solution for this, previous work on quantizing
Transformer based models use floating-point arithmetic during inference, which
cannot efficiently utilize integer-only logical units such as the recent Turing
Tensor Cores, or traditional integer-only ARM processors. In this work, we
propose I-BERT, a novel quantization scheme for Transformer based models that
quantizes the entire inference with integer-only arithmetic. Based on
lightweight integer-only approximation methods for nonlinear operations, e.g.,
GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end
integer-only BERT inference without any floating point calculation. We evaluate
our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that
for both cases, I-BERT achieves similar (and slightly higher) accuracy as
compared to the full-precision baseline. Furthermore, our preliminary
implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4
GPU system as compared to FP32 inference. The framework has been developed in
PyTorch and has been open-sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexical Semantic Recognition. (arXiv:2004.15008v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.15008</id>
        <link href="http://arxiv.org/abs/2004.15008"/>
        <updated>2021-06-09T02:01:48.103Z</updated>
        <summary type="html"><![CDATA[In lexical semantics, full-sentence segmentation and segment labeling of
various phenomena are generally treated separately, despite their
interdependence. We hypothesize that a unified lexical semantic recognition
task is an effective way to encapsulate previously disparate styles of
annotation, including multiword expression identification / classification and
supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence
tagger and evaluate its performance along various axes of annotation. As the
label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally
evaluate how well the model generalizes to those test sets, finding that it
approaches or surpasses existing models despite training only on STREUSLE. Our
work also establishes baseline models and evaluation metrics for integrated and
accurate modeling of lexical semantics, facilitating future work in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nelson F. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kranzlein_M/0/1/0/all/0/1"&gt;Michael Kranzlein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nathan Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Itihasa: A large-scale corpus for Sanskrit to English translation. (arXiv:2106.03269v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03269</id>
        <link href="http://arxiv.org/abs/2106.03269"/>
        <updated>2021-06-09T02:01:48.097Z</updated>
        <summary type="html"><![CDATA[This work introduces Itihasa, a large-scale translation dataset containing
93,000 pairs of Sanskrit shlokas and their English translations. The shlokas
are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We
first describe the motivation behind the curation of such a dataset and follow
up with empirical analysis to bring out its nuances. We then benchmark the
performance of standard translation models on this corpus and show that even
state-of-the-art transformer architectures perform poorly, emphasizing the
complexity of the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1"&gt;Rahul Aralikatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1"&gt;Miryam de Lhoneux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1"&gt;Anoop Kunchukuttan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1"&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Multi-scale Fusion Network for RGB-D Salient Object Detection. (arXiv:2106.03941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03941</id>
        <link href="http://arxiv.org/abs/2106.03941"/>
        <updated>2021-06-09T02:01:48.092Z</updated>
        <summary type="html"><![CDATA[Salient object detection(SOD) aims at locating the most significant object
within a given image. In recent years, great progress has been made in applying
SOD on many vision tasks. The depth map could provide additional spatial prior
and boundary cues to boost the performance. Combining the depth information
with image data obtained from standard visual cameras has been widely used in
recent SOD works, however, introducing depth information in a suboptimal fusion
strategy may have negative influence in the performance of SOD. In this paper,
we discuss about the advantages of the so-called progressive multi-scale fusion
method and propose a mask-guided feature aggregation module(MGFA). The proposed
framework can effectively combine the two features of different modalities and,
furthermore, alleviate the impact of erroneous depth features, which are
inevitably caused by the variation of depth quality. We further introduce a
mask-guided refinement module(MGRM) to complement the high-level semantic
features and reduce the irrelevant features from multi-scale fusion, leading to
an overall refinement of detection. Experiments on five challenging benchmarks
demonstrate that the proposed method outperforms 11 state-of-the-art methods
under different evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1"&gt;Guangyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yanchu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1"&gt;Tianhong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1"&gt;Tania Stathaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12002</id>
        <link href="http://arxiv.org/abs/2105.12002"/>
        <updated>2021-06-09T02:01:48.084Z</updated>
        <summary type="html"><![CDATA[The Lottery Ticket Hypothesis suggests that an over-parametrized network
consists of ``lottery tickets'', and training a certain collection of them
(i.e., a subnetwork) can match the performance of the full model. In this
paper, we study such a collection of tickets, which is referred to as ``winning
tickets'', in extremely over-parametrized models, e.g., pre-trained language
models. We observe that at certain compression ratios, the generalization
performance of the winning tickets can not only match but also exceed that of
the full model. In particular, we observe a phase transition phenomenon: As the
compression ratio increases, generalization performance of the winning tickets
first improves then deteriorates after a certain threshold. We refer to the
tickets on the threshold as ``super tickets''. We further show that the phase
transition is task and model dependent -- as the model size becomes larger and
the training data set becomes smaller, the transition becomes more pronounced.
Our experiments on the GLUE benchmark show that the super tickets improve
single task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on
BERT-large, in terms of task-average score. We also demonstrate that adaptively
sharing the super tickets across tasks benefits multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Simiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction. (arXiv:2106.03518v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03518</id>
        <link href="http://arxiv.org/abs/2106.03518"/>
        <updated>2021-06-09T02:01:48.078Z</updated>
        <summary type="html"><![CDATA[The Emotion Cause Extraction (ECE)} task aims to identify clauses which
contain emotion-evoking information for a particular emotion expressed in text.
We observe that a widely-used ECE dataset exhibits a bias that the majority of
annotated cause clauses are either directly before their associated emotion
clauses or are the emotion clauses themselves. Existing models for ECE tend to
explore such relative position information and suffer from the dataset bias. To
investigate the degree of reliance of existing ECE models on clause relative
positions, we propose a novel strategy to generate adversarial examples in
which the relative position information is no longer the indicative feature of
cause clauses. We test the performance of existing models on such adversarial
examples and observe a significant performance drop. To address the dataset
bias, we propose a novel graph-based method to explicitly model the emotion
triggering paths by leveraging the commonsense knowledge to enhance the
semantic dependencies between a candidate clause and an emotion clause.
Experimental results show that our proposed approach performs on par with the
existing state-of-the-art methods on the original ECE dataset, and is more
robust against adversarial attacks compared to existing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hanqi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1"&gt;Lin Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1"&gt;Gabriele Pergola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-09T02:01:48.072Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Temperature Matters in Abstractive Summarization Distillation. (arXiv:2106.03441v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03441</id>
        <link href="http://arxiv.org/abs/2106.03441"/>
        <updated>2021-06-09T02:01:48.048Z</updated>
        <summary type="html"><![CDATA[Recent progress of abstractive text summarization largely relies on large
pre-trained sequence-to-sequence Transformer models, which are computationally
expensive. This paper aims to distill these large models into smaller ones for
faster inference and minimal performance loss. Pseudo-labeling based methods
are popular in sequence-to-sequence model distillation. In this paper, we find
simply manipulating attention temperatures in Transformers can make pseudo
labels easier to learn for student models. Our experiments on three
summarization datasets show our proposed method consistently improves over
vanilla pseudo-labeling based methods. We also find that both the pseudo labels
and summaries produced by our students are shorter and more abstractive. We
will make our code and models publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingxing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. (arXiv:2009.07526v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07526</id>
        <link href="http://arxiv.org/abs/2009.07526"/>
        <updated>2021-06-09T02:01:48.042Z</updated>
        <summary type="html"><![CDATA[Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2. (arXiv:2004.02251v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02251</id>
        <link href="http://arxiv.org/abs/2004.02251"/>
        <updated>2021-06-09T02:01:48.036Z</updated>
        <summary type="html"><![CDATA[The semantics of a text is manifested not only by what is read, but also by
what is not read. In this article, we will study how the implicit "not read"
information such as end-of-paragraph (\eop) and end-of-sequence (\eos) affect
the quality of text generation. Specifically, we find that the pre-trained
language model GPT2 can generate better continuations by learning to generate
the \eop in the fine-tuning stage. Experimental results on English story
generation show that \eop can lead to higher BLEU score and lower \eos
perplexity. We also conduct experiments on a self-collected Chinese essay
dataset with Chinese-GPT2, a character level LM without \eop or \eos during
pre-training. Experimental results show that the Chinese GPT2 can generate
better essay endings with \eop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;He Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1"&gt;Luchen Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1"&gt;Kun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suicidal Ideation and Mental Disorder Detection with Attentive Relation Networks. (arXiv:2004.07601v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07601</id>
        <link href="http://arxiv.org/abs/2004.07601"/>
        <updated>2021-06-09T02:01:48.030Z</updated>
        <summary type="html"><![CDATA[Mental health is a critical issue in modern society, and mental disorders
could sometimes turn to suicidal ideation without effective treatment. Early
detection of mental disorders and suicidal ideation from social content
provides a potential way for effective social intervention. However,
classifying suicidal ideation and other mental disorders is challenging as they
share similar patterns in language usage and sentimental polarity. This paper
enhances text representation with lexicon-based sentiment scores and latent
topics and proposes using relation networks to detect suicidal ideation and
mental disorders with related risk indicators. The relation module is further
equipped with the attention mechanism to prioritize more critical relational
features. Through experiments on three real-world datasets, our model
outperforms most of its counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaoxiong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data. (arXiv:2010.04806v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04806</id>
        <link href="http://arxiv.org/abs/2010.04806"/>
        <updated>2021-06-09T02:01:48.024Z</updated>
        <summary type="html"><![CDATA[We propose AutoQA, a methodology and toolkit to generate semantic parsers
that answer questions on databases, with no manual effort. Given a database
schema and its data, AutoQA automatically generates a large set of high-quality
questions for training that covers different database operations. It uses
automatic paraphrasing combined with template-based parsing to find alternative
expressions of an attribute in different parts of speech. It also uses a novel
filtered auto-paraphraser to generate correct paraphrases of entire sentences.
We apply AutoQA to the Schema2QA dataset and obtain an average logical form
accuracy of 62.9% when tested on natural questions, which is only 6.4% lower
than a model trained with expert natural language annotations and paraphrase
data collected from crowdworkers. To demonstrate the generality of AutoQA, we
also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy,
16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower
than the same model trained with human data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Silei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1"&gt;Sina J. Semnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1"&gt;Giovanni Campagna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1"&gt;Monica S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-09T02:01:48.008Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Transformation Learning for Robust Out-of-Distribution Detection. (arXiv:2106.03899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03899</id>
        <link href="http://arxiv.org/abs/2106.03899"/>
        <updated>2021-06-09T02:01:48.003Z</updated>
        <summary type="html"><![CDATA[Detecting out-of-distribution (OOD) samples plays a key role in open-world
and safety-critical applications such as autonomous systems and healthcare.
Self-supervised representation learning techniques (e.g., contrastive learning
and pretext learning) are well suited for learning representation that can
identify OOD samples. In this paper, we propose a simple framework that
leverages multi-task transformation learning for training effective
representation for OOD detection which outperforms state-of-the-art OOD
detection performance and robustness on several image datasets. We empirically
observe that the OOD performance depends on the choice of data transformations
which itself depends on the in-domain training set. To address this problem, we
propose a simple mechanism for selecting the transformations automatically and
modulate their effect on representation learning without requiring any OOD
training samples. We characterize the criteria for a desirable OOD detector for
real-world applications and demonstrate the efficacy of our proposed technique
against a diverse range of the state-of-the-art OOD detection techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1"&gt;Sina Mohseni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1"&gt;Arash Vahdat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadawa_J/0/1/0/all/0/1"&gt;Jay Yadawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets. (arXiv:2101.00063v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00063</id>
        <link href="http://arxiv.org/abs/2101.00063"/>
        <updated>2021-06-09T02:01:47.997Z</updated>
        <summary type="html"><![CDATA[Heavily overparameterized language models such as BERT, XLNet and T5 have
achieved impressive success in many NLP tasks. However, their high model
complexity requires enormous computation resources and extremely long training
time for both pre-training and fine-tuning. Many works have studied model
compression on large NLP models, but only focusing on reducing inference time
while still requiring an expensive training process. Other works use extremely
large batch sizes to shorten the pre-training time, at the expense of higher
computational resource demands. In this paper, inspired by the Early-Bird
Lottery Tickets recently studied for computer vision tasks, we propose
EarlyBERT, a general computationally-efficient training algorithm applicable to
both pre-training and fine-tuning of large-scale language models. By slimming
the self-attention and fully-connected sub-layers inside a transformer, we are
the first to identify structured winning tickets in the early stage of BERT
training. We apply those tickets towards efficient BERT training, and conduct
comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD
downstream tasks. Our results show that EarlyBERT achieves comparable
performance to standard BERT, with 35~45% less training time. Code is available
at https://github.com/VITA-Group/EarlyBERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TIMEDIAL: Temporal Commonsense Reasoning in Dialog. (arXiv:2106.04571v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04571</id>
        <link href="http://arxiv.org/abs/2106.04571"/>
        <updated>2021-06-09T02:01:47.991Z</updated>
        <summary type="html"><![CDATA[Everyday conversations require understanding everyday events, which in turn,
requires understanding temporal commonsense concepts interwoven with those
events. Despite recent progress with massive pre-trained language models (LMs)
such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains
largely under-explored. In this paper, we present the first study to
investigate pre-trained LMs for their temporal reasoning capabilities in
dialogs by introducing a new task and a crowd-sourced English challenge set,
TIMEDIAL. We formulate TIME-DIAL as a multiple-choice cloze task with over 1.1K
carefully curated dialogs. Empirical results demonstrate that even the best
performing models struggle on this task compared to humans, with 23 absolute
points of gap in accuracy. Furthermore, our analysis reveals that the models
fail to reason about dialog context correctly; instead, they rely on shallow
cues based on existing temporal patterns in context, motivating future research
for modeling temporal concepts in text and robust contextual reasoning about
them. The dataset is publicly available at:
https://github.com/google-research-datasets/timedial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Lianhui Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Aditya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Shyam Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Luheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1"&gt;Manaal Faruqui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing Natural Language Processing in A Natural Way: An NLP toolkit based on object-oriented knowledge base and multi-level grammar base. (arXiv:2105.05227v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05227</id>
        <link href="http://arxiv.org/abs/2105.05227"/>
        <updated>2021-06-09T02:01:47.985Z</updated>
        <summary type="html"><![CDATA[We introduce an NLP toolkit based on object-oriented knowledge base and
multi-level grammar base. This toolkit focuses on semantic parsing, it also has
abilities to discover new knowledge and grammar automatically, new discovered
knowledge and grammar will be identified by human, and will be used to update
the knowledge base and grammar base. This process can be iterated many times to
improve the toolkit continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training. (arXiv:2010.15366v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15366</id>
        <link href="http://arxiv.org/abs/2010.15366"/>
        <updated>2021-06-09T02:01:47.966Z</updated>
        <summary type="html"><![CDATA[Speech separation has been well developed, with the very successful
permutation invariant training (PIT) approach, although the frequent label
assignment switching happening during PIT training remains to be a problem when
better convergence speed and achievable performance are desired. In this paper,
we propose to perform self-supervised pre-training to stabilize the label
assignment in training the speech separation model. Experiments over several
types of self-supervised approaches, several typical speech separation models
and two different datasets showed that very good improvements are achievable if
a proper self-supervised approach is chosen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sung-Feng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1"&gt;Shun-Po Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Da-Rong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Gene-Ping Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a New Nonlinear Gradient Method for Solving Large Scale Convex Optimization Problems with an Application on Arabic Medical Text. (arXiv:2106.04383v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04383</id>
        <link href="http://arxiv.org/abs/2106.04383"/>
        <updated>2021-06-09T02:01:47.960Z</updated>
        <summary type="html"><![CDATA[Gradient methods have applications in multiple fields, including signal
processing, image processing, and dynamic systems. In this paper, we present a
nonlinear gradient method for solving convex supra-quadratic functions by
developing the search direction, that done by hybridizing between the two
conjugate coefficients HRM [2] and NHS [1]. The numerical results proved the
effectiveness of the presented method by applying it to solve standard problems
and reaching the exact solution if the objective function is quadratic convex.
Also presented in this article, an application to the problem of named entities
in the Arabic medical language, as it proved the stability of the proposed
method and its efficiency in terms of execution time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hammoud_J/0/1/0/all/0/1"&gt;Jaafar Hammoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eisab_A/0/1/0/all/0/1"&gt;Ali Eisab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dobrenkoa_N/0/1/0/all/0/1"&gt;Natalia Dobrenkoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gusarovaa_N/0/1/0/all/0/1"&gt;Natalia Gusarovaa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation. (arXiv:2106.04447v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04447</id>
        <link href="http://arxiv.org/abs/2106.04447"/>
        <updated>2021-06-09T02:01:47.955Z</updated>
        <summary type="html"><![CDATA[Answering a programming question using only its title is difficult as salient
contextual information is omitted. Based on this observation, we present a
corpus of over 40,000 StackOverflow question texts to be used in conjunction
with their corresponding intents from the CoNaLa dataset (Yin et al., 2018).
Using both the intent and question body, we use BART to establish a baseline
BLEU score of 34.35 for this new task. We find further improvements of $2.8\%$
by combining the mined CoNaLa data with the labeled data to achieve a 35.32
BLEU score. We evaluate prior state-of-the-art CoNaLa models with this
additional data and find that our proposed method of using the body and mined
data beats the BLEU score of the prior state-of-the-art by $71.96\%$. Finally,
we perform ablations to demonstrate that BART is an unsupervised multimodal
learner and examine its extractive behavior. The code and data can be found
https://github.com/gabeorlanski/stackoverflow-encourages-cheating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orlanski_G/0/1/0/all/0/1"&gt;Gabriel Orlanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1"&gt;Alex Gittens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04564</id>
        <link href="http://arxiv.org/abs/2106.04564"/>
        <updated>2021-06-09T02:01:47.948Z</updated>
        <summary type="html"><![CDATA[Pretrained Transformer-based models were reported to be robust in intent
classification. In this work, we first point out the importance of in-domain
out-of-scope detection in few-shot intent recognition tasks and then illustrate
the vulnerability of pretrained Transformer-based models against samples that
are in-domain but out-of-scope (ID-OOS). We empirically show that pretrained
models do not perform well on both ID-OOS examples and general out-of-scope
examples, especially on fine-grained few-shot intent detection tasks. To figure
out how the models mistakenly classify ID-OOS intents as in-scope intents, we
further conduct analysis on confidence scores and the overlapping keywords and
provide several prospective directions for future work. We release the relevant
resources to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian-Guo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1"&gt;Kazuma Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yao Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-09T02:01:47.932Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Lifelong Learning of End-to-end ASR. (arXiv:2104.01616v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01616</id>
        <link href="http://arxiv.org/abs/2104.01616"/>
        <updated>2021-06-09T02:01:47.915Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) technologies today are primarily optimized
for given datasets; thus, any changes in the application environment (e.g.,
acoustic conditions or topic domains) may inevitably degrade the performance.
We can collect new data describing the new environment and fine-tune the
system, but this naturally leads to higher error rates for the earlier
datasets, referred to as catastrophic forgetting. The concept of lifelong
learning (LLL) aiming to enable a machine to sequentially learn new tasks from
new datasets describing the changing real world without forgetting the
previously learned knowledge is thus brought to attention. This paper reports,
to our knowledge, the first effort to extensively consider and analyze the use
of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel
methods in saving data for past domains to mitigate the catastrophic forgetting
problem. An overall relative reduction of 28.7% in WER was achieved compared to
the fine-tuning baseline when sequentially learning on three very different
benchmark corpora. This can be the first step toward the highly desired ASR
technologies capable of synchronizing with the continuously changing real
world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Heng-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lin-shan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turing: an Accurate and Interpretable Multi-Hypothesis Cross-Domain Natural Language Database Interface. (arXiv:2106.04559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04559</id>
        <link href="http://arxiv.org/abs/2106.04559"/>
        <updated>2021-06-09T02:01:47.909Z</updated>
        <summary type="html"><![CDATA[A natural language database interface (NLDB) can democratize data-driven
insights for non-technical users. However, existing Text-to-SQL semantic
parsers cannot achieve high enough accuracy in the cross-database setting to
allow good usability in practice. This work presents Turing, a NLDB system
toward bridging this gap. The cross-domain semantic parser of Turing with our
novel value prediction method achieves $75.1\%$ execution accuracy, and
$78.3\%$ top-5 beam execution accuracy on the Spider validation set. To benefit
from the higher beam accuracy, we design an interactive system where the SQL
hypotheses in the beam are explained step-by-step in natural language, with
their differences highlighted. The user can then compare and judge the
hypotheses to select which one reflects their intention if any. The English
explanations of SQL queries in Turing are produced by our high-precision
natural language generation system based on synchronous grammars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zi_W/0/1/0/all/0/1"&gt;Wenjie Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahidi_H/0/1/0/all/0/1"&gt;Hamidreza Shahidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadar_A/0/1/0/all/0/1"&gt;&amp;#xc1;kos K&amp;#xe1;d&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Keyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ateeq_J/0/1/0/all/0/1"&gt;Jawad Ateeq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barot_H/0/1/0/all/0/1"&gt;Harsh Barot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alon_M/0/1/0/all/0/1"&gt;Meidan Alon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yanshuai Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obtaining Better Static Word Embeddings Using Contextual Embedding Models. (arXiv:2106.04302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04302</id>
        <link href="http://arxiv.org/abs/2106.04302"/>
        <updated>2021-06-09T02:01:47.903Z</updated>
        <summary type="html"><![CDATA[The advent of contextual word embeddings -- representations of words which
incorporate semantic and syntactic information from their context -- has led to
tremendous improvements on a wide variety of NLP tasks. However, recent
contextual models have prohibitively high computational cost in many use-cases
and are often hard to interpret. In this work, we demonstrate that our proposed
distillation method, which is a simple extension of CBOW-based training, allows
to significantly improve computational efficiency of NLP applications, while
outperforming the quality of existing static embeddings trained from scratch as
well as those distilled from previously proposed methods. As a side-effect, our
approach also allows a fair comparison of both contextual and static embeddings
via standard lexical evaluation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prakhar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering. (arXiv:2106.04441v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04441</id>
        <link href="http://arxiv.org/abs/2106.04441"/>
        <updated>2021-06-09T02:01:47.855Z</updated>
        <summary type="html"><![CDATA[We present the first end-to-end, transformer-based table question answering
(QA) system that takes natural language questions and massive table corpus as
inputs to retrieve the most relevant tables and locate the correct table cells
to answer the question. Our system, CLTR, extends the current state-of-the-art
QA over tables model to build an end-to-end table QA architecture. This system
has successfully tackled many real-world table QA problems with a simple,
unified pipeline. Our proposed system can also generate a heatmap of candidate
columns and rows over complex tables and allow users to quickly identify the
correct cells to answer questions. In addition, we introduce two new
open-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural
language questions over 76,242 tables. The benchmarks are designed to validate
CLTR as well as accommodate future table retrieval and end-to-end table QA
research and experiments. Our experiments demonstrate that our system is the
current state-of-the-art model on the table retrieval task and produces
promising results for end-to-end table QA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feifei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1"&gt;Mustafa Canim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1"&gt;Michael Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_P/0/1/0/all/0/1"&gt;Peter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04484</id>
        <link href="http://arxiv.org/abs/2106.04484"/>
        <updated>2021-06-09T02:01:47.849Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;Daniel Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks. (arXiv:2106.04489v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04489</id>
        <link href="http://arxiv.org/abs/2106.04489"/>
        <updated>2021-06-09T02:01:47.770Z</updated>
        <summary type="html"><![CDATA[State-of-the-art parameter-efficient fine-tuning methods rely on introducing
adapter modules between the layers of a pretrained language model. However,
such modules are trained separately for each task and thus do not enable
sharing information across tasks. In this paper, we show that we can learn
adapter parameters for all layers and tasks by generating them using shared
hypernetworks, which condition on task, adapter position, and layer id in a
transformer model. This parameter-efficient multi-task learning framework
allows us to achieve the best of both worlds by sharing knowledge across tasks
via hypernetworks while enabling the model to adapt to each individual task
through task-specific adapters. Experiments on the well-known GLUE benchmark
show improved performance in multi-task learning while adding only 0.29%
parameters per task. We additionally demonstrate substantial performance
improvements in few-shot domain generalization across a variety of tasks. Our
code is publicly available in https://github.com/rabeehk/hyperformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1"&gt;Rabeeh Karimi Mahabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1"&gt;James Henderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Training for Machine Reading Comprehension with Virtual Embeddings. (arXiv:2106.04437v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04437</id>
        <link href="http://arxiv.org/abs/2106.04437"/>
        <updated>2021-06-09T02:01:47.677Z</updated>
        <summary type="html"><![CDATA[Adversarial training (AT) as a regularization method has proved its
effectiveness on various tasks. Though there are successful applications of AT
on some NLP tasks, the distinguishing characteristics of NLP tasks have not
been exploited. In this paper, we aim to apply AT on machine reading
comprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing
a novel adversarial training method called PQAT that perturbs the embedding
matrix instead of word vectors. To differentiate the roles of passages and
questions, PQAT uses additional virtual P/Q-embedding matrices to gather the
global perturbations of words from passages and questions separately. We test
the method on a wide range of MRC tasks, including span-based extractive RC and
multiple-choice RC. The results show that adversarial training is effective
universally, and PQAT further improves the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziqing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoping Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Generation for Adaptive Education. (arXiv:2106.04262v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04262</id>
        <link href="http://arxiv.org/abs/2106.04262"/>
        <updated>2021-06-09T02:01:47.656Z</updated>
        <summary type="html"><![CDATA[Intelligent and adaptive online education systems aim to make high-quality
education available for a diverse range of students. However, existing systems
usually depend on a pool of hand-made questions, limiting how fine-grained and
open-ended they can be in adapting to individual students. We explore targeted
question generation as a controllable sequence generation task. We first show
how to fine-tune pre-trained language models for deep knowledge tracing
(LM-KT). This model accurately predicts the probability of a student answering
a question correctly, and generalizes to questions not seen in training. We
then use LM-KT to specify the objective and data for training a model to
generate questions conditioned on the student and target difficulty. Our
results show we succeed at generating novel, well-calibrated language
translation questions for second language learners from a real online education
platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1"&gt;Megha Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Graph enhanced Embedding Neural Network for CTR Prediction. (arXiv:2106.00314v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00314</id>
        <link href="http://arxiv.org/abs/2106.00314"/>
        <updated>2021-06-09T02:01:47.582Z</updated>
        <summary type="html"><![CDATA[CTR prediction, which aims to estimate the probability that a user will click
an item, plays a crucial role in online advertising and recommender system.
Feature interaction modeling based and user interest mining based methods are
the two kinds of most popular techniques that have been extensively explored
for many years and have made great progress for CTR prediction. However, (1)
feature interaction based methods which rely heavily on the co-occurrence of
different features, may suffer from the feature sparsity problem (i.e., many
features appear few times); (2) user interest mining based methods which need
rich user behaviors to obtain user's diverse interests, are easy to encounter
the behavior sparsity problem (i.e., many users have very short behavior
sequences). To solve these problems, we propose a novel module named Dual Graph
enhanced Embedding, which is compatible with various CTR prediction models to
alleviate these two problems. We further propose a Dual Graph enhanced
Embedding Neural Network (DG-ENN) for CTR prediction. Dual Graph enhanced
Embedding exploits the strengths of graph representation with two carefully
designed learning strategies (divide-and-conquer, curriculum-learning-inspired
organized learning) to refine the embedding. We conduct comprehensive
experiments on three real-world industrial datasets. The experimental results
show that our proposed DG-ENN significantly outperforms state-of-the-art CTR
prediction models. Moreover, when applying to state-of-the-art CTR prediction
models, Dual graph enhanced embedding always obtains better performance.
Further case studies prove that our proposed dual graph enhanced embedding
could alleviate the feature sparsity and behavior sparsity problems. Our
framework will be open-source based on MindSpore in the near future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1"&gt;Rong Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1"&gt;Renhao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yingxue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhirong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation. (arXiv:2106.04403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04403</id>
        <link href="http://arxiv.org/abs/2106.04403"/>
        <updated>2021-06-09T02:01:47.564Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have brought significant progress in visual
grounding tasks such as language-guided video object segmentation. However,
collecting large datasets for these tasks is expensive in terms of annotation
time, which represents a bottleneck. To this end, we propose a novel method,
namely SynthRef, for generating synthetic referring expressions for target
objects in an image (or video frame), and we also present and disseminate the
first large-scale dataset with synthetic referring expressions for video object
segmentation. Our experiments demonstrate that by training with our synthetic
referring expressions one can improve the ability of a model to generalize
across different datasets, without any additional annotation cost. Moreover,
our formulation allows its application to any object detection or segmentation
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazakos_I/0/1/0/all/0/1"&gt;Ioannis Kazakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellver_M/0/1/0/all/0/1"&gt;Miriam Bellver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1"&gt;Carina Silberer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning compositional structures for semantic graph parsing. (arXiv:2106.04398v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04398</id>
        <link href="http://arxiv.org/abs/2106.04398"/>
        <updated>2021-06-09T02:01:47.559Z</updated>
        <summary type="html"><![CDATA[AM dependency parsing is a method for neural semantic graph parsing that
exploits the principle of compositionality. While AM dependency parsers have
been shown to be fast and accurate across several graphbanks, they require
explicit annotations of the compositional tree structures for training. In the
past, these were obtained using complex graphbank-specific heuristics written
by experts. Here we show how they can instead be trained directly on the graphs
with a neural latent-variable model, drastically reducing the amount and
complexity of manual heuristics. We demonstrate that our model picks up on
several linguistic phenomena on its own and achieves comparable accuracy to
supervised training, greatly facilitating the use of AM dependency parsing for
new sembanks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Groschwitz_J/0/1/0/all/0/1"&gt;Jonas Groschwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlie_M/0/1/0/all/0/1"&gt;Meaghan Fowlie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1"&gt;Alexander Koller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-09T02:01:47.553Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cyberbullying Detection Using Deep Neural Network from Social Media Comments in Bangla Language. (arXiv:2106.04506v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04506</id>
        <link href="http://arxiv.org/abs/2106.04506"/>
        <updated>2021-06-09T02:01:47.548Z</updated>
        <summary type="html"><![CDATA[Cyberbullying or Online harassment detection on social media for various
major languages is currently being given a good amount of focus by researchers
worldwide. Being the seventh most speaking language in the world and increasing
usage of online platform among the Bengali speaking people urge to find
effective detection technique to handle the online harassment. In this paper,
we have proposed binary and multiclass classification model using hybrid neural
network for bully expression detection in Bengali language. We have used 44,001
users comments from popular public Facebook pages, which fall into five classes
- Non-bully, Sexual, Threat, Troll and Religious. We have examined the
performance of our proposed models from different perspective. Our binary
classification model gives 87.91% accuracy, whereas introducing ensemble
technique after neural network for multiclass classification, we got 85%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Md Faisal Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_Z/0/1/0/all/0/1"&gt;Zalish Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biash_Z/0/1/0/all/0/1"&gt;Zarin Tasnim Biash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryen_A/0/1/0/all/0/1"&gt;Ahmed Ann Noor Ryen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_A/0/1/0/all/0/1"&gt;Arman Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashraf_F/0/1/0/all/0/1"&gt;Faisal Bin Ashraf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Framework for Aspect-Based Sentiment Analysis. (arXiv:2106.04300v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04300</id>
        <link href="http://arxiv.org/abs/2106.04300"/>
        <updated>2021-06-09T02:01:47.541Z</updated>
        <summary type="html"><![CDATA[Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms,
their corresponding sentiment polarities, and the opinion terms. There exist
seven subtasks in ABSA. Most studies only focus on the subsets of these
subtasks, which leads to various complicated ABSA models while hard to solve
these subtasks in a unified framework. In this paper, we redefine every subtask
target as a sequence mixed by pointer indexes and sentiment class indexes,
which converts all ABSA subtasks into a unified generative formulation. Based
on the unified formulation, we exploit the pre-training sequence-to-sequence
model BART to solve all ABSA subtasks in an end-to-end framework. Extensive
experiments on four ABSA datasets for seven subtasks demonstrate that our
framework achieves substantial performance gain and provides a real unified
end-to-end solution for the whole ABSA subtasks, which could benefit multiple
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Junqi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ji_T/0/1/0/all/0/1"&gt;Tuo ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets. (arXiv:2106.04476v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04476</id>
        <link href="http://arxiv.org/abs/2106.04476"/>
        <updated>2021-06-09T02:01:47.536Z</updated>
        <summary type="html"><![CDATA[Semantic parsers map natural language utterances to meaning representations.
The lack of a single standard for meaning representations led to the creation
of a plethora of semantic parsing datasets. To unify different datasets and
train a single model for them, we investigate the use of Multi-Task Learning
(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,
Overnight, AMR). We find that an MTL architecture that shares the entire
network across datasets yields competitive or better parsing accuracies than
the single-task baselines, while reducing the total number of parameters by
68%. We further provide evidence that MTL has also better compositional
generalization than single-task models. We also present a comparison of task
sampling methods and propose a competitive alternative to widespread
proportional sampling strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1"&gt;Marco Damonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1"&gt;Emilio Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable agent communication from scratch(with a generic visual processor emerging on the side). (arXiv:2106.04258v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04258</id>
        <link href="http://arxiv.org/abs/2106.04258"/>
        <updated>2021-06-09T02:01:47.519Z</updated>
        <summary type="html"><![CDATA[As deep networks begin to be deployed as autonomous agents, the issue of how
they can communicate with each other becomes important. Here, we train two deep
nets from scratch to perform realistic referent identification through
unsupervised emergent communication. We show that the largely interpretable
emergent protocol allows the nets to successfully communicate even about object
types they did not see at training time. The visual representations induced as
a by-product of our training regime, moreover, show comparable quality, when
re-used as generic visual features, to a recent self-supervised learning model.
Our results provide concrete evidence of the viability of (interpretable)
emergent deep net communication in a more realistic scenario than previously
considered, as well as establishing an intriguing link between this field and
self-supervised visual learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1"&gt;Roberto Dess&amp;#xec;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1"&gt;Eugene Kharitonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1"&gt;Marco Baroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staircase Attention for Recurrent Processing of Sequences. (arXiv:2106.04279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04279</id>
        <link href="http://arxiv.org/abs/2106.04279"/>
        <updated>2021-06-09T02:01:47.513Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have become a standard tool for sequence modeling tasks,
in particular by stacking self-attention layers over the entire input sequence
as in the Transformer architecture. In this work we introduce a novel attention
procedure called staircase attention that, unlike self-attention, operates
across the sequence (in time) recurrently processing the input by adding
another step of processing. A step in the staircase comprises of backward
tokens (encoding the sequence so far seen) and forward tokens (ingesting a new
part of the sequence), or an extreme Ladder version with a forward step of zero
that simply repeats the Transformer on each step of the ladder, sharing the
weights. We thus describe a family of such models that can trade off
performance and compute, by either increasing the amount of recurrence through
time, the amount of sequential processing via recurrence in depth, or both.
Staircase attention is shown to be able to solve tasks that involve tracking
that conventional Transformers cannot, due to this recurrence. Further, it is
shown to provide improved modeling power for the same size model (number of
parameters) compared to self-attentive Transformers on large language modeling
and dialogue tasks, yielding significant perplexity gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures. (arXiv:2106.04311v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04311</id>
        <link href="http://arxiv.org/abs/2106.04311"/>
        <updated>2021-06-09T02:01:47.508Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) completion has been excessively studied with a massive
number of models proposed for the Link Prediction (LP) task. The main
limitation of such models is their insensitivity to time. Indeed, the temporal
aspect of stored facts is often ignored. To this end, more and more works
consider time as a parameter to complete KGs. In this paper, we first
demonstrate that, by simply increasing the number of negative samples, the
recent AttH model can achieve competitive or even better performance than the
state-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further
propose Hercules, a time-aware extension of AttH model, which defines the
curvature of a Riemannian manifold as the product of both relation and time.
Our experiments show that both Hercules and AttH achieve competitive or new
state-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore,
one should raise awareness when learning TKGs representations to identify
whether time truly boosts performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montella_S/0/1/0/all/0/1"&gt;Sebastien Montella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1"&gt;Lina Rojas-Barahona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinecke_J/0/1/0/all/0/1"&gt;Johannes Heinecke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic Evaluation Principles for Cross-document Coreference Resolution. (arXiv:2106.04192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04192</id>
        <link href="http://arxiv.org/abs/2106.04192"/>
        <updated>2021-06-09T02:01:47.502Z</updated>
        <summary type="html"><![CDATA[We point out that common evaluation practices for cross-document coreference
resolution have been unrealistically permissive in their assumed settings,
yielding inflated results. We propose addressing this issue via two evaluation
methodology principles. First, as in other tasks, models should be evaluated on
predicted mentions rather than on gold mentions. Doing this raises a subtle
issue regarding singleton coreference clusters, which we address by decoupling
the evaluation of mention detection from that of coreference linking. Second,
we argue that models should not exploit the synthetic topic structure of the
standard ECB+ dataset, forcing models to confront the lexical ambiguity
challenge, as intended by the dataset creators. We demonstrate empirically the
drastic impact of our more realistic evaluation principles on a competitive
model, yielding a score which is 33 F1 lower compared to evaluating by prior
lenient practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1"&gt;Arie Cattan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eirew_A/0/1/0/all/0/1"&gt;Alon Eirew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1"&gt;Gabriel Stanovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Word Segmentation from Discrete Speech Units in Low-Resource Settings. (arXiv:2106.04298v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04298</id>
        <link href="http://arxiv.org/abs/2106.04298"/>
        <updated>2021-06-09T02:01:47.496Z</updated>
        <summary type="html"><![CDATA[When documenting oral-languages, Unsupervised Word Segmentation (UWS) from
speech is a useful, yet challenging, task. It can be performed from phonetic
transcriptions, or in the absence of these, from the output of unsupervised
speech discretization models. These discretization models are trained using raw
speech only, producing discrete speech units which can be applied for
downstream (text-based) tasks. In this paper we compare five of these models:
three Bayesian and two neural approaches, with regards to the exploitability of
the produced units for UWS. Two UWS models are experimented with and we report
results for Finnish, Hungarian, Mboshi, Romanian and Russian in a low-resource
setting (using only 5k sentences). Our results suggest that neural models for
speech discretization are difficult to exploit in our setting, and that it
might be necessary to adapt them to limit sequence length. We obtain our best
UWS results by using the SHMM and H-SHMM Bayesian models, which produce high
quality, yet compressed, discrete representations of the input speech signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1"&gt;Marcely Zanon Boito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yusuf_B/0/1/0/all/0/1"&gt;Bolaji Yusuf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondel_L/0/1/0/all/0/1"&gt;Lucas Ondel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1"&gt;Aline Villavicencio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon Learning for Few-Shot Neural Sequence Modeling. (arXiv:2106.03993v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03993</id>
        <link href="http://arxiv.org/abs/2106.03993"/>
        <updated>2021-06-09T02:01:47.480Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence transduction is the core problem in language processing
applications as diverse as semantic parsing, machine translation, and
instruction following. The neural network models that provide the dominant
solution to these problems are brittle, especially in low-resource settings:
they fail to generalize correctly or systematically from small datasets. Past
work has shown that many failures of systematic generalization arise from
neural models' inability to disentangle lexical phenomena from syntactic ones.
To address this, we augment neural decoders with a lexical translation
mechanism that generalizes existing copy mechanisms to incorporate learned,
decontextualized, token-level translation rules. We describe how to initialize
this mechanism using a variety of lexicon learning algorithms, and show that it
improves systematic generalization on a diverse set of sequence modeling tasks
drawn from cognitive science, formal semantics, and machine translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags for Low Resource UD Parsing. (arXiv:2106.04222v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04222</id>
        <link href="http://arxiv.org/abs/2106.04222"/>
        <updated>2021-06-09T02:01:47.474Z</updated>
        <summary type="html"><![CDATA[We evaluate the efficacy of predicted UPOS tags as input features for
dependency parsers in lower resource settings to evaluate how treebank size
affects the impact tagging accuracy has on parsing performance. We do this for
real low resource universal dependency treebanks, artificially low resource
data with varying treebank sizes, and for very small treebanks with varying
amounts of augmented data. We find that predicted UPOS tags are somewhat
helpful for low resource treebanks, especially when fewer fully-annotated trees
are available. We also find that this positive impact diminishes as the amount
of data increases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehouck_M/0/1/0/all/0/1"&gt;Mathieu Dehouck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question Answering. (arXiv:2106.04016v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04016</id>
        <link href="http://arxiv.org/abs/2106.04016"/>
        <updated>2021-06-09T02:01:47.468Z</updated>
        <summary type="html"><![CDATA[Disfluencies is an under-studied topic in NLP, even though it is ubiquitous
in human conversation. This is largely due to the lack of datasets containing
disfluencies. In this paper, we present a new challenge question answering
dataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual
disfluencies in previously fluent questions. Disfl-QA contains a variety of
challenging disfluencies that require a more comprehensive understanding of the
text than what was necessary in prior datasets. Experiments show that the
performance of existing state-of-the-art question answering models degrades
significantly when tested on Disfl-QA in a zero-shot setting.We show data
augmentation methods partially recover the loss in performance and also
demonstrate the efficacy of using gold data for fine-tuning. We argue that we
need large-scale disfluency datasets in order for NLP models to be robust to
them. The dataset is publicly available at:
https://github.com/google-research-datasets/disfl-qa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Aditya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiacheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Shyam Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Diyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1"&gt;Manaal Faruqui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making. (arXiv:2106.04174v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04174</id>
        <link href="http://arxiv.org/abs/2106.04174"/>
        <updated>2021-06-09T02:01:47.448Z</updated>
        <summary type="html"><![CDATA[Entity Matching (EM) aims at recognizing entity records that denote the same
real-world object. Neural EM models learn vector representation of entity
descriptions and match entities end-to-end. Though robust, these methods
require many resources for training, and lack of interpretability. In this
paper, we propose a novel EM framework that consists of Heterogeneous
Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple
feature representation from matching decision. Using self-supervised learning
and mask mechanism in pre-trained language modeling, HIF learns the embeddings
of noisy attribute values by inter-attribute attention with unlabeled data.
Using a set of comparison features and a limited amount of annotated data, KAT
Induction learns an efficient decision tree that can be interpreted by
generating entity matching rules whose structure is advocated by domain
experts. Experiments on 6 public datasets and 3 industrial datasets show that
our method is highly efficient and outperforms SOTA EM models in most cases.
Our codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zijun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengjiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1"&gt;Tiansi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jifan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Lei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zelin Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021. (arXiv:2106.04216v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04216</id>
        <link href="http://arxiv.org/abs/2106.04216"/>
        <updated>2021-06-09T02:01:47.442Z</updated>
        <summary type="html"><![CDATA[We evaluate three leading dependency parser systems from different paradigms
on a small yet diverse subset of languages in terms of their
accuracy-efficiency Pareto front. As we are interested in efficiency, we
evaluate core parsers without pretrained language models (as these are
typically huge networks and would constitute most of the compute time) or other
augmentations that can be transversally applied to any of them. Biaffine
parsing emerges as a well-balanced default choice, with sequence-labelling
parsing being preferable if inference speed (but not training energy cost) is
the priority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mar Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference. (arXiv:2106.03983v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03983</id>
        <link href="http://arxiv.org/abs/2106.03983"/>
        <updated>2021-06-09T02:01:47.437Z</updated>
        <summary type="html"><![CDATA[Multilingual transformers (XLM, mT5) have been shown to have remarkable
transfer skills in zero-shot settings. Most transfer studies, however, rely on
automatically translated resources (XNLI, XQuAD), making it hard to discern the
particular linguistic knowledge that is being transferred, and the role of
expert annotated monolingual datasets when developing task-specific models. We
investigate the cross-lingual transfer abilities of XLM-R for Chinese and
English natural language inference (NLI), with a focus on the recent
large-scale Chinese dataset OCNLI. To better understand linguistic transfer, we
created 4 categories of challenge and adversarial tasks (totaling 17 new
datasets) for Chinese that build on several well-known resources for English
(e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on
English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our
challenge categories, they perform as well/better than the best monolingual
models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro
drop). These results, however, come with important caveats: cross-lingual
models often perform best when trained on a mixture of English and high-quality
monolingual NLI data (OCNLI), and are often hindered by automatically
translated resources (XNLI-zh). For many phenomena, all models continue to
struggle, highlighting the need for our new diagnostics to help benchmark
Chinese and cross-lingual models. All new datasets/code are released at
https://github.com/huhailinguist/ChineseNLIProbing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;He Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zuoyu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yina Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yixin Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1"&gt;Kyle Richardson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised and Supervised Joint Training for Resource-rich Machine Translation. (arXiv:2106.04060v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04060</id>
        <link href="http://arxiv.org/abs/2106.04060"/>
        <updated>2021-06-09T02:01:47.420Z</updated>
        <summary type="html"><![CDATA[Self-supervised pre-training of text representations has been successfully
applied to low-resource Neural Machine Translation (NMT). However, it usually
fails to achieve notable gains on resource-rich NMT. In this paper, we propose
a joint training approach, $F_2$-XEnDec, to combine self-supervised and
supervised learning to optimize NMT models. To exploit complementary
self-supervised signals for supervised learning, NMT models are trained on
examples that are interbred from monolingual and parallel sentences through a
new process called crossover encoder-decoder. Experiments on two resource-rich
translation benchmarks, WMT'14 English-German and WMT'14 English-French,
demonstrate that our approach achieves substantial improvements over several
strong baseline methods and obtains a new state of the art of 46.19 BLEU on
English-French when incorporating back translation. Results also show that our
approach is capable of improving model robustness to input perturbations such
as code-switching noise which frequently appears on social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1"&gt;Wolfgang Macherey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Through Regularization for Learning to Rank. (arXiv:2102.05996v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05996</id>
        <link href="http://arxiv.org/abs/2102.05996"/>
        <updated>2021-06-09T02:01:47.413Z</updated>
        <summary type="html"><![CDATA[Given the abundance of applications of ranking in recent years, addressing
fairness concerns around automated ranking systems becomes necessary for
increasing the trust among end-users. Previous work on fair ranking has mostly
focused on application-specific fairness notions, often tailored to online
advertising, and it rarely considers learning as part of the process. In this
work, we show how to transfer numerous fairness notions from binary
classification to a learning to rank setting. Our formalism allows us to design
methods for incorporating fairness objectives with provable generalization
guarantees. An extensive experimental evaluation shows that our method can
improve ranking fairness substantially with no or only little loss of model
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1"&gt;Nikola Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insight from NLP Analysis: COVID-19 Vaccines Sentiments on Social Media. (arXiv:2106.04081v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04081</id>
        <link href="http://arxiv.org/abs/2106.04081"/>
        <updated>2021-06-09T02:01:47.407Z</updated>
        <summary type="html"><![CDATA[Social media is an appropriate source for analyzing public attitudes towards
the COVID-19 vaccine and various brands. Nevertheless, there are few relevant
studies. In the research, we collected tweet posts by the UK and US residents
from the Twitter API during the pandemic and designed experiments to answer
three main questions concerning vaccination. To get the dominant sentiment of
the civics, we performed sentiment analysis by VADER and proposed a new method
that can count the individual's influence. This allows us to go a step further
in sentiment analysis and explain some of the fluctuations in the data
changing. The results indicated that celebrities could lead the opinion shift
on social media in vaccination progress. Moreover, at the peak, nearly 40\% of
the population in both countries have a negative attitude towards COVID-19
vaccines. Besides, we investigated how people's opinions toward different
vaccine brands are. We found that the Pfizer vaccine enjoys the most popular
among people. By applying the sentiment analysis tool, we discovered most
people hold positive views toward the COVID-19 vaccine manufactured by most
brands. In the end, we carried out topic modelling by using the LDA model. We
found residents in the two countries are willing to share their views and
feelings concerning the vaccine. Several death cases have occurred after
vaccination. Due to these negative events, US residents are more worried about
the side effects and safety of the vaccine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Na_T/0/1/0/all/0/1"&gt;Tao Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wanyu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongjiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning to Compositionally Generalize. (arXiv:2106.04252v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04252</id>
        <link href="http://arxiv.org/abs/2106.04252"/>
        <updated>2021-06-09T02:01:47.401Z</updated>
        <summary type="html"><![CDATA[Natural language is compositional; the meaning of a sentence is a function of
the meaning of its parts. This property allows humans to create and interpret
novel sentences, generalizing robustly outside their prior experience. Neural
networks have been shown to struggle with this kind of generalization, in
particular performing poorly on tasks designed to assess compositional
generalization (i.e. where training and testing distributions differ in ways
that would be trivial for a compositional strategy to resolve). Their poor
performance on these tasks may in part be due to the nature of supervised
learning which assumes training and testing data to be drawn from the same
distribution. We implement a meta-learning augmented version of supervised
learning whose objective directly optimizes for out-of-distribution
generalization. We construct pairs of tasks for meta-learning by sub-sampling
existing training data. Each pair of tasks is constructed to contain relevant
examples, as determined by a similarity metric, in an effort to inhibit models
from memorizing their input. Experimental results on the COGS and SCAN datasets
show that our similarity-driven meta-learning can improve generalization
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conklin_H/0/1/0/all/0/1"&gt;Henry Conklin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kenny Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning. (arXiv:2106.03921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03921</id>
        <link href="http://arxiv.org/abs/2106.03921"/>
        <updated>2021-06-09T02:01:47.387Z</updated>
        <summary type="html"><![CDATA[Imagine you are in a supermarket. You have two bananas in your basket and
want to buy four apples. How many fruits do you have in total? This seemingly
straightforward question can be challenging for data-driven language models,
even if trained at scale. However, we would expect such generic language models
to possess some mathematical abilities in addition to typical linguistic
competence. Towards this goal, we investigate if a commonly used language
model, BERT, possesses such mathematical abilities and, if so, to what degree.
For that, we fine-tune BERT on a popular dataset for word math problems,
AQuA-RAT, and conduct several tests to understand learned representations
better. Since we teach models trained on natural language to do formal
mathematics, we hypothesize that such models would benefit from training on
semi-formal steps that explain how math results are derived. To better
accommodate such training, we also propose new pretext tasks for learning
mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or
NROP). With this new model, we achieve significantly better outcomes than
data-driven baselines and even on-par with more tailored models. We also show
how to reduce positional bias in such models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piekos_P/0/1/0/all/0/1"&gt;Piotr Pi&amp;#x119;kos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1"&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study. (arXiv:2106.03958v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03958</id>
        <link href="http://arxiv.org/abs/2106.03958"/>
        <updated>2021-06-09T02:01:47.381Z</updated>
        <summary type="html"><![CDATA[Recent research in multilingual language models (LM) has demonstrated their
ability to effectively handle multiple languages in a single model. This holds
promise for low web-resource languages (LRL) as multilingual models can enable
transfer of supervision from high resource languages to LRLs. However,
incorporating a new language in an LM still remains a challenge, particularly
for languages with limited corpora and in unseen scripts. In this paper we
argue that relatedness among languages in a language family may be exploited to
overcome some of the corpora limitations of LRLs, and propose RelateLM. We
focus on Indian languages, and exploit relatedness along two dimensions: (1)
script (since many Indic scripts originated from the Brahmic script), and (2)
sentence structure. RelateLM uses transliteration to convert the unseen script
of limited LRL text into the script of a Related Prominent Language (RPL)
(Hindi in our case). While exploiting similar sentence structures, RelateLM
utilizes readily available bilingual dictionaries to pseudo translate RPL text
into LRL corpora. Experiments on multiple real-world benchmark datasets provide
validation to our hypothesis that using a related language as pivot, along with
transliteration and pseudo translation based data augmentation, can be an
effective way to adapt LMs for LRLs, rather than direct training or pivoting
through English.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khemchandani_Y/0/1/0/all/0/1"&gt;Yash Khemchandani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehtani_S/0/1/0/all/0/1"&gt;Sarvesh Mehtani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1"&gt;Vaidehi Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1"&gt;Sunita Sarawagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model. (arXiv:2106.04098v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04098</id>
        <link href="http://arxiv.org/abs/2106.04098"/>
        <updated>2021-06-09T02:01:47.374Z</updated>
        <summary type="html"><![CDATA[Recently, there is an effort to extend fine-grained entity typing by using a
richer and ultra-fine set of types, and labeling noun phrases including
pronouns and nominal nouns instead of just named entity mentions. A key
challenge for this ultra-fine entity typing task is that human annotated data
are extremely scarce, and the annotation ability of existing distant or weak
supervision approaches is very limited. To remedy this problem, in this paper,
we propose to obtain training data for ultra-fine entity typing by using a BERT
Masked Language Model (MLM). Given a mention in a sentence, our approach
constructs an input for the BERT MLM so that it predicts context dependent
hypernyms of the mention, which can be used as type labels. Experimental
results demonstrate that, with the help of these automatically generated
labels, the performance of an ultra-fine entity typing model can be improved
substantially. We also show that our approach can be applied to improve
traditional fine-grained entity typing after performing simple type mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hongliang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yangqiu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haixun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation. (arXiv:2106.04080v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04080</id>
        <link href="http://arxiv.org/abs/2106.04080"/>
        <updated>2021-06-09T02:01:47.366Z</updated>
        <summary type="html"><![CDATA[To date, most abstractive summarisation models have relied on variants of the
negative log-likelihood (NLL) as their training objective. In some cases,
reinforcement learning has been added to train the models with an objective
that is closer to their evaluation measures (e.g. ROUGE). However, the reward
function to be used within the reinforcement learning approach can play a key
role for performance and is still partially unexplored. For this reason, in
this paper, we propose two reward functions for the task of abstractive
summarisation: the first function, referred to as RwB-Hinge, dynamically
selects the samples for the gradient update. The second function, nicknamed
RISK, leverages a small pool of strong candidates to inform the reward. In the
experiments, we probe the proposed approach by fine-tuning an NLL pre trained
model over nine summarisation datasets of diverse size and nature. The
experimental results show a consistent improvement over the negative
log-likelihood baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parnell_J/0/1/0/all/0/1"&gt;Jacob Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1"&gt;Inigo Jauregi Unanue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1"&gt;Massimo Piccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-09T02:01:47.360Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers
through reducing their costs of trial and error for discovering the optimal
advertising strategies is crucial for the long-term prosperity of online
advertising. To achieve this goal, the advertising platform needs to identify
the advertisers' marketing objectives, and then recommend the corresponding
strategies to fulfill this objective. In this work, we first deploy a prototype
of strategy recommender system on Taobao display advertising platform,
recommending bid prices and targeted users to advertisers. We further augment
this prototype system by directly revealing the advertising performance, and
then infer the advertisers' marketing objectives through their adoptions of
different recommending advertising performance. We use the techniques from
context bandit to jointly learn the advertisers' marketing objectives and the
recommending strategies. Online evaluations show that the designed advertising
strategy recommender system can optimize the advertisers' advertising
performance and increase the platform's revenue. Simulation experiments based
on Taobao online bidding data show that the designed contextual bandit
algorithm can effectively optimize the strategy adoption rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIGTYP 2021 Shared Task: Robust Spoken Language Identification. (arXiv:2106.03895v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03895</id>
        <link href="http://arxiv.org/abs/2106.03895"/>
        <updated>2021-06-09T02:01:47.353Z</updated>
        <summary type="html"><![CDATA[While language identification is a fundamental speech and language processing
task, for many languages and language families it remains a challenging task.
For many low-resource and endangered languages this is in part due to resource
availability: where larger datasets exist, they may be single-speaker or have
different domains than desired application scenarios, demanding a need for
domain and speaker-invariant language identification systems. This year's
shared task on robust spoken language identification sought to investigate just
this scenario: systems were to be trained on largely single-speaker speech from
one domain, but evaluated on data in other domains recorded from speakers under
different recording circumstances, mimicking realistic low-resource scenarios.
We see that domain and speaker mismatch proves very challenging for current
methods which can perform above 95% accuracy in-domain, which domain adaptation
can address to some degree, but that these conditions merit further
investigation to make spoken language identification accessible in many
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1"&gt;Elizabeth Salesky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1"&gt;Badr M. Abdullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1"&gt;Sabrina J. Mielke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klyachko_E/0/1/0/all/0/1"&gt;Elena Klyachko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serikov_O/0/1/0/all/0/1"&gt;Oleg Serikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ritesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vylomova_E/0/1/0/all/0/1"&gt;Ekaterina Vylomova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality. (arXiv:2106.04102v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04102</id>
        <link href="http://arxiv.org/abs/2106.04102"/>
        <updated>2021-06-09T02:01:47.346Z</updated>
        <summary type="html"><![CDATA[We release a new benchmark for lexical substitution, the task of finding
appropriate substitutes for a target word in a context. To assist humans with
writing, lexical substitution systems can suggest words that humans cannot
easily think of. However, existing benchmarks depend on human recall as the
only source of data, and therefore lack coverage of the substitutes that would
be most helpful to humans. Furthermore, annotators often provide substitutes of
low quality, which are not actually appropriate in the given context. We
collect higher-coverage and higher-quality data by framing lexical substitution
as a classification problem, guided by the intuition that it is easier for
humans to judge the appropriateness of candidate substitutes than conjure them
from memory. To this end, we use a context-free thesaurus to produce candidates
and rely on human judgement to determine contextual appropriateness. Compared
to the previous largest benchmark, our Swords benchmark has 4.1x more
substitutes per target word for the same level of quality, and its substitutes
are 1.5x more appropriate (based on human judgement) for the same number of
substitutes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mina Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1"&gt;Alexander Iyabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cheap and Good? Simple and Effective Data Augmentation for Low Resource Machine Reading. (arXiv:2106.04134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04134</id>
        <link href="http://arxiv.org/abs/2106.04134"/>
        <updated>2021-06-09T02:01:47.340Z</updated>
        <summary type="html"><![CDATA[We propose a simple and effective strategy for data augmentation for
low-resource machine reading comprehension (MRC). Our approach first pretrains
the answer extraction components of a MRC system on the augmented data that
contains approximate context of the correct answers, before training it on the
exact answer spans. The approximate context helps the QA method components in
narrowing the location of the answers. We demonstrate that our simple strategy
substantially improves both document retrieval and answer extraction
performance by providing larger context of the answers and additional training
data. In particular, our method significantly improves the performance of BERT
based retriever (15.12\%), and answer extractor (4.33\% F1) on TechQA, a
complex, low-resource MRC task. Further, our data augmentation strategy yields
significant improvements of up to 3.9\% exact match (EM) and 2.7\% F1 for
answer extraction on PolicyQA, another practical but moderate sized QA dataset
that also contains long answer spans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1"&gt;Hoang Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1"&gt;Vikas Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1"&gt;Mihai Surdeanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions. (arXiv:2106.03873v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03873</id>
        <link href="http://arxiv.org/abs/2106.03873"/>
        <updated>2021-06-09T02:01:47.302Z</updated>
        <summary type="html"><![CDATA[In conversation, uptake happens when a speaker builds on the contribution of
their interlocutor by, for example, acknowledging, repeating or reformulating
what they have said. In education, teachers' uptake of student contributions
has been linked to higher student achievement. Yet measuring and improving
teachers' uptake at scale is challenging, as existing methods require expensive
annotation by experts. We propose a framework for computationally measuring
uptake, by (1) releasing a dataset of student-teacher exchanges extracted from
US math classroom transcripts annotated for uptake by experts; (2) formalizing
uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next
utterance classification; (3) conducting a linguistically-motivated comparison
of different unsupervised measures and (4) correlating these measures with
educational outcomes. We find that although repetition captures a significant
part of uptake, pJSD outperforms repetition-based baselines, as it is capable
of identifying a wider range of uptake phenomena like question answering and
reformulation. We apply our uptake measure to three different educational
datasets with outcome indicators. Unlike baseline measures, pJSD correlates
significantly with instruction quality in all three, providing evidence for its
generalizability and for its potential to serve as an automated professional
development tool for teachers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1"&gt;Dorottya Demszky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancenido_Z/0/1/0/all/0/1"&gt;Zid Mancenido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Julie Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_H/0/1/0/all/0/1"&gt;Heather Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1"&gt;Tatsunori Hashimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressivity of Emergent Language is a Trade-off between Contextual Complexity and Unpredictability. (arXiv:2106.03982v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03982</id>
        <link href="http://arxiv.org/abs/2106.03982"/>
        <updated>2021-06-09T02:01:47.259Z</updated>
        <summary type="html"><![CDATA[Researchers are now using deep learning models to explore the emergence of
language in various language games, where simulated agents interact and develop
an emergent language to solve a task. Although it is quite intuitive that
different types of language games posing different communicative challenges
might require emergent languages which encode different levels of information,
there is no existing work exploring the expressivity of the emergent languages.
In this work, we propose a definition of partial order between expressivity
based on the generalisation performance across different language games. We
also validate the hypothesis that expressivity of emergent languages is a
trade-off between the complexity and unpredictability of the context those
languages are used in. Our second novel contribution is introducing contrastive
loss into the implementation of referential games. We show that using our
contrastive loss alleviates the collapse of message types seen using standard
referential loss functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangmin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1"&gt;Kory Mathewson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirby_S/0/1/0/all/0/1"&gt;Simon Kirby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kenny Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03953</id>
        <link href="http://arxiv.org/abs/2106.03953"/>
        <updated>2021-06-09T02:01:47.245Z</updated>
        <summary type="html"><![CDATA[Summarization has usually relied on gold standard summaries to train
extractive or abstractive models. Social media brings a hurdle to summarization
techniques since it requires addressing a multi-document multi-author approach.
We address this challenging task by introducing a novel method that generates
abstractive summaries of online news discussions. Our method extends a
BERT-based architecture, including an attention encoding that fed comments'
likes during the training stage. To train our model, we define a task which
consists of reconstructing high impact comments based on popularity (likes).
Accordingly, our model learns to summarize online discussions based on their
most relevant comments. Our novel approach provides a summary that represents
the most relevant aspects of a news item that users comment on, incorporating
the social context as a source of information to summarize texts in online
social networks. Our model is evaluated using ROUGE scores between the
generated summary and each comment on the thread. Our model, including the
social attention encoding, significantly outperforms both extractive and
abstractive summarization methods based on such evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1"&gt;Ignacio Tampe Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Marcelo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos Milios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Different Types of Subtle Toxicity in Unhealthy Online Conversations. (arXiv:2106.03952v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03952</id>
        <link href="http://arxiv.org/abs/2106.03952"/>
        <updated>2021-06-09T02:01:47.223Z</updated>
        <summary type="html"><![CDATA[This paper investigates the use of machine learning models for the
classification of unhealthy online conversations containing one or more forms
of subtler abuse, such as hostility, sarcasm, and generalization. We leveraged
a public dataset of 44K online comments containing healthy and unhealthy
comments labeled with seven forms of subtle toxicity. We were able to
distinguish between these comments with a top micro F1-score, macro F1-score,
and ROC-AUC of 88.76%, 67.98%, and 0.71, respectively. Hostile comments were
easier to detect than other types of unhealthy comments. We also conducted a
sentiment analysis which revealed that most types of unhealthy comments were
associated with a slight negative sentiment, with hostile comments being the
most negative ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gilda_S/0/1/0/all/0/1"&gt;Shlok Gilda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mirela Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1"&gt;Luiz Giovanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Daniela Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic selection of clustering algorithms using supervised graph embedding. (arXiv:2011.08225v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08225</id>
        <link href="http://arxiv.org/abs/2011.08225"/>
        <updated>2021-06-09T02:01:47.217Z</updated>
        <summary type="html"><![CDATA[The widespread adoption of machine learning (ML) techniques and the extensive
expertise required to apply them have led to increased interest in automated ML
solutions that reduce the need for human intervention. One of the main
challenges in applying ML to previously unseen problems is algorithm selection
- the identification of high-performing algorithm(s) for a given dataset, task,
and evaluation measure. This study addresses the algorithm selection challenge
for data clustering, a fundamental task in data mining that is aimed at
grouping similar objects. We present MARCO-GE, a novel meta-learning approach
for the automated recommendation of clustering algorithms. MARCO-GE first
transforms datasets into graphs and then utilizes a graph convolutional neural
network technique to extract their latent representation. Using the embedding
representations obtained, MARCO-GE trains a ranking meta-model capable of
accurately recommending top-performing algorithms for a new dataset and
clustering evaluation measure. Extensive evaluation on 210 datasets, 13
clustering algorithms, and 10 clustering measures demonstrates the
effectiveness of our approach and its superiority in terms of predictive and
generalization performance over state-of-the-art clustering meta-learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Shapira_N/0/1/0/all/0/1"&gt;Noy Cohen-Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1"&gt;Lior Rokach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NaturalProofs: Mathematical Theorem Proving in Natural Language. (arXiv:2104.01112v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01112</id>
        <link href="http://arxiv.org/abs/2104.01112"/>
        <updated>2021-06-09T02:01:47.195Z</updated>
        <summary type="html"><![CDATA[Understanding and creating mathematics using natural mathematical language -
the mixture of symbolic and natural language used by humans - is a challenging
and important problem for driving progress in machine learning. As a step in
this direction, we develop NaturalProofs, a multi-domain corpus of mathematical
statements and their proofs, written in natural mathematical language.
NaturalProofs unifies broad coverage, deep coverage, and low-resource
mathematical sources, allowing for evaluating both in-distribution and
zero-shot generalization. Using NaturalProofs, we benchmark strong neural
methods on mathematical reference retrieval and generation tasks which test a
system's ability to determine key results that appear in a proof. Large-scale
sequence models show promise compared to classical information retrieval
methods, yet their performance and out-of-domain generalization leave
substantial room for improvement. NaturalProofs opens many avenues for research
on challenging mathematical tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1"&gt;Ronan Le Bras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-Aware Query Auto-completion using Extreme Multi-label Ranking. (arXiv:2012.07654v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07654</id>
        <link href="http://arxiv.org/abs/2012.07654"/>
        <updated>2021-06-09T02:01:47.187Z</updated>
        <summary type="html"><![CDATA[Query auto-completion (QAC) is a fundamental feature in search engines where
the task is to suggest plausible completions of a prefix typed in the search
bar. Previous queries in the user session can provide useful context for the
user's intent and can be leveraged to suggest auto-completions that are more
relevant while adhering to the user's prefix. Such session-aware QACs can be
generated by recent sequence-to-sequence deep learning models; however, these
generative approaches often do not meet the stringent latency requirements of
responding to each user keystroke. Moreover, these generative approaches pose
the risk of showing nonsensical queries.

In this paper, we provide a solution to this problem: we take the novel
approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)
problem where the input is the previous query in the session and the user's
current prefix, while the output space is the set of tens of millions of
queries entered by users in the recent past. We adapt a popular XMR algorithm
for this purpose by proposing several modifications to the key steps in the
algorithm. The proposed modifications yield a 10x improvement in terms of Mean
Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs
dataset. We are able to maintain an inference latency of less than 10 ms while
still using session context. When compared against baseline models of
acceptable latency, we observed a 33% improvement in MRR for short prefixes of
up to 3 characters. Moreover, our model yielded a statistically significant
improvement of 2.81% over a production QAC system in terms of suggestion
acceptance rate, when deployed on the search bar of an online shopping store as
part of an A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nishant Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Daniel N. Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Document Collection Visual Question Answering. (arXiv:2104.14336v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14336</id>
        <link href="http://arxiv.org/abs/2104.14336"/>
        <updated>2021-06-09T02:01:47.176Z</updated>
        <summary type="html"><![CDATA[Current tasks and methods in Document Understanding aims to process documents
as single elements. However, documents are usually organized in collections
(historical records, purchase invoices), that provide context useful for their
interpretation. To address this problem, we introduce Document Collection
Visual Question Answering (DocCVQA) a new dataset and related task, where
questions are posed over a whole collection of document images and the goal is
not only to provide the answer to the given question, but also to retrieve the
set of documents that contain the information needed to infer the answer. Along
with the dataset we propose a new evaluation metric and baselines which provide
further insights to the new dataset and task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1"&gt;Rub&amp;#xe8;n Tito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1"&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1"&gt;Ernest Valveny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Large-Scale Analysis of Mixed Initiative in Information-Seeking Dialogues for Conversational Search. (arXiv:2104.07096v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07096</id>
        <link href="http://arxiv.org/abs/2104.07096"/>
        <updated>2021-06-09T02:01:47.169Z</updated>
        <summary type="html"><![CDATA[Conversational search is a relatively young area of research that aims at
automating an information-seeking dialogue. In this paper we help to position
it with respect to other research areas within conversational Artificial
Intelligence (AI) by analysing the structural properties of an
information-seeking dialogue. To this end, we perform a large-scale dialogue
analysis of more than 150K transcripts from 16 publicly available dialogue
datasets. These datasets were collected to inform different dialogue-based
tasks including conversational search. We extract different patterns of mixed
initiative from these dialogue transcripts and use them to compare dialogues of
different types. Moreover, we contrast the patterns found in
information-seeking dialogues that are being used for research purposes with
the patterns found in virtual reference interviews that were conducted by
professional librarians. The insights we provide (1) establish close relations
between conversational search and other conversational AI tasks; and (2)
uncover limitations of existing conversational datasets to inform future data
collection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1"&gt;Svitlana Vakulenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1"&gt;Evangelos Kanoulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Hypothetical Events for Abductive Inference. (arXiv:2106.03973v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03973</id>
        <link href="http://arxiv.org/abs/2106.03973"/>
        <updated>2021-06-09T02:01:47.160Z</updated>
        <summary type="html"><![CDATA[Abductive reasoning starts from some observations and aims at finding the
most plausible explanation for these observations. To perform abduction, humans
often make use of temporal and causal inferences, and knowledge about how some
hypothetical situation can result in different outcomes. This work offers the
first study of how such knowledge impacts the Abductive NLI task -- which
consists in choosing the more likely explanation for given observations. We
train a specialized language model LMI that is tasked to generate what could
happen next from a hypothetical scenario that evolves from a given event. We
then propose a multi-task model MTL to solve the Abductive NLI task, which
predicts a plausible explanation by a) considering different possible events
emerging from candidate hypotheses -- events generated by LMI -- and b)
selecting the one that is most similar to the observed outcome. We show that
our MTL model improves over prior vanilla pre-trained LMs fine-tuned on
Abductive NLI. Our manual evaluation and analysis suggest that learning about
possible next events from different hypothetical scenarios supports abductive
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debjit Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Fairness in Classification with a Model-Agnostic Multi-Objective Algorithm. (arXiv:2009.04441v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04441</id>
        <link href="http://arxiv.org/abs/2009.04441"/>
        <updated>2021-06-09T02:01:47.142Z</updated>
        <summary type="html"><![CDATA[The goal of fairness in classification is to learn a classifier that does not
discriminate against groups of individuals based on sensitive attributes, such
as race and gender. One approach to designing fair algorithms is to use
relaxations of fairness notions as regularization terms or in a constrained
optimization problem. We observe that the hyperbolic tangent function can
approximate the indicator function. We leverage this property to define a
differentiable relaxation that approximates fairness notions provably better
than existing relaxations. In addition, we propose a model-agnostic
multi-objective architecture that can simultaneously optimize for multiple
fairness notions and multiple sensitive attributes and supports all statistical
parity-based notions of fairness. We use our relaxation with the
multi-objective architecture to learn fair classifiers. Experiments on public
datasets show that our method suffers a significantly lower loss of accuracy
than current debiasing algorithms relative to the unconstrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padh_K/0/1/0/all/0/1"&gt;Kirtan Padh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in North Carolina. (arXiv:2106.04515v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04515</id>
        <link href="http://arxiv.org/abs/2106.04515"/>
        <updated>2021-06-09T02:01:47.102Z</updated>
        <summary type="html"><![CDATA[Coronavirus disease (COVID-19) pandemic has changed various aspects of
people's lives and behaviors. At this stage, there are no other ways to control
the natural progression of the disease than adopting mitigation strategies such
as wearing masks, watching distance, and washing hands. Moreover, at this time
of social distancing, social media plays a key role in connecting people and
providing a platform for expressing their feelings. In this study, we tap into
social media to surveil the uptake of mitigation and detection strategies, and
capture issues and concerns about the pandemic. In particular, we explore the
research question, "how much can be learned regarding the public uptake of
mitigation strategies and concerns about COVID-19 pandemic by using natural
language processing on Reddit posts?" After extracting COVID-related posts from
the four largest subreddit communities of North Carolina over six months, we
performed NLP-based preprocessing to clean the noisy data. We employed a custom
Named-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)
method for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',
and 'testing' are the most prevalent named-entities for "Personal Protective
Equipment", "symptoms", and "testing" categories, respectively. We also
observed that the most discussed topics are related to testing, masks, and
employment. The mitigation measures are the most prevalent theme of discussion
across all subreddits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitfield_C/0/1/0/all/0/1"&gt;Christopher Whitfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_M/0/1/0/all/0/1"&gt;Mohad Anwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A highly scalable repository of waveform and vital signs data from bedside monitoring devices. (arXiv:2106.03965v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.03965</id>
        <link href="http://arxiv.org/abs/2106.03965"/>
        <updated>2021-06-09T02:01:47.095Z</updated>
        <summary type="html"><![CDATA[The advent of cost effective cloud computing over the past decade and
ever-growing accumulation of high-fidelity clinical data in a modern hospital
setting is leading to new opportunities for translational medicine. Machine
learning is driving the appetite of the research community for various types of
signal data such as patient vitals. Health care systems, however, are ill
suited for massive processing of large volumes of data. In addition, due to the
sheer magnitude of the data being collected, it is not feasible to retain all
of the data in health care systems in perpetuity. This gold mine of information
gets purged periodically thereby losing invaluable future research
opportunities. We have developed a highly scalable solution that: a) siphons
off patient vital data on a nightly basis from on-premises bio-medical systems
to a cloud storage location as a permanent archive, b) reconstructs the
database in the cloud, c) generates waveforms, alarms and numeric data in a
research-ready format, and d) uploads the processed data to a storage location
in the cloud ready for research.

The data is de-identified and catalogued such that it can be joined with
Electronic Medical Records (EMR) and other ancillary data types such as
electroencephalogram (EEG), radiology, video monitoring etc. This technique
eliminates the research burden from health care systems. This highly scalable
solution is used to process high density patient monitoring data aggregated by
the Philips Patient Information Center iX (PIC iX) hospital surveillance system
for archival storage in the Philips Data Warehouse Connect enterprise-level
database. The solution is part of a broader platform that supports a secure
high performance clinical data science platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malunjkar_S/0/1/0/all/0/1"&gt;Sanjay Malunjkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1"&gt;Susan Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1"&gt;Somalee Datta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Periodicity and Interactivity in Multi-Interest Framework for Sequential Recommendation. (arXiv:2106.04415v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04415</id>
        <link href="http://arxiv.org/abs/2106.04415"/>
        <updated>2021-06-09T02:01:47.082Z</updated>
        <summary type="html"><![CDATA[Sequential recommendation systems alleviate the problem of information
overload, and have attracted increasing attention in the literature. Most prior
works usually obtain an overall representation based on the user's behavior
sequence, which can not sufficiently reflect the multiple interests of the
user. To this end, we propose a novel method called PIMI to mitigate this
issue. PIMI can model the user's multi-interest representation effectively by
considering both the periodicity and interactivity in the item sequence.
Specifically, we design a periodicity-aware module to utilize the time interval
information between user's behaviors. Meanwhile, an ingenious graph is proposed
to enhance the interactivity between items in user's behavior sequence, which
can capture both global and local item features. Finally, a multi-interest
extraction module is applied to describe user's multiple interests based on the
obtained item representation. Extensive experiments on two real-world datasets
Amazon and Taobao show that PIMI outperforms state-of-the-art methods
consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gaode Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinghua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yanyan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1"&gt;Cong Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1"&gt;Ji Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. (arXiv:2106.04408v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04408</id>
        <link href="http://arxiv.org/abs/2106.04408"/>
        <updated>2021-06-09T02:01:47.074Z</updated>
        <summary type="html"><![CDATA[User interest modeling is critical for personalized news recommendation.
Existing news recommendation methods usually learn a single user embedding for
each user from their previous behaviors to represent their overall interest.
However, user interest is usually diverse and multi-grained, which is difficult
to be accurately modeled by a single user embedding. In this paper, we propose
a news recommendation method with hierarchical user interest modeling, named
HieRec. Instead of a single user embedding, in our method each user is
represented in a hierarchical interest tree to better capture their diverse and
multi-grained interest in news. We use a three-level hierarchy to represent 1)
overall user interest; 2) user interest in coarse-grained topics like sports;
and 3) user interest in fine-grained topics like football. Moreover, we propose
a hierarchical user interest matching framework to match candidate news with
different levels of user interest for more accurate user interest targeting.
Extensive experiments on two real-world datasets validate our method can
effectively improve the performance of user modeling for personalized news
recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Peiru Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. (arXiv:2106.04128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04128</id>
        <link href="http://arxiv.org/abs/2106.04128"/>
        <updated>2021-06-09T02:01:47.055Z</updated>
        <summary type="html"><![CDATA[We study the task of conversational fashion image retrieval via multiturn
natural language feedback. Most previous studies are based on single-turn
settings. Existing models on multiturn conversational fashion image retrieval
have limitations, such as employing traditional models, and leading to
ineffective performance. We propose a novel framework that can effectively
handle conversational fashion image retrieval with multiturn natural language
feedback texts. One characteristic of the framework is that it searches for
candidate images based on exploitation of the encoded reference image and
feedback text information together with the conversation history. Furthermore,
the image fashion attribute information is leveraged via a mutual attention
strategy. Since there is no existing fashion dataset suitable for the multiturn
setting of our task, we derive a large-scale multiturn fashion dataset via
additional manual annotation efforts on an existing single-turn dataset. The
experiments show that our proposed model significantly outperforms existing
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yifei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-Start Users. (arXiv:2005.12979v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12979</id>
        <link href="http://arxiv.org/abs/2005.12979"/>
        <updated>2021-06-09T02:01:47.041Z</updated>
        <summary type="html"><![CDATA[Static recommendation methods like collaborative filtering suffer from the
inherent limitation of performing real-time personalization for cold-start
users. Online recommendation, e.g., multi-armed bandit approach, addresses this
limitation by interactively exploring user preference online and pursuing the
exploration-exploitation (EE) trade-off. However, existing bandit-based methods
model recommendation actions homogeneously. Specifically, they only consider
the items as the arms, being incapable of handling the item attributes, which
naturally provide interpretable information of user's current demands and can
effectively filter out undesired items. In this work, we consider the
conversational recommendation for cold-start users, where a system can both ask
the attributes from and recommend items to a user interactively. This important
scenario was studied in a recent work. However, it employs a hand-crafted
function to decide when to ask attributes or make recommendations. Such
separate modeling of attributes and items makes the effectiveness of the system
highly rely on the choice of the hand-crafted function, thus introducing
fragility to the system. To address this limitation, we seamlessly unify
attributes and items in the same arm space and achieve their EE trade-offs
automatically using the framework of Thompson Sampling. Our Conversational
Thompson Sampling (ConTS) model holistically solves all questions in
conversational recommendation by choosing the arm with the maximal reward to
play. Extensive experiments on three benchmark datasets show that ConTS
outperforms the state-of-the-art methods Conversational UCB (ConUCB) and
Estimation-Action-Reflection model in both metrics of success rate and average
number of conversation turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shijun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defining definition: a Text mining Approach to Define Innovative Technological Fields. (arXiv:2106.04210v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04210</id>
        <link href="http://arxiv.org/abs/2106.04210"/>
        <updated>2021-06-09T02:01:47.032Z</updated>
        <summary type="html"><![CDATA[One of the first task of an innovative project is delineating the scope of
the project itself or of the product/service to be developed. A wrong scope
definition can determine (in the worst case) project failure. A good scope
definition become even more relevant in technological intensive innovation
projects, nowadays characterized by a highly dynamic multidisciplinary,
turbulent and uncertain environment. In these cases, the boundaries of the
project are not easily detectable and it is difficult to decide what it is
in-scope and out-of-scope. The present work proposes a tool for the scope
delineation process, that automatically define an innovative technological
field or a new technology. The tool is based on Text Mining algorithm that
exploits Elsevier's Scopus abstracts in order to the extract relevant data to
define a technological scope. The automatic definition tool is then applied on
four case studies: Artificial Intelligence and Data Science. The results show
how the tool can provide many crucial information in the definition process of
a technological field. In particular for the target technological field (or
technology), it provides the definition and other elements related to the
target.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giordano_V/0/1/0/all/0/1"&gt;Vito Giordano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1"&gt;Filippo Chiarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervelli_E/0/1/0/all/0/1"&gt;Elena Cervelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MindReader: Recommendation over Knowledge Graph Entities with Explicit User Ratings. (arXiv:2106.04209v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04209</id>
        <link href="http://arxiv.org/abs/2106.04209"/>
        <updated>2021-06-09T02:01:47.017Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KGs) have been integrated in several models of
recommendation to augment the informational value of an item by means of its
related entities in the graph. Yet, existing datasets only provide explicit
ratings on items and no information is provided about user opinions of other
(non-recommendable) entities. To overcome this limitation, we introduce a new
dataset, called the MindReader, providing explicit user ratings both for items
and for KG entities. In this first version, the MindReader dataset provides
more than 102 thousands explicit ratings collected from 1,174 real users on
both items and entities from a KG in the movie domain. This dataset has been
collected through an online interview application that we also release open
source. As a demonstration of the importance of this new dataset, we present a
comparative study of the effect of the inclusion of ratings on non-item KG
entities in a variety of state-of-the-art recommendation models. In particular,
we show that most models, whether designed specifically for graph data or not,
see improvements in recommendation quality when trained on explicit non-item
ratings. Moreover, for some models, we show that non-item ratings can
effectively replace item ratings without loss of recommendation quality. This
finding, thanks also to an observed greater familiarity of users towards common
KG entities than towards long-tail items, motivates the use of KG entities for
both warm and cold-start recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brams_A/0/1/0/all/0/1"&gt;Anders H. Brams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobsen_A/0/1/0/all/0/1"&gt;Anders L. Jakobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jendal_T/0/1/0/all/0/1"&gt;Theis E. Jendal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lissandrini_M/0/1/0/all/0/1"&gt;Matteo Lissandrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolog_P/0/1/0/all/0/1"&gt;Peter Dolog&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hose_K/0/1/0/all/0/1"&gt;Katja Hose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Neural Collaborative Filtering. (arXiv:2106.04405v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04405</id>
        <link href="http://arxiv.org/abs/2106.04405"/>
        <updated>2021-06-09T02:01:46.977Z</updated>
        <summary type="html"><![CDATA[In this work, we present a federated version of the state-of-the-art Neural
Collaborative Filtering (NCF) approach for item recommendations. The system,
named FedNCF, allows learning without requiring users to expose or transmit
their raw data. Experimental validation shows that FedNCF achieves comparable
recommendation quality to the original NCF system. Although federated learning
(FL) enables learning without raw data transmission, recent attacks showed that
FL alone does not eliminate privacy concerns. To overcome this challenge, we
integrate a privacy-preserving enhancement with a secure aggregation scheme
that satisfies the security requirements against an honest-but-curious (HBC)
entity, without affecting the quality of the original model. Finally, we
discuss the peculiarities observed in the application of FL in a collaborative
filtering (CF) task as well as we evaluate the privacy-preserving mechanism in
terms of computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perifanis_V/0/1/0/all/0/1"&gt;Vasileios Perifanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efraimidis_P/0/1/0/all/0/1"&gt;Pavlos S. Efraimidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConSTR: A Contextual Search Term Recommender. (arXiv:2106.04376v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.04376</id>
        <link href="http://arxiv.org/abs/2106.04376"/>
        <updated>2021-06-09T02:01:46.968Z</updated>
        <summary type="html"><![CDATA[In this demo paper, we present ConSTR, a novel Contextual Search Term
Recommender that utilises the user's interaction context for search term
recommendation and literature retrieval. ConSTR integrates a two-layered
recommendation interface: the first layer suggests terms with respect to a
user's current search term, and the second layer suggests terms based on the
users' previous search activities (interaction context). For the demonstration,
ConSTR is built on the arXiv, an academic repository consisting of 1.8 million
documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kramer_T/0/1/0/all/0/1"&gt;Thomas Kr&amp;#xe4;mer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carevic_Z/0/1/0/all/0/1"&gt;Zeljko Carevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Dwaipayan Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klas_C/0/1/0/all/0/1"&gt;Claus-Peter Klas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"&gt;Philipp Mayr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Struggle with Academic Plagiarism: Approaches based on Semantic Similarity. (arXiv:2106.04404v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04404</id>
        <link href="http://arxiv.org/abs/2106.04404"/>
        <updated>2021-06-09T02:01:46.958Z</updated>
        <summary type="html"><![CDATA[Academic plagiarism is a serious problem nowadays. Due to the existence of
inexhaustible sources of digital information, today it is easier to plagiarize
more than ever before. The good thing is that plagiarism detection techniques
have improved and are powerful enough to detect attempts of plagiarism in
education. We are now witnessing efficient plagiarism detection software in
action, such as Turnitin, iThenticate or SafeAssign. In the introduction we
explore software that is used within the Croatian academic community for
plagiarism detection in universities and/or in scientific journals. The
question is: is this enough? Current software has proven to be successful,
however the problem of identifying paraphrasing or obfuscation plagiarism
remains unresolved. In this paper we present a report of how semantic
similarity measures can be used in the plagiarism detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-09T02:01:46.947Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Service Addition in Multilevel Index Model for Edge Computing. (arXiv:2106.04494v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04494</id>
        <link href="http://arxiv.org/abs/2106.04494"/>
        <updated>2021-06-09T02:01:46.934Z</updated>
        <summary type="html"><![CDATA[With the development of Edge Computing and Artificial Intelligence (AI)
technologies, edge devices are witnessed to generate data at unprecedented
volume. The Edge Intelligence (EI) has led to the emergence of edge devices in
various application domains. The EI can provide efficient services to
delay-sensitive applications, where the edge devices are deployed as edge nodes
to host the majority of execution, which can effectively manage services and
improve service discovery efficiency. The multilevel index model is a
well-known model used for indexing service, such a model is being introduced
and optimized in the edge environments to efficiently services discovery whilst
managing large volumes of data. However, effectively updating the multilevel
index model by adding new services timely and precisely in the dynamic Edge
Computing environments is still a challenge. Addressing this issue, this paper
proposes a designated key selection method to improve the efficiency of adding
services in the multilevel index models. Our experimental results show that in
the partial index and the full index of multilevel index model, our method
reduces the service addition time by around 84% and 76%, respectively when
compared with the original key selection method and by around 78% and 66%,
respectively when compared with the random selection method. Our proposed
method significantly improves the service addition efficiency in the multilevel
index model, when compared with existing state-of-the-art key selection
methods, without compromising the service retrieval stability to any notable
level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiayan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1"&gt;Ashiq Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneerselvam_J/0/1/0/all/0/1"&gt;John Panneerselvam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review Polarity-wise Recommender. (arXiv:2106.04155v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04155</id>
        <link href="http://arxiv.org/abs/2106.04155"/>
        <updated>2021-06-09T02:01:46.754Z</updated>
        <summary type="html"><![CDATA[Utilizing review information to enhance recommendation, the de facto
review-involved recommender systems, have received increasing interests over
the past few years. Thereinto, one advanced branch is to extract salient
aspects from textual reviews (i.e., the item attributes that users express) and
combine them with the matrix factorization technique. However, existing
approaches all ignore the fact that semantically different reviews often
include opposite aspect information. In particular, positive reviews usually
express aspects that users prefer, while negative ones describe aspects that
users reject. As a result, it may mislead the recommender systems into making
incorrect decisions pertaining to user preference modeling. Towards this end,
in this paper, we propose a Review Polarity-wise Recommender model, dubbed as
RPR, to discriminately treat reviews with different polarities. To be specific,
in this model, positive and negative reviews are separately gathered and
utilized to model the user-preferred and user-rejected aspects, respectively.
Besides, in order to overcome the imbalance problem of semantically different
reviews, we also develop an aspect-aware importance weighting approach to align
the aspect importance for these two kinds of reviews. Extensive experiments
conducted on eight benchmark datasets have demonstrated the superiority of our
model as compared to a series of state-of-the-art review-involved baselines.
Moreover, our method can provide certain explanations to the real-world rating
prediction scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Han Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yangyang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianhua Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. (arXiv:2009.07526v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07526</id>
        <link href="http://arxiv.org/abs/2009.07526"/>
        <updated>2021-06-09T02:01:46.700Z</updated>
        <summary type="html"><![CDATA[Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation. (arXiv:2106.04403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04403</id>
        <link href="http://arxiv.org/abs/2106.04403"/>
        <updated>2021-06-09T02:01:46.689Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have brought significant progress in visual
grounding tasks such as language-guided video object segmentation. However,
collecting large datasets for these tasks is expensive in terms of annotation
time, which represents a bottleneck. To this end, we propose a novel method,
namely SynthRef, for generating synthetic referring expressions for target
objects in an image (or video frame), and we also present and disseminate the
first large-scale dataset with synthetic referring expressions for video object
segmentation. Our experiments demonstrate that by training with our synthetic
referring expressions one can improve the ability of a model to generalize
across different datasets, without any additional annotation cost. Moreover,
our formulation allows its application to any object detection or segmentation
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazakos_I/0/1/0/all/0/1"&gt;Ioannis Kazakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellver_M/0/1/0/all/0/1"&gt;Miriam Bellver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1"&gt;Carina Silberer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Video Configuration and Bitrate Allocation for Vehicles. (arXiv:2102.10898v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10898</id>
        <link href="http://arxiv.org/abs/2102.10898"/>
        <updated>2021-06-09T02:01:46.656Z</updated>
        <summary type="html"><![CDATA[Vehicles with autonomous driving capabilities are present on public streets.
However, edge cases remain that still require a human in-vehicle driver.
Assuming the vehicle manages to come to a safe state in an automated fashion,
teleoperated driving technology enables a human to resolve the situation
remotely by a control interface connected via a mobile network. While this is a
promising solution, it also introduces technical challenges, one of them being
the necessity to transmit video data of multiple cameras from the vehicle to
the human operator. In this paper, an adaptive video streaming framework
specifically designed for teleoperated vehicles is proposed and demonstrated.
The framework enables automatic reconfiguration of the video streams of the
multi-camera system at runtime. Predictions of variable transmission service
quality are taken into account. With the objective to improve visual quality,
the framework uses so-called rate-quality models to dynamically allocate
bitrates and select resolution scaling factors. Results from deploying the
proposed framework on an actual teleoperated driving system are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schimpe_A/0/1/0/all/0/1"&gt;Andreas Schimpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_S/0/1/0/all/0/1"&gt;Simon Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diermeyer_F/0/1/0/all/0/1"&gt;Frank Diermeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding. (arXiv:2106.04053v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04053</id>
        <link href="http://arxiv.org/abs/2106.04053"/>
        <updated>2021-06-09T02:01:46.640Z</updated>
        <summary type="html"><![CDATA[In this paper, we are tackling the weakly-supervised referring expression
grounding task, for the localization of a referent object in an image according
to a query sentence, where the mapping between image regions and queries are
not available during the training stage. In traditional methods, an object
region that best matches the referring expression is picked out, and then the
query sentence is reconstructed from the selected region, where the
reconstruction difference serves as the loss for back-propagation. The existing
methods, however, conduct both the matching and the reconstruction
approximately as they ignore the fact that the matching correctness is unknown.
To overcome this limitation, a discriminative triad is designed here as the
basis to the solution, through which a query can be converted into one or
multiple discriminative triads in a very scalable way. Based on the
discriminative triad, we further propose the triad-level matching and
reconstruction modules which are lightweight yet effective for the
weakly-supervised training, making it three times lighter and faster than the
previous state-of-the-art methods. One important merit of our work is its
superior performance despite the simple and neat design. Specifically, the
proposed method achieves a new state-of-the-art accuracy when evaluated on
RefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is
4.17%, 4.08% and 7.8% higher than the previous one, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingjie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulermas_J/0/1/0/all/0/1"&gt;John Y. Goulermas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation. (arXiv:2106.04151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04151</id>
        <link href="http://arxiv.org/abs/2106.04151"/>
        <updated>2021-06-09T02:01:46.600Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned
from a well-labeled source domain to an unlabeled target domain. Recently,
adversarial domain adaptation with two distinct classifiers (bi-classifier) has
been introduced into UDA which is effective to align distributions between
different domains. Previous bi-classifier adversarial learning methods only
focus on the similarity between the outputs of two distinct classifiers.
However, the similarity of the outputs cannot guarantee the accuracy of target
samples, i.e., target samples may match to wrong categories even if the
discrepancy between two classifiers is small. To challenge this issue, in this
paper, we propose a cross-domain gradient discrepancy minimization (CGDM)
method which explicitly minimizes the discrepancy of gradients generated by
source samples and target samples. Specifically, the gradient gives a cue for
the semantic information of target samples so it can be used as a good
supervision to improve the accuracy of target samples. In order to compute the
gradient signal of target samples, we further obtain target pseudo labels
through a clustering-based self-supervised learning. Extensive experiments on
three widely used UDA datasets show that our method surpasses many previous
state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hongzu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-09T00:28:49.163Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-09T00:28:49.118Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Compression-Compilation Framework for On-mobile Real-time BERT Applications. (arXiv:2106.00526v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00526</id>
        <link href="http://arxiv.org/abs/2106.00526"/>
        <updated>2021-06-08T22:44:25.107Z</updated>
        <summary type="html"><![CDATA[Transformer-based deep learning models have increasingly demonstrated high
accuracy on many natural language processing (NLP) tasks. In this paper, we
propose a compression-compilation co-design framework that can guarantee the
identified model to meet both resource and real-time specifications of mobile
devices. Our framework applies a compiler-aware neural architecture
optimization method (CANAO), which can generate the optimal compressed model
that balances both accuracy and latency. We are able to achieve up to 7.8x
speedup compared with TensorFlow-Lite with only minor accuracy loss. We present
two types of BERT applications on mobile devices: Question Answering (QA) and
Text Generation. Both can be executed in real-time with latency as low as 45ms.
Videos for demonstrating the framework can be found on
https://www.youtube.com/watch?v=_WIRvK_2PZI]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhenglun Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jiexiong Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Caiwen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained Document Representations. (arXiv:2106.00590v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-08T22:44:25.008Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Regret for Bandit Convex Optimisation of Ridge Functions. (arXiv:2106.00444v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00444</id>
        <link href="http://arxiv.org/abs/2106.00444"/>
        <updated>2021-06-08T22:44:24.996Z</updated>
        <summary type="html"><![CDATA[We analyse adversarial bandit convex optimisation with an adversary that is
restricted to playing functions of the form $f_t(x) = g_t(\langle x,
\theta\rangle)$ for convex $g_t : \mathbb R \to \mathbb R$ and unknown $\theta
\in \mathbb R^d$ that is homogeneous over time. We provide a short
information-theoretic proof that the minimax regret is at most $O(d \sqrt{n}
\log(n \operatorname{diam}(\mathcal K)))$ where $n$ is the number of
interactions, $d$ the dimension and $\operatorname{diam}(\mathcal K)$ is the
diameter of the constraint set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenBox: A Generalized Black-box Optimization Service. (arXiv:2106.00421v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00421</id>
        <link href="http://arxiv.org/abs/2106.00421"/>
        <updated>2021-06-08T22:44:24.840Z</updated>
        <summary type="html"><![CDATA[Black-box optimization (BBO) has a broad range of applications, including
automatic machine learning, engineering, physics, and experimental design.
However, it remains a challenge for users to apply BBO methods to their
problems at hand with existing software packages, in terms of applicability,
performance, and efficiency. In this paper, we build OpenBox, an open-source
and general-purpose BBO service with improved usability. The modular design
behind OpenBox also facilitates flexible abstraction and optimization of basic
BBO components that are common in other existing systems. OpenBox is
distributed, fault-tolerant, and scalable. To improve efficiency, OpenBox
further utilizes "algorithm agnostic" parallelization and transfer learning.
Our experimental results demonstrate the effectiveness and efficiency of
OpenBox compared to existing systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huaijun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingchao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anti-Koopmanism. (arXiv:2106.00106v2 [math.FA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00106</id>
        <link href="http://arxiv.org/abs/2106.00106"/>
        <updated>2021-06-08T22:44:24.822Z</updated>
        <summary type="html"><![CDATA[This article addresses several longstanding misconceptions concerning Koopman
operators, including the existence of lattices of eigenfunctions, common
eigenfunctions between Koopman operators, and boundedness and compactness of
Koopman operators, among others. Counterexamples are provided for each
misconception. This manuscript also proves that the Gaussian RBF's native space
only supports bounded Koopman operator corresponding to affine dynamics, which
shows that the assumption of boundedness is very limiting. A framework for DMD
is presented that requires only densely defined Koopman operators over
reproducing kernel Hilbert spaces, and the effectiveness of this approach is
demonstrated through reconstruction examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gonzalez_E/0/1/0/all/0/1"&gt;Efrain Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Abudia_M/0/1/0/all/0/1"&gt;Moad Abudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jury_M/0/1/0/all/0/1"&gt;Michael Jury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look Wide and Interpret Twice: Improving Performance on Interactive Instruction-following Tasks. (arXiv:2106.00596v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00596</id>
        <link href="http://arxiv.org/abs/2106.00596"/>
        <updated>2021-06-08T22:44:24.609Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in the community in making an embodied AI agent
perform a complicated task while interacting with an environment following
natural language directives. Recent studies have tackled the problem using
ALFRED, a well-designed dataset for the task, but achieved only very low
accuracy. This paper proposes a new method, which outperforms the previous
methods by a large margin. It is based on a combination of several new ideas.
One is a two-stage interpretation of the provided instructions. The method
first selects and interprets an instruction without using visual information,
yielding a tentative action sequence prediction. It then integrates the
prediction with the visual information etc., yielding the final prediction of
an action and an object. As the object's class to interact is identified in the
first stage, it can accurately select the correct object from the input image.
Moreover, our method considers multiple egocentric views of the environment and
extracts essential information by applying hierarchical attention conditioned
on the current instruction. This contributes to the accurate prediction of
actions for navigation. A preliminary version of the method won the ALFRED
Challenge 2020. The current version achieves the unseen environment's success
rate of 4.45% with a single view, which is further improved to 8.37% with
multiple views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Van-Quang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1"&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1"&gt;Takayuki Okatani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpanNER: Named Entity Re-/Recognition as Span Prediction. (arXiv:2106.00641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00641</id>
        <link href="http://arxiv.org/abs/2106.00641"/>
        <updated>2021-06-08T22:44:24.222Z</updated>
        <summary type="html"><![CDATA[Recent years have seen the paradigm shift of Named Entity Recognition (NER)
systems from sequence labeling to span prediction. Despite its preliminary
effectiveness, the span prediction model's architectural bias has not been
fully understood. In this paper, we first investigate the strengths and
weaknesses when the span prediction model is used for named entity recognition
compared with the sequence labeling framework and how to further improve it,
which motivates us to make complementary advantages of systems based on
different paradigms. We then reveal that span prediction, simultaneously, can
serve as a system combiner to re-recognize named entities from different
systems' outputs. We experimentally implement 154 systems on 11 datasets,
covering three languages, comprehensive results show the effectiveness of span
prediction models that both serve as base NER systems and system combiners. We
make all code and datasets available: \url{https://github.com/neulab/spanner},
as well as an online system demo: \url{this http URL}. Our model also has
been deployed into the ExplainaBoard platform, which allows users to flexibly
perform a system combination of top-scoring systems in an interactive way:
\url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jinlan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained Document Representations. (arXiv:2106.00590v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-08T22:44:24.202Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction. (arXiv:2106.00459v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00459</id>
        <link href="http://arxiv.org/abs/2106.00459"/>
        <updated>2021-06-08T22:44:23.904Z</updated>
        <summary type="html"><![CDATA[We present a novel method for relation extraction (RE) from a single
sentence, mapping the sentence and two given entities to a canonical fact in a
knowledge graph (KG). Especially in this presumed sentential RE setting, the
context of a single sentence is often sparse. This paper introduces the KGPool
method to address this sparsity, dynamically expanding the context with
additional facts from the KG. It learns the representation of these facts
(entity alias, entity descriptions, etc.) using neural methods, supplementing
the sentential context. Unlike existing methods that statically use all
expanded facts, KGPool conditions this expansion on the sentence. We study the
efficacy of KGPool by evaluating it with different neural models and KGs
(Wikidata and NYT Freebase). Our experimental evaluation on standard datasets
shows that by feeding the KGPool representation into a Graph Neural Network,
the overall method is significantly more accurate than state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1"&gt;Abhishek Nadgeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1"&gt;Anson Bastos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1"&gt;Isaiah Onando Mulang&amp;#x27;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1"&gt;Johannes Hoffart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1"&gt;Saeedeh Shekarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraswat_V/0/1/0/all/0/1"&gt;Vijay Saraswat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained Document Representations. (arXiv:2106.00590v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-08T22:44:23.879Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03515</id>
        <link href="http://arxiv.org/abs/2012.03515"/>
        <updated>2021-06-08T02:20:28.134Z</updated>
        <summary type="html"><![CDATA[Few-shot learning methods offer pre-training techniques optimized for easier
later adaptation of the model to new classes (unseen during training) using one
or a few examples. This adaptivity to unseen classes is especially important
for many practical applications where the pre-trained label space cannot remain
fixed for effective use and the model needs to be "specialized" to support new
categories on the fly. One particularly interesting scenario, essentially
overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where
the training classes (e.g. animals) are of much `coarser granularity' than the
target (test) classes (e.g. breeds). A very practical example of C2FS is when
the target classes are sub-classes of the training classes. Intuitively, it is
especially challenging as (both regular and few-shot) supervised pre-training
tends to learn to ignore intra-class variability which is essential for
separating sub-classes. In this paper, we introduce a novel 'Angular
normalization' module that allows to effectively combine supervised and
self-supervised contrastive pre-training to approach the proposed C2FS task,
demonstrating significant gains in a broad study over multiple baselines and
datasets. We hope that this work will help to pave the way for future research
on this new, challenging, and very practical topic of C2FS classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1"&gt;Guy Bukchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1"&gt;Eli Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1"&gt;Ori Shahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:28.103Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping Saddle Points Faster with Stochastic Momentum. (arXiv:2106.02985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02985</id>
        <link href="http://arxiv.org/abs/2106.02985"/>
        <updated>2021-06-08T02:20:28.095Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) with stochastic momentum is popular in
nonconvex stochastic optimization and particularly for the training of deep
neural networks. In standard SGD, parameters are updated by improving along the
path of the gradient at the current iterate on a batch of examples, where the
addition of a ``momentum'' term biases the update in the direction of the
previous change in parameters. In non-stochastic convex optimization one can
show that a momentum adjustment provably reduces convergence time in many
settings, yet such results have been elusive in the stochastic and non-convex
settings. At the same time, a widely-observed empirical phenomenon is that in
training deep networks stochastic momentum appears to significantly improve
convergence time, variants of it have flourished in the development of other
popular update methods, e.g. ADAM [KB15], AMSGrad [RKK18], etc. Yet theoretical
justification for the use of stochastic momentum has remained a significant
open question. In this paper we propose an answer: stochastic momentum improves
deep network training because it modifies SGD to escape saddle points faster
and, consequently, to more quickly find a second order stationary point. Our
theoretical results also shed light on the related question of how to choose
the ideal momentum parameter--our analysis suggests that $\beta \in [0,1)$
should be large (close to 1), which comports with empirical findings. We also
provide experimental findings that further validate these conclusions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating SGD with Data Ordering Attacks. (arXiv:2104.09667v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09667</id>
        <link href="http://arxiv.org/abs/2104.09667"/>
        <updated>2021-06-08T02:20:28.089Z</updated>
        <summary type="html"><![CDATA[Machine learning is vulnerable to a wide variety of attacks. It is now well
understood that by changing the underlying data distribution, an adversary can
poison the model trained with it or introduce backdoors. In this paper we
present a novel class of training-time attacks that require no changes to the
underlying dataset or model architecture, but instead only change the order in
which data are supplied to the model. In particular, we find that the attacker
can either prevent the model from learning, or poison it to learn behaviours
specified by the attacker. Furthermore, we find that even a single
adversarially-ordered epoch can be enough to slow down model learning, or even
to reset all of the learning progress. Indeed, the attacks presented here are
not specific to the model or dataset, but rather target the stochastic nature
of modern learning procedures. We extensively evaluate our attacks on computer
vision and natural language benchmarks to find that the adversary can disrupt
model training and even introduce backdoors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1"&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks. (arXiv:2106.02804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02804</id>
        <link href="http://arxiv.org/abs/2106.02804"/>
        <updated>2021-06-08T02:20:28.082Z</updated>
        <summary type="html"><![CDATA[In applied image segmentation tasks, the ability to provide numerous and
precise labels for training is paramount to the accuracy of the model at
inference time. However, this overhead is often neglected, and recently
proposed segmentation architectures rely heavily on the availability and
fidelity of ground truth labels to achieve state-of-the-art accuracies. Failure
to acknowledge the difficulty in creating adequate ground truths can lead to an
over-reliance on pre-trained models or a lack of adoption in real-world
applications. We introduce Points2Polygons (P2P), a model which makes use of
contextual metric learning techniques that directly addresses this problem.
Points2Polygons performs well against existing fully-supervised segmentation
baselines with limited training data, despite using lightweight segmentation
models (U-Net with a ResNet18 backbone) and having access to only weak labels
in the form of object centroids and no pre-training. We demonstrate this on
several different small but non-trivial datasets. We show that metric learning
using contextual data provides key insights for self-supervised tasks in
general, and allow segmentation models to easily generalize across
traditionally label-intensive domains in computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kuai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_H/0/1/0/all/0/1"&gt;Hakeem Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchGen: Generating Constrained CAD Sketches. (arXiv:2106.02711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02711</id>
        <link href="http://arxiv.org/abs/2106.02711"/>
        <updated>2021-06-08T02:20:28.076Z</updated>
        <summary type="html"><![CDATA[Computer-aided design (CAD) is the most widely used modeling approach for
technical design. The typical starting point in these designs is 2D sketches
which can later be extruded and combined to obtain complex three-dimensional
assemblies. Such sketches are typically composed of parametric primitives, such
as points, lines, and circular arcs, augmented with geometric constraints
linking the primitives, such as coincidence, parallelism, or orthogonality.
Sketches can be represented as graphs, with the primitives as nodes and the
constraints as edges. Training a model to automatically generate CAD sketches
can enable several novel workflows, but is challenging due to the complexity of
the graphs and the heterogeneity of the primitives and constraints. In
particular, each type of primitive and constraint may require a record of
different size and parameter types. We propose SketchGen as a generative model
based on a transformer architecture to address the heterogeneity problem by
carefully designing a sequential language for the primitives and constraints
that allows distinguishing between different primitive or constraint types and
their parameters, while encouraging our model to re-use information across
related parameters, encoding shared structure. A particular highlight of our
work is the ability to produce primitives linked via constraints that enables
the final output to be further regularized via a constraint solver. We evaluate
our model by demonstrating constraint prediction for given sets of primitives
and full sketch generation from scratch, showing that our approach
significantly out performs the state-of-the-art in CAD sketch generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1"&gt;Wamiq Reyaz Para&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1"&gt;Tom Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Fewer Tasks through Task Interpolation. (arXiv:2106.02695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02695</id>
        <link href="http://arxiv.org/abs/2106.02695"/>
        <updated>2021-06-08T02:20:28.057Z</updated>
        <summary type="html"><![CDATA[Meta-learning enables algorithms to quickly learn a newly encountered task
with just a few labeled examples by transferring previously learned knowledge.
However, the bottleneck of current meta-learning algorithms is the requirement
of a large number of meta-training tasks, which may not be accessible in
real-world scenarios. To address the challenge that available tasks may not
densely sample the space of tasks, we propose to augment the task set through
interpolation. By meta-learning with task interpolation (MLTI), our approach
effectively generates additional tasks by randomly sampling a pair of tasks and
interpolating the corresponding features and labels. Under both gradient-based
and metric-based meta-learning settings, our theoretical analysis shows MLTI
corresponds to a data-adaptive meta-regularization and further improves the
generalization. Empirically, in our experiments on eight datasets from diverse
domains including image recognition, pose prediction, molecule property
prediction, and medical image classification, we find that the proposed general
MLTI framework is compatible with representative meta-learning algorithms and
consistently outperforms other state-of-the-art strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Classification of Very Large Images with Tiny Objects. (arXiv:2106.02694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02694</id>
        <link href="http://arxiv.org/abs/2106.02694"/>
        <updated>2021-06-08T02:20:28.051Z</updated>
        <summary type="html"><![CDATA[An increasing number of applications in the computer vision domain,
specially, in medical imaging and remote sensing, are challenging when the goal
is to classify very large images with tiny objects. More specifically, these
type of classification tasks face two key challenges: $i$) the size of the
input image in the target dataset is usually in the order of megapixels,
however, existing deep architectures do not easily operate on such big images
due to memory constraints, consequently, we seek a memory-efficient method to
process these images; and $ii$) only a small fraction of the input images are
informative of the label of interest, resulting in low region of interest (ROI)
to image ratio. However, most of the current convolutional neural networks
(CNNs) are designed for image classification datasets that have relatively
large ROIs and small image size (sub-megapixel). Existing approaches have
addressed these two challenges in isolation. We present an end-to-end CNN model
termed Zoom-In network that leverages hierarchical attention sampling for
classification of large images with tiny objects using a single GPU. We
evaluate our method on two large-image datasets and one gigapixel dataset.
Experimental results show that our model achieves higher accuracy than existing
methods while requiring less computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fanjie Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer. (arXiv:2011.12454v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12454</id>
        <link href="http://arxiv.org/abs/2011.12454"/>
        <updated>2021-06-08T02:20:28.044Z</updated>
        <summary type="html"><![CDATA[Dealing with severe class imbalance poses a major challenge for real-world
applications, especially when the accurate classification and generalization of
minority classes is of primary interest. In computer vision, learning from long
tailed datasets is a recurring theme, especially for natural image datasets.
While existing solutions mostly appeal to sampling or weighting adjustments to
alleviate the pathological imbalance, or imposing inductive bias to prioritize
non-spurious associations, we take novel perspectives to promote sample
efficiency and model generalization based on the invariance principles of
causality. Our proposal posits a meta-distributional scenario, where the data
generating mechanism is invariant across the label-conditional feature
distributions. Such causal assumption enables efficient knowledge transfer from
the dominant classes to their under-represented counterparts, even if the
respective feature distributions show apparent disparities. This allows us to
leverage a causal data inflation procedure to enlarge the representation of
minority classes. Our development is orthogonal to the existing extreme
classification techniques thus can be seamlessly integrated. The utility of our
proposal is validated with an extensive set of synthetic and real-world
computer vision tasks against SOTA solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_Z/0/1/0/all/0/1"&gt;Zidi Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-08T02:20:28.038Z</updated>
        <summary type="html"><![CDATA[Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time
retinal images with high resolution down to 2 $\mu m$. This technique enables
detection of the morphologies of individual microaneurysms (MAs), which are one
of the earliest signs of diabetic retinopathy (DR), a frequent complication of
diabetes that can lead to visual impairment and blindness. In contrast to
previous automatic models developed for MA detection on standard fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policy,
including preprocessing, data augmentation and transfer learning, to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and
segmentation, leading to correct MA morphological classification, while
outperforming the state-of-the-art both in accuracy and cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Machine Learning Classification Using Quantum Annealing for Real-world Applications. (arXiv:2106.02964v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02964</id>
        <link href="http://arxiv.org/abs/2106.02964"/>
        <updated>2021-06-08T02:20:28.031Z</updated>
        <summary type="html"><![CDATA[Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Nath_R/0/1/0/all/0/1"&gt;Rajdeep Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Thapliyal_H/0/1/0/all/0/1"&gt;Himanshu Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Humble_T/0/1/0/all/0/1"&gt;Travis S. Humble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning. (arXiv:2007.11622v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11622</id>
        <link href="http://arxiv.org/abs/2007.11622"/>
        <updated>2021-06-08T02:20:28.025Z</updated>
        <summary type="html"><![CDATA[On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 34.1%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Han Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Ligeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. (arXiv:2104.09435v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09435</id>
        <link href="http://arxiv.org/abs/2104.09435"/>
        <updated>2021-06-08T02:20:28.005Z</updated>
        <summary type="html"><![CDATA[Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_M/0/1/0/all/0/1"&gt;Myeongsu Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Bumju Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Soohyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Hean Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sunghoe Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Learning of Hate Speech Classification on Social Media. (arXiv:2106.02821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02821</id>
        <link href="http://arxiv.org/abs/2106.02821"/>
        <updated>2021-06-08T02:20:27.999Z</updated>
        <summary type="html"><![CDATA[Existing work on automated hate speech classification assumes that the
dataset is fixed and the classes are pre-defined. However, the amount of data
in social media increases every day, and the hot topics changes rapidly,
requiring the classifiers to be able to continuously adapt to new data without
forgetting the previously learned knowledge. This ability, referred to as
lifelong learning, is crucial for the real-word application of hate speech
classifiers in social media. In this work, we propose lifelong learning of hate
speech classification on social media. To alleviate catastrophic forgetting, we
propose to use Variational Representation Learning (VRL) along with a memory
module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural
Network). Experimentally, we show that combining variational representation
learning and the LB-SOINN memory module achieves better performance than the
commonly-used lifelong learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1"&gt;Mai ElSherief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View Fundamental Matrices. (arXiv:1812.00426v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00426</id>
        <link href="http://arxiv.org/abs/1812.00426"/>
        <updated>2021-06-08T02:20:27.993Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of recovering projective camera matrices
from collections of fundamental matrices in multiview settings. We make two
main contributions. First, given ${n \choose 2}$ fundamental matrices computed
for $n$ images, we provide a complete algebraic characterization in the form of
conditions that are both necessary and sufficient to enabling the recovery of
camera matrices. These conditions are based on arranging the fundamental
matrices as blocks in a single matrix, called the $n$-view fundamental matrix,
and characterizing this matrix in terms of the signs of its eigenvalues and
rank structures. Secondly, we propose a concrete algorithm for projective
structure-from-motion that utilizes this characterization. Given a complete or
partial collection of measured fundamental matrices, our method seeks camera
matrices that minimize a global algebraic error for the measured fundamental
matrices. In contrast to existing methods, our optimization, without any
initialization, produces a consistent set of fundamental matrices that
corresponds to a unique set of cameras (up to a choice of projective frame).
Our experiments indicate that our method achieves state of the art performance
in both accuracy and running time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1"&gt;Yoni Kasten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geifman_A/0/1/0/all/0/1"&gt;Amnon Geifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1"&gt;Meirav Galun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Contrastive Learning: Removing Undesirable Information in Self-Supervised Representations. (arXiv:2106.02866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02866</id>
        <link href="http://arxiv.org/abs/2106.02866"/>
        <updated>2021-06-08T02:20:27.986Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning is a form of unsupervised learning that leverages
rich information in data to learn representations. However, data sometimes
contains certain information that may be undesirable for downstream tasks. For
instance, gender information may lead to biased decisions on many
gender-irrelevant tasks. In this paper, we develop conditional contrastive
learning to remove undesirable information in self-supervised representations.
To remove the effect of the undesirable variable, our proposed approach
conditions on the undesirable variable (i.e., by fixing the variations of it)
during the contrastive learning process. In particular, inspired by the
contrastive objective InfoNCE, we introduce Conditional InfoNCE (C-InfoNCE),
and its computationally efficient variant, Weak-Conditional InfoNCE
(WeaC-InfoNCE), for conditional contrastive learning. We demonstrate
empirically that our methods can successfully learn self-supervised
representations for downstream tasks while removing a great level of
information related to the undesirable variables. We study three scenarios,
each with a different type of undesirable variables: task-irrelevant
meta-information for self-supervised speech representation learning, sensitive
attributes for fair representation learning, and domain specification for
multi-domain visual representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Martin Q. Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPS300+: a Challenging Multimodal Dataset for Intersection Perception System. (arXiv:2106.02781v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02781</id>
        <link href="http://arxiv.org/abs/2106.02781"/>
        <updated>2021-06-08T02:20:27.980Z</updated>
        <summary type="html"><![CDATA[Due to the high complexity and occlusion, insufficient perception in the
crowded urban intersection can be a serious safety risk for both human drivers
and autonomous algorithms, whereas CVIS (Cooperative Vehicle Infrastructure
System) is a proposed solution for full-participants perception in this
scenario. However, the research on roadside multimodal perception is still in
its infancy, and there is no open-source dataset for such scenario.
Accordingly, this paper fills the gap. Through an IPS (Intersection Perception
System) installed at the diagonal of the intersection, this paper proposes a
high-quality multimodal dataset for the intersection perception task. The
center of the experimental intersection covers an area of 3000m2, and the
extended distance reaches 300m, which is typical for CVIS. The first batch of
open-source data includes 14198 frames, and each frame has an average of 319.84
labels, which is 9.6 times larger than the most crowded dataset (H3D dataset in
2019) by now. In order to facilitate further study, this dataset tries to keep
the label documents consistent with the KITTI dataset, and a standardized
benchmark is created for algorithm evaluation. Our dataset is available at:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huanan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shuyue Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yongqiang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks. (arXiv:2106.02817v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02817</id>
        <link href="http://arxiv.org/abs/2106.02817"/>
        <updated>2021-06-08T02:20:27.959Z</updated>
        <summary type="html"><![CDATA[Imbalanced classification on graphs is ubiquitous yet challenging in many
real-world applications, such as fraudulent node detection. Recently, graph
neural networks (GNNs) have shown promising performance on many network
analysis tasks. However, most existing GNNs have almost exclusively focused on
the balanced networks, and would get unappealing performance on the imbalanced
networks. To bridge this gap, in this paper, we present a generative
adversarial graph network model, called ImGAGN to address the imbalanced
classification problem on graphs. It introduces a novel generator for graph
structure data, named GraphGenerator, which can simulate both the minority
class nodes' attribute distribution and network topological structure
distribution by generating a set of synthetic minority nodes such that the
number of nodes in different classes can be balanced. Then a graph
convolutional network (GCN) discriminator is trained to discriminate between
real nodes and fake (i.e., generated) nodes, and also between minority nodes
and majority nodes on the synthetic balanced network. To validate the
effectiveness of the proposed method, extensive experiments are conducted on
four real-world imbalanced network datasets. Experimental results demonstrate
that the proposed method ImGAGN outperforms state-of-the-art algorithms for
semi-supervised imbalanced node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liang Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huaisheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Ruiqi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuhui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes with Applications to Anomaly Detection. (arXiv:2002.11169v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11169</id>
        <link href="http://arxiv.org/abs/2002.11169"/>
        <updated>2021-06-08T02:20:27.950Z</updated>
        <summary type="html"><![CDATA[Our work focuses on unsupervised and generative methods that address the
following goals: (a) learning unsupervised generative representations that
discover latent factors controlling image semantic attributes, (b) studying how
this ability to control attributes formally relates to the issue of latent
factor disentanglement, clarifying related but dissimilar concepts that had
been confounded in the past, and (c) developing anomaly detection methods that
leverage representations learned in (a). For (a), we propose a network
architecture that exploits the combination of multiscale generative models with
mutual information (MI) maximization. For (b), we derive an analytical result
(Lemma 1) that brings clarity to two related but distinct concepts: the ability
of generative networks to control semantic attributes of images they generate,
resulting from MI maximization, and the ability to disentangle latent space
representations, obtained via total correlation minimization. More
specifically, we demonstrate that maximizing semantic attribute control
encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in
our loss function, we then show empirically that, for image generation tasks,
the proposed approach exhibits superior performance as measured in the quality
and disentanglement trade space, when compared to other state of the art
methods, with quality assessed via the Frechet Inception Distance (FID), and
disentanglement via mutual information gap. For (c), we design several systems
for anomaly detection exploiting representations learned in (a), and
demonstrate their performance benefits when compared to state-of-the-art
generative and discriminative algorithms. The above contributions in
representation learning have potential applications in addressing other
important problems in computer vision, such as bias and privacy in AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_I/0/1/0/all/0/1"&gt;I-Jeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Philippe Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual communication of object concepts at different levels of abstraction. (arXiv:2106.02775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02775</id>
        <link href="http://arxiv.org/abs/2106.02775"/>
        <updated>2021-06-08T02:20:27.943Z</updated>
        <summary type="html"><![CDATA[People can produce drawings of specific entities (e.g., Garfield), as well as
general categories (e.g., "cat"). What explains this ability to produce such
varied drawings of even highly familiar object concepts? We hypothesized that
drawing objects at different levels of abstraction depends on both sensory
information and representational goals, such that drawings intended to portray
a recently seen object preserve more detail than those intended to represent a
category. Participants drew objects cued either with a photo or a category
label. For each cue type, half the participants aimed to draw a specific
exemplar; the other half aimed to draw the category. We found that label-cued
category drawings were the most recognizable at the basic level, whereas
photo-cued exemplar drawings were the least recognizable. Together, these
findings highlight the importance of task context for explaining how people use
drawings to communicate visual concepts in different ways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Justin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Judith E. Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Replay-based Continual Zero-Shot Learning. (arXiv:2101.08894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08894</id>
        <link href="http://arxiv.org/abs/2101.08894"/>
        <updated>2021-06-08T02:20:27.934Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning is a new paradigm to classify objects from classes that
are not available at training time. Zero-shot learning (ZSL) methods have
attracted considerable attention in recent years because of their ability to
classify unseen/novel class examples. Most of the existing approaches on ZSL
works when all the samples from seen classes are available to train the model,
which does not suit real life. In this paper, we tackle this hindrance by
developing a generative replay-based continual ZSL (GRCZSL). The proposed
method endows traditional ZSL to learn from streaming data and acquire new
knowledge without forgetting the previous tasks' gained experience. We handle
catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen
classes, which have appeared in the earlier tasks. These synthetic samples are
synthesized using the trained conditional variational autoencoder (VAE) over
the immediate past task. Moreover, we only require the current and immediate
previous VAE at any time for training and testing. The proposed GRZSL method is
developed for a single-head setting of continual learning, simulating a
real-world problem setting. In this setting, task identity is given during
training but unavailable during testing. GRCZSL performance is evaluated on
five benchmark datasets for the generalized setup of ZSL with fixed and dynamic
(incremental class) settings of continual learning. The existing class setting
presented recently in the literature is not suitable for a class-incremental
setting. Therefore, this paper proposes a new setting to address this issue.
Experimental results show that the proposed method significantly outperforms
the baseline and the state-of-the-art method and makes it more suitable for
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_C/0/1/0/all/0/1"&gt;Chandan Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1"&gt;Sethupathy Parameswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Ashish Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:27.927Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02914</id>
        <link href="http://arxiv.org/abs/2106.02914"/>
        <updated>2021-06-08T02:20:27.909Z</updated>
        <summary type="html"><![CDATA[Pruning is a model compression method that removes redundant parameters in
deep neural networks (DNNs) while maintaining accuracy. Most available filter
pruning methods require complex treatments such as iterative pruning, features
statistics/ranking, or additional optimization designs in the training process.
In this paper, we propose a simple and effective regularization strategy from a
new perspective of evolution of features, which we call feature flow
regularization (FFR), for improving structured sparsity and filter pruning in
DNNs. Specifically, FFR imposes controls on the gradient and curvature of
feature flow along the neural network, which implicitly increases the sparsity
of the parameters. The principle behind FFR is that coherent and smooth
evolution of features will lead to an efficient network that avoids redundant
parameters. The high structured sparsity obtained from FFR enables us to prune
filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and
Tiny ImageNet datasets demonstrate that FFR can significantly improve both
unstructured and structured sparsity. Our pruning results in terms of reduction
of parameters and FLOPs are comparable to or even better than those of
state-of-the-art pruning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luchan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yang Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions. (arXiv:2105.12883v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12883</id>
        <link href="http://arxiv.org/abs/2105.12883"/>
        <updated>2021-06-08T02:20:27.903Z</updated>
        <summary type="html"><![CDATA[We present a method for localizing a single camera with respect to a point
cloud map in indoor and outdoor scenes. The problem is challenging because
correspondences of local invariant features are inconsistent across the domains
between image and 3D. The problem is even more challenging as the method must
handle various environmental conditions such as illumination, weather, and
seasonal changes. Our method can match equirectangular images to the 3D range
projections by extracting cross-domain symmetric place descriptors. Our key
insight is to retain condition-invariant 3D geometry features from limited data
samples while eliminating the condition-related features by a designed
Generative Adversarial Network. Based on such features, we further design a
spherical convolution network to learn viewpoint-invariant symmetric place
descriptors. We evaluate our method on extensive self-collected datasets, which
involve \textit{Long-term} (variant appearance conditions),
\textit{Large-scale} (up to $2km$ structure/unstructured environment), and
\textit{Multistory} (four-floor confined space). Our method surpasses other
current state-of-the-arts by achieving around $3$ times higher place retrievals
to inconsistent environments, and above $3$ times accuracy on online
localization. To highlight our method's generalization capabilities, we also
evaluate the recognition across different datasets. With a single trained
model, i3dLoc can demonstrate reliable visual localization in random
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Generalized Additive 2 Model with Consideration of High-Order Interactions. (arXiv:2106.02836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02836</id>
        <link href="http://arxiv.org/abs/2106.02836"/>
        <updated>2021-06-08T02:20:27.896Z</updated>
        <summary type="html"><![CDATA[In recent years, machine learning and AI have been introduced in many
industrial fields. In fields such as finance, medicine, and autonomous driving,
where the inference results of a model may have serious consequences, high
interpretability as well as prediction accuracy is required. In this study, we
propose CGA2M+, which is based on the Generalized Additive 2 Model (GA2M) and
differs from it in two major ways. The first is the introduction of
monotonicity. Imposing monotonicity on some functions based on an analyst's
knowledge is expected to improve not only interpretability but also
generalization performance. The second is the introduction of a higher-order
term: given that GA2M considers only second-order interactions, we aim to
balance interpretability and prediction accuracy by introducing a higher-order
term that can capture higher-order interactions. In this way, we can improve
prediction performance without compromising interpretability by applying
learning innovation. Numerical experiments showed that the proposed model has
high predictive performance and interpretability. Furthermore, we confirmed
that generalization performance is improved by introducing monotonicity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_A/0/1/0/all/0/1"&gt;Akihisa Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuramata_M/0/1/0/all/0/1"&gt;Michiya Kuramata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majima_K/0/1/0/all/0/1"&gt;Kaito Majima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1"&gt;Haruka Kiyohara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondo_K/0/1/0/all/0/1"&gt;Kensho Kondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kazuhide Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft-Attention Improves Skin Cancer Classification Performance. (arXiv:2105.03358v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03358</id>
        <link href="http://arxiv.org/abs/2105.03358"/>
        <updated>2021-06-08T02:20:27.890Z</updated>
        <summary type="html"><![CDATA[In clinical applications, neural networks must focus on and highlight the
most important parts of an input image. Soft-Attention mechanism enables a
neural network toachieve this goal. This paper investigates the effectiveness
of Soft-Attention in deep neural architectures. The central aim of
Soft-Attention is to boost the value of important features and suppress the
noise-inducing features. We compare the performance of VGG, ResNet,
InceptionResNetv2 and DenseNet architectures with and without the
Soft-Attention mechanism, while classifying skin lesions. The original network
when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while
achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,
Soft-Attention coupling improves the sensitivity score by 3.8% compared to
baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly
available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Datta_S/0/1/0/all/0/1"&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammad Abuzar Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1"&gt;Sargur N. Srihari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingchen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Sparse Deep Coordination Graphs. (arXiv:2106.02886v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02886</id>
        <link href="http://arxiv.org/abs/2106.02886"/>
        <updated>2021-06-08T02:20:27.884Z</updated>
        <summary type="html"><![CDATA[Learning sparse coordination graphs adaptive to the coordination dynamics
among agents is a long-standing problem in cooperative multi-agent learning.
This paper studies this problem by proposing several value-based and
observation-based schemes for learning dynamic topologies and evaluating them
on a new Multi-Agent COordination (MACO) benchmark. The benchmark collects
classic coordination problems in the literature, increases their difficulty,
and classifies them into different types. By analyzing the individual
advantages of each learning scheme on each type of problem and their overall
performance, we propose a novel method using the variance of utility difference
functions to learn context-aware sparse coordination topologies. Moreover, our
method learns action representations that effectively reduce the influence of
utility functions' estimation errors on graph construction. Experiments show
that our method significantly outperforms dense and static topologies across
the MACO and StarCraft II micromanagement benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1"&gt;Liang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weijun Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianlan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. (arXiv:2106.02968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02968</id>
        <link href="http://arxiv.org/abs/2106.02968"/>
        <updated>2021-06-08T02:20:27.867Z</updated>
        <summary type="html"><![CDATA[Given restrictions on the availability of data, active learning is the
process of training a model with limited labeled data by selecting a core
subset of an unlabeled data pool to label. Although selecting the most useful
points for training is an optimization problem, the scale of deep learning data
sets forces most selection strategies to employ efficient heuristics. Instead,
we propose a new integer optimization problem for selecting a core set that
minimizes the discrete Wasserstein distance from the unlabeled pool. We
demonstrate that this problem can be tractably solved with a Generalized
Benders Decomposition algorithm. Our strategy requires high-quality latent
features which we obtain by unsupervised learning on the unlabeled pool.
Numerical results on several data sets show that our optimization approach is
competitive with baselines and particularly outperforms them in the low budget
regime where less than one percent of the data set is labeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rafid Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PURS: Personalized Unexpected Recommender System for Improving User Satisfaction. (arXiv:2106.02771v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02771</id>
        <link href="http://arxiv.org/abs/2106.02771"/>
        <updated>2021-06-08T02:20:27.861Z</updated>
        <summary type="html"><![CDATA[Classical recommender system methods typically face the filter bubble problem
when users only receive recommendations of their familiar items, making them
bored and dissatisfied. To address the filter bubble problem, unexpected
recommendations have been proposed to recommend items significantly deviating
from user's prior expectations and thus surprising them by presenting "fresh"
and previously unexplored items to the users. In this paper, we describe a
novel Personalized Unexpected Recommender System (PURS) model that incorporates
unexpectedness into the recommendation process by providing multi-cluster
modeling of user interests in the latent space and personalized unexpectedness
via the self-attention mechanism and via selection of an appropriate unexpected
activation function. Extensive offline experiments on three real-world datasets
illustrate that the proposed PURS model significantly outperforms the
state-of-the-art baseline approaches in terms of both accuracy and
unexpectedness measures. In addition, we conduct an online A/B test at a major
video platform Alibaba-Youku, where our model achieves over 3\% increase in the
average video view per user metric. The proposed model is in the process of
being deployed by the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:27.854Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. (arXiv:2001.06471v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.06471</id>
        <link href="http://arxiv.org/abs/2001.06471"/>
        <updated>2021-06-08T02:20:27.845Z</updated>
        <summary type="html"><![CDATA[We consider a discrete optimization formulation for learning sparse
classifiers, where the outcome depends upon a linear combination of a small
subset of features. Recent work has shown that mixed integer programming (MIP)
can be used to solve (to optimality) $\ell_0$-regularized regression problems
at scales much larger than what was conventionally considered possible. Despite
their usefulness, MIP-based global optimization approaches are significantly
slower compared to the relatively mature algorithms for $\ell_1$-regularization
and heuristics for nonconvex regularized problems. We aim to bridge this gap in
computation times by developing new MIP-based algorithms for
$\ell_0$-regularized classification. We propose two classes of scalable
algorithms: an exact algorithm that can handle $p\approx 50,000$ features in a
few minutes, and approximate algorithms that can address instances with
$p\approx 10^6$ in times comparable to the fast $\ell_1$-based algorithms. Our
exact algorithm is based on the novel idea of \textsl{integrality generation},
which solves the original problem (with $p$ binary variables) via a sequence of
mixed integer programs that involve a small number of binary variables. Our
approximate algorithms are based on coordinate descent and local combinatorial
search. In addition, we present new estimation error bounds for a class of
$\ell_0$-regularized estimators. Experiments on real and synthetic data
demonstrate that our approach leads to models with considerably improved
statistical performance (especially, variable selection) when compared to
competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1"&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hazimeh_H/0/1/0/all/0/1"&gt;Hussein Hazimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1"&gt;Rahul Mazumder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Variational Bayesian Framework for Blind Image Deblurring. (arXiv:2106.02884v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02884</id>
        <link href="http://arxiv.org/abs/2106.02884"/>
        <updated>2021-06-08T02:20:27.839Z</updated>
        <summary type="html"><![CDATA[Blind image deblurring is an important yet very challenging problem in
low-level vision. Traditional optimization based methods generally formulate
this task as a maximum-a-posteriori estimation or variational inference
problem, whose performance highly relies on the handcraft priors for both the
latent image and the blur kernel. In contrast, recent deep learning methods
generally learn, from a large collection of training images, deep neural
networks (DNNs) directly mapping the blurry image to the clean one or to the
blur kernel, paying less attention to the physical degradation process of the
blurry image. In this paper, we present a deep variational Bayesian framework
for blind image deblurring. Under this framework, the posterior of the latent
clean image and blur kernel can be jointly estimated in an amortized inference
fashion with DNNs, and the involved inference DNNs can be trained by fully
considering the physical blur model, together with the supervision of data
driven priors for the clean image and blur kernel, which is naturally led to by
the evidence lower bound objective. Comprehensive experiments are conducted to
substantiate the effectiveness of the proposed framework. The results show that
it can not only achieve a promising performance with relatively simple
networks, but also enhance the performance of existing DNNs for deblurring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Failure Criterion for Composites using k-Nearest Neighbor Classification. (arXiv:2106.02714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02714</id>
        <link href="http://arxiv.org/abs/2106.02714"/>
        <updated>2021-06-08T02:20:27.824Z</updated>
        <summary type="html"><![CDATA[Numerous theories of failure have been postulated and implemented in various
commercial programs for composite materials. Even the best theories have had
limited success in predicting damage and failure in validation exercises. In
view of this background, many researchers have started exploring the use of
multiscale modeling to improve the fidelity of the modeling and simulation of
various structural and materials systems. In this paper, a multi-scale modeling
scheme is used to illustrate how a combination of virtual and laboratory
testing programs can be used to generate a point cloud of failure surface data
that can then be queried during finite element analysis at the continuum scale
to ascertain if the onset of failure has occurred. The k-nearest neighbor
(k-NN) classification concept is used to obtain the answer to the query. A
linear, elastic, static finite element example using a unidirectional composite
shows that the framework can be generated and used effectively and efficiently
with the possibility to extend the approach for all types of composite
architectures and behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1"&gt;Subramaniam Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khaled_B/0/1/0/all/0/1"&gt;Bilal Khaled&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shyamsunder_L/0/1/0/all/0/1"&gt;Loukham Shyamsunder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should Graph Convolution Trust Neighbors? A Simple Causal Inference Method. (arXiv:2010.11797v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11797</id>
        <link href="http://arxiv.org/abs/2010.11797"/>
        <updated>2021-06-08T02:20:26.099Z</updated>
        <summary type="html"><![CDATA[Graph Convolutional Network (GCN) is an emerging technique for information
retrieval (IR) applications. While GCN assumes the homophily property of a
graph, real-world graphs are never perfect: the local structure of a node may
contain discrepancy, e.g., the labels of a node's neighbors could vary. This
pushes us to consider the discrepancy of local structure in GCN modeling.
Existing work approaches this issue by introducing an additional module such as
graph attention, which is expected to learn the contribution of each neighbor.
However, such module may not work reliably as expected, especially when there
lacks supervision signal, e.g., when the labeled data is small. Moreover,
existing methods focus on modeling the nodes in the training data, and never
consider the local structure discrepancy of testing nodes.

This work focuses on the local structure discrepancy issue for testing nodes,
which has received little scrutiny. From a novel perspective of causality, we
investigate whether a GCN should trust the local structure of a testing node
when predicting its label. To this end, we analyze the working mechanism of GCN
with causal graph, estimating the causal effect of a node's local structure for
the prediction. The idea is simple yet effective: given a trained GCN model, we
first intervene the prediction by blocking the graph structure; we then compare
the original prediction with the intervened prediction to assess the causal
effect of the local structure on the prediction. Through this way, we can
eliminate the impact of local structure discrepancy and make more accurate
prediction. Extensive experiments on seven node classification datasets show
that our method effectively enhances the inference stage of GCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weiran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation Learning via Simultaneous Optimization of Policies and Auxiliary Trajectories. (arXiv:2105.03019v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03019</id>
        <link href="http://arxiv.org/abs/2105.03019"/>
        <updated>2021-06-08T02:20:26.066Z</updated>
        <summary type="html"><![CDATA[Imitation learning (IL) is a frequently used approach for data-efficient
policy learning. Many IL methods, such as Dataset Aggregation (DAgger), combat
challenges like distributional shift by interacting with oracular experts.
Unfortunately, assuming access to oracular experts is often unrealistic in
practice; data used in IL frequently comes from offline processes such as
lead-through or teleoperation. In this paper, we present a novel imitation
learning technique called Collocation for Demonstration Encoding (CoDE) that
operates on only a fixed set of trajectory demonstrations. We circumvent
challenges with methods like back-propagation-through-time by introducing an
auxiliary trajectory network, which takes inspiration from collocation
techniques in optimal control. Our method generalizes well and more accurately
reproduces the demonstrated behavior with fewer guiding trajectories when
compared to standard behavioral cloning methods. We present simulation results
on a 7-degree-of-freedom (DoF) robotic manipulator that learns to exhibit
lifting, target-reaching, and obstacle avoidance behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1"&gt;Mandy Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Anqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1"&gt;Karl Van Wyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dellaert_F/0/1/0/all/0/1"&gt;Frank Dellaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratliff_N/0/1/0/all/0/1"&gt;Nathan Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Kickstarting Deep Reinforcement Learning with Privacy-Aware Learners. (arXiv:2102.09599v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09599</id>
        <link href="http://arxiv.org/abs/2102.09599"/>
        <updated>2021-06-08T02:20:26.023Z</updated>
        <summary type="html"><![CDATA[Kickstarting deep reinforcement learning algorithms facilitate a
teacher-student relationship among the agents and allow for a well-performing
teacher to share demonstrations with a student to expedite the student's
training. However, despite the known benefits, the demonstrations may contain
sensitive information about the teacher's training data and existing
kickstarting methods do not take any measures to protect it. Therefore, we use
the framework of differential privacy to develop a mechanism that securely
shares the teacher's demonstrations with the student. The mechanism allows for
the teacher to decide upon the accuracy of its demonstrations with respect to
the privacy budget that it consumes, thereby granting the teacher full control
over its data privacy. We then develop a kickstarted deep reinforcement
learning algorithm for the student that is privacy-aware because we calibrate
its objective with the parameters of the teacher's privacy mechanism. The
privacy-aware design of the algorithm makes it possible to kickstart the
student's learning despite the perturbations induced by the privacy mechanism.
From numerical experiments, we highlight three empirical results: (i) the
algorithm succeeds in expediting the student's learning, (ii) the student
converges to a performance level that was not possible without the
demonstrations, and (iii) the student maintains its enhanced performance even
after the teacher stops sharing useful demonstrations due to its privacy budget
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gohari_P/0/1/0/all/0/1"&gt;Parham Gohari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_M/0/1/0/all/0/1"&gt;Matthew Hale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification. (arXiv:2009.07536v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07536</id>
        <link href="http://arxiv.org/abs/2009.07536"/>
        <updated>2021-06-08T02:20:26.017Z</updated>
        <summary type="html"><![CDATA[Extracting effective and discriminative features is very important for
addressing the challenging person re-identification (re-ID) task. Prevailing
deep convolutional neural networks (CNNs) usually use high-level features for
identifying pedestrian. However, some essential spatial information resided in
low-level features such as shape, texture and color will be lost when learning
the high-level features, due to extensive padding and pooling operations in the
training stage. In addition, most existing person re-ID methods are mainly
based on hand-craft bounding boxes where images are precisely aligned. It is
unrealistic in practical applications, since the exploited object detection
algorithms often produce inaccurate bounding boxes. This will inevitably
degrade the performance of existing algorithms. To address these problems, we
put forward a novel person re-ID model that fuses high- and low-level
embeddings to reduce the information loss caused in learning high-level
features. Then we divide the fused embedding into several parts and reconnect
them to obtain the global feature and more significant local features, so as to
alleviate the affect caused by the inaccurate bounding boxes. In addition, we
also introduce the spatial and channel attention mechanisms in our model, which
aims to mine more discriminative features related to the target. Finally, we
reconstruct the feature extractor to ensure that our model can obtain more
richer and robust features. Extensive experiments display the superiority of
our approach compared with existing approaches. Our code is available at
https://github.com/libraflower/MutipleFeature-for-PRID.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junchuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengyong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03736</id>
        <link href="http://arxiv.org/abs/2104.03736"/>
        <updated>2021-06-08T02:20:25.998Z</updated>
        <summary type="html"><![CDATA[Meta-learning can extract an inductive bias from previous learning experience
and assist the training processes of new tasks. It is often realized through
optimizing a meta-model with the evaluation loss of a series of task-specific
solvers. Most existing algorithms sample non-overlapping $\mathit{support}$
sets and $\mathit{query}$ sets to train and evaluate the solvers respectively
due to simplicity ($\mathcal{S}/\mathcal{Q}$ protocol). However, another
evaluation method that assesses the discrepancy between the solver and a target
model is short of research ($\mathcal{S}/\mathcal{T}$ protocol).
$\mathcal{S}/\mathcal{T}$ protocol has unique advantages such as offering more
informative supervision, but it is computationally expensive. This paper looks
into this special evaluation method and takes a step towards putting it into
practice. We find that with a small ratio of tasks armed with target models,
classic meta-learning algorithms can be improved a lot without consuming many
resources. Furthermore, we empirically verify the effectiveness of
$\mathcal{S}/\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers to target models via knowledge distillation. Experiments
demonstrate the superiority of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Su Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Minimization Experience Replay. (arXiv:2105.07253v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07253</id>
        <link href="http://arxiv.org/abs/2105.07253"/>
        <updated>2021-06-08T02:20:25.992Z</updated>
        <summary type="html"><![CDATA[In reinforcement learning, experience replay stores past samples for further
reuse. Prioritized sampling is a promising technique to better utilize these
samples. Previous criteria of prioritization include TD error, recentness and
corrective feedback, which are mostly heuristically designed. In this work, we
start from the regret minimization objective, and obtain an optimal
prioritization strategy for Bellman update that can directly maximize the
return of the policy. The theory suggests that data with higher hindsight TD
error, better on-policiness and more accurate Q value should be assigned with
higher weights during sampling. Thus most previous criteria only consider this
strategy partially. We not only provide theoretical justifications for previous
criteria, but also propose two new methods to compute the prioritization
weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT
exploits the temporal ordering of states. Both methods outperform previous
prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo,
Atari and Meta-World.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhenghai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu-Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jing-Cheng Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengyi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Feng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Discrepancy in Strategic Learning. (arXiv:2103.01028v3 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01028</id>
        <link href="http://arxiv.org/abs/2103.01028"/>
        <updated>2021-06-08T02:20:25.986Z</updated>
        <summary type="html"><![CDATA[We study the effects of information discrepancy across sub-populations on
their ability to simultaneously improve their features in strategic learning
settings. Specifically, we consider a game where a principal deploys a decision
rule in an attempt to optimize the whole population's welfare, and agents
strategically adapt to it to receive better scores. Inspired by real-life
settings, such as loan approvals and college admissions, we remove the typical
assumption made in the strategic learning literature that the decision rule is
fully known to the agents, and focus on settings where it is inaccessible. In
their lack of knowledge, individuals try to infer this rule by learning from
their peers (e.g., friends and acquaintances who previously applied for a
loan), naturally forming groups in the population, each with possibly different
type and level of information about the decision rule. In our equilibrium
analysis, we show that the principal's decision rule optimizing the welfare
across subgroups may cause a surprising negative externality; the true quality
of some of the subgroups can actually deteriorate. On the positive side, we
show that in many natural cases, optimal improvement is guaranteed
simultaneously for all subgroups in equilibrium. We also characterize the
disparity in improvements across subgroups via a measure of their informational
overlap. Finally, we complement our theoretical analysis with experiments on
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bechavod_Y/0/1/0/all/0/1"&gt;Yahav Bechavod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podimata_C/0/1/0/all/0/1"&gt;Chara Podimata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziani_J/0/1/0/all/0/1"&gt;Juba Ziani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data. (arXiv:2010.13523v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13523</id>
        <link href="http://arxiv.org/abs/2010.13523"/>
        <updated>2021-06-08T02:20:25.979Z</updated>
        <summary type="html"><![CDATA[Directional data consist of observations distributed on a (hyper)sphere, and
appear in many applied fields, such as astronomy, ecology, and environmental
science. This paper studies both statistical and computational problems of
kernel smoothing for directional data. We generalize the classical mean shift
algorithm to directional data, which allows us to identify local modes of the
directional kernel density estimator (KDE). The statistical convergence rates
of the directional KDE and its derivatives are derived, and the problem of mode
estimation is examined. We also prove the ascending property of the directional
mean shift algorithm and investigate a general problem of gradient ascent on
the unit hypersphere. To demonstrate the applicability of the algorithm, we
evaluate it as a mode clustering method on both simulated and real-world data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Matters: Assessing the Importance of Subgroup Allocations in Training Data. (arXiv:2103.03399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03399</id>
        <link href="http://arxiv.org/abs/2103.03399"/>
        <updated>2021-06-08T02:20:25.973Z</updated>
        <summary type="html"><![CDATA[Collecting more diverse and representative training data is often touted as a
remedy for the disparate performance of machine learning predictors across
subpopulations. However, a precise framework for understanding how dataset
properties like diversity affect learning outcomes is largely lacking. By
casting data collection as part of the learning process, we demonstrate that
diverse representation in training data is key not only to increasing subgroup
performances, but also to achieving population level objectives. Our analysis
and experiments describe how dataset compositions influence performance and
provide constructive results for using trends in existing data, alongside
domain knowledge, to help guide intentional, objective-aware dataset design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1"&gt;Esther Rolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worledge_T/0/1/0/all/0/1"&gt;Theodora Worledge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1"&gt;Benjamin Recht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Simplicial Neural Networks for Trajectory Prediction. (arXiv:2102.10058v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10058</id>
        <link href="http://arxiv.org/abs/2102.10058"/>
        <updated>2021-06-08T02:20:25.965Z</updated>
        <summary type="html"><![CDATA[We consider the construction of neural network architectures for data on
simplicial complexes. In studying maps on the chain complex of a simplicial
complex, we define three desirable properties of a simplicial neural network
architecture: namely, permutation equivariance, orientation equivariance, and
simplicial awareness. The first two properties respectively account for the
fact that the node indexing and the simplex orientations in a simplicial
complex are arbitrary. The last property encodes the desirable feature that the
output of the neural network depends on the entire simplicial complex and not
on a subset of its dimensions. Based on these properties, we propose a simple
convolutional architecture, rooted in tools from algebraic topology, for the
problem of trajectory prediction, and show that it obeys all three of these
properties when an odd, nonlinear activation function is used. We then
demonstrate the effectiveness of this architecture in extrapolating
trajectories on synthetic and real datasets, with particular emphasis on the
gains in generalizability to unseen trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roddenberry_T/0/1/0/all/0/1"&gt;T. Mitchell Roddenberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaze_N/0/1/0/all/0/1"&gt;Nicholas Glaze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1"&gt;Santiago Segarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility. (arXiv:2006.08267v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08267</id>
        <link href="http://arxiv.org/abs/2006.08267"/>
        <updated>2021-06-08T02:20:25.948Z</updated>
        <summary type="html"><![CDATA[Bipartite ranking, which aims to learn a scoring function that ranks positive
individuals higher than negative ones from labeled data, is widely adopted in
various applications where sample prioritization is needed. Recently, there
have been rising concerns on whether the learned scoring function can cause
systematic disparity across different protected groups defined by sensitive
attributes. While there could be trade-off between fairness and performance, in
this paper we propose a model agnostic post-processing framework for balancing
them in the bipartite ranking scenario. Specifically, we maximize a weighted
sum of the utility and fairness by directly adjusting the relative ordering of
samples across groups. By formulating this problem as the identification of an
optimal warping path across different protected groups, we propose a
non-parametric method to search for such an optimal path through a dynamic
programming process. Our method is compatible with various classification
models and applicable to a variety of ranking fairness metrics. Comprehensive
experiments on a suite of benchmark data sets and two real-world patient
electronic health record repositories show that our method can achieve a great
balance between the algorithm utility and ranking fairness. Furthermore, we
experimentally verify the robustness of our method when faced with the fewer
training samples and the difference between training and testing ranking score
distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Sen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weishen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention. (arXiv:2008.01739v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01739</id>
        <link href="http://arxiv.org/abs/2008.01739"/>
        <updated>2021-06-08T02:20:25.941Z</updated>
        <summary type="html"><![CDATA[Natural language processing techniques have demonstrated promising results in
keyphrase generation. However, one of the major challenges in \emph{neural}
keyphrase generation is processing long documents using deep neural networks.
Generally, documents are truncated before given as inputs to neural networks.
Consequently, the models may miss essential points conveyed in the target
document. To overcome this limitation, we propose \emph{SEG-Net}, a neural
keyphrase generation model that is composed of two major components, (1) a
selector that selects the salient sentences in a document and (2) an
extractor-generator that jointly extracts and generates keyphrases from the
selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as
the basic building block with a novel \emph{layer-wise} coverage attention to
summarize most of the points discussed in the document. The experimental
results on seven keyphrase generation benchmarks from scientific and web
documents demonstrate that SEG-Net outperforms the state-of-the-art neural
generative methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Soomin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Efficient Representations for Keyword Spotting with Triplet Loss. (arXiv:2101.04792v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04792</id>
        <link href="http://arxiv.org/abs/2101.04792"/>
        <updated>2021-06-08T02:20:25.935Z</updated>
        <summary type="html"><![CDATA[In the past few years, triplet loss-based metric embeddings have become a
de-facto standard for several important computer vision problems, most
no-tably, person reidentification. On the other hand, in the area of speech
recognition the metric embeddings generated by the triplet loss are rarely used
even for classification problems. We fill this gap showing that a combination
of two representation learning techniques: a triplet loss-based embedding and a
variant of kNN for classification instead of cross-entropy loss significantly
(by 26% to 38%) improves the classification accuracy for convolutional networks
on a LibriSpeech-derived LibriWords datasets. To do so, we propose a novel
phonetic similarity based triplet mining approach. We also improve the current
best published SOTA for Google Speech Commands dataset V1 10+2 -class
classification by about 34%, achieving 98.55% accuracy, V2 10+2-class
classification by about 20%, achieving 98.37% accuracy, and V2 35-class
classification by over 50%, achieving 97.0% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vygon_R/0/1/0/all/0/1"&gt;Roman Vygon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1"&gt;Nikolay Mikhaylovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Reconstruction: Partially Local Federated Learning. (arXiv:2102.03448v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03448</id>
        <link href="http://arxiv.org/abs/2102.03448"/>
        <updated>2021-06-08T02:20:25.930Z</updated>
        <summary type="html"><![CDATA[Personalization methods in federated learning aim to balance the benefits of
federated and local training for data availability, communication cost, and
robustness to client heterogeneity. Approaches that require clients to
communicate all model parameters can be undesirable due to privacy and
communication constraints. Other approaches require always-available or
stateful clients, impractical in large-scale cross-device settings. We
introduce Federated Reconstruction, the first model-agnostic framework for
partially local federated learning suitable for training and inference at
scale. We motivate the framework via a connection to model-agnostic meta
learning, empirically demonstrate its performance over existing approaches for
collaborative filtering and next word prediction, and release an open-source
library for evaluating approaches in this setting. We also describe the
successful deployment of this approach at scale for federated collaborative
filtering in a mobile keyboard application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1"&gt;Karan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidahmed_H/0/1/0/all/0/1"&gt;Hakim Sidahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shanshan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1"&gt;Keith Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1"&gt;Sushant Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction. (arXiv:2104.03279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03279</id>
        <link href="http://arxiv.org/abs/2104.03279"/>
        <updated>2021-06-08T02:20:25.923Z</updated>
        <summary type="html"><![CDATA[Finding synthesis routes for molecules of interest is an essential step in
the discovery of new drugs and materials. To find such routes,
computer-assisted synthesis planning (CASP) methods are employed which rely on
a model of chemical reactivity. In this study, we model single-step
retrosynthesis in a template-based approach using modern Hopfield networks
(MHNs). We adapt MHNs to associate different modalities, reaction templates and
molecules, which allows the model to leverage structural information about
reaction templates. This approach significantly improves the performance of
template relevance prediction, especially for templates with few or zero
training examples. With inference speed several times faster than that of
baseline methods, we improve predictive performance for top-k exact match
accuracy for $\mathrm{k}\geq5$ in the retrosynthesis benchmark USPTO-50k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_P/0/1/0/all/0/1"&gt;Philipp Seidl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1"&gt;Philipp Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyubankova_N/0/1/0/all/0/1"&gt;Natalia Dyubankova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neves_P/0/1/0/all/0/1"&gt;Paulo Neves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1"&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg K. Wegner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization. (arXiv:2103.17182v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17182</id>
        <link href="http://arxiv.org/abs/2103.17182"/>
        <updated>2021-06-08T02:20:25.906Z</updated>
        <summary type="html"><![CDATA[It is well-known that stochastic gradient noise (SGN) acts as implicit
regularization for deep learning and is essentially important for both
optimization and generalization of deep networks. Some works attempted to
artificially simulate SGN by injecting random noise to improve deep learning.
However, it turned out that the injected simple random noise cannot work as
well as SGN, which is anisotropic and parameter-dependent. For simulating SGN
at low computational costs and without changing the learning rate or batch
size, we propose the Positive-Negative Momentum (PNM) approach that is a
powerful alternative to conventional Momentum in classic optimizers. The
introduced PNM method maintains two approximate independent momentum terms.
Then, we can control the magnitude of SGN explicitly by adjusting the momentum
difference. We theoretically prove the convergence guarantee and the
generalization advantage of PNM over Stochastic Gradient Descent (SGD). By
incorporating PNM into the two conventional optimizers, SGD with Momentum and
Adam, our extensive experiments empirically verified the significant advantage
of the PNM-based variants over the corresponding conventional Momentum-based
optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zeke Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeadCache: Regret-Optimal Caching in Networks. (arXiv:2009.08228v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08228</id>
        <link href="http://arxiv.org/abs/2009.08228"/>
        <updated>2021-06-08T02:20:25.900Z</updated>
        <summary type="html"><![CDATA[We consider a set-valued online prediction problem in the context of network
caching. Assume that users are connected to a number of caches via a bipartite
network. At any time slot, each user requests some file chosen from a large
catalog. A user's request is met if the requested file is cached in at least
one of the caches connected to the user. The objective is to predict and
optimally store the files on the caches to maximize the total number of cache
hits. We propose $\texttt{LeadCache}$ - an online caching policy based on the
Follow-the-Perturbed-Leader paradigm. We show that the policy is regret-optimal
up to a factor of $\tilde{O}(n^{3/8}),$ where $n$ is the number of users. We
implement the policy by designing a new linear-time Pipage rounding algorithm.
With an additional Strong-Law-type assumption, we show that the total number of
file fetches under $\texttt{LeadCache}$ remains almost surely finite.
Additionally, we derive a tight regret lower bound using results from graph
coloring. Our conclusion is that the proposed learning-based caching policy
decisively outperforms the classical policies both theoretically and
empirically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paria_D/0/1/0/all/0/1"&gt;Debjit Paria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Abhishek Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling a Deep Learned Volume Formula. (arXiv:2012.03955v2 [hep-th] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03955</id>
        <link href="http://arxiv.org/abs/2012.03955"/>
        <updated>2021-06-08T02:20:25.894Z</updated>
        <summary type="html"><![CDATA[We present a simple phenomenological formula which approximates the
hyperbolic volume of a knot using only a single evaluation of its Jones
polynomial at a root of unity. The average error is just $2.86$% on the first
$1.7$ million knots, which represents a large improvement over previous
formulas of this kind. To find the approximation formula, we use layer-wise
relevance propagation to reverse engineer a black box neural network which
achieves a similar average error for the same approximation task when trained
on $10$% of the total dataset. The particular roots of unity which appear in
our analysis cannot be written as $e^{2\pi i / (k+2)}$ with integer $k$;
therefore, the relevant Jones polynomial evaluations are not given by
unknot-normalized expectation values of Wilson loop operators in conventional
$SU(2)$ Chern$\unicode{x2013}$Simons theory with level $k$. Instead, they
correspond to an analytic continuation of such expectation values to fractional
level. We briefly review the continuation procedure and comment on the presence
of certain Lefschetz thimbles, to which our approximation formula is sensitive,
in the analytically continued Chern$\unicode{x2013}$Simons integration cycle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Craven_J/0/1/0/all/0/1"&gt;Jessica Craven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Jejjala_V/0/1/0/all/0/1"&gt;Vishnu Jejjala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Kar_A/0/1/0/all/0/1"&gt;Arjun Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation. (arXiv:2106.02833v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02833</id>
        <link href="http://arxiv.org/abs/2106.02833"/>
        <updated>2021-06-08T02:20:25.888Z</updated>
        <summary type="html"><![CDATA[Multiple different responses are often plausible for a given open domain
dialog context. Prior work has shown the importance of having multiple valid
reference responses for meaningful and robust automated evaluations. In such
cases, common practice has been to collect more human written references.
However, such collection can be expensive, time consuming, and not easily
scalable. Instead, we propose a novel technique for automatically expanding a
human generated reference to a set of candidate references. We fetch plausible
references from knowledge sources, and adapt them so that they are more fluent
in context of the dialog instance in question. More specifically, we use (1) a
commonsense knowledge base to elicit a large number of plausible reactions
given the dialog history (2) relevant instances retrieved from dialog corpus,
using similar past as well as future contexts. We demonstrate that our
automatically expanded reference sets lead to large improvements in
correlations of automated metrics with human ratings of system outputs for
DailyDialog dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1"&gt;Harsh Jhamtani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework to Learn with Interpretation. (arXiv:2010.09345v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09345</id>
        <link href="http://arxiv.org/abs/2010.09345"/>
        <updated>2021-06-08T02:20:25.882Z</updated>
        <summary type="html"><![CDATA[To tackle interpretability in deep learning, we present a novel framework to
jointly learn a predictive model and its associated interpretation model. The
interpreter provides both local and global interpretability about the
predictive model in terms of human-understandable high level attribute
functions, with minimal loss of accuracy. This is achieved by a dedicated
architecture and well chosen regularization penalties. We seek for a small-size
dictionary of high level attribute functions that take as inputs the outputs of
selected hidden layers and whose outputs feed a linear classifier. We impose
strong conciseness on the activation of attributes with an entropy-based
criterion while enforcing fidelity to both inputs and outputs of the predictive
model. A detailed pipeline to visualize the learnt features is also developed.
Moreover, besides generating interpretable models by design, our approach can
be specialized to provide post-hoc interpretations for a pre-trained neural
network. We validate our approach against several state-of-the-art methods on
multiple datasets and show its efficacy on both kinds of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_J/0/1/0/all/0/1"&gt;Jayneel Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozharovskyi_P/0/1/0/all/0/1"&gt;Pavlo Mozharovskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+dAlche_Buc_F/0/1/0/all/0/1"&gt;Florence d&amp;#x27;Alch&amp;#xe9;-Buc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Composite Optimization. (arXiv:2011.08474v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08474</id>
        <link href="http://arxiv.org/abs/2011.08474"/>
        <updated>2021-06-08T02:20:25.875Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a distributed learning paradigm that scales
on-device learning collaboratively and privately. Standard FL algorithms such
as FedAvg are primarily geared towards smooth unconstrained settings. In this
paper, we study the Federated Composite Optimization (FCO) problem, in which
the loss function contains a non-smooth regularizer. Such problems arise
naturally in FL applications that involve sparsity, low-rank, monotonicity, or
more general constraints. We first show that straightforward extensions of
primal algorithms such as FedAvg are not well-suited for FCO since they suffer
from the "curse of primal averaging," resulting in poor convergence. As a
solution, we propose a new primal-dual algorithm, Federated Dual Averaging
(FedDualAvg), which by employing a novel server dual averaging procedure
circumvents the curse of primal averaging. Our theoretical analysis and
empirical experiments demonstrate that FedDualAvg outperforms the other
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank Reddi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of deep learning models for multi-step ahead time series prediction. (arXiv:2103.14250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14250</id>
        <link href="http://arxiv.org/abs/2103.14250"/>
        <updated>2021-06-08T02:20:25.858Z</updated>
        <summary type="html"><![CDATA[Time series prediction with neural networks has been the focus of much
research in the past few decades. Given the recent deep learning revolution,
there has been much attention in using deep learning models for time series
prediction, and hence it is important to evaluate their strengths and
weaknesses. In this paper, we present an evaluation study that compares the
performance of deep learning models for multi-step ahead time series
prediction. The deep learning methods comprise simple recurrent neural
networks, long short-term memory (LSTM) networks, bidirectional LSTM networks,
encoder-decoder LSTM networks, and convolutional neural networks. We provide a
further comparison with simple neural networks that use stochastic gradient
descent and adaptive moment estimation (Adam) for training. We focus on
univariate time series for multi-step-ahead prediction from benchmark
time-series datasets and provide a further comparison of the results with
related methods from the literature. The results show that the bidirectional
and encoder-decoder LSTM network provides the best performance in accuracy for
the given time series problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohitash Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1"&gt;Shaurya Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rishabh Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02400</id>
        <link href="http://arxiv.org/abs/2102.02400"/>
        <updated>2021-06-08T02:20:25.852Z</updated>
        <summary type="html"><![CDATA[In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously optimize two objectives: the
cross entropy loss between the prediction by the neural network and the given
noisy label, and the volume of the simplex formed by the columns of the
transition matrix. Our proposed framework can identify the transition matrix if
the clean class-posterior probabilities are sufficiently scattered. This is by
far the mildest assumption under which the transition matrix is provably
identifiable and the learned classifier is statistically consistent.
Experimental results on benchmark datasets demonstrate the effectiveness and
robustness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation. (arXiv:2106.02960v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02960</id>
        <link href="http://arxiv.org/abs/2106.02960"/>
        <updated>2021-06-08T02:20:25.844Z</updated>
        <summary type="html"><![CDATA[A critical challenge faced by supervised word sense disambiguation (WSD) is
the lack of large annotated datasets with sufficient coverage of words in their
diversity of senses. This inspired recent research on few-shot WSD using
meta-learning. While such work has successfully applied meta-learning to learn
new word senses from very few examples, its performance still lags behind its
fully supervised counterpart. Aiming to further close this gap, we propose a
model of semantic memory for WSD in a meta-learning setting. Semantic memory
encapsulates prior experiences seen throughout the lifetime of the model, which
aids better generalization in limited data settings. Our model is based on
hierarchical variational inference and incorporates an adaptive memory update
rule via a hypernetwork. We show our model advances the state of the art in
few-shot WSD, supports effective learning in extremely data scarce (e.g.
one-shot) scenarios and produces meaning prototypes that capture similar senses
of distinct words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yingjun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1"&gt;Nithin Holla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G.M. Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1"&gt;Ekaterina Shutova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and Algorithms. (arXiv:2012.00889v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00889</id>
        <link href="http://arxiv.org/abs/2012.00889"/>
        <updated>2021-06-08T02:20:25.838Z</updated>
        <summary type="html"><![CDATA[We provide new perspectives and inference algorithms for Maximum Entropy
(MaxEnt) Inverse Reinforcement Learning (IRL), which provides a principled
method to find a most non-committal reward function consistent with given
expert demonstrations, among many consistent reward functions.

We first present a generalized MaxEnt formulation based on minimizing a
KL-divergence instead of maximizing an entropy. This improves the previous
heuristic derivation of the MaxEnt IRL model (for stochastic MDPs), allows a
unified view of MaxEnt IRL and Relative Entropy IRL, and leads to a model-free
learning algorithm for the MaxEnt IRL model. Second, a careful review of
existing inference algorithms and implementations showed that they
approximately compute the marginals required for learning the model. We provide
examples to illustrate this, and present an efficient and exact inference
algorithm. Our algorithm can handle variable length demonstrations; in
addition, while a basic version takes time quadratic in the maximum
demonstration length L, an improved version of this algorithm reduces this to
linear using a padding trick.

Experiments show that our exact algorithm improves reward learning as
compared to the approximate ones. Furthermore, our algorithm scales up to a
large, real-world dataset involving driver behaviour forecasting. We provide an
optimized implementation compatible with the OpenAI Gym interface. Our new
insight and algorithms could possibly lead to further interest and exploration
of the original MaxEnt IRL model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1"&gt;Aaron J. Snoswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Surya P. N. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nan Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Versus Adversarial Euler-based Generators For Time Series. (arXiv:2102.05313v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05313</id>
        <link href="http://arxiv.org/abs/2102.05313"/>
        <updated>2021-06-08T02:20:25.820Z</updated>
        <summary type="html"><![CDATA[We introduce three new generative models for time series. Based on Euler
discretization and Wasserstein metrics, they are able to capture time marginal
distributions and temporal dynamics. Two of these methods rely on the
adaptation of generative adversarial networks (GANs) to time series. Both of
them outperform state-of-the-art benchmarks by capturing the underlying
temporal structure on synthetic time series. The third algorithm, called
Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the
transition probability distributions over all time steps. In the context of Ito
processes, we provide theoretical guarantees that minimizing this criterion
implies accurate estimations of the drift and volatility parameters. We
demonstrate empirically that CEGEN outperforms state-of-the-art and GAN
generators on both marginal and temporal dynamics metrics. Besides, it
identifies accurate correlation structures in high dimension. When few data
points are available, we verify the effectiveness of CEGEN, when combined with
transfer learning methods on Monte Carlo simulations. Finally, we illustrate
the robustness of our method on various real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Remlinger_C/0/1/0/all/0/1"&gt;Carl Remlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikael_J/0/1/0/all/0/1"&gt;Joseph Mikael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High Dimensional Wasserstein Geodesics. (arXiv:2102.02992v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02992</id>
        <link href="http://arxiv.org/abs/2102.02992"/>
        <updated>2021-06-08T02:20:25.814Z</updated>
        <summary type="html"><![CDATA[We propose a new formulation and learning strategy for computing the
Wasserstein geodesic between two probability distributions in high dimensions.
By applying the method of Lagrange multipliers to the dynamic formulation of
the optimal transport (OT) problem, we derive a minimax problem whose saddle
point is the Wasserstein geodesic. We then parametrize the functions by deep
neural networks and design a sample based bidirectional learning algorithm for
training. The trained networks enable sampling from the Wasserstein geodesic.
As by-products, the algorithm also computes the Wasserstein distance and OT map
between the marginal distributions. We demonstrate the performance of our
algorithms through a series of experiments with both synthetic and realistic
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaojun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haomin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03417</id>
        <link href="http://arxiv.org/abs/2103.03417"/>
        <updated>2021-06-08T02:20:25.808Z</updated>
        <summary type="html"><![CDATA[The measurement of bias in machine learning often focuses on model
performance across identity subgroups (such as man and woman) with respect to
groundtruth labels. However, these methods do not directly measure the
associations that a model may have learned, for example between labels and
identity subgroups. Further, measuring a model's bias requires a fully
annotated evaluation dataset which may not be easily available in practice. We
present an elegant mathematical solution that tackles both issues
simultaneously, using image classification as a working example. By treating a
classification model's predictions for a given image as a set of labels
analogous to a bag of words, we rank the biases that a model has learned with
respect to different identity labels. We use (man, woman) as a concrete example
of an identity label set (although this set need not be binary), and present
rankings for the labels that are most biased towards one identity or the other.
We demonstrate how the statistical properties of different association metrics
can lead to different rankings of the most "gender biased" labels, and conclude
that normalized pointwise mutual information (nPMI) is most useful in practice.
Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aka_O/0/1/0/all/0/1"&gt;Osman Aka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_K/0/1/0/all/0/1"&gt;Ken Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1"&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_C/0/1/0/all/0/1"&gt;Christina Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Margaret Mitchell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to distribute data across tasks for meta-learning?. (arXiv:2103.08463v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08463</id>
        <link href="http://arxiv.org/abs/2103.08463"/>
        <updated>2021-06-08T02:20:25.801Z</updated>
        <summary type="html"><![CDATA[Meta-learning models transfer the knowledge acquired from previous tasks to
quickly learn new ones. They are trained on benchmarks with a fixed number of
data points per task. This number is usually arbitrary and it is unknown how it
affects performance at testing. Since labelling of data is expensive, finding
the optimal allocation of labels across training tasks may reduce costs. Given
a fixed budget of labels, should we use a small number of highly labelled
tasks, or many tasks with few labels each? Should we allocate more labels to
some tasks and less to others? We show that: 1) If tasks are homogeneous, there
is a uniform optimal allocation, whereby all tasks get the same amount of data;
2) At fixed budget, there is a trade-off between number of tasks and number of
data points per task, with a unique and constant optimum; 3) When trained
separately, harder task should get more data, at the cost of a smaller number
of tasks; 4) When training on a mixture of easy and hard tasks, more data
should be allocated to easy tasks. Interestingly, Neuroscience experiments have
shown that human visual skills also transfer better from easy tasks. We prove
these results mathematically on mixed linear regression, and we show
empirically that the same results hold for few-shot image classification on
CIFAR-FS and mini-ImageNet. Our results provide guidance for allocating labels
across tasks when collecting data for meta-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cioba_A/0/1/0/all/0/1"&gt;Alexandru Cioba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bromberg_M/0/1/0/all/0/1"&gt;Michael Bromberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niyogi_R/0/1/0/all/0/1"&gt;Ritwik Niyogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;Jezabel Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PeerGAN: Generative Adversarial Networks with a Competing Peer Discriminator. (arXiv:2101.07524v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07524</id>
        <link href="http://arxiv.org/abs/2101.07524"/>
        <updated>2021-06-08T02:20:25.794Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce PeerGAN, a generative adversarial network (GAN)
solution to improve the stability of the generated samples and to mitigate mode
collapse. Built upon the Vanilla GAN's two-player game between the
discriminator $D_1$ and the generator $G$, we introduce a peer discriminator
$D_2$ to the min-max game. Similar to previous work using two discriminators,
the first role of both $D_1$, $D_2$ is to distinguish between generated samples
and real ones, while the generator tries to generate high-quality samples which
are able to fool both discriminators. Different from existing methods, we
introduce another game between $D_1$ and $D_2$ to discourage their agreement
and therefore increase the level of diversity of the generated samples. This
property alleviates the issue of early mode collapse by preventing $D_1$ and
$D_2$ from converging too fast. We provide theoretical analysis for the
equilibrium of the min-max game formed among $G, D_1, D_2$. We offer
convergence behavior of PeerGAN as well as stability of the min-max game. It's
worth mentioning that PeerGAN operates in the unsupervised setting, and the
additional game between $D_1$ and $D_2$ does not need any label supervision.
Experiments results on a synthetic dataset and on real-world image datasets
(MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG) demonstrate that PeerGAN
outperforms competitive baseline work in generating diverse and high-quality
samples, while only introduces negligible computation cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiahao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiutong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Invariant State Abstractions for Model-Based Reinforcement Learning. (arXiv:2102.09850v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09850</id>
        <link href="http://arxiv.org/abs/2102.09850"/>
        <updated>2021-06-08T02:20:25.788Z</updated>
        <summary type="html"><![CDATA[Accuracy and generalization of dynamics models is key to the success of
model-based reinforcement learning (MBRL). As the complexity of tasks
increases, so does the sample inefficiency of learning accurate dynamics
models. However, many complex tasks also exhibit sparsity in the dynamics,
i.e., actions have only a local effect on the system dynamics. In this paper,
we exploit this property with a causal invariance perspective in the
single-task setting, introducing a new type of state abstraction called
\textit{model-invariance}. Unlike previous forms of state abstractions, a
model-invariance state abstraction leverages causal sparsity over state
variables. This allows for compositional generalization to unseen states,
something that non-factored forms of state abstractions cannot do. We prove
that an optimal policy can be learned over this model-invariance state
abstraction and show improved generalization in a simple toy domain. Next, we
propose a practical method to approximately learn a model-invariant
representation for complex domains and validate our approach by showing
improved modelling performance over standard maximum likelihood approaches on
challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL
setting we show strong performance gains with respect to sample efficiency
across a host of other continuous control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1"&gt;Manan Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1"&gt;Roberto Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource Allocation in Multi-armed Bandit Exploration: Overcoming Sublinear Scaling with Adaptive Parallelism. (arXiv:2011.00330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00330</id>
        <link href="http://arxiv.org/abs/2011.00330"/>
        <updated>2021-06-08T02:20:25.770Z</updated>
        <summary type="html"><![CDATA[We study exploration in stochastic multi-armed bandits when we have access to
a divisible resource that can be allocated in varying amounts to arm pulls. We
focus in particular on the allocation of distributed computing resources, where
we may obtain results faster by allocating more resources per pull, but might
have reduced throughput due to nonlinear scaling. For example, in
simulation-based scientific studies, an expensive simulation can be sped up by
running it on multiple cores. This speed-up however, is partly offset by the
communication among cores, which results in lower throughput than if fewer
cores were allocated per trial to run more trials in parallel. In this paper,
we explore these trade-offs in two settings. First, in a fixed confidence
setting, we need to find the best arm with a given target success probability
as quickly as possible. We propose an algorithm which trades off between
information accumulation and throughput and show that the time taken can be
upper bounded by the solution of a dynamic program whose inputs are the gaps
between the sub-optimal and optimal arms. We also prove a matching hardness
result. Second, we present an algorithm for a fixed deadline setting, where we
are given a time deadline and need to maximize the probability of finding the
best arm. We corroborate our theoretical insights with simulation experiments
that show that the algorithms consistently match or outperform baseline
algorithms on a variety of problem instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1"&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Witness Two-Sample Test. (arXiv:2102.05573v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05573</id>
        <link href="http://arxiv.org/abs/2102.05573"/>
        <updated>2021-06-08T02:20:25.764Z</updated>
        <summary type="html"><![CDATA[The Maximum Mean Discrepancy (MMD) has been the state-of-the-art
nonparametric test for tackling the two-sample problem. Its statistic is given
by the difference in expectations of the witness function, a real-valued
function defined as a weighted sum of kernel evaluations on a set of basis
points. Typically the kernel is optimized on a training set, and hypothesis
testing is performed on a separate test set to avoid overfitting (i.e., control
type-I error). That is, the test set is used to simultaneously estimate the
expectations and define the basis points, while the training set only serves to
select the kernel and is discarded. In this work, we argue that this data
splitting scheme is overly conservative, and propose to use the training data
to also define the weights and the basis points for better data efficiency. We
show that 1) the new test is consistent and has a well-controlled type-I error;
2) the optimal witness function is given by a precision-weighted mean in the
reproducing kernel Hilbert space associated with the kernel, and is closely
related to kernel Fisher discriminant analysis; and 3) the test power of the
proposed test is comparable or exceeds that of the MMD and other modern tests,
as verified empirically on challenging synthetic and real problems (e.g., Higgs
data).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kubler_J/0/1/0/all/0/1"&gt;Jonas M. K&amp;#xfc;bler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitkrittum_W/0/1/0/all/0/1"&gt;Wittawat Jitkrittum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction. (arXiv:2103.05908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05908</id>
        <link href="http://arxiv.org/abs/2103.05908"/>
        <updated>2021-06-08T02:20:25.757Z</updated>
        <summary type="html"><![CDATA[We address the challenge of extracting structured information from business
documents without detailed annotations. We propose Deep Conditional
Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional
complex documents and use Recursive Neural Networks to create an end-to-end
system for finding the most probable parse that represents the structured
information to be extracted. This system is trained end-to-end with scanned
documents as input and only relational-records as labels. The
relational-records are extracted from existing databases avoiding the cost of
annotating documents by hand. We apply this approach to extract information
from scanned invoices achieving state-of-the-art results despite using no
hand-annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chua_F/0/1/0/all/0/1"&gt;Freddy C. Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P. Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integral Probability Metric based Regularization for Optimal Transport. (arXiv:2011.05001v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05001</id>
        <link href="http://arxiv.org/abs/2011.05001"/>
        <updated>2021-06-08T02:20:25.750Z</updated>
        <summary type="html"><![CDATA[Regularization in Optimal Transport (OT) problems has been shown to
critically affect the associated computational and sample complexities. It also
has been observed that regularization effectively helps in handling noisy
marginals as well as marginals with unequal masses. However, existing works on
OT restrict themselves to $\phi$-divergences based regularization. In this
work, we propose and analyze Integral Probability Metric (IPM) based
regularization in OT problems. While it is expected that the well-established
advantages of IPMs are inherited by the IPM-regularized OT variants, we
interestingly observe that some useful aspects of $\phi$-regularization are
preserved. For example, we show that the OT formulation, where the marginal
constraints are relaxed using IPM-regularization, also lifts the ground metric
to that over (perhaps un-normalized) measures. Infact, the lifted metric turns
out to be another IPM whose generating set is the intersection of that of the
IPM employed for regularization and the set of 1-Lipschitz functions under the
ground metric. Also, in the special case where the regularization is squared
maximum mean discrepancy based, the proposed OT variant, as well as the
corresponding Barycenter formulation, turn out to be those of minimizing a
convex quadratic subject to non-negativity/simplex constraints and hence can be
solved efficiently. Simulations confirm that the optimal transport plans/maps
obtained with IPM-regularization are intrinsically different from those
obtained with $\phi$-regularization. Empirical results illustrate the efficacy
of the proposed IPM-regularized OT formulation.

This draft contains the main paper and the Appendices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manupriya_P/0/1/0/all/0/1"&gt;Piyushi Manupriya&lt;/a&gt; (IIT Hyderabad, INDIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_J/0/1/0/all/0/1"&gt;J. Saketha Nath&lt;/a&gt; (IIT Hyderabad, INDIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1"&gt;Pratik Jawanpuria&lt;/a&gt; (Microsoft IDC, INDIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thresholded Lasso Bandit. (arXiv:2010.11994v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11994</id>
        <link href="http://arxiv.org/abs/2010.11994"/>
        <updated>2021-06-08T02:20:25.725Z</updated>
        <summary type="html"><![CDATA[In this paper, we revisit the regret minimization problem in sparse
stochastic contextual linear bandits, where feature vectors may be of large
dimension $d$, but where the reward function depends on a few, say $s_0\ll d$,
of these features only. We present Thresholded Lasso bandit, an algorithm that
(i) estimates the vector defining the reward function as well as its sparse
support, i.e., significant feature elements, using the Lasso framework with
thresholding, and (ii) selects an arm greedily according to this estimate
projected on its support. The algorithm does not require prior knowledge of the
sparsity index $s_0$. For this simple algorithm, we establish non-asymptotic
regret upper bounds scaling as $\mathcal{O}( \log d + \sqrt{T} )$ in general,
and as $\mathcal{O}( \log d + \log T)$ under the so-called margin condition (a
setting where arms are well separated). The regret of previous algorithms
scales as $\mathcal{O}( \log d + \sqrt{T \log (d T)})$ and $\mathcal{O}( \log T
\log d)$ in the two settings, respectively. Through numerical experiments, we
confirm that our algorithm outperforms existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1"&gt;Kaito Ariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abe_K/0/1/0/all/0/1"&gt;Kenshi Abe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Prouti&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anonymizing Machine Learning Models. (arXiv:2007.13086v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13086</id>
        <link href="http://arxiv.org/abs/2007.13086"/>
        <updated>2021-06-08T02:20:25.718Z</updated>
        <summary type="html"><![CDATA[There is a known tension between the need to analyze personal data to drive
business and privacy concerns. Many data protection regulations, including the
EU General Data Protection Regulation (GDPR) and the California Consumer
Protection Act (CCPA), set out strict restrictions and obligations on companies
that collect or process personal data. Moreover, machine learning models
themselves can be used to derive personal information, as demonstrated by
recent membership and attribute inference attacks. Anonymized data, however, is
exempt from data protection principles and obligations. Thus, models built on
anonymized data are also exempt from any privacy obligations, in addition to
providing better protection against such attacks on the training data. Learning
on anonymized data typically results in a significant degradation in accuracy.
We address this challenge by guiding our anonymization using the knowledge
encoded within the model, and targeting it to minimize the impact on the
model's accuracy, a process we call accuracy-guided anonymization. We
demonstrate that by focusing on the model's accuracy rather than information
loss, our method outperforms state of the art k-anonymity methods in terms of
the achieved utility, in particular with high values of k and large numbers of
quasi-identifiers. We also demonstrate that our approach achieves similar
results in its ability to prevent membership inference attacks as alternative
approaches based on differential privacy. This shows that model-guided
anonymization can, in some cases, be a legitimate substitute for such methods,
while averting some of their inherent drawbacks such as complexity, performance
overhead and being fitted to specific model types. As opposed to methods that
rely on adding noise during training, our approach does not rely on making any
modifications to the training algorithm itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1"&gt;Abigail Goldsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1"&gt;Gilad Ezov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1"&gt;Micha Moffie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1"&gt;Ariel Farkash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Ball Model for Embedding Hierarchical Structures in the Complex Hyperbolic Space. (arXiv:2105.03966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03966</id>
        <link href="http://arxiv.org/abs/2105.03966"/>
        <updated>2021-06-08T02:20:25.711Z</updated>
        <summary type="html"><![CDATA[Learning the representation of data with hierarchical structures in the
hyperbolic space attracts increasing attention in recent years. Due to the
constant negative curvature, the hyperbolic space resembles tree metrics and
captures the tree-like properties naturally, which enables the hyperbolic
embeddings to improve over traditional Euclidean models. However, many
real-world hierarchically structured data such as taxonomies and multitree
networks have varying local structures and they are not trees, thus they do not
ubiquitously match the constant curvature property of the hyperbolic space. To
address this limitation of hyperbolic embeddings, we explore the complex
hyperbolic space, which has the variable negative curvature, for representation
learning. Specifically, we propose to learn the embeddings of hierarchically
structured data in the unit ball model of the complex hyperbolic space. The
unit ball model based embeddings have a more powerful representation capacity
to capture a variety of hierarchical structures. Through experiments on
synthetic and real-world data, we show that our approach improves over the
hyperbolic embedding models significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huiru Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Caigao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yangqiu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;James Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Junwu Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Embed Categorical Features without Embedding Tables for Recommendation. (arXiv:2010.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10784</id>
        <link href="http://arxiv.org/abs/2010.10784"/>
        <updated>2021-06-08T02:20:25.704Z</updated>
        <summary type="html"><![CDATA[Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Derek Zhiyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tiansheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning. (arXiv:2007.11622v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11622</id>
        <link href="http://arxiv.org/abs/2007.11622"/>
        <updated>2021-06-08T02:20:25.698Z</updated>
        <summary type="html"><![CDATA[On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 34.1%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Han Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Ligeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Open-Ended Commonsense Reasoning. (arXiv:2010.14439v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14439</id>
        <link href="http://arxiv.org/abs/2010.14439"/>
        <updated>2021-06-08T02:20:25.690Z</updated>
        <summary type="html"><![CDATA[Current commonsense reasoning research focuses on developing models that use
commonsense knowledge to answer multiple-choice questions. However, systems
designed to answer multiple-choice questions may not be useful in applications
that do not provide a small list of candidate answers to choose from. As a step
towards making commonsense reasoning research more realistic, we propose to
study open-ended commonsense reasoning (OpenCSR) -- the task of answering a
commonsense question without any pre-defined choices -- using as a resource
only a corpus of commonsense facts written in natural language. OpenCSR is
challenging due to a large decision space, and because many questions require
implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an
efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To
evaluate OpenCSR methods, we adapt several popular commonsense reasoning
benchmarks, and collect multiple new answers for each test question via
crowd-sourcing. Experiments show that DrFact outperforms strong baseline
methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Declarative Approaches to Counterfactual Explanations for Classification. (arXiv:2011.07423v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07423</id>
        <link href="http://arxiv.org/abs/2011.07423"/>
        <updated>2021-06-08T02:20:25.684Z</updated>
        <summary type="html"><![CDATA[We propose answer-set programs that specify and compute counterfactual
interventions on entities that are input on a classification model. In relation
to the outcome of the model, the resulting counterfactual entities serve as a
basis for the definition and computation of causality-based explanation scores
for the feature values in the entity under classification, namely
"responsibility scores". The approach and the programs can be applied with
black-box models, and also with models that can be specified as logic programs,
such as rule-based classifiers. The main focus of this work is on the
specification and computation of "best" counterfactual entities, i.e. those
that lead to maximum responsibility scores. From them one can read off the
explanations as maximum responsibility feature values in the original entity.
We also extend the programs to bring into the picture semantic or domain
knowledge. We show how the approach could be extended by means of probabilistic
methods, and how the underlying probability distributions could be modified
through the use of constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1"&gt;Leopoldo Bertossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Performance of Knowledge Graph Embeddings in Drug Discovery. (arXiv:2105.10488v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10488</id>
        <link href="http://arxiv.org/abs/2105.10488"/>
        <updated>2021-06-08T02:20:25.677Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models
have recently begun to be explored in the context of drug discovery and have
the potential to assist in key challenges such as target identification. In the
drug discovery domain, KGs can be employed as part of a process which can
result in lab-based experiments being performed, or impact on other decisions,
incurring significant time and financial costs and most importantly, ultimately
influencing patient healthcare. For KGE models to have impact in this domain, a
better understanding of not only of performance, but also the various factors
which determine it, is required.

In this study we investigate, over the course of many thousands of
experiments, the predictive performance of five KGE models on two public drug
discovery-oriented KGs. Our goal is not to focus on the best overall model or
configuration, instead we take a deeper look at how performance can be affected
by changes in the training setup, choice of hyperparameters, model parameter
initialisation seed and different splits of the datasets. Our results highlight
that these factors have significant impact on performance and can even affect
the ranking of models. Indeed these factors should be reported along with model
architectures to ensure complete reproducibility and fair comparisons of future
work, and we argue this is critical for the acceptance of use, and impact of
KGEs in a biomedical setting. To aid reproducibility of our own work, we
release all experimentation code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian P Barrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Charles Tapley Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the originality of intellectual property assets based on machine learning outputs. (arXiv:2010.06997v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06997</id>
        <link href="http://arxiv.org/abs/2010.06997"/>
        <updated>2021-06-08T02:20:25.623Z</updated>
        <summary type="html"><![CDATA[Originality criteria are frequently used to compare assets and, in
particular, to assess the validity of intellectual property (IP) rights such as
copyright and design rights. In this work, the originality of an asset is
formulated as a function of the distances between this asset and its
comparands, using concepts of maximum entropy and surprisal analysis. Namely,
the originality function is defined according to the surprisal associated with
a given asset. Creative assets can be justifiably compared to particles that
repel each other via an electrostatic-like pair potential. This allows a very
simple, suitably bounded formula to be obtained, in which the originality of an
asset writes as the ratio of a reference energy to an interaction energy
imparted to that asset. In particular, the originality of an asset can be
expressed as a ratio of two average distances, i.e., the harmonic mean of the
distances from this asset to its comparands divided by the harmonic mean of the
distances between the sole comparands. Accordingly, the originality of objects
such as IP assets can be simply estimated based on distances computed thanks to
unsupervised machine learning techniques or other distance computation
algorithms. Application is made to various types of assets, including emojis,
typeface designs, paintings, and novel titles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ragot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNiTE: Unitary N-body Tensor Equivariant Network with Applications to Quantum Chemistry. (arXiv:2105.14655v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14655</id>
        <link href="http://arxiv.org/abs/2105.14655"/>
        <updated>2021-06-08T02:20:25.610Z</updated>
        <summary type="html"><![CDATA[Equivariant neural networks have been successful in incorporating various
types of symmetries, but are mostly limited to vector representations of
geometric objects. Despite the prevalence of higher-order tensors in various
application domains, e.g. in quantum chemistry, equivariant neural networks for
general tensors remain unexplored. Previous strategies for learning equivariant
functions on tensors mostly rely on expensive tensor factorization which is not
scalable when the dimensionality of the problem becomes large. In this work, we
propose unitary $N$-body tensor equivariant neural network (UNiTE), an
architecture for a general class of symmetric tensors called $N$-body tensors.
The proposed neural network is equivariant with respect to the actions of a
unitary group, such as the group of 3D rotations. Furthermore, it has a linear
time complexity with respect to the number of non-zero elements in the tensor.
We also introduce a normalization method, viz., Equivariant Normalization, to
improve generalization of the neural network while preserving symmetry. When
applied to quantum chemistry, UNiTE outperforms all state-of-the-art machine
learning methods of that domain with over 110% average improvements on multiple
benchmarks. Finally, we show that UNiTE achieves a robust zero-shot
generalization performance on diverse down stream chemistry tasks, while being
three orders of magnitude faster than conventional numerical methods with
competitive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhuoran Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1"&gt;Anders S. Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welborn_M/0/1/0/all/0/1"&gt;Matthew Welborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manby_F/0/1/0/all/0/1"&gt;Frederick R. Manby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1"&gt;Thomas F. Miller III&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrepancy-Based Active Learning for Domain Adaptation. (arXiv:2103.03757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03757</id>
        <link href="http://arxiv.org/abs/2103.03757"/>
        <updated>2021-06-08T02:20:25.600Z</updated>
        <summary type="html"><![CDATA[The goal of the paper is to design active learning strategies which lead to
domain adaptation under an assumption of covariate shift in the case of
Lipschitz labeling function. Building on previous work by Mansour et al. (2009)
we adapt the concept of discrepancy distance between source and target
distributions to restrict the maximization over the hypothesis class to a
localized class of functions which are performing accurate labeling on the
source domain. We derive generalization error bounds for such active learning
strategies in terms of Rademacher average and localized discrepancy for general
loss functions which satisfy a regularity condition. A practical K-medoids
algorithm that can address the case of large data set is inferred from the
theoretical bounds. Our numerical experiments show that the proposed algorithm
is competitive against other state-of-the-art active learning techniques in the
context of domain adaptation, in particular on large data sets of around one
hundred thousand images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Francois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Deep Generative Models via Discriminator Gradient Flow. (arXiv:2012.00780v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00780</id>
        <link href="http://arxiv.org/abs/2012.00780"/>
        <updated>2021-06-08T02:20:25.585Z</updated>
        <summary type="html"><![CDATA[Deep generative modeling has seen impressive advances in recent years, to the
point where it is now commonplace to see simulated samples (e.g., images) that
closely resemble real-world data. However, generation quality is generally
inconsistent for any given model and can vary dramatically between samples. We
introduce Discriminator Gradient flow (DGflow), a new technique that improves
generated samples via the gradient flow of entropy-regularized f-divergences
between the real and the generated data distributions. The gradient flow takes
the form of a non-linear Fokker-Plank equation, which can be easily simulated
by sampling from the equivalent McKean-Vlasov process. By refining inferior
samples, our technique avoids wasteful sample rejection used by previous
methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN
variants, we show our refinement approach can be applied to GANs with
vector-valued critics and even other deep generative models such as VAEs and
Normalizing Flows. Empirical results on multiple synthetic, image, and text
datasets demonstrate that DGflow leads to significant improvement in the
quality of generated samples for a variety of generative models, outperforming
the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator
Driven Latent Sampling (DDLS) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1"&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Ming Liang Ang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1"&gt;Harold Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neograd: Near-Ideal Gradient Descent. (arXiv:2010.07873v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07873</id>
        <link href="http://arxiv.org/abs/2010.07873"/>
        <updated>2021-06-08T02:20:25.572Z</updated>
        <summary type="html"><![CDATA[The purpose of this paper is to improve upon existing variants of gradient
descent by solving two problems: (1) removing (or reducing) the plateau that
occurs while minimizing the cost function,(2) continually adjusting the
learning rate to an "ideal" value. The approach taken is to approximately solve
for the learning rate as a function of a trust metric. When this technique is
hybridized with momentum, it creates an especially effective gradient descent
variant, called NeogradM. It is shown to outperform Adam on several test
problems, and can easily reach cost function values that are smaller by a
factor of $10^8$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Michael F. Zimmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching in Selective and Balanced Representation Space for Treatment Effects Estimation. (arXiv:2009.06828v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06828</id>
        <link href="http://arxiv.org/abs/2009.06828"/>
        <updated>2021-06-08T02:20:25.555Z</updated>
        <summary type="html"><![CDATA[The dramatically growing availability of observational data is being
witnessed in various domains of science and technology, which facilitates the
study of causal inference. However, estimating treatment effects from
observational data is faced with two major challenges, missing counterfactual
outcomes and treatment selection bias. Matching methods are among the most
widely used and fundamental approaches to estimating treatment effects, but
existing matching methods have poor performance when facing data with high
dimensional and complicated variables. We propose a feature selection
representation matching (FSRM) method based on deep representation learning and
matching, which maps the original covariate space into a selective, nonlinear,
and balanced representation space, and then conducts matching in the learned
representation space. FSRM adopts deep feature selection to minimize the
influence of irrelevant variables for estimating treatment effects and
incorporates a regularizer based on the Wasserstein distance to learn balanced
representations. We evaluate the performance of our FSRM method on three
datasets, and the results demonstrate superiority over the state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rathbun_S/0/1/0/all/0/1"&gt;Stephen L. Rathbun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning. (arXiv:2102.06866v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06866</id>
        <link href="http://arxiv.org/abs/2102.06866"/>
        <updated>2021-06-08T02:20:25.537Z</updated>
        <summary type="html"><![CDATA[Instance discriminative self-supervised representation learning has been
attracted attention thanks to its unsupervised nature and informative feature
representation for downstream tasks. In practice, it commonly uses a larger
number of negative samples than the number of supervised classes. However,
there is an inconsistency in the existing analysis; theoretically, a large
number of negative samples degrade classification performance on a downstream
supervised task, while empirically, they improve the performance. We provide a
novel framework to analyze this empirical result regarding negative samples
using the coupon collector's problem. Our bound can implicitly incorporate the
supervised loss of the downstream task in the self-supervised loss by
increasing the number of negative samples. We confirm that our proposed
analysis holds on real-world benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nozawa_K/0/1/0/all/0/1"&gt;Kento Nozawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Issei Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning. (arXiv:2009.04324v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04324</id>
        <link href="http://arxiv.org/abs/2009.04324"/>
        <updated>2021-06-08T02:20:25.531Z</updated>
        <summary type="html"><![CDATA[As annotations of data can be scarce in large-scale practical problems,
leveraging unlabelled examples is one of the most important aspects of machine
learning. This is the aim of semi-supervised learning. To benefit from the
access to unlabelled data, it is natural to diffuse smoothly knowledge of
labelled data to unlabelled one. This induces to the use of Laplacian
regularization. Yet, current implementations of Laplacian regularization suffer
from several drawbacks, notably the well-known curse of dimensionality. In this
paper, we provide a statistical analysis to overcome those issues, and unveil a
large body of spectral filtering methods that exhibit desirable behaviors. They
are implemented through (reproducing) kernel methods, for which we provide
realistic computational guidelines in order to make our method usable with
large amounts of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1"&gt;Loucas Pillaud-Vivien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconditional Downscaling with Gaussian Processes. (arXiv:2105.12909v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12909</id>
        <link href="http://arxiv.org/abs/2105.12909"/>
        <updated>2021-06-08T02:20:25.525Z</updated>
        <summary type="html"><![CDATA[Refining low-resolution (LR) spatial fields with high-resolution (HR)
information is challenging as the diversity of spatial datasets often prevents
direct matching of observations. Yet, when LR samples are modeled as aggregate
conditional means of HR samples with respect to a mediating variable that is
globally observed, the recovery of the underlying fine-grained field can be
framed as taking an "inverse" of the conditional expectation, namely a
deconditioning problem. In this work, we introduce conditional mean processes
(CMP), a new class of Gaussian Processes describing conditional means. By
treating CMPs as inter-domain features of the underlying field, a posterior for
the latent field can be established as a solution to the deconditioning
problem. Furthermore, we show that this solution can be viewed as a two-staged
vector-valued kernel ridge regressor and show that it has a minimax optimal
convergence rate under mild assumptions. Lastly, we demonstrate its proficiency
in a synthetic and a real-world atmospheric field downscaling problem, showing
substantial improvements over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1"&gt;Siu Lun Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouabid_S/0/1/0/all/0/1"&gt;Shahine Bouabid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;Dino Sejdinovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning by Self-Transition for Handling Noisy Labels. (arXiv:2012.04337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04337</id>
        <link href="http://arxiv.org/abs/2012.04337"/>
        <updated>2021-06-08T02:20:25.519Z</updated>
        <summary type="html"><![CDATA[Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-06-08T02:20:25.512Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Approaches for Binary Classification to Discover Liver Diseases using Clinical Data. (arXiv:2104.12055v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12055</id>
        <link href="http://arxiv.org/abs/2104.12055"/>
        <updated>2021-06-08T02:20:25.495Z</updated>
        <summary type="html"><![CDATA[For a medical diagnosis, health professionals use different kinds of
pathological ways to make a decision for medical reports in terms of patients
medical condition. In the modern era, because of the advantage of computers and
technologies, one can collect data and visualize many hidden outcomes from
them. Statistical machine learning algorithms based on specific problems can
assist one to make decisions. Machine learning data driven algorithms can be
used to validate existing methods and help researchers to suggest potential new
decisions. In this paper, multiple imputation by chained equations was applied
to deal with missing data, and Principal Component Analysis to reduce the
dimensionality. To reveal significant findings, data visualizations were
implemented. We presented and compared many binary classifier machine learning
algorithms (Artificial Neural Network, Random Forest, Support Vector Machine)
which were used to classify blood donors and non-blood donors with hepatitis,
fibrosis and cirrhosis diseases. From the data published in UCI-MLR [1], all
mentioned techniques were applied to find one better method to classify blood
donors and non-blood donors (hepatitis, fibrosis, and cirrhosis) that can help
health professionals in a laboratory to make better decisions. Our proposed
ML-method showed better accuracy score (e.g. 98.23% for SVM). Thus, it improved
the quality of classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mostafa_F/0/1/0/all/0/1"&gt;Fahad B. Mostafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Md Easin Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Learning with 1D Convolutions on Random Walks. (arXiv:2102.08786v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08786</id>
        <link href="http://arxiv.org/abs/2102.08786"/>
        <updated>2021-06-08T02:20:25.488Z</updated>
        <summary type="html"><![CDATA[We propose CRaWl (CNNs for Random Walks), a novel neural network architecture
for graph learning. It is based on processing sequences of small subgraphs
induced by random walks with standard 1D CNNs. Thus, CRaWl is fundamentally
different from typical message passing graph neural network architectures. It
is inspired by techniques counting small subgraphs, such as the graphlet kernel
and motif counting, and combines them with random walk based techniques in a
highly efficient and scalable neural architecture. We demonstrate empirically
that CRaWl matches or outperforms state-of-the-art GNN architectures across a
multitude of benchmark datasets for classification and regression on graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toenshoff_J/0/1/0/all/0/1"&gt;Jan Toenshoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritzert_M/0/1/0/all/0/1"&gt;Martin Ritzert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_H/0/1/0/all/0/1"&gt;Hinrikus Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1"&gt;Martin Grohe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations. (arXiv:2106.02974v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02974</id>
        <link href="http://arxiv.org/abs/2106.02974"/>
        <updated>2021-06-08T02:20:25.482Z</updated>
        <summary type="html"><![CDATA[Automatic construction of a taxonomy supports many applications in
e-commerce, web search, and question answering. Existing taxonomy expansion or
completion methods assume that new concepts have been accurately extracted and
their embedding vectors learned from the text corpus. However, one critical and
fundamental challenge in fixing the incompleteness of taxonomies is the
incompleteness of the extracted concepts, especially for those whose names have
multiple words and consequently low frequency in the corpus. To resolve the
limitations of extraction-based methods, we propose GenTaxo to enhance taxonomy
completion by identifying positions in existing taxonomies that need new
concepts and then generating appropriate concept names. Instead of relying on
the corpus for concept embeddings, GenTaxo learns the contextual embeddings
from their surrounding graph-based and language-based relational information,
and leverages the corpus for pre-training a concept name generator.
Experimental results demonstrate that GenTaxo improves the completeness of
taxonomies over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1"&gt;Qingkai Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jinfeng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenhao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleland_Huang_J/0/1/0/all/0/1"&gt;Jane Cleland-Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity of Stability Constrained Imitation Learning. (arXiv:2102.09161v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09161</id>
        <link href="http://arxiv.org/abs/2102.09161"/>
        <updated>2021-06-08T02:20:25.476Z</updated>
        <summary type="html"><![CDATA[We study the following question in the context of imitation learning for
continuous control: how are the underlying stability properties of an expert
policy reflected in the sample-complexity of an imitation learning task? We
provide the first results showing that a surprisingly granular connection can
be made between the underlying expert system's incremental gain stability, a
novel measure of robust convergence between pairs of system trajectories, and
the dependency on the task horizon $T$ of the resulting generalization bounds.
In particular, we propose and analyze incremental gain stability constrained
versions of behavior cloning and a DAgger-like algorithm, and show that the
resulting sample-complexity bounds naturally reflect the underlying stability
properties of the expert system. As a special case, we delineate a class of
systems for which the number of trajectories needed to achieve
$\varepsilon$-suboptimality is sublinear in the task horizon $T$, and do so
without requiring (strong) convexity of the loss function in the policy
parameters. Finally, we conduct numerical experiments demonstrating the
validity of our insights on both a simple nonlinear system for which the
underlying stability properties can be easily tuned, and on a high-dimensional
quadrupedal robotic simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Stephen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tingnan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1"&gt;Nikolai Matni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strength of Minibatch Noise in SGD. (arXiv:2102.05375v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05375</id>
        <link href="http://arxiv.org/abs/2102.05375"/>
        <updated>2021-06-08T02:20:25.460Z</updated>
        <summary type="html"><![CDATA[The noise in stochastic gradient descent (SGD), caused by minibatch sampling,
is poorly understood despite its practical importance in deep learning. In this
work, we study the nature of SGD noise and fluctuation. We show that some
degree of mismatch between model and data complexity is needed for SGD to
``stir" a noise; such mismatch may be due to a label or input noise,
regularization, or underparametrization. Compared with previous works, the
present work focuses on deriving exactly solvable analytical results. Our work
also motivates a more accurate general formulation to describe minibatch noise,
and we show that the SGD noise takes different shapes and strengths in
different kinds of minima.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1"&gt;Takashi Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Distribution-Free Hypothesis Tests for the Regression Function of Binary Classification via Conditional Kernel Mean Embeddings. (arXiv:2103.05126v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05126</id>
        <link href="http://arxiv.org/abs/2103.05126"/>
        <updated>2021-06-08T02:20:25.366Z</updated>
        <summary type="html"><![CDATA[In this paper we suggest two statistical hypothesis tests for the regression
function of binary classification based on conditional kernel mean embeddings.
The regression function is a fundamental object in classification as it
determines both the Bayes optimal classifier and the misclassification
probabilities. A resampling based framework is presented and combined with
consistent point estimators of the conditional kernel mean map, in order to
construct distribution-free hypothesis tests. These tests are introduced in a
flexible manner allowing us to control the exact probability of type I error
for any sample size. We also prove that both proposed techniques are consistent
under weak statistical assumptions, i.e., the type II error probabilities
pointwise converge to zero.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1"&gt;Ambrus Tam&amp;#xe1;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Csan&amp;#xe1;d Cs&amp;#xe1;ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning. (arXiv:2102.13515v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13515</id>
        <link href="http://arxiv.org/abs/2102.13515"/>
        <updated>2021-06-08T02:20:25.365Z</updated>
        <summary type="html"><![CDATA[Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Campos_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprechmann_P/0/1/0/all/0/1"&gt;Pablo Sprechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1"&gt;Steven Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andre Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapturowski_S/0/1/0/all/0/1"&gt;Steven Kapturowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitvitskyi_A/0/1/0/all/0/1"&gt;Alex Vitvitskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badia_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Puigdom&amp;#xe8;nech Badia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction. (arXiv:2103.05908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05908</id>
        <link href="http://arxiv.org/abs/2103.05908"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[We address the challenge of extracting structured information from business
documents without detailed annotations. We propose Deep Conditional
Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional
complex documents and use Recursive Neural Networks to create an end-to-end
system for finding the most probable parse that represents the structured
information to be extracted. This system is trained end-to-end with scanned
documents as input and only relational-records as labels. The
relational-records are extracted from existing databases avoiding the cost of
annotating documents by hand. We apply this approach to extract information
from scanned invoices achieving state-of-the-art results despite using no
hand-annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chua_F/0/1/0/all/0/1"&gt;Freddy C. Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P. Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees. (arXiv:2106.02697v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02697</id>
        <link href="http://arxiv.org/abs/2106.02697"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[Tree-based models underpin many modern semantic search engines and
recommender systems due to their sub-linear inference times. In industrial
applications, these models operate at extreme scales, where every bit of
performance is critical. Memory constraints at extreme scales also require that
models be sparse, hence tree-based models are often back-ended by sparse matrix
algebra routines. However, there are currently no sparse matrix techniques
specifically designed for the sparsity structure one encounters in tree-based
models for extreme multi-label ranking/classification (XMR/XMC) problems. To
address this issue, we present the masked sparse chunk multiplication (MSCM)
technique, a sparse matrix technique specifically tailored to XMR trees. MSCM
is easy to implement, embarrassingly parallelizable, and offers a significant
performance boost to any existing tree inference pipeline at no cost. We
perform a comprehensive study of MSCM applied to several different sparse
inference schemes and benchmark our methods on a general purpose extreme
multi-label ranking framework. We observe that MSCM gives consistently dramatic
speedups across both the online and batch inference settings, single- and
multi-threaded settings, and on many different tree models and datasets. To
demonstrate its utility in industrial applications, we apply MSCM to an
enterprise-scale semantic product search problem with 100 million products and
achieve sub-millisecond latency of 0.88 ms per query on a single thread -- an
8x reduction in latency over vanilla inference techniques. The MSCM technique
requires absolutely no sacrifices to model accuracy as it gives exactly the
same results as standard sparse matrix techniques. Therefore, we believe that
MSCM will enable users of XMR trees to save a substantial amount of compute
resources in their inference pipelines at very little cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Etter_P/0/1/0/all/0/1"&gt;Philip A. Etter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lexing Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethink the Connections among Generalization, Memorization and the Spectral Bias of DNNs. (arXiv:2004.13954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13954</id>
        <link href="http://arxiv.org/abs/2004.13954"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[Over-parameterized deep neural networks (DNNs) with sufficient capacity to
memorize random noise can achieve excellent generalization performance,
challenging the bias-variance trade-off in classical learning theory. Recent
studies claimed that DNNs first learn simple patterns and then memorize noise;
some other works showed a phenomenon that DNNs have a spectral bias to learn
target functions from low to high frequencies during training. However, we show
that the monotonicity of the learning bias does not always hold: under the
experimental setup of deep double descent, the high-frequency components of
DNNs diminish in the late stage of training, leading to the second descent of
the test error. Besides, we find that the spectrum of DNNs can be applied to
indicating the second descent of the test error, even though it is calculated
from the training set only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Distributed Source Coding. (arXiv:2106.02797v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02797</id>
        <link href="http://arxiv.org/abs/2106.02797"/>
        <updated>2021-06-08T02:20:25.294Z</updated>
        <summary type="html"><![CDATA[Distributed source coding is the task of encoding an input in the absence of
correlated side information that is only available to the decoder. Remarkably,
Slepian and Wolf showed in 1973 that an encoder that has no access to the
correlated side information can asymptotically achieve the same compression
rate as when the side information is available at both the encoder and the
decoder. While there is significant prior work on this topic in information
theory, practical distributed source coding has been limited to synthetic
datasets and specific correlation structures. Here we present a general
framework for lossy distributed source coding that is agnostic to the
correlation structure and can scale to high dimensions. Rather than relying on
hand-crafted source-modeling, our method utilizes a powerful conditional deep
generative model to learn the distributed encoder and decoder. We evaluate our
method on realistic high-dimensional datasets and show substantial improvements
in distributed compression performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1"&gt;Jay Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyeji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Inference for Latent Block Models. (arXiv:2005.13273v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13273</id>
        <link href="http://arxiv.org/abs/2005.13273"/>
        <updated>2021-06-08T02:20:25.294Z</updated>
        <summary type="html"><![CDATA[Model selection in latent block models has been a challenging but important
task in the field of statistics. Specifically, a major challenge is encountered
when constructing a test on a block structure obtained by applying a specific
clustering algorithm to a finite size matrix. In this case, it becomes crucial
to consider the selective bias in the block structure, that is, the block
structure is selected from all the possible cluster memberships based on some
criterion by the clustering algorithm. To cope with this problem, this study
provides a selective inference method for latent block models. Specifically, we
construct a statistical test on a set of row and column cluster memberships of
a latent block model, which is given by a squared residue minimization
algorithm. The proposed test, by its nature, includes and thus can also be used
as the test on the set of row and column cluster numbers. We also propose an
approximated version of the test based on simulated annealing to avoid
combinatorial explosion in searching the optimal block structure. The results
show that the proposed exact and approximated tests work effectively, compared
to the naive test that did not take the selective bias into account.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Web based disease prediction and recommender system. (arXiv:2106.02813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02813</id>
        <link href="http://arxiv.org/abs/2106.02813"/>
        <updated>2021-06-08T02:20:25.285Z</updated>
        <summary type="html"><![CDATA[Worldwide, several cases go undiagnosed due to poor healthcare support in
remote areas. In this context, a centralized system is needed for effective
monitoring and analysis of the medical records. A web-based patient diagnostic
system is a central platform to store the medical history and predict the
possible disease based on the current symptoms experienced by a patient to
ensure faster and accurate diagnosis. Early disease prediction can help the
users determine the severity of the disease and take quick action. The proposed
web-based disease prediction system utilizes machine learning based
classification techniques on a data set acquired from the National Centre of
Disease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive
bayes classification approaches are utilized and an ensemble voting algorithm
is also proposed where each classifier is assigned weights dynamically based on
the prediction confidence. The proposed system is also equipped with a
recommendation scheme to recommend the type of tests based on the existing
symptoms of the patient, so that necessary precautions can be taken. A
centralized database ensures that the medical data is preserved and there is
transparency in the system. The tampering into the system is prevented by
giving the no "updation" rights once the diagnosis is created.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajora_H/0/1/0/all/0/1"&gt;Harish Rajora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial Awareness. (arXiv:2105.14083v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14083</id>
        <link href="http://arxiv.org/abs/2105.14083"/>
        <updated>2021-06-08T02:20:25.285Z</updated>
        <summary type="html"><![CDATA[Most studies on learning from noisy labels rely on unrealistic models of
i.i.d. label noise, such as class-conditional transition matrices. More recent
work on instance-dependent noise models are more realistic, but assume a single
generative process for label noise across the entire dataset. We propose a more
principled model of label noise that generalizes instance-dependent noise to
multiple labelers, based on the observation that modern datasets are typically
annotated using distributed crowdsourcing methods. Under our labeler-dependent
model, label noise manifests itself under two modalities: natural error of
good-faith labelers, and adversarial labels provided by malicious actors. We
present two adversarial attack vectors that more accurately reflect the label
noise that may be encountered in real-world settings, and demonstrate that
under our multimodal noisy labels model, state-of-the-art approaches for
learning from noisy labels are defeated by adversarial label attacks. Finally,
we propose a multi-stage, labeler-aware, model-agnostic framework that reliably
filters noisy labels by leveraging knowledge about which data partitions were
labeled by which labeler, and show that our proposed framework remains robust
even in the presence of extreme adversarial label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1"&gt;Glenn Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1"&gt;Robi Polikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation. (arXiv:2104.00994v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00994</id>
        <link href="http://arxiv.org/abs/2104.00994"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[This paper tackles automatically discovering phone-like acoustic units (AUD)
from unlabeled speech data. Past studies usually proposed single-step
approaches. We propose a two-stage approach: the first stage learns a
subword-discriminative feature representation and the second stage applies
clustering to the learned representation and obtains phone-like clusters as the
discovered acoustic units. In the first stage, a recently proposed method in
the task of unsupervised subword modeling is improved by replacing a
monolingual out-of-domain (OOD) ASR system with a multilingual one to create a
subword-discriminative representation that is more language-independent. In the
second stage, segment-level k-means is adopted, and two methods to represent
the variable-length speech segments as fixed-dimension feature vectors are
compared. Experiments on a very low-resource Mboshi language corpus show that
our approach outperforms state-of-the-art AUD in both normalized mutual
information (NMI) and F-score. The multilingual ASR improved upon the
monolingual ASR in providing OOD phone labels and in estimating the phone
boundaries. A comparison of our systems with and without knowing the
ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that
the current approach can significantly benefit from improved phone boundary
estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_S/0/1/0/all/0/1"&gt;Siyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Vel&amp;#xe1;zquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scharenborg_O/0/1/0/all/0/1"&gt;Odette Scharenborg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics. (arXiv:2106.02749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02749</id>
        <link href="http://arxiv.org/abs/2106.02749"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[Deep neural networks excel at image classification, but their performance is
far less robust to input perturbations than human perception. In this work we
explore whether this shortcoming may be partly addressed by incorporating
brain-inspired recurrent dynamics in deep convolutional networks. We take
inspiration from a popular framework in neuroscience: 'predictive coding'. At
each layer of the hierarchical model, generative feedback 'predicts' (i.e.,
reconstructs) the pattern of activity in the previous layer. The reconstruction
errors are used to iteratively update the network's representations across
timesteps, and to optimize the network's feedback weights over the natural
image dataset-a form of unsupervised training. We show that implementing this
strategy into two popular networks, VGG16 and EfficientNetB0, improves their
robustness against various corruptions. We hypothesize that other feedforward
networks could similarly benefit from the proposed framework. To promote
research in this direction, we provide an open-sourced PyTorch-based package
called Predify, which can be used to implement and investigate the impacts of
the predictive coding dynamics in any convolutional neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1"&gt;Milad Mozafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1"&gt;Callum Biggs O&amp;#x27;May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ador_B/0/1/0/all/0/1"&gt;Benjamin Ador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dodrio: Exploring Transformer Models with Interactive Visualization. (arXiv:2103.14625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14625</id>
        <link href="http://arxiv.org/abs/2103.14625"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[Why do large pre-trained transformer-based models perform so well across a
wide variety of NLP tasks? Recent research suggests the key may lie in
multi-headed attention mechanism's ability to learn and represent linguistic
information. Understanding how these models represent both syntactic and
semantic knowledge is vital to investigate why they succeed and fail, what they
have learned, and how they can improve. We present Dodrio, an open-source
interactive visualization tool to help NLP researchers and practitioners
analyze attention mechanisms in transformer-based models with linguistic
knowledge. Dodrio tightly integrates an overview that summarizes the roles of
different attention heads, and detailed views that help users compare attention
weights with the syntactic structure and semantic information in the input
text. To facilitate the visual comparison of attention weights and linguistic
knowledge, Dodrio applies different graph visualization techniques to represent
attention weights scalable to longer input text. Case studies highlight how
Dodrio provides insights into understanding the attention mechanism in
transformer-based models. Dodrio is available at
https://poloclub.github.io/dodrio/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turko_R/0/1/0/all/0/1"&gt;Robert Turko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[hBert + BiasCorp -- Fighting Racism on the Web. (arXiv:2104.02242v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02242</id>
        <link href="http://arxiv.org/abs/2104.02242"/>
        <updated>2021-06-08T02:20:25.283Z</updated>
        <summary type="html"><![CDATA[Subtle and overt racism is still present both in physical and online
communities today and has impacted many lives in different segments of the
society. In this short piece of work, we present how we're tackling this
societal issue with Natural Language Processing. We are releasing BiasCorp, a
dataset containing 139,090 comments and news segment from three specific
sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually
annotated) is ready for publication. We are currently in the final phase of
manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has
been used widely in several downstream tasks. In this work, we present hBERT,
where we modify certain layers of the pretrained BERT model with the new
Hopfield Layer. hBert generalizes well across different distributions with the
added advantage of a reduced model complexity. We are also releasing a
JavaScript library and a Chrome Extension Application, to help developers make
use of our trained model in web applications (say chat application) and for
users to identify and report racially biased contents on the web respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onabola_O/0/1/0/all/0/1"&gt;Olawale Onabola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akera_B/0/1/0/all/0/1"&gt;Benjamin Akera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibraheem_A/0/1/0/all/0/1"&gt;Abdulrahman Ibraheem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jia Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning Based Classification Algorithm Recommendation. (arXiv:2101.05993v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.05993</id>
        <link href="http://arxiv.org/abs/2101.05993"/>
        <updated>2021-06-08T02:20:25.282Z</updated>
        <summary type="html"><![CDATA[Recommending appropriate algorithms to a classification problem is one of the
most challenging issues in the field of data mining. The existing algorithm
recommendation models are generally constructed on only one kind of
meta-features by single learners. Considering that i) ensemble learners usually
show better performance and ii) different kinds of meta-features characterize
the classification problems in different viewpoints independently, and further
the models constructed with different sets of meta-features will be
complementary with each other and applicable for ensemble. This paper proposes
an ensemble learning-based algorithm recommendation method. To evaluate the
proposed recommendation method, extensive experiments with 13 well-known
candidate classification algorithms and five different kinds of meta-features
are conducted on 1090 benchmark classification problems. The results show the
effectiveness of the proposed ensemble learning based recommendation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qinbao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpinionRank: Extracting Ground Truth Labels from Unreliable Expert Opinions with Graph-Based Spectral Ranking. (arXiv:2102.05884v2 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.05884</id>
        <link href="http://arxiv.org/abs/2102.05884"/>
        <updated>2021-06-08T02:20:25.282Z</updated>
        <summary type="html"><![CDATA[As larger and more comprehensive datasets become standard in contemporary
machine learning, it becomes increasingly more difficult to obtain reliable,
trustworthy label information with which to train sophisticated models. To
address this problem, crowdsourcing has emerged as a popular, inexpensive, and
efficient data mining solution for performing distributed label collection.
However, crowdsourced annotations are inherently untrustworthy, as the labels
are provided by anonymous volunteers who may have varying, unreliable
expertise. Worse yet, some participants on commonly used platforms such as
Amazon Mechanical Turk may be adversarial, and provide intentionally incorrect
label information without the end user's knowledge. We discuss three
conventional models of the label generation process, describing their
parameterizations and the model-based approaches used to solve them. We then
propose OpinionRank, a model-free, interpretable, graph-based spectral
algorithm for integrating crowdsourced annotations into reliable labels for
performing supervised or semi-supervised learning. Our experiments show that
OpinionRank performs favorably when compared against more highly parameterized
algorithms. We also show that OpinionRank is scalable to very large datasets
and numbers of label sources, and requires considerably fewer computational
resources than previous approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1"&gt;Glenn Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1"&gt;Robi Polikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsification for Sums of Exponentials and its Algorithmic Applications. (arXiv:2106.02774v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02774</id>
        <link href="http://arxiv.org/abs/2106.02774"/>
        <updated>2021-06-08T02:20:25.281Z</updated>
        <summary type="html"><![CDATA[Many works in signal processing and learning theory operate under the
assumption that the underlying model is simple, e.g. that a signal is
approximately $k$-Fourier-sparse or that a distribution can be approximated by
a mixture model that has at most $k$ components. However the problem of fitting
the parameters of such a model becomes more challenging when the
frequencies/components are too close together.

In this work we introduce new methods for sparsifying sums of exponentials
and give various algorithmic applications. First we study Fourier-sparse
interpolation without a frequency gap, where Chen et al. gave an algorithm for
finding an $\epsilon$-approximate solution which uses $k' = \mbox{poly}(k, \log
1/\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in
one dimension without a separation condition. Kernel density estimators give an
$\epsilon$-approximation that uses $k' = O(k/\epsilon^2)$ components. These
methods both output models that are much more complex than what we started out
with. We show how to post-process to reduce the number of
frequencies/components down to $k' = \widetilde{O}(k)$, which is optimal up to
logarithmic factors. Moreover we give applications to model selection. In
particular, we give the first algorithms for approximately (and robustly)
determining the number of components in a Gaussian mixture model that work
without a separation condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations Can Be Manipulated. (arXiv:2106.02666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02666</id>
        <link href="http://arxiv.org/abs/2106.02666"/>
        <updated>2021-06-08T02:20:25.275Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are emerging as an attractive option for
providing recourse to individuals adversely impacted by algorithmic decisions.
As they are deployed in critical applications (e.g. law enforcement, financial
lending), it becomes important to ensure that we clearly understand the
vulnerabilities of these methods and find ways to address them. However, there
is little understanding of the vulnerabilities and shortcomings of
counterfactual explanations. In this work, we introduce the first framework
that describes the vulnerabilities of counterfactual explanations and shows how
they can be manipulated. More specifically, we show counterfactual explanations
may converge to drastically different counterfactuals under a small
perturbation indicating they are not robust. Leveraging this insight, we
introduce a novel objective to train seemingly fair models where counterfactual
explanations find much lower cost recourse under a slight perturbation. We
describe how these models can unfairly provide low-cost recourse for specific
subgroups in the data while appearing fair to auditors. We perform experiments
on loan and violent crime prediction data sets where certain subgroups achieve
up to 20x lower cost recourse under the perturbation. These results raise
concerns regarding the dependability of current counterfactual explanation
techniques, which we hope will inspire investigations in robust counterfactual
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer. (arXiv:2010.02036v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02036</id>
        <link href="http://arxiv.org/abs/2010.02036"/>
        <updated>2021-06-08T02:20:25.271Z</updated>
        <summary type="html"><![CDATA[State-of-the-art image-to-image translation methods tend to struggle in an
imbalanced domain setting, where one image domain lacks richness and diversity.
We introduce a new unsupervised translation network, BalaGAN, specifically
designed to tackle the domain imbalance problem. We leverage the latent
modalities of the richer domain to turn the image-to-image translation problem,
between two imbalanced domains, into a balanced, multi-class, and conditional
translation problem, more resembling the style transfer setting. Specifically,
we analyze the source domain and learn a decomposition of it into a set of
latent modes or classes, without any supervision. This leaves us with a
multitude of balanced cross-domain translation tasks, between all pairs of
classes, including the target domain. During inference, the trained network
takes as input a source image, as well as a reference or style image from one
of the modes as a condition, and produces an image which resembles the source
on the pixel-wise level, but shares the same mode as the reference. We show
that employing modalities within the dataset improves the quality of the
translated images, and that BalaGAN outperforms strong baselines of both
unconditioned and style-transfer-based image-to-image translation methods, in
terms of image quality and diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1"&gt;Or Patashnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1"&gt;Dov Danon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Hyper-Flow Diffusion. (arXiv:2102.07945v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07945</id>
        <link href="http://arxiv.org/abs/2102.07945"/>
        <updated>2021-06-08T02:20:25.270Z</updated>
        <summary type="html"><![CDATA[Recently, hypergraphs have attracted a lot of attention due to their ability
to capture complex relations among entities. The insurgence of hypergraphs has
resulted in data of increasing size and complexity that exhibit interesting
small-scale and local structure, e.g., small-scale communities and localized
node-ranking around a given set of seed nodes. Popular and principled ways to
capture the local structure are the local hypergraph clustering problem and
related seed set expansion problem. In this work, we propose the first local
diffusion method that achieves edge-size-independent Cheeger-type guarantee for
the problem of local hypergraph clustering while applying to a rich class of
higher-order relations that covers many previously studied special cases. Our
method is based on a primal-dual optimization formulation where the primal
problem has a natural network flow interpretation, and the dual problem has a
cut-based interpretation using the $\ell_2$-norm penalty on associated
cut-costs. We demonstrate the new technique is significantly better than
state-of-the-art methods on both synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1"&gt;Kimon Fountoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shenghao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Uncertainty under Laplace Approximations. (arXiv:2010.02720v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02720</id>
        <link href="http://arxiv.org/abs/2010.02720"/>
        <updated>2021-06-08T02:20:25.225Z</updated>
        <summary type="html"><![CDATA[Laplace approximations are classic, computationally lightweight means for
constructing Bayesian neural networks (BNNs). As in other approximate BNNs, one
cannot necessarily expect the induced predictive uncertainty to be calibrated.
Here we develop a formalism to explicitly "train" the uncertainty in a
decoupled way to the prediction itself. To this end, we introduce uncertainty
units for Laplace-approximated networks: Hidden units associated with a
particular weight structure that can be added to any pre-trained,
point-estimated network. Due to their weights, these units are inactive -- they
do not affect the predictions. But their presence changes the geometry (in
particular the Hessian) of the loss landscape, thereby affecting the network's
uncertainty estimates under a Laplace approximation. We show that such units
can be trained via an uncertainty-aware objective, improving standard Laplace
approximations' performance in various uncertainty quantification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1"&gt;Agustinus Kristiadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustly Learning Mixtures of $k$ Arbitrary Gaussians. (arXiv:2012.02119v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02119</id>
        <link href="http://arxiv.org/abs/2012.02119"/>
        <updated>2021-06-08T02:20:25.176Z</updated>
        <summary type="html"><![CDATA[We give a polynomial-time algorithm for the problem of robustly estimating a
mixture of $k$ arbitrary Gaussians in $\mathbb{R}^d$, for any fixed $k$, in the
presence of a constant fraction of arbitrary corruptions. This resolves the
main open problem in several previous works on algorithmic robust statistics,
which addressed the special cases of robustly estimating (a) a single Gaussian,
(b) a mixture of TV-distance separated Gaussians, and (c) a uniform mixture of
two Gaussians. Our main tools are an efficient \emph{partial clustering}
algorithm that relies on the sum-of-squares method, and a novel \emph{tensor
decomposition} algorithm that allows errors in both Frobenius norm and low-rank
terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1"&gt;Ainesh Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;He Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1"&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1"&gt;Santosh S. Vempala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video. (arXiv:2010.10637v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10637</id>
        <link href="http://arxiv.org/abs/2010.10637"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[How to extract effective expression representations that invariant to the
identity-specific attributes is a long-lasting problem for facial expression
recognition (FER). Most of the previous methods process the RGB images of a
sequence, while we argue that the off-the-shelf and valuable expression-related
muscle movement is already embedded in the compression format. In this paper,
we target to explore the inter-subject variations eliminated facial expression
representation in the compressed video domain. In the up to two orders of
magnitude compressed domain, we can explicitly infer the expression from the
residual frames and possibly extract identity factors from the I frame with a
pre-trained face recognition network. By enforcing the marginal independence of
them, the expression feature is expected to be purer for the expression and be
robust to identity shifts. Specifically, we propose a novel collaborative
min-min game for mutual information (MI) minimization in latent space. We do
not need the identity label or multiple expression samples from the same person
for identity elimination. Moreover, when the apex frame is annotated in the
dataset, the complementary constraint can be further added to regularize the
feature-level game. In testing, only the compressed residual frames are
required to achieve expression prediction. Our solution can achieve comparable
or better performance than the recent decoded image-based methods on the
typical FER benchmarks with about 3 times faster inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Interpretable and Trustworthy are GAMs?. (arXiv:2006.06466v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06466</id>
        <link href="http://arxiv.org/abs/2006.06466"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[Generalized additive models (GAMs) have become a leading modelclass for
interpretable machine learning. However, there are many algorithms for training
GAMs, and these can learn different or even contradictory models, while being
equally accurate. Which GAM should we trust? In this paper, we quantitatively
and qualitatively investigate a variety of GAM algorithms on real and simulated
datasets. We find that GAMs with high feature sparsity (only using afew
variables to make predictions) can miss patterns in the data and be unfair to
rare subpopulations. Our results suggest that inductive bias plays a crucial
role in what interpretable models learn and that tree-based GAMs represent the
best balance of sparsity, fidelity and accuracy and thus appear to be the most
trustworthy GAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Sarah Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1"&gt;Ben Lengerich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1"&gt;Rich Caruana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance Weight Estimation and Generalization in Domain Adaptation under Label Shift. (arXiv:2011.14251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14251</id>
        <link href="http://arxiv.org/abs/2011.14251"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[We study generalization under labeled shift for categorical and general
normed label spaces. We propose a series of methods to estimate the importance
weights from labeled source to unlabeled target domain and provide confidence
bounds for these estimators. We deploy these estimators and provide
generalization bounds in the unlabeled target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Replay-based Continual Zero-Shot Learning. (arXiv:2101.08894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08894</id>
        <link href="http://arxiv.org/abs/2101.08894"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning is a new paradigm to classify objects from classes that
are not available at training time. Zero-shot learning (ZSL) methods have
attracted considerable attention in recent years because of their ability to
classify unseen/novel class examples. Most of the existing approaches on ZSL
works when all the samples from seen classes are available to train the model,
which does not suit real life. In this paper, we tackle this hindrance by
developing a generative replay-based continual ZSL (GRCZSL). The proposed
method endows traditional ZSL to learn from streaming data and acquire new
knowledge without forgetting the previous tasks' gained experience. We handle
catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen
classes, which have appeared in the earlier tasks. These synthetic samples are
synthesized using the trained conditional variational autoencoder (VAE) over
the immediate past task. Moreover, we only require the current and immediate
previous VAE at any time for training and testing. The proposed GRZSL method is
developed for a single-head setting of continual learning, simulating a
real-world problem setting. In this setting, task identity is given during
training but unavailable during testing. GRCZSL performance is evaluated on
five benchmark datasets for the generalized setup of ZSL with fixed and dynamic
(incremental class) settings of continual learning. The existing class setting
presented recently in the literature is not suitable for a class-incremental
setting. Therefore, this paper proposes a new setting to address this issue.
Experimental results show that the proposed method significantly outperforms
the baseline and the state-of-the-art method and makes it more suitable for
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_C/0/1/0/all/0/1"&gt;Chandan Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1"&gt;Sethupathy Parameswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Ashish Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving hybrid machine learning tasks by traversing weight space geodesics. (arXiv:2106.02793v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02793</id>
        <link href="http://arxiv.org/abs/2106.02793"/>
        <updated>2021-06-08T02:20:25.173Z</updated>
        <summary type="html"><![CDATA[Machine learning problems have an intrinsic geometric structure as central
objects including a neural network's weight space and the loss function
associated with a particular task can be viewed as encoding the intrinsic
geometry of a given machine learning problem. Therefore, geometric concepts can
be applied to analyze and understand theoretical properties of machine learning
strategies as well as to develop new algorithms. In this paper, we address
three seemingly unrelated open questions in machine learning by viewing them
through a unified framework grounded in differential geometry. Specifically, we
view the weight space of a neural network as a manifold endowed with a
Riemannian metric that encodes performance on specific tasks. By defining a
metric, we can construct geodesic, minimum length, paths in weight space that
represent sets of networks of equivalent or near equivalent functional
performance on a specific task. We, then, traverse geodesic paths while
identifying networks that satisfy a second objective. Inspired by the geometric
insight, we apply our geodesic framework to 3 major applications: (i) Network
sparsification (ii) Mitigating catastrophic forgetting by constructing networks
with high performance on a series of objectives and (iii) Finding high-accuracy
paths connecting distinct local optima of deep networks in the non-convex loss
landscape. Our results are obtained on a wide range of network architectures
(MLP, VGG11/16) trained on MNIST, CIFAR-10/100. Broadly, we introduce a
geometric framework that unifies a range of machine learning objectives and
that can be applied to multiple classes of neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_G/0/1/0/all/0/1"&gt;Guruprasad Raghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1"&gt;Matt Thomson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00815</id>
        <link href="http://arxiv.org/abs/2102.00815"/>
        <updated>2021-06-08T02:20:25.173Z</updated>
        <summary type="html"><![CDATA[Finding the minimal structural assumptions that empower sample-efficient
learning is one of the most important research directions in Reinforcement
Learning (RL). This paper advances our understanding of this fundamental
question by introducing a new complexity measure -- Bellman Eluder (BE)
dimension. We show that the family of RL problems of low BE dimension is
remarkably rich, which subsumes a vast majority of existing tractable RL
problems including but not limited to tabular MDPs, linear MDPs, reactive
POMDPs, low Bellman rank problems as well as low Eluder dimension problems.
This paper further designs a new optimization-based algorithm -- GOLF, and
reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang
et al., 2017). We prove that both algorithms learn the near-optimal policies of
low BE dimension problems in a number of samples that is polynomial in all
relevant parameters, but independent of the size of state-action space. Our
regret and sample complexity results match or improve the best existing results
for several well-known subclasses of low BE dimension problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qinghua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Accelerated Stochastic Gradient Descent. (arXiv:2006.08950v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08950</id>
        <link href="http://arxiv.org/abs/2006.08950"/>
        <updated>2021-06-08T02:20:25.166Z</updated>
        <summary type="html"><![CDATA[We propose Federated Accelerated Stochastic Gradient Descent (FedAc), a
principled acceleration of Federated Averaging (FedAvg, also known as Local
SGD) for distributed optimization. FedAc is the first provable acceleration of
FedAvg that improves convergence speed and communication efficiency on various
types of convex functions. For example, for strongly convex and smooth
functions, when using $M$ workers, the previous state-of-the-art FedAvg
analysis can achieve a linear speedup in $M$ if given $M$ rounds of
synchronization, whereas FedAc only requires $M^{\frac{1}{3}}$ rounds.
Moreover, we prove stronger guarantees for FedAc when the objectives are
third-order smooth. Our technique is based on a potential-based perturbed
iterate analysis, a novel stability analysis of generalized accelerated SGD,
and a strategic tradeoff between acceleration and stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue Modeling. (arXiv:2106.02787v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02787</id>
        <link href="http://arxiv.org/abs/2106.02787"/>
        <updated>2021-06-08T02:20:25.159Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialogue (ToD) benchmarks provide an important avenue to
measure progress and develop better conversational agents. However, existing
datasets for end-to-end ToD modeling are limited to a single language,
hindering the development of robust end-to-end ToD systems for multilingual
countries and regions. Here we introduce BiToD, the first bilingual
multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD
contains over 7k multi-domain dialogues (144k utterances) with a large and
realistic bilingual knowledge base. It serves as an effective benchmark for
evaluating bilingual ToD systems and cross-lingual transfer learning
approaches. We provide state-of-the-art baselines under three evaluation
settings (monolingual, bilingual, and cross-lingual). The analysis of our
baselines in different settings highlights 1) the effectiveness of training a
bilingual ToD system compared to two independent monolingual ToD systems, and
2) the potential of leveraging a bilingual knowledge base and cross-lingual
transfer learning to improve the system performance under low resource
condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1"&gt;Andrea Madotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1"&gt;Feijun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuxiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perfect reconstruction of sparse signals with piecewise continuous nonconvex penalties and nonconvexity control. (arXiv:1902.07436v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.07436</id>
        <link href="http://arxiv.org/abs/1902.07436"/>
        <updated>2021-06-08T02:20:25.147Z</updated>
        <summary type="html"><![CDATA[We consider compressed sensing formulated as a minimization problem of
nonconvex sparse penalties, Smoothly Clipped Absolute deviation (SCAD) and
Minimax Concave Penalty (MCP). The nonconvexity of these penalties is
controlled by nonconvexity parameters, and L1 penalty is contained as a limit
with respect to these parameters. The analytically derived reconstruction limit
overcomes that of L1 and the algorithmic limit in the Bayes-optimal setting,
when the nonconvexity parameters have suitable values. However, for small
nonconvexity parameters, where the reconstruction of the relatively dense
signals is theoretically guaranteed, the corresponding approximate message
passing (AMP) cannot achieve perfect reconstruction. We identify that the
shrinks in the basin of attraction to the perfect reconstruction causes the
discrepancy between the AMP and corresponding theory using state evolution. A
part of the discrepancy is resolved by introducing the control of the
nonconvexity parameters to guide the AMP trajectory to the basin of the
attraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sakata_A/0/1/0/all/0/1"&gt;Ayaka Sakata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Obuchi_T/0/1/0/all/0/1"&gt;Tomoyuki Obuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02994</id>
        <link href="http://arxiv.org/abs/2106.02994"/>
        <updated>2021-06-08T02:20:25.139Z</updated>
        <summary type="html"><![CDATA[We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1"&gt;Safa Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Linear Optimization over Wasserstein Balls. (arXiv:2004.07162v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07162</id>
        <link href="http://arxiv.org/abs/2004.07162"/>
        <updated>2021-06-08T02:20:25.138Z</updated>
        <summary type="html"><![CDATA[Wasserstein balls, which contain all probability measures within a
pre-specified Wasserstein distance to a reference measure, have recently
enjoyed wide popularity in the distributionally robust optimization and machine
learning communities to formulate and solve data-driven optimization problems
with rigorous statistical guarantees. In this technical note we prove that the
Wasserstein ball is weakly compact under mild conditions, and we offer
necessary and sufficient conditions for the existence of optimal solutions. We
also characterize the sparsity of solutions if the Wasserstein ball is centred
at a discrete reference measure. In comparison with the existing literature,
which has proved similar results under different conditions, our proofs are
self-contained and shorter, yet mathematically rigorous, and our necessary and
sufficient conditions for the existence of optimal solutions are easily
verifiable in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yue_M/0/1/0/all/0/1"&gt;Man-Chung Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wiesemann_W/0/1/0/all/0/1"&gt;Wolfram Wiesemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Vision Transformers to Adversarial Examples. (arXiv:2104.02610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02610</id>
        <link href="http://arxiv.org/abs/2104.02610"/>
        <updated>2021-06-08T02:20:25.137Z</updated>
        <summary type="html"><![CDATA[Recent advances in attention-based networks have shown that Vision
Transformers can achieve state-of-the-art or near state-of-the-art results on
many image classification tasks. This puts transformers in the unique position
of being a promising alternative to traditional convolutional neural networks
(CNNs). While CNNs have been carefully studied with respect to adversarial
attacks, the same cannot be said of Vision Transformers. In this paper, we
study the robustness of Vision Transformers to adversarial examples. Our
analyses of transformer security is divided into three parts. First, we test
the transformer under standard white-box and black-box attacks. Second, we
study the transferability of adversarial examples between CNNs and
transformers. We show that adversarial examples do not readily transfer between
CNNs and transformers. Based on this finding, we analyze the security of a
simple ensemble defense of CNNs and transformers. By creating a new attack, the
self-attention blended gradient attack, we show that such an ensemble is not
secure under a white-box adversary. However, under a black-box adversary, we
show that an ensemble can achieve unprecedented robustness without sacrificing
clean accuracy. Our analysis for this work is done using six types of white-box
attacks and two types of black-box attacks. Our study encompasses multiple
Vision Transformers, Big Transfer Models and CNN architectures trained on
CIFAR-10, CIFAR-100 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rigel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Inference System for Improved Curvature Estimation in the Level-Set Method Using Machine Learning. (arXiv:2104.02951v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02951</id>
        <link href="http://arxiv.org/abs/2104.02951"/>
        <updated>2021-06-08T02:20:25.136Z</updated>
        <summary type="html"><![CDATA[We present a novel hybrid strategy based on machine learning to improve
curvature estimation in the level-set method. The proposed inference system
couples enhanced neural networks with standard numerical schemes to compute
curvature more accurately. The core of our hybrid framework is a switching
mechanism that relies on well established numerical techniques to gauge
curvature. If the curvature magnitude is larger than a resolution-dependent
threshold, it uses a neural network to yield a better approximation. Our
networks are multilayer perceptrons fitted to synthetic data sets composed of
sinusoidal- and circular-interface samples at various configurations. To reduce
data set size and training complexity, we leverage the problem's characteristic
symmetry and build our models on just half of the curvature spectrum. These
savings lead to a powerful inference system able to outperform any of its
numerical or neural component alone. Experiments with static, smooth interfaces
show that our hybrid solver is notably superior to conventional numerical
methods in coarse grids and along steep interface regions. Compared to prior
research, we have observed outstanding gains in precision after training the
regression model with data pairs from more than a single interface type and
transforming data with specialized input preprocessing. In particular, our
findings confirm that machine learning is a promising venue for reducing or
removing mass loss in the level-set method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larios_Cardenas_L/0/1/0/all/0/1"&gt;Luis &amp;#xc1;ngel Larios-C&amp;#xe1;rdenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibou_F/0/1/0/all/0/1"&gt;Frederic Gibou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05123</id>
        <link href="http://arxiv.org/abs/2002.05123"/>
        <updated>2021-06-08T02:20:25.135Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pony_R/0/1/0/all/0/1"&gt;Roi Pony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed+: A Unified Approach to Robust Personalized Federated Learning. (arXiv:2009.06303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06303</id>
        <link href="http://arxiv.org/abs/2009.06303"/>
        <updated>2021-06-08T02:20:25.134Z</updated>
        <summary type="html"><![CDATA[We present a class of methods for robust, personalized federated learning,
called Fed+, that unifies many federated learning algorithms. The principal
advantage of this class of methods is to better accommodate the real-world
characteristics found in federated training, such as the lack of IID data
across parties, the need for robustness to outliers or stragglers, and the
requirement to perform well on party-specific datasets. We achieve this through
a problem formulation that allows the central server to employ robust ways of
aggregating the local models while keeping the structure of local computation
intact. Without making any statistical assumption on the degree of
heterogeneity of local data across parties, we provide convergence guarantees
for Fed+ for convex and non-convex loss functions and robust aggregation. The
Fed+ theory is also equipped to handle heterogeneous computing environments
including stragglers without additional assumptions; specifically, the
convergence results cover the general setting where the number of local update
steps across parties can vary. We demonstrate the benefits of Fed+ through
extensive experiments across standard benchmark datasets as well as on a
challenging real-world problem in financial portfolio management where the
heterogeneity of party-level data can lead to training failure in standard
federated learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Pengqian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1"&gt;Achintya Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wynter_L/0/1/0/all/0/1"&gt;Laura Wynter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Shiau Hong Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration. (arXiv:2008.02437v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02437</id>
        <link href="http://arxiv.org/abs/2008.02437"/>
        <updated>2021-06-08T02:20:25.133Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop novel perturbation bounds for the high-order
orthogonal iteration (HOOI) [DLDMV00b]. Under mild regularity conditions, we
establish blockwise tensor perturbation bounds for HOOI with guarantees for
both tensor reconstruction in Hilbert-Schmidt norm $\|\widehat{\bcT} - \bcT
\|_{\tHS}$ and mode-$k$ singular subspace estimation in Schatten-$q$ norm $\|
\sin \Theta (\widehat{\U}_k, \U_k) \|_q$ for any $q \geq 1$. We show the upper
bounds of mode-$k$ singular subspace estimation are unilateral and converge
linearly to a quantity characterized by blockwise errors of the perturbation
and signal strength. For the tensor reconstruction error bound, we express the
bound through a simple quantity $\xi$, which depends only on perturbation and
the multilinear rank of the underlying signal. Rate matching deterministic
lower bound for tensor reconstruction, which demonstrates the optimality of
HOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI
with only a single iteration) is also optimal in terms of tensor reconstruction
and can be used to lower the computational cost. The perturbation results are
also extended to the case that only partial modes of $\bcT$ have low-rank
structure. We support our theoretical results by extensive numerical studies.
Finally, we apply the novel perturbation bounds of HOOI on two applications,
tensor denoising and tensor co-clustering, from machine learning and
statistics, which demonstrates the superiority of the new perturbation results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Raskutti_G/0/1/0/all/0/1"&gt;Garvesh Raskutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Ming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing. (arXiv:2106.02809v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02809</id>
        <link href="http://arxiv.org/abs/2106.02809"/>
        <updated>2021-06-08T02:20:25.132Z</updated>
        <summary type="html"><![CDATA[Hazy images reduce the visibility of the image content, and haze will lead to
failure in handling subsequent computer vision tasks. In this paper, we address
the problem of image dehazing by proposing a dehazing network named T-Net,
which consists of a backbone network based on the U-Net architecture and a dual
attention module. And it can achieve multi-scale feature fusion by using skip
connections with a new fusion strategy. Furthermore, by repeatedly unfolding
the plain T-Net, Stack T-Net is proposed to take advantage of the dependence of
deep features across stages via a recursive strategy. In order to reduce
network parameters, the intra-stage recursive computation of ResNet is adopted
in our Stack T-Net. And we take both the stage-wise result and the original
hazy image as input to each T-Net and finally output the prediction of clean
image. Experimental results on both synthetic and real-world images demonstrate
that our plain T-Net and the advanced Stack T-Net perform favorably against the
state-of-the-art dehazing algorithms, and show that our Stack T-Net could
further improve the dehazing effect, demonstrating the effectiveness of the
recursive strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lirong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanshan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Wenhan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oops I Took A Gradient: Scalable Sampling for Discrete Distributions. (arXiv:2102.04509v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04509</id>
        <link href="http://arxiv.org/abs/2102.04509"/>
        <updated>2021-06-08T02:20:25.131Z</updated>
        <summary type="html"><![CDATA[We propose a general and scalable approximate sampling strategy for
probabilistic models with discrete variables. Our approach uses gradients of
the likelihood function with respect to its discrete inputs to propose updates
in a Metropolis-Hastings sampler. We show empirically that this approach
outperforms generic samplers in a number of difficult settings including Ising
models, Potts models, restricted Boltzmann machines, and factorial hidden
Markov models. We also demonstrate the use of our improved sampler for training
deep energy-based models on high dimensional discrete data. This approach
outperforms variational auto-encoders and existing energy-based models.
Finally, we give bounds showing that our approach is near-optimal in the class
of samplers which propose local updates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1"&gt;Will Grathwohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1"&gt;Kevin Swersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1"&gt;Milad Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07868</id>
        <link href="http://arxiv.org/abs/2102.07868"/>
        <updated>2021-06-08T02:20:25.130Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are non-parametric, flexible, models that work well
in many tasks. Combining GPs with deep learning methods via deep kernel
learning (DKL) is especially compelling due to the strong representational
power induced by the network. However, inference in GPs, whether with or
without DKL, can be computationally challenging on large datasets. Here, we
propose GP-Tree, a novel method for multi-class classification with Gaussian
processes and DKL. We develop a tree-based hierarchical model in which each
internal node of the tree fits a GP to the data using the P\'olya-Gamma
augmentation scheme. As a result, our method scales well with both the number
of classes and data size. We demonstrate the effectiveness of our method
against other Gaussian process training baselines, and we show how our general
GP approach achieves improved accuracy on standard incremental few-shot
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1"&gt;Idan Achituve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1"&gt;Aviv Navon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1"&gt;Yochai Yemini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths. (arXiv:2101.09612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09612</id>
        <link href="http://arxiv.org/abs/2101.09612"/>
        <updated>2021-06-08T02:20:25.129Z</updated>
        <summary type="html"><![CDATA[We give a simple proof for the global convergence of gradient descent in
training deep ReLU networks with the standard square loss, and show some of its
improvements over the state-of-the-art. In particular, while prior works
require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$
being the number of training samples), we require a single wide layer of
linear, quadratic or cubic width depending on the type of initialization.
Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof
need not track the evolution of the entire NTK matrix, or more generally, any
quantities related to the changes of activation patterns during training.
Instead, we only need to track the evolution of the output at the last hidden
layer, which can be done much more easily thanks to the Lipschitz property of
ReLU. Some highlights of our setting: (i) all the layers are trained with
standard gradient descent, (ii) the network has standard parameterization as
opposed to the NTK one, and (iii) the network has a single wide layer as
opposed to having all wide hidden layers as in most of NTK-related results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quynh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orbital MCMC. (arXiv:2010.08047v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08047</id>
        <link href="http://arxiv.org/abs/2010.08047"/>
        <updated>2021-06-08T02:20:25.125Z</updated>
        <summary type="html"><![CDATA[Markov Chain Monte Carlo (MCMC) algorithms ubiquitously employ complex
deterministic transformations to generate proposal points that are then
filtered by the Metropolis-Hastings-Green (MHG) test. However, the condition of
the target measure invariance puts restrictions on the design of these
transformations. In this paper, we first derive the acceptance test for the
stochastic Markov kernel considering arbitrary deterministic maps as proposal
generators. When applied to the transformations with orbits of period two
(involutions), the test reduces to the MHG test. Based on the derived test we
propose two practical algorithms: one operates by constructing periodic orbits
from any diffeomorphism, another on contractions of the state space (such as
optimization trajectories). Finally, we perform an empirical study
demonstrating the practical advantages of both kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neklyudov_K/0/1/0/all/0/1"&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Implementation Matters in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v11 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03479</id>
        <link href="http://arxiv.org/abs/2102.03479"/>
        <updated>2021-06-08T02:20:25.124Z</updated>
        <summary type="html"><![CDATA[Multi-Agent Reinforcement Learning (MARL) has seen revolutionary
breakthroughs with its successful application to multi-agent cooperative tasks
such as computer games and robot swarms. QMIX, a widely popular MARL algorithm,
has been used to solve cooperative tasks, e.g. Starcraft Multi-Agent Challenge
(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX
target relaxing the monotonicity constraint of QMIX, allowing for performance
improvement in SMAC. However, in this paper, we investigate the code-level
optimizations of these variants and the monotonicity constraint. We find that
(1) such improvements of the variants are significantly affected by various
code-level optimizations; (2) QMIX with normalized optimizations outperforms
other previous works in SMAC; (3) the monotonicity constraint may improve
sample efficiency in SMAC and DEPP. Last, a discussion with theoretical
analysis is demonstrated about why QMIX works well in SMAC. We open-source the
code at \url{https://github.com/hijkzzz/pymarl2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Siyang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1"&gt;Seth Austin Harding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shih-wei Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Draw: Emergent Communication through Sketching. (arXiv:2106.02067v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.02067</id>
        <link href="http://arxiv.org/abs/2106.02067"/>
        <updated>2021-06-08T02:20:25.103Z</updated>
        <summary type="html"><![CDATA[Evidence that visual communication preceded written language and provided a
basis for it goes back to prehistory, in forms such as cave and rock paintings
depicting traces of our distant ancestors. Emergent communication research has
sought to explore how agents can learn to communicate in order to
collaboratively solve tasks. Existing research has focused on language, with a
learned communication channel transmitting sequences of discrete tokens between
the agents. In this work, we explore a visual communication channel between
agents that are allowed to draw with simple strokes. Our agents are
parameterised by deep neural networks, and the drawing procedure is
differentiable, allowing for end-to-end training. In the framework of a
referential communication game, we demonstrate that agents can not only
successfully learn to communicate by drawing, but with appropriate inductive
biases, can do so in a fashion that humans can interpret. We hope to encourage
future research to consider visual communication as a more flexible and
directly interpretable alternative of training collaborative agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Theory of Reinforcement Learning with Once-per-Episode Feedback. (arXiv:2105.14363v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14363</id>
        <link href="http://arxiv.org/abs/2105.14363"/>
        <updated>2021-06-08T02:20:25.097Z</updated>
        <summary type="html"><![CDATA[We study a theory of reinforcement learning (RL) in which the learner
receives binary feedback only once at the end of an episode. While this is an
extreme test case for theory, it is also arguably more representative of
real-world applications than the traditional requirement in RL practice that
the learner receive feedback at every time step. Indeed, in many real-world
applications of reinforcement learning, such as self-driving cars and robotics,
it is easier to evaluate whether a learner's complete trajectory was either
"good" or "bad," but harder to provide a reward signal at each step. To show
that learning is possible in this more challenging setting, we study the case
where trajectory labels are generated by an unknown parametric model, and
provide a statistically and computationally efficient algorithm that achieves
sub-linear regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Positions in CountSketch. (arXiv:2007.09890v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09890</id>
        <link href="http://arxiv.org/abs/2007.09890"/>
        <updated>2021-06-08T02:20:25.089Z</updated>
        <summary type="html"><![CDATA[We consider sketching algorithms which first quickly compress data by
multiplication with a random sketch matrix, and then apply the sketch to
quickly solve an optimization problem, e.g., low rank approximation. In the
learning-based sketching paradigm proposed by Indyk et al. [2019], the sketch
matrix is found by choosing a random sparse matrix, e.g., the CountSketch, and
then updating the values of the non-zero entries by running gradient descent on
a training data set. Despite the growing body of work on this paradigm, a
noticeable omission is that the locations of the non-zero entries of previous
algorithms were fixed, and only their values were learned. In this work we
propose the first learning algorithm that also optimizes the locations of the
non-zero entries. We show this algorithm gives better accuracy for low rank
approximation than previous work, and apply it to other problems such as
$k$-means clustering for the first time. We show that our algorithm is provably
better in the spiked covariance model and for Zipfian matrices. We also show
the importance of the sketch monotonicity property for combining learned
sketches. Our empirical results show the importance of optimizing not only the
values of the non-zero entries but also their positions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Simin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianrui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1"&gt;Ali Vakilian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yulin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04522</id>
        <link href="http://arxiv.org/abs/2105.04522"/>
        <updated>2021-06-08T02:20:25.078Z</updated>
        <summary type="html"><![CDATA[Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (WebVision) noise with varying noise rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1"&gt;Erik Englesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Biasing of Language Models for Speech Recognition in Goal-Oriented Conversational Agents. (arXiv:2103.10325v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10325</id>
        <link href="http://arxiv.org/abs/2103.10325"/>
        <updated>2021-06-08T02:20:25.064Z</updated>
        <summary type="html"><![CDATA[Goal-oriented conversational interfaces are designed to accomplish specific
tasks and typically have interactions that tend to span multiple turns adhering
to a pre-defined structure and a goal. However, conventional neural language
models (NLM) in Automatic Speech Recognition (ASR) systems are mostly trained
sentence-wise with limited context. In this paper, we explore different ways to
incorporate context into a LSTM based NLM in order to model long range
dependencies and improve speech recognition. Specifically, we use context carry
over across multiple turns and use lexical contextual cues such as system
dialog act from Natural Language Understanding (NLU) models and the user
provided structure of the chatbot. We also propose a new architecture that
utilizes context embeddings derived from BERT on sample utterances provided
during inference time. Our experiments show a word error rate (WER) relative
reduction of 7% over non-contextual utterance-level NLM rescorers on
goal-oriented audio datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS. (arXiv:2103.15060v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15060</id>
        <link href="http://arxiv.org/abs/2103.15060"/>
        <updated>2021-06-08T02:20:25.058Z</updated>
        <summary type="html"><![CDATA[This paper introduces PnG BERT, a new encoder model for neural TTS. This
model is augmented from the original BERT model, by taking both phoneme and
grapheme representations of text as input, as well as the word-level alignment
between them. It can be pre-trained on a large text corpus in a self-supervised
manner, and fine-tuned in a TTS task. Experimental results show that a neural
TTS model using a pre-trained PnG BERT as its encoder yields more natural
prosody and more accurate pronunciation than a baseline model using only
phoneme input with no pre-training. Subjective side-by-side preference
evaluations show that raters have no statistically significant preference
between the speech synthesized using a PnG BERT and ground truth recordings
from professional speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1"&gt;Heiga Zen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jonathan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yonghui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable UAV Collision Avoidance using Deep Reinforcement Learning. (arXiv:2105.12254v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12254</id>
        <link href="http://arxiv.org/abs/2105.12254"/>
        <updated>2021-06-08T02:20:25.039Z</updated>
        <summary type="html"><![CDATA[The significant components of any successful autonomous flight system are
task completion and collision avoidance. Most deep learning algorithms
successfully execute these aspects under the environment and conditions they
are trained. However, they fail when subjected to novel environments. This
paper presents an autonomous multi-rotor flight algorithm, using Deep
Reinforcement Learning augmented with Self-Attention Models, that can
effectively reason when subjected to varying inputs. In addition to their
reasoning ability, they are also interpretable, enabling it to be used under
real-world conditions. We have tested our algorithm under different weather
conditions and environments and found it robust compared to conventional Deep
Reinforcement Learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1"&gt;Deepak-George Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshanskyi_D/0/1/0/all/0/1"&gt;Daniil Olshanskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_K/0/1/0/all/0/1"&gt;Karter Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1"&gt;Tichakorn Wongpiromsarn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1"&gt;Ali Jannesari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15203</id>
        <link href="http://arxiv.org/abs/2105.15203"/>
        <updated>2021-06-08T02:20:25.030Z</updated>
        <summary type="html"><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigating to the Best Policy in Markov Decision Processes. (arXiv:2106.02847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02847</id>
        <link href="http://arxiv.org/abs/2106.02847"/>
        <updated>2021-06-08T02:20:25.022Z</updated>
        <summary type="html"><![CDATA[We investigate the classical active pure exploration problem in Markov
Decision Processes, where the agent sequentially selects actions and, from the
resulting system trajectory, aims at identifying the best policy as fast as
possible. We propose an information-theoretic lower bound on the average number
of steps required before a correct answer can be given with probability at
least $1-\delta$. This lower bound involves a non-convex optimization problem,
for which we propose a convex relaxation. We further provide an algorithm whose
sample complexity matches the relaxed lower bound up to a factor $2$. This
algorithm addresses general communicating MDPs; we propose a variant with
reduced exploration rate (and hence faster convergence) under an additional
ergodicity assumption. This work extends previous results relative to the
\emph{generative setting}~\cite{marjani2020adaptive}, where the agent could at
each step observe the random outcome of any (state, action) pair. In contrast,
we show here how to deal with the \emph{navigation constraints}. Our analysis
relies on an ergodic theorem for non-homogeneous Markov chains which we
consider of wide interest in the analysis of Markov Decision Processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Marjani_A/0/1/0/all/0/1"&gt;Aymen Al Marjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garivier_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Garivier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Proutiere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision. (arXiv:2105.06464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06464</id>
        <link href="http://arxiv.org/abs/2105.06464"/>
        <updated>2021-06-08T02:20:25.014Z</updated>
        <summary type="html"><![CDATA[We introduce DiscoBox, a novel framework that jointly learns instance
segmentation and semantic correspondence using bounding box supervision.
Specifically, we propose a self-ensembling framework where instance
segmentation and semantic correspondence are jointly guided by a structured
teacher in addition to the bounding box supervision. The teacher is a
structured energy model incorporating a pairwise potential and a cross-image
potential to model the pairwise pixel relationships both within and across the
boxes. Minimizing the teacher energy simultaneously yields refined object masks
and dense correspondences between intra-class objects, which are taken as
pseudo-labels to supervise the task network and provide positive/negative
correspondence pairs for dense constrastive learning. We show a symbiotic
relationship where the two tasks mutually benefit from each other. Our best
model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly
supervised methods and is competitive to supervised methods. We also obtain
state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with
real-time inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1"&gt;Shiyi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1"&gt;Christopher Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1"&gt;Subhashree Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity. (arXiv:2002.00558v4 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00558</id>
        <link href="http://arxiv.org/abs/2002.00558"/>
        <updated>2021-06-08T02:20:25.006Z</updated>
        <summary type="html"><![CDATA[We consider incentivized exploration: a version of multi-armed bandits where
the choice of arms is controlled by self-interested agents, and the algorithm
can only issue recommendations. The algorithm controls the flow of information,
and the information asymmetry can incentivize the agents to explore. Prior work
achieves optimal regret rates up to multiplicative factors that become
arbitrarily large depending on the Bayesian priors, and scale exponentially in
the number of arms. A more basic problem of sampling each arm once runs into
similar factors.

We focus on the price of incentives: the loss in performance, broadly
construed, incurred for the sake of incentive-compatibility. We prove that
Thompson Sampling, a standard bandit algorithm, is incentive-compatible if
initialized with sufficiently many data points. The performance loss due to
incentives is therefore limited to the initial rounds when these data points
are collected. The problem is largely reduced to that of sample complexity: how
many rounds are needed? We address this question, providing matching upper and
lower bounds and instantiating them in various corollaries. Typically, the
optimal sample complexity is polynomial in the number of arms and exponential
in the "strength of beliefs".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Stochastic Simulation with Interactive Neural Processes. (arXiv:2106.02770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02770</id>
        <link href="http://arxiv.org/abs/2106.02770"/>
        <updated>2021-06-08T02:20:24.988Z</updated>
        <summary type="html"><![CDATA[Stochastic simulations such as large-scale, spatiotemporal, age-structured
epidemic models are computationally expensive at fine-grained resolution. We
propose Interactive Neural Process (INP), an interactive framework to
continuously learn a deep learning surrogate model and accelerate simulation.
Our framework is based on the novel integration of Bayesian active learning,
stochastic simulation and deep sequence modeling. In particular, we develop a
novel spatiotemporal neural process model to mimic the underlying process
dynamics. Our model automatically infers the latent process which describes the
intrinsic uncertainty of the simulator. This also gives rise to a new
acquisition function that can quantify the uncertainty of deep learning
predictions. We design Bayesian active learning algorithms to iteratively query
the simulator, gather more data, and continuously improve the model. We perform
theoretical analysis and demonstrate that our approach reduces sample
complexity compared with random sampling in high dimension. Empirically, we
demonstrate our framework can faithfully imitate the behavior of a complex
infectious disease simulator with a small number of examples, enabling rapid
simulation and scenario exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1"&gt;Matteo Chinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1"&gt;Alessandro Vespignani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi-An Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained episodic reinforcement learning in concave-convex and knapsack settings. (arXiv:2006.05051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05051</id>
        <link href="http://arxiv.org/abs/2006.05051"/>
        <updated>2021-06-08T02:20:24.977Z</updated>
        <summary type="html"><![CDATA[We propose an algorithm for tabular episodic reinforcement learning with
constraints. We provide a modular analysis with strong theoretical guarantees
for settings with concave rewards and convex constraints, and for settings with
hard constraints (knapsacks). Most of the previous work in constrained
reinforcement learning is limited to linear constraints, and the remaining work
focuses on either the feasibility question or settings with a single episode.
Our experiments demonstrate that the proposed algorithm significantly
outperforms these approaches in existing constrained episodic environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1"&gt;Kiant&amp;#xe9; Brantley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miroslav Dudik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1"&gt;Thodoris Lykouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11646</id>
        <link href="http://arxiv.org/abs/2008.11646"/>
        <updated>2021-06-08T02:20:24.967Z</updated>
        <summary type="html"><![CDATA[Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yaoqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bolun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighting Adversarial Neural Network for Domain Adaptation in Regression. (arXiv:2006.08251v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08251</id>
        <link href="http://arxiv.org/abs/2006.08251"/>
        <updated>2021-06-08T02:20:24.958Z</updated>
        <summary type="html"><![CDATA[We present a novel instance-based approach to handle regression tasks in the
context of supervised domain adaptation. The approach developed in this paper
relies on the assumption that the task on the target domain can be efficiently
learned by adequately reweighting the source instances during training phase.
We introduce a novel formulation of the optimization objective for domain
adaptation which relies on a discrepancy distance characterizing the difference
between domains according to a specific task and a class of hypotheses. To
solve this problem, we develop an adversarial network algorithm which learns
both the source weighting scheme and the task in one feed-forward gradient
descent. We provide numerical evidence of the relevance of the method on public
datasets for domain adaptation through reproducible experiments accessible via
an online demo interface at: https://antoinedemathelin.github.io/demo/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1"&gt;Guillaume Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Francois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgroup Fairness in Two-Sided Markets. (arXiv:2106.02702v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02702</id>
        <link href="http://arxiv.org/abs/2106.02702"/>
        <updated>2021-06-08T02:20:24.947Z</updated>
        <summary type="html"><![CDATA[It is well known that two-sided markets are unfair in a number of ways. For
instance, female workers at Uber earn less than their male colleagues per mile
driven. Similar observations have been made for other minority subgroups in
other two-sided markets. Here, we suggest a novel market-clearing mechanism for
two-sided markets, which promotes equalisation of the pay per hour worked
across multiple subgroups, as well as within each subgroup. In the process, we
introduce a novel notion of subgroup fairness (which we call Inter-fairness),
which can be combined with other notions of fairness within each subgroup
(called Intra-fairness), and the utility for the customers (Customer-Care) in
the objective of the market-clearing problem. While the novel non-linear terms
in the objective complicate market clearing by making the problem non-convex,
we show that a certain non-convex augmented Lagrangian relaxation can be
approximated to any precision in time polynomial in the number of market
participants using semi-definite programming. This makes it possible to
implement the market-clearing mechanism efficiently. On the example of
driver-ride assignment in an Uber-like system, we demonstrate the efficacy and
scalability of the approach, and trade-offs between Inter- and Intra-fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shorten_R/0/1/0/all/0/1"&gt;Robert N. Shorten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05558</id>
        <link href="http://arxiv.org/abs/2008.05558"/>
        <updated>2021-06-08T02:20:24.927Z</updated>
        <summary type="html"><![CDATA[We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic Gradient Descent and Thompson Sampling. (arXiv:2006.04012v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04012</id>
        <link href="http://arxiv.org/abs/2006.04012"/>
        <updated>2021-06-08T02:20:24.912Z</updated>
        <summary type="html"><![CDATA[We consider the contextual bandit problem, where a player sequentially makes
decisions based on past observations to maximize the cumulative reward.
Although many algorithms have been proposed for contextual bandit, most of them
rely on finding the maximum likelihood estimator at each iteration, which
requires $O(t)$ time at the $t$-th iteration and are memory inefficient. A
natural way to resolve this problem is to apply online stochastic gradient
descent (SGD) so that the per-step time and memory complexity can be reduced to
constant with respect to $t$, but a contextual bandit policy based on online
SGD updates that balances exploration and exploitation has remained elusive. In
this work, we show that online SGD can be applied to the generalized linear
bandit problem. The proposed SGD-TS algorithm, which uses a single-step SGD
update to exploit past information and uses Thompson Sampling for exploration,
achieves $\tilde{O}(\sqrt{T})$ regret with the total time complexity that
scales linearly in $T$ and $d$, where $T$ is the total number of rounds and $d$
is the number of features. Experimental results show that SGD-TS consistently
outperforms existing algorithms on both synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dodrio: Exploring Transformer Models with Interactive Visualization. (arXiv:2103.14625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14625</id>
        <link href="http://arxiv.org/abs/2103.14625"/>
        <updated>2021-06-08T02:20:24.906Z</updated>
        <summary type="html"><![CDATA[Why do large pre-trained transformer-based models perform so well across a
wide variety of NLP tasks? Recent research suggests the key may lie in
multi-headed attention mechanism's ability to learn and represent linguistic
information. Understanding how these models represent both syntactic and
semantic knowledge is vital to investigate why they succeed and fail, what they
have learned, and how they can improve. We present Dodrio, an open-source
interactive visualization tool to help NLP researchers and practitioners
analyze attention mechanisms in transformer-based models with linguistic
knowledge. Dodrio tightly integrates an overview that summarizes the roles of
different attention heads, and detailed views that help users compare attention
weights with the syntactic structure and semantic information in the input
text. To facilitate the visual comparison of attention weights and linguistic
knowledge, Dodrio applies different graph visualization techniques to represent
attention weights scalable to longer input text. Case studies highlight how
Dodrio provides insights into understanding the attention mechanism in
transformer-based models. Dodrio is available at
https://poloclub.github.io/dodrio/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turko_R/0/1/0/all/0/1"&gt;Robert Turko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Adversarial Attacks. (arXiv:2103.02014v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02014</id>
        <link href="http://arxiv.org/abs/2103.02014"/>
        <updated>2021-06-08T02:20:24.905Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important vulnerabilities of deep learning models,
yet little attention has been paid to settings where data arrives as a stream.
In this paper, we formalize the online adversarial attack problem, emphasizing
two key elements found in real-world use-cases: attackers must operate under
partial knowledge of the target model, and the decisions made by the attacker
are irrevocable since they operate on a transient data stream. We first
rigorously analyze a deterministic variant of the online threat model by
drawing parallels to the well-studied $k$-secretary problem in theoretical
computer science and propose Virtual+, a simple yet practical online algorithm.
Our main theoretical result show Virtual+ yields provably the best competitive
ratio over all single-threshold algorithms for $k<5$ -- extending previous
analysis of the $k$-secretary problem. We also introduce the \textit{stochastic
$k$-secretary} -- effectively reducing online blackbox transfer attacks to a
$k$-secretary problem under noise -- and prove theoretical bounds on the
performance of \textit{any} online algorithms adapted to this setting. Finally,
we complement our theoretical results by conducting experiments on both MNIST
and CIFAR-10 with both vanilla and robust classifiers, revealing not only the
necessity of online algorithms in achieving near-optimal performance but also
the rich interplay of a given attack strategy towards online attack selection,
enabling simple strategies like FGSM to outperform classically strong whitebox
adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mladenovic_A/0/1/0/all/0/1"&gt;Andjela Mladenovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1"&gt;Avishek Joey Bose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1"&gt;Hugo Berard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1"&gt;Pascal Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2004.09764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-08T02:20:24.903Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Causal Inference Using Control Variates. (arXiv:2103.16689v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16689</id>
        <link href="http://arxiv.org/abs/2103.16689"/>
        <updated>2021-06-08T02:20:24.903Z</updated>
        <summary type="html"><![CDATA[While many areas of machine learning have benefited from the increasing
availability of large and varied datasets, the benefit to causal inference has
been limited given the strong assumptions needed to ensure identifiability of
causal effects; these are often not satisfied in real-world datasets. For
example, many large observational datasets (e.g., case-control studies in
epidemiology, click-through data in recommender systems) suffer from selection
bias on the outcome, which makes the average treatment effect (ATE)
unidentifiable. We propose a general algorithm to estimate causal effects from
\emph{multiple} data sources, where the ATE may be identifiable only in some
datasets but not others. The key idea is to construct control variates using
the datasets in which the ATE is not identifiable. We show theoretically that
this reduces the variance of the ATE estimate. We apply this framework to
inference from observational data under outcome selection bias, assuming access
to an auxiliary small dataset from which we can obtain a consistent estimate of
the ATE. We construct a control variate by taking the difference of the odds
ratio estimates from the two datasets. Across simulations and two case studies
with real data, we show that this control variate can significantly reduce the
variance of the ATE estimate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Serena Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Peng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mirror Descent Policy Optimization. (arXiv:2005.09814v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09814</id>
        <link href="http://arxiv.org/abs/2005.09814"/>
        <updated>2021-06-08T02:20:24.895Z</updated>
        <summary type="html"><![CDATA[Mirror descent (MD), a well-known first-order method in constrained convex
optimization, has recently been shown as an important tool to analyze
trust-region algorithms in reinforcement learning (RL). However, there remains
a considerable gap between such theoretically analyzed algorithms and the ones
used in practice. Inspired by this, we propose an efficient RL algorithm,
called {\em mirror descent policy optimization} (MDPO). MDPO iteratively
updates the policy by {\em approximately} solving a trust-region problem, whose
objective function consists of two terms: a linearization of the standard RL
objective and a proximity term that restricts two consecutive policies to be
close to each other. Each update performs this approximation by taking multiple
gradient steps on this objective function. We derive {\em on-policy} and {\em
off-policy} variants of MDPO, while emphasizing important design choices
motivated by the existing theory of MD in RL. We highlight the connections
between on-policy MDPO and two popular trust-region RL algorithms: TRPO and
PPO, and show that explicitly enforcing the trust-region constraint is in fact
{\em not} a necessity for high performance gains in TRPO. We then show how the
popular soft actor-critic (SAC) algorithm can be derived by slight
modifications of off-policy MDPO. Overall, MDPO is derived from the MD
principles, offers a unified approach to viewing a number of popular RL
algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a
number of continuous control tasks. Code is available at
\url{https://github.com/manantomar/Mirror-Descent-Policy-Optimization}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1"&gt;Manan Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shani_L/0/1/0/all/0/1"&gt;Lior Shani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1"&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Random Feature Model for Input-Output Maps between Banach Spaces. (arXiv:2005.10224v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10224</id>
        <link href="http://arxiv.org/abs/2005.10224"/>
        <updated>2021-06-08T02:20:24.886Z</updated>
        <summary type="html"><![CDATA[Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nelsen_N/0/1/0/all/0/1"&gt;Nicholas H. Nelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization Induced Equilibrium Networks. (arXiv:2105.13228v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13228</id>
        <link href="http://arxiv.org/abs/2105.13228"/>
        <updated>2021-06-08T02:20:24.875Z</updated>
        <summary type="html"><![CDATA[Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by
implicit equations, have been becoming more and more attractive recently. In
this paper, we investigate an emerging question: can an implicit equilibrium
model's equilibrium point be regarded as the solution of an optimization
problem? To this end, we first decompose DNNs into a new class of unit layer
that is the proximal operator of an implicit convex function while keeping its
output unchanged. Then, the equilibrium model of the unit layer can be derived,
named Optimization Induced Equilibrium Networks (OptEq), which can be easily
extended to deep layers. The equilibrium point of OptEq can be theoretically
connected to the solution of its corresponding convex optimization problem with
explicit objectives. Based on this, we can flexibly introduce prior properties
to the equilibrium points: 1) modifying the underlying convex problems
explicitly so as to change the architectures of OptEq; and 2) merging the
information into the fixed point iteration, which guarantees to choose the
desired equilibrium point when the fixed point set is non-singleton. We show
that deep OptEq outperforms previous implicit models even with fewer
parameters. This work establishes the first step towards the
optimization-guided design of deep models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xingyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiuhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zenan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangcan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06429</id>
        <link href="http://arxiv.org/abs/2009.06429"/>
        <updated>2021-06-08T02:20:24.869Z</updated>
        <summary type="html"><![CDATA[Neural-network classifiers achieve high accuracy when predicting the class of
an input that they were trained to identify. Maintaining this accuracy in
dynamic environments, where inputs frequently fall outside the fixed set of
initially known classes, remains a challenge. The typical approach is to detect
inputs from novel classes and retrain the classifier on an augmented dataset.
However, not only the classifier but also the detection mechanism needs to
adapt in order to distinguish between newly learned and yet unknown input
classes. To address this challenge, we introduce an algorithmic framework for
active monitoring of a neural network. A monitor wrapped in our framework
operates in parallel with the neural network and interacts with a human user
via a series of interpretable labeling queries for incremental adaptation. In
addition, we propose an adaptive quantitative monitor to improve precision. An
experimental evaluation on a diverse set of benchmarks with varying numbers of
classes confirms the benefits of our active monitoring framework in dynamic
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1"&gt;Anna Lukina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1"&gt;Christian Schilling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1"&gt;Thomas A. Henzinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition. (arXiv:2005.00777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00777</id>
        <link href="http://arxiv.org/abs/2005.00777"/>
        <updated>2021-06-08T02:20:24.848Z</updated>
        <summary type="html"><![CDATA[Recognition accuracy and response time are both critically essential ahead of
building practical electroencephalography (EEG) based brain-computer interface
(BCI). Recent approaches, however, have either compromised in the
classification accuracy or responding time. This paper presents a novel deep
learning approach designed towards remarkably accurate and responsive motor
imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term
Memory (BiLSTM) with the Attention mechanism manages to derive relevant
features from raw EEG signals. The connected graph convolutional neural network
(GCN) promotes the decoding performance by cooperating with the topological
structure of features, which are estimated from the overall data. The
0.4-second detection framework has shown effective and efficient prediction
based on individual and group-wise training, with 98.81% and 94.64% accuracy,
respectively, which outperformed all the state-of-the-art studies. The
introduced deep feature mining approach can precisely recognize human motion
intents from raw EEG signals, which paves the road to translate the EEG based
MI recognition to practical BCI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. (arXiv:2002.10085v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10085</id>
        <link href="http://arxiv.org/abs/2002.10085"/>
        <updated>2021-06-08T02:20:24.819Z</updated>
        <summary type="html"><![CDATA[Spiking neural networks (SNNs) are well suited for spatio-temporal learning
and implementations on energy-efficient event-driven neuromorphic processors.
However, existing SNN error backpropagation (BP) methods lack proper handling
of spiking discontinuities and suffer from low performance compared with the BP
methods for traditional artificial neural networks. In addition, a large number
of time steps are typically required to achieve decent performance, leading to
high latency and rendering spike-based computation unscalable to deep
architectures. We present a novel Temporal Spike Sequence Learning
Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down
error backpropagation across two types of inter-neuron and intra-neuron
dependencies and leads to improved temporal learning precision. It captures
inter-neuron dependencies through presynaptic firing times by considering the
all-or-none characteristics of firing activities and captures intra-neuron
dependencies by handling the internal evolution of each neuronal state in time.
TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of
a few steps while improving the accuracy for various image classification
datasets including CIFAR10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Auxiliary Information in Self-supervised Learning. (arXiv:2106.02869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02869</id>
        <link href="http://arxiv.org/abs/2106.02869"/>
        <updated>2021-06-08T02:20:24.807Z</updated>
        <summary type="html"><![CDATA[This paper presents to integrate the auxiliary information (e.g., additional
attributes for data such as the hashtags for Instagram images) in the
self-supervised learning process. We first observe that the auxiliary
information may bring us useful information about data structures: for
instance, the Instagram images with the same hashtags can be semantically
similar. Hence, to leverage the structural information from the auxiliary
information, we present to construct data clusters according to the auxiliary
information. Then, we introduce the Clustering InfoNCE (Cl-InfoNCE) objective
that learns similar representations for augmented variants of data from the
same cluster and dissimilar representations for data from different clusters.
Our approach contributes as follows: 1) Comparing to conventional
self-supervised representations, the auxiliary-information-infused
self-supervised representations bring the performance closer to the supervised
representations; 2) The presented Cl-InfoNCE can also work with unsupervised
constructed clusters (e.g., k-means clusters) and outperform strong
clustering-based self-supervised learning approaches, such as the Prototypical
Contrastive Learning (PCL) method; 3) We show that Cl-InfoNCE may be a better
approach to leverage the data clustering information, by comparing it to the
baseline approach - learning to predict the clustering assignments with
cross-entropy loss. For analysis, we connect the goodness of the learned
representations with the statistical relationships: i) the mutual information
between the labels and the clusters and ii) the conditional entropy of the
clusters given the labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1"&gt;Peiyuan Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Bounds between $f$-Divergences and Integral Probability Metrics. (arXiv:2006.05973v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05973</id>
        <link href="http://arxiv.org/abs/2006.05973"/>
        <updated>2021-06-08T02:20:24.801Z</updated>
        <summary type="html"><![CDATA[The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and
Integral Probability Metrics (e.g. total variation distance or maximum mean
discrepancies) are widely used to quantify the similarity between probability
distributions. In this work, we systematically study the relationship between
these two families from the perspective of convex duality. Starting from a
tight variational representation of the $f$-divergence, we derive a
generalization of the moment-generating function, which we show exactly
characterizes the best lower bound of the $f$-divergence as a function of a
given IPM. Using this characterization, we obtain new bounds while also
recovering in a unified manner well-known results, such as Hoeffding's lemma,
Pinsker's inequality and its extension to subgaussian functions, and the
Hammersley-Chapman-Robbins bound. This characterization also allows us to prove
new results on topological properties of the divergence which may be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Agrawal_R/0/1/0/all/0/1"&gt;Rohit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Horel_T/0/1/0/all/0/1"&gt;Thibaut Horel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Curves for SGD on Structured Features. (arXiv:2106.02713v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02713</id>
        <link href="http://arxiv.org/abs/2106.02713"/>
        <updated>2021-06-08T02:20:24.795Z</updated>
        <summary type="html"><![CDATA[The generalization performance of a machine learning algorithm such as a
neural network depends in a non-trivial way on the structure of the data
distribution. Models of generalization in machine learning theory often ignore
the low-dimensional structure of natural signals, either by considering
data-agnostic bounds or by studying the performance of the algorithm when
trained on uncorrelated features. To analyze the influence of data structure on
test loss dynamics, we study an exactly solveable model of stochastic gradient
descent (SGD) which predicts test loss when training on features with arbitrary
covariance structure. We solve the theory exactly for both Gaussian features
and arbitrary features and we show that the simpler Gaussian model accurately
predicts test loss of nonlinear random-feature models and deep neural networks
trained with SGD on real datasets such as MNIST and CIFAR-10. We show that
modeling the geometry of the data in the induced feature space is indeed
crucial to accurately predict the test error throughout learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1"&gt;Blake Bordelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Over-Parameterized Training. (arXiv:2004.04690v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04690</id>
        <link href="http://arxiv.org/abs/2004.04690"/>
        <updated>2021-06-08T02:20:24.778Z</updated>
        <summary type="html"><![CDATA[The inductive bias of a neural network is largely determined by the
architecture and the training algorithm. To achieve good generalization, how to
effectively train a neural network is of great importance. We propose a novel
orthogonal over-parameterized training (OPT) framework that can provably
minimize the hyperspherical energy which characterizes the diversity of neurons
on a hypersphere. By maintaining the minimum hyperspherical energy during
training, OPT can greatly improve the empirical generalization. Specifically,
OPT fixes the randomly initialized weights of the neurons and learns an
orthogonal transformation that applies to these neurons. We consider multiple
ways to learn such an orthogonal transformation, including unrolling
orthogonalization algorithms, applying orthogonal parameterization, and
designing orthogonality-preserving gradient descent. For better scalability, we
propose the stochastic OPT which performs orthogonal transformation
stochastically for partial dimensions of neurons. Interestingly, OPT reveals
that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better
generalization. Extensive experiments validate the superiority of OPT over the
standard training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Power of Invariant and Equivariant Graph Neural Networks. (arXiv:2006.15646v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15646</id>
        <link href="http://arxiv.org/abs/2006.15646"/>
        <updated>2021-06-08T02:20:24.771Z</updated>
        <summary type="html"><![CDATA[Various classes of Graph Neural Networks (GNN) have been proposed and shown
to be successful in a wide range of applications with graph structured data. In
this paper, we propose a theoretical framework able to compare the expressive
power of these GNN architectures. The current universality theorems only apply
to intractable classes of GNNs. Here, we prove the first approximation
guarantees for practical GNNs, paving the way for a better understanding of
their generalization. Our theoretical results are proved for invariant GNNs
computing a graph embedding (permutation of the nodes of the input graph does
not affect the output) and equivariant GNNs computing an embedding of the nodes
(permutation of the input permutes the output). We show that Folklore Graph
Neural Networks (FGNN), which are tensor based GNNs augmented with matrix
multiplication are the most expressive architectures proposed so far for a
given tensor order. We illustrate our results on the Quadratic Assignment
Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to
learn how to solve the problem, leading to much better average performances
than existing algorithms (based on spectral, SDP or other GNNs architectures).
On a practical side, we also implement masked tensors to handle batches of
graphs of varying sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizian_W/0/1/0/all/0/1"&gt;Wa&amp;#xef;ss Azizian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UFO-BLO: Unbiased First-Order Bilevel Optimization. (arXiv:2006.03631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03631</id>
        <link href="http://arxiv.org/abs/2006.03631"/>
        <updated>2021-06-08T02:20:24.763Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization (BLO) is a popular approach with many applications
including hyperparameter optimization, neural architecture search, adversarial
robustness and model-agnostic meta-learning. However, the approach suffers from
time and memory complexity proportional to the length $r$ of its inner
optimization loop, which has led to several modifications being proposed. One
such modification is \textit{first-order} BLO (FO-BLO) which approximates
outer-level gradients by zeroing out second derivative terms, yielding
significant speed gains and requiring only constant memory as $r$ varies.
Despite FO-BLO's popularity, there is a lack of theoretical understanding of
its convergence properties. We make progress by demonstrating a rich family of
examples where FO-BLO-based stochastic optimization does not converge to a
stationary point of the BLO objective. We address this concern by proposing a
new FO-BLO-based unbiased estimate of outer-level gradients, enabling us to
theoretically guarantee this convergence, with no harm to memory and expected
time complexity. Our findings are supported by experimental results on Omniglot
and Mini-ImageNet, popular few-shot meta-learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Churn Reduction via Distillation. (arXiv:2106.02654v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02654</id>
        <link href="http://arxiv.org/abs/2106.02654"/>
        <updated>2021-06-08T02:20:24.754Z</updated>
        <summary type="html"><![CDATA[In real-world systems, models are frequently updated as more data becomes
available, and in addition to achieving high accuracy, the goal is to also
maintain a low difference in predictions compared to the base model (i.e.
predictive ``churn''). If model retraining results in vastly different
behavior, then it could cause negative effects in downstream systems,
especially if this churn can be avoided with limited impact on model accuracy.
In this paper, we show an equivalence between training with distillation using
the base model as the teacher and training with an explicit constraint on the
predictive churn. We then show that distillation performs strongly for low
churn training against a number of recent baselines on a wide range of datasets
and model architectures, including fully-connected networks, convolutional
networks, and transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1"&gt;Andrew Cotter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-rank Characteristic Tensor Density Estimation Part I: Foundations. (arXiv:2008.12315v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12315</id>
        <link href="http://arxiv.org/abs/2008.12315"/>
        <updated>2021-06-08T02:20:24.737Z</updated>
        <summary type="html"><![CDATA[Effective non-parametric density estimation is a key challenge in
high-dimensional multivariate data analysis. In this paper,we propose a novel
approach that builds upon tensor factorization tools. Any multivariate density
can be represented by its characteristic function, via the Fourier transform.
If the sought density is compactly supported, then its characteristic function
can be approximated, within controllable error, by a finite tensor of leading
Fourier coefficients, whose size de-pends on the smoothness of the underlying
density. This tensor can be naturally estimated from observed realizations of
the random vector of interest, via sample averaging. In order to circumvent the
curse of dimensionality, we introduce a low-rank model of this characteristic
tensor, which significantly improves the density estimate especially for
high-dimensional data and/or in the sample-starved regime. By virtue of
uniqueness of low-rank tensor decomposition, under certain conditions, our
method enables learning the true data-generating distribution. We demonstrate
the very promising performance of the proposed method using several measured
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Amiridi_M/0/1/0/all/0/1"&gt;Magda Amiridi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kargas_N/0/1/0/all/0/1"&gt;Nikos Kargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sidiropoulos_N/0/1/0/all/0/1"&gt;Nicholas D. Sidiropoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models. (arXiv:2010.08258v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08258</id>
        <link href="http://arxiv.org/abs/2010.08258"/>
        <updated>2021-06-08T02:20:24.735Z</updated>
        <summary type="html"><![CDATA[The learning and evaluation of energy-based latent variable models (EBLVMs)
without any structural assumptions are highly challenging, because the true
posteriors and the partition functions in such models are generally
intractable. This paper presents variational estimates of the score function
and its gradient with respect to the model parameters in a general EBLVM,
referred to as VaES and VaGES respectively. The variational posterior is
trained to minimize a certain divergence to the true model posterior and the
bias in both estimates can be bounded by the divergence theoretically. With a
minimal model assumption, VaES and VaGES can be applied to the kernelized Stein
discrepancy (KSD) and score matching (SM)-based methods to learn EBLVMs.
Besides, VaES can also be used to estimate the exact Fisher divergence between
the data and general EBLVMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1"&gt;Fan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongxuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphMI: Extracting Private Graph Data from Graph Neural Networks. (arXiv:2106.02820v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02820</id>
        <link href="http://arxiv.org/abs/2106.02820"/>
        <updated>2021-06-08T02:20:24.729Z</updated>
        <summary type="html"><![CDATA[As machine learning becomes more widely used for critical applications, the
need to study its implications in privacy turns to be urgent. Given access to
the target model and auxiliary information, the model inversion attack aims to
infer sensitive features of the training dataset, which leads to great privacy
concerns. Despite its success in grid-like domains, directly applying model
inversion techniques on non-grid domains such as graph achieves poor attack
performance due to the difficulty to fully exploit the intrinsic properties of
graphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge
this gap, we present \textbf{Graph} \textbf{M}odel \textbf{I}nversion attack
(GraphMI), which aims to extract private graph data of the training graph by
inverting GNN, one of the state-of-the-art graph analysis tools. Specifically,
we firstly propose a projected gradient module to tackle the discreteness of
graph edges while preserving the sparsity and smoothness of graph features.
Then we design a graph auto-encoder module to efficiently exploit graph
topology, node attributes, and target model parameters for edge inference. With
the proposed methods, we study the connection between model inversion risk and
edge influence and show that edges with greater influence are more likely to be
recovered. Extensive experiments over several public datasets demonstrate the
effectiveness of our method. We also show that differential privacy in its
canonical form can hardly defend our attack while preserving decent utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zaixi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenya Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chengqiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanren Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modular Analysis of Provable Acceleration via Polyak's Momentum: Training a Wide ReLU Network and a Deep Linear Network. (arXiv:2010.01618v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01618</id>
        <link href="http://arxiv.org/abs/2010.01618"/>
        <updated>2021-06-08T02:20:24.710Z</updated>
        <summary type="html"><![CDATA[Incorporating a so-called "momentum" dynamic in gradient descent methods is
widely used in neural net training as it has been broadly observed that, at
least empirically, it often leads to significantly faster convergence. At the
same time, there are very few theoretical guarantees in the literature to
explain this apparent acceleration effect. Even for the classical strongly
convex quadratic problems, several existing results only show Polyak's momentum
has an accelerated linear rate asymptotically. In this paper, we first revisit
the quadratic problems and show a non-asymptotic accelerated linear rate of
Polyak's momentum. Then, we provably show that Polyak's momentum achieves
acceleration for training a one-layer wide ReLU network and a deep linear
network, which are perhaps the two most popular canonical models for studying
optimization and deep learning in the literature. Prior work Du at al. 2019 and
Wu et al. 2019 showed that using vanilla gradient descent, and with an use of
over-parameterization, the error decays as $(1- \Theta(\frac{1}{ \kappa'}))^t$
after $t$ iterations, where $\kappa'$ is the condition number of a Gram Matrix.
Our result shows that with the appropriate choice of parameters Polyak's
momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa'}}))^t$. For the deep
linear network, prior work Hu et al. 2020 showed that vanilla gradient descent
has a rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition
number of a data matrix. Our result shows an acceleration rate $(1-
\Theta(\frac{1}{\sqrt{\kappa}}))^t$ is achievable by Polyak's momentum. All the
results in this work are obtained from a modular analysis, which can be of
independent interest. This work establishes that momentum does indeed speed up
neural net training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based Neuromorphic Processors. (arXiv:2012.05419v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05419</id>
        <link href="http://arxiv.org/abs/2012.05419"/>
        <updated>2021-06-08T02:20:24.710Z</updated>
        <summary type="html"><![CDATA[A set of highly-optimized custom macro extensions is developed for a 7nm CMOS
cell library for implementing Temporal Neural Networks (TNNs) that can mimic
brain-like sensory processing with extreme energy efficiency. A TNN prototype
(13,750 neurons and 315,000 synapses) for MNIST requires only 1.56mm2 die area
and consumes only 1.69mW.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nair_H/0/1/0/all/0/1"&gt;Harideep Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vellaisamy_P/0/1/0/all/0/1"&gt;Prabhu Vellaisamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhasuthkar_S/0/1/0/all/0/1"&gt;Santha Bhasuthkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;John Paul Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing the Latent Space of Molecular Deep Generative Models with Persistent Homology Metrics. (arXiv:2010.08548v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08548</id>
        <link href="http://arxiv.org/abs/2010.08548"/>
        <updated>2021-06-08T02:20:24.709Z</updated>
        <summary type="html"><![CDATA[Deep generative models are increasingly becoming integral parts of the in
silico molecule design pipeline and have dual goals of learning the chemical
and structural features that render candidate molecules viable while also being
flexible enough to generate novel designs. Specifically, Variational Auto
Encoders (VAEs) are generative models in which encoder-decoder network pairs
are trained to reconstruct training data distributions in such a way that the
latent space of the encoder network is smooth. Therefore, novel candidates can
be found by sampling from this latent space. However, the scope of
architectures and hyperparameters is vast and choosing the best combination for
in silico discovery has important implications for downstream success.
Therefore, it is important to develop a principled methodology for
distinguishing how well a given generative model is able to learn salient
molecular features. In this work, we propose a method for measuring how well
the latent space of deep generative models is able to encode structural and
chemical features of molecular datasets by correlating latent space metrics
with metrics from the field of topological data analysis (TDA). We apply our
evaluation methodology to a VAE trained on SMILES strings and show that 3D
topology information is consistently encoded throughout the latent space of the
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Decompose a Tensor with Group Structure. (arXiv:2106.02680v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02680</id>
        <link href="http://arxiv.org/abs/2106.02680"/>
        <updated>2021-06-08T02:20:24.708Z</updated>
        <summary type="html"><![CDATA[In this work we study the orbit recovery problem, which is a natural
abstraction for the problem of recovering a planted signal from noisy
measurements under unknown group actions. Many important inverse problems in
statistics, engineering and the sciences fit into this framework. Prior work
has studied cases when the group is discrete and/or abelian. However
fundamentally new techniques are needed in order to handle more complex group
actions.

Our main result is a quasi-polynomial time algorithm to solve orbit recovery
over $SO(3)$ - i.e. the cryo-electron tomography problem which asks to recover
the three-dimensional structure of a molecule from noisy measurements of
randomly rotated copies of it. We analyze a variant of the frequency marching
heuristic in the framework of smoothed analysis. Our approach exploits the
layered structure of the invariant polynomials, and simultaneously yields a new
class of tensor decomposition algorithms that work in settings when the tensor
is not low-rank but rather where the factors are algebraically related to each
other by a group action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Contextual Bandit Bake-off. (arXiv:1802.04064v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1802.04064</id>
        <link href="http://arxiv.org/abs/1802.04064"/>
        <updated>2021-06-08T02:20:24.699Z</updated>
        <summary type="html"><![CDATA[Contextual bandit algorithms are essential for solving many real-world
interactive machine learning problems. Despite multiple recent successes on
statistically and computationally efficient methods, the practical behavior of
these algorithms is still poorly understood. We leverage the availability of
large numbers of supervised learning datasets to empirically evaluate
contextual bandit algorithms, focusing on practical methods that learn by
relying on optimization oracles from supervised learning. We find that a recent
method (Foster et al., 2018) using optimism under uncertainty works the best
overall. A surprisingly close second is a simple greedy baseline that only
explores implicitly through the diversity of contexts, followed by a variant of
Online Cover (Agarwal et al., 2014) which tends to be more conservative but
robust to problem specification by design. Along the way, we also evaluate
various components of contextual bandit algorithm design such as loss
estimators. Overall, this is a thorough study and review of contextual bandit
methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models. (arXiv:2106.02679v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02679</id>
        <link href="http://arxiv.org/abs/2106.02679"/>
        <updated>2021-06-08T02:20:24.693Z</updated>
        <summary type="html"><![CDATA[The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.

In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1"&gt;Joel Lamy-Poirier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Neural Posterior and Likelihood Approximation. (arXiv:2102.06522v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06522</id>
        <link href="http://arxiv.org/abs/2102.06522"/>
        <updated>2021-06-08T02:20:24.668Z</updated>
        <summary type="html"><![CDATA[We introduce the sequential neural posterior and likelihood approximation
(SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inference
in implicit models, and therefore is a simulation-based inference method that
only requires simulations from a generative model. SNPLA avoids Markov chain
Monte Carlo sampling and correction-steps of the parameter proposal function
that are introduced in similar methods, but that can be numerically unstable or
restrictive. By utilizing the reverse KL divergence, SNPLA manages to learn
both the likelihood and the posterior in a sequential manner. Over four
experiments, we show that SNPLA performs competitively when utilizing the same
number of model simulations as used in other methods, even though the inference
problem for SNPLA is more complex due to the joint learning of posterior and
likelihood function. Due to utilizing normalizing flows SNPLA generates
posterior draws much faster (4 orders of magnitude) than MCMC-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wiqvist_S/0/1/0/all/0/1"&gt;Samuel Wiqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Picchini_U/0/1/0/all/0/1"&gt;Umberto Picchini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context Disambiguation. (arXiv:2104.10375v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10375</id>
        <link href="http://arxiv.org/abs/2104.10375"/>
        <updated>2021-06-08T02:20:24.658Z</updated>
        <summary type="html"><![CDATA[This paper presents the PALI team's winning system for SemEval-2021 Task 2:
Multilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune
XLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to
determine whether the target word in the two contexts contains the same meaning
or not. In the implementation, we first specifically design an input tag to
emphasize the target word in the contexts. Second, we construct a new vector on
the fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected
network to output the probability of whether the target word in the context has
the same meaning or not. The new vector is attained by concatenating the
embedding of the [CLS] token and the embeddings of the target word in the
contexts. In training, we explore several tricks, such as the Ranger optimizer,
data augmentation, and adversarial training, to improve the model prediction.
Consequently, we attain first place in all four cross-lingual tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Beam Association for High Mobility mmWave Vehicular Networks: Lightweight Parallel Reinforcement Learning Approach. (arXiv:2005.00694v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00694</id>
        <link href="http://arxiv.org/abs/2005.00694"/>
        <updated>2021-06-08T02:20:24.658Z</updated>
        <summary type="html"><![CDATA[In intelligent transportation systems (ITS), vehicles are expected to feature
with advanced applications and services which demand ultra-high data rates and
low-latency communications. For that, the millimeter wave (mmWave)
communication has been emerging as a very promising solution. However,
incorporating the mmWave into ITS is particularly challenging due to the high
mobility of vehicles and the inherent sensitivity of mmWave beams to dynamic
blockages. This article addresses these problems by developing an optimal beam
association framework for mmWave vehicular networks under high mobility.
Specifically, we use the semi-Markov decision process to capture the dynamics
and uncertainty of the environment. The Q-learning algorithm is then often used
to find the optimal policy. However, Q-learning is notorious for its
slow-convergence. Instead of adopting deep reinforcement learning structures
(like most works in the literature), we leverage the fact that there are
usually multiple vehicles on the road to speed up the learning process. To that
end, we develop a lightweight yet very effective parallel Q-learning algorithm
to quickly obtain the optimal policy by simultaneously learning from various
vehicles. Extensive simulations demonstrate that our proposed solution can
increase the data rate by 47% and reduce the disconnection probability by 29%
compared to other solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1"&gt;Nguyen Van Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1"&gt;Eryk Dutkiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Invariant Dictionary Learning. (arXiv:2007.07550v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07550</id>
        <link href="http://arxiv.org/abs/2007.07550"/>
        <updated>2021-06-08T02:20:24.657Z</updated>
        <summary type="html"><![CDATA[The dictionary learning problem concerns the task of representing data as
sparse linear sums drawn from a smaller collection of basic building blocks. In
application domains where such techniques are deployed, we frequently encounter
datasets where some form of symmetry or invariance is present. Motivated by
this observation, we develop a framework for learning dictionaries for data
under the constraint that the collection of basic building blocks remains
invariant under such symmetries. Our procedure for learning such dictionaries
relies on representing the symmetry as the action of a matrix group acting on
the data, and subsequently introducing a convex penalty function so as to
induce sparsity with respect to the collection of matrix group elements. Our
framework specializes to the convolutional dictionary learning problem when we
consider integer shifts. Using properties of positive semidefinite Hermitian
Toeplitz matrices, we develop an extension that learns dictionaries that are
invariant under continuous shifts. Our numerical experiments on synthetic data
and ECG data show that the incorporation of such symmetries as priors are most
valuable when the dataset has few data-points, or when the full range of
symmetries is inadequately expressed in the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Soh_Y/0/1/0/all/0/1"&gt;Yong Sheng Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Learning in the Jungle. (arXiv:2008.00742v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00742</id>
        <link href="http://arxiv.org/abs/2008.00742"/>
        <updated>2021-06-08T02:20:24.643Z</updated>
        <summary type="html"><![CDATA[We study Byzantine collaborative learning, where $n$ nodes seek to
collectively learn from each others' local data. The data distribution may vary
from one node to another. No node is trusted, and $f < n$ nodes can behave
arbitrarily. We prove that collaborative learning is equivalent to a new form
of agreement, which we call averaging agreement. In this problem, nodes start
each with an initial vector and seek to approximately agree on a common vector,
which is close to the average of honest nodes' initial vectors. We present two
asynchronous solutions to averaging agreement, each we prove optimal according
to some dimension. The first, based on the minimum-diameter averaging, requires
$ n \geq 6f+1$, but achieves asymptotically the best-possible averaging
constant up to a multiplicative constant. The second, based on reliable
broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine
resilience, i.e., $n \geq 3f+1$. Each of these algorithms induces an optimal
Byzantine collaborative learning protocol. In particular, our equivalence
yields new impossibility theorems on what any collaborative learning algorithm
can achieve in adversarial and heterogeneous environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1"&gt;El-Mahdi El-Mhamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadkhani_S/0/1/0/all/0/1"&gt;Sadegh Farhadkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guirguis_A/0/1/0/all/0/1"&gt;Arsany Guirguis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea; Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction. (arXiv:1912.01756v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01756</id>
        <link href="http://arxiv.org/abs/1912.01756"/>
        <updated>2021-06-08T02:20:24.637Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel message passing neural (MPN) architecture
Conv-MPN, which reconstructs an outdoor building as a planar graph from a
single RGB image. Conv-MPN is specifically designed for cases where nodes of a
graph have explicit spatial embedding. In our problem, nodes correspond to
building edges in an image. Conv-MPN is different from MPN in that 1) the
feature associated with a node is represented as a feature volume instead of a
1D vector; and 2) convolutions encode messages instead of fully connected
layers. Conv-MPN learns to select a true subset of nodes (i.e., building edges)
to reconstruct a building planar graph. Our qualitative and quantitative
evaluations over 2,000 buildings show that Conv-MPN makes significant
improvements over the existing fully neural solutions. We believe that the
paper has a potential to open a new line of graph neural network research for
structured geometry reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nauata_N/0/1/0/all/0/1"&gt;Nelson Nauata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1"&gt;Yasutaka Furukawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic of Machine Learning. (arXiv:2006.09500v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09500</id>
        <link href="http://arxiv.org/abs/2006.09500"/>
        <updated>2021-06-08T02:20:24.635Z</updated>
        <summary type="html"><![CDATA[ML is approached from logic point of view as a problem of maximizing
consistency of a hypothesis in a context of a given training set. Nonjudgmental
logic (NjL) with modalities ``It appears that'', ``Assume that'' is introduced
to formalize and quantify the concepts of inconsistency. Two conjectures are
formulated. First, there are only 5 types of steps for all learners. Second,
any learner minimizes a criterion, which can be represented as a measure of
inconsistency in a semantic of NjL. Many popular ML algorithms (from
hierarchical clustering to k-NN and SVM) are shown to corroborate both
conjectures. In addition, it is demonstrated that NjL allows to formalize and
solve several general learning problems which are not considered as ML usually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sapir_M/0/1/0/all/0/1"&gt;Marina Sapir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singular Dynamic Mode Decompositions. (arXiv:2106.02639v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02639</id>
        <link href="http://arxiv.org/abs/2106.02639"/>
        <updated>2021-06-08T02:20:24.615Z</updated>
        <summary type="html"><![CDATA[This manuscript is aimed at addressing several long standing limitations of
dynamic mode decompositions in the application of Koopman analysis. Principle
among these limitations are the convergence of associated Dynamic Mode
Decomposition algorithms and the existence of Koopman modes. To address these
limitations, two major modifications are made, where Koopman operators are
removed from the analysis in light of Liouville operators (known as Koopman
generators in special cases), and these operators are shown to be compact for
certain pairs of Hilbert spaces selected separately as the domain and range of
the operator. While eigenfunctions are discarded in this analysis, a viable
reconstruction algorithm is still demonstrated, and the sacrifice of
eigenfunctions realizes the theoretical goals of DMD analysis that have yet to
be achieved in other contexts. The manuscript concludes with the description of
a Dynamic Mode Decomposition algorithm that converges when a dense collection
of occupation kernels, arising from the data, are leveraged in the analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Regret Active learning. (arXiv:2104.02822v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02822</id>
        <link href="http://arxiv.org/abs/2104.02822"/>
        <updated>2021-06-08T02:20:24.601Z</updated>
        <summary type="html"><![CDATA[We develop an online learning algorithm for identifying unlabeled data points
that are most informative for training (i.e., active learning). By formulating
the active learning problem as the prediction with sleeping experts problem, we
provide a framework for identifying informative data with respect to any given
definition of informativeness. At the core of our work is an efficient
algorithm for sleeping experts that is tailored to achieve low regret on
predictable (easy) instances while remaining resilient to adversarial ones.
This stands in contrast to state-of-the-art active learning methods that are
overwhelmingly based on greedy selection, and hence cannot ensure good
performance across varying problem instances. We present empirical results
demonstrating that our method (i) instantiated with an informativeness measure
consistently outperforms its greedy counterpart and (ii) reliably outperforms
uniform sampling on real-world data sets and models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1"&gt;Cenk Baykal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1"&gt;Dan Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Constant Approximation Algorithm for Sequential Random-Order No-Substitution k-Median Clustering. (arXiv:2102.04050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04050</id>
        <link href="http://arxiv.org/abs/2102.04050"/>
        <updated>2021-06-08T02:20:24.598Z</updated>
        <summary type="html"><![CDATA[We study k-median clustering under the sequential no-substitution setting. In
this setting, a data stream is sequentially observed, and some of the points
are selected by the algorithm as cluster centers. However, a point can be
selected as a center only immediately after it is observed, before observing
the next point. In addition, a selected center cannot be substituted later. We
give the first algorithm for this setting that obtains a constant approximation
factor on the optimal risk under a random arrival order, an exponential
improvement over previous work. This is also the first constant approximation
guarantee that holds without any structural assumptions on the input data.
Moreover, the number of selected centers is only quasi-linear in k. Our
algorithm and analysis are based on a careful risk estimation that avoids
outliers, a new concept of a linear bin division, and a multiscale approach to
center selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Tom Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshkovitz_M/0/1/0/all/0/1"&gt;Michal Moshkovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabato_S/0/1/0/all/0/1"&gt;Sivan Sabato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Treatment Effects in Panels with General Intervention Patterns. (arXiv:2106.02780v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02780</id>
        <link href="http://arxiv.org/abs/2106.02780"/>
        <updated>2021-06-08T02:20:24.597Z</updated>
        <summary type="html"><![CDATA[The problem of causal inference with panel data is a central econometric
question. The following is a fundamental version of this problem: Let $M^*$ be
a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix
$Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} :=
M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are
unknown, heterogenous treatment effects. The problem requires we estimate the
average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} /
\sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to
estimating $\tau^*$ when $Z$ places support on a single row. This paper extends
that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus
broadly expanding its applicability. Our guarantees are the first of their type
in this general setting. Computational experiments on synthetic and real-world
data show a substantial advantage over competing estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Farias_V/0/1/0/all/0/1"&gt;Vivek F. Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_A/0/1/0/all/0/1"&gt;Andrew A. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peng_T/0/1/0/all/0/1"&gt;Tianyi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph Symmetrisation Bound on Channel Information Leakage under Blowfish Privacy. (arXiv:2007.05975v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05975</id>
        <link href="http://arxiv.org/abs/2007.05975"/>
        <updated>2021-06-08T02:20:24.589Z</updated>
        <summary type="html"><![CDATA[Blowfish privacy is a recent generalisation of differential privacy that
enables improved utility while maintaining privacy policies with semantic
guarantees, a factor that has driven the popularity of differential privacy in
computer science. This paper relates Blowfish privacy to an important measure
of privacy loss of information channels from the communications theory
community: min-entropy leakage. Symmetry in an input data neighbouring relation
is central to known connections between differential privacy and min-entropy
leakage. But while differential privacy exhibits strong symmetry, Blowfish
neighbouring relations correspond to arbitrary simple graphs owing to the
framework's flexible privacy policies. To bound the min-entropy leakage of
Blowfish-private mechanisms we organise our analysis over symmetrical
partitions corresponding to orbits of graph automorphism groups. A construction
meeting our bound with asymptotic equality demonstrates tightness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_T/0/1/0/all/0/1"&gt;Tobias Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zuhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sanming Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness. (arXiv:2106.02734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02734</id>
        <link href="http://arxiv.org/abs/2106.02734"/>
        <updated>2021-06-08T02:20:24.576Z</updated>
        <summary type="html"><![CDATA[We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck
as a regularizer for learning an adversarially robust deep neural network
classifier. We show that the HSIC bottleneck enhances robustness to adversarial
attacks both theoretically and experimentally. Our experiments on multiple
benchmark datasets and architectures demonstrate that incorporating an HSIC
bottleneck regularizer attains competitive natural accuracy and improves
adversarial robustness, both with and without adversarial examples during
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_T/0/1/0/all/0/1"&gt;Tong Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masoomi_A/0/1/0/all/0/1"&gt;Aria Masoomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1"&gt;Stratis Ioannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1"&gt;Jennifer Dy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandwidth-based Step-Sizes for Non-Convex Stochastic Optimization. (arXiv:2106.02888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02888</id>
        <link href="http://arxiv.org/abs/2106.02888"/>
        <updated>2021-06-08T02:20:24.569Z</updated>
        <summary type="html"><![CDATA[Many popular learning-rate schedules for deep neural networks combine a
decaying trend with local perturbations that attempt to escape saddle points
and bad local minima. We derive convergence guarantees for bandwidth-based
step-sizes, a general class of learning-rates that are allowed to vary in a
banded region. This framework includes cyclic and non-monotonic step-sizes for
which no theoretical guarantees were previously known. We provide worst-case
guarantees for SGD on smooth non-convex problems under several bandwidth-based
step sizes, including stagewise $1/\sqrt{t}$ and the popular step-decay
(constant and then drop by a constant), which is also shown to be optimal.
Moreover, we show that its momentum variant (SGDM) converges as fast as SGD
with the bandwidth-based step-decay step-size. Finally, we propose some novel
step-size schemes in the bandwidth-based family and verify their efficiency on
several deep neural network training tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_M/0/1/0/all/0/1"&gt;Mikael Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov Models. (arXiv:1905.11824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11824</id>
        <link href="http://arxiv.org/abs/1905.11824"/>
        <updated>2021-06-08T02:20:24.551Z</updated>
        <summary type="html"><![CDATA[Cyber threat intelligence is one of the emerging areas of focus in
information security. Much of the recent work has focused on rule-based methods
and detection of network attacks using Intrusion Detection algorithms. In this
paper we propose a framework for inspecting and modelling the behavioural
aspect of an attacker to obtain better insight predictive power on his future
actions. For modelling we propose a novel semi-supervised algorithm called
Fusion Hidden Markov Model (FHMM) which is more robust to noise, requires
comparatively less training time, and utilizes the benefits of ensemble
learning to better model temporal relationships in data. This paper evaluates
the performances of FHMM and compares it with both traditional algorithms like
Markov Chain, Hidden Markov Model (HMM) and recently developed Deep Recurrent
Neural Network (Deep RNN) architectures. We conduct the experiments on dataset
consisting of real data attacks on a Cowrie honeypot system. FHMM provides
accuracy comparable to deep RNN architectures at significant lower training
time. Given these experimental results, we recommend using FHMM for modelling
discrete temporal data for significantly faster training and better performance
than existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1"&gt;Soham Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1"&gt;Rahul Rade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazi_D/0/1/0/all/0/1"&gt;Dr. Faruk Kazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedNL: Making Newton-Type Methods Applicable to Federated Learning. (arXiv:2106.02969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02969</id>
        <link href="http://arxiv.org/abs/2106.02969"/>
        <updated>2021-06-08T02:20:24.536Z</updated>
        <summary type="html"><![CDATA[Inspired by recent work of Islamov et al (2021), we propose a family of
Federated Newton Learn (FedNL) methods, which we believe is a marked step in
the direction of making second-order methods applicable to FL. In contrast to
the aforementioned work, FedNL employs a different Hessian learning technique
which i) enhances privacy as it does not rely on the training data to be
revealed to the coordinating server, ii) makes it applicable beyond generalized
linear models, and iii) provably works with general contractive compression
operators for compressing the local Hessians, such as Top-$K$ or Rank-$R$,
which are vastly superior in practice. Notably, we do not need to rely on error
feedback for our methods to work with contractive compressors. Moreover, we
develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that
support partial participation, and globalization via cubic regularization and
line search, respectively, and FedNL-BC, which is a variant that can further
benefit from bidirectional compression of gradients and models, i.e., smart
uplink gradient and smart downlink model compression. We prove local
convergence rates that are independent of the condition number, the number of
training data points, and compression variance. Our communication efficient
Hessian learning technique provably learns the Hessian at the optimum. Finally,
we perform a variety of numerical experiments that show that our FedNL methods
have state-of-the-art communication complexity when compared to key baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safaryan_M/0/1/0/all/0/1"&gt;Mher Safaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islamov_R/0/1/0/all/0/1"&gt;Rustem Islamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stochastic Behaviour from Aggregate Data. (arXiv:2002.03513v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03513</id>
        <link href="http://arxiv.org/abs/2002.03513"/>
        <updated>2021-06-08T02:20:24.529Z</updated>
        <summary type="html"><![CDATA[Learning nonlinear dynamics from aggregate data is a challenging problem
because the full trajectory of each individual is not available, namely, the
individual observed at one time may not be observed at the next time point, or
the identity of individual is unavailable. This is in sharp contrast to
learning dynamics with full trajectory data, on which the majority of existing
methods are based. We propose a novel method using the weak form of Fokker
Planck Equation (FPE) -- a partial differential equation -- to describe the
density evolution of data in a sampled form, which is then combined with
Wasserstein generative adversarial network (WGAN) in the training process. In
such a sample-based framework we are able to learn the nonlinear dynamics from
aggregate data without explicitly solving the partial differential equation
(PDE) FPE. We demonstrate our approach in the context of a series of synthetic
and real-world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaojun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haomin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructive Universal High-Dimensional Distribution Generation through Deep ReLU Networks. (arXiv:2006.16664v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16664</id>
        <link href="http://arxiv.org/abs/2006.16664"/>
        <updated>2021-06-08T02:20:24.525Z</updated>
        <summary type="html"><![CDATA[We present an explicit deep neural network construction that transforms
uniformly distributed one-dimensional noise into an arbitrarily close
approximation of any two-dimensional Lipschitz-continuous target distribution.
The key ingredient of our design is a generalization of the "space-filling"
property of sawtooth functions discovered in (Bailey & Telgarsky, 2018). We
elicit the importance of depth - in our neural network construction - in
driving the Wasserstein distance between the target distribution and the
approximation realized by the network to zero. An extension to output
distributions of arbitrary dimension is outlined. Finally, we show that the
proposed construction does not incur a cost - in terms of error measured in
Wasserstein-distance - relative to generating $d$-dimensional target
distributions from $d$ independent random variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1"&gt;Dmytro Perekrestenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Stephan M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1"&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph to Graphs Framework for Retrosynthesis Prediction. (arXiv:2003.12725v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12725</id>
        <link href="http://arxiv.org/abs/2003.12725"/>
        <updated>2021-06-08T02:20:24.516Z</updated>
        <summary type="html"><![CDATA[A fundamental problem in computational chemistry is to find a set of
reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction.
Existing state-of-the-art methods rely on matching the target molecule with a
large set of reaction templates, which are very computationally expensive and
also suffer from the problem of coverage. In this paper, we propose a novel
template-free approach called G2Gs by transforming a target molecular graph
into a set of reactant molecular graphs. G2Gs first splits the target molecular
graph into a set of synthons by identifying the reaction centers, and then
translates the synthons to the final reactant graphs via a variational graph
translation framework. Experimental results show that G2Gs significantly
outperforms existing template-free approaches by up to 63% in terms of the
top-1 accuracy and achieves a performance close to that of state-of-the-art
template based approaches, but does not require domain knowledge and is much
more scalable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[k-Mixup Regularization for Deep Learning via Optimal Transport. (arXiv:2106.02933v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02933</id>
        <link href="http://arxiv.org/abs/2106.02933"/>
        <updated>2021-06-08T02:20:24.485Z</updated>
        <summary type="html"><![CDATA[Mixup is a popular regularization technique for training deep neural networks
that can improve generalization and increase adversarial robustness. It
perturbs input training data in the direction of other randomly-chosen
instances in the training set. To better leverage the structure of the data, we
extend mixup to \emph{$k$-mixup} by perturbing $k$-batches of training points
in the direction of other $k$-batches using displacement interpolation,
interpolation under the Wasserstein metric. We demonstrate theoretically and in
simulations that $k$-mixup preserves cluster and manifold structures, and we
extend theory studying efficacy of standard mixup. Our empirical results show
that training with $k$-mixup further improves generalization and robustness on
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greenewald_K/0/1/0/all/0/1"&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1"&gt;Anming Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1"&gt;Justin Solomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Edward Chien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[W-RST: Towards a Weighted RST-style Discourse Framework. (arXiv:2106.02658v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02658</id>
        <link href="http://arxiv.org/abs/2106.02658"/>
        <updated>2021-06-08T02:20:24.469Z</updated>
        <summary type="html"><![CDATA[Aiming for a better integration of data-driven and linguistically-inspired
approaches, we explore whether RST Nuclearity, assigning a binary assessment of
importance between text segments, can be replaced by automatically generated,
real-valued scores, in what we call a Weighted-RST framework. In particular, we
find that weighted discourse trees from auxiliary tasks can benefit key NLP
downstream applications, compared to nuclearity-centered approaches. We further
show that real-valued importance distributions partially and interestingly
align with the assessment and uncertainty of human annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1"&gt;Patrick Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PID-GAN: A GAN Framework based on a Physics-informed Discriminator for Uncertainty Quantification with Physics. (arXiv:2106.02993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02993</id>
        <link href="http://arxiv.org/abs/2106.02993"/>
        <updated>2021-06-08T02:20:24.456Z</updated>
        <summary type="html"><![CDATA[As applications of deep learning (DL) continue to seep into critical
scientific use-cases, the importance of performing uncertainty quantification
(UQ) with DL has become more pressing than ever before. In scientific
applications, it is also important to inform the learning of DL models with
knowledge of physics of the problem to produce physically consistent and
generalized solutions. This is referred to as the emerging field of
physics-informed deep learning (PIDL). We consider the problem of developing
PIDL formulations that can also perform UQ. To this end, we propose a novel
physics-informed GAN architecture, termed PID-GAN, where the knowledge of
physics is used to inform the learning of both the generator and discriminator
models, making ample use of unlabeled data instances. We show that our proposed
PID-GAN framework does not suffer from imbalance of generator gradients from
multiple loss terms as compared to state-of-the-art. We also empirically
demonstrate the efficacy of our proposed framework on a variety of case studies
involving benchmark physics-based PDEs as well as imperfect physics. All the
code and datasets used in this study have been made available on this link :
https://github.com/arkadaw9/PID-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daw_A/0/1/0/all/0/1"&gt;Arka Daw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruf_M/0/1/0/all/0/1"&gt;M. Maruf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1"&gt;Anuj Karpatne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data. (arXiv:2106.02881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02881</id>
        <link href="http://arxiv.org/abs/2106.02881"/>
        <updated>2021-06-08T02:20:24.434Z</updated>
        <summary type="html"><![CDATA[Treatment effect estimation from observational data is a critical research
topic across many domains. The foremost challenge in treatment effect
estimation is how to capture hidden confounders. Recently, the growing
availability of networked observational data offers a new opportunity to deal
with the issue of hidden confounders. Unlike networked data in traditional
graph learning tasks, such as node classification and link detection, the
networked data under the causal inference problem has its particularity, i.e.,
imbalanced network structure. In this paper, we propose a Graph Infomax
Adversarial Learning (GIAL) model for treatment effect estimation, which makes
full use of the network structure to capture more information by recognizing
the imbalance in network structure. We evaluate the performance of our GIAL
model on two benchmark datasets, and the results demonstrate superiority over
the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathbun_S/0/1/0/all/0/1"&gt;Stephen L. Rathbun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attribute-Aligned Strategy for Learning Speech Representation. (arXiv:2106.02810v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02810</id>
        <link href="http://arxiv.org/abs/2106.02810"/>
        <updated>2021-06-08T02:20:24.391Z</updated>
        <summary type="html"><![CDATA[Advancement in speech technology has brought convenience to our life.
However, the concern is on the rise as speech signal contains multiple personal
attributes, which would lead to either sensitive information leakage or bias
toward decision. In this work, we propose an attribute-aligned learning
strategy to derive speech representation that can flexibly address these issues
by attribute-selection mechanism. Specifically, we propose a
layered-representation variational autoencoder (LR-VAE), which factorizes
speech representation into attribute-sensitive nodes, to derive an
identity-free representation for speech emotion recognition (SER), and an
emotionless representation for speaker verification (SV). Our proposed method
achieves competitive performances on identity-free SER and a better performance
on emotionless SV, comparing to the current state-of-the-art method of using
adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also,
our proposed learning strategy reduces the model and training process needed to
achieve multiple privacy-preserving tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Lin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Su_B/0/1/0/all/0/1"&gt;Bo-Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Y.-W. Peter Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chi-Chun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Time Attention Networks for Irregularly Sampled Time Series. (arXiv:2101.10318v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10318</id>
        <link href="http://arxiv.org/abs/2101.10318"/>
        <updated>2021-06-08T02:20:24.112Z</updated>
        <summary type="html"><![CDATA[Irregular sampling occurs in many time series modeling applications where it
presents a significant challenge to standard deep learning models. This work is
motivated by the analysis of physiological time series data in electronic
health records, which are sparse, irregularly sampled, and multivariate. In
this paper, we propose a new deep learning framework for this setting that we
call Multi-Time Attention Networks. Multi-Time Attention Networks learn an
embedding of continuous-time values and use an attention mechanism to produce a
fixed-length representation of a time series containing a variable number of
observations. We investigate the performance of this framework on interpolation
and classification tasks using multiple datasets. Our results show that the
proposed approach performs as well or better than a range of baseline and
recently proposed models while offering significantly faster training times
than current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marlin_B/0/1/0/all/0/1"&gt;Benjamin M. Marlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HMRL: Hyper-Meta Learning for Sparse Reward Reinforcement Learning Problem. (arXiv:2002.04238v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04238</id>
        <link href="http://arxiv.org/abs/2002.04238"/>
        <updated>2021-06-08T02:20:24.070Z</updated>
        <summary type="html"><![CDATA[In spite of the success of existing meta reinforcement learning methods, they
still have difficulty in learning a meta policy effectively for RL problems
with sparse reward. In this respect, we develop a novel meta reinforcement
learning framework called Hyper-Meta RL(HMRL), for sparse reward RL problems.
It is consisted with three modules including the cross-environment meta state
embedding module which constructs a common meta state space to adapt to
different environments; the meta state based environment-specific meta reward
shaping which effectively extends the original sparse reward trajectory by
cross-environmental knowledge complementarity and as a consequence the meta
policy achieves better generalization and efficiency with the shaped meta
reward. Experiments with sparse-reward environments show the superiority of
HMRL on both transferability and policy learning efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yun Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1"&gt;Bo Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Leakage: The Role of Information Complexity in Privacy Leakage. (arXiv:2106.02818v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02818</id>
        <link href="http://arxiv.org/abs/2106.02818"/>
        <updated>2021-06-08T02:20:24.063Z</updated>
        <summary type="html"><![CDATA[We study the role of information complexity in privacy leakage about an
attribute of an adversary's interest, which is not known a priori to the system
designer. Considering the supervised representation learning setup and using
neural networks to parameterize the variational bounds of information
quantities, we study the impact of the following factors on the amount of
information leakage: information complexity regularizer weight, latent space
dimension, the cardinalities of the known utility and unknown sensitive
attribute sets, the correlation between utility and sensitive attributes, and a
potential bias in a sensitive attribute of adversary's interest. We conduct
extensive experiments on Colored-MNIST and CelebA datasets to evaluate the
effect of information complexity on the amount of intrinsic leakage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atashin_A/0/1/0/all/0/1"&gt;Amir Ahooye Atashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razeghi_B/0/1/0/all/0/1"&gt;Behrooz Razeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1"&gt;Slava Voloshynovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Bandits with Unknown Graph Structure. (arXiv:2106.02988v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02988</id>
        <link href="http://arxiv.org/abs/2106.02988"/>
        <updated>2021-06-08T02:20:24.056Z</updated>
        <summary type="html"><![CDATA[In causal bandit problems, the action set consists of interventions on
variables of a causal graph. Several researchers have recently studied such
bandit problems and pointed out their practical applications. However, all
existing works rely on a restrictive and impractical assumption that the
learner is given full knowledge of the causal graph structure upfront. In this
paper, we develop novel causal bandit algorithms without knowing the causal
graph. Our algorithms work well for causal trees, causal forests and a general
class of causal graphs. The regret guarantees of our algorithms greatly improve
upon those of standard multi-armed bandit (MAB) algorithms under mild
conditions. Lastly, we prove our mild conditions are necessary: without them
one cannot do better than standard MAB bandit algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yangyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meisami_A/0/1/0/all/0/1"&gt;Amirhossein Meisami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ambuj Tewari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical Composition of Differential Privacy. (arXiv:2106.02848v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02848</id>
        <link href="http://arxiv.org/abs/2106.02848"/>
        <updated>2021-06-08T02:20:24.051Z</updated>
        <summary type="html"><![CDATA[We give a fast algorithm to optimally compose privacy guarantees of
differentially private (DP) algorithms to arbitrary accuracy. Our method is
based on the notion of privacy loss random variables to quantify the privacy
loss of DP algorithms. The running time and memory needed for our algorithm to
approximate the privacy curve of a DP algorithm composed with itself $k$ times
is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela
et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We
demonstrate the utility of our algorithm by accurately computing the privacy
loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm
speeds up the privacy computations by a few orders of magnitude compared to
prior work, while maintaining similar accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-DimensionalSpatial Positional Encoding. (arXiv:2106.02795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-08T02:20:24.044Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Learning with Robustness to Adversarial Regressors. (arXiv:2005.01529v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01529</id>
        <link href="http://arxiv.org/abs/2005.01529"/>
        <updated>2021-06-08T02:20:24.038Z</updated>
        <summary type="html"><![CDATA[High order momentum-based parameter update algorithms have seen widespread
applications in training machine learning models. Recently, connections with
variational approaches have led to the derivation of new learning algorithms
with accelerated learning guarantees. Such methods however, have only
considered the case of static regressors. There is a significant need for
parameter update algorithms which can be proven stable in the presence of
adversarial time-varying regressors, as is commonplace in control theory. In
this paper, we propose a new discrete time algorithm which 1) provides
stability and asymptotic convergence guarantees in the presence of adversarial
regressors by leveraging insights from adaptive control theory and 2) provides
non-asymptotic accelerated learning guarantees leveraging insights from convex
optimization. In particular, our algorithm reaches an $\epsilon$ sub-optimal
point in at most $\tilde{\mathcal{O}}(1/\sqrt{\epsilon})$ iterations when
regressors are constant - matching lower bounds due to Nesterov of
$\Omega(1/\sqrt{\epsilon})$, up to a $\log(1/\epsilon)$ factor and provides
guaranteed bounds for stability when regressors are time-varying. We provide
numerical experiments for a variant of Nesterov's provably hard convex
optimization problem with time-varying regressors, as well as the problem of
recovering an image with a time-varying blur and noise using streaming data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gaudio_J/0/1/0/all/0/1"&gt;Joseph E. Gaudio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Annaswamy_A/0/1/0/all/0/1"&gt;Anuradha M. Annaswamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Moreu_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Moreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bolender_M/0/1/0/all/0/1"&gt;Michael A. Bolender&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gibson_T/0/1/0/all/0/1"&gt;Travis E. Gibson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Same State, Different Task: Continual Reinforcement Learning without Interference. (arXiv:2106.02940v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02940</id>
        <link href="http://arxiv.org/abs/2106.02940"/>
        <updated>2021-06-08T02:20:24.032Z</updated>
        <summary type="html"><![CDATA[Continual Learning (CL) considers the problem of training an agent
sequentially on a set of tasks while seeking to retain performance on all
previous tasks. A key challenge in CL is catastrophic forgetting, which arises
when performance on a previously mastered task is reduced when learning a new
task. While a variety of methods exist to combat forgetting, in some cases
tasks are fundamentally incompatible with each other and thus cannot be learnt
by a single policy. This can occur, in reinforcement learning (RL) when an
agent may be rewarded for achieving different goals from the same observation.
In this paper we formalize this ``interference'' as distinct from the problem
of forgetting. We show that existing CL methods based on single neural network
predictors with shared replay buffers fail in the presence of interference.
Instead, we propose a simple method, OWL, to address this challenge. OWL learns
a factorized policy, using shared feature extraction layers, but separate
heads, each specializing on a new task. The separate heads in OWL are used to
prevent interference. At test time, we formulate policy selection as a
multi-armed bandit problem, and show it is possible to select the best policy
for an unknown task using feedback from the environment. The use of bandit
algorithms allows the OWL agent to constructively re-use different continually
learnt policies at different times during an episode. We show in multiple RL
environments that existing replay based CL methods fail, while OWL is able to
achieve close to optimal performance when training sequentially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1"&gt;Philip Ball&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen J. Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks. (arXiv:2106.02978v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02978</id>
        <link href="http://arxiv.org/abs/2106.02978"/>
        <updated>2021-06-08T02:20:24.025Z</updated>
        <summary type="html"><![CDATA[Stochastic linear contextual bandit algorithms have substantial applications
in practice, such as recommender systems, online advertising, clinical trials,
etc. Recent works show that optimal bandit algorithms are vulnerable to
adversarial attacks and can fail completely in the presence of attacks.
Existing robust bandit algorithms only work for the non-contextual setting
under the attack of rewards and cannot improve the robustness in the general
and popular contextual bandit environment. In addition, none of the existing
methods can defend against attacked context. In this work, we provide the first
robust bandit algorithm for stochastic linear contextual bandit setting under a
fully adaptive and omniscient attack. Our algorithm not only works under the
attack of rewards, but also under attacked context. Moreover, it does not need
any information about the attack budget or the particular form of the attack.
We provide theoretical guarantees for our proposed algorithm and show by
extensive experiments that our proposed algorithm significantly improves the
robustness against various kinds of popular attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Word Embeddings by Averaging in a Shared Space. (arXiv:2106.02954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02954</id>
        <link href="http://arxiv.org/abs/2106.02954"/>
        <updated>2021-06-08T02:20:24.001Z</updated>
        <summary type="html"><![CDATA[We introduce a new approach for smoothing and improving the quality of word
embeddings. We consider a method of fusing word embeddings that were trained on
the same corpus but with different initializations. We project all the models
to a shared vector space using an efficient implementation of the Generalized
Procrustes Analysis (GPA) procedure, previously used in multilingual word
translation. Our word representation demonstrates consistent improvements over
the raw models as well as their simplistic average, on a range of tasks. As the
new representations are more stable and reliable, there is a noticeable
improvement in rare word evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1"&gt;Jacob Goldberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural dSCA: demixing multimodal interaction among brain areas during naturalistic experiments. (arXiv:2106.02948v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.02948</id>
        <link href="http://arxiv.org/abs/2106.02948"/>
        <updated>2021-06-08T02:20:23.981Z</updated>
        <summary type="html"><![CDATA[Multi-regional interaction among neuronal populations underlies the brain's
processing of rich sensory information in our daily lives. Recent neuroscience
and neuroimaging studies have increasingly used naturalistic stimuli and
experimental design to identify such realistic sensory computation in the
brain. However, existing methods for cross-areal interaction analysis with
dimensionality reduction, such as reduced-rank regression and canonical
correlation analysis, have limited applicability and interpretability in
naturalistic settings because they usually do not appropriately 'demix' neural
interactions into those associated with different types of task parameters or
stimulus features (e.g., visual or audio). In this paper, we develop a new
method for cross-areal interaction analysis that uses the rich task or stimulus
parameters to reveal how and what types of information are shared by different
neural populations. The proposed neural demixed shared component analysis
combines existing dimensionality reduction methods with a practical neural
network implementation of functional analysis of variance with latent
variables, thereby efficiently demixing nonlinear effects of continuous and
multimodal stimuli. We also propose a simplifying alternative under the
assumptions of linear effects and unimodal stimuli. To demonstrate our methods,
we analyzed two human neuroimaging datasets of participants watching
naturalistic videos of movies and dance movements. The results demonstrate that
our methods provide new insights into multi-regional interaction in the brain
during naturalistic sensory inputs, which cannot be captured by conventional
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Takagi_Y/0/1/0/all/0/1"&gt;Yu Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hunt_L/0/1/0/all/0/1"&gt;Laurence T. Hunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ohata_R/0/1/0/all/0/1"&gt;Ryu Ohata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Imamizu_H/0/1/0/all/0/1"&gt;Hiroshi Imamizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hirayama_J/0/1/0/all/0/1"&gt;Jun-ichiro Hirayama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Temporal Graph Neural Network for Trajectory Prediction. (arXiv:2106.02930v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02930</id>
        <link href="http://arxiv.org/abs/2106.02930"/>
        <updated>2021-06-08T02:20:23.942Z</updated>
        <summary type="html"><![CDATA[An effective understanding of the contextual environment and accurate motion
forecasting of surrounding agents is crucial for the development of autonomous
vehicles and social mobile robots. This task is challenging since the behavior
of an autonomous agent is not only affected by its own intention, but also by
the static environment and surrounding dynamically interacting agents. Previous
works focused on utilizing the spatial and temporal information in time domain
while not sufficiently taking advantage of the cues in frequency domain. To
this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which
can capture inter-agent correlations and temporal dependency simultaneously in
frequency domain in addition to time domain. SpecTGNN operates on both an agent
graph with dynamic state information and an environment graph with the features
extracted from context images in two streams. The model integrates graph
Fourier transform, spectral graph convolution and temporal gated convolution to
encode history information and forecast future trajectories. Moreover, we
incorporate a multi-head spatio-temporal attention mechanism to mitigate the
effect of error propagation in a long time horizon. We demonstrate the
performance of SpecTGNN on two public trajectory prediction benchmark datasets,
which achieves state-of-the-art performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:23.924Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIVERSE: Bayesian Data IntegratiVE learning for precise drug ResponSE prediction. (arXiv:2104.00520v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00520</id>
        <link href="http://arxiv.org/abs/2104.00520"/>
        <updated>2021-06-08T02:20:23.913Z</updated>
        <summary type="html"><![CDATA[Detecting predictive biomarkers from multi-omics data is important for
precision medicine, to improve diagnostics of complex diseases and for better
treatments. This needs substantial experimental efforts that are made difficult
by the heterogeneity of cell lines and huge cost. An effective solution is to
build a computational model over the diverse omics data, including genomic,
molecular, and environmental information. However, choosing informative and
reliable data sources from among the different types of data is a challenging
problem. We propose DIVERSE, a framework of Bayesian importance-weighted tri-
and bi-matrix factorization(DIVERSE3 or DIVERSE2) to predict drug responses
from data of cell lines, drugs, and gene interactions. DIVERSE integrates the
data sources systematically, in a step-wise manner, examining the importance of
each added data set in turn. More specifically, we sequentially integrate five
different data sets, which have not all been combined in earlier bioinformatic
methods for predicting drug responses. Empirical experiments show that DIVERSE
clearly outperformed five other methods including three state-of-the-art
approaches, under cross-validation, particularly in out-of-matrix prediction,
which is closer to the setting of real use cases and more challenging than
simpler in-matrix prediction. Additionally, case studies for discovering new
drugs further confirmed the performance advantage of DIVERSE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Paltun_B/0/1/0/all/0/1"&gt;Bet&amp;#xfc;l G&amp;#xfc;ven&amp;#xe7; Paltun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mamitsuka_H/0/1/0/all/0/1"&gt;Hiroshi Mamitsuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syndicated Bandits: A Framework for Auto Tuning Hyper-parameters in Contextual Bandit Algorithms. (arXiv:2106.02979v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02979</id>
        <link href="http://arxiv.org/abs/2106.02979"/>
        <updated>2021-06-08T02:20:23.906Z</updated>
        <summary type="html"><![CDATA[The stochastic contextual bandit problem, which models the trade-off between
exploration and exploitation, has many real applications, including recommender
systems, online advertising and clinical trials. As many other machine learning
algorithms, contextual bandit algorithms often have one or more
hyper-parameters. As an example, in most optimal stochastic contextual bandit
algorithms, there is an unknown exploration parameter which controls the
trade-off between exploration and exploitation. A proper choice of the
hyper-parameters is essential for contextual bandit algorithms to perform well.
However, it is infeasible to use offline tuning methods to select
hyper-parameters in contextual bandit environment since there is no
pre-collected dataset and the decisions have to be made in real time. To tackle
this problem, we first propose a two-layer bandit structure for auto tuning the
exploration parameter and further generalize it to the Syndicated Bandits
framework which can learn multiple hyper-parameters dynamically in contextual
bandit environment. We show our Syndicated Bandits framework can achieve the
optimal regret upper bounds and is general enough to handle the tuning tasks in
many popular contextual bandit algorithms, such as LinUCB, LinTS, UCB-GLM, etc.
Experiments on both synthetic and real datasets validate the effectiveness of
our proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi-Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Privacy Filters and Odometers with R\'enyi Differential Privacy and Applications to Differentially Private Deep Learning. (arXiv:2103.01379v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01379</id>
        <link href="http://arxiv.org/abs/2103.01379"/>
        <updated>2021-06-08T02:20:23.899Z</updated>
        <summary type="html"><![CDATA[Differential Privacy (DP) is the leading approach to privacy preserving deep
learning. As such, there are multiple efforts to provide drop-in integration of
DP into popular frameworks. These efforts, which add noise to each gradient
computation to make it DP, rely on composition theorems to bound the total
privacy loss incurred over this sequence of DP computations.

However, existing composition theorems present a tension between efficiency
and flexibility. Most theorems require all computations in the sequence to have
a predefined DP parameter, called the privacy budget. This prevents the design
of training algorithms that adapt the privacy budget on the fly, or that
terminate early to reduce the total privacy loss. Alternatively, the few
existing composition results for adaptive privacy budgets provide complex
bounds on the privacy loss, with constants too large to be practical.

In this paper, we study DP composition under adaptive privacy budgets through
the lens of R\'enyi Differential Privacy, proving a simpler composition theorem
with smaller constants, making it practical enough to use in algorithm design.
We demonstrate two applications of this theorem for DP deep learning: adapting
the noise or batch size online to improve a model's accuracy within a fixed
total privacy loss, and stopping early when fine-tuning a model to reduce total
privacy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lecuyer_M/0/1/0/all/0/1"&gt;Mathias L&amp;#xe9;cuyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology. (arXiv:2106.02926v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02926</id>
        <link href="http://arxiv.org/abs/2106.02926"/>
        <updated>2021-06-08T02:20:23.893Z</updated>
        <summary type="html"><![CDATA[In real-world applications of influence maximization (IM), the network
structure is often unknown. In this case, we may identify the most influential
seed nodes by exploring only a part of the underlying network given a small
budget for node queries. Motivated by the fact that collecting node metadata is
more cost-effective than investigating the relationship between nodes via
queried nodes, we develop IM-META, an end-to-end solution to IM in networks
with unknown topology by retrieving information from both queries and node
metadata. However, using such metadata to aid the IM process is not without
risk due to the noisy nature of metadata and uncertainties in connectivity
inference. To tackle these challenges, we formulate an IM problem that aims to
find two sets, i.e., seed nodes and queried nodes. We propose an effective
method that iteratively performs three steps: 1) we learn the relationship
between collected metadata and edges via a Siamese neural network model, 2) we
select a number of inferred influential edges to construct a reinforced graph
used for discovering an optimal seed set, and 3) we identify the next node to
query by maximizing the inferred influence spread using a topology-aware
ranking strategy. By querying only 5% of nodes, IM-META reaches 93% of the
upper bound performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won-Yong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1"&gt;Andreas Spitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction. (arXiv:2105.04544v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04544</id>
        <link href="http://arxiv.org/abs/2105.04544"/>
        <updated>2021-06-08T02:20:23.873Z</updated>
        <summary type="html"><![CDATA[We address the problem of causal effect estimation in the presence of
unobserved confounding, but where proxies for the latent confounder(s) are
observed. We propose two kernel-based methods for nonlinear causal effect
estimation in this setting: (a) a two-stage regression approach, and (b) a
maximum moment restriction approach. We focus on the proximal causal learning
setting, but our methods can be used to solve a wider class of inverse problems
characterised by a Fredholm integral equation. In particular, we provide a
unifying view of two-stage and moment restriction approaches for solving this
problem in a nonlinear setting. We provide consistency guarantees for each
algorithm, and we demonstrate these approaches achieve competitive results on
synthetic data and data simulating a real-world task. In particular, our
approach outperforms earlier methods that are not suited to leveraging proxy
variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mastouri_A/0/1/0/all/0/1"&gt;Afsaneh Mastouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gultchin_L/0/1/0/all/0/1"&gt;Limor Gultchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korba_A/0/1/0/all/0/1"&gt;Anna Korba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-armed Bandit Algorithms on System-on-Chip: Go Frequentist or Bayesian?. (arXiv:2106.02855v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02855</id>
        <link href="http://arxiv.org/abs/2106.02855"/>
        <updated>2021-06-08T02:20:23.867Z</updated>
        <summary type="html"><![CDATA[Multi-armed Bandit (MAB) algorithms identify the best arm among multiple arms
via exploration-exploitation trade-off without prior knowledge of arm
statistics. Their usefulness in wireless radio, IoT, and robotics demand
deployment on edge devices, and hence, a mapping on system-on-chip (SoC) is
desired. Theoretically, the Bayesian approach-based Thompson Sampling (TS)
algorithm offers better performance than the frequentist approach-based Upper
Confidence Bound (UCB) algorithm. However, TS is not synthesizable due to Beta
function. We address this problem by approximating it via a pseudo-random
number generator-based approach and efficiently realize the TS algorithm on
Zynq SoC. In practice, the type of arms distribution (e.g., Bernoulli,
Gaussian, etc.) is unknown and hence, a single algorithm may not be optimal. We
propose a reconfigurable and intelligent MAB (RI-MAB) framework. Here,
intelligence enables the identification of appropriate MAB algorithms for a
given environment, and reconfigurability allows on-the-fly switching between
algorithms on the SoC. This eliminates the need for parallel implementation of
algorithms resulting in huge savings in resources and power consumption. We
analyze the functional correctness, area, power, and execution time of the
proposed and existing architectures for various arm distributions, word-length,
and hardware-software co-design approaches. We demonstrate the superiority of
the RI-MAB over TS and UCB only architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Santosh_S/0/1/0/all/0/1"&gt;S. V. Sai Santosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Darak_S/0/1/0/all/0/1"&gt;Sumit J. Darak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2106.02982v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02982</id>
        <link href="http://arxiv.org/abs/2106.02982"/>
        <updated>2021-06-08T02:20:23.860Z</updated>
        <summary type="html"><![CDATA[In this study, a sensor fusion based GNSS spoofing attack detection framework
is presented that consists of three concurrent strategies for an autonomous
vehicle (AV): (i) prediction of location shift, (ii) detection of turns (left
or right), and (iii) recognition of motion state (including standstill state).
Data from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering
angle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural
network model, which is a long short-term memory (LSTM) network for predicting
the location shift, i.e., the distance that an AV travels between two
consecutive timestamps. We have then combined k-Nearest Neighbors (k-NN) and
Dynamic Time Warping (DTW) algorithms to detect turns using data from the
steering angle sensor. In addition, data from an AV's speed sensor is used to
recognize the AV's motion state including the standstill state. To prove the
efficacy of the sensor fusion-based attack detection framework, attack datasets
are created for three unique and sophisticated spoofing attacks turn by turn,
overshoot, and stop using the publicly available real-world Honda Research
Institute Driving Dataset (HDD). Our analysis reveals that the sensor
fusion-based detection framework successfully detects all three types of
spoofing attacks within the required computational latency threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mhafuzul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02468</id>
        <link href="http://arxiv.org/abs/2105.02468"/>
        <updated>2021-06-08T02:20:23.854Z</updated>
        <summary type="html"><![CDATA[Understanding why deep nets can classify data in large dimensions remains a
challenge. It has been proposed that they do so by becoming stable to
diffeomorphisms, yet existing empirical measurements support that it is often
not the case. We revisit this question by defining a maximum-entropy
distribution on diffeomorphisms, that allows to study typical diffeomorphisms
of a given norm. We confirm that stability toward diffeomorphisms does not
strongly correlate to performance on benchmark data sets of images. By
contrast, we find that the stability toward diffeomorphisms relative to that of
generic transformations $R_f$ correlates remarkably with the test error
$\epsilon_t$. It is of order unity at initialization but decreases by several
decades during training for state-of-the-art architectures. For CIFAR10 and 15
known architectures, we find $\epsilon_t\approx 0.2\sqrt{R_f}$, suggesting that
obtaining a small $R_f$ is important to achieve good performance. We study how
$R_f$ depends on the size of the training set and compare it to a simple model
of invariant learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1"&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02736</id>
        <link href="http://arxiv.org/abs/2106.02736"/>
        <updated>2021-06-08T02:20:23.846Z</updated>
        <summary type="html"><![CDATA[While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable and
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable \emph{sampling}
scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our
approach, samples are proposed from the same masked conditionals used for
training the masked language models, and they are accepted or rejected based on
their energy values according to the target distribution. We validate the
effectiveness of the proposed parametrizations by exploring the quality of
samples drawn from these energy-based models on the conditional generation task
of machine translation. We theoretically and empirically justify our sampling
algorithm by showing that the masked conditionals on their own do not yield a
Markov chain whose stationary distribution is that of our target distribution,
and our approach generates higher quality samples than other recently proposed
undirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,
2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1"&gt;Kartik Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1"&gt;Chris Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs. (arXiv:2006.04373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04373</id>
        <link href="http://arxiv.org/abs/2006.04373"/>
        <updated>2021-06-08T02:20:23.828Z</updated>
        <summary type="html"><![CDATA[In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1"&gt;Geewon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft-Attention Improves Skin Cancer Classification Performance. (arXiv:2105.03358v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03358</id>
        <link href="http://arxiv.org/abs/2105.03358"/>
        <updated>2021-06-08T02:20:23.821Z</updated>
        <summary type="html"><![CDATA[In clinical applications, neural networks must focus on and highlight the
most important parts of an input image. Soft-Attention mechanism enables a
neural network toachieve this goal. This paper investigates the effectiveness
of Soft-Attention in deep neural architectures. The central aim of
Soft-Attention is to boost the value of important features and suppress the
noise-inducing features. We compare the performance of VGG, ResNet,
InceptionResNetv2 and DenseNet architectures with and without the
Soft-Attention mechanism, while classifying skin lesions. The original network
when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while
achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,
Soft-Attention coupling improves the sensitivity score by 3.8% compared to
baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly
available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Datta_S/0/1/0/all/0/1"&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammad Abuzar Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1"&gt;Sargur N. Srihari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingchen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning of User Verification Models Without Sharing Embeddings. (arXiv:2104.08776v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08776</id>
        <link href="http://arxiv.org/abs/2104.08776"/>
        <updated>2021-06-08T02:20:23.815Z</updated>
        <summary type="html"><![CDATA[We consider the problem of training User Verification (UV) models in
federated setting, where each user has access to the data of only one class and
user embeddings cannot be shared with the server or other users. To address
this problem, we propose Federated User Verification (FedUV), a framework in
which users jointly learn a set of vectors and maximize the correlation of
their instance embeddings with a secret linear combination of those vectors. We
show that choosing the linear combinations from the codewords of an
error-correcting code allows users to collaboratively train the model without
revealing their embedding vectors. We present the experimental results for user
verification with voice, face, and handwriting data and show that FedUV is on
par with existing approaches, while not sharing the embeddings with other users
or the server.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1"&gt;Hossein Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunsin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sungrack Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1"&gt;Christos Louizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1"&gt;Joseph Soriaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-06-08T02:20:23.807Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02902</id>
        <link href="http://arxiv.org/abs/2106.02902"/>
        <updated>2021-06-08T02:20:23.795Z</updated>
        <summary type="html"><![CDATA[Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
article, we probe BERT specifically to understand and measure the relational
knowledge it captures in its parametric memory. While probing for linguistic
understanding is commonly applied to all layers of BERT as well as fine-tuned
models, this has not been done for factual knowledge. We utilize existing
knowledge base completion tasks (LAMA) to probe every layer of pre-trained as
well as fine-tuned BERT models(ranking, question answering, NER). Our findings
show that knowledge is not just contained in BERT's final layers. Intermediate
layers contribute a significant amount (17-60%) to the total knowledge found.
Probing intermediate layers also reveals how different types of knowledge
emerge at varying rates. When BERT is fine-tuned, relational knowledge is
forgotten. The extent of forgetting is impacted by the fine-tuning objective
and the training data. We found that ranking models forget the least and retain
more knowledge in their final layer compared to masked language modeling and
question-answering. However, masked language modeling performed the best at
acquiring new knowledge from the training data. When it comes to learning
facts, we found that capacity and fact density are key factors. We hope this
initial work will spur further research into understanding the parametric
memory of language models and the effect of training objectives on factual
knowledge. The code to repeat the experiments is publicly available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1"&gt;Jonas Wallat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forced Variational Integrator Networks for Prediction and Control of Mechanical Systems. (arXiv:2106.02973v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02973</id>
        <link href="http://arxiv.org/abs/2106.02973"/>
        <updated>2021-06-08T02:20:23.782Z</updated>
        <summary type="html"><![CDATA[As deep learning becomes more prevalent for prediction and control of real
physical systems, it is important that these overparameterized models are
consistent with physically plausible dynamics. This elicits a problem with how
much inductive bias to impose on the model through known physical parameters
and principles to reduce complexity of the learning problem to give us more
reliable predictions. Recent work employs discrete variational integrators
parameterized as a neural network architecture to learn conservative Lagrangian
systems. The learned model captures and enforces global energy preserving
properties of the system from very few trajectories. However, most real systems
are inherently non-conservative and, in practice, we would also like to apply
actuation. In this paper we extend this paradigm to account for general forcing
(e.g. control input and damping) via discrete d'Alembert's principle which may
ultimately be used for control applications. We show that this forced
variational integrator networks (FVIN) architecture allows us to accurately
account for energy dissipation and external forcing while still capturing the
true underlying energy-based passive dynamics. We show that in application this
can result in highly-data efficient model-based control and can predict on real
non-conservative systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havens_A/0/1/0/all/0/1"&gt;Aaron Havens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1"&gt;Girish Chowdhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Adversarial Robustness and Optimization Bias. (arXiv:2102.08868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08868</id>
        <link href="http://arxiv.org/abs/2102.08868"/>
        <updated>2021-06-08T02:20:23.761Z</updated>
        <summary type="html"><![CDATA[We demonstrate that the choice of optimizer, neural network architecture, and
regularizer significantly affect the adversarial robustness of linear neural
networks, providing guarantees without the need for adversarial training. To
this end, we revisit a known result linking maximally robust classifiers and
minimum norm solutions, and combine it with recent results on the implicit bias
of optimizers. First, we show that, under certain conditions, it is possible to
achieve both perfect standard accuracy and a certain degree of robustness,
simply by training an overparametrized model using the implicit bias of the
optimization. In that regime, there is a direct relationship between the type
of the optimizer and the attack to which the model is robust. To the best of
our knowledge, this work is the first to study the impact of optimization
methods such as sign gradient descent and proximal methods on adversarial
robustness. Second, we characterize the robustness of linear convolutional
models, showing that they resist attacks subject to a constraint on the
Fourier-$\ell_\infty$ norm. To illustrate these findings we design a novel
Fourier-$\ell_\infty$ attack that finds adversarial examples with controllable
frequencies. We evaluate Fourier-$\ell_\infty$ robustness of
adversarially-trained deep CIFAR-10 models from the standard RobustBench
benchmark and visualize adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Approximation Algorithms for Active Sequential Hypothesis Testing. (arXiv:2103.04250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04250</id>
        <link href="http://arxiv.org/abs/2103.04250"/>
        <updated>2021-06-08T02:20:23.753Z</updated>
        <summary type="html"><![CDATA[In the problem of active sequential hypotheses testing (ASHT), a learner
seeks to identify the true hypothesis from among a known set of hypotheses. The
learner is given a set of actions and knows the random distribution of the
outcome of any action under any true hypothesis. Given a target error
$\delta>0$, the goal is to sequentially select the fewest number of actions so
as to identify the true hypothesis with probability at least $1 - \delta$.
Motivated by applications in which the number of hypotheses or actions is
massive (e.g. genomics-based cancer detection), we propose efficient (greedy,
in fact) algorithms and provide the first approximation guarantees for ASHT,
under two types of adaptivity. Both of our guarantees are independent of the
number of actions and logarithmic in the number of hypotheses. We numerically
evaluate the performance of our algorithms using both synthetic and real DNA
mutation data, demonstrating that our algorithms outperform previous heuristic
policies by large margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_K/0/1/0/all/0/1"&gt;Kyra Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1"&gt;Su Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Andrew Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:23.738Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Assignment Problem with Time Constraints. (arXiv:2106.02856v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02856</id>
        <link href="http://arxiv.org/abs/2106.02856"/>
        <updated>2021-06-08T02:20:23.721Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end framework for the Assignment Problem with multiple
tasks mapped to a group of workers, using reinforcement learning while
preserving many constraints. Tasks and workers have time constraints and there
is a cost associated with assigning a worker to a task. Each worker can perform
multiple tasks until it exhausts its allowed time units (capacity). We train a
reinforcement learning agent to find near optimal solutions to the problem by
minimizing total cost associated with the assignments while maintaining hard
constraints. We use proximal policy optimization to optimize model parameters.
The model generates a sequence of actions in real-time which correspond to task
assignment to workers, without having to retrain for changes in the dynamic
state of the environment. In our problem setting reward is computed as negative
of the assignment cost. We also demonstrate our results on bin packing and
capacitated vehicle routing problem, using the same framework. Our results
outperform Google OR-Tools using MIP and CP-SAT solvers with large problem
instances, in terms of solution quality and computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pathan_S/0/1/0/all/0/1"&gt;Sharmin Pathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1"&gt;Vyom Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-08T02:20:23.696Z</updated>
        <summary type="html"><![CDATA[Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time
retinal images with high resolution down to 2 $\mu m$. This technique enables
detection of the morphologies of individual microaneurysms (MAs), which are one
of the earliest signs of diabetic retinopathy (DR), a frequent complication of
diabetes that can lead to visual impairment and blindness. In contrast to
previous automatic models developed for MA detection on standard fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policy,
including preprocessing, data augmentation and transfer learning, to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and
segmentation, leading to correct MA morphological classification, while
outperforming the state-of-the-art both in accuracy and cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant message passing for the prediction of tensorial properties and molecular spectra. (arXiv:2102.03150v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03150</id>
        <link href="http://arxiv.org/abs/2102.03150"/>
        <updated>2021-06-08T02:20:23.689Z</updated>
        <summary type="html"><![CDATA[Message passing neural networks have become a method of choice for learning
on graphs, in particular the prediction of chemical properties and the
acceleration of molecular dynamics studies. While they readily scale to large
training data sets, previous approaches have proven to be less data efficient
than kernel methods. We identify limitations of invariant representations as a
major reason and extend the message passing formulation to rotationally
equivariant representations. On this basis, we propose the polarizable atom
interaction neural network (PaiNN) and improve on common molecule benchmarks
over previous networks, while reducing model size and inference time. We
leverage the equivariant atomwise representations obtained by PaiNN for the
prediction of tensorial properties. Finally, we apply this to the simulation of
molecular spectra, achieving speedups of 4-5 orders of magnitude compared to
the electronic structure reference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutt_K/0/1/0/all/0/1"&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unke_O/0/1/0/all/0/1"&gt;Oliver T. Unke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gastegger_M/0/1/0/all/0/1"&gt;Michael Gastegger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07085</id>
        <link href="http://arxiv.org/abs/2104.07085"/>
        <updated>2021-06-08T02:20:23.677Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel layer based on fast Walsh-Hadamard
transform (WHT) and smooth-thresholding to replace $1\times 1$ convolution
layers in deep neural networks. In the WHT domain, we denoise the transform
domain coefficients using the new smooth-thresholding non-linearity, a smoothed
version of the well-known soft-thresholding operator. We also introduce a
family of multiplication-free operators from the basic 2$\times$2 Hadamard
transform to implement $3\times 3$ depthwise separable convolution layers.
Using these two types of layers, we replace the bottleneck layers in
MobileNet-V2 to reduce the network's number of parameters with a slight loss in
accuracy. For example, by replacing the final third bottleneck layers, we
reduce the number of parameters from 2.270M to 540K. This reduces the accuracy
from 95.21\% to 92.98\% on the CIFAR-10 dataset. Our approach significantly
improves the speed of data processing. The fast Walsh-Hadamard transform has a
computational complexity of $O(m\log_2 m)$. As a result, it is computationally
more efficient than the $1\times1$ convolution layer. The fast Walsh-Hadamard
layer processes a tensor in $\mathbb{R}^{10\times32\times32\times1024}$ about 2
times faster than $1\times1$ convolution layer on NVIDIA Jetson Nano computer
board.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hongyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabawi_D/0/1/0/all/0/1"&gt;Diaa Dabawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1"&gt;Ahmet Enis Cetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Inference with Sparse and Quantized Communication. (arXiv:2004.01302v4 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01302</id>
        <link href="http://arxiv.org/abs/2004.01302"/>
        <updated>2021-06-08T02:20:23.670Z</updated>
        <summary type="html"><![CDATA[We consider the problem of distributed inference where agents in a network
observe a stream of private signals generated by an unknown state, and aim to
uniquely identify this state from a finite set of hypotheses. We focus on
scenarios where communication between agents is costly, and takes place over
channels with finite bandwidth. To reduce the frequency of communication, we
develop a novel event-triggered distributed learning rule that is based on the
principle of diffusing low beliefs on each false hypothesis. Building on this
principle, we design a trigger condition under which an agent broadcasts only
those components of its belief vector that have adequate innovation, to only
those neighbors that require such information. We prove that our rule
guarantees convergence to the true state exponentially fast almost surely
despite sparse communication, and that it has the potential to significantly
reduce information flow from uninformative agents to informative agents. Next,
to deal with finite-precision communication channels, we propose a distributed
learning rule that leverages the idea of adaptive quantization. We show that by
sequentially refining the range of the quantizers, every agent can learn the
truth exponentially fast almost surely, while using just $1$ bit to encode its
belief on each hypothesis. For both our proposed algorithms, we rigorously
characterize the trade-offs between communication-efficiency and the learning
rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_A/0/1/0/all/0/1"&gt;Aritra Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Richards_J/0/1/0/all/0/1"&gt;John A. Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shreyas Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (arXiv:2103.09593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09593</id>
        <link href="http://arxiv.org/abs/2103.09593"/>
        <updated>2021-06-08T02:20:23.663Z</updated>
        <summary type="html"><![CDATA[Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Tests and Always-Valid Confidence Intervals for contingency tables and beyond. (arXiv:2106.02693v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.02693</id>
        <link href="http://arxiv.org/abs/2106.02693"/>
        <updated>2021-06-08T02:20:23.657Z</updated>
        <summary type="html"><![CDATA[We develop E variables for testing whether two data streams come from the
same source or not, and more generally, whether the difference between the
sources is larger than some minimal effect size. These E variables lead to
tests that remain safe, i.e. keep their Type-I error guarantees, under flexible
sampling scenarios such as optional stopping and continuation. We also develop
the corresponding always-valid confidence intervals. In special cases our E
variables also have an optimal `growth' property under the alternative. We
illustrate the generic construction through the special case of 2x2 contingency
tables, where we also allow for the incorporation of different restrictions on
a composite alternative. Comparison to p-value analysis in simulations and a
real-world example show that E variables, through their flexibility, often
allow for early stopping of data collection, thereby retaining similar power as
classical methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1"&gt;Rosanne Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ly_A/0/1/0/all/0/1"&gt;Alexander Ly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grunwald_P/0/1/0/all/0/1"&gt;Peter Gr&amp;#xfc;nwald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Uniform Convergence for Multicalibration. (arXiv:2005.01757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01757</id>
        <link href="http://arxiv.org/abs/2005.01757"/>
        <updated>2021-06-08T02:20:23.634Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in societal concerns in machine learning systems,
especially in fairness. Multicalibration gives a comprehensive methodology to
address group fairness. In this work, we address the multicalibration error and
decouple it from the prediction error. The importance of decoupling the
fairness metric (multicalibration) and the accuracy (prediction error) is due
to the inherent trade-off between the two, and the societal decision regarding
the "right tradeoff" (as imposed many times by regulators). Our work gives
sample complexity bounds for uniform convergence guarantees of multicalibration
error, which implies that regardless of the accuracy, we can guarantee that the
empirical and (true) multicalibration errors are close. We emphasize that our
results: (1) are more general than previous bounds, as they apply to both
agnostic and realizable settings, and do not rely on a specific type of
algorithm (such as deferentially private), (2) improve over previous
multicalibration sample complexity bounds and (3) implies uniform convergence
guarantees for the classical calibration error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabat_E/0/1/0/all/0/1"&gt;Eliran Shabat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1"&gt;Lee Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exact Solver for the Weston-Watkins SVM Subproblem. (arXiv:2102.05640v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05640</id>
        <link href="http://arxiv.org/abs/2102.05640"/>
        <updated>2021-06-08T02:20:23.627Z</updated>
        <summary type="html"><![CDATA[Recent empirical evidence suggests that the Weston-Watkins support vector
machine is among the best performing multiclass extensions of the binary SVM.
Current state-of-the-art solvers repeatedly solve a particular subproblem
approximately using an iterative strategy. In this work, we propose an
algorithm that solves the subproblem exactly using a novel reparametrization of
the Weston-Watkins dual problem. For linear WW-SVMs, our solver shows
significant speed-up over the state-of-the-art solver when the number of
classes is large. Our exact subproblem solver also allows us to prove linear
convergence of the overall solver.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yutong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scott_C/0/1/0/all/0/1"&gt;Clayton D. Scott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Multi-Armed Bandits in the Shuffle Model. (arXiv:2106.02900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02900</id>
        <link href="http://arxiv.org/abs/2106.02900"/>
        <updated>2021-06-08T02:20:23.618Z</updated>
        <summary type="html"><![CDATA[We give an $(\varepsilon,\delta)$-differentially private algorithm for the
multi-armed bandit (MAB) problem in the shuffle model with a
distribution-dependent regret of $O\left(\left(\sum_{a\in
[k]:\Delta_a>0}\frac{\log
T}{\Delta_a}\right)+\frac{k\sqrt{\log\frac{1}{\delta}}\log
T}{\varepsilon}\right)$, and a distribution-independent regret of
$O\left(\sqrt{kT\log T}+\frac{k\sqrt{\log\frac{1}{\delta}}\log
T}{\varepsilon}\right)$, where $T$ is the number of rounds, $\Delta_a$ is the
suboptimality gap of the arm $a$, and $k$ is the total number of arms. Our
upper bound almost matches the regret of the best known algorithms for the
centralized model, and significantly outperforms the best known algorithm in
the local model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Jay Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1"&gt;Haim Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1"&gt;Uri Stemmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2004.09764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-08T02:20:23.611Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation. (arXiv:2103.10702v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10702</id>
        <link href="http://arxiv.org/abs/2103.10702"/>
        <updated>2021-06-08T02:20:23.579Z</updated>
        <summary type="html"><![CDATA[Text-based video segmentation is a challenging task that segments out the
natural language referred objects in videos. It essentially requires semantic
comprehension and fine-grained video understanding. Existing methods introduce
language representation into segmentation models in a bottom-up manner, which
merely conducts vision-language interaction within local receptive fields of
ConvNets. We argue that such interaction is not fulfilled since the model can
barely construct region-level relationships given partial observations, which
is contrary to the description logic of natural language/referring expressions.
In fact, people usually describe a target object using relations with other
objects, which may not be easily understood without seeing the whole video. To
address the issue, we introduce a novel top-down approach by imitating how we
human segment an object with the language guidance. We first figure out all
candidate objects in videos and then choose the refereed one by parsing
relations among those high-level objects. Three kinds of object-level relations
are investigated for precise relationship understanding, i.e., positional
relation, text-guided semantic relation, and temporal relation. Extensive
experiments on A2D Sentences and J-HMDB Sentences show our method outperforms
state-of-the-art methods by a large margin. Qualitative results also show our
results are more explainable. Besides, based on the inspiration, we win the
first place in CVPR2021 Referring Youtube-VOS challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yawei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09943</id>
        <link href="http://arxiv.org/abs/2103.09943"/>
        <updated>2021-06-08T02:20:23.556Z</updated>
        <summary type="html"><![CDATA[Blind pansharpening addresses the problem of generating a high
spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution
multi-spectral (LRMS) image with the guidance of its associated spatially
misaligned high spatial-resolution panchromatic (PAN) image without parametric
side information. In this paper, we propose a fast approach to blind
pansharpening and achieve state-of-the-art image reconstruction quality.
Typical blind pansharpening algorithms are often computationally intensive
since the blur kernel and the target HRMS image are often computed using
iterative solvers and in an alternating fashion. To achieve fast blind
pansharpening, we decouple the solution of the blur kernel and of the HRMS
image. First, we estimate the blur kernel by computing the kernel coefficients
with minimum total generalized variation that blur a downsampled version of the
PAN image to approximate a linear combination of the LRMS image channels. Then,
we estimate each channel of the HRMS image using local Laplacian prior to
regularize the relationship between each HRMS channel and the PAN image.
Solving the HRMS image is accelerated by both parallelizing across the channels
and by fast numerical algorithms for each channel. Due to the fast scheme and
the powerful priors we used on the blur kernel coefficients (total generalized
variation) and on the cross-channel relationship (local Laplacian prior),
numerical experiments demonstrate that our algorithm outperforms
state-of-the-art model-based counterparts in terms of both computational time
and reconstruction quality of the HRMS images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dehong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1"&gt;Hassan Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1"&gt;Petros T. Boufounos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Principle of Relevant Information for Hyperspectral Image Classification. (arXiv:1907.06022v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.06022</id>
        <link href="http://arxiv.org/abs/1907.06022"/>
        <updated>2021-06-08T02:20:23.549Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel architecture, termed multiscale principle of
relevant information (MPRI), to learn discriminative spectral-spatial features
for hyperspectral image (HSI) classification. MPRI inherits the merits of the
principle of relevant information (PRI) to effectively extract multiscale
information embedded in the given data, and also takes advantage of the
multilayer structure to learn representations in a coarse-to-fine manner.
Specifically, MPRI performs spectral-spatial pixel characterization (using PRI)
and feature dimensionality reduction (using regularized linear discriminant
analysis) iteratively and successively. Extensive experiments on three
benchmark data sets demonstrate that MPRI outperforms existing state-of-the-art
methods (including deep learning based ones) qualitatively and quantitatively,
especially in the scenario of limited training samples. Code of MPRI is
available at \url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yantao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shujian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giraldo_L/0/1/0/all/0/1"&gt;Luis Sanchez Giraldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1"&gt;Jose C. Principe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Abstractions of Neural Networks. (arXiv:2106.02997v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02997</id>
        <link href="http://arxiv.org/abs/2106.02997"/>
        <updated>2021-06-08T02:20:23.542Z</updated>
        <summary type="html"><![CDATA[Structural analysis methods (e.g., probing and feature attribution) are
increasingly important tools for neural network analysis. We propose a new
structural analysis method grounded in a formal theory of \textit{causal
abstraction} that provides rich characterizations of model-internal
representations and their roles in input/output behavior. In this method,
neural representations are aligned with variables in interpretable causal
models, and then \textit{interchange interventions} are used to experimentally
verify that the neural representations have the causal properties of their
aligned variables. We apply this method in a case study to analyze neural
models trained on Multiply Quantified Natural Language Inference (MQNLI)
corpus, a highly complex NLI dataset that was constructed with a
tree-structured natural logic causal model. We discover that a BERT-based model
with state-of-the-art performance successfully realizes the approximate causal
structure of the natural logic causal model, whereas a simpler baseline model
fails to show any such structure, demonstrating that neural representations
encode the compositional structure of MQNLI examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Atticus Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hanson Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1"&gt;Thomas Icard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immediate Proximity Detection Using Wi-Fi-Enabled Smartphones. (arXiv:2106.02777v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02777</id>
        <link href="http://arxiv.org/abs/2106.02777"/>
        <updated>2021-06-08T02:20:23.534Z</updated>
        <summary type="html"><![CDATA[Smartphone apps for exposure notification and contact tracing have been shown
to be effective in controlling the COVID-19 pandemic. However, Bluetooth Low
Energy tokens similar to those broadcast by existing apps can still be picked
up far away from the transmitting device. In this paper, we present a new class
of methods for detecting whether or not two Wi-Fi-enabled devices are in
immediate physical proximity, i.e. 2 or fewer meters apart, as established by
the U.S. Centers for Disease Control and Prevention (CDC). Our goal is to
enhance the accuracy of smartphone-based exposure notification and contact
tracing systems. We present a set of binary machine learning classifiers that
take as input pairs of Wi-Fi RSSI fingerprints. We empirically verify that a
single classifier cannot generalize well to a range of different environments
with vastly different numbers of detectable Wi-Fi Access Points (APs). However,
specialized classifiers, tailored to situations where the number of detectable
APs falls within a certain range, are able to detect immediate physical
proximity significantly more accurately. As such, we design three classifiers
for situations with low, medium, and high numbers of detectable APs. These
classifiers distinguish between pairs of RSSI fingerprints recorded 2 or fewer
meters apart and pairs recorded further apart but still in Bluetooth range. We
characterize their balanced accuracy for this task to be between 66.8% and
77.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hyfte_Z/0/1/0/all/0/1"&gt;Zach Van Hyfte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1"&gt;Avideh Zakhor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-06-08T02:20:23.495Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tetrad: Actively Secure 4PC for Secure Training and Inference. (arXiv:2106.02850v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02850</id>
        <link href="http://arxiv.org/abs/2106.02850"/>
        <updated>2021-06-08T02:20:23.469Z</updated>
        <summary type="html"><![CDATA[In this work, we design an efficient mixed-protocol framework, Tetrad, with
applications to privacy-preserving machine learning. It is designed for the
four-party setting with at most one active corruption and supports rings.

Our fair multiplication protocol requires communicating only 5 ring elements
improving over the state-of-the-art protocol of Trident (Chaudhari et al.
NDSS'20). The technical highlights of Tetrad include efficient (a) truncation
without any overhead, (b) multi-input multiplication protocols for arithmetic
and boolean worlds, (c) garbled-world, tailor-made for the mixed-protocol
framework, and (d) conversion mechanisms to switch between the computation
styles. The fair framework is also extended to provide robustness without
inflating the costs.

The competence of Tetrad is tested with benchmarks for deep neural networks
such as LeNet and VGG16 and support vector machines. One variant of our
framework aims at minimizing the execution time, while the other focuses on the
monetary cost. We observe improvements up to 6x over Trident across these
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koti_N/0/1/0/all/0/1"&gt;Nishat Koti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arpita Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rachuri_R/0/1/0/all/0/1"&gt;Rahul Rachuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ajith Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Time-Adaptive Drift-Diffusion Model. (arXiv:2106.02742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02742</id>
        <link href="http://arxiv.org/abs/2106.02742"/>
        <updated>2021-06-08T02:20:23.458Z</updated>
        <summary type="html"><![CDATA[Animals can quickly learn the timing of events with fixed intervals and their
rate of acquisition does not depend on the length of the interval. In contrast,
recurrent neural networks that use gradient based learning have difficulty
predicting the timing of events that depend on stimulus that occurred long ago.
We present the latent time-adaptive drift-diffusion model (LTDDM), an extension
to the time-adaptive drift-diffusion model (TDDM), a model for animal learning
of timing that exhibits behavioural properties consistent with experimental
data from animals. The performance of LTDDM is compared to that of a state of
the art long short-term memory (LSTM) recurrent neural network across three
timing tasks. Differences in the relative performance of these two models is
discussed and it is shown how LTDDM can learn these events time series orders
of magnitude faster than recurrent neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cimolino_G/0/1/0/all/0/1"&gt;Gabriele Cimolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivest_F/0/1/0/all/0/1"&gt;Francois Rivest&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpikePropamine: Differentiable Plasticity in Spiking Neural Networks. (arXiv:2106.02681v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.02681</id>
        <link href="http://arxiv.org/abs/2106.02681"/>
        <updated>2021-06-08T02:20:23.220Z</updated>
        <summary type="html"><![CDATA[The adaptive changes in synaptic efficacy that occur between spiking neurons
have been demonstrated to play a critical role in learning for biological
neural networks. Despite this source of inspiration, many learning focused
applications using Spiking Neural Networks (SNNs) retain static synaptic
connections, preventing additional learning after the initial training period.
Here, we introduce a framework for simultaneously learning the underlying
fixed-weights and the rules governing the dynamics of synaptic plasticity and
neuromodulated synaptic plasticity in SNNs through gradient descent. We further
demonstrate the capabilities of this framework on a series of challenging
benchmarks, learning the parameters of several plasticity rules including BCM,
Oja's, and their respective set of neuromodulatory variants. The experimental
results display that SNNs augmented with differentiable plasticity are
sufficient for solving a set of challenging temporal learning tasks that a
traditional SNN fails to solve, even in the presence of significant noise.
These networks are also shown to be capable of producing locomotion on a
high-dimensional robotic learning task, where near-minimal degradation in
performance is observed in the presence of novel conditions not seen during the
initial training period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmidgall_S/0/1/0/all/0/1"&gt;Samuel Schmidgall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashkanazy_J/0/1/0/all/0/1"&gt;Julia Ashkanazy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawson_W/0/1/0/all/0/1"&gt;Wallace Lawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1"&gt;Joe Hays&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. (arXiv:2104.09435v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09435</id>
        <link href="http://arxiv.org/abs/2104.09435"/>
        <updated>2021-06-08T02:20:23.214Z</updated>
        <summary type="html"><![CDATA[Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_M/0/1/0/all/0/1"&gt;Myeongsu Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Bumju Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Soohyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Hean Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sunghoe Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Long Context NLM for ASR Rescoring in Conversational Agents. (arXiv:2104.11070v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11070</id>
        <link href="http://arxiv.org/abs/2104.11070"/>
        <updated>2021-06-08T02:20:23.207Z</updated>
        <summary type="html"><![CDATA[Neural Language Models (NLM), when trained and evaluated with context
spanning multiple utterances, have been shown to consistently outperform both
conventional n-gram language models and NLMs that use limited context. In this
paper, we investigate various techniques to incorporate turn based context
history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent
based NLMs, we explore context carry over mechanism and feature based
augmentation, where we incorporate other forms of contextual information such
as bot response and system dialogue acts as classified by a Natural Language
Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem
with contextual NLM, we propose the use of attention layer over lexical
metadata to improve feature based augmentation. Additionally, we adapt our
contextual NLM towards user provided on-the-fly speech patterns by leveraging
encodings from a large pre-trained masked language model and performing fusion
with a Transformer-XL based NLM. We test our proposed models using N-best
rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on
downstream NLU tasks such as intent classification and slot labeling. The best
performing model shows a relative WER between 1.6% and 9.1% and a slot labeling
F1 score improvement of 4% over non-contextual baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1"&gt;Monica Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1"&gt;Srikanth Ronanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context. (arXiv:2007.08911v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08911</id>
        <link href="http://arxiv.org/abs/2007.08911"/>
        <updated>2021-06-08T02:20:23.184Z</updated>
        <summary type="html"><![CDATA[Concerns about the societal impact of AI-based services and systems has
encouraged governments and other organisations around the world to propose AI
policy frameworks to address fairness, accountability, transparency and related
topics. To achieve the objectives of these frameworks, the data and software
engineers who build machine-learning systems require knowledge about a variety
of relevant supporting tools and techniques. In this paper we provide an
overview of technologies that support building trustworthy machine learning
systems, i.e., systems whose properties justify that people place trust in
them. We argue that four categories of system properties are instrumental in
achieving the policy objectives, namely fairness, explainability, auditability
and safety & security (FEAS). We discuss how these properties need to be
considered across all stages of the machine learning life cycle, from data
collection through run-time model inference. As a consequence, we survey in
this paper the main technologies with respect to all four of the FEAS
properties, for data-centric as well as model-centric stages of the machine
learning system life cycle. We conclude with an identification of open research
problems, with a particular focus on the connection between trustworthy machine
learning technologies and their implications for individuals and society.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toreini_E/0/1/0/all/0/1"&gt;Ehsan Toreini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitken_M/0/1/0/all/0/1"&gt;Mhairi Aitken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coopamootoo_K/0/1/0/all/0/1"&gt;Kovila P. L. Coopamootoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elliott_K/0/1/0/all/0/1"&gt;Karen Elliott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelaya_V/0/1/0/all/0/1"&gt;Vladimiro Gonzalez Zelaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Missier_P/0/1/0/all/0/1"&gt;Paolo Missier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Magdalene Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moorsel_A/0/1/0/all/0/1"&gt;Aad van Moorsel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Birds of a Feather Flock Together: A Close Look at Cooperation Emergence via Multi-Agent RL. (arXiv:2104.11455v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11455</id>
        <link href="http://arxiv.org/abs/2104.11455"/>
        <updated>2021-06-08T02:20:23.178Z</updated>
        <summary type="html"><![CDATA[How cooperation emerges is a long-standing and interdisciplinary problem.
Game-theoretical studies on social dilemmas reveal that altruistic incentives
are critical to the emergence of cooperation but their analyses are limited to
stateless games. For more realistic scenarios, multi-agent reinforcement
learning has been used to study sequential social dilemmas (SSDs). Recent works
show that learning to incentivize other agents can promote cooperation in SSDs.
However, we find that, with these incentivizing mechanisms, the team
cooperation level does not converge and regularly oscillates between
cooperation and defection during learning. We show that a second-order social
dilemma resulting from the incentive mechanisms is the main reason for such
fragile cooperation. We formally analyze the dynamics of second-order social
dilemmas and find that a typical tendency of humans, called homophily, provides
a promising solution. We propose a novel learning framework to encourage
homophilic incentives and show that it achieves stable cooperation in both SSDs
of public goods and tragedy of the commons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Heng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Geometric Matrix Completion. (arXiv:1911.07255v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07255</id>
        <link href="http://arxiv.org/abs/1911.07255"/>
        <updated>2021-06-08T02:20:23.171Z</updated>
        <summary type="html"><![CDATA[Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boyarski_A/0/1/0/all/0/1"&gt;Amit Boyarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Sanketh Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic-Guided Reinforcement Learning. (arXiv:2106.02757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02757</id>
        <link href="http://arxiv.org/abs/2106.02757"/>
        <updated>2021-06-08T02:20:23.164Z</updated>
        <summary type="html"><![CDATA[We provide a framework for accelerating reinforcement learning (RL)
algorithms by heuristics constructed from domain knowledge or offline data.
Tabula rasa RL algorithms require environment interactions or computation that
scales with the horizon of the sequential decision-making task. Using our
framework, we show how heuristic-guided RL induces a much shorter-horizon
subproblem that provably solves the original task. Our framework can be viewed
as a horizon-based regularization for controlling bias and variance in RL under
a finite interaction budget. On the theoretical side, we characterize
properties of a good heuristic and its impact on RL acceleration. In
particular, we introduce the novel concept of an "improvable heuristic" -- a
heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On
the empirical side, we instantiate our framework to accelerate several
state-of-the-art algorithms in simulated robotic control tasks and procedurally
generated games. Our framework complements the rich literature on warm-starting
RL with expert demonstrations or exploratory datasets, and introduces a
principled method for injecting prior knowledge into RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1"&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1"&gt;Adith Swaminathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end learnable EEG channel selection for deep neural networks with Gumbel-softmax. (arXiv:2102.09050v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09050</id>
        <link href="http://arxiv.org/abs/2102.09050"/>
        <updated>2021-06-08T02:20:23.158Z</updated>
        <summary type="html"><![CDATA[Many electroencephalography (EEG) applications rely on channel selection
methods to remove the least informative channels, e.g., to reduce the amount of
electrodes to be mounted, to decrease the computational load, or to reduce
overfitting effects and improve performance. Wrapper-based channel selection
methods aim to match the channel selection step to the target model, yet they
require to re-train the model multiple times on different candidate channel
subsets, which often leads to an unacceptably high computational cost,
especially when said model is a (deep) neural network. To alleviate this, we
propose a framework to embed the EEG channel selection in the neural network
itself to jointly learn the network weights and optimal channels in an
end-to-end manner by traditional backpropagation algorithms. We deal with the
discrete nature of this new optimization problem by employing continuous
relaxations of the discrete channel selection parameters based on the
Gumbel-softmax trick. We also propose a regularization method that discourages
selecting channels more than once. This generic approach is evaluated on two
different EEG tasks: motor imagery brain-computer interfaces and auditory
attention decoding. The results demonstrate that our framework is generally
applicable, while being competitive with state-of-the art EEG channel selection
methods, tailored to these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Strypsteen_T/0/1/0/all/0/1"&gt;Thomas Strypsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertrand_A/0/1/0/all/0/1"&gt;Alexander Bertrand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Robust Graph Neural Networks with Topology Adaptive Edge Dropping. (arXiv:2106.02892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02892</id>
        <link href="http://arxiv.org/abs/2106.02892"/>
        <updated>2021-06-08T02:20:23.139Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are processing architectures that exploit graph
structural information to model representations from network data. Despite
their success, GNNs suffer from sub-optimal generalization performance given
limited training data, referred to as over-fitting. This paper proposes
Topology Adaptive Edge Dropping (TADropEdge) method as an adaptive data
augmentation technique to improve generalization performance and learn robust
GNN models. We start by explicitly analyzing how random edge dropping increases
the data diversity during training, while indicating i.i.d. edge dropping does
not account for graph structural information and could result in noisy
augmented data degrading performance. To overcome this issue, we consider graph
connectivity as the key property that captures graph topology. TADropEdge
incorporates this factor into random edge dropping such that the edge-dropped
subgraphs maintain similar topology as the underlying graph, yielding more
satisfactory data augmentation. In particular, TADropEdge first leverages the
graph spectrum to assign proper weights to graph edges, which represent their
criticality for establishing the graph connectivity. It then normalizes the
edge weights and drops graph edges adaptively based on their normalized
weights. Besides improving generalization performance, TADropEdge reduces
variance for efficient training and can be applied as a generic method modular
to different GNN models. Intensive experiments on real-life and synthetic
datasets corroborate theory and verify the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Subhrajit Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blum_R/0/1/0/all/0/1"&gt;Rick S. Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1"&gt;Brian M. Sadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Latent Space Tuning for Non-Stationary Distributions. (arXiv:2105.03584v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03584</id>
        <link href="http://arxiv.org/abs/2105.03584"/>
        <updated>2021-06-08T02:20:23.132Z</updated>
        <summary type="html"><![CDATA[Powerful deep learning tools, such as convolutional neural networks (CNN),
are able to learn the input-output relationships of large complicated systems
directly from data. Encoder-decoder deep CNNs are able to extract features
directly from images, mix them with scalar inputs within a general
low-dimensional latent space, and then generate new complex 2D outputs which
represent complex physical phenomenon. One important challenge faced by deep
learning methods is large non-stationary systems whose characteristics change
quickly with time for which re-training is not feasible. In this paper we
present a method for adaptive tuning of the low-dimensional latent space of
deep encoder-decoder style CNNs based on real-time feedback to quickly
compensate for unknown and fast distribution shifts. We demonstrate our
approach for predicting the properties of a time-varying charged particle beam
in a particle accelerator whose components (accelerating electric fields and
focusing magnetic fields) are also quickly changing with time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Scheinker_A/0/1/0/all/0/1"&gt;Alexander Scheinker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cropp_F/0/1/0/all/0/1"&gt;Frederick Cropp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paiagua_S/0/1/0/all/0/1"&gt;Sergio Paiagua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Filippetto_D/0/1/0/all/0/1"&gt;Daniele Filippetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. (arXiv:2106.02768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02768</id>
        <link href="http://arxiv.org/abs/2106.02768"/>
        <updated>2021-06-08T02:20:23.125Z</updated>
        <summary type="html"><![CDATA[Cross domain recommender system constitutes a powerful method to tackle the
cold-start and sparsity problem by aggregating and transferring user
preferences across multiple category domains. Therefore, it has great potential
to improve click-through-rate prediction performance in online commerce
platforms having many domains of products. While several cross domain
sequential recommendation models have been proposed to leverage information
from a source domain to improve CTR predictions in a target domain, they did
not take into account bidirectional latent relations of user preferences across
source-target domain pairs. As such, they cannot provide enhanced cross-domain
CTR predictions for both domains simultaneously. In this paper, we propose a
novel approach to cross-domain sequential recommendations based on the dual
learning mechanism that simultaneously transfers information between two
related domains in an iterative manner until the learning process stabilizes.
In particular, the proposed Dual Attentive Sequential Learning (DASL) model
consists of two novel components Dual Embedding and Dual Attention, which
jointly establish the two-stage learning process: we first construct dual
latent embeddings that extract user preferences in both domains simultaneously,
and subsequently provide cross-domain recommendations by matching the extracted
latent embeddings with candidate items through dual-attention learning
mechanism. We conduct extensive offline experiments on three real-world
datasets to demonstrate the superiority of our proposed model, which
significantly and consistently outperforms several state-of-the-art baselines
across all experimental settings. We also conduct an online A/B test at a major
video streaming platform Alibaba-Youku, where our proposed model significantly
improves business performance over the latest production system in the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02923</id>
        <link href="http://arxiv.org/abs/2106.02923"/>
        <updated>2021-06-08T02:20:23.112Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues. Variational Auto-Encoders (VAEs) and their extensions
such as $\beta$-VAEs have been shown to locally align latent variables with PCA
directions, which can help to improve model disentanglement under some
conditions. Borrowing inspiration from Independent Component Analysis (ICA) and
sparse coding, we propose applying an $L_1$ loss to the VAE's generative
Jacobian during training to encourage local latent variable alignment with
independent factors of variation in the data. We demonstrate our results on a
variety of datasets, giving qualitative and quantitative results using
information theoretic and modularity measures that show our added $L_1$ cost
encourages local axis alignment of the latent representation with individual
factors of variation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1"&gt;Travers Rhodes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Temperature Imaging Using Pseudo-Inversed Convolutional Neural Network Aided TDLAS Tomography. (arXiv:2106.02901v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02901</id>
        <link href="http://arxiv.org/abs/2106.02901"/>
        <updated>2021-06-08T02:20:23.105Z</updated>
        <summary type="html"><![CDATA[As an in situ combustion diagnostic tool, Tunable Diode Laser Absorption
Spectroscopy (TDLAS) tomography has been widely used for imaging of
two-dimensional temperature distributions in reactive flows. Compared with the
computational tomographic algorithms, Convolutional Neural Networks (CNNs) have
been proofed to be more robust and accurate for image reconstruction,
particularly in case of limited access of laser beams in the Region of Interest
(RoI). In practice, flame in the RoI that requires to be reconstructed with
good spatial resolution is commonly surrounded by low-temperature background.
Although the background is not of high interest, spectroscopic absorption still
exists due to heat dissipation and gas convection. Therefore, we propose a
Pseudo-Inversed CNN (PI-CNN) for hierarchical temperature imaging that (a) uses
efficiently the training and learning resources for temperature imaging in the
RoI with good spatial resolution, and (b) reconstructs the less spatially
resolved background temperature by adequately addressing the integrity of the
spectroscopic absorption model. In comparison with the traditional CNN, the
newly introduced pseudo inversion of the RoI sensitivity matrix is more
penetrating for revealing the inherent correlation between the projection data
and the RoI to be reconstructed, thus prioritising the temperature imaging in
the RoI with high accuracy and high computational efficiency. In this paper,
the proposed algorithm was validated by both numerical simulation and lab-scale
experiment, indicating good agreement between the phantoms and the
high-fidelity reconstructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Si_J/0/1/0/all/0/1"&gt;Jingjing Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yinbo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Enemali_G/0/1/0/all/0/1"&gt;Godwin Enemali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel multi-scale loss function for classification problems in machine learning. (arXiv:2106.02676v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.02676</id>
        <link href="http://arxiv.org/abs/2106.02676"/>
        <updated>2021-06-08T02:20:23.083Z</updated>
        <summary type="html"><![CDATA[We introduce two-scale loss functions for use in various gradient descent
algorithms applied to classification problems via deep neural networks. This
new method is generic in the sense that it can be applied to a wide range of
machine learning architectures, from deep neural networks to support vector
machines for example. These two-scale loss functions allow to focus the
training onto objects in the training set which are not well classified. This
leads to an increase in several measures of performance for
appropriately-defined two-scale loss functions with respect to the more
classical cross-entropy when tested on traditional deep neural networks on the
MNIST, CIFAR10, and CIFAR100 data-sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Berlyand_L/0/1/0/all/0/1"&gt;Leonid Berlyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Creese_R/0/1/0/all/0/1"&gt;Robert Creese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jabin_P/0/1/0/all/0/1"&gt;Pierre-Emmanuel Jabin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel approximation on algebraic varieties. (arXiv:2106.02755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02755</id>
        <link href="http://arxiv.org/abs/2106.02755"/>
        <updated>2021-06-08T02:20:23.077Z</updated>
        <summary type="html"><![CDATA[Low-rank approximation of kernels is a fundamental mathematical problem with
widespread algorithmic applications. Often the kernel is restricted to an
algebraic variety, e.g., in problems involving sparse or low-rank data. We show
that significantly better approximations are obtainable in this setting: the
rank required to achieve a given error depends on the variety's dimension
rather than the ambient dimension, which is typically much larger. This is true
in both high-precision and high-dimensional regimes. Our results are presented
for smooth isotropic kernels, the predominant class of kernels used in
applications. Our main technical insight is to approximate smooth kernels by
polynomial kernels, and leverage two key properties of polynomial kernels that
hold when they are restricted to a variety. First, their ranks decrease
exponentially in the variety's co-dimension. Second, their maximum values are
governed by their values over a small set of points. Together, our results
provide a general approach for exploiting (approximate) "algebraic structure"
in datasets in order to efficiently solve large-scale data science problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Altschuler_J/0/1/0/all/0/1"&gt;Jason M. Altschuler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parrilo_P/0/1/0/all/0/1"&gt;Pablo A. Parrilo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks using a Single Neuron: Folded-in-Time Architecture using Feedback-Modulated Delay Loops. (arXiv:2011.10115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10115</id>
        <link href="http://arxiv.org/abs/2011.10115"/>
        <updated>2021-06-08T02:20:23.071Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are among the most widely applied machine learning tools
showing outstanding performance in a broad range of tasks. We present a method
for folding a deep neural network of arbitrary size into a single neuron with
multiple time-delayed feedback loops. This single-neuron deep neural network
comprises only a single nonlinearity and appropriately adjusted modulations of
the feedback signals. The network states emerge in time as a temporal unfolding
of the neuron's dynamics. By adjusting the feedback-modulation within the
loops, we adapt the network's connection weights. These connection weights are
determined via a back-propagation algorithm, where both the delay-induced and
local network connections must be taken into account. Our approach can fully
represent standard Deep Neural Networks (DNN), encompasses sparse DNNs, and
extends the DNN concept toward dynamical systems implementations. The new
method, which we call Folded-in-time DNN (Fit-DNN), exhibits promising
performance in a set of benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1"&gt;Florian Stelzer&lt;/a&gt; (1, 2 and 4), &lt;a href="http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; R&amp;#xf6;hm&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_R/0/1/0/all/0/1"&gt;Raul Vicente&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1"&gt;Ingo Fischer&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1"&gt;Serhiy Yanchuk&lt;/a&gt; (1) ((1) Institute of Mathematics, Technische Universit&amp;#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&amp;#xe4;t zu Berlin, Germany, (3) Instituto de F&amp;#xed;sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Spain, (4) Institute of Computer Science, University of Tartu, Estonia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?. (arXiv:2106.02890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02890</id>
        <link href="http://arxiv.org/abs/2106.02890"/>
        <updated>2021-06-08T02:20:23.061Z</updated>
        <summary type="html"><![CDATA[Can models with particular structure avoid being biased towards spurious
correlation in out-of-distribution (OOD) generalization? Peters et al. (2016)
provides a positive answer for linear cases. In this paper, we use a functional
modular probing method to analyze deep model structures under OOD setting. We
demonstrate that even in biased models (which focus on spurious correlation)
there still exist unbiased functional subnetworks. Furthermore, we articulate
and demonstrate the functional lottery ticket hypothesis: full network contains
a subnetwork that can achieve better OOD performance. We then propose Modular
Risk Minimization to solve the subnetwork selection problem. Our algorithm
learns the subnetwork structure from a given dataset, and can be combined with
any other OOD regularization methods. Experiments on various OOD generalization
tasks corroborate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dinghuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yilun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Defense with Data Diversity: Weak Correlation Implies Strong Robustness. (arXiv:2106.02867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02867</id>
        <link href="http://arxiv.org/abs/2106.02867"/>
        <updated>2021-06-08T02:20:23.056Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a framework of filter-based ensemble of deep
neuralnetworks (DNNs) to defend against adversarial attacks. The framework
builds an ensemble of sub-models -- DNNs with differentiated preprocessing
filters. From the theoretical perspective of DNN robustness, we argue that
under the assumption of high quality of the filters, the weaker the
correlations of the sensitivity of the filters are, the more robust the
ensemble model tends to be, and this is corroborated by the experiments of
transfer-based attacks. Correspondingly, we propose a principle that chooses
the specific filters with smaller Pearson correlation coefficients, which
ensures the diversity of the inputs received by DNNs, as well as the
effectiveness of the entire framework against attacks. Our ensemble models are
more robust than those constructed by previous defense methods like adversarial
training, and even competitive with the classical ensemble of adversarial
trained DNNs under adversarial attacks when the attacking radius is large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Renjue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pengfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1"&gt;Bai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Q-Learning in Zero-sum Markov Games. (arXiv:2106.02748v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.02748</id>
        <link href="http://arxiv.org/abs/2106.02748"/>
        <updated>2021-06-08T02:20:23.040Z</updated>
        <summary type="html"><![CDATA[We study multi-agent reinforcement learning (MARL) in infinite-horizon
discounted zero-sum Markov games. We focus on the practical but challenging
setting of decentralized MARL, where agents make decisions without coordination
by a centralized controller, but only based on their own payoffs and local
actions executed. The agents need not observe the opponent's actions or
payoffs, possibly being even oblivious to the presence of the opponent, nor be
aware of the zero-sum structure of the underlying game, a setting also referred
to as radically uncoupled in the literature of learning in games. In this
paper, we develop for the first time a radically uncoupled Q-learning dynamics
that is both rational and convergent: the learning dynamics converges to the
best response to the opponent's strategy when the opponent follows an
asymptotically stationary strategy; the value function estimates converge to
the payoffs at a Nash equilibrium when both agents adopt the dynamics. The key
challenge in this decentralized setting is the non-stationarity of the learning
environment from an agent's perspective, since both her own payoffs and the
system evolution depend on the actions of other agents, and each agent adapts
their policies simultaneously and independently. To address this issue, we
develop a two-timescale learning dynamics where each agent updates her local
Q-function and value function estimates concurrently, with the latter happening
at a slower timescale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sayin_M/0/1/0/all/0/1"&gt;Muhammed O. Sayin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leslie_D/0/1/0/all/0/1"&gt;David S. Leslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basar_T/0/1/0/all/0/1"&gt;Tamer Basar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Lottery Ticket Hypothesis for Graph Neural Networks. (arXiv:2102.06790v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06790</id>
        <link href="http://arxiv.org/abs/2102.06790"/>
        <updated>2021-06-08T02:20:23.033Z</updated>
        <summary type="html"><![CDATA[With graphs rapidly growing in size and deeper graph neural networks (GNNs)
emerging, the training and inference of GNNs become increasingly expensive.
Existing network weight pruning algorithms cannot address the main space and
computational bottleneck in GNNs, caused by the size and connectivity of the
graph. To this end, this paper first presents a unified GNN sparsification
(UGS) framework that simultaneously prunes the graph adjacency matrix and the
model weights, for effectively accelerating GNN inference on large-scale
graphs. Leveraging this new tool, we further generalize the recently popular
lottery ticket hypothesis to GNNs for the first time, by defining a graph
lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network,
which can be jointly identified from the original GNN and the full dense graph
by iteratively applying UGS. Like its counterpart in convolutional neural
networks, GLT can be trained in isolation to match the performance of training
with the full model and graph, and can be drawn from both randomly initialized
and self-supervised pre-trained GNNs. Our proposal has been experimentally
verified across various GNN architectures and diverse tasks, on both
small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale
datasets from the challenging Open Graph Benchmark (OGB). Specifically, for
node classification, our found GLTs achieve the same accuracies with 20%~98%
MACs saving on small graphs and 25%~85% MACs saving on large ones. For link
prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph
datasets, respectively, without compromising predictive performance. Codes
available at https://github.com/VITA-Group/Unified-LTH-GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1"&gt;Yongduo Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principle Bit Analysis: Autoencoding with Schur-Concave Loss. (arXiv:2106.02796v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02796</id>
        <link href="http://arxiv.org/abs/2106.02796"/>
        <updated>2021-06-08T02:20:23.025Z</updated>
        <summary type="html"><![CDATA[We consider a linear autoencoder in which the latent variables are quantized,
or corrupted by noise, and the constraint is Schur-concave in the set of latent
variances. Although finding the optimal encoder/decoder pair for this setup is
a nonconvex optimization problem, we show that decomposing the source into its
principal components is optimal. If the constraint is strictly Schur-concave
and the empirical covariance matrix has only simple eigenvalues, then any
optimal encoder/decoder must decompose the source in this way. As one
application, we consider a strictly Schur-concave constraint that estimates the
number of bits needed to represent the latent variables under fixed-rate
encoding, a setup that we call \emph{Principal Bit Analysis (PBA)}. This yields
a practical, general-purpose, fixed-rate compressor that outperforms existing
algorithms. As a second application, we show that a prototypical
autoencoder-based variable-rate compressor is guaranteed to decompose the
source into its principal components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1"&gt;Sourbh Bhadane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Aaron B. Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks. (arXiv:2106.02743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02743</id>
        <link href="http://arxiv.org/abs/2106.02743"/>
        <updated>2021-06-08T02:20:23.019Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are the first choice methods for graph machine
learning problems thanks to their ability to learn state-of-the-art level
representations from graph-structured data. However, centralizing a massive
amount of real-world graph data for GNN training is prohibitive due to
user-side privacy concerns, regulation restrictions, and commercial
competition. Federated Learning is the de-facto standard for collaborative
training of machine learning models over many distributed edge devices without
the need for centralization. Nevertheless, training graph neural networks in a
federated setting is vaguely defined and brings statistical and systems
challenges. This work proposes SpreadGNN, a novel multi-task federated training
framework capable of operating in the presence of partial labels and absence of
a central server for the first time in the literature. SpreadGNN extends
federated multi-task learning to realistic serverless settings for GNNs, and
utilizes a novel optimization algorithm with a convergence guarantee,
Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized
multi-task learning problems. We empirically demonstrate the efficacy of our
framework on a variety of non-I.I.D. distributed graph-level molecular property
prediction datasets with partial labels. Our results show that SpreadGNN
outperforms GNN models trained over a central server-dependent federated
learning system, even in constrained topologies. The source code is publicly
available at https://github.com/FedML-AI/SpreadGNN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceyani_E/0/1/0/all/0/1"&gt;Emir Ceyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_K/0/1/0/all/0/1"&gt;Keshav Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1"&gt;Murali Annavaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No MCMC for me: Amortized sampling for fast and stable training of energy-based models. (arXiv:2010.04230v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04230</id>
        <link href="http://arxiv.org/abs/2010.04230"/>
        <updated>2021-06-08T02:20:23.012Z</updated>
        <summary type="html"><![CDATA[Energy-Based Models (EBMs) present a flexible and appealing way to represent
uncertainty. Despite recent advances, training EBMs on high-dimensional data
remains a challenging problem as the state-of-the-art approaches are costly,
unstable, and require considerable tuning and domain expertise to apply
successfully. In this work, we present a simple method for training EBMs at
scale which uses an entropy-regularized generator to amortize the MCMC sampling
typically used in EBM training. We improve upon prior MCMC-based entropy
regularization methods with a fast variational approximation. We demonstrate
the effectiveness of our approach by using it to train tractable likelihood
models. Next, we apply our estimator to the recently proposed Joint Energy
Model (JEM), where we match the original performance with faster and stable
training. This allows us to extend JEM models to semi-supervised classification
on tabular data from a variety of continuous domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1"&gt;Will Grathwohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jacob Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1"&gt;Milad Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1"&gt;Kevin Swersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Estimation by Mixing: Adaptivity and More. (arXiv:2106.02803v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02803</id>
        <link href="http://arxiv.org/abs/2106.02803"/>
        <updated>2021-06-08T02:20:22.994Z</updated>
        <summary type="html"><![CDATA[Networks analysis has been commonly used to study the interactions between
units of complex systems. One problem of particular interest is learning the
network's underlying connection pattern given a single and noisy instantiation.
While many methods have been proposed to address this problem in recent years,
they usually assume that the true model belongs to a known class, which is not
verifiable in most real-world applications. Consequently, network modeling
based on these methods either suffers from model misspecification or relies on
additional model selection procedures that are not well understood in theory
and can potentially be unstable in practice. To address this difficulty, we
propose a mixing strategy that leverages available arbitrary models to improve
their individual performances. The proposed method is computationally efficient
and almost tuning-free; thus, it can be used as an off-the-shelf method for
network modeling. We show that the proposed method performs equally well as the
oracle estimate when the true model is included as individual candidates. More
importantly, the method remains robust and outperforms all current estimates
even when the models are misspecified. Extensive simulation examples are used
to verify the advantage of the proposed mixing method. Evaluation of link
prediction performance on 385 real-world networks from six domains also
demonstrates the universal competitiveness of the mixing method across multiple
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Le_C/0/1/0/all/0/1"&gt;Can M. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increasing Depth Leads to U-Shaped Test Risk in Over-parameterized Convolutional Networks. (arXiv:2010.09610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09610</id>
        <link href="http://arxiv.org/abs/2010.09610"/>
        <updated>2021-06-08T02:20:22.987Z</updated>
        <summary type="html"><![CDATA[Recent works have demonstrated that increasing model capacity through width
in over-parameterized neural networks leads to a decrease in test risk. For
neural networks, however, model capacity can also be increased through depth,
yet understanding the impact of increasing depth on test risk remains an open
question. In this work, we demonstrate that the test risk of over-parameterized
convolutional networks is a U-shaped curve (i.e. monotonically decreasing, then
increasing) with increasing depth. We first provide empirical evidence for this
phenomenon via image classification experiments using both ResNets and the
convolutional neural tangent kernel (CNTK). We then present a novel linear
regression framework for characterizing the impact of depth on test risk, and
show that increasing depth leads to a U-shaped test risk for the linear CNTK.
In particular, we prove that the linear CNTK corresponds to a depth-dependent
linear transformation on the original space and characterize properties of this
transformation. We then analyze over-parameterized linear regression under
arbitrary linear transformations and, in simplified settings, provably identify
the depths which minimize each of the bias and variance terms of the test risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1"&gt;Eshaan Nichani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Adityanarayanan Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[W-RST: Towards a Weighted RST-style Discourse Framework. (arXiv:2106.02658v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02658</id>
        <link href="http://arxiv.org/abs/2106.02658"/>
        <updated>2021-06-08T02:20:22.981Z</updated>
        <summary type="html"><![CDATA[Aiming for a better integration of data-driven and linguistically-inspired
approaches, we explore whether RST Nuclearity, assigning a binary assessment of
importance between text segments, can be replaced by automatically generated,
real-valued scores, in what we call a Weighted-RST framework. In particular, we
find that weighted discourse trees from auxiliary tasks can benefit key NLP
downstream applications, compared to nuclearity-centered approaches. We further
show that real-valued importance distributions partially and interestingly
align with the assessment and uncertainty of human annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1"&gt;Patrick Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02852</id>
        <link href="http://arxiv.org/abs/2106.02852"/>
        <updated>2021-06-08T02:20:22.975Z</updated>
        <summary type="html"><![CDATA[This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Variational Perspective on Diffusion-Based Generative Models and Score Matching. (arXiv:2106.02808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02808</id>
        <link href="http://arxiv.org/abs/2106.02808"/>
        <updated>2021-06-08T02:20:22.968Z</updated>
        <summary type="html"><![CDATA[Discrete-time diffusion-based generative models and score matching methods
have shown promising results in modeling high-dimensional image data. Recently,
Song et al. (2021) show that diffusion processes that transform data into noise
can be reversed via learning the score function, i.e. the gradient of the
log-density of the perturbed data. They propose to plug the learned score
function into an inverse formula to define a generative diffusion process.
Despite the empirical success, a theoretical underpinning of this procedure is
still lacking. In this work, we approach the (continuous-time) generative
diffusion directly and derive a variational framework for likelihood
estimation, which includes continuous-time normalizing flows as a special case,
and can be seen as an infinitely deep variational autoencoder. Under this
framework, we show that minimizing the score-matching loss is equivalent to
maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed
by Song et al. (2021), bridging the theoretical gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chin-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Jae Hyun Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning. (arXiv:2106.02720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02720</id>
        <link href="http://arxiv.org/abs/2106.02720"/>
        <updated>2021-06-08T02:20:22.953Z</updated>
        <summary type="html"><![CDATA[We present and analyze an algorithm for optimizing smooth and convex or
strongly convex objectives using minibatch stochastic gradient estimates. The
algorithm is optimal with respect to its dependence on both the minibatch size
and minimum expected loss simultaneously. This improves over the optimal method
of Lan (2012), which is insensitive to the minimum expected loss; over the
optimistic acceleration of Cotter et al. (2011), which has suboptimal
dependence on the minibatch size; and over the algorithm of Liu and Belkin
(2018), which is limited to least squares problems and is also similarly
suboptimal with respect to the minibatch size. Applied to interpolation
learning, the improvement over Cotter et al. and Liu and Belkin translates to a
linear, rather than square-root, parallelization speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:22.946Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images. (arXiv:2010.08115v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08115</id>
        <link href="http://arxiv.org/abs/2010.08115"/>
        <updated>2021-06-08T02:20:22.935Z</updated>
        <summary type="html"><![CDATA[The infection of respiratory coronavirus disease 2019 (COVID-19) starts with
the upper respiratory tract and as the virus grows, the infection can progress
to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is
reverse transcription polymerase chain reaction (RT-PCR), which is less
sensitive during early stages; especially if the patient is asymptomatic, which
may further cause more severe pneumonia. In this context, several deep learning
models have been proposed to identify pulmonary infections using publicly
available chest X-ray (CXR) image datasets for early diagnosis, better
treatment and quick cure. In these datasets, presence of less number of
COVID-19 positive samples compared to other classes (normal, pneumonia and
Tuberculosis) raises the challenge for unbiased learning of deep learning
models. All deep learning models opted class balancing techniques to solve this
issue; which however should be avoided in any medical diagnosis process.
Moreover, the deep learning models are also data hungry and need massive
computation resources. Therefore for quicker diagnosis, this research proposes
a novel pinball loss function based one-class support vector machine
(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples
with objectives to maximize the learning efficiency and to minimize the false
predictions. The performance of the proposed model is compared with
conventional OCSVM and existing deep learning models, and the experimental
results prove that the proposed model outperformed over state-of-the-art
methods. To validate the robustness of the proposed model, experiments are also
performed with noisy CXR images and UCI benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagabhushan_P/0/1/0/all/0/1"&gt;P. Nagabhushan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilled One-Shot Federated Learning. (arXiv:2009.07999v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07999</id>
        <link href="http://arxiv.org/abs/2009.07999"/>
        <updated>2021-06-08T02:20:22.918Z</updated>
        <summary type="html"><![CDATA[Current federated learning algorithms take tens of communication rounds
transmitting unwieldy model weights under ideal circumstances and hundreds when
data is poorly distributed. Inspired by recent work on dataset distillation and
distributed one-shot learning, we propose Distilled One-Shot Federated Learning
(DOSFL) to significantly reduce the communication cost while achieving
comparable performance. In just one round, each client distills their private
dataset, sends the synthetic data (e.g. images or sentences) to the server, and
collectively trains a global model. The distilled data look like noise and are
only useful to the specific model weights, i.e., become useless after the model
updates. With this weight-less and gradient-less design, the total
communication cost of DOSFL is up to three orders of magnitude less than FedAvg
while preserving between 93% to 99% performance of a centralized counterpart.
Afterwards, clients could switch to traditional methods such as FedAvg to
finetune the last few percent to fit personalized local models with local
datasets. Through comprehensive experiments, we show the accuracy and
communication performance of DOSFL on both vision and language tasks with
different models including CNN, LSTM, Transformer, etc. We demonstrate that an
eavesdropping attacker cannot properly train a good model using the leaked
distilled data, without knowing the initial model weights. DOSFL serves as an
inexpensive method to quickly converge on a performant pre-trained model with
less than 0.1% communication cost of traditional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanlin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;George Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiyao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dapeng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Normal Training for Deep Learning Models. (arXiv:2106.02925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02925</id>
        <link href="http://arxiv.org/abs/2106.02925"/>
        <updated>2021-06-08T02:20:22.909Z</updated>
        <summary type="html"><![CDATA[Despite the predominant use of first-order methods for training deep learning
models, second-order methods, and in particular, natural gradient methods,
remain of interest because of their potential for accelerating training through
the use of curvature information. Several methods with non-diagonal
preconditioning matrices, including KFAC and Shampoo, have been proposed and
shown to be effective. Based on the so-called tensor normal (TN) distribution,
we propose and analyze a brand new approximate natural gradient method, Tensor
Normal Training (TNT), which like Shampoo, only requires knowledge on the shape
of the training parameters. By approximating the probabilistically based Fisher
matrix, as opposed to the empirical Fisher matrix, our method uses the
layer-wise covariance of the sampling based gradient as the pre-conditioning
matrix. Moreover, the assumption that the sampling-based (tensor) gradient
follows a TN distribution, ensures that its covariance has a Kronecker
separable structure, which leads to a tractable approximation to the Fisher
matrix. Consequently, TNT's memory requirements and per-iteration computational
costs are only slightly higher than those for first-order methods. In our
experiments, TNT exhibited superior optimization performance to KFAC and
Shampoo, and to state-of-the-art first-order methods. Moreover, TNT
demonstrated its ability to generalize as well as these first-order methods,
using fewer epochs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldfarb_D/0/1/0/all/0/1"&gt;Donald Goldfarb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic Refinery: Cost-Aware Data Representation for Machine Learning on Network Traffic. (arXiv:2010.14605v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14605</id>
        <link href="http://arxiv.org/abs/2010.14605"/>
        <updated>2021-06-08T02:20:22.898Z</updated>
        <summary type="html"><![CDATA[Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bronzino_F/0/1/0/all/0/1"&gt;Francesco Bronzino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_P/0/1/0/all/0/1"&gt;Paul Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoubi_S/0/1/0/all/0/1"&gt;Sara Ayoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyojoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teixeira_R/0/1/0/all/0/1"&gt;Renata Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feamster_N/0/1/0/all/0/1"&gt;Nick Feamster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding Time-resolved EEG Motor Imagery Signals. (arXiv:2006.08924v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08924</id>
        <link href="http://arxiv.org/abs/2006.08924"/>
        <updated>2021-06-08T02:20:22.781Z</updated>
        <summary type="html"><![CDATA[Towards developing effective and efficient brain-computer interface (BCI)
systems, precise decoding of brain activity measured by electroencephalogram
(EEG), is highly demanded. Traditional works classify EEG signals without
considering the topological relationship among electrodes. However,
neuroscience research has increasingly emphasized network patterns of brain
dynamics. Thus, the Euclidean structure of electrodes might not adequately
reflect the interaction between signals. To fill the gap, a novel deep learning
framework based on the graph convolutional neural networks (GCNs) was presented
to enhance the decoding performance of raw EEG signals during different types
of motor imagery (MI) tasks while cooperating with the functional topological
relationship of electrodes. Based on the absolute Pearson's matrix of overall
signals, the graph Laplacian of EEG electrodes was built up. The GCNs-Net
constructed by graph convolutional layers learns the generalized features. The
followed pooling layers reduce dimensionality, and the fully-connected softmax
layer derives the final prediction. The introduced approach has been shown to
converge for both personalized and group-wise predictions. It has achieved the
highest averaged accuracy, 93.056% and 88.57% (PhysioNet Dataset), 96.24% and
80.89% (High Gamma Dataset), at the subject and group level, respectively,
compared with existing studies, which suggests adaptability and robustness to
individual variability. Moreover, the performance was stably reproducible among
repetitive experiments for cross-validation. To conclude, the GCNs-Net filters
EEG signals based on the functional topological relationship, which manages to
decode relevant features for brain motor imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Tsetlin Machines for the IDENTITY- and NOT Operators. (arXiv:2007.14268v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14268</id>
        <link href="http://arxiv.org/abs/2007.14268"/>
        <updated>2021-06-08T02:20:22.770Z</updated>
        <summary type="html"><![CDATA[The Tsetlin Machine (TM) is a recent machine learning algorithm with several
distinct properties, such as interpretability, simplicity, and
hardware-friendliness. Although numerous empirical evaluations report on its
performance, the mathematical analysis of its convergence is still open. In
this article, we analyze the convergence of the TM with only one clause
involved for classification. More specifically, we examine two basic logical
operators, namely, the "IDENTITY"- and "NOT" operators. Our analysis reveals
that the TM, with just one clause, can converge correctly to the intended
logical operator, learning from training data over an infinite time horizon.
Besides, it can capture arbitrarily rare patterns and select the most accurate
one when two candidate patterns are incompatible, by configuring a granularity
parameter. The analysis of the convergence of the two basic operators lays the
foundation for analyzing other logical operators. These analyses altogether,
from a mathematical perspective, provide new insights on why TMs have obtained
state-of-the-art performance on several pattern recognition problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Lei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1"&gt;Ole-Christoffer Granmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1"&gt;Morten Goodwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02994</id>
        <link href="http://arxiv.org/abs/2106.02994"/>
        <updated>2021-06-08T02:20:22.729Z</updated>
        <summary type="html"><![CDATA[We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1"&gt;Safa Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02914</id>
        <link href="http://arxiv.org/abs/2106.02914"/>
        <updated>2021-06-08T02:20:22.703Z</updated>
        <summary type="html"><![CDATA[Pruning is a model compression method that removes redundant parameters in
deep neural networks (DNNs) while maintaining accuracy. Most available filter
pruning methods require complex treatments such as iterative pruning, features
statistics/ranking, or additional optimization designs in the training process.
In this paper, we propose a simple and effective regularization strategy from a
new perspective of evolution of features, which we call feature flow
regularization (FFR), for improving structured sparsity and filter pruning in
DNNs. Specifically, FFR imposes controls on the gradient and curvature of
feature flow along the neural network, which implicitly increases the sparsity
of the parameters. The principle behind FFR is that coherent and smooth
evolution of features will lead to an efficient network that avoids redundant
parameters. The high structured sparsity obtained from FFR enables us to prune
filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and
Tiny ImageNet datasets demonstrate that FFR can significantly improve both
unstructured and structured sparsity. Our pruning results in terms of reduction
of parameters and FLOPs are comparable to or even better than those of
state-of-the-art pruning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luchan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yang Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Routines for Effective Off-Policy Reinforcement Learning. (arXiv:2106.02943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02943</id>
        <link href="http://arxiv.org/abs/2106.02943"/>
        <updated>2021-06-08T02:20:22.697Z</updated>
        <summary type="html"><![CDATA[The performance of reinforcement learning depends upon designing an
appropriate action space, where the effect of each action is measurable, yet,
granular enough to permit flexible behavior. So far, this process involved
non-trivial user choices in terms of the available actions and their execution
frequency. We propose a novel framework for reinforcement learning that
effectively lifts such constraints. Within our framework, agents learn
effective behavior over a routine space: a new, higher-level action space,
where each routine represents a set of 'equivalent' sequences of granular
actions with arbitrary length. Our routine space is learned end-to-end to
facilitate the accomplishment of underlying off-policy reinforcement learning
objectives. We apply our framework to two state-of-the-art off-policy
algorithms and show that the resulting agents obtain relevant performance
improvements while requiring fewer interactions with the environment per
episode, improving computational efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_E/0/1/0/all/0/1"&gt;Edoardo Cetin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celiktutan_O/0/1/0/all/0/1"&gt;Oya Celiktutan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Weighted Automata for Approximate Minimization in Language Modelling. (arXiv:2106.02965v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02965</id>
        <link href="http://arxiv.org/abs/2106.02965"/>
        <updated>2021-06-08T02:20:22.691Z</updated>
        <summary type="html"><![CDATA[In this paper we study the approximate minimization problem for language
modelling. We assume we are given some language model as a black box. The
objective is to obtain a weighted finite automaton (WFA) that fits within a
given size constraint and which mimics the behaviour of the original model
while minimizing some notion of distance between the black box and the
extracted WFA. We provide an algorithm for the approximate minimization of
black boxes trained for language modelling of sequential data over a one-letter
alphabet. By reformulating the problem in terms of Hankel matrices, we leverage
classical results on the approximation of Hankel operators, namely the
celebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral
norm to measure the distance between the black box and the WFA. We provide
theoretical guarantees to study the potentially infinite-rank Hankel matrix of
the black box, without accessing the training data, and we prove that our
method returns an asymptotically-optimal approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lacroce_C/0/1/0/all/0/1"&gt;Clara Lacroce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1"&gt;Prakash Panangaden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards High Fidelity Face Relighting with Realistic Shadows. (arXiv:2104.00825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00825</id>
        <link href="http://arxiv.org/abs/2104.00825"/>
        <updated>2021-06-08T02:20:22.685Z</updated>
        <summary type="html"><![CDATA[Existing face relighting methods often struggle with two problems:
maintaining the local facial details of the subject and accurately removing and
synthesizing shadows in the relit image, especially hard shadows. We propose a
novel deep face relighting method that addresses both problems. Our method
learns to predict the ratio (quotient) image between a source image and the
target image with the desired lighting, allowing us to relight the image while
maintaining the local facial details. During training, our model also learns to
accurately modify shadows by using estimated shadow masks to emphasize on the
high-contrast shadow borders. Furthermore, we introduce a method to use the
shadow mask to estimate the ambient light intensity in an image, and are thus
able to leverage multiple datasets during training with different global
lighting intensities. With quantitative and qualitative evaluations on the
Multi-PIE and FFHQ datasets, we demonstrate that our proposed method faithfully
maintains the local facial details of the subject and can accurately handle
hard shadows while achieving state-of-the-art face relighting performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_A/0/1/0/all/0/1"&gt;Andrew Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ze Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1"&gt;Michel Sarkis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1"&gt;Ning Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yiying Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:22.677Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning. (arXiv:2106.02705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02705</id>
        <link href="http://arxiv.org/abs/2106.02705"/>
        <updated>2021-06-08T02:20:22.670Z</updated>
        <summary type="html"><![CDATA[As multi-task models gain popularity in a wider range of machine learning
applications, it is becoming increasingly important for practitioners to
understand the fairness implications associated with those models. Most
existing fairness literature focuses on learning a single task more fairly,
while how ML fairness interacts with multiple tasks in the joint learning
setting is largely under-explored. In this paper, we are concerned with how
group fairness (e.g., equal opportunity, equalized odds) as an ML fairness
concept plays out in the multi-task scenario. In multi-task learning, several
tasks are learned jointly to exploit task correlations for a more efficient
inductive transfer. This presents a multi-dimensional Pareto frontier on (1)
the trade-off between group fairness and accuracy with respect to each task, as
well as (2) the trade-offs across multiple tasks. We aim to provide a deeper
understanding on how group fairness interacts with accuracy in multi-task
learning, and we show that traditional approaches that mainly focus on
optimizing the Pareto frontier of multi-task accuracy might not perform well on
fairness goals. We propose a new set of metrics to better capture the
multi-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely
presented in a multi-task learning setting. We further propose a
Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task
learning. Experiments on several real-world datasets demonstrate the
effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1"&gt;Alex Beutel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1"&gt;Flavien Prost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Optimization of Fluid Systems at Varying Length Scales. (arXiv:2105.13553v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13553</id>
        <link href="http://arxiv.org/abs/2105.13553"/>
        <updated>2021-06-08T02:20:22.664Z</updated>
        <summary type="html"><![CDATA[Autonomous optimization is a process by which hardware conditions are
discovered that generate an optimized experimental product without the guidance
of a domain expert. We design an autonomous optimization framework to discover
the experimental conditions within fluid systems that generate discrete and
uniform droplet patterns. Generating discrete and uniform droplets requires
high-precision control over the experimental conditions of a fluid system.
Fluid stream instabilities, such as Rayleigh-Plateau instability and capillary
instability, drive the separation of a flow into individual droplets. However,
because this phenomenon leverages an instability, by nature the hardware must
be precisely tuned to achieve uniform, repeatable droplets. Typically this
requires a domain expert in the loop and constant re-tuning depending on the
hardware configuration and liquid precursor selection. Herein, we propose a
computer vision-driven Bayesian optimization framework to discover the precise
hardware conditions that generate uniform, reproducible droplets with the
desired features, leveraging flow instability without a domain expert in the
loop. This framework is validated on two fluid systems, at the micrometer and
millimeter length scales, using microfluidic and inkjet systems, respectively,
indicating the application breadth of this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siemenn_A/0/1/0/all/0/1"&gt;Alexander E. Siemenn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaulsky_E/0/1/0/all/0/1"&gt;Evyatar Shaulsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1"&gt;Matthew Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buonassisi_T/0/1/0/all/0/1"&gt;Tonio Buonassisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashmi_S/0/1/0/all/0/1"&gt;Sara M. Hashmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating SGD with Data Ordering Attacks. (arXiv:2104.09667v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09667</id>
        <link href="http://arxiv.org/abs/2104.09667"/>
        <updated>2021-06-08T02:20:22.657Z</updated>
        <summary type="html"><![CDATA[Machine learning is vulnerable to a wide variety of attacks. It is now well
understood that by changing the underlying data distribution, an adversary can
poison the model trained with it or introduce backdoors. In this paper we
present a novel class of training-time attacks that require no changes to the
underlying dataset or model architecture, but instead only change the order in
which data are supplied to the model. In particular, we find that the attacker
can either prevent the model from learning, or poison it to learn behaviours
specified by the attacker. Furthermore, we find that even a single
adversarially-ordered epoch can be enough to slow down model learning, or even
to reset all of the learning progress. Indeed, the attacks presented here are
not specific to the model or dataset, but rather target the stochastic nature
of modern learning procedures. We extensively evaluate our attacks on computer
vision and natural language benchmarks to find that the adversary can disrupt
model training and even introduce backdoors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1"&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Supervised Method without supervision for foreground segmentation. (arXiv:2011.07954v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07954</id>
        <link href="http://arxiv.org/abs/2011.07954"/>
        <updated>2021-06-08T02:20:22.640Z</updated>
        <summary type="html"><![CDATA[Neural networks are a powerful framework for foreground segmentation in video
acquired by static cameras, segmenting moving objects from the background in a
robust way in various challenging scenarios. The premier methods are those
based on supervision requiring a final training stage on a database of tens to
hundreds of manually segmented images from the specific static camera. In this
work, we propose a method to automatically create an "artificial" database that
is sufficient for training the supervised methods so that it performs better
than current unsupervised methods. It is based on combining a weak foreground
segmenter, compared to the supervised method, to extract suitable objects from
the training images and randomly inserting these objects back into a background
image. Test results are shown on the test sequences in CDnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassel_L/0/1/0/all/0/1"&gt;Levi Kassel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1"&gt;Michael Werman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression. (arXiv:2106.02875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02875</id>
        <link href="http://arxiv.org/abs/2106.02875"/>
        <updated>2021-06-08T02:20:22.623Z</updated>
        <summary type="html"><![CDATA[Modeling a system's temporal behaviour in reaction to external stimuli is a
fundamental problem in many areas. Pure Machine Learning (ML) approaches often
fail in the small sample regime and cannot provide actionable insights beyond
predictions. A promising modification has been to incorporate expert domain
knowledge into ML models. The application we consider is predicting the
progression of disease under medications, where a plethora of domain knowledge
is available from pharmacology. Pharmacological models describe the dynamics of
carefully-chosen medically meaningful variables in terms of systems of Ordinary
Differential Equations (ODEs). However, these models only describe a limited
collection of variables, and these variables are often not observable in
clinical environments. To close this gap, we propose the latent hybridisation
model (LHM) that integrates a system of expert-designed ODEs with
machine-learned Neural ODEs to fully describe the dynamics of the system and to
link the expert and latent variables to observable quantities. We evaluated LHM
on synthetic data as well as real-world intensive care data of COVID-19
patients. LHM consistently outperforms previous works, especially when few
training samples are available such as at the beginning of the pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zame_W/0/1/0/all/0/1"&gt;William R. Zame&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleuren_L/0/1/0/all/0/1"&gt;Lucas M. Fleuren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elbers_P/0/1/0/all/0/1"&gt;Paul Elbers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries. (arXiv:2104.08382v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08382</id>
        <link href="http://arxiv.org/abs/2104.08382"/>
        <updated>2021-06-08T02:20:22.603Z</updated>
        <summary type="html"><![CDATA[Understanding the fundamental limits of robust supervised learning has
emerged as a problem of immense interest, from both practical and theoretical
standpoints. In particular, it is critical to determine classifier-agnostic
bounds on the training loss to establish when learning is possible. In this
paper, we determine optimal lower bounds on the cross-entropy loss in the
presence of test-time adversaries, along with the corresponding optimal
classification outputs. Our formulation of the bound as a solution to an
optimization problem is general enough to encompass any loss function depending
on soft classifier outputs. We also propose and provide a proof of correctness
for a bespoke algorithm to compute this lower bound efficiently, allowing us to
determine lower bounds for multiple practical datasets of interest. We use our
lower bounds as a diagnostic tool to determine the effectiveness of current
robust training methods and find a gap from optimality at larger budgets.
Finally, we investigate the possibility of using of optimal classification
outputs as soft labels to empirically improve robust training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1"&gt;Arjun Nitin Bhagoji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cullina_D/0/1/0/all/0/1"&gt;Daniel Cullina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02835</id>
        <link href="http://arxiv.org/abs/2106.02835"/>
        <updated>2021-06-08T02:20:22.589Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is an important but challenging task
in many scientific fields. Recently, NOTEARS [Zheng et al., 2018] formulates
the causal structure learning problem as a continuous optimization problem
using least-square loss with an acyclicity constraint. Though the least-square
loss function is well justified under the standard Gaussian noise assumption,
it is limited if the assumption does not hold. In this work, we theoretically
show that the violation of the Gaussian noise assumption will hinder the causal
direction identification, making the causal orientation fully determined by the
causal strength as well as the variances of noises in the linear case and the
noises of strong non-Gaussianity in the nonlinear case. Consequently, we
propose a more general entropy-based loss that is theoretically consistent with
the likelihood score under any noise distribution. We run extensive empirical
evaluations on both synthetic data and real-world data to validate the
effectiveness of the proposed method and show that our method achieves the best
in Structure Hamming Distance, False Discovery Rate, and True Positive Rate
matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1"&gt;Jie Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Hawkes Processes for Discovering Time-evolving Communities' States behind Diffusion Processes. (arXiv:2105.11152v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11152</id>
        <link href="http://arxiv.org/abs/2105.11152"/>
        <updated>2021-06-08T02:20:22.582Z</updated>
        <summary type="html"><![CDATA[Sequences of events including infectious disease outbreaks, social network
activities, and crimes are ubiquitous and the data on such events carry
essential information about the underlying diffusion processes between
communities (e.g., regions, online user groups). Modeling diffusion processes
and predicting future events are crucial in many applications including
epidemic control, viral marketing, and predictive policing. Hawkes processes
offer a central tool for modeling the diffusion processes, in which the
influence from the past events is described by the triggering kernel. However,
the triggering kernel parameters, which govern how each community is influenced
by the past events, are assumed to be static over time. In the real world, the
diffusion processes depend not only on the influences from the past, but also
the current (time-evolving) states of the communities, e.g., people's awareness
of the disease and people's current interests. In this paper, we propose a
novel Hawkes process model that is able to capture the underlying dynamics of
community states behind the diffusion processes and predict the occurrences of
events based on the dynamics. Specifically, we model the latent dynamic
function that encodes these hidden dynamics by a mixture of neural networks.
Then we design the triggering kernel using the latent dynamic function and its
integral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a
flexible way to learn complex representations of the time-evolving communities'
states, while at the same time it allows to computing the exact likelihood,
which makes parameter learning tractable. Extensive experiments on four
real-world event datasets show that DHP outperforms five widely adopted methods
for event prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okawa_M/0/1/0/all/0/1"&gt;Maya Okawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1"&gt;Tomoharu Iwata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1"&gt;Yusuke Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_H/0/1/0/all/0/1"&gt;Hiroyuki Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurashima_T/0/1/0/all/0/1"&gt;Takeshi Kurashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Causal Explanations for Graph Neural Networks. (arXiv:2104.06643v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06643</id>
        <link href="http://arxiv.org/abs/2104.06643"/>
        <updated>2021-06-08T02:20:22.575Z</updated>
        <summary type="html"><![CDATA[This paper presents Gem, a model-agnostic approach for providing
interpretable explanations for any GNNs on various graph learning tasks.
Specifically, we formulate the problem of providing explanations for the
decisions of GNNs as a causal learning task. Then we train a causal explanation
model equipped with a loss function based on Granger causality. Different from
existing explainers for GNNs, Gem explains GNNs on graph-structured data from a
causal perspective. It has better generalization ability as it has no
requirements on the internal structure of the GNNs or prior knowledge on the
graph learning tasks. In addition, Gem, once trained, can be used to explain
the target GNN very quickly. Our theoretical analysis shows that several recent
explainers fall into a unified framework of additive feature attribution
methods. Experimental results on synthetic and real-world datasets show that
Gem achieves a relative increase of the explanation accuracy by up to $30\%$
and speeds up the explanation process by up to $110\times$ as compared to its
state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wanyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1"&gt;Hao Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baochun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Training of Tree Ensembles over Continuous Data. (arXiv:2106.02769v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02769</id>
        <link href="http://arxiv.org/abs/2106.02769"/>
        <updated>2021-06-08T02:20:22.556Z</updated>
        <summary type="html"><![CDATA[Most existing Secure Multi-Party Computation (MPC) protocols for
privacy-preserving training of decision trees over distributed data assume that
the features are categorical. In real-life applications, features are often
numerical. The standard ``in the clear'' algorithm to grow decision trees on
data with continuous values requires sorting of training examples for each
feature in the quest for an optimal cut-point in the range of feature values in
each node. Sorting is an expensive operation in MPC, hence finding secure
protocols that avoid such an expensive step is a relevant problem in
privacy-preserving machine learning. In this paper we propose three more
efficient alternatives for secure training of decision tree based models on
data with continuous features, namely: (1) secure discretization of the data,
followed by secure training of a decision tree over the discretized data; (2)
secure discretization of the data, followed by secure training of a random
forest over the discretized data; and (3) secure training of extremely
randomized trees (``extra-trees'') on the original data. Approaches (2) and (3)
both involve randomizing feature choices. In addition, in approach (3)
cut-points are chosen randomly as well, thereby alleviating the need to sort or
to discretize the data up front. We implemented all proposed solutions in the
semi-honest setting with additive secret sharing based MPC. In addition to
mathematically proving that all proposed approaches are correct and secure, we
experimentally evaluated and compared them in terms of classification accuracy
and runtime. We privately train tree ensembles over data sets with 1000s of
instances or features in a few minutes, with accuracies that are at par with
those obtained in the clear. This makes our solution orders of magnitude more
efficient than the existing approaches, which are based on oblivious sorting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1"&gt;Samuel Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_C/0/1/0/all/0/1"&gt;Chaitali Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cock_M/0/1/0/all/0/1"&gt;Martine De Cock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1"&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melanson_D/0/1/0/all/0/1"&gt;David Melanson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1"&gt;Anderson C. A. Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Railsback_D/0/1/0/all/0/1"&gt;Davis Railsback&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianwei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Neural Architecture Optimization for Keyword Spotting. (arXiv:2106.02738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02738</id>
        <link href="http://arxiv.org/abs/2106.02738"/>
        <updated>2021-06-08T02:20:22.549Z</updated>
        <summary type="html"><![CDATA[Keyword spotting aims to identify specific keyword audio utterances. In
recent years, deep convolutional neural networks have been widely utilized in
keyword spotting systems. However, their model architectures are mainly based
on off-the shelfbackbones such as VGG-Net or ResNet, instead of specially
designed for the task. In this paper, we utilize neural architecture search to
design convolutional neural network models that can boost the performance of
keyword spotting while maintaining an acceptable memory footprint.
Specifically, we search the model operators and their connections in a specific
search space with Encoder-Decoder neural architecture optimization. Extensive
evaluations on Google's Speech Commands Dataset show that the model
architecture searched by our approach achieves a state-of-the-art accuracy of
over 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1"&gt;Tong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Learning for Cooperative Games, with Applications to Feature/Data/Model Valuations. (arXiv:2106.02938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02938</id>
        <link href="http://arxiv.org/abs/2106.02938"/>
        <updated>2021-06-08T02:20:22.543Z</updated>
        <summary type="html"><![CDATA[Valuation problems, such as attribution-based feature interpretation, data
valuation and model valuation for ensembles, become increasingly more important
in many machine learning applications. Such problems are commonly solved by
well-known game-theoretic criteria, such as Shapley value or Banzhaf index. In
this work, we present a novel energy-based treatment for cooperative games,
with a theoretical justification by the maximum entropy framework.
Surprisingly, by conducting variational inference of the energy-based model, we
recover various game-theoretic valuation criteria, such as Shapley value and
Banzhaf index, through conducting one-step gradient ascent for maximizing the
mean-field ELBO objective. This observation also verifies the rationality of
existing criteria, as they are all trying to decouple the correlations among
the players through the mean-field approach. By running gradient ascent for
multiple steps, we achieve a trajectory of the valuations, among which we
define the valuation with the best conceivable decoupling error as the
Variational Index. We experimentally demonstrate that the proposed Variational
Index enjoys intriguing properties on certain synthetic and real-world
valuation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1"&gt;Yatao Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tingyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaxiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discrete Variational Derivation of Accelerated Methods in Optimization. (arXiv:2106.02700v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.02700</id>
        <link href="http://arxiv.org/abs/2106.02700"/>
        <updated>2021-06-08T02:20:22.537Z</updated>
        <summary type="html"><![CDATA[Many of the new developments in machine learning are connected with
gradient-based optimization methods. Recently, these methods have been studied
using a variational perspective. This has opened up the possibility of
introducing variational and symplectic integration methods using geometric
integrators. In particular, in this paper, we introduce variational integrators
which allow us to derive different methods for optimization. Using both,
Hamilton's principle and Lagrange-d'Alembert's, we derive two families of
optimization methods in one-to-one correspondence that generalize Polyak's
heavy ball and the well known Nesterov accelerated gradient method, mimicking
the behavior of the latter which reduces the oscillations of typical momentum
methods. However, since the systems considered are explicitly time-dependent,
the preservation of symplecticity of autonomous systems occurs here solely on
the fibers. Several experiments exemplify the result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Campos_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric M. Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mahillo_A/0/1/0/all/0/1"&gt;Alejandro Mahillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diego_D/0/1/0/all/0/1"&gt;David Mart&amp;#xed;n de Diego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-08T02:20:22.530Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-06-08T02:20:22.512Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Deep Learning under the Fairness Lens. (arXiv:2106.02674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02674</id>
        <link href="http://arxiv.org/abs/2106.02674"/>
        <updated>2021-06-08T02:20:22.505Z</updated>
        <summary type="html"><![CDATA[Differential Privacy (DP) is an important privacy-enhancing technology for
private machine learning systems. It allows to measure and bound the risk
associated with an individual participation in a computation. However, it was
recently observed that DP learning systems may exacerbate bias and unfairness
for different groups of individuals. This paper builds on these important
observations and sheds light on the causes of the disparate impacts arising in
the problem of differentially private empirical risk minimization. It focuses
on the accuracy disparity arising among groups of individuals in two
well-studied DP learning methods: output perturbation and differentially
private stochastic gradient descent. The paper analyzes which data and model
properties are responsible for the disproportionate impacts, why these aspects
are affecting different groups disproportionately and proposes guidelines to
mitigate these effects. The proposed approach is evaluated on several datasets
and settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cuong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_M/0/1/0/all/0/1"&gt;My H. Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Task Adaptation using Natural Language. (arXiv:2106.02972v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02972</id>
        <link href="http://arxiv.org/abs/2106.02972"/>
        <updated>2021-06-08T02:20:22.499Z</updated>
        <summary type="html"><![CDATA[Imitation learning and instruction-following are two common approaches to
communicate a user's intent to a learning agent. However, as the complexity of
tasks grows, it could be beneficial to use both demonstrations and language to
communicate with an agent. In this work, we propose a novel setting where an
agent is given both a demonstration and a description, and must combine
information from both the modalities. Specifically, given a demonstration for a
task (the source task), and a natural language description of the differences
between the demonstrated task and a related but different task (the target
task), our goal is to train an agent to complete the target task in a zero-shot
setting, that is, without any demonstrations for the target task. To this end,
we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a
source demonstration and a linguistic description of how the target task
differs, learns to output a reward / value function that accurately describes
the target task. Our experiments show that on a diverse set of adaptations, our
approach is able to complete more than 95% of target tasks when using
template-based descriptions, and more than 70% when using free-form natural
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven discovery of interacting particle systems using Gaussian processes. (arXiv:2106.02735v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02735</id>
        <link href="http://arxiv.org/abs/2106.02735"/>
        <updated>2021-06-08T02:20:22.491Z</updated>
        <summary type="html"><![CDATA[Interacting particle or agent systems that display a rich variety of
collection motions are ubiquitous in science and engineering. A fundamental and
challenging goal is to understand the link between individual interaction rules
and collective behaviors. In this paper, we study the data-driven discovery of
distance-based interaction laws in second-order interacting particle systems.
We propose a learning approach that models the latent interaction kernel
functions as Gaussian processes, which can simultaneously fulfill two inference
goals: one is the nonparametric inference of interaction kernel function with
the pointwise uncertainty quantification, and the other one is the inference of
unknown parameters in the non-collective forces of the system. We formulate
learning interaction kernel functions as a statistical inverse problem and
provide a detailed analysis of recoverability conditions, establishing that a
coercivity condition is sufficient for recoverability. We provide a
finite-sample analysis, showing that our posterior mean estimator converges at
an optimal rate equal to the one in the classical 1-dimensional Kernel Ridge
regression. Numerical results on systems that exhibit different collective
behaviors demonstrate efficient learning of our approach from scarce noisy
trajectory data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinchao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yunxiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sui Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Apophis Asteroid Flyby Optimal Trajectories and Data Fusion of Earth-Apophis Mission Launch Windows using Deep Neural Networks. (arXiv:2104.06249v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06249</id>
        <link href="http://arxiv.org/abs/2104.06249"/>
        <updated>2021-06-08T02:20:22.483Z</updated>
        <summary type="html"><![CDATA[In recent years, understanding asteroids has shifted from light worlds to
geological worlds by exploring modern spacecraft and advanced radar and
telescopic surveys. However, flyby in 2029 will be an opportunity to conduct an
internal geophysical study and test the current hypothesis on the effects of
tidal forces on asteroids. The Earth-Apophis mission is driven by additional
factors and scientific goals beyond the unique opportunity for natural
experimentation. However, the internal geophysical structures remain largely
unknown. Understanding the strength and internal integrity of asteroids is not
just a matter of scientific curiosity. It is a practical imperative to advance
knowledge for planetary defense against the possibility of an asteroid impact.
This paper presents a conceptual robotics system required for efficiency at
every stage from entry to post-landing and for asteroid monitoring. In short,
asteroid surveillance missions are futuristic frontiers, with the potential for
technological growth that could revolutionize space exploration. Advanced space
technologies and robotic systems are needed to minimize risk and prepare these
technologies for future missions. A neural network model is implemented to
track and predict asteroids' orbits. Advanced algorithms are also needed to
numerically predict orbital events to minimize error]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ntumba_M/0/1/0/all/0/1"&gt;Manuel Ntumba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gore_S/0/1/0/all/0/1"&gt;Saurabh Gore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Awanyo_J/0/1/0/all/0/1"&gt;Jean-Baptiste Awanyo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Classification of Very Large Images with Tiny Objects. (arXiv:2106.02694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02694</id>
        <link href="http://arxiv.org/abs/2106.02694"/>
        <updated>2021-06-08T02:20:22.464Z</updated>
        <summary type="html"><![CDATA[An increasing number of applications in the computer vision domain,
specially, in medical imaging and remote sensing, are challenging when the goal
is to classify very large images with tiny objects. More specifically, these
type of classification tasks face two key challenges: $i$) the size of the
input image in the target dataset is usually in the order of megapixels,
however, existing deep architectures do not easily operate on such big images
due to memory constraints, consequently, we seek a memory-efficient method to
process these images; and $ii$) only a small fraction of the input images are
informative of the label of interest, resulting in low region of interest (ROI)
to image ratio. However, most of the current convolutional neural networks
(CNNs) are designed for image classification datasets that have relatively
large ROIs and small image size (sub-megapixel). Existing approaches have
addressed these two challenges in isolation. We present an end-to-end CNN model
termed Zoom-In network that leverages hierarchical attention sampling for
classification of large images with tiny objects using a single GPU. We
evaluate our method on two large-image datasets and one gigapixel dataset.
Experimental results show that our model achieves higher accuracy than existing
methods while requiring less computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fanjie Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gradient Fields for Molecular Conformation Generation. (arXiv:2105.03902v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03902</id>
        <link href="http://arxiv.org/abs/2105.03902"/>
        <updated>2021-06-08T02:20:22.457Z</updated>
        <summary type="html"><![CDATA[We study a fundamental problem in computational chemistry known as molecular
conformation generation, trying to predict stable 3D structures from 2D
molecular graphs. Existing machine learning approaches usually first predict
distances between atoms and then generate a 3D structure satisfying the
distances, where noise in predicted distances may induce extra errors during 3D
coordinate generation. Inspired by the traditional force field methods for
molecular dynamics simulation, in this paper, we propose a novel approach
called ConfGF by directly estimating the gradient fields of the log density of
atomic coordinates. The estimated gradient fields allow directly generating
stable conformations via Langevin dynamics. However, the problem is very
challenging as the gradient fields are roto-translation equivariant. We notice
that estimating the gradient fields of atomic coordinates can be translated to
estimating the gradient fields of interatomic distances, and hence develop a
novel algorithm based on recent score-based generative models to effectively
estimate these gradients. Experimental results across multiple tasks show that
ConfGF outperforms previous state-of-the-art baselines by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shitong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology. (arXiv:2104.07365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07365</id>
        <link href="http://arxiv.org/abs/2104.07365"/>
        <updated>2021-06-08T02:20:22.450Z</updated>
        <summary type="html"><![CDATA[The convergence speed of machine learning models trained with Federated
Learning is significantly affected by non-independent and identically
distributed (non-IID) data partitions, even more so in a fully decentralized
setting without a central server. In this paper, we show that the impact of
local class bias, an important type of data non-IIDness, can be significantly
reduced by carefully designing the underlying communication topology. We
present D-Cliques, a novel topology that reduces gradient bias by grouping
nodes in interconnected cliques such that the local joint distribution in a
clique is representative of the global class distribution. We also show how to
adapt the updates of decentralized SGD to obtain unbiased gradients and
implement an effective momentum with D-Cliques. Our empirical evaluation on
MNIST and CIFAR10 demonstrates that our approach provides similar convergence
speed as a fully-connected topology with a significant reduction in the number
of edges and messages. In a 1000-node topology, D-Cliques requires 98% less
edges and 96% less total messages, with further possible gains using a
small-world topology across cliques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1"&gt;Anne-Marie Kermarrec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavoie_E/0/1/0/all/0/1"&gt;Erick Lavoie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Entropy Subspace Clustering Network. (arXiv:2012.03176v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03176</id>
        <link href="http://arxiv.org/abs/2012.03176"/>
        <updated>2021-06-08T02:20:22.443Z</updated>
        <summary type="html"><![CDATA[Deep subspace clustering networks have attracted much attention in subspace
clustering, in which an auto-encoder non-linearly maps the input data into a
latent space, and a fully connected layer named self-expressiveness module is
introduced to learn the affinity matrix via a typical regularization term
(e.g., sparse or low-rank). However, the adopted regularization terms ignore
the connectivity within each subspace, limiting their clustering performance.
In addition, the adopted framework suffers from the coupling issue between the
auto-encoder module and the self-expressiveness module, making the network
training non-trivial. To tackle these two issues, we propose a novel deep
subspace clustering method named Maximum Entropy Subspace Clustering Network
(MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix
to promote the connectivity within each subspace, in which its elements
corresponding to the same subspace are uniformly and densely distributed.
Furthermore, we design a novel framework to explicitly decouple the
auto-encoder module and the self-expressiveness module. We also theoretically
prove that the learned affinity matrix satisfies the block-diagonal property
under the independent subspaces. Extensive quantitative and qualitative results
on commonly used benchmark datasets validate MESC-Net significantly outperforms
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhihao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yuheng Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingfu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chromatic and spatial analysis of one-pixel attacks against an image classifier. (arXiv:2105.13771v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13771</id>
        <link href="http://arxiv.org/abs/2105.13771"/>
        <updated>2021-06-08T02:20:22.436Z</updated>
        <summary type="html"><![CDATA[One-pixel attack is a curious way of deceiving neural network classifier by
changing only one pixel in the input image. The full potential and boundaries
of this attack method are not yet fully understood. In this research, the
successful and unsuccessful attacks are studied in more detail to illustrate
the working mechanisms of a one-pixel attack created using differential
evolution. The data comes from our earlier studies where we applied the attack
against medical imaging. We used a real breast cancer tissue dataset and a real
classifier as the attack target. This research presents ways to analyze
chromatic and spatial distributions of one-pixel attacks. In addition, we
present one-pixel attack confidence maps to illustrate the behavior of the
target classifier. We show that the more effective attacks change the color of
the pixel more, and that the successful attacks are situated at the center of
the images. This kind of analysis is not only useful for understanding the
behavior of the attack but also the qualities of the classifying neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alatalo_J/0/1/0/all/0/1"&gt;Janne Alatalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Pay Less Attention in Vision Transformers. (arXiv:2105.14217v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14217</id>
        <link href="http://arxiv.org/abs/2105.14217"/>
        <updated>2021-06-08T02:20:22.415Z</updated>
        <summary type="html"><![CDATA[Transformers have become one of the dominant architectures in deep learning,
particularly as a powerful alternative to convolutional neural networks (CNNs)
in computer vision. However, Transformer training and inference in previous
works can be prohibitively expensive due to the quadratic complexity of
self-attention over a long sequence of representations, especially for
high-resolution dense prediction tasks. To this end, we present a novel Less
attention vIsion Transformer (LIT), building upon the fact that convolutions,
fully-connected (FC) layers, and self-attentions have almost equivalent
mathematical expressions for processing image patch sequences. Specifically, we
propose a hierarchical Transformer where we use pure multi-layer perceptrons
(MLPs) to encode rich local patterns in the early stages while applying
self-attention modules to capture longer dependencies in deeper layers.
Moreover, we further propose a learned deformable token merging module to
adaptively fuse informative patches in a non-uniform manner. The proposed LIT
achieves promising performance on image recognition tasks, including image
classification, object detection and instance segmentation, serving as a strong
backbone for many vision tasks. Code is available at:
https://github.com/MonashAI/LIT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zizheng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs. (arXiv:2106.02684v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02684</id>
        <link href="http://arxiv.org/abs/2106.02684"/>
        <updated>2021-06-08T02:20:22.407Z</updated>
        <summary type="html"><![CDATA[We address the issue of safety in reinforcement learning. We pose the problem
in an episodic framework of a constrained Markov decision process. Existing
results have shown that it is possible to achieve a reward regret of
$\tilde{\mathcal{O}}(\sqrt{K})$ while allowing an
$\tilde{\mathcal{O}}(\sqrt{K})$ constraint violation in $K$ episodes. A
critical question that arises is whether it is possible to keep the constraint
violation even smaller. We show that when a strictly safe policy is known, then
one can confine the system to zero constraint violation with arbitrarily high
probability while keeping the reward regret of order
$\tilde{\mathcal{O}}(\sqrt{K})$. The algorithm which does so employs the
principle of optimistic pessimism in the face of uncertainty to achieve safe
exploration. When no strictly safe policy is known, though one is known to
exist, then it is possible to restrict the system to bounded constraint
violation with arbitrarily high probability. This is shown to be realized by a
primal-dual algorithm with an optimistic primal estimate and a pessimistic dual
update.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1"&gt;Ruida Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1"&gt;Dileep Kalathil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1"&gt;P. R. Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chao Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian Optimization. (arXiv:2106.02732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02732</id>
        <link href="http://arxiv.org/abs/2106.02732"/>
        <updated>2021-06-08T02:20:22.400Z</updated>
        <summary type="html"><![CDATA[Decision-based attacks (DBA), wherein attackers perturb inputs to spoof
learning algorithms by observing solely the output labels, are a type of severe
adversarial attacks against Deep Neural Networks (DNNs) requiring minimal
knowledge of attackers. State-of-the-art DBA attacks relying on zeroth-order
gradient estimation require an excessive number of queries. Recently, Bayesian
optimization (BO) has shown promising in reducing the number of queries in
score-based attacks (SBA), in which attackers need to observe real-valued
probability scores as outputs. However, extending BO to the setting of DBA is
nontrivial because in DBA only output labels instead of real-valued scores, as
needed by BO, are available to attackers. In this paper, we close this gap by
proposing an efficient DBA attack, namely BO-DBA. Different from existing
approaches, BO-DBA generates adversarial examples by searching so-called
\emph{directions of perturbations}. It then formulates the problem as a BO
problem that minimizes the real-valued distortion of perturbations. With the
optimized perturbation generation process, BO-DBA converges much faster than
the state-of-the-art DBA techniques. Experimental results on pre-trained
ImageNet classifiers show that BO-DBA converges within 200 queries while the
state-of-the-art DBA techniques need over 15,000 queries to achieve the same
level of perturbation distortion. BO-DBA also shows similar attack success
rates even as compared to BO-based SBA attacks but with less distortion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shucheng Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15203</id>
        <link href="http://arxiv.org/abs/2105.15203"/>
        <updated>2021-06-08T02:20:22.389Z</updated>
        <summary type="html"><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Video based Heart and Respiration Rate Monitoring. (arXiv:2106.02669v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02669</id>
        <link href="http://arxiv.org/abs/2106.02669"/>
        <updated>2021-06-08T02:20:22.368Z</updated>
        <summary type="html"><![CDATA[In recent years, research about monitoring vital signs by smartphones grows
significantly. There are some special sensors like Electrocardiogram (ECG) and
Photoplethysmographic (PPG) to detect heart rate (HR) and respiration rate
(RR). Smartphone cameras also can measure HR by detecting and processing
imaging Photoplethysmographic (iPPG) signals from the video of a user's face.
Indeed, the variation in the intensity of the green channel can be measured by
the iPPG signals of the video. This study aimed to provide a method to extract
heart rate and respiration rate using the video of individuals' faces. The
proposed method is based on measuring fluctuations in the Hue, and can
therefore extract both HR and RR from the video of a user's face. The proposed
method is evaluated by performing on 25 healthy individuals. For each subject,
20 seconds video of his/her face is recorded. Results show that the proposed
approach of measuring iPPG using Hue gives more accurate rates than the Green
channel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourbemany_J/0/1/0/all/0/1"&gt;Jafar Pourbemany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essa_A/0/1/0/all/0/1"&gt;Almabrok Essa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity. (arXiv:2106.02692v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02692</id>
        <link href="http://arxiv.org/abs/2106.02692"/>
        <updated>2021-06-08T02:20:22.359Z</updated>
        <summary type="html"><![CDATA[Humans are increasingly interacting with machines through language, sometimes
in contexts where the user may not know they are talking to a machine (like
over the phone or a text chatbot). We aim to understand how system designers
and researchers might allow their systems to confirm its non-human identity. We
collect over 2,500 phrasings related to the intent of ``Are you a robot?". This
is paired with over 2,500 adversarially selected utterances where only
confirming the system is non-human would be insufficient or disfluent. We
compare classifiers to recognize the intent and discuss the precision/recall
and model complexity tradeoffs. Such classifiers could be integrated into
dialog systems to avoid undesired deception. We then explore how both a
generative research model (Blender) as well as two deployed systems (Amazon
Alexa, Google Assistant) handle this intent, finding that systems often fail to
confirm their non-human identity. Finally, we try to understand what a good
response to the intent would be, and conduct a user study to compare the
important aspects when responding to this intent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1"&gt;David Gros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03736</id>
        <link href="http://arxiv.org/abs/2104.03736"/>
        <updated>2021-06-08T02:20:22.234Z</updated>
        <summary type="html"><![CDATA[Meta-learning can extract an inductive bias from previous learning experience
and assist the training processes of new tasks. It is often realized through
optimizing a meta-model with the evaluation loss of a series of task-specific
solvers. Most existing algorithms sample non-overlapping $\mathit{support}$
sets and $\mathit{query}$ sets to train and evaluate the solvers respectively
due to simplicity ($\mathcal{S}/\mathcal{Q}$ protocol). However, another
evaluation method that assesses the discrepancy between the solver and a target
model is short of research ($\mathcal{S}/\mathcal{T}$ protocol).
$\mathcal{S}/\mathcal{T}$ protocol has unique advantages such as offering more
informative supervision, but it is computationally expensive. This paper looks
into this special evaluation method and takes a step towards putting it into
practice. We find that with a small ratio of tasks armed with target models,
classic meta-learning algorithms can be improved a lot without consuming many
resources. Furthermore, we empirically verify the effectiveness of
$\mathcal{S}/\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers to target models via knowledge distillation. Experiments
demonstrate the superiority of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Su Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13677</id>
        <link href="http://arxiv.org/abs/2105.13677"/>
        <updated>2021-06-08T02:20:22.202Z</updated>
        <summary type="html"><![CDATA[This paper presents an efficient multi-scale vision Transformer, called ResT,
that capably served as a general-purpose backbone for image recognition. Unlike
existing Transformer methods, which employ standard Transformer blocks to
tackle raw images with a fixed resolution, our ResT have several advantages:
(1) A memory-efficient multi-head self-attention is built, which compresses the
memory by a simple depth-wise convolution, and projects the interaction across
the attention-heads dimension while keeping the diversity ability of
multi-heads; (2) Position encoding is constructed as spatial attention, which
is more flexible and can tackle with input images of arbitrary size without
interpolation or fine-tune; (3) Instead of the straightforward tokenization at
the beginning of each stage, we design the patch embedding as a stack of
overlapping convolution operation with stride on the 2D-reshaped token map. We
comprehensively validate ResT on image classification and downstream tasks.
Experimental results show that the proposed ResT can outperform the recently
state-of-the-art backbones by a large margin, demonstrating the potential of
ResT as strong backbones. The code and models will be made publicly available
at https://github.com/wofmanaf/ResT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yubin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stylizing 3D Scene via Implicit Representation and HyperNetwork. (arXiv:2105.13016v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13016</id>
        <link href="http://arxiv.org/abs/2105.13016"/>
        <updated>2021-06-08T02:20:22.196Z</updated>
        <summary type="html"><![CDATA[In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance field model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance field model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1"&gt;Pei-Ze Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1"&gt;Meng-Shiun Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1"&gt;Hung-Yu Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1"&gt;Wei-sheng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1"&gt;Wei-Chen Chiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-06-08T02:20:22.189Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion. (arXiv:2104.13095v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13095</id>
        <link href="http://arxiv.org/abs/2104.13095"/>
        <updated>2021-06-08T02:20:22.169Z</updated>
        <summary type="html"><![CDATA[Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),
few-shot knowledge graph completion (FKGC) has recently gained more research
interests. Some existing models employ a few-shot relation's multi-hop neighbor
information to enhance its semantic representation. However, noise neighbor
information might be amplified when the neighborhood is excessively sparse and
no neighbor is available to represent the few-shot relation. Moreover, modeling
and inferring complex relations of one-to-many (1-N), many-to-one (N-1), and
many-to-many (N-N) by previous knowledge graph completion approaches requires
high model complexity and a large amount of training instances. Thus, inferring
complex relations in the few-shot scenario is difficult for FKGC models due to
limited training instances. In this paper, we propose a few-shot relational
learning with global-local framework to address the above issues. At the global
stage, a novel gated and attentive neighbor aggregator is built for accurately
integrating the semantics of a few-shot relation's neighborhood, which helps
filtering the noise neighbors even if a KG contains extremely sparse
neighborhoods. For the local stage, a meta-learning based TransH (MTransH)
method is designed to model complex relations and train our model in a few-shot
learning fashion. Extensive experiments show that our model outperforms the
state-of-the-art FKGC approaches on the frequently-used benchmark datasets
NELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model
achieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on
Wiki-One by the metric Hits@10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chengguang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1"&gt;Ruiying Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jian Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Vision Transformers to Adversarial Examples. (arXiv:2104.02610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02610</id>
        <link href="http://arxiv.org/abs/2104.02610"/>
        <updated>2021-06-08T02:20:22.163Z</updated>
        <summary type="html"><![CDATA[Recent advances in attention-based networks have shown that Vision
Transformers can achieve state-of-the-art or near state-of-the-art results on
many image classification tasks. This puts transformers in the unique position
of being a promising alternative to traditional convolutional neural networks
(CNNs). While CNNs have been carefully studied with respect to adversarial
attacks, the same cannot be said of Vision Transformers. In this paper, we
study the robustness of Vision Transformers to adversarial examples. Our
analyses of transformer security is divided into three parts. First, we test
the transformer under standard white-box and black-box attacks. Second, we
study the transferability of adversarial examples between CNNs and
transformers. We show that adversarial examples do not readily transfer between
CNNs and transformers. Based on this finding, we analyze the security of a
simple ensemble defense of CNNs and transformers. By creating a new attack, the
self-attention blended gradient attack, we show that such an ensemble is not
secure under a white-box adversary. However, under a black-box adversary, we
show that an ensemble can achieve unprecedented robustness without sacrificing
clean accuracy. Our analysis for this work is done using six types of white-box
attacks and two types of black-box attacks. Our study encompasses multiple
Vision Transformers, Big Transfer Models and CNN architectures trained on
CIFAR-10, CIFAR-100 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rigel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Scene Completion via Integrating Instances and Scene in-the-Loop. (arXiv:2104.03640v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03640</id>
        <link href="http://arxiv.org/abs/2104.03640"/>
        <updated>2021-06-08T02:20:22.154Z</updated>
        <summary type="html"><![CDATA[Semantic Scene Completion aims at reconstructing a complete 3D scene with
precise voxel-wise semantics from a single-view depth or RGBD image. It is a
crucial but challenging problem for indoor scene understanding. In this work,
we present a novel framework named Scene-Instance-Scene Network
(\textit{SISNet}), which takes advantages of both instance and scene level
semantic information. Our method is capable of inferring fine-grained shape
details as well as nearby objects whose semantic categories are easily
mixed-up. The key insight is that we decouple the instances from a coarsely
completed semantic scene instead of a raw input image to guide the
reconstruction of instances and the overall scene. SISNet conducts iterative
scene-to-instance (SI) and instance-to-scene (IS) semantic completion.
Specifically, the SI is able to encode objects' surrounding context for
effectively decoupling instances from the scene and each instance could be
voxelized into higher resolution to capture finer details. With IS,
fine-grained instance information can be integrated back into the 3D scene and
thus leads to more accurate semantic scene completion. Utilizing such an
iterative mechanism, the scene and instance completion benefits each other to
achieve higher completion accuracy. Extensively experiments show that our
proposed method consistently outperforms state-of-the-art methods on both real
NYU, NYUCAD and synthetic SUNCG-RGBD datasets. The code and the supplementary
material will be available at \url{https://github.com/yjcaimeow/SISNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yingjie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuesong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kwan-Yee Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConVEx: Data-Efficient and Few-Shot Slot Labeling. (arXiv:2010.11791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11791</id>
        <link href="http://arxiv.org/abs/2010.11791"/>
        <updated>2021-06-08T02:20:22.147Z</updated>
        <summary type="html"><![CDATA[We propose ConVEx (Conversational Value Extractor), an efficient pretraining
and fine-tuning neural approach for slot-labeling dialog tasks. Instead of
relying on more general pretraining objectives from prior work (e.g., language
modeling, response selection), ConVEx's pretraining objective, a novel pairwise
cloze task using Reddit data, is well aligned with its intended usage on
sequence labeling tasks. This enables learning domain-specific slot labelers by
simply fine-tuning decoding layers of the pretrained general-purpose sequence
labeling model, while the majority of the pretrained model's parameters are
kept frozen. We report state-of-the-art performance of ConVEx across a range of
diverse domains and data sets for dialog slot-labeling, with the largest gains
in the most challenging, few-shot setups. We believe that ConVEx's reduced
pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its
efficient fine-tuning and strong performance, promise wider portability and
scalability for data-efficient sequence-labeling tasks in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_M/0/1/0/all/0/1"&gt;Matthew Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11646</id>
        <link href="http://arxiv.org/abs/2008.11646"/>
        <updated>2021-06-08T02:20:22.136Z</updated>
        <summary type="html"><![CDATA[Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yaoqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bolun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04522</id>
        <link href="http://arxiv.org/abs/2105.04522"/>
        <updated>2021-06-08T02:20:22.114Z</updated>
        <summary type="html"><![CDATA[Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (WebVision) noise with varying noise rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1"&gt;Erik Englesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Conditional Disentanglement Framework for Multimodal Brain MR Image Translation. (arXiv:2101.05434v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05434</id>
        <link href="http://arxiv.org/abs/2101.05434"/>
        <updated>2021-06-08T02:20:22.107Z</updated>
        <summary type="html"><![CDATA[Multimodal MRI provides complementary and clinically relevant information to
probe tissue condition and to characterize various diseases. However, it is
often difficult to acquire sufficiently many modalities from the same subject
due to limitations in study plans, while quantitative analysis is still
demanded. In this work, we propose a unified conditional disentanglement
framework to synthesize any arbitrary modality from an input modality. Our
framework hinges on a cycle-constrained conditional adversarial training
approach, where it can extract a modality-invariant anatomical feature with a
modality-agnostic encoder and generate a target modality with a conditioned
decoder. We validate our framework on four MRI modalities, including
T1-weighted, T1 contrast enhanced, T2-weighted, and FLAIR MRI, from the
BraTS'18 database, showing superior performance on synthesis quality over the
comparison methods. In addition, we report results from experiments on a tumor
segmentation task carried out with synthesized data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Registration of serial sections: An evaluation method based on distortions of the ground truths. (arXiv:2011.11060v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11060</id>
        <link href="http://arxiv.org/abs/2011.11060"/>
        <updated>2021-06-08T02:20:22.100Z</updated>
        <summary type="html"><![CDATA[Registration of histological serial sections is a challenging task. Serial
sections exhibit distortions and damage from sectioning. Missing information on
how the tissue looked before cutting makes a realistic validation of 2D
registrations extremely difficult.

This work proposes methods for ground-truth-based evaluation of
registrations. Firstly, we present a methodology to generate test data for
registrations. We distort an innately registered image stack in the manner
similar to the cutting distortion of serial sections. Test cases are generated
from existing 3D data sets, thus the ground truth is known. Secondly, our test
case generation premises evaluation of the registrations with known ground
truths. Our methodology for such an evaluation technique distinguishes this
work from other approaches. Both under- and over-registration become evident in
our evaluations. We also survey existing validation efforts.

We present a full-series evaluation across six different registration methods
applied to our distorted 3D data sets of animal lungs. Our distorted and ground
truth data sets are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobachev_O/0/1/0/all/0/1"&gt;Oleg Lobachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funatomi_T/0/1/0/all/0/1"&gt;Takuya Funatomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfaffenroth_A/0/1/0/all/0/1"&gt;Alexander Pfaffenroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_R/0/1/0/all/0/1"&gt;Reinhold F&amp;#xf6;rster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knudsen_L/0/1/0/all/0/1"&gt;Lars Knudsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wrede_C/0/1/0/all/0/1"&gt;Christoph Wrede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guthe_M/0/1/0/all/0/1"&gt;Michael Guthe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haberthur_D/0/1/0/all/0/1"&gt;David Haberth&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hlushchuk_R/0/1/0/all/0/1"&gt;Ruslan Hlushchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salaets_T/0/1/0/all/0/1"&gt;Thomas Salaets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toelen_J/0/1/0/all/0/1"&gt;Jaan Toelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffling_S/0/1/0/all/0/1"&gt;Simone Gaffling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhlfeld_C/0/1/0/all/0/1"&gt;Christian M&amp;#xfc;hlfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grothausmann_R/0/1/0/all/0/1"&gt;Roman Grothausmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02468</id>
        <link href="http://arxiv.org/abs/2105.02468"/>
        <updated>2021-06-08T02:20:22.090Z</updated>
        <summary type="html"><![CDATA[Understanding why deep nets can classify data in large dimensions remains a
challenge. It has been proposed that they do so by becoming stable to
diffeomorphisms, yet existing empirical measurements support that it is often
not the case. We revisit this question by defining a maximum-entropy
distribution on diffeomorphisms, that allows to study typical diffeomorphisms
of a given norm. We confirm that stability toward diffeomorphisms does not
strongly correlate to performance on benchmark data sets of images. By
contrast, we find that the stability toward diffeomorphisms relative to that of
generic transformations $R_f$ correlates remarkably with the test error
$\epsilon_t$. It is of order unity at initialization but decreases by several
decades during training for state-of-the-art architectures. For CIFAR10 and 15
known architectures, we find $\epsilon_t\approx 0.2\sqrt{R_f}$, suggesting that
obtaining a small $R_f$ is important to achieve good performance. We study how
$R_f$ depends on the size of the training set and compare it to a simple model
of invariant learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1"&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02923</id>
        <link href="http://arxiv.org/abs/2106.02923"/>
        <updated>2021-06-08T02:20:22.084Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues. Variational Auto-Encoders (VAEs) and their extensions
such as $\beta$-VAEs have been shown to locally align latent variables with PCA
directions, which can help to improve model disentanglement under some
conditions. Borrowing inspiration from Independent Component Analysis (ICA) and
sparse coding, we propose applying an $L_1$ loss to the VAE's generative
Jacobian during training to encourage local latent variable alignment with
independent factors of variation in the data. We demonstrate our results on a
variety of datasets, giving qualitative and quantitative results using
information theoretic and modularity measures that show our added $L_1$ cost
encourages local axis alignment of the latent representation with individual
factors of variation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1"&gt;Travers Rhodes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels. (arXiv:2103.04400v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04400</id>
        <link href="http://arxiv.org/abs/2103.04400"/>
        <updated>2021-06-08T02:20:22.061Z</updated>
        <summary type="html"><![CDATA[Scene text recognition (STR) task has a common practice: All state-of-the-art
STR models are trained on large synthetic data. In contrast to this practice,
training STR models only on fewer real labels (STR with fewer labels) is
important when we have to train STR models without synthetic data: for
handwritten or artistic texts that are difficult to generate synthetically and
for languages other than English for which we do not always have synthetic
data. However, there has been implicit common knowledge that training STR
models on real data is nearly impossible because real data is insufficient. We
consider that this common knowledge has obstructed the study of STR with fewer
labels. In this work, we would like to reactivate STR with fewer labels by
disproving the common knowledge. We consolidate recently accumulated public
real data and show that we can train STR models satisfactorily only with real
labeled data. Subsequently, we find simple data augmentation to fully exploit
real data. Furthermore, we improve the models by collecting unlabeled data and
introducing semi- and self-supervised methods. As a result, we obtain a
competitive model to state-of-the-art methods. To the best of our knowledge,
this is the first study that 1) shows sufficient performance by only using real
labels and 2) introduces semi- and self-supervised methods into STR with fewer
labels. Our code and data are available:
https://github.com/ku21fan/STR-Fewer-Labels]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jeonghun Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsui_Y/0/1/0/all/0/1"&gt;Yusuke Matsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1"&gt;Kiyoharu Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Local Self-Attention for Parameter Efficient Visual Backbones. (arXiv:2103.12731v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12731</id>
        <link href="http://arxiv.org/abs/2103.12731"/>
        <updated>2021-06-08T02:20:22.054Z</updated>
        <summary type="html"><![CDATA[Self-attention has the promise of improving computer vision systems due to
parameter-independent scaling of receptive fields and content-dependent
interactions, in contrast to parameter-dependent scaling and
content-independent interactions of convolutions. Self-attention models have
recently been shown to have encouraging improvements on accuracy-parameter
trade-offs compared to baseline convolutional models such as ResNet-50. In this
work, we aim to develop self-attention models that can outperform not just the
canonical baseline models, but even the high-performing convolutional models.
We propose two extensions to self-attention that, in conjunction with a more
efficient implementation of self-attention, improve the speed, memory usage,
and accuracy of these models. We leverage these improvements to develop a new
self-attention model family, HaloNets, which reach state-of-the-art accuracies
on the parameter-limited setting of the ImageNet classification benchmark. In
preliminary transfer learning experiments, we find that HaloNet models
outperform much larger models and have better inference performance. On harder
tasks such as object detection and instance segmentation, our simple local
self-attention and convolutional hybrids show improvements over very strong
baselines. These results mark another step in demonstrating the efficacy of
self-attention models on settings traditionally dominated by convolutional
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1"&gt;Ashish Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1"&gt;Prajit Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1"&gt;Niki Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hechtman_B/0/1/0/all/0/1"&gt;Blake Hechtman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision. (arXiv:2105.06464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06464</id>
        <link href="http://arxiv.org/abs/2105.06464"/>
        <updated>2021-06-08T02:20:22.044Z</updated>
        <summary type="html"><![CDATA[We introduce DiscoBox, a novel framework that jointly learns instance
segmentation and semantic correspondence using bounding box supervision.
Specifically, we propose a self-ensembling framework where instance
segmentation and semantic correspondence are jointly guided by a structured
teacher in addition to the bounding box supervision. The teacher is a
structured energy model incorporating a pairwise potential and a cross-image
potential to model the pairwise pixel relationships both within and across the
boxes. Minimizing the teacher energy simultaneously yields refined object masks
and dense correspondences between intra-class objects, which are taken as
pseudo-labels to supervise the task network and provide positive/negative
correspondence pairs for dense constrastive learning. We show a symbiotic
relationship where the two tasks mutually benefit from each other. Our best
model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly
supervised methods and is competitive to supervised methods. We also obtain
state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with
real-time inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1"&gt;Shiyi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1"&gt;Christopher Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1"&gt;Subhashree Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement Verification and Evidence Finding with Tables. (arXiv:2104.10366v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10366</id>
        <link href="http://arxiv.org/abs/2104.10366"/>
        <updated>2021-06-08T02:20:22.037Z</updated>
        <summary type="html"><![CDATA[Question answering from semi-structured tables can be seen as a semantic
parsing task and is significant and practical for pushing the boundary of
natural language understanding. Existing research mainly focuses on
understanding contents from unstructured evidence, e.g., news, natural language
sentences, and documents. The task of verification from structured evidence,
such as tables, charts, and databases, is still less explored. This paper
describes sattiy team's system in SemEval-2021 task 9: Statement Verification
and Evidence Finding with Tables (SEM-TAB-FACT). This competition aims to
verify statements and to find evidence from tables for scientific articles and
to promote the proper interpretation of the surrounding article. In this paper,
we exploited ensemble models of pre-trained language models over tables, TaPas
and TaBERT, for Task A and adjust the result based on some rules extracted for
Task B. Finally, in the leaderboard, we attain the F1 scores of 0.8496 and
0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1
score of 0.4856 in Task B.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1"&gt;Xiaoyi Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Meizhi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Resolution Network. (arXiv:2106.02898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02898</id>
        <link href="http://arxiv.org/abs/2106.02898"/>
        <updated>2021-06-08T02:20:22.029Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) are often of sophisticated design
with numerous convolutional layers and learnable parameters for the accuracy
reason. To alleviate the expensive costs of deploying them on mobile devices,
recent works have made huge efforts for excavating redundancy in pre-defined
architectures. Nevertheless, the redundancy on the input resolution of modern
CNNs has not been fully investigated, i.e., the resolution of input image is
fixed. In this paper, we observe that the smallest resolution for accurately
predicting the given image is different using the same neural network. To this
end, we propose a novel dynamic-resolution network (DRNet) in which the
resolution is determined dynamically based on each input sample. Thus, a
resolution predictor with negligible computational costs is explored and
optimized jointly with the desired network. In practice, the predictor learns
the smallest resolution that can retain and even exceed the original
recognition accuracy for each image. During the inference, each input image
will be resized to its predicted resolution for minimizing the overall
computation burden. We then conduct extensive experiments on several benchmark
networks and datasets. The results show that our DRNet can be embedded in any
off-the-shelf network architecture to obtain a considerable reduction in
computational complexity. For instance, DRNet achieves similar performance with
an about 34% computation reduction, while gains 1.4% accuracy increase with 10%
computation reduction compared to the original ResNet-50 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingjian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1"&gt;Enhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiulin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Ying Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zhenzhong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Long Context NLM for ASR Rescoring in Conversational Agents. (arXiv:2104.11070v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11070</id>
        <link href="http://arxiv.org/abs/2104.11070"/>
        <updated>2021-06-08T02:20:22.023Z</updated>
        <summary type="html"><![CDATA[Neural Language Models (NLM), when trained and evaluated with context
spanning multiple utterances, have been shown to consistently outperform both
conventional n-gram language models and NLMs that use limited context. In this
paper, we investigate various techniques to incorporate turn based context
history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent
based NLMs, we explore context carry over mechanism and feature based
augmentation, where we incorporate other forms of contextual information such
as bot response and system dialogue acts as classified by a Natural Language
Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem
with contextual NLM, we propose the use of attention layer over lexical
metadata to improve feature based augmentation. Additionally, we adapt our
contextual NLM towards user provided on-the-fly speech patterns by leveraging
encodings from a large pre-trained masked language model and performing fusion
with a Transformer-XL based NLM. We test our proposed models using N-best
rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on
downstream NLU tasks such as intent classification and slot labeling. The best
performing model shows a relative WER between 1.6% and 9.1% and a slot labeling
F1 score improvement of 4% over non-contextual baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1"&gt;Monica Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1"&gt;Srikanth Ronanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Geometric Matrix Completion. (arXiv:1911.07255v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07255</id>
        <link href="http://arxiv.org/abs/1911.07255"/>
        <updated>2021-06-08T02:20:22.003Z</updated>
        <summary type="html"><![CDATA[Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boyarski_A/0/1/0/all/0/1"&gt;Amit Boyarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Sanketh Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCT: Point cloud transformer. (arXiv:2012.09688v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09688</id>
        <link href="http://arxiv.org/abs/2012.09688"/>
        <updated>2021-06-08T02:20:21.995Z</updated>
        <summary type="html"><![CDATA[The irregular domain and lack of ordering make it challenging to design deep
neural networks for point cloud processing. This paper presents a novel
framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is
based on Transformer, which achieves huge success in natural language
processing and displays great potential in image processing. It is inherently
permutation invariant for processing a sequence of points, making it
well-suited for point cloud learning. To better capture local context within
the point cloud, we enhance input embedding with the support of farthest point
sampling and nearest neighbor search. Extensive experiments demonstrate that
the PCT achieves the state-of-the-art performance on shape classification, part
segmentation and normal estimation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent Biases. (arXiv:2106.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02953</id>
        <link href="http://arxiv.org/abs/2106.02953"/>
        <updated>2021-06-08T02:20:21.988Z</updated>
        <summary type="html"><![CDATA[Visual search is a ubiquitous and often challenging daily task, exemplified
by looking for the car keys at home or a friend in a crowd. An intriguing
property of some classical search tasks is an asymmetry such that finding a
target A among distractors B can be easier than finding B among A. To elucidate
the mechanisms responsible for asymmetry in visual search, we propose a
computational model that takes a target and a search image as inputs and
produces a sequence of eye movements until the target is found. The model
integrates eccentricity-dependent visual recognition with target-dependent
top-down cues. We compared the model against human behavior in six paradigmatic
search tasks that show asymmetry in humans. Without prior exposure to the
stimuli or task-specific training, the model provides a plausible mechanism for
search asymmetry. We hypothesized that the polarity of search asymmetry arises
from experience with the natural environment. We tested this hypothesis by
training the model on an augmented version of ImageNet where the biases of
natural images were either removed or reversed. The polarity of search
asymmetry disappeared or was altered depending on the training protocol. This
study highlights how classical perceptual properties can emerge in neural
network models, without the need for task-specific training, but rather as a
consequence of the statistical properties of the developmental diet fed to the
model. All source code and stimuli are publicly available
https://github.com/kreimanlab/VisualSearchAsymmetry]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shashi Kant Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chia-Chien Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_J/0/1/0/all/0/1"&gt;Jeremy M. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1"&gt;Gabriel Kreiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02852</id>
        <link href="http://arxiv.org/abs/2106.02852"/>
        <updated>2021-06-08T02:20:21.981Z</updated>
        <summary type="html"><![CDATA[This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Joint Sparse Non-negative Matrix Factorization Framework for Identifying the Common and Subject-specific Functional Units of Tongue Motion During Speech. (arXiv:2007.04865v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04865</id>
        <link href="http://arxiv.org/abs/2007.04865"/>
        <updated>2021-06-08T02:20:21.974Z</updated>
        <summary type="html"><![CDATA[Intelligible speech is produced by creating varying internal local muscle
groupings -- i.e., functional units -- that are generated in a systematic and
coordinated manner. There are two major challenges in characterizing and
analyzing functional units.~First, due to the complex and convoluted nature of
tongue structure and function, it is of great importance to develop a method
that can accurately decode complex muscle coordination patterns during speech.
Second, it is challenging to keep identified functional units across subjects
comparable due to their substantial variability. In this work, to address these
challenges, we develop a new deep learning framework to identify common and
subject-specific functional units of tongue motion during speech.~Our framework
hinges on joint deep graph-regularized sparse non-negative matrix factorization
(NMF) using motion quantities derived from displacements by tagged Magnetic
Resonance Imaging. More specifically, we transform NMF with sparse and graph
regularizations into modular architectures akin to deep neural networks by
means of unfolding the Iterative Shrinkage-Thresholding Algorithm to learn
interpretable building blocks and associated weighting map. We then apply
spectral clustering to common and subject-specific weighting maps from which we
jointly determine the common and subject-specific functional units. Experiments
carried out with simulated datasets show that the proposed method achieved on
par or better clustering performance over the comparison methods. Experiments
carried out with in vivo tongue motion data show that the proposed method can
determine the common and subject-specific functional units with increased
interpretability and decreased size variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Arnold Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reese_T/0/1/0/all/0/1"&gt;Timothy G. Reese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedeen_V/0/1/0/all/0/1"&gt;Van J. Wedeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06255</id>
        <link href="http://arxiv.org/abs/2010.06255"/>
        <updated>2021-06-08T02:20:21.955Z</updated>
        <summary type="html"><![CDATA[Aerial tracking, which has exhibited its omnipresent dedication and splendid
performance, is one of the most active applications in the remote sensing
field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,
equipped with a visual tracking approach, has been widely used in aviation,
navigation, agriculture,transportation, and public security, etc. As is
mentioned above, the UAV-based aerial tracking platform has been gradually
developed from research to practical application stage, reaching one of the
main aerial remote sensing technologies in the future. However, due to the
real-world onerous situations, e.g., harsh external challenges, the vibration
of the UAV mechanical structure (especially under strong wind conditions), the
maneuvering flight in complex environment, and the limited computation
resources onboard, accuracy, robustness, and high efficiency are all crucial
for the onboard tracking methods. Recently, the discriminative correlation
filter (DCF)-based trackers have stood out for their high computational
efficiency and appealing robustness on a single CPU, and have flourished in the
UAV visual tracking community. In this work, the basic framework of the
DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art
DCF-based trackers are orderly summarized according to their innovations for
solving various issues. Besides, exhaustive and quantitative experiments have
been extended on various prevailing UAV tracking benchmarks, i.e., UAV123,
UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903
frames in total. The experiments show the performance, verify the feasibility,
and demonstrate the current challenges of DCF-based trackers onboard UAV
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Geng Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition. (arXiv:2005.00777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00777</id>
        <link href="http://arxiv.org/abs/2005.00777"/>
        <updated>2021-06-08T02:20:21.917Z</updated>
        <summary type="html"><![CDATA[Recognition accuracy and response time are both critically essential ahead of
building practical electroencephalography (EEG) based brain-computer interface
(BCI). Recent approaches, however, have either compromised in the
classification accuracy or responding time. This paper presents a novel deep
learning approach designed towards remarkably accurate and responsive motor
imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term
Memory (BiLSTM) with the Attention mechanism manages to derive relevant
features from raw EEG signals. The connected graph convolutional neural network
(GCN) promotes the decoding performance by cooperating with the topological
structure of features, which are estimated from the overall data. The
0.4-second detection framework has shown effective and efficient prediction
based on individual and group-wise training, with 98.81% and 94.64% accuracy,
respectively, which outperformed all the state-of-the-art studies. The
introduced deep feature mining approach can precisely recognize human motion
intents from raw EEG signals, which paves the road to translate the EEG based
MI recognition to practical BCI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-DimensionalSpatial Positional Encoding. (arXiv:2106.02795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-08T02:20:21.870Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Category Contrast for Unsupervised Domain Adaptation in Visual Tasks. (arXiv:2106.02885v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02885</id>
        <link href="http://arxiv.org/abs/2106.02885"/>
        <updated>2021-06-08T02:20:21.862Z</updated>
        <summary type="html"><![CDATA[Instance contrast for unsupervised representation learning has achieved great
success in recent years. In this work, we explore the idea of instance
contrastive learning in unsupervised domain adaptation (UDA) and propose a
novel Category Contrast technique (CaCo) that introduces semantic priors on top
of instance discrimination for visual UDA tasks. By considering instance
contrastive learning as a dictionary look-up operation, we construct a
semantics-aware dictionary with samples from both source and target domains
where each target sample is assigned a (pseudo) category label based on the
category priors of source samples. This allows category contrastive learning
(between target queries and the category-level dictionary) for
category-discriminative yet domain-invariant feature representations: samples
of the same category (from either source or target domain) are pulled closer
while those of different categories are pushed apart simultaneously. Extensive
UDA experiments in multiple visual tasks ($e.g.$, segmentation, classification
and detection) show that the simple implementation of CaCo achieves superior
performance as compared with the highly-optimized state-of-the-art methods.
Analytically and empirically, the experiments also demonstrate that CaCo is
complementary to existing UDA methods and generalizable to other learning
setups such as semi-supervised learning, unsupervised model adaptation, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning. (arXiv:2009.08348v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08348</id>
        <link href="http://arxiv.org/abs/2009.08348"/>
        <updated>2021-06-08T02:20:21.830Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) provides a crucial tool for visual similarity and
zero-shot applications by learning generalizing embedding spaces, although
recent work in DML has shown strong performance saturation across training
objectives. However, generalization capacity is known to scale with the
embedding space dimensionality. Unfortunately, high dimensional embeddings also
create higher retrieval cost for downstream applications. To remedy this, we
propose \emph{Simultaneous Similarity-based Self-distillation (S2SD). S2SD
extends DML with knowledge distillation from auxiliary, high-dimensional
embedding and feature spaces to leverage complementary context during training
while retaining test-time cost and with negligible changes to the training
time. Experiments and ablations across different objectives and standard
benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while
also setting a new state-of-the-art. Code available at
https://github.com/MLforHealth/S2SD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-aware Adaptive Instance Normalization for Image Harmonization. (arXiv:2106.02853v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02853</id>
        <link href="http://arxiv.org/abs/2106.02853"/>
        <updated>2021-06-08T02:20:21.794Z</updated>
        <summary type="html"><![CDATA[Image composition plays a common but important role in photo editing. To
acquire photo-realistic composite images, one must adjust the appearance and
visual style of the foreground to be compatible with the background. Existing
deep learning methods for harmonizing composite images directly learn an image
mapping network from the composite to the real one, without explicit
exploration on visual style consistency between the background and the
foreground images. To ensure the visual style consistency between the
foreground and the background, in this paper, we treat image harmonization as a
style transfer problem. In particular, we propose a simple yet effective
Region-aware Adaptive Instance Normalization (RAIN) module, which explicitly
formulates the visual style from the background and adaptively applies them to
the foreground. With our settings, our RAIN module can be used as a drop-in
module for existing image harmonization networks and is able to bring
significant improvements. Extensive experiments on the existing image
harmonization benchmark datasets show the superior capability of the proposed
method. Code is available at {https://github.com/junleen/RainNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jun Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Han Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Li Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1"&gt;Rong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiao Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Networks with Gated Recurrent Connections. (arXiv:2106.02859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02859</id>
        <link href="http://arxiv.org/abs/2106.02859"/>
        <updated>2021-06-08T02:20:21.772Z</updated>
        <summary type="html"><![CDATA[The convolutional neural network (CNN) has become a basic model for solving
many computer vision problems. In recent years, a new class of CNNs, recurrent
convolution neural network (RCNN), inspired by abundant recurrent connections
in the visual systems of animals, was proposed. The critical element of RCNN is
the recurrent convolutional layer (RCL), which incorporates recurrent
connections between neurons in the standard convolutional layer. With
increasing number of recurrent computations, the receptive fields (RFs) of
neurons in RCL expand unboundedly, which is inconsistent with biological facts.
We propose to modulate the RFs of neurons by introducing gates to the recurrent
connections. The gates control the amount of context information inputting to
the neurons and the neurons' RFs therefore become adaptive. The resulting layer
is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a
deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several
computer vision tasks including object recognition, scene text recognition and
object detection, and obtained much better results than the RCNN. In addition,
when combined with other adaptive RF techniques, the GRCNN demonstrated
competitive performance to the state-of-the-art models on benchmark datasets
for these tasks. The codes are released at
\href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaolin Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principle Bit Analysis: Autoencoding with Schur-Concave Loss. (arXiv:2106.02796v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02796</id>
        <link href="http://arxiv.org/abs/2106.02796"/>
        <updated>2021-06-08T02:20:21.763Z</updated>
        <summary type="html"><![CDATA[We consider a linear autoencoder in which the latent variables are quantized,
or corrupted by noise, and the constraint is Schur-concave in the set of latent
variances. Although finding the optimal encoder/decoder pair for this setup is
a nonconvex optimization problem, we show that decomposing the source into its
principal components is optimal. If the constraint is strictly Schur-concave
and the empirical covariance matrix has only simple eigenvalues, then any
optimal encoder/decoder must decompose the source in this way. As one
application, we consider a strictly Schur-concave constraint that estimates the
number of bits needed to represent the latent variables under fixed-rate
encoding, a setup that we call \emph{Principal Bit Analysis (PBA)}. This yields
a practical, general-purpose, fixed-rate compressor that outperforms existing
algorithms. As a second application, we show that a prototypical
autoencoder-based variable-rate compressor is guaranteed to decompose the
source into its principal components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1"&gt;Sourbh Bhadane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Aaron B. Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Video Generation for Complex Data. (arXiv:2106.02719v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02719</id>
        <link href="http://arxiv.org/abs/2106.02719"/>
        <updated>2021-06-08T02:20:21.755Z</updated>
        <summary type="html"><![CDATA[Videos can often be created by first outlining a global description of the
scene and then adding local details. Inspired by this we propose a hierarchical
model for video generation which follows a coarse to fine approach. First our
model generates a low resolution video, establishing the global scene
structure, that is then refined by subsequent levels in the hierarchy. We train
each level in our hierarchy sequentially on partial views of the videos. This
reduces the computational complexity of our generative model, which scales to
high-resolution videos beyond a few frames. We validate our approach on
Kinetics-600 and BDD100K, for which we train a three level model capable of
generating 256x256 videos with 48 frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castrejon_L/0/1/0/all/0/1"&gt;Lluis Castrejon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shellcode_IA32: A Dataset for Automatic Shellcode Generation. (arXiv:2104.13100v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13100</id>
        <link href="http://arxiv.org/abs/2104.13100"/>
        <updated>2021-06-08T02:20:21.731Z</updated>
        <summary type="html"><![CDATA[We take the first step to address the task of automatically generating
shellcodes, i.e., small pieces of code used as a payload in the exploitation of
a software vulnerability, starting from natural language comments. We assemble
and release a novel dataset (Shellcode_IA32), consisting of challenging but
common assembly instructions with their natural language descriptions. We
experiment with standard methods in neural machine translation (NMT) to
establish baseline performance levels on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1"&gt;Pietro Liguori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Hossami_E/0/1/0/all/0/1"&gt;Erfan Al-Hossami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1"&gt;Domenico Cotroneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1"&gt;Roberto Natella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1"&gt;Bojan Cukic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1"&gt;Samira Shaikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Random Network for Fine-grained Image Classification. (arXiv:2103.07230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07230</id>
        <link href="http://arxiv.org/abs/2103.07230"/>
        <updated>2021-06-08T02:20:21.716Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Network (DCNN) and Transformer have achieved
remarkable successes in image recognition. However, their performance in
fine-grained image recognition is still difficult to meet the requirements of
actual needs. This paper proposes a Sequence Random Network (SRN) to enhance
the performance of DCNN. The output of DCNN is one-dimensional features. This
one-dimensional feature abstractly represents image information, but it does
not express well the detailed information of image. To address this issue, we
use the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks
(called BiLSTM-TDN), to further process DCNN one-dimensional features for
highlighting the detail information of image. After the feature transform by
BiLSTM-TDN, the recognition performance has been greatly improved. We conducted
the experiments on six fine-grained image datasets. Except for FGVC-Aircraft,
the accuracies of the proposed methods on the other datasets exceeded 99%.
Experimental results show that BiLSTM-TDN is far superior to the existing
state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended
to other models, such as Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaorong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Malu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Fengqing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Anping Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuanyuan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:21.706Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding. (arXiv:2009.06097v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06097</id>
        <link href="http://arxiv.org/abs/2009.06097"/>
        <updated>2021-06-08T02:20:21.699Z</updated>
        <summary type="html"><![CDATA[Transformer has become ubiquitous in the deep learning field. One of the key
ingredients that destined its success is the self-attention mechanism, which
allows fully-connected contextual encoding over input tokens. However, despite
its effectiveness in modeling short sequences, self-attention suffers when
handling inputs with extreme long-range dependencies, as its complexity grows
quadratically with respect to the sequence length. Therefore, long sequences
are often encoded by Transformer in chunks using a sliding window. In this
paper, we propose Cluster-Former, a novel clustering-based sparse Transformer
to perform attention across chunked sequences. The proposed framework is
pivoted on two unique types of Transformer layer: Sliding-Window Layer and
Cluster-Former Layer, which encode local sequence information and global
context jointly and iteratively. This new design allows information integration
beyond local windows, which is especially beneficial for question answering
(QA) tasks that rely on long-range dependencies. Experiments show that
Cluster-Former achieves state-of-the-art performance on several major QA
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Siqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Damaging Contrastive Learning. (arXiv:2106.02990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02990</id>
        <link href="http://arxiv.org/abs/2106.02990"/>
        <updated>2021-06-08T02:20:21.693Z</updated>
        <summary type="html"><![CDATA[The recent breakthrough achieved by contrastive learning accelerates the pace
for deploying unsupervised training on real-world data applications. However,
unlabeled data in reality is commonly imbalanced and shows a long-tail
distribution, and it is unclear how robustly the latest contrastive learning
methods could perform in the practical scenario. This paper proposes to
explicitly tackle this challenge, via a principled framework called
Self-Damaging Contrastive Learning (SDCLR), to automatically balance the
representation learning without knowing the classes. Our main inspiration is
drawn from the recent finding that deep models have difficult-to-memorize
samples, and those may be exposed through network pruning. It is further
natural to hypothesize that long-tail samples are also tougher for the model to
learn well due to insufficient examples. Hence, the key innovation in SDCLR is
to create a dynamic self-competitor model to contrast with the target model,
which is a pruned version of the latter. During training, contrasting the two
models will lead to adaptive online mining of the most easily forgotten samples
for the current target model, and implicitly emphasize them more in the
contrastive loss. Extensive experiments across multiple datasets and imbalance
settings show that SDCLR significantly improves not only overall accuracies but
also balancedness, in terms of linear evaluation on the full-shot and few-shot
settings. Our code is available at: https://github.com/VITA-Group/SDCLR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Ziyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1"&gt;Bobak Mortazavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02701</id>
        <link href="http://arxiv.org/abs/2106.02701"/>
        <updated>2021-06-08T02:20:21.686Z</updated>
        <summary type="html"><![CDATA[Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of projection neuron morphology, but
manual neuron reconstruction remains a bottleneck. Here we present a method
inspired by hidden Markov modeling and appearance modeling of fluorescent
neuron images that can automatically trace neuronal processes. Our method
leverages dynamic programming to scale to terabyte sized image data and can be
applied to images with one or more neurons. We applied our algorithm to the
output of image segmentation models where false negatives severed neuronal
processes, and showed that it can follow axons in the presence of noise or
nearby neurons. Our method has the potential to be integrated into a semi or
fully automated reconstruction pipeline. Additionally, it creates a framework
through which users can intervene with hard constraints to, for example, rule
out certain reconstructions, or assign axons to particular cell bodies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1"&gt;Thomas L. Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1"&gt;Daniel Tward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1"&gt;Ulrich Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1"&gt;Michael I. Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEEPMIR: A DEEP neural network for differential detection of cerebral Microbleeds and IRon deposits in MRI. (arXiv:2010.00148v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00148</id>
        <link href="http://arxiv.org/abs/2010.00148"/>
        <updated>2021-06-08T02:20:21.659Z</updated>
        <summary type="html"><![CDATA[Lobar cerebral microbleeds (CMBs) and localized non-hemorrhage iron deposits
in the basal ganglia have been associated with brain aging, vascular disease
and neurodegenerative disorders. Particularly, CMBs are small lesions and
require multiple neuroimaging modalities for accurate detection. Quantitative
susceptibility mapping (QSM) derived from in vivo magnetic resonance imaging
(MRI) is necessary to differentiate between iron content and mineralization. We
set out to develop a deep learning-based segmentation method suitable for
segmenting both CMBs and iron deposits. We included a convenience sample of 24
participants from the MESA cohort and used T2-weighted images, susceptibility
weighted imaging (SWI), and QSM to segment the two types of lesions. We
developed a protocol for simultaneous manual annotation of CMBs and
non-hemorrhage iron deposits in the basal ganglia. This manual annotation was
then used to train a deep convolution neural network (CNN). Specifically, we
adapted the U-Net model with a higher number of resolution layers to be able to
detect small lesions such as CMBs from standard resolution MRI. We tested
different combinations of the three modalities to determine the most
informative data sources for the detection tasks. In the detection of CMBs
using single class and multiclass models, we achieved an average sensitivity
and precision of between 0.84-0.88 and 0.40-0.59, respectively. The same
framework detected non-hemorrhage iron deposits with an average sensitivity and
precision of about 0.75-0.81 and 0.62-0.75, respectively. Our results showed
that deep learning could automate the detection of small vessel disease lesions
and including multimodal MR data (particularly QSM) can improve the detection
of CMB and non-hemorrhage iron deposits with sensitivity and precision that is
compatible with use in large-scale research studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tanweer Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdulkadir_A/0/1/0/all/0/1"&gt;Ahmed Abdulkadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nasrallah_I/0/1/0/all/0/1"&gt;Ilya M. Nasrallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ware_J/0/1/0/all/0/1"&gt;Jeffrey B. Ware&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hangfan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spincemaille_P/0/1/0/all/0/1"&gt;Pascal Spincemaille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Romero_J/0/1/0/all/0/1"&gt;J. Rafael Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bryan_R/0/1/0/all/0/1"&gt;R. Nick Bryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heckbert_S/0/1/0/all/0/1"&gt;Susan R. Heckbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Habes_M/0/1/0/all/0/1"&gt;Mohamad Habes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:21.652Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos. (arXiv:1811.11849v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.11849</id>
        <link href="http://arxiv.org/abs/1811.11849"/>
        <updated>2021-06-08T02:20:21.643Z</updated>
        <summary type="html"><![CDATA[Group-level emotion recognition (ER) is a growing research area as the
demands for assessing crowds of all sizes are becoming an interest in both the
security arena as well as social media. This work extends the earlier ER
investigations, which focused on either group-level ER on single images or
within a video, by fully investigating group-level expression recognition on
crowd videos. In this paper, we propose an effective deep feature level fusion
mechanism to model the spatial-temporal information in the crowd videos. In our
approach, the fusing process is performed on the deep feature domain by a
generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that
models spatial information relationships. Furthermore, we extend our proposed
spatial NVPF approach to the spatial-temporal NVPF approach to learn the
temporal information between frames. To demonstrate the robustness and
effectiveness of each component in the proposed approach, three experiments
were conducted: (i) evaluation on AffectNet database to benchmark the proposed
EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to
benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)
examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos
(GECV) dataset composed of 627 videos collected from publicly available
sources. GECV dataset is a collection of videos containing crowds of people.
Each video is labeled with emotion categories at three levels: individual
faces, group of people, and the entire video frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1"&gt;Kha Gia Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalata_I/0/1/0/all/0/1"&gt;Ibsa Jalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1"&gt;Kaushik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Over-Parameterized Training. (arXiv:2004.04690v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04690</id>
        <link href="http://arxiv.org/abs/2004.04690"/>
        <updated>2021-06-08T02:20:21.636Z</updated>
        <summary type="html"><![CDATA[The inductive bias of a neural network is largely determined by the
architecture and the training algorithm. To achieve good generalization, how to
effectively train a neural network is of great importance. We propose a novel
orthogonal over-parameterized training (OPT) framework that can provably
minimize the hyperspherical energy which characterizes the diversity of neurons
on a hypersphere. By maintaining the minimum hyperspherical energy during
training, OPT can greatly improve the empirical generalization. Specifically,
OPT fixes the randomly initialized weights of the neurons and learns an
orthogonal transformation that applies to these neurons. We consider multiple
ways to learn such an orthogonal transformation, including unrolling
orthogonalization algorithms, applying orthogonal parameterization, and
designing orthogonality-preserving gradient descent. For better scalability, we
propose the stochastic OPT which performs orthogonal transformation
stochastically for partial dimensions of neurons. Interestingly, OPT reveals
that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better
generalization. Extensive experiments validate the superiority of OPT over the
standard training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05123</id>
        <link href="http://arxiv.org/abs/2002.05123"/>
        <updated>2021-06-08T02:20:21.618Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pony_R/0/1/0/all/0/1"&gt;Roi Pony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEFT: Detection Embeddings for Tracking. (arXiv:2102.02267v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02267</id>
        <link href="http://arxiv.org/abs/2102.02267"/>
        <updated>2021-06-08T02:20:21.611Z</updated>
        <summary type="html"><![CDATA[Most modern multiple object tracking (MOT) systems follow the
tracking-by-detection paradigm, consisting of a detector followed by a method
for associating detections into tracks. There is a long history in tracking of
combining motion and appearance features to provide robustness to occlusions
and other challenges, but typically this comes with the trade-off of a more
complex and slower implementation. Recent successes on popular 2D tracking
benchmarks indicate that top-scores can be achieved using a state-of-the-art
detector and relatively simple associations relying on single-frame spatial
offsets -- notably outperforming contemporary methods that leverage learned
appearance features to help re-identify lost tracks. In this paper, we propose
an efficient joint detection and tracking model named DEFT, or "Detection
Embeddings for Tracking." Our approach relies on an appearance-based object
matching network jointly-learned with an underlying object detection network.
An LSTM is also added to capture motion constraints. DEFT has comparable
accuracy and speed to the top methods on 2D online tracking leaderboards while
having significant advantages in robustness when applied to more challenging
tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking
challenge, more than doubling the performance of the previous top method. Code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaabane_M/0/1/0/all/0/1"&gt;Mohamed Chaabane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peter Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_J/0/1/0/all/0/1"&gt;J. Ross Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OHara_S/0/1/0/all/0/1"&gt;Stephen O&amp;#x27;Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Middle-level Fusion for Lightweight RGB-D Salient Object Detection. (arXiv:2104.11543v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11543</id>
        <link href="http://arxiv.org/abs/2104.11543"/>
        <updated>2021-06-08T02:20:21.601Z</updated>
        <summary type="html"><![CDATA[Most existing lightweight RGB-D salient object detection (SOD) models are
based on two-stream structure or single-stream structure. The former one first
uses two sub-networks to extract unimodal features from RGB and depth images,
respectively, and then fuses them for SOD. While, the latter one directly
extracts multi-modal features from the input RGB-D images and then focuses on
exploiting cross-level complementary information. However, two-stream structure
based models inevitably require more parameters and single-stream structure
based ones cannot well exploit the cross-modal complementary information since
they ignore the modality difference. To address these issues, we propose to
employ the middle-level fusion structure for designing lightweight RGB-D SOD
model in this paper, which first employs two sub-networks to extract low- and
middle-level unimodal features, respectively, and then fuses those extracted
middle-level unimodal features for extracting corresponding high-level
multi-modal features in the subsequent sub-network. Different from existing
models, this structure can effectively exploit the cross-modal complementary
information and significantly reduce the network's parameters, simultaneously.
Therefore, a novel lightweight SOD model is designed, which contains a
information-aware multi-modal feature fusion (IMFF) module for effectively
capturing the cross-modal complementary information and a lightweight
feature-level and decision-level feature fusion (LFDF) module for aggregating
the feature-level and the decision-level saliency information in different
stages with less parameters. Our proposed model has only 3.9M parameters and
runs at 33 FPS. The experimental results on several benchmark datasets verify
the effectiveness and superiority of the proposed method over some
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1"&gt;Nianchang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jungong Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Video based Heart and Respiration Rate Monitoring. (arXiv:2106.02669v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02669</id>
        <link href="http://arxiv.org/abs/2106.02669"/>
        <updated>2021-06-08T02:20:21.593Z</updated>
        <summary type="html"><![CDATA[In recent years, research about monitoring vital signs by smartphones grows
significantly. There are some special sensors like Electrocardiogram (ECG) and
Photoplethysmographic (PPG) to detect heart rate (HR) and respiration rate
(RR). Smartphone cameras also can measure HR by detecting and processing
imaging Photoplethysmographic (iPPG) signals from the video of a user's face.
Indeed, the variation in the intensity of the green channel can be measured by
the iPPG signals of the video. This study aimed to provide a method to extract
heart rate and respiration rate using the video of individuals' faces. The
proposed method is based on measuring fluctuations in the Hue, and can
therefore extract both HR and RR from the video of a user's face. The proposed
method is evaluated by performing on 25 healthy individuals. For each subject,
20 seconds video of his/her face is recorded. Results show that the proposed
approach of measuring iPPG using Hue gives more accurate rates than the Green
channel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourbemany_J/0/1/0/all/0/1"&gt;Jafar Pourbemany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essa_A/0/1/0/all/0/1"&gt;Almabrok Essa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intent Classification and Slot Filling for Privacy Policies. (arXiv:2101.00123v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00123</id>
        <link href="http://arxiv.org/abs/2101.00123"/>
        <updated>2021-06-08T02:20:21.587Z</updated>
        <summary type="html"><![CDATA[Understanding privacy policies is crucial for users as it empowers them to
learn about the information that matters to them. Sentences written in a
privacy policy document explain privacy practices, and the constituent text
spans convey further specific information about that practice. We refer to
predicting the privacy practice explained in a sentence as intent
classification and identifying the text spans sharing specific information as
slot filling. In this work, we propose PolicyIE, an English corpus consisting
of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of
websites and mobile applications. PolicyIE corpus is a challenging real-world
benchmark with limited labeled examples reflecting the cost of collecting
large-scale annotations from domain experts. We present two alternative neural
approaches as baselines, (1) intent classification and slot filling as a joint
sequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq)
learning task. The experiment results show that both approaches perform
comparably in intent classification, while the Seq2Seq method outperforms the
sequence tagging approach in slot filling by a large margin. We perform a
detailed error analysis to reveal the challenges of the proposed corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1"&gt;Jianfeng Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tu Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norton_T/0/1/0/all/0/1"&gt;Thomas Norton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02689</id>
        <link href="http://arxiv.org/abs/2106.02689"/>
        <updated>2021-06-08T02:20:21.580Z</updated>
        <summary type="html"><![CDATA[Vision transformer (ViT) has recently showed its strong capability in
achieving comparable results to convolutional neural networks (CNNs) on image
classification. However, vanilla ViT simply inherits the same architecture from
the natural language processing directly, which is often not optimized for
vision applications. Motivated by this, in this paper, we propose a new
architecture that adopts the pyramid structure and employ a novel
regional-to-local attention rather than global self-attention in vision
transformers. More specifically, our model first generates regional tokens and
local tokens from an image with different patch sizes, where each regional
token is associated with a set of local tokens based on the spatial location.
The regional-to-local attention includes two steps: first, the regional
self-attention extract global information among all regional tokens and then
the local self-attention exchanges the information among one regional token and
the associated local tokens via self-attention. Therefore, even though local
self-attention confines the scope in a local region but it can still receive
global information. Extensive experiments on three vision tasks, including
image classification, object detection and action recognition, show that our
approach outperforms or is on par with state-of-the-art ViT variants including
many concurrent works. Our source codes and models will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Quanfu Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Open-Ended Commonsense Reasoning. (arXiv:2010.14439v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14439</id>
        <link href="http://arxiv.org/abs/2010.14439"/>
        <updated>2021-06-08T02:20:21.562Z</updated>
        <summary type="html"><![CDATA[Current commonsense reasoning research focuses on developing models that use
commonsense knowledge to answer multiple-choice questions. However, systems
designed to answer multiple-choice questions may not be useful in applications
that do not provide a small list of candidate answers to choose from. As a step
towards making commonsense reasoning research more realistic, we propose to
study open-ended commonsense reasoning (OpenCSR) -- the task of answering a
commonsense question without any pre-defined choices -- using as a resource
only a corpus of commonsense facts written in natural language. OpenCSR is
challenging due to a large decision space, and because many questions require
implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an
efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To
evaluate OpenCSR methods, we adapt several popular commonsense reasoning
benchmarks, and collect multiple new answers for each test question via
crowd-sourcing. Experiments show that DrFact outperforms strong baseline
methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Adversarial Robustness and Optimization Bias. (arXiv:2102.08868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08868</id>
        <link href="http://arxiv.org/abs/2102.08868"/>
        <updated>2021-06-08T02:20:21.553Z</updated>
        <summary type="html"><![CDATA[We demonstrate that the choice of optimizer, neural network architecture, and
regularizer significantly affect the adversarial robustness of linear neural
networks, providing guarantees without the need for adversarial training. To
this end, we revisit a known result linking maximally robust classifiers and
minimum norm solutions, and combine it with recent results on the implicit bias
of optimizers. First, we show that, under certain conditions, it is possible to
achieve both perfect standard accuracy and a certain degree of robustness,
simply by training an overparametrized model using the implicit bias of the
optimization. In that regime, there is a direct relationship between the type
of the optimizer and the attack to which the model is robust. To the best of
our knowledge, this work is the first to study the impact of optimization
methods such as sign gradient descent and proximal methods on adversarial
robustness. Second, we characterize the robustness of linear convolutional
models, showing that they resist attacks subject to a constraint on the
Fourier-$\ell_\infty$ norm. To illustrate these findings we design a novel
Fourier-$\ell_\infty$ attack that finds adversarial examples with controllable
frequencies. We evaluate Fourier-$\ell_\infty$ robustness of
adversarially-trained deep CIFAR-10 models from the standard RobustBench
benchmark and visualize adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Context Enhancement Network for Automatic Skin Lesion Segmentation. (arXiv:2004.08107v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08107</id>
        <link href="http://arxiv.org/abs/2004.08107"/>
        <updated>2021-06-08T02:20:21.547Z</updated>
        <summary type="html"><![CDATA[Skin lesion segmentation is an important step for automatic melanoma
diagnosis. Due to the non-negligible diversity of lesions from different
patients, extracting powerful context for fine-grained semantic segmentation is
still challenging today. Although the deep convolutional neural network (CNNs)
have made significant improvements on skin lesion segmentation, they often fail
to reserve the spatial details and long-range dependencies context due to
consecutive convolution striding and pooling operations inside CNNs. In this
paper, we formulate a cascaded context enhancement neural network for automatic
skin lesion segmentation. A new cascaded context aggregation (CCA) module with
a gate-based information integration approach is proposed to sequentially and
selectively aggregate original image and multi-level features from the encoder
sub-network. The generated context is further utilized to guide discriminative
features extraction by the designed context-guided local affinity (CGL) module.
Furthermore, an auxiliary loss is added to the CCA module for refining the
prediction. In our work, we evaluate our approach on four public skin
dermoscopy image datasets. The proposed method achieves the Jaccard Index (JA)
of 87.1%, 80.3%, 83.4%, and 86.6% on ISIC-2016, ISIC-2017, ISIC-2018, and PH2
datasets, which are higher than other state-of-the-art models respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chaojie Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Ye Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning by Self-Transition for Handling Noisy Labels. (arXiv:2012.04337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04337</id>
        <link href="http://arxiv.org/abs/2012.04337"/>
        <updated>2021-06-08T02:20:21.531Z</updated>
        <summary type="html"><![CDATA[Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Image Deformation Based on Landmark Detection. (arXiv:1910.13671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.13671</id>
        <link href="http://arxiv.org/abs/1910.13671"/>
        <updated>2021-06-08T02:20:21.524Z</updated>
        <summary type="html"><![CDATA[In this work, we use facial landmarks to make the deformation for facial
images more authentic. The deformation includes the expansion of eyes and the
shrinking of noses, mouths, and cheeks. An advanced 106-point facial landmark
detector is utilized to provide control points for deformation. Bilinear
interpolation is used in the expansion and Moving Least Squares methods (MLS)
including Affine Deformation, Similarity Deformation and Rigid Deformation are
used in the shrinking. We compare the running time as well as the quality of
deformed images using different MLS methods. The experimental results show that
the Rigid Deformation which can keep other parts of the images unchanged
performs better even if it takes the longest time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chaoyue Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yugang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shulai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Mesh Regression with Dense Correspondence. (arXiv:2006.05734v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05734</id>
        <link href="http://arxiv.org/abs/2006.05734"/>
        <updated>2021-06-08T02:20:21.516Z</updated>
        <summary type="html"><![CDATA[Estimating 3D mesh of the human body from a single 2D image is an important
task with many applications such as augmented reality and Human-Robot
interaction. However, prior works reconstructed 3D mesh from global image
feature extracted by using convolutional neural network (CNN), where the dense
correspondences between the mesh surface and the image pixels are missing,
leading to suboptimal solution. This paper proposes a model-free 3D human mesh
estimation framework, named DecoMR, which explicitly establishes the dense
correspondence between the mesh and the local image features in the UV space
(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts
pixel-to-surface dense correspondence map (i.e., IUV image), with which we
transfer local features from the image space to the UV space. Then the
transferred local image features are processed in the UV space to regress a
location map, which is well aligned with transferred features. Finally we
reconstruct 3D human mesh from the regressed location map with a predefined
mapping function. We also observe that the existing discontinuous UV map are
unfriendly to the learning of network. Therefore, we propose a novel UV map
that maintains most of the neighboring relations on the original mesh surface.
Experiments demonstrate that our proposed local feature alignment and
continuous UV map outperforms existing 3D mesh based methods on multiple public
benchmarks. Code will be made available at
https://github.com/zengwang430521/DecoMR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Temporal Graph Neural Network for Trajectory Prediction. (arXiv:2106.02930v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02930</id>
        <link href="http://arxiv.org/abs/2106.02930"/>
        <updated>2021-06-08T02:20:21.508Z</updated>
        <summary type="html"><![CDATA[An effective understanding of the contextual environment and accurate motion
forecasting of surrounding agents is crucial for the development of autonomous
vehicles and social mobile robots. This task is challenging since the behavior
of an autonomous agent is not only affected by its own intention, but also by
the static environment and surrounding dynamically interacting agents. Previous
works focused on utilizing the spatial and temporal information in time domain
while not sufficiently taking advantage of the cues in frequency domain. To
this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which
can capture inter-agent correlations and temporal dependency simultaneously in
frequency domain in addition to time domain. SpecTGNN operates on both an agent
graph with dynamic state information and an environment graph with the features
extracted from context images in two streams. The model integrates graph
Fourier transform, spectral graph convolution and temporal gated convolution to
encode history information and forecast future trajectories. Moreover, we
incorporate a multi-head spatio-temporal attention mechanism to mitigate the
effect of error propagation in a long time horizon. We demonstrate the
performance of SpecTGNN on two public trajectory prediction benchmark datasets,
which achieves state-of-the-art performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism. (arXiv:2103.07054v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07054</id>
        <link href="http://arxiv.org/abs/2103.07054"/>
        <updated>2021-06-08T02:20:21.501Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on category-level 6D pose and size estimation from
monocular RGB-D image. Previous methods suffer from inefficient category-level
pose feature extraction which leads to low accuracy and inference speed. To
tackle this problem, we propose a fast shape-based network (FS-Net) with
efficient category-level feature extraction for 6D pose estimation. First, we
design an orientation aware autoencoder with 3D graph convolution for latent
feature extraction. The learned latent feature is insensitive to point shift
and object size thanks to the shift and scale-invariance properties of the 3D
graph convolution. Then, to efficiently decode category-level rotation
information from the latent feature, we propose a novel decoupled rotation
mechanism that employs two decoders to complementarily access the rotation
information. Meanwhile, we estimate translation and size by two residuals,
which are the difference between the mean of object points and ground truth
translation, and the difference between the mean size of the category and
ground truth size, respectively. Finally, to increase the generalization
ability of FS-Net, we propose an online box-cage based 3D deformation mechanism
to augment the training data. Extensive experiments on two benchmark datasets
show that the proposed method achieves state-of-the-art performance in both
category- and instance-level 6D object pose estimation. Especially in
category-level pose estimation, without extra synthetic data, our method
outperforms existing methods by 6.3% on the NOCS-REAL dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xi Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinming Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guidance and Teaching Network for Video Salient Object Detection. (arXiv:2105.10110v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10110</id>
        <link href="http://arxiv.org/abs/2105.10110"/>
        <updated>2021-06-08T02:20:21.494Z</updated>
        <summary type="html"><![CDATA[Owing to the difficulties of mining spatial-temporal cues, the existing
approaches for video salient object detection (VSOD) are limited in
understanding complex and noisy scenarios, and often fail in inferring
prominent objects. To alleviate such shortcomings, we propose a simple yet
efficient architecture, termed Guidance and Teaching Network (GTNet), to
independently distil effective spatial and temporal cues with implicit guidance
and explicit teaching at feature- and decision-level, respectively. To be
specific, we (a) introduce a temporal modulator to implicitly bridge features
from motion into the appearance branch, which is capable of fusing cross-modal
features collaboratively, and (b) utilise motion-guided mask to propagate the
explicit cues during the feature aggregation. This novel learning strategy
achieves satisfactory results via decoupling the complex spatial-temporal cues
and mapping informative cues across different modalities. Extensive experiments
on three challenging benchmarks show that the proposed method can run at ~28
fps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yingxia Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shouyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Ge Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Prostate Cancer Detection in bpMRI via 3D CNNs: Effects of Attention Mechanisms, Clinical Priori and Decoupled False Positive Reduction. (arXiv:2101.03244v8 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03244</id>
        <link href="http://arxiv.org/abs/2101.03244"/>
        <updated>2021-06-08T02:20:21.487Z</updated>
        <summary type="html"><![CDATA[We present a multi-stage 3D computer-aided detection and diagnosis (CAD)
model for automated localization of clinically significant prostate cancer
(csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive
its detection network, targeting salient structures and highly discriminative
feature dimensions across multiple resolutions. Its goal is to accurately
identify csPCa lesions from indolent cancer and the wide range of benign
pathology that can afflict the prostate gland. Simultaneously, a decoupled
residual classifier is used to achieve consistent false positive reduction,
without sacrificing high sensitivity or computational efficiency. In order to
guide model generalization with domain-specific clinical knowledge, a
probabilistic anatomical prior is used to encode the spatial prevalence and
zonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired
with radiologically-estimated annotations, we hypothesize that such CNN-based
models can be trained to detect biopsy-confirmed malignancies in an independent
cohort.

For 486 institutional testing scans, the 3D CAD system achieves
83.69$\pm$5.22% and 93.19$\pm$2.96% detection sensitivity at 0.50 and 1.46
false positive(s) per patient, respectively, with 0.882$\pm$0.030 AUROC in
patient-based diagnosis $-$significantly outperforming four state-of-the-art
baseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from
recent literature. For 296 external biopsy-confirmed testing scans, the
ensembled CAD system shares moderate agreement with a consensus of expert
radiologists (76.69%; $kappa$ $=$ 0.51$\pm$0.04) and independent pathologists
(81.08%; $kappa$ $=$ 0.56$\pm$0.06); demonstrating strong generalization to
histologically-confirmed csPCa diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1"&gt;Anindo Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1"&gt;Matin Hosseinzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1"&gt;Henkjan Huisman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Segmentation Learning from Sparse Annotations and Hierarchical Descriptors. (arXiv:2105.12885v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12885</id>
        <link href="http://arxiv.org/abs/2105.12885"/>
        <updated>2021-06-08T02:20:21.471Z</updated>
        <summary type="html"><![CDATA[One of the main obstacles to 3D semantic segmentation is the significant
amount of endeavor required to generate expensive point-wise annotations for
fully supervised training. To alleviate manual efforts, we propose GIDSeg, a
novel approach that can simultaneously learn segmentation from sparse
annotations via reasoning global-regional structures and individual-vicinal
properties. GIDSeg depicts global- and individual- relation via a dynamic edge
convolution network coupled with a kernelized identity descriptor. The ensemble
effects are obtained by endowing a fine-grained receptive field to a
low-resolution voxelized map. In our GIDSeg, an adversarial learning module is
also designed to further enhance the conditional constraint of identity
descriptors within the joint feature distribution. Despite the apparent
simplicity, our proposed approach achieves superior performance over
state-of-the-art for inferencing 3D dense segmentation with only sparse
annotations. Particularly, with $5\%$ annotations of raw data, GIDSeg
outperforms other 3D segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jianmin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images. (arXiv:2010.08115v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08115</id>
        <link href="http://arxiv.org/abs/2010.08115"/>
        <updated>2021-06-08T02:20:21.444Z</updated>
        <summary type="html"><![CDATA[The infection of respiratory coronavirus disease 2019 (COVID-19) starts with
the upper respiratory tract and as the virus grows, the infection can progress
to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is
reverse transcription polymerase chain reaction (RT-PCR), which is less
sensitive during early stages; especially if the patient is asymptomatic, which
may further cause more severe pneumonia. In this context, several deep learning
models have been proposed to identify pulmonary infections using publicly
available chest X-ray (CXR) image datasets for early diagnosis, better
treatment and quick cure. In these datasets, presence of less number of
COVID-19 positive samples compared to other classes (normal, pneumonia and
Tuberculosis) raises the challenge for unbiased learning of deep learning
models. All deep learning models opted class balancing techniques to solve this
issue; which however should be avoided in any medical diagnosis process.
Moreover, the deep learning models are also data hungry and need massive
computation resources. Therefore for quicker diagnosis, this research proposes
a novel pinball loss function based one-class support vector machine
(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples
with objectives to maximize the learning efficiency and to minimize the false
predictions. The performance of the proposed model is compared with
conventional OCSVM and existing deep learning models, and the experimental
results prove that the proposed model outperformed over state-of-the-art
methods. To validate the robustness of the proposed model, experiments are also
performed with noisy CXR images and UCI benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagabhushan_P/0/1/0/all/0/1"&gt;P. Nagabhushan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-06-08T02:20:21.419Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification. (arXiv:2106.02864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02864</id>
        <link href="http://arxiv.org/abs/2106.02864"/>
        <updated>2021-06-08T02:20:21.393Z</updated>
        <summary type="html"><![CDATA[Researchers working on computational analysis of Whole Slide Images (WSIs) in
histopathology have primarily resorted to patch-based modelling due to large
resolution of each WSI. The large resolution makes WSIs infeasible to be fed
directly into the machine learning models due to computational constraints.
However, due to patch-based analysis, most of the current methods fail to
exploit the underlying spatial relationship among the patches. In our work, we
have tried to integrate this relationship along with feature-based correlation
among the extracted patches from the particular tumorous region. For the given
task of classification, we have used BiLSTMs to model both forward and backward
contextual relationship. RNN based models eliminate the limitation of sequence
size by allowing the modelling of variable size images within a deep learning
model. We have also incorporated the effect of spatial continuity by exploring
different scanning techniques used to sample patches. To establish the
efficiency of our approach, we trained and tested our model on two datasets,
microscopy images and WSI tumour regions. After comparing with contemporary
literature we achieved the better performance with accuracy of 90% for
microscopy image dataset. For WSI tumour region dataset, we compared the
classification results with deep learning networks such as ResNet, DenseNet,
and InceptionV3 using maximum voting technique. We achieved the highest
performance accuracy of 84%. We found out that BiLSTMs with CNN features have
performed much better in modelling patches into an end-to-end Image
classification network. Additionally, the variable dimensions of WSI tumour
regions were used for classification without the need for resizing. This
suggests that our method is independent of tumour image size and can process
large dimensional images without losing the resolution details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwee Kuan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03417</id>
        <link href="http://arxiv.org/abs/2103.03417"/>
        <updated>2021-06-08T02:20:21.386Z</updated>
        <summary type="html"><![CDATA[The measurement of bias in machine learning often focuses on model
performance across identity subgroups (such as man and woman) with respect to
groundtruth labels. However, these methods do not directly measure the
associations that a model may have learned, for example between labels and
identity subgroups. Further, measuring a model's bias requires a fully
annotated evaluation dataset which may not be easily available in practice. We
present an elegant mathematical solution that tackles both issues
simultaneously, using image classification as a working example. By treating a
classification model's predictions for a given image as a set of labels
analogous to a bag of words, we rank the biases that a model has learned with
respect to different identity labels. We use (man, woman) as a concrete example
of an identity label set (although this set need not be binary), and present
rankings for the labels that are most biased towards one identity or the other.
We demonstrate how the statistical properties of different association metrics
can lead to different rankings of the most "gender biased" labels, and conclude
that normalized pointwise mutual information (nPMI) is most useful in practice.
Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aka_O/0/1/0/all/0/1"&gt;Osman Aka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_K/0/1/0/all/0/1"&gt;Ken Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1"&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_C/0/1/0/all/0/1"&gt;Christina Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Margaret Mitchell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning. (arXiv:2105.14167v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14167</id>
        <link href="http://arxiv.org/abs/2105.14167"/>
        <updated>2021-06-08T02:20:21.356Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) based language models achieve high performance on various
benchmarks for Natural Language Inference (NLI). And at this time, symbolic
approaches to NLI are receiving less attention. Both approaches (symbolic and
DL) have their advantages and weaknesses. However, currently, no method
combines them in a system to solve the task of NLI. To merge symbolic and deep
learning methods, we propose an inference framework called NeuralLog, which
utilizes both a monotonicity-based logical inference engine and a neural
network language model for phrase alignment. Our framework models the NLI task
as a classic search problem and uses the beam search algorithm to search for
optimal inference paths. Experiments show that our joint logic and neural
inference system improves accuracy on the NLI task and can achieve state-of-art
accuracy on the SICK and MED datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qiyue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moss_L/0/1/0/all/0/1"&gt;Lawrence S. Moss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZeroWaste Dataset: Towards Automated Waste Recycling. (arXiv:2106.02740v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02740</id>
        <link href="http://arxiv.org/abs/2106.02740"/>
        <updated>2021-06-08T02:20:21.296Z</updated>
        <summary type="html"><![CDATA[Less than 35% of recyclable waste is being actually recycled in the US, which
leads to increased soil and sea pollution and is one of the major concerns of
environmental researchers as well as the common public. At the heart of the
problem is the inefficiencies of the waste sorting process (separating paper,
plastic, metal, glass, etc.) due to the extremely complex and cluttered nature
of the waste stream. Automated waste detection strategies have a great
potential to enable more efficient, reliable and safer waste sorting practices,
but the literature lacks comprehensive datasets and methodology for the
industrial waste sorting solutions. In this paper, we take a step towards
computer-aided waste detection and present the first in-the-wild
industrial-grade waste detection and segmentation dataset, ZeroWaste. This
dataset contains over1800fully segmented video frames collected from a real
waste sorting plant along with waste material labels for training and
evaluation of the segmentation methods, as well as over6000unlabeled frames
that can be further used for semi-supervised and self-supervised learning
techniques. ZeroWaste also provides frames of the conveyor belt before and
after the sorting process, comprising a novel setup that can be used for
weakly-supervised segmentation. We present baselines for fully-, semi- and
weakly-supervised segmentation methods. Our experimental results demonstrate
that state-of-the-art segmentation methods struggle to correctly detect and
classify target objects which suggests the challenging nature of our proposed
in-the-wild dataset. We believe that ZeroWastewill catalyze research in object
detection and semantic segmentation in extreme clutter as well as applications
in the recycling domain. Our project page can be found
atthis http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Ziliang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akl_J/0/1/0/all/0/1"&gt;James Akl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alladkani_F/0/1/0/all/0/1"&gt;Fadi Alladkani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Ping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1"&gt;Vitaly Ablavsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1"&gt;Berk Calli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02874</id>
        <link href="http://arxiv.org/abs/2106.02874"/>
        <updated>2021-06-08T02:20:21.234Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled
source domain and an unsupervised loss in an unlabeled target domain, which
often faces more severe overfitting (than classical supervised learning) as the
supervised source loss has clear domain gap and the unsupervised target loss is
often noisy due to the lack of annotations. This paper presents RDA, a robust
domain adaptation technique that introduces adversarial attacking to mitigate
overfitting in UDA. We achieve robust domain adaptation by a novel Fourier
adversarial attacking (FAA) method that allows large magnitude of perturbation
noises but has minimal modification of image semantics, the former is critical
to the effectiveness of its generated adversarial samples due to the existence
of 'domain gaps'. Specifically, FAA decomposes images into multiple frequency
components (FCs) and generates adversarial samples by just perturbating certain
FCs that capture little semantic information. With FAA-generated samples, the
training can continue the 'random walk' and drift into an area with a flat loss
landscape, leading to more robust domain adaptation. Extensive experiments over
multiple domain adaptation tasks show that RDA can work with different computer
vision tasks with superior performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:21.199Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar-Camera Pixel Depth Association for Depth Completion. (arXiv:2106.02778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02778</id>
        <link href="http://arxiv.org/abs/2106.02778"/>
        <updated>2021-06-08T02:20:21.190Z</updated>
        <summary type="html"><![CDATA[While radar and video data can be readily fused at the detection level,
fusing them at the pixel level is potentially more beneficial. This is also
more challenging in part due to the sparsity of radar, but also because
automotive radar beams are much wider than a typical pixel combined with a
large baseline between camera and radar, which results in poor association
between radar pixels and color pixel. A consequence is that depth completion
methods designed for LiDAR and video fare poorly for radar and video. Here we
propose a radar-to-pixel association stage which learns a mapping from radar
returns to pixels. This mapping also serves to densify radar returns. Using
this as a first stage, followed by a more traditional depth completion method,
we are able to achieve image-guided depth completion with radar and video. We
demonstrate performance superior to camera and radar alone on the nuScenes
dataset. Our source code is available at https://github.com/longyunf/rc-pda.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;Daniel Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_M/0/1/0/all/0/1"&gt;Marcos Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1"&gt;Punarjay Chakravarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1"&gt;Praveen Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Task Adaptation using Natural Language. (arXiv:2106.02972v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02972</id>
        <link href="http://arxiv.org/abs/2106.02972"/>
        <updated>2021-06-08T02:20:21.183Z</updated>
        <summary type="html"><![CDATA[Imitation learning and instruction-following are two common approaches to
communicate a user's intent to a learning agent. However, as the complexity of
tasks grows, it could be beneficial to use both demonstrations and language to
communicate with an agent. In this work, we propose a novel setting where an
agent is given both a demonstration and a description, and must combine
information from both the modalities. Specifically, given a demonstration for a
task (the source task), and a natural language description of the differences
between the demonstrated task and a related but different task (the target
task), our goal is to train an agent to complete the target task in a zero-shot
setting, that is, without any demonstrations for the target task. To this end,
we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a
source demonstration and a linguistic description of how the target task
differs, learns to output a reward / value function that accurately describes
the target task. Our experiments show that on a diverse set of adaptations, our
approach is able to complete more than 95% of target tasks when using
template-based descriptions, and more than 70% when using free-form natural
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Domain Adaptation via Adaptive and Progressive Feature Alignment. (arXiv:2106.02845v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02845</id>
        <link href="http://arxiv.org/abs/2106.02845"/>
        <updated>2021-06-08T02:20:21.174Z</updated>
        <summary type="html"><![CDATA[Contemporary domain adaptive semantic segmentation aims to address data
annotation challenges by assuming that target domains are completely
unannotated. However, annotating a few target samples is usually very
manageable and worthwhile especially if it improves the adaptation performance
substantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive
image Segmentation network that employs a few labeled target samples as anchors
for adaptive and progressive feature alignment between labeled source samples
and unlabeled target samples. We position the few labeled target samples as
references that gauge the similarity between source and target features and
guide adaptive inter-domain alignment for learning more similar source
features. In addition, we replace the dissimilar source features by
high-confidence target features continuously during the iterative training
process, which achieves progressive intra-domain alignment between confident
and unconfident target features. Extensive experiments show the proposed SSDAS
greatly outperforms a number of baselines, i.e., UDA-based semantic
segmentation and SSDA-based image classification. In addition, SSDAS is
complementary and can be easily incorporated into UDA-based methods with
consistent improvements in domain adaptive semantic segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v3 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11015</id>
        <link href="http://arxiv.org/abs/2008.11015"/>
        <updated>2021-06-08T02:20:21.152Z</updated>
        <summary type="html"><![CDATA[It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingtao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xinyi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuejiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (arXiv:2103.09593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09593</id>
        <link href="http://arxiv.org/abs/2103.09593"/>
        <updated>2021-06-08T02:20:21.141Z</updated>
        <summary type="html"><![CDATA[Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making CNNs Interpretable by Building Dynamic Sequential Decision Forests with Top-down Hierarchy Learning. (arXiv:2106.02824v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02824</id>
        <link href="http://arxiv.org/abs/2106.02824"/>
        <updated>2021-06-08T02:20:21.130Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a generic model transfer scheme to make
Convlutional Neural Networks (CNNs) interpretable, while maintaining their high
classification accuracy. We achieve this by building a differentiable decision
forest on top of CNNs, which enjoys two characteristics: 1) During training,
the tree hierarchies of the forest are learned in a top-down manner under the
guidance from the category semantics embedded in the pre-trained CNN weights;
2) During inference, a single decision tree is dynamically selected from the
forest for each input sample, enabling the transferred model to make sequential
decisions corresponding to the attributes shared by semantically-similar
categories, rather than directly performing flat classification. We name the
transferred model deep Dynamic Sequential Decision Forest (dDSDF). Experimental
results show that dDSDF not only achieves higher classification accuracy than
its conuterpart, i.e., the original CNN, but has much better interpretability,
as qualitatively it has plausible hierarchies and quantitatively it leads to
more precise saliency maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yilin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shaozuo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval. (arXiv:2010.11915v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11915</id>
        <link href="http://arxiv.org/abs/2010.11915"/>
        <updated>2021-06-08T02:20:21.119Z</updated>
        <summary type="html"><![CDATA[Recent pretrained language models "solved" many reading comprehension
benchmarks, where questions are written with access to the evidence document.
However, datasets containing information-seeking queries where evidence
documents are provided after the queries are written independently remain
challenging. We analyze why answering information-seeking queries is more
challenging and where their prevalent unanswerabilities arise, on Natural
Questions and TyDi QA. Our controlled experiments suggest two headrooms --
paragraph selection and answerability prediction, i.e. whether the paired
evidence document contains the answer to the query or not. When provided with a
gold paragraph and knowing when to abstain from answering, existing models
easily outperform a human annotator. However, predicting answerability itself
remains challenging. We manually annotate 800 unanswerable examples across six
languages on what makes them challenging to answer. With this new data, we
conduct per-category answerability prediction, revealing issues in the current
dataset collection as well as task formulation. Together, our study points to
avenues for future research in information-seeking question answering, both for
dataset creation and model development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1"&gt;Akari Asai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Eunsol Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor Based on Multi-Task Adversarial Training. (arXiv:2104.10336v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10336</id>
        <link href="http://arxiv.org/abs/2104.10336"/>
        <updated>2021-06-08T02:20:21.109Z</updated>
        <summary type="html"><![CDATA[This paper describes MagicPai's system for SemEval 2021 Task 7, HaHackathon:
Detecting and Rating Humor and Offense. This task aims to detect whether the
text is humorous and how humorous it is. There are four subtasks in the
competition. In this paper, we mainly present our solution, a multi-task
learning model based on adversarial examples, for task 1a and 1b. More
specifically, we first vectorize the cleaned dataset and add the perturbation
to obtain more robust embedding representations. We then correct the loss via
the confidence level. Finally, we perform interactive joint learning on
multiple tasks to capture the relationship between whether the text is humorous
and how humorous it is. The final result shows the effectiveness of our system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1"&gt;Xiaoyi Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchGen: Generating Constrained CAD Sketches. (arXiv:2106.02711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02711</id>
        <link href="http://arxiv.org/abs/2106.02711"/>
        <updated>2021-06-08T02:20:21.086Z</updated>
        <summary type="html"><![CDATA[Computer-aided design (CAD) is the most widely used modeling approach for
technical design. The typical starting point in these designs is 2D sketches
which can later be extruded and combined to obtain complex three-dimensional
assemblies. Such sketches are typically composed of parametric primitives, such
as points, lines, and circular arcs, augmented with geometric constraints
linking the primitives, such as coincidence, parallelism, or orthogonality.
Sketches can be represented as graphs, with the primitives as nodes and the
constraints as edges. Training a model to automatically generate CAD sketches
can enable several novel workflows, but is challenging due to the complexity of
the graphs and the heterogeneity of the primitives and constraints. In
particular, each type of primitive and constraint may require a record of
different size and parameter types. We propose SketchGen as a generative model
based on a transformer architecture to address the heterogeneity problem by
carefully designing a sequential language for the primitives and constraints
that allows distinguishing between different primitive or constraint types and
their parameters, while encouraging our model to re-use information across
related parameters, encoding shared structure. A particular highlight of our
work is the ability to produce primitives linked via constraints that enables
the final output to be further regularized via a constraint solver. We evaluate
our model by demonstrating constraint prediction for given sets of primitives
and full sketch generation from scratch, showing that our approach
significantly out performs the state-of-the-art in CAD sketch generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1"&gt;Wamiq Reyaz Para&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1"&gt;Tom Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Camera Vehicle Counting Using Edge-AI. (arXiv:2106.02842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02842</id>
        <link href="http://arxiv.org/abs/2106.02842"/>
        <updated>2021-06-08T02:20:21.079Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel solution to automatically count vehicles in a
parking lot using images captured by smart cameras. Unlike most of the
literature on this task, which focuses on the analysis of single images, this
paper proposes the use of multiple visual sources to monitor a wider parking
area from different perspectives. The proposed multi-camera system is capable
of automatically estimate the number of cars present in the entire parking lot
directly on board the edge devices. It comprises an on-device deep
learning-based detector that locates and counts the vehicles from the captured
images and a decentralized geometric-based approach that can analyze the
inter-camera shared areas and merge the data acquired by all the devices. We
conduct the experimental evaluation on an extended version of the CNRPark-EXT
dataset, a collection of images taken from the parking lot on the campus of the
National Research Council (CNR) in Pisa, Italy. We show that our system is
robust and takes advantage of the redundant information deriving from the
different cameras, improving the overall performance without requiring any
extra geometrical information of the monitored scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciampi_L/0/1/0/all/0/1"&gt;Luca Ciampi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1"&gt;Claudio Gennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrara_F/0/1/0/all/0/1"&gt;Fabio Carrara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1"&gt;Fabrizio Falchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vairo_C/0/1/0/all/0/1"&gt;Claudio Vairo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1"&gt;Giuseppe Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO: accurate Discrete Scale Convolutions. (arXiv:2106.02733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02733</id>
        <link href="http://arxiv.org/abs/2106.02733"/>
        <updated>2021-06-08T02:20:21.069Z</updated>
        <summary type="html"><![CDATA[Scale is often seen as a given, disturbing factor in many vision tasks. When
doing so it is one of the factors why we need more data during learning. In
recent work scale equivariance was added to convolutional neural networks. It
was shown to be effective for a range of tasks. We aim for accurate
scale-equivariant convolutional neural networks (SE-CNNs) applicable for
problems where high granularity of scale and small filter sizes are required.
Current SE-CNNs rely on weight sharing and filter rescaling, the latter of
which is accurate for integer scales only. To reach accurate scale
equivariance, we derive general constraints under which scale-convolution
remains equivariant to discrete rescaling. We find the exact solution for all
cases where it exists, and compute the approximation for the rest. The discrete
scale-convolution pays off, as demonstrated in a new state-of-the-art
classification on MNIST-scale and improving the results on STL-10. With the
same SE scheme, we also improve the computational effort of a scale-equivariant
Siamese tracker on OTB-13.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1"&gt;Artem Moskalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-hop Question Answering via Reasoning Chains. (arXiv:1910.02610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.02610</id>
        <link href="http://arxiv.org/abs/1910.02610"/>
        <updated>2021-06-08T02:20:21.060Z</updated>
        <summary type="html"><![CDATA[Multi-hop question answering requires models to gather information from
different parts of a text to answer a question. Most current approaches learn
to address this task in an end-to-end way with neural networks, without
maintaining an explicit representation of the reasoning process. We propose a
method to extract a discrete reasoning chain over the text, which consists of a
series of sentences leading to the answer. We then feed the extracted chains to
a BERT-based QA model to do final answer prediction. Critically, we do not rely
on gold annotated chains or "supporting facts:" at training time, we derive
pseudogold reasoning chains using heuristics based on named entity recognition
and coreference resolution. Nor do we rely on these annotations at test time,
as our model learns to extract chains from raw text alone. We test our approach
on two recently proposed large multi-hop question answering datasets: WikiHop
and HotpotQA, and achieve state-of-art performance on WikiHop and strong
performance on HotpotQA. Our analysis shows the properties of chains that are
crucial for high performance: in particular, modeling extraction sequentially
is important, as is dealing with each candidate sentence in a context-aware
way. Furthermore, human evaluation shows that our extracted chains allow humans
to give answers with high confidence, indicating that these are a strong
intermediate abstraction for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shih-ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLSD: The Global Large-Scale Ship Database and Baseline Evaluations. (arXiv:2106.02773v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02773</id>
        <link href="http://arxiv.org/abs/2106.02773"/>
        <updated>2021-06-08T02:20:21.051Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a challenging global large-scale ship database
(called GLSD), designed specifically for ship detection tasks. The designed
GLSD database includes a total of 140,616 annotated instances from 100,729
images. Based on the collected images, we propose 13 categories that widely
exists in international routes. These categories include sailing boat, fishing
boat, passenger ship, war ship, general cargo ship, container ship, bulk cargo
carrier, barge, ore carrier, speed boat, canoe, oil carrier, and tug. The
motivations of developing GLSD include the following: 1) providing a refined
ship detection database; 2) providing the worldwide researchers of ship
detection and exhaustive label information (bounding box and ship class label)
in one uniform global database; and 3) providing a large-scale ship database
with geographic information (port and country information) that benefits
multi-modal analysis. In addition, we discuss the evaluation protocols given
image characteristics in GLSD and analyze the performance of selected
state-of-the-art object detection algorithms on GSLD, providing baselines for
future studies. More information regarding the designed GLSD can be found at
https://github.com/jiaming-wang/GLSD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lianbing Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xianwei Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Inference for Reasoning Over Paragraphs. (arXiv:2004.02995v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02995</id>
        <link href="http://arxiv.org/abs/2004.02995"/>
        <updated>2021-06-08T02:20:21.026Z</updated>
        <summary type="html"><![CDATA[Complex reasoning over text requires understanding and chaining together
free-form predicates and logical connectives. Prior work has largely tried to
do this either symbolically or with black-box transformers. We present a middle
ground between these two extremes: a compositional model reminiscent of neural
module networks that can perform chained logical reasoning. This model first
finds relevant sentences in the context and then chains them together using
neural modules. Our model gives significant performance improvements (up to
29\% relative error reduction when comfibined with a reranker) on ROPES, a
recently introduced complex reasoning dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiangming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Shay B. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GO FIGURE: A Meta Evaluation of Factuality in Summarization. (arXiv:2010.12834v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12834</id>
        <link href="http://arxiv.org/abs/2010.12834"/>
        <updated>2021-06-08T02:20:21.017Z</updated>
        <summary type="html"><![CDATA[While neural language models can generate text with remarkable fluency and
coherence, controlling for factual correctness in generation remains an open
research question. This major discrepancy between the surface-level fluency and
the content-level correctness of neural generation has motivated a new line of
research that seeks automatic metrics for evaluating the factuality of machine
text. In this paper, we introduce GO FIGURE, a meta-evaluation framework for
evaluating factuality evaluation metrics. We propose five necessary and
intuitive conditions to evaluate factuality metrics on diagnostic factuality
data across three different summarization tasks. Our benchmark analysis on ten
factuality metrics reveals that our meta-evaluation framework provides a robust
and efficient evaluation that is extensible to multiple types of factual
consistency and standard generation metrics, including QA metrics. It also
reveals that while QA metrics generally improve over standard metrics that
measure factuality across domains, performance is highly dependent on the way
in which questions are generated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1"&gt;Saadia Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1"&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1"&gt;Rahul Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lookup-Table Recurrent Language Models for Long Tail Speech Recognition. (arXiv:2104.04552v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04552</id>
        <link href="http://arxiv.org/abs/2104.04552"/>
        <updated>2021-06-08T02:20:21.004Z</updated>
        <summary type="html"><![CDATA[We introduce Lookup-Table Language Models (LookupLM), a method for scaling up
the size of RNN language models with only a constant increase in the floating
point operations, by increasing the expressivity of the embedding table. In
particular, we instantiate an (additional) embedding table which embeds the
previous n-gram token sequence, rather than a single token. This allows the
embedding table to be scaled up arbitrarily -- with a commensurate increase in
performance -- without changing the token vocabulary. Since embeddings are
sparsely retrieved from the table via a lookup; increasing the size of the
table adds neither extra operations to each forward pass nor extra parameters
that need to be stored on limited GPU/TPU memory. We explore scaling n-gram
embedding tables up to nearly a billion parameters. When trained on a 3-billion
sentence corpus, we find that LookupLM improves long tail log perplexity by
2.44 and long tail WER by 23.4% on a downstream speech recognition task over a
standard RNN language model baseline, an improvement comparable to a scaling up
the baseline by 6.2x the number of floating point operations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;W. Ronny Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1"&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1"&gt;Cal Peyser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Shankar Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1"&gt;David Rybach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1"&gt;Trevor Strohman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning. (arXiv:2012.15699v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15699</id>
        <link href="http://arxiv.org/abs/2012.15699"/>
        <updated>2021-06-08T02:20:20.995Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) perform poorly under adversarial attacks.
To improve the adversarial robustness, adversarial data augmentation (ADA) has
been widely adopted to cover more search space of adversarial attacks by adding
textual adversarial examples during training. However, the number of
adversarial examples for text augmentation is still extremely insufficient due
to the exponentially large attack search space. In this work, we propose a
simple and effective method to cover a much larger proportion of the attack
search space, called Adversarial and Mixup Data Augmentation (AMDA).
Specifically, AMDA linearly interpolates the representations of pairs of
training samples to form new virtual samples, which are more abundant and
diverse than the discrete text adversarial examples in conventional ADA.
Moreover, to fairly evaluate the robustness of different models, we adopt a
challenging evaluation setup, which generates a new set of adversarial examples
targeting each model. In text classification experiments of BERT and RoBERTa,
AMDA achieves significant robustness gains under two strong adversarial attacks
and alleviates the performance degradation of ADA on the clean data. Our code
is available at: https://github.com/thunlp/MixADA .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:20.962Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global-aware Beam Search for Neural Abstractive Summarization. (arXiv:2009.06891v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06891</id>
        <link href="http://arxiv.org/abs/2009.06891"/>
        <updated>2021-06-08T02:20:20.879Z</updated>
        <summary type="html"><![CDATA[This study develops a calibrated beam-based algorithm with global awareness
for neural abstractive summarization, aiming to improve the local optimality
problem of the original beam search in a rigorous way. Specifically, a novel
global protocol is proposed based on the attention distribution to stipulate
how a global optimal hypothesis should attend to the source. A global scoring
function is then developed to regulate beam search to generate summaries in a
more near-global optimal fashion. This novel design enjoys a distinctive
property, i.e. the global attention distribution could be predicted before
inference, enabling stepwise improvements on the beam search through the global
scoring function. Extensive experiments on $9$ datasets show that the
global-aware inference significantly improves state-of-the-art summarization
models even using empirical hyper-parameters. The algorithm is also proven
robust as it remains to generate meaningful texts with corrupted attention
distributions. The codes and a comprehensive set of examples are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Ye Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zixun Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1"&gt;Lu Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Word Embeddings by Averaging in a Shared Space. (arXiv:2106.02954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02954</id>
        <link href="http://arxiv.org/abs/2106.02954"/>
        <updated>2021-06-08T02:20:20.848Z</updated>
        <summary type="html"><![CDATA[We introduce a new approach for smoothing and improving the quality of word
embeddings. We consider a method of fusing word embeddings that were trained on
the same corpus but with different initializations. We project all the models
to a shared vector space using an efficient implementation of the Generalized
Procrustes Analysis (GPA) procedure, previously used in multilingual word
translation. Our word representation demonstrates consistent improvements over
the raw models as well as their simplistic average, on a range of tasks. As the
new representations are more stable and reliable, there is a noticeable
improvement in rare word evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1"&gt;Jacob Goldberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains. (arXiv:2106.02792v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02792</id>
        <link href="http://arxiv.org/abs/2106.02792"/>
        <updated>2021-06-08T02:20:20.839Z</updated>
        <summary type="html"><![CDATA[Social media has become a valuable resource for the study of suicidal
ideation and the assessment of suicide risk. Among social media platforms,
Reddit has emerged as the most promising one due to its anonymity and its focus
on topic-based communities (subreddits) that can be indicative of someone's
state of mind or interest regarding mental health disorders such as
r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on
suicide risk assessment has been the small amount of labeled data. We propose
an empirical investigation into several classes of weakly-supervised
approaches, and show that using pseudo-labeling based on related issues around
mental health (e.g., anxiety, depression) helps improve model performance for
suicide risk assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yudong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Learning of Hate Speech Classification on Social Media. (arXiv:2106.02821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02821</id>
        <link href="http://arxiv.org/abs/2106.02821"/>
        <updated>2021-06-08T02:20:20.826Z</updated>
        <summary type="html"><![CDATA[Existing work on automated hate speech classification assumes that the
dataset is fixed and the classes are pre-defined. However, the amount of data
in social media increases every day, and the hot topics changes rapidly,
requiring the classifiers to be able to continuously adapt to new data without
forgetting the previously learned knowledge. This ability, referred to as
lifelong learning, is crucial for the real-word application of hate speech
classifiers in social media. In this work, we propose lifelong learning of hate
speech classification on social media. To alleviate catastrophic forgetting, we
propose to use Variational Representation Learning (VRL) along with a memory
module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural
Network). Experimentally, we show that combining variational representation
learning and the LB-SOINN memory module achieves better performance than the
commonly-used lifelong learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1"&gt;Mai ElSherief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel method for recommendation systems using invasive weed optimization. (arXiv:2106.02831v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02831</id>
        <link href="http://arxiv.org/abs/2106.02831"/>
        <updated>2021-06-08T02:20:20.805Z</updated>
        <summary type="html"><![CDATA[One of the popular approaches in recommendation systems is Collaborative
Filtering (CF). The most significant step in CF is choosing the appropriate set
of users. For this purpose, similarity measures are usually used for computing
the similarity between a specific user and the other users. This paper proposes
a new invasive weed optimization (IWO) based CF approach that uses users'
context to identify important and effective users set. By using a newly defined
similarity measure based on both rating values and a measure values called
confidence, the proposed approach calculates the similarity between users and
thus identifies and filters the most similar users to a specific user. It then
uses IWO to calculate the importance degree of users and finally, by using the
identified important users and their importance degrees it predicts unknown
ratings. To evaluate the proposed method, several experiments have been
performed on two known real world datasets and the results show that the
proposed method improves the state of the art results up to 15% in terms of
Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soltaninejad_F/0/1/0/all/0/1"&gt;Fahimeh Soltaninejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidgoly_A/0/1/0/all/0/1"&gt;Amir Jalaly Bidgoly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MergeDistill: Merging Pre-trained Language Models using Distillation. (arXiv:2106.02834v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02834</id>
        <link href="http://arxiv.org/abs/2106.02834"/>
        <updated>2021-06-08T02:20:20.787Z</updated>
        <summary type="html"><![CDATA[Pre-trained multilingual language models (LMs) have achieved state-of-the-art
results in cross-lingual transfer, but they often lead to an inequitable
representation of languages due to limited capacity, skewed pre-training data,
and sub-optimal vocabularies. This has prompted the creation of an ever-growing
pre-trained model universe, where each model is trained on large amounts of
language or domain specific data with a carefully curated, linguistically
informed vocabulary. However, doing so brings us back full circle and prevents
one from leveraging the benefits of multilinguality. To address the gaps at
both ends of the spectrum, we propose MergeDistill, a framework to merge
pre-trained LMs in a way that can best leverage their assets with minimal
dependencies, using task-agnostic knowledge distillation. We demonstrate the
applicability of our framework in a practical setting by leveraging
pre-existing teacher LMs and training student LMs that perform competitively
with or even outperform teacher LMs trained on several orders of magnitude more
data and with a fixed model capacity. We also highlight the importance of
teacher selection and its impact on student model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1"&gt;Simran Khanuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Melvin Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiOpEd: A Corpus of Multi-Perspective News Editorials. (arXiv:2106.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02725</id>
        <link href="http://arxiv.org/abs/2106.02725"/>
        <updated>2021-06-08T02:20:20.776Z</updated>
        <summary type="html"><![CDATA[We propose MultiOpEd, an open-domain news editorial corpus that supports
various tasks pertaining to the argumentation structure in news editorials,
focusing on automatic perspective discovery. News editorial is a genre of
persuasive text, where the argumentation structure is usually implicit.
However, the arguments presented in an editorial typically center around a
concise, focused thesis, which we refer to as their perspective. MultiOpEd aims
at supporting the study of multiple tasks relevant to automatic perspective
discovery, where a system is expected to produce a single-sentence thesis
statement summarizing the arguments presented. We argue that identifying and
abstracting such natural language perspectives from editorials is a crucial
step toward studying the implicit argumentation structure in news editorials.
We first discuss the challenges and define a few conceptual tasks towards our
goal. To demonstrate the utility of MultiOpEd and the induced tasks, we study
the problem of perspective summarization in a multi-task learning setting, as a
case study. We show that, with the induced tasks as auxiliary tasks, we can
improve the quality of the perspective summary generated. We hope that
MultiOpEd will be a useful resource for future studies on argumentation in the
news editorial domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uyttendaele_X/0/1/0/all/0/1"&gt;Xander Uyttendaele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02736</id>
        <link href="http://arxiv.org/abs/2106.02736"/>
        <updated>2021-06-08T02:20:20.767Z</updated>
        <summary type="html"><![CDATA[While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable and
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable \emph{sampling}
scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our
approach, samples are proposed from the same masked conditionals used for
training the masked language models, and they are accepted or rejected based on
their energy values according to the target distribution. We validate the
effectiveness of the proposed parametrizations by exploring the quality of
samples drawn from these energy-based models on the conditional generation task
of machine translation. We theoretically and empirically justify our sampling
algorithm by showing that the masked conditionals on their own do not yield a
Markov chain whose stationary distribution is that of our target distribution,
and our approach generates higher quality samples than other recently proposed
undirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,
2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1"&gt;Kartik Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1"&gt;Chris Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PURS: Personalized Unexpected Recommender System for Improving User Satisfaction. (arXiv:2106.02771v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02771</id>
        <link href="http://arxiv.org/abs/2106.02771"/>
        <updated>2021-06-08T02:20:20.759Z</updated>
        <summary type="html"><![CDATA[Classical recommender system methods typically face the filter bubble problem
when users only receive recommendations of their familiar items, making them
bored and dissatisfied. To address the filter bubble problem, unexpected
recommendations have been proposed to recommend items significantly deviating
from user's prior expectations and thus surprising them by presenting "fresh"
and previously unexplored items to the users. In this paper, we describe a
novel Personalized Unexpected Recommender System (PURS) model that incorporates
unexpectedness into the recommendation process by providing multi-cluster
modeling of user interests in the latent space and personalized unexpectedness
via the self-attention mechanism and via selection of an appropriate unexpected
activation function. Extensive offline experiments on three real-world datasets
illustrate that the proposed PURS model significantly outperforms the
state-of-the-art baseline approaches in terms of both accuracy and
unexpectedness measures. In addition, we conduct an online A/B test at a major
video platform Alibaba-Youku, where our model achieves over 3\% increase in the
average video view per user metric. The proposed model is in the process of
being deployed by the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Communication of Generalizations. (arXiv:2106.02668v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02668</id>
        <link href="http://arxiv.org/abs/2106.02668"/>
        <updated>2021-06-08T02:20:20.733Z</updated>
        <summary type="html"><![CDATA[To build agents that can collaborate effectively with others, recent research
has trained artificial agents to communicate with each other in Lewis-style
referential games. However, this often leads to successful but uninterpretable
communication. We argue that this is due to the game objective: communicating
about a single object in a shared visual context is prone to overfitting and
does not encourage language useful beyond concrete reference. In contrast,
human language conveys a rich variety of abstract ideas. To promote such
skills, we propose games that require communicating generalizations over sets
of objects representing abstract visual concepts, optionally with separate
contexts for each agent. We find that these games greatly improve systematicity
and interpretability of the learned languages, according to several metrics in
the literature. Finally, we propose a method for identifying logical operations
embedded in the emergent languages by learning an approximate compositional
reconstruction of the language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1"&gt;Jesse Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models. (arXiv:2106.02679v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02679</id>
        <link href="http://arxiv.org/abs/2106.02679"/>
        <updated>2021-06-08T02:20:20.719Z</updated>
        <summary type="html"><![CDATA[The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.

In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1"&gt;Joel Lamy-Poirier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning Based Classification Algorithm Recommendation. (arXiv:2101.05993v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.05993</id>
        <link href="http://arxiv.org/abs/2101.05993"/>
        <updated>2021-06-08T02:20:20.580Z</updated>
        <summary type="html"><![CDATA[Recommending appropriate algorithms to a classification problem is one of the
most challenging issues in the field of data mining. The existing algorithm
recommendation models are generally constructed on only one kind of
meta-features by single learners. Considering that i) ensemble learners usually
show better performance and ii) different kinds of meta-features characterize
the classification problems in different viewpoints independently, and further
the models constructed with different sets of meta-features will be
complementary with each other and applicable for ensemble. This paper proposes
an ensemble learning-based algorithm recommendation method. To evaluate the
proposed recommendation method, extensive experiments with 13 well-known
candidate classification algorithms and five different kinds of meta-features
are conducted on 1090 benchmark classification problems. The results show the
effectiveness of the proposed ensemble learning based recommendation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qinbao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional Distillation for Top-K Recommender System. (arXiv:2106.02870v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02870</id>
        <link href="http://arxiv.org/abs/2106.02870"/>
        <updated>2021-06-08T02:20:20.569Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RS) have started to employ knowledge distillation, which
is a model compression technique training a compact model (student) with the
knowledge transferred from a cumbersome model (teacher). The state-of-the-art
methods rely on unidirectional distillation transferring the knowledge only
from the teacher to the student, with an underlying assumption that the teacher
is always superior to the student. However, we demonstrate that the student
performs better than the teacher on a significant proportion of the test set,
especially for RS. Based on this observation, we propose Bidirectional
Distillation (BD) framework whereby both the teacher and the student
collaboratively improve with each other. Specifically, each model is trained
with the distillation loss that makes to follow the other's prediction along
with its original loss function. For effective bidirectional distillation, we
propose rank discrepancy-aware sampling scheme to distill only the informative
knowledge that can fully enhance each other. The proposed scheme is designed to
effectively cope with a large performance gap between the teacher and the
student. Trained in the bidirectional way, it turns out that both the teacher
and the student are significantly improved compared to when being trained
separately. Our extensive experiments on real-world datasets show that our
proposed framework consistently outperforms the state-of-the-art competitors.
We also provide analyses for an in-depth understanding of BD and ablation
studies to verify the effectiveness of each proposed component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. (arXiv:2106.02768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02768</id>
        <link href="http://arxiv.org/abs/2106.02768"/>
        <updated>2021-06-08T02:20:20.522Z</updated>
        <summary type="html"><![CDATA[Cross domain recommender system constitutes a powerful method to tackle the
cold-start and sparsity problem by aggregating and transferring user
preferences across multiple category domains. Therefore, it has great potential
to improve click-through-rate prediction performance in online commerce
platforms having many domains of products. While several cross domain
sequential recommendation models have been proposed to leverage information
from a source domain to improve CTR predictions in a target domain, they did
not take into account bidirectional latent relations of user preferences across
source-target domain pairs. As such, they cannot provide enhanced cross-domain
CTR predictions for both domains simultaneously. In this paper, we propose a
novel approach to cross-domain sequential recommendations based on the dual
learning mechanism that simultaneously transfers information between two
related domains in an iterative manner until the learning process stabilizes.
In particular, the proposed Dual Attentive Sequential Learning (DASL) model
consists of two novel components Dual Embedding and Dual Attention, which
jointly establish the two-stage learning process: we first construct dual
latent embeddings that extract user preferences in both domains simultaneously,
and subsequently provide cross-domain recommendations by matching the extracted
latent embeddings with candidate items through dual-attention learning
mechanism. We conduct extensive offline experiments on three real-world
datasets to demonstrate the superiority of our proposed model, which
significantly and consistently outperforms several state-of-the-art baselines
across all experimental settings. We also conduct an online A/B test at a major
video streaming platform Alibaba-Youku, where our proposed model significantly
improves business performance over the latest production system in the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video. (arXiv:2010.10637v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10637</id>
        <link href="http://arxiv.org/abs/2010.10637"/>
        <updated>2021-06-08T02:20:20.501Z</updated>
        <summary type="html"><![CDATA[How to extract effective expression representations that invariant to the
identity-specific attributes is a long-lasting problem for facial expression
recognition (FER). Most of the previous methods process the RGB images of a
sequence, while we argue that the off-the-shelf and valuable expression-related
muscle movement is already embedded in the compression format. In this paper,
we target to explore the inter-subject variations eliminated facial expression
representation in the compressed video domain. In the up to two orders of
magnitude compressed domain, we can explicitly infer the expression from the
residual frames and possibly extract identity factors from the I frame with a
pre-trained face recognition network. By enforcing the marginal independence of
them, the expression feature is expected to be purer for the expression and be
robust to identity shifts. Specifically, we propose a novel collaborative
min-min game for mutual information (MI) minimization in latent space. We do
not need the identity label or multiple expression samples from the same person
for identity elimination. Moreover, when the apex frame is annotated in the
dataset, the complementary constraint can be further added to regularize the
feature-level game. In testing, only the compressed residual frames are
required to achieve expression prediction. Our solution can achieve comparable
or better performance than the recent decoded image-based methods on the
typical FER benchmarks with about 3 times faster inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auditing Source Diversity Bias in Video Search Results Using Virtual Agents. (arXiv:2106.02715v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02715</id>
        <link href="http://arxiv.org/abs/2106.02715"/>
        <updated>2021-06-08T02:20:20.489Z</updated>
        <summary type="html"><![CDATA[We audit the presence of domain-level source diversity bias in video search
results. Using a virtual agent-based approach, we compare outputs of four
Western and one non-Western search engines for English and Russian queries. Our
findings highlight that source diversity varies substantially depending on the
language with English queries returning more diverse outputs. We also find
disproportionately high presence of a single platform, YouTube, in top search
outputs for all Western search engines except Google. At the same time, we
observe that Youtube's major competitors such as Vimeo or Dailymotion do not
appear in the sampled Google's video search results. This finding suggests that
Google might be downgrading the results from the main competitors of
Google-owned Youtube and highlights the necessity for further studies focusing
on the presence of own-content bias in Google's search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1"&gt;Aleksandra Urman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1"&gt;Mykola Makhortykh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1"&gt;Roberto Ulloa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System. (arXiv:2010.15363v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15363</id>
        <link href="http://arxiv.org/abs/2010.15363"/>
        <updated>2021-06-08T02:20:20.471Z</updated>
        <summary type="html"><![CDATA[The general aim of the recommender system is to provide personalized
suggestions to users, which is opposed to suggesting popular items. However,
the normal training paradigm, i.e., fitting a recommender model to recover the
user behavior data with pointwise or pairwise loss, makes the model biased
towards popular items. This results in the terrible Matthew effect, making
popular items be more frequently recommended and become even more popular.
Existing work addresses this issue with Inverse Propensity Weighting (IPW),
which decreases the impact of popular items on the training and increases the
impact of long-tail items. Although theoretically sound, IPW methods are highly
sensitive to the weighting strategy, which is notoriously difficult to tune. In
this work, we explore the popularity bias issue from a novel and fundamental
perspective -- cause-effect. We identify that popularity bias lies in the
direct effect from the item node to the ranking score, such that an item's
intrinsic property is the cause of mistakenly assigning it a higher ranking
score. To eliminate popularity bias, it is essential to answer the
counterfactual question that what the ranking score would be if the model only
uses item property. To this end, we formulate a causal graph to describe the
important cause-effect relations in the recommendation process. During
training, we perform multi-task learning to achieve the contribution of each
cause; during testing, we perform counterfactual inference to remove the effect
of item popularity. Remarkably, our solution amends the learning process of
recommendation which is agnostic to a wide range of models -- it can be easily
implemented in existing methods. We demonstrate it on Matrix Factorization (MF)
and LightGCN [20]. Experiments on five real-world datasets demonstrate the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianxin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs. (arXiv:2006.04373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04373</id>
        <link href="http://arxiv.org/abs/2006.04373"/>
        <updated>2021-06-08T02:20:20.446Z</updated>
        <summary type="html"><![CDATA[In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1"&gt;Geewon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Embed Categorical Features without Embedding Tables for Recommendation. (arXiv:2010.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10784</id>
        <link href="http://arxiv.org/abs/2010.10784"/>
        <updated>2021-06-08T02:20:20.433Z</updated>
        <summary type="html"><![CDATA[Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Derek Zhiyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tiansheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:20.418Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepQTMT: A Deep Learning Approach for Fast QTMT-based CU Partition of Intra-mode VVC. (arXiv:2006.13125v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13125</id>
        <link href="http://arxiv.org/abs/2006.13125"/>
        <updated>2021-06-08T02:20:20.377Z</updated>
        <summary type="html"><![CDATA[Versatile Video Coding (VVC), as the latest standard, significantly improves
the coding efficiency over its ancestor standard High Efficiency Video Coding
(HEVC), but at the expense of sharply increased complexity. In VVC, the
quad-tree plus multi-type tree (QTMT) structure of coding unit (CU) partition
accounts for over 97% of the encoding time, due to the brute-force search for
recursive rate-distortion (RD) optimization. Instead of the brute-force QTMT
search, this paper proposes a deep learning approach to predict the QTMT-based
CU partition, for drastically accelerating the encoding process of intra-mode
VVC. First, we establish a large-scale database containing sufficient CU
partition patterns with diverse video content, which can facilitate the
data-driven VVC complexity reduction. Next, we propose a multi-stage exit CNN
(MSE-CNN) model with an early-exit mechanism to determine the CU partition, in
accord with the flexible QTMT structure at multiple stages. Then, we design an
adaptive loss function for training the MSE-CNN model, synthesizing both the
uncertain number of split modes and the target on minimized RD cost. Finally, a
multi-threshold decision scheme is developed, achieving desirable trade-off
between complexity and RD performance. Experimental results demonstrate that
our approach can reduce the encoding time of VVC by 44.65%-66.88% with the
negligible Bj{\o}ntegaard delta bit-rate (BD-BR) of 1.322%-3.188%, which
significantly outperforms other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_R/0/1/0/all/0/1"&gt;Runzhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_Q/0/1/0/all/0/1"&gt;Qunliang Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification. (arXiv:2106.02864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02864</id>
        <link href="http://arxiv.org/abs/2106.02864"/>
        <updated>2021-06-08T02:20:20.291Z</updated>
        <summary type="html"><![CDATA[Researchers working on computational analysis of Whole Slide Images (WSIs) in
histopathology have primarily resorted to patch-based modelling due to large
resolution of each WSI. The large resolution makes WSIs infeasible to be fed
directly into the machine learning models due to computational constraints.
However, due to patch-based analysis, most of the current methods fail to
exploit the underlying spatial relationship among the patches. In our work, we
have tried to integrate this relationship along with feature-based correlation
among the extracted patches from the particular tumorous region. For the given
task of classification, we have used BiLSTMs to model both forward and backward
contextual relationship. RNN based models eliminate the limitation of sequence
size by allowing the modelling of variable size images within a deep learning
model. We have also incorporated the effect of spatial continuity by exploring
different scanning techniques used to sample patches. To establish the
efficiency of our approach, we trained and tested our model on two datasets,
microscopy images and WSI tumour regions. After comparing with contemporary
literature we achieved the better performance with accuracy of 90% for
microscopy image dataset. For WSI tumour region dataset, we compared the
classification results with deep learning networks such as ResNet, DenseNet,
and InceptionV3 using maximum voting technique. We achieved the highest
performance accuracy of 84%. We found out that BiLSTMs with CNN features have
performed much better in modelling patches into an end-to-end Image
classification network. Additionally, the variable dimensions of WSI tumour
regions were used for classification without the need for resizing. This
suggests that our method is independent of tumour image size and can process
large dimensional images without losing the resolution details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwee Kuan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Neural Architecture Optimization for Keyword Spotting. (arXiv:2106.02738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02738</id>
        <link href="http://arxiv.org/abs/2106.02738"/>
        <updated>2021-06-08T02:20:20.207Z</updated>
        <summary type="html"><![CDATA[Keyword spotting aims to identify specific keyword audio utterances. In
recent years, deep convolutional neural networks have been widely utilized in
keyword spotting systems. However, their model architectures are mainly based
on off-the shelfbackbones such as VGG-Net or ResNet, instead of specially
designed for the task. In this paper, we utilize neural architecture search to
design convolutional neural network models that can boost the performance of
keyword spotting while maintaining an acceptable memory footprint.
Specifically, we search the model operators and their connections in a specific
search space with Encoder-Decoder neural architecture optimization. Extensive
evaluations on Google's Speech Commands Dataset show that the model
architecture searched by our approach achieves a state-of-the-art accuracy of
over 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1"&gt;Tong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14117</id>
        <link href="http://arxiv.org/abs/2105.14117"/>
        <updated>2021-06-07T23:29:40.150Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning methods for computer vision have demonstrated the
effectiveness of pre-training feature representations, resulting in
well-generalizing Deep Neural Networks, even if the annotated data are limited.
However, representation learning techniques require a significant amount of
time for model training, with most of it time spent on precise hyper-parameter
optimization and selection of augmentation techniques. We hypothesized that if
the annotated dataset has enough morphological diversity to capture the general
population's as is common in medical imaging, for example, due to conserved
similarities of tissue mythologies, the variance error of the trained model is
the prevalent component of the Bias-Variance Trade-off. We propose the Variance
Aware Training (VAT) method that exploits this property by introducing the
variance error into the model loss function, i.e., enabling minimizing the
variance explicitly. Additionally, we provide the theoretical formulation and
proof of the proposed method to aid in interpreting the approach. Our method
requires selecting only one hyper-parameter and was able to match or improve
the state-of-the-art performance of self-supervised methods while achieving an
order of magnitude reduction in the GPU training time. We validated VAT on
three medical imaging datasets from diverse domains and various learning
objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the
heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography
dataset for ordinary regression of diabetic retinopathy progression (Kaggle
2019 APTOS Blindness Detection challenge), and classification of
histopathologic scans of lymph node sections (PatchCamelyon dataset).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1"&gt;Dmitrii Shubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1"&gt;Danny Eytan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1"&gt;Sebastian D. Goodfellow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14106</id>
        <link href="http://arxiv.org/abs/2105.14106"/>
        <updated>2021-06-07T23:29:40.127Z</updated>
        <summary type="html"><![CDATA[The design of machines and algorithms capable of learning in a dynamically
changing environment has become an increasingly topical problem with the
increase of the size and heterogeneity of data available to learning systems.
As a consequence, the key issue of Continual Learning has become that of
addressing the stability-plasticity dilemma of connectionist systems, as they
need to adapt their model without forgetting previously acquired knowledge.
Within this context, rehearsal-based methods i.e., solutions in where the
learner exploits memory to revisit past data, has proven to be very effective,
leading to performance at the state-of-the-art. In our study, we propose an
analysis of the memory quantity/quality trade-off adopting various data
reduction approaches to increase the number of instances storable in memory. In
particular, we investigate complex instance compression techniques such as deep
encoders, but also trivial approaches such as image resizing and linear
dimensionality reduction. Our findings suggest that the optimal trade-off is
severely skewed toward instance quantity, where rehearsal approaches with
several heavily compressed instances easily outperform state-of-the-art
approaches with the same amount of memory at their disposal. Further, in high
memory configurations, deep approaches extracting spatial structure combined
with extreme resizing (of the order of $8\times8$ images) yield the best
results, while in memory-constrained configurations where deep approaches
cannot be used due to their memory requirement in training, Extreme Learning
Machines (ELM) offer a clear advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1"&gt;Francesco Pelosin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1"&gt;Andrea Torsello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems. (arXiv:2105.11558v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11558</id>
        <link href="http://arxiv.org/abs/2105.11558"/>
        <updated>2021-06-07T22:33:05.470Z</updated>
        <summary type="html"><![CDATA[We consider the setting of vector valued non-linear dynamical systems
$X_{t+1} = \phi(A^* X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi
: \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain
{\em expansivity property}. The goal is to learn $A^*$ from a single trajectory
$X_1,\cdots,X_T$ of {\em dependent or correlated} samples. While the problem is
well-studied in the linear case, where $\phi$ is identity, with optimal error
rates even for non-mixing systems, existing results in the non-linear case hold
only for mixing systems. In this work, we improve existing results for learning
nonlinear systems in a number of ways: a) we provide the first offline
algorithm that can learn non-linear dynamical systems without the mixing
assumption, b) we significantly improve upon the sample complexity of existing
results for mixing systems, c) in the much harder one-pass, streaming setting
we study a SGD with Reverse Experience Replay ($\mathsf{SGD-RER}$) method, and
demonstrate that for mixing systems, it achieves the same sample complexity as
our offline algorithm, d) we justify the expansivity assumption by showing that
for the popular ReLU link function -- a non-expansive but easy to learn link
function with i.i.d. samples -- any method would require exponentially many
samples (with respect to dimension of $X_t$) from the dynamical system. We
validate our results via. simulations and demonstrate that a naive application
of SGD can be highly sub-optimal. Indeed, our work demonstrates that for
correlated data, specialized methods designed for the dependency structure in
data can significantly outperform standard SGD based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equilibrium and non-Equilibrium regimes in the learning of Restricted Boltzmann Machines. (arXiv:2105.13889v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13889</id>
        <link href="http://arxiv.org/abs/2105.13889"/>
        <updated>2021-06-07T22:33:05.420Z</updated>
        <summary type="html"><![CDATA[Training Restricted Boltzmann Machines (RBMs) has been challenging for a long
time due to the difficulty of computing precisely the log-likelihood gradient.
Over the past decades, many works have proposed more or less successful
training recipes but without studying the crucial quantity of the problem: the
mixing time i.e. the number of Monte Carlo iterations needed to sample new
configurations from a model. In this work, we show that this mixing time plays
a crucial role in the dynamics and stability of the trained model, and that
RBMs operate in two well-defined regimes, namely equilibrium and
out-of-equilibrium, depending on the interplay between this mixing time of the
model and the number of steps, $k$, used to approximate the gradient. We
further show empirically that this mixing time increases with the learning,
which often implies a transition from one regime to another as soon as $k$
becomes smaller than this time. In particular, we show that using the popular
$k$ (persistent) contrastive divergence approaches, with $k$ small, the
dynamics of the learned model are extremely slow and often dominated by strong
out-of-equilibrium effects. On the contrary, RBMs trained in equilibrium
display faster dynamics, and a smooth convergence to dataset-like
configurations during the sampling. Finally we discuss how to exploit in
practice both regimes depending on the task one aims to fulfill: (i) short $k$s
can be used to generate convincing samples in short times, (ii) large $k$ (or
increasingly large) must be used to learn the correct equilibrium distribution
of the RBM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furtlehner_C/0/1/0/all/0/1"&gt;Cyril Furtlehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seoane_B/0/1/0/all/0/1"&gt;Beatriz Seoane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Online-Bandit Strategies for Minimax Learning Problems. (arXiv:2105.13939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13939</id>
        <link href="http://arxiv.org/abs/2105.13939"/>
        <updated>2021-06-07T22:33:05.409Z</updated>
        <summary type="html"><![CDATA[Several learning problems involve solving min-max problems, e.g., empirical
distributional robust learning or learning with non-standard aggregated losses.
More specifically, these problems are convex-linear problems where the
minimization is carried out over the model parameters $w\in\mathcal{W}$ and the
maximization over the empirical distribution $p\in\mathcal{K}$ of the training
set indexes, where $\mathcal{K}$ is the simplex or a subset of it. To design
efficient methods, we let an online learning algorithm play against a
(combinatorial) bandit algorithm. We argue that the efficiency of such
approaches critically depends on the structure of $\mathcal{K}$ and propose two
properties of $\mathcal{K}$ that facilitate designing efficient algorithms. We
focus on a specific family of sets $\mathcal{S}_{n,k}$ encompassing various
learning applications and provide high-probability convergence guarantees to
the minimax values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roux_C/0/1/0/all/0/1"&gt;Christophe Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wirth_E/0/1/0/all/0/1"&gt;Elias Wirth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;Sebastian Pokutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerdreux_T/0/1/0/all/0/1"&gt;Thomas Kerdreux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A nearly Blackwell-optimal policy gradient method. (arXiv:2105.13609v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13609</id>
        <link href="http://arxiv.org/abs/2105.13609"/>
        <updated>2021-06-07T22:33:05.399Z</updated>
        <summary type="html"><![CDATA[For continuing environments, reinforcement learning methods commonly maximize
a discounted reward criterion with discount factor close to 1 in order to
approximate the steady-state reward (the gain). However, such a criterion only
considers the long-run performance, ignoring the transient behaviour. In this
work, we develop a policy gradient method that optimizes the gain, then the
bias (which indicates the transient performance and is important to capably
select from policies with equal gain). We derive expressions that enable
sampling for the gradient of the bias, and its preconditioning Fisher matrix.
We further propose an algorithm that solves the corresponding bi-level
optimization using a logarithmic barrier. Experimental results provide insights
into the fundamental mechanisms of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1"&gt;Vektor Dewanto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1"&gt;Marcus Gallagher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Bandit under Differential Privacy. (arXiv:2105.11126v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11126</id>
        <link href="http://arxiv.org/abs/2105.11126"/>
        <updated>2021-06-07T22:33:05.386Z</updated>
        <summary type="html"><![CDATA[This paper studies \emph{differential privacy (DP)} and \emph{local
differential privacy (LDP)} in cascading bandits. Under DP, we propose an
algorithm which guarantees $\epsilon$-indistinguishability and a regret of
$\mathcal{O}((\frac{\log T}{\epsilon})^{1+\xi})$ for an arbitrarily small
$\xi$. This is a significant improvement from the previous work of
$\mathcal{O}(\frac{\log^3 T}{\epsilon})$ regret. Under
($\epsilon$,$\delta$)-LDP, we relax the $K^2$ dependence through the tradeoff
between privacy budget $\epsilon$ and error probability $\delta$, and obtain a
regret of $\mathcal{O}(\frac{K\log (1/\delta) \log T}{\epsilon^2})$, where $K$
is the size of the arm subset. This result holds for both Gaussian mechanism
and Laplace mechanism by analyses on the composition. Our results extend to
combinatorial semi-bandit. We show respective lower bounds for DP and LDP
cascading bandits. Extensive experiments corroborate our theoretic findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1"&gt;Shuo Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Capacity and a Variant of the Kalman Filter with ARMA Gaussian Noises: Explicit Bounds and Feedback Coding Design. (arXiv:2001.03108v6 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03108</id>
        <link href="http://arxiv.org/abs/2001.03108"/>
        <updated>2021-06-07T22:33:05.322Z</updated>
        <summary type="html"><![CDATA[In this paper, we relate a feedback channel with any finite-order
autoregressive moving-average (ARMA) Gaussian noises to a variant of the Kalman
filter. In light of this, we obtain relatively explicit lower bounds on the
feedback capacity for such colored Gaussian noises, and the bounds are seen to
be consistent with various existing results in the literature. Meanwhile, this
variant of the Kalman filter also leads to explicit recursive coding schemes
with clear structures to achieve the lower bounds. In general, our results
provide an alternative perspective while pointing to potentially tighter bounds
for the feedback capacity problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08791</id>
        <link href="http://arxiv.org/abs/2105.08791"/>
        <updated>2021-06-07T22:33:05.308Z</updated>
        <summary type="html"><![CDATA[Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of
thousands of vehicles in a city to millions of ride demands throughout the day,
providing great promises for improving transportation efficiency through the
tasks of order dispatching and vehicle repositioning. Existing studies,
however, usually consider the two tasks in simplified settings that hardly
address the complex interactions between the two, the real-time fluctuations
between supply and demand, and the necessary coordinations due to the
large-scale nature of the problem. In this paper we propose a unified
value-based dynamic learning framework (V1D3) for tackling both tasks. At the
center of the framework is a globally shared value function that is updated
continuously using online experiences generated from real-time platform
transactions. To improve the sample-efficiency and the robustness, we further
propose a novel periodic ensemble method combining the fast online learning
with a large-scale offline training scheme that leverages the abundant
historical driver trajectory data. This allows the proposed framework to adapt
quickly to the highly dynamic environment, to generalize robustly to recurrent
patterns and to drive implicit coordinations among the population of managed
vehicles. Extensive experiments based on real-world datasets show considerably
improvements over other recently proposed methods on both tasks. Particularly,
V1D3 outperforms the first prize winners of both dispatching and repositioning
tracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results
on improving both total driver income and user experience related metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dingyuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bingchen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10375</id>
        <link href="http://arxiv.org/abs/2105.10375"/>
        <updated>2021-06-07T22:33:05.238Z</updated>
        <summary type="html"><![CDATA[Face recognition has achieved significant progress in deep-learning era due
to the ultra-large-scale and well-labeled datasets.

However, training on ultra-large-scale datasets is time-consuming and takes
up a lot of hardware resource.

Therefore, designing an efficient training approach is crucial and
indispensable.

The heavy computational and memory costs mainly result from the high
dimensionality of the Fully-Connected (FC) layer.

Specifically, the dimensionality is determined by the number of face
identities, which can be million-level or even more.

To this end, we propose a novel training approach for ultra-large-scale face
datasets, termed Faster Face Classification (F$^2$C).

In F$^2$C, we first define a Gallery Net and a Probe Net that are used to
generate identities' centers and extract faces' features for face recognition,
respectively.

Gallery Net has the same structure as Probe Net and inherits the parameters
from Probe Net with a moving average paradigm.

After that, to reduce the training time and hardware costs of the FC layer,
we propose a Dynamic Class Pool (DCP) that stores the features from Gallery Net
and calculates the inner product (logits) with positive samples (whose
identities are in the DCP) in each mini-batch.

DCP can be regarded as a substitute for the FC layer but it is far smaller,
thus greatly reducing the computational and memory costs.

For negative samples (whose identities are not in DCP), we minimize the
cosine similarities between negative samples and those in DCP.

Then, to improve the update efficiency of DCP's parameters, we design a dual
data-loader including identity-based and instance-based loaders to generate a
certain of identities and samples in mini-batches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baigui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11237</id>
        <link href="http://arxiv.org/abs/2105.11237"/>
        <updated>2021-06-07T22:33:05.218Z</updated>
        <summary type="html"><![CDATA[Recently, most siamese network based trackers locate targets via object
classification and bounding-box regression. Generally, they select the
bounding-box with maximum classification confidence as the final prediction.
This strategy may miss the right result due to the accuracy misalignment
between classification and regression. In this paper, we propose a novel
siamese tracking algorithm called SiamRCR, addressing this problem with a
simple, light and effective solution. It builds reciprocal links between
classification and regression branches, which can dynamically re-weight their
losses for each positive sample. In addition, we add a localization branch to
predict the localization accuracy, so that it can work as the replacement of
the regression assistance link during inference. This branch makes the training
and inference more consistent. Extensive experimental results demonstrate the
effectiveness of SiamRCR and its superiority over the state-of-the-art
competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.
Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jinlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yueyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11578</id>
        <link href="http://arxiv.org/abs/2105.11578"/>
        <updated>2021-06-07T22:33:05.198Z</updated>
        <summary type="html"><![CDATA[Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset containing
various real-life daily scenes borrowed from this http URL, with
hierarchical annotations for 6,268 key frames uniformly sampled from 37,403
omnidirectional video frames at 4K resolution. Since so far there is no method
proposed for 360{\deg} image/video SHD, we systematically benchmark 11
representative state-of-the-art salient object detection approaches on our
SHD360. We hope our proposed dataset and benchmark could serve as a good
starting point for advancing human-centric researches towards 360{\deg}
panoramic data. Our dataset and benchmark will be publicly available at
https://github.com/PanoAsh/SHD360.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-06-07T22:33:05.164Z</updated>
        <summary type="html"><![CDATA[Random field and random cluster theory is used to prove certain mathematical
results concerning the probability distribution of images characterized as
generic $2D$ integer arrays during simultaneous learning. Example models in
image classification and object segmentation illustrate the mathematical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness Testing of Language Understanding in Task-Oriented Dialog. (arXiv:2012.15262v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15262</id>
        <link href="http://arxiv.org/abs/2012.15262"/>
        <updated>2021-06-07T22:33:05.098Z</updated>
        <summary type="html"><![CDATA[Most language understanding models in task-oriented dialog systems are
trained on a small amount of annotated training data, and evaluated in a small
set from the same distribution. However, these models can lead to system
failure or undesirable output when being exposed to natural language
perturbation or variation in practice. In this paper, we conduct comprehensive
evaluation and analysis with respect to the robustness of natural language
understanding models, and introduce three important aspects related to language
understanding in real-world dialog systems, namely, language variety, speech
characteristics, and noise perturbation. We propose a model-agnostic toolkit
LAUG to approximate natural language perturbations for testing the robustness
issues in task-oriented dialog. Four data augmentation approaches covering the
three aspects are assembled in LAUG, which reveals critical robustness issues
in state-of-the-art models. The augmented dataset through LAUG can be used to
facilitate future research on the robustness testing of language understanding
in task-oriented dialog.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiexi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jiaxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Dazhen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1"&gt;Weiran Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sigma-Delta and Distributed Noise-Shaping Quantization Methods for Random Fourier Features. (arXiv:2106.02614v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02614</id>
        <link href="http://arxiv.org/abs/2106.02614"/>
        <updated>2021-06-07T03:06:17.007Z</updated>
        <summary type="html"><![CDATA[We propose the use of low bit-depth Sigma-Delta and distributed noise-shaping
methods for quantizing the Random Fourier features (RFFs) associated with
shift-invariant kernels. We prove that our quantized RFFs -- even in the case
of $1$-bit quantization -- allow a high accuracy approximation of the
underlying kernels, and the approximation error decays at least polynomially
fast as the dimension of the RFFs increases. We also show that the quantized
RFFs can be further compressed, yielding an excellent trade-off between memory
use and accuracy. Namely, the approximation error now decays exponentially as a
function of the bits used. Moreover, we empirically show by testing the
performance of our methods on several machine learning tasks that our method
compares favorably to other state of the art quantization methods in this
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;Alexander Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1"&gt;Rayan Saab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ES-ENAS: Controller-Based Architecture Search for Evolutionary Reinforcement Learning. (arXiv:2101.07415v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07415</id>
        <link href="http://arxiv.org/abs/2101.07415"/>
        <updated>2021-06-07T03:06:17.000Z</updated>
        <summary type="html"><![CDATA[We introduce ES-ENAS, a simple yet general evolutionary joint optimization
procedure by combining continuous optimization via Evolutionary Strategies (ES)
and combinatorial optimization via Efficient NAS (ENAS) in a highly scalable
and intuitive way. Our main insight is noticing that ES is already a highly
distributed algorithm involving hundreds of forward passes which can not only
be used for training neural network weights, but also for jointly training a
NAS controller, both in a blackbox fashion. By doing so, we also bridge the gap
from NAS research in supervised learning settings to the reinforcement learning
scenario through this relatively simple marriage between two different yet
common lines of research. We demonstrate the utility and effectiveness of our
method over a large search space by training highly combinatorial neural
network architectures for RL problems in continuous control, via edge pruning
and quantization. We also incorporate a wide variety of popular techniques from
modern NAS literature including multiobjective optimization along with various
controller methods, to showcase their promise in the RL field and discuss
possible extensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Deepali Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wenbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarlos_T/0/1/0/all/0/1"&gt;Tamas Sarlos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SmartON: Just-in-Time Active Event Detection on Energy Harvesting Systems. (arXiv:2103.00749v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00749</id>
        <link href="http://arxiv.org/abs/2103.00749"/>
        <updated>2021-06-07T03:06:16.993Z</updated>
        <summary type="html"><![CDATA[We propose SmartON, a batteryless system that learns to wake up proactively
at the right moment in order to detect events of interest. It does so by
adapting the duty cycle to match the distribution of event arrival times under
the constraints of harvested energy. While existing energy harvesting systems
either wake up periodically at a fixed rate to sense and process the data, or
wake up only in accordance with the availability of the energy source, SmartON
employs a three-phase learning framework to learn the energy harvesting pattern
as well as the pattern of events at run-time, and uses that knowledge to wake
itself up when events are most likely to occur. The three-phase learning
framework enables rapid adaptation to environmental changes in both short and
long terms. Being able to remain asleep more often than a CTID
(charging-then-immediate-discharging) wake-up system and adapt to the event
pattern, SmartON is able to reduce energy waste, increase energy efficiency,
and capture more events. To realize SmartON we have developed a dedicated
hardware platform whose power management module activates capacitors on-the-fly
to dynamically increase its storage capacitance. We conduct both
simulation-driven and real-system experiments to demonstrate that SmartON
captures 1X--7X more events and is 8X--17X more energy-efficient than a CTID
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yubo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nirjon_S/0/1/0/all/0/1"&gt;Shahriar Nirjon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convergent and Dimension-Independent First-Order Algorithm for Min-Max Optimization. (arXiv:2006.12376v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12376</id>
        <link href="http://arxiv.org/abs/2006.12376"/>
        <updated>2021-06-07T03:06:16.987Z</updated>
        <summary type="html"><![CDATA[Motivated by the recent work of Mangoubi and Vishnoi (STOC 2021), we propose
a variant of the min-max optimization framework where the max-player is
constrained to update the maximization variable in a greedy manner until it
reaches a *first-order* stationary point. We present an algorithm that provably
converges to an approximate local equilibrium for our framework from any
initialization and for nonconvex-nonconcave loss functions. Compared to the
second-order algorithm of Mangoubi and Vishnoi, whose iteration bound is
polynomial in the dimension, our algorithm is first-order and its iteration
bound is independent of dimension. We empirically evaluate our algorithm on
challenging nonconvex-nonconcave test-functions and loss functions that arise
in GAN training. Our algorithm converges on these test functions and, when used
to train GANs on synthetic and real-world datasets, trains stably and avoids
mode collapse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1"&gt;Vijay Keswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1"&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1"&gt;Sushant Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1"&gt;Nisheeth K. Vishnoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Optimization Is Deceiving Us, and How to Stop It. (arXiv:2102.03034v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03034</id>
        <link href="http://arxiv.org/abs/2102.03034"/>
        <updated>2021-06-07T03:06:16.981Z</updated>
        <summary type="html"><![CDATA[Recent empirical work shows that inconsistent results, based on choice of
hyperparameter optimization (HPO) configuration, are a widespread problem in ML
research. When comparing two algorithms J and K, searching one subspace can
yield the conclusion that J outperforms K, whereas searching another can entail
the opposite. In short, the way we choose hyperparameters can deceive us. We
provide a theoretical complement to this prior work, arguing that, to avoid
such deception, the process of drawing conclusions from HPO should be made more
rigorous. We call this process epistemic hyperparameter optimization (EHPO),
and put forth a logical framework to capture its semantics and how it can lead
to inconsistent conclusions about performance. Our framework enables us to
prove EHPO methods that are guaranteed to be defended against deception. We
demonstrate its utility by proving and empirically validating a defended
variant of random search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Feder Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1"&gt;Jessica Zosa Forde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Exploration via Axiomatic Bargaining. (arXiv:2106.02553v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02553</id>
        <link href="http://arxiv.org/abs/2106.02553"/>
        <updated>2021-06-07T03:06:16.974Z</updated>
        <summary type="html"><![CDATA[Motivated by the consideration of fairly sharing the cost of exploration
between multiple groups in learning problems, we develop the Nash bargaining
solution in the context of multi-armed bandits. Specifically, the 'grouped'
bandit associated with any multi-armed bandit problem associates, with each
time step, a single group from some finite set of groups. The utility gained by
a given group under some learning policy is naturally viewed as the reduction
in that group's regret relative to the regret that group would have incurred
'on its own'. We derive policies that yield the Nash bargaining solution
relative to the set of incremental utilities possible under any policy. We show
that on the one hand, the 'price of fairness' under such policies is limited,
while on the other hand, regret optimal policies are arbitrarily unfair under
generic conditions. Our theoretical development is complemented by a case study
on contextual bandits for warfarin dosing where we are concerned with the cost
of exploration across multiple races and age groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jackie Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farias_V/0/1/0/all/0/1"&gt;Vivek F. Farias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence Estimation and Maximization via Neural Mean-Field Dynamics. (arXiv:2106.02608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02608</id>
        <link href="http://arxiv.org/abs/2106.02608"/>
        <updated>2021-06-07T03:06:16.956Z</updated>
        <summary type="html"><![CDATA[We propose a novel learning framework using neural mean-field (NMF) dynamics
for inference and estimation problems on heterogeneous diffusion networks. Our
new framework leverages the Mori-Zwanzig formalism to obtain an exact evolution
equation of the individual node infection probabilities, which renders a delay
differential equation with memory integral approximated by learnable time
convolution operators. Directly using information diffusion cascade data, our
framework can simultaneously learn the structure of the diffusion network and
the evolution of node infection probabilities. Connections between parameter
learning and optimal control are also established, leading to a rigorous and
implementable algorithm for training NMF. Moreover, we show that the projected
gradient descent method can be employed to solve the challenging influence
maximization problem, where the gradient is computed extremely fast by
integrating NMF forward in time just once in each iteration. Extensive
empirical studies show that our approach is versatile and robust to variations
of the underlying diffusion network models, and significantly outperform
existing approaches in accuracy and efficiency on both synthetic and real-world
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shushan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojing Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Graph Models for Retrosynthesis Prediction. (arXiv:2006.07038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07038</id>
        <link href="http://arxiv.org/abs/2006.07038"/>
        <updated>2021-06-07T03:06:16.949Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis prediction is a fundamental problem in organic synthesis,
where the task is to identify precursor molecules that can be used to
synthesize a target molecule. A key consideration in building neural models for
this task is aligning model design with strategies adopted by chemists.
Building on this viewpoint, this paper introduces a graph-based approach that
capitalizes on the idea that the graph topology of precursor molecules is
largely unaltered during a chemical reaction. The model first predicts the set
of graph edits transforming the target into incomplete molecules called
synthons. Next, the model learns to expand synthons into complete molecules by
attaching relevant leaving groups. This decomposition simplifies the
architecture, making its predictions more interpretable, and also amenable to
manual correction. Our model achieves a top-1 accuracy of $53.7\%$,
outperforming previous template-free and semi-template-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somnath_V/0/1/0/all/0/1"&gt;Vignesh Ram Somnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1"&gt;Charlotte Bunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor W. Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Procedural World Generation Framework for Systematic Evaluation of Continual Learning. (arXiv:2106.02585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02585</id>
        <link href="http://arxiv.org/abs/2106.02585"/>
        <updated>2021-06-07T03:06:16.942Z</updated>
        <summary type="html"><![CDATA[Several families of continual learning techniques have been proposed to
alleviate catastrophic interference in deep neural network training on
non-stationary data. However, a comprehensive comparison and analysis of
limitations remains largely open due to the inaccessibility to suitable
datasets. Empirical examination not only varies immensely between individual
works, it further currently relies on contrived composition of benchmarks
through subdivision and concatenation of various prevalent static vision
datasets. In this work, our goal is to bridge this gap by introducing a
computer graphics simulation framework that repeatedly renders only upcoming
urban scene fragments in an endless real-time procedural world generation
process. At its core lies a modular parametric generative model with adaptable
generative factors. The latter can be used to flexibly compose data streams,
which significantly facilitates a detailed analysis and allows for effortless
investigation of various continual learning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Timm Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1"&gt;Martin Mundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1"&gt;Iuliia Pliushch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Visvanathan Ramesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causally-motivated Shortcut Removal Using Auxiliary Labels. (arXiv:2105.06422v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06422</id>
        <link href="http://arxiv.org/abs/2105.06422"/>
        <updated>2021-06-07T03:06:16.936Z</updated>
        <summary type="html"><![CDATA[Robustness to certain forms of distribution shift is a key concern in many ML
applications. Often, robustness can be formulated as enforcing invariances to
particular interventions on the data generating process. Here, we study a
flexible, causally-motivated approach to enforcing such invariances, paying
special attention to shortcut learning, where a robust predictor can achieve
optimal i.i.d generalization in principle, but instead it relies on spurious
correlations or shortcuts in practice. Our approach uses auxiliary labels,
typically available at training time, to enforce conditional independences
between the latent factors that determine these labels. We show both
theoretically and empirically that causally-motivated regularization schemes
(a) lead to more robust estimators that generalize well under distribution
shift, and (b) have better finite sample efficiency compared to usual
regularization schemes, even in the absence of distribution shifts. Our
analysis highlights important theoretical properties of training techniques
commonly used in causal inference, fairness, and disentanglement literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makar_M/0/1/0/all/0/1"&gt;Maggie Makar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Packer_B/0/1/0/all/0/1"&gt;Ben Packer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moldovan_D/0/1/0/all/0/1"&gt;Dan Moldovan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blalock_D/0/1/0/all/0/1"&gt;Davis Blalock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halpern_Y/0/1/0/all/0/1"&gt;Yoni Halpern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing 3D Artefacts and Functional Machines with Neural Cellular Automata. (arXiv:2103.08737v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08737</id>
        <link href="http://arxiv.org/abs/2103.08737"/>
        <updated>2021-06-07T03:06:16.920Z</updated>
        <summary type="html"><![CDATA[Neural Cellular Automata (NCAs) have been proven effective in simulating
morphogenetic processes, the continuous construction of complex structures from
very few starting cells. Recent developments in NCAs lie in the 2D domain,
namely reconstructing target images from a single pixel or infinitely growing
2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D
convolutions in the proposed neural network architecture. Minecraft is selected
as the environment for our automaton since it allows the generation of both
static structures and moving machines. We show that despite their simplicity,
NCAs are capable of growing complex entities such as castles, apartment blocks,
and trees, some of which are composed of over 3,000 blocks. Additionally, when
trained for regeneration, the system is able to regrow parts of simple
functional machines, significantly expanding the capabilities of simulated
morphogenetic systems. The code for the experiment in this paper can be found
at: https://github.com/real-itu/3d-artefacts-nca.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1"&gt;Shyam Sudhakaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1"&gt;Djordje Grbic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1"&gt;Adam Katona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1"&gt;Elias Najarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1"&gt;Claire Glanois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1"&gt;Sebastian Risi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation. (arXiv:2102.02805v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02805</id>
        <link href="http://arxiv.org/abs/2102.02805"/>
        <updated>2021-06-07T03:06:16.914Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting undermines the effectiveness of deep neural networks
(DNNs) in scenarios such as continual learning and lifelong learning. While
several methods have been proposed to tackle this problem, there is limited
work explaining why these methods work well. This paper has the goal of better
explaining a popularly used technique for avoiding catastrophic forgetting:
quadratic regularization. We show that quadratic regularizers prevent
forgetting of past tasks by interpolating current and previous values of model
parameters at every training iteration. Over multiple training iterations, this
interpolation operation reduces the learning rates of more important model
parameters, thereby minimizing their movement. Our analysis also reveals two
drawbacks of quadratic regularization: (a) dependence of parameter
interpolation on training hyperparameters, which often leads to training
instability and (b) assignment of lower importance to deeper layers, which are
generally the place forgetting occurs in DNNs. Via a simple modification to the
order of operations, we show these drawbacks can be easily avoided, resulting
in 6.2% higher average accuracy at 4.5% lower average forgetting. Code
available at \url{https://github.com/EkdeepSLubana/QRforgetting}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1"&gt;Ekdeep Singh Lubana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1"&gt;Puja Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1"&gt;Robert P. Dick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates. (arXiv:1905.09997v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.09997</id>
        <link href="http://arxiv.org/abs/1905.09997"/>
        <updated>2021-06-07T03:06:16.908Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that stochastic gradient descent (SGD) achieves the
fast convergence rates of full-batch gradient descent for over-parameterized
models satisfying certain interpolation conditions. However, the step-size used
in these works depends on unknown quantities and SGD's practical performance
heavily relies on the choice of this step-size. We propose to use line-search
techniques to automatically set the step-size when training models that can
interpolate the data. In the interpolation setting, we prove that SGD with a
stochastic variant of the classic Armijo line-search attains the deterministic
convergence rates for both convex and strongly-convex functions. Under
additional assumptions, SGD with Armijo line-search is shown to achieve fast
convergence for non-convex functions. Furthermore, we show that stochastic
extra-gradient with a Lipschitz line-search attains linear convergence for an
important class of non-convex functions and saddle-point problems satisfying
interpolation. To improve the proposed methods' practical performance, we give
heuristics to use larger step-sizes and acceleration. We compare the proposed
algorithms against numerous optimization methods on standard classification
tasks using both kernel methods and deep networks. The proposed methods result
in competitive performance across all models and datasets, while being robust
to the precise choices of hyper-parameters. For multi-class classification
using deep networks, SGD with Armijo line-search results in both faster
convergence and better generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_S/0/1/0/all/0/1"&gt;Sharan Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishkin_A/0/1/0/all/0/1"&gt;Aaron Mishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1"&gt;Issam Laradji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Effects of Quantisation on Model Uncertainty in Bayesian Neural Networks. (arXiv:2102.11062v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11062</id>
        <link href="http://arxiv.org/abs/2102.11062"/>
        <updated>2021-06-07T03:06:16.902Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks (BNNs) are making significant progress in many
research areas where decision-making needs to be accompanied by uncertainty
estimation. Being able to quantify uncertainty while making decisions is
essential for understanding when the model is over-/under-confident, and hence
BNNs are attracting interest in safety-critical applications, such as
autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been
as widely used in industrial practice, mainly because of their increased memory
and compute costs. In this work, we investigate quantisation of BNNs by
compressing 32-bit floating-point weights and activations to their integer
counterparts, that has already been successful in reducing the compute demand
in standard pointwise neural networks. We study three types of quantised BNNs,
we evaluate them under a wide range of different settings, and we empirically
demonstrate that a uniform quantisation scheme applied to BNNs does not
substantially decrease their quality of uncertainty estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1"&gt;Martin Ferianc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1"&gt;Partha Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1"&gt;Matthew Mattina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collection and harmonization of system logs and prototypal Analytics services with the Elastic (ELK) suite at the INFN-CNAF computing centre. (arXiv:2106.02612v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.02612</id>
        <link href="http://arxiv.org/abs/2106.02612"/>
        <updated>2021-06-07T03:06:16.895Z</updated>
        <summary type="html"><![CDATA[The distributed Grid infrastructure for High Energy Physics experiments at
the Large Hadron Collider (LHC) in Geneva comprises a set of computing centres,
spread all over the world, as part of the Worldwide LHC Computing Grid (WLCG).
In Italy, the Tier-1 functionalities are served by the INFN-CNAF data center,
which provides also computing and storage resources to more than twenty non-LHC
experiments. For this reason, a high amount of logs are collected each day from
various sources, which are highly heterogeneous and difficult to harmonize. In
this contribution, a working implementation of a system that collects, parses
and displays the log information from CNAF data sources and the investigation
of a Machine Learning based predictive maintenance system, is presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diotalevi_T/0/1/0/all/0/1"&gt;Tommaso Diotalevi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falabella_A/0/1/0/all/0/1"&gt;Antonio Falabella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martelli_B/0/1/0/all/0/1"&gt;Barbara Martelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michelotto_D/0/1/0/all/0/1"&gt;Diego Michelotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morganti_L/0/1/0/all/0/1"&gt;Lucia Morganti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonacorsi_D/0/1/0/all/0/1"&gt;Daniele Bonacorsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giommi_L/0/1/0/all/0/1"&gt;Luca Giommi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tisbeni_S/0/1/0/all/0/1"&gt;Simone Rossi Tisbeni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Multi-aspect Detection of Misinformation using Hierarchical Joint Decomposition. (arXiv:2005.04310v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.04310</id>
        <link href="http://arxiv.org/abs/2005.04310"/>
        <updated>2021-06-07T03:06:16.886Z</updated>
        <summary type="html"><![CDATA[Distinguishing between misinformation and real information is one of the most
challenging problems in today's interconnected world. The vast majority of the
state-of-the-art in detecting misinformation is fully supervised, requiring a
large number of high-quality human annotations. However, the availability of
such annotations cannot be taken for granted, since it is very costly,
time-consuming, and challenging to do so in a way that keeps up with the
proliferation of misinformation. In this work, we are interested in exploring
scenarios where the number of annotations is limited. In such scenarios, we
investigate how tapping on a diverse number of resources that characterize a
news article, henceforth referred to as "aspects" can compensate for the lack
of labels. In particular, our contributions in this paper are twofold: 1) We
propose the use of three different aspects: article content, context of social
sharing behaviors, and host website/domain features, and 2) We introduce a
principled tensor based embedding framework that combines all those aspects
effectively. We propose HiJoD a 2-level decomposition pipeline which not only
outperforms state-of-the-art methods with F1-scores of 74% and 81% on Twitter
and Politifact datasets respectively but also is an order of magnitude faster
than similar ensemble approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1"&gt;Sara Abdali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1"&gt;Evangelos E. Papalexakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental tradeoffs between memorization and robustness in random features and neural tangent regimes. (arXiv:2106.02630v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02630</id>
        <link href="http://arxiv.org/abs/2106.02630"/>
        <updated>2021-06-07T03:06:16.878Z</updated>
        <summary type="html"><![CDATA[This work studies the (non)robustness of two-layer neural networks in various
high-dimensional linearized regimes. We establish fundamental trade-offs
between memorization and robustness, as measured by the Sobolev-seminorm of the
model w.r.t the data distribution, i.e the square root of the average squared
$L_2$-norm of the gradients of the model w.r.t the its input. More precisely,
if $n$ is the number of training examples, $d$ is the input dimension, and $k$
is the number of hidden neurons in a two-layer neural network, we prove for a
large class of activation functions that, if the model memorizes even a
fraction of the training, then its Sobolev-seminorm is lower-bounded by (i)
$\sqrt{n}$ in case of infinite-width random features (RF) or neural tangent
kernel (NTK) with $d \gtrsim n$; (ii) $\sqrt{n}$ in case of finite-width RF
with proportionate scaling of $d$ and $k$; and (iii) $\sqrt{n/k}$ in case of
finite-width NTK with proportionate scaling of $d$ and $k$. Moreover, all of
these lower-bounds are tight: they are attained by the min-norm / least-squares
interpolator (when $n$, $d$, and $k$ are in the appropriate interpolating
regime). All our results hold as soon as data is log-concave isotropic, and
there is label-noise, i.e the target variable is not a deterministic function
of the data / features. We empirically validate our theoretical results with
experiments. Accidentally, these experiments also reveal for the first time,
(iv) a multiple-descent phenomenon in the robustness of the min-norm
interpolator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dohmatob_E/0/1/0/all/0/1"&gt;Elvis Dohmatob&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R. (arXiv:2103.09603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09603</id>
        <link href="http://arxiv.org/abs/2103.09603"/>
        <updated>2021-06-07T03:06:16.871Z</updated>
        <summary type="html"><![CDATA[The R package DoubleML implements the double/debiased machine learning
framework of Chernozhukov et al. (2018). It provides functionalities to
estimate parameters in causal models based on machine learning methods. The
double machine learning framework consist of three key ingredients: Neyman
orthogonality, high-quality machine learning estimation and sample splitting.
Estimation of nuisance components can be performed by various state-of-the-art
machine learning methods that are available in the mlr3 ecosystem. DoubleML
makes it possible to perform inference in a variety of causal models, including
partially linear and interactive regression models and their extensions to
instrumental variable estimation. The object-oriented implementation of
DoubleML enables a high flexibility for the model specification and makes it
easily extendable. This paper serves as an introduction to the double machine
learning framework and the R package DoubleML. In reproducible code examples
with simulated and real data sets, we demonstrate how DoubleML users can
perform valid inference based on machine learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bach_P/0/1/0/all/0/1"&gt;Philipp Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kurz_M/0/1/0/all/0/1"&gt;Malte S. Kurz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spindler_M/0/1/0/all/0/1"&gt;Martin Spindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Precision Reinforcement Learning: Running Soft Actor-Critic in Half Precision. (arXiv:2102.13565v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13565</id>
        <link href="http://arxiv.org/abs/2102.13565"/>
        <updated>2021-06-07T03:06:16.840Z</updated>
        <summary type="html"><![CDATA[Low-precision training has become a popular approach to reduce compute
requirements, memory footprint, and energy consumption in supervised learning.
In contrast, this promising approach has not yet enjoyed similarly widespread
adoption within the reinforcement learning (RL) community, partly because RL
agents can be notoriously hard to train even in full precision. In this paper
we consider continuous control with the state-of-the-art SAC agent and
demonstrate that a na\"ive adaptation of low-precision methods from supervised
learning fails. We propose a set of six modifications, all straightforward to
implement, that leaves the underlying agent and its hyperparameters unchanged
but improves the numerical stability dramatically. The resulting modified SAC
agent has lower memory and compute requirements while matching full-precision
rewards, demonstrating that low-precision training can substantially accelerate
state-of-the-art RL without parameter tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1"&gt;Johan Bjorck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Way to Overcome the Catastrophic Forgetting in Neural Networks. (arXiv:2005.07107v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07107</id>
        <link href="http://arxiv.org/abs/2005.07107"/>
        <updated>2021-06-07T03:06:16.822Z</updated>
        <summary type="html"><![CDATA[Not so long ago, a method was discovered that successfully overcomes the
catastrophic forgetting in neural networks. Although we know about the cases of
using this method to preserve skills when adapting pre-trained networks to
particular tasks, it has not obtained widespread distribution yet. In this
paper, we would like to propose an alternative method of overcoming
catastrophic forgetting based on the total absolute signal passing through each
connection in the network. This method has a simple implementation and seems to
us essentially close to the processes occurring in the brain of animals to
preserve previously learned skills during subsequent learning. We hope that the
ease of implementation of this method will serve its wide application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutalev_A/0/1/0/all/0/1"&gt;Alexey Kutalev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning. (arXiv:2003.01384v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01384</id>
        <link href="http://arxiv.org/abs/2003.01384"/>
        <updated>2021-06-07T03:06:16.801Z</updated>
        <summary type="html"><![CDATA[Current deep reinforcement learning (RL) approaches incorporate minimal prior
knowledge about the environment, limiting computational and sample efficiency.
\textit{Objects} provide a succinct and causal description of the world, and
many recent works have proposed unsupervised object representation learning
using priors and losses over static object properties like visual consistency.
However, object dynamics and interactions are also critical cues for
objectness. In this paper we propose a framework for reasoning about object
dynamics and behavior to rapidly determine minimal and task-specific object
representations. To demonstrate the need to reason over object behavior and
dynamics, we introduce a suite of RGBD MuJoCo object collection and avoidance
tasks that, while intuitive and visually simple, confound state-of-the-art
unsupervised object representation learning algorithms. We also highlight the
potential of this framework on several Atari games, using our object
representation and standard RL and planning algorithms to learn dramatically
faster than existing deep RL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1"&gt;William Agnew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domingos_P/0/1/0/all/0/1"&gt;Pedro Domingos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Controlled Stochastic Dynamical Systems: An Information-Theoretic Approach. (arXiv:2012.12174v6 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12174</id>
        <link href="http://arxiv.org/abs/2012.12174"/>
        <updated>2021-06-07T03:06:16.779Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the fundamental performance limitations in the
control of stochastic dynamical systems; more specifically, we derive generic
$\mathcal{L}_p$ bounds that hold for any causal (stabilizing) controllers and
any stochastic disturbances, by an information-theoretic analysis. We first
consider the scenario where the plant (i.e., the dynamical system to be
controlled) is linear time-invariant, and it is seen in general that the lower
bounds are characterized by the unstable poles (or nonminimum-phase zeros) of
the plant as well as the conditional entropy of the disturbance. We then
analyze the setting where the plant is assumed to be (strictly) causal, for
which case the lower bounds are determined by the conditional entropy of the
disturbance. We also discuss the special cases of $p = 2$ and $p = \infty$,
which correspond to minimum-variance control and controlling the maximum
deviations, respectively. In addition, we investigate the power-spectral
characterization of the lower bounds as well as its relation to the
Kolmogorov-Szeg\"o formula.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MUSBO: Model-based Uncertainty Regularized and Sample Efficient Batch Optimization for Deployment Constrained Reinforcement Learning. (arXiv:2102.11448v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11448</id>
        <link href="http://arxiv.org/abs/2102.11448"/>
        <updated>2021-06-07T03:06:16.770Z</updated>
        <summary type="html"><![CDATA[In many contemporary applications such as healthcare, finance, robotics, and
recommendation systems, continuous deployment of new policies for data
collection and online learning is either cost ineffective or impractical. We
consider a setting that lies between pure offline reinforcement learning (RL)
and pure online RL called deployment constrained RL in which the number of
policy deployments for data sampling is limited. To solve this challenging
task, we propose a new algorithmic learning framework called Model-based
Uncertainty regularized and Sample Efficient Batch Optimization (MUSBO). Our
framework discovers novel and high quality samples for each deployment to
enable efficient data collection. During each offline training session, we
bootstrap the policy update by quantifying the amount of uncertainty within our
collected data. In the high support region (low uncertainty), we encourage our
policy by taking an aggressive update. In the low support region (high
uncertainty) when the policy bootstraps into the out-of-distribution region, we
downweight it by our estimated uncertainty quantification. Experimental results
show that MUSBO achieves state-of-the-art performance in the deployment
constrained RL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1"&gt;DiJia Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulvey_J/0/1/0/all/0/1"&gt;John M. Mulvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout. (arXiv:2102.13451v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13451</id>
        <link href="http://arxiv.org/abs/2102.13451"/>
        <updated>2021-06-07T03:06:16.751Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has been gaining significant traction across
different ML tasks, ranging from vision to keyboard predictions. In large-scale
deployments, client heterogeneity is a fact, and constitutes a primary problem
for fairness, training performance and accuracy. Although significant efforts
have been made into tackling statistical data heterogeneity, the diversity in
the processing capabilities and network bandwidth of clients, termed as system
heterogeneity, has remained largely unexplored. Current solutions either
disregard a large portion of available devices or set a uniform limit on the
model's capacity, restricted by the least capable participants. In this work,
we introduce Ordered Dropout, a mechanism that achieves an ordered, nested
representation of knowledge in Neural Networks and enables the extraction of
lower footprint submodels without the need of retraining. We further show that
for linear maps our Ordered Dropout is equivalent to SVD. We employ this
technique, along with a self-distillation methodology, in the realm of FL in a
framework called FjORD. FjORD alleviates the problem of client system
heterogeneity by tailoring the model width to the client's capabilities.
Extensive evaluation on both CNNs and RNNs across diverse modalities shows that
FjORD consistently leads to significant performance gains over state-of-the-art
baselines, while maintaining its nested structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1"&gt;Samuel Horvath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1"&gt;Stefanos Laskaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_M/0/1/0/all/0/1"&gt;Mario Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1"&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion. (arXiv:2004.13631v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13631</id>
        <link href="http://arxiv.org/abs/2004.13631"/>
        <updated>2021-06-07T03:06:16.744Z</updated>
        <summary type="html"><![CDATA[A comprehensive knowledge graph (KG) contains an instance-level entity graph
and an ontology-level concept graph. The two-view KG provides a testbed for
models to "simulate" human's abilities on knowledge abstraction,
concretization, and completion (KACC), which are crucial for human to recognize
the world and manage learned knowledge. Existing studies mainly focus on
partial aspects of KACC. In order to promote thorough analyses for KACC
abilities of models, we propose a unified KG benchmark by improving existing
benchmarks in terms of dataset scale, task coverage, and difficulty.
Specifically, we collect new datasets that contain larger concept graphs,
abundant cross-view links as well as dense entity graphs. Based on the
datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),
multi-hop knowledge concretization (MKC) and then design a comprehensive
benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical
triples as harder samples. The experimental results of existing methods
demonstrate the challenges of our benchmark. The resource is available at
https://github.com/thunlp/KACC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning One Representation to Optimize All Rewards. (arXiv:2103.07945v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07945</id>
        <link href="http://arxiv.org/abs/2103.07945"/>
        <updated>2021-06-07T03:06:16.728Z</updated>
        <summary type="html"><![CDATA[We introduce the forward-backward (FB) representation of the dynamics of a
reward-free Markov decision process. It provides explicit near-optimal policies
for any reward specified a posteriori. During an unsupervised phase, we use
reward-free interactions with the environment to learn two representations via
off-the-shelf deep learning methods and temporal difference (TD) learning. In
the test phase, a reward representation is estimated either from observations
or an explicit reward description (e.g., a target state). The optimal policy
for that reward is directly obtained from these representations, with no
planning. We assume access to an exploration scheme or replay buffer for the
first phase.

The unsupervised FB loss is well-principled: if training is perfect, the
policies obtained are provably optimal for any reward function. With imperfect
training, the sub-optimality is proportional to the unsupervised approximation
error. The FB representation learns long-range relationships between states and
actions, via a predictive occupancy map, without having to synthesize states as
in model-based approaches.

This is a step towards learning controllable agents in arbitrary black-box
stochastic environments. This approach compares well to goal-oriented RL
algorithms on discrete and continuous mazes, pixel-based MsPacman, and the
FetchReach virtual robot arm. We also illustrate how the agent can immediately
adapt to new tasks beyond goal-oriented RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1"&gt;Ahmed Touati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1"&gt;Yann Ollivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. (arXiv:2005.12964v9 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12964</id>
        <link href="http://arxiv.org/abs/2005.12964"/>
        <updated>2021-06-07T03:06:16.706Z</updated>
        <summary type="html"><![CDATA[Deep candidate generation (DCG) that narrows down the collection of relevant
items from billions to hundreds via representation learning has become
prevalent in industrial recommender systems. Standard approaches approximate
maximum likelihood estimation (MLE) through sampling for better scalability and
address the problem of DCG in a way similar to language modeling. However, live
recommender systems face severe exposure bias and have a vocabulary several
orders of magnitude larger than that of natural language, implying that MLE
will preserve and even exacerbate the exposure bias in the long run in order to
faithfully fit the observed samples. In this paper, we theoretically prove that
a popular choice of contrastive loss is equivalent to reducing the exposure
bias via inverse propensity weighting, which provides a new perspective for
understanding the effectiveness of contrastive learning. Based on the
theoretical discovery, we design CLRec, a contrastive learning method to
improve DCG in terms of fairness, effectiveness and efficiency in recommender
systems with extremely large candidate size. We further improve upon CLRec and
propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our
methods have been successfully deployed in Taobao, where at least four-month
online A/B tests and offline analyses demonstrate its substantial improvements,
including a dramatic reduction in the Matthew effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Convex Optimization Perspective for Learning from Dynamically Revealed Preferences. (arXiv:2008.10460v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10460</id>
        <link href="http://arxiv.org/abs/2008.10460"/>
        <updated>2021-06-07T03:06:16.665Z</updated>
        <summary type="html"><![CDATA[We study the problem of online learning (OL) from revealed preferences: a
learner wishes to learn a non-strategic agent's private utility function
through observing the agent's utility-maximizing actions in a changing
environment. We adopt an online inverse optimization setup, where the learner
observes a stream of agent's actions in an online fashion and the learning
performance is measured by regret associated with a loss function. We first
characterize a special but broad class of agent's utility functions, then
utilize this structure in designing a new convex loss function. We establish
that the regret with respect to our new loss function also bounds the regret
with respect to all other usual loss functions in the literature. This allows
us to design a flexible OL framework that enables a unified treatment of loss
functions and supports a variety of online convex optimization algorithms. We
demonstrate with theoretical and empirical evidence that our framework based on
the new loss function (in particular online Mirror Descent) has significant
advantages in terms of regret performance and solution time over other OL
algorithms from the literature and bypasses the previous technical assumptions
as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_V/0/1/0/all/0/1"&gt;Violet Xinying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kilinc_Karzan_F/0/1/0/all/0/1"&gt;Fatma K&amp;#x131;l&amp;#x131;n&amp;#xe7;-Karzan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Covering. (arXiv:2106.02552v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02552</id>
        <link href="http://arxiv.org/abs/2106.02552"/>
        <updated>2021-06-07T03:06:16.658Z</updated>
        <summary type="html"><![CDATA[We analyze the problem of active covering, where the learner is given an
unlabeled dataset and can sequentially label query examples. The objective is
to label query all of the positive examples in the fewest number of total label
queries. We show under standard non-parametric assumptions that a classical
support estimator can be repurposed as an offline algorithm attaining an excess
query cost of $\widetilde{\Theta}(n^{D/(D+1)})$ compared to the optimal
learner, where $n$ is the number of datapoints and $D$ is the dimension. We
then provide a simple active learning method that attains an improved excess
query cost of $\widetilde{O}(n^{(D-1)/D})$. Furthermore, the proposed
algorithms only require access to the positive labeled examples, which in
certain settings provides additional computational and privacy benefits.
Finally, we show that the active learning method consistently outperforms
offline methods as well as a variety of baselines on a wide range of benchmark
image-based datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivalence of Dataflow Graphs via Rewrite Rules Using a Graph-to-Sequence Neural Model. (arXiv:2002.06799v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06799</id>
        <link href="http://arxiv.org/abs/2002.06799"/>
        <updated>2021-06-07T03:06:16.651Z</updated>
        <summary type="html"><![CDATA[In this work we target the problem of provably computing the equivalence
between two programs represented as dataflow graphs. To this end, we formalize
the problem of equivalence between two programs as finding a set of
semantics-preserving rewrite rules from one into the other, such that after the
rewrite the two programs are structurally identical, and therefore trivially
equivalent. We then develop the first graph-to-sequence neural network system
for program equivalence, trained to produce such rewrite sequences from a
carefully crafted automatic example generation algorithm. We extensively
evaluate our system on a rich multi-type linear algebra expression language,
using arbitrary combinations of 100+ graph-rewriting axioms of equivalence. Our
system outputs via inference a correct rewrite sequence for 96% of the 10,000
program pairs isolated for testing, using 30-term programs. And in all cases,
the validity of the sequence produced and therefore the provable assertion of
program equivalence is computable, in negligible time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barollet_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Barollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:16.634Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Estimation of Derivatives Using Plug-in KRR Estimators. (arXiv:2006.01350v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01350</id>
        <link href="http://arxiv.org/abs/2006.01350"/>
        <updated>2021-06-07T03:06:16.624Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating the derivatives of the regression
function, which has a wide range of applications as a key nonparametric
functional of unknown functions. Standard analysis may be tailored to specific
derivative orders, and parameter tuning remains a daunting challenge
particularly for high-order derivatives. In this article, we propose a simple
plug-in kernel ridge regression (KRR) estimator in nonparametric regression
with random design that is broadly applicable for multi-dimensional support and
arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to
study the behavior of the proposed estimator, leading to two error bounds for a
general class of kernels under the strong $L_\infty$ norm. In a concrete
example specialized to kernels with polynomially decaying eigenvalues, the
proposed estimator recovers the minimax optimal rate up to a logarithmic factor
for estimating derivatives of functions in H\"older class. Interestingly, the
proposed estimator achieves the optimal rate of convergence with the same
choice of tuning parameter for any order of derivatives. Hence, the proposed
estimator enjoys a remarkable \textit{plug-in property} for derivatives in that
it automatically adapts to the order of derivatives to be estimated, enabling
easy tuning in practice. Our simulation studies show favorable finite sample
performance of the proposed method relative to several existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zejian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1"&gt;Meng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Aggregation Functions. (arXiv:2012.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08482</id>
        <link href="http://arxiv.org/abs/2012.08482"/>
        <updated>2021-06-07T03:06:16.611Z</updated>
        <summary type="html"><![CDATA[Learning on sets is increasingly gaining attention in the machine learning
community, due to its widespread applicability. Typically, representations over
sets are computed by using fixed aggregation functions such as sum or maximum.
However, recent results showed that universal function representation by sum-
(or max-) decomposition requires either highly discontinuous (and thus poorly
learnable) mappings, or a latent dimension equal to the maximum number of
elements in the set. To mitigate this problem, we introduce a learnable
aggregation function (LAF) for sets of arbitrary cardinality. LAF can
approximate several extensively used aggregators (such as average, sum,
maximum) as well as more complex functions (e.g., variance and skewness). We
report experiments on semi-synthetic and real data showing that LAF outperforms
state-of-the-art sum- (max-) decomposition architectures such as DeepSets and
library-based architectures like Principal Neighborhood Aggregation, and can be
effectively combined with attention-based architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pellegrini_G/0/1/0/all/0/1"&gt;Giovanni Pellegrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tibo_A/0/1/0/all/0/1"&gt;Alessandro Tibo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasconi_P/0/1/0/all/0/1"&gt;Paolo Frasconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1"&gt;Andrea Passerini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaeger_M/0/1/0/all/0/1"&gt;Manfred Jaeger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editing Conditional Radiance Fields. (arXiv:2105.06466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06466</id>
        <link href="http://arxiv.org/abs/2105.06466"/>
        <updated>2021-06-07T03:06:16.515Z</updated>
        <summary type="html"><![CDATA[A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richard Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"&gt;Bryan Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance Weighted Policy Learning and Adaptation. (arXiv:2009.04875v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04875</id>
        <link href="http://arxiv.org/abs/2009.04875"/>
        <updated>2021-06-07T03:06:16.506Z</updated>
        <summary type="html"><![CDATA[The ability to exploit prior experience to solve novel problems rapidly is a
hallmark of biological learning systems and of great practical importance for
artificial ones. In the meta reinforcement learning literature much recent work
has focused on the problem of optimizing the learning process itself. In this
paper we study a complementary approach which is conceptually simple, general,
modular and built on top of recent improvements in off-policy learning. The
framework is inspired by ideas from the probabilistic inference literature and
combines robust off-policy learning with a behavior prior, or default behavior
that constrains the space of solutions and serves as a bias for exploration; as
well as a representation for the value function, both of which are easily
learned from a number of training tasks in a multi-task scenario. Our approach
achieves competitive adaptation performance on hold-out tasks compared to meta
reinforcement learning baselines and can scale to complex sparse-reward
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1"&gt;Alexandre Galashov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1"&gt;Jakub Sygnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desjardins_G/0/1/0/all/0/1"&gt;Guillaume Desjardins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Humplik_J/0/1/0/all/0/1"&gt;Jan Humplik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1"&gt;Leonard Hasenclever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_R/0/1/0/all/0/1"&gt;Rae Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some observations on high-dimensional partial differential equations with Barron data. (arXiv:2012.01484v3 [math.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01484</id>
        <link href="http://arxiv.org/abs/2012.01484"/>
        <updated>2021-06-07T03:06:16.243Z</updated>
        <summary type="html"><![CDATA[We use explicit representation formulas to show that solutions to certain
partial differential equations lie in Barron spaces or multilayer spaces if the
PDE data lie in such function spaces. Consequently, these solutions can be
represented efficiently using artificial neural networks, even in high
dimension. Conversely, we present examples in which the solution fails to lie
in the function space associated to a neural network under consideration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does CTC result in peaky behavior?. (arXiv:2105.14849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14849</id>
        <link href="http://arxiv.org/abs/2105.14849"/>
        <updated>2021-06-07T03:06:16.242Z</updated>
        <summary type="html"><![CDATA[The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-07T03:06:16.231Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning. (arXiv:2106.02597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02597</id>
        <link href="http://arxiv.org/abs/2106.02597"/>
        <updated>2021-06-07T03:06:16.230Z</updated>
        <summary type="html"><![CDATA[Counterfactual instances are a powerful tool to obtain valuable insights into
automated decision processes, describing the necessary minimal changes in the
input space to alter the prediction towards a desired target. Most previous
approaches require a separate, computationally expensive optimization procedure
per instance, making them impractical for both large amounts of data and
high-dimensional data. Moreover, these methods are often restricted to certain
subclasses of machine learning models (e.g. differentiable or tree-based
models). In this work, we propose a deep reinforcement learning approach that
transforms the optimization procedure into an end-to-end learnable process,
allowing us to generate batches of counterfactual instances in a single forward
pass. Our experiments on real-world data show that our method i) is
model-agnostic (does not assume differentiability), relying only on feedback
from model predictions; ii) allows for generating target-conditional
counterfactual instances; iii) allows for flexible feature range constraints
for numerical and categorical attributes, including the immutability of
protected features (e.g. gender, race); iv) is easily extended to other data
modalities such as images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samoilescu_R/0/1/0/all/0/1"&gt;Robert-Florian Samoilescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Looveren_A/0/1/0/all/0/1"&gt;Arnaud Van Looveren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klaise_J/0/1/0/all/0/1"&gt;Janis Klaise&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NF-GNN: Network Flow Graph Neural Networks for Malware Detection and Classification. (arXiv:2103.03939v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03939</id>
        <link href="http://arxiv.org/abs/2103.03939"/>
        <updated>2021-06-07T03:06:16.224Z</updated>
        <summary type="html"><![CDATA[Malicious software (malware) poses an increasing threat to the security of
communication systems as the number of interconnected mobile devices increases
exponentially. While some existing malware detection and classification
approaches successfully leverage network traffic data, they treat network flows
between pairs of endpoints independently and thus fail to leverage rich
communication patterns present in the complete network. Our approach first
extracts flow graphs and subsequently classifies them using a novel edge
feature-based graph neural network model. We present three variants of our base
model, which support malware detection and classification in supervised and
unsupervised settings. We evaluate our approach on flow graphs that we extract
from a recently published dataset for mobile malware detection that addresses
several issues with previously available datasets. Experiments on four
different prediction tasks consistently demonstrate the advantages of our
approach and show that our graph neural network model can boost detection
performance by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Busch_J/0/1/0/all/0/1"&gt;Julian Busch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocheturov_A/0/1/0/all/0/1"&gt;Anton Kocheturov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1"&gt;Thomas Seidl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09258</id>
        <link href="http://arxiv.org/abs/2101.09258"/>
        <updated>2021-06-07T03:06:16.211Z</updated>
        <summary type="html"><![CDATA[Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based models
can be tractably computed through a connection to continuous normalizing flows,
but log-likelihood is not directly optimized by the weighted combination of
score matching losses. We show that for a specific weighting scheme, the
objective upper bounds the negative log-likelihood, thus enabling approximate
maximum likelihood training of score-based models. We empirically observe that
maximum likelihood training consistently improves the likelihood of score-based
models across multiple datasets, stochastic processes, and model architectures.
Our best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on
CIFAR-10 and down-sampled ImageNet, outperforming all existing likelihood-based
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1"&gt;Conor Durkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"&gt;Iain Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alert Classification for the ALeRCE Broker System: The Real-time Stamp Classifier. (arXiv:2008.03309v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03309</id>
        <link href="http://arxiv.org/abs/2008.03309"/>
        <updated>2021-06-07T03:06:16.204Z</updated>
        <summary type="html"><![CDATA[We present a real-time stamp classifier of astronomical events for the ALeRCE
(Automatic Learning for the Rapid Classification of Events) broker. The
classifier is based on a convolutional neural network, trained on alerts
ingested from the Zwicky Transient Facility (ZTF). Using only the
\textit{science, reference} and \textit{difference} images of the first
detection as inputs, along with the metadata of the alert as features, the
classifier is able to correctly classify alerts from active galactic nuclei,
supernovae (SNe), variable stars, asteroids and bogus classes, with high
accuracy ($\sim$94\%) in a balanced test set. In order to find and analyze SN
candidates selected by our classifier from the ZTF alert stream, we designed
and deployed a visualization tool called SN Hunter, where relevant information
about each possible SN is displayed for the experts to choose among candidates
to report to the Transient Name Server database. From June 26th 2019 to
February 28th 2021, we have reported 6846 SN candidates to date (11.8
candidates per day on average), of which 971 have been confirmed
spectroscopically. Our ability to report objects using only a single detection
means that 70\% of the reported SNe occurred within one day after the first
detection. ALeRCE has only reported candidates not otherwise detected or
selected by other groups, therefore adding new early transients to the bulk of
objects available for early follow-up. Our work represents an important
milestone toward rapid alert classifications with the next generation of large
etendue telescopes, such as the Vera C. Rubin Observatory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Carrasco_Davis_R/0/1/0/all/0/1"&gt;Rodrigo Carrasco-Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Reyes_E/0/1/0/all/0/1"&gt;Esteban Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Valenzuela_C/0/1/0/all/0/1"&gt;Camilo Valenzuela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Forster_F/0/1/0/all/0/1"&gt;Francisco F&amp;#xf6;rster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Estevez_P/0/1/0/all/0/1"&gt;Pablo A. Est&amp;#xe9;vez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pignata_G/0/1/0/all/0/1"&gt;Giuliano Pignata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bauer_F/0/1/0/all/0/1"&gt;Franz E. Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Reyes_I/0/1/0/all/0/1"&gt;Ignacio Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sanchez_Saez_P/0/1/0/all/0/1"&gt;Paula S&amp;#xe1;nchez-S&amp;#xe1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cabrera_Vives_G/0/1/0/all/0/1"&gt;Guillermo Cabrera-Vives&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Eyheramendy_S/0/1/0/all/0/1"&gt;Susana Eyheramendy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Catelan_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rcio Catelan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Arredondo_J/0/1/0/all/0/1"&gt;Javier Arredondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Castillo_Navarrete_E/0/1/0/all/0/1"&gt;Ernesto Castillo-Navarrete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rodriguez_Mancini_D/0/1/0/all/0/1"&gt;Diego Rodr&amp;#xed;guez-Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ruz_Mieres_D/0/1/0/all/0/1"&gt;Daniela Ruz-Mieres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Moya_A/0/1/0/all/0/1"&gt;Alberto Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sabatini_Gacitua_L/0/1/0/all/0/1"&gt;Luis Sabatini-Gacit&amp;#xfa;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sepulveda_Cobo_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Sep&amp;#xfa;lveda-Cobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Mahabal_A/0/1/0/all/0/1"&gt;Ashish A. Mahabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Silva_Farfan_J/0/1/0/all/0/1"&gt;Javier Silva-Farf&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Camacho_Iniquez_E/0/1/0/all/0/1"&gt;Ernesto Camacho-I&amp;#xf1;iquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Galbany_L/0/1/0/all/0/1"&gt;Llu&amp;#xed;s Galbany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Democracy: Using Deep Learning to Identify and Prevent Misinformation. (arXiv:2106.02607v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02607</id>
        <link href="http://arxiv.org/abs/2106.02607"/>
        <updated>2021-06-07T03:06:16.183Z</updated>
        <summary type="html"><![CDATA[The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhm_A/0/1/0/all/0/1"&gt;Alyssa Suhm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahankal_P/0/1/0/all/0/1"&gt;Prathamesh Mahankal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuntharaj_S/0/1/0/all/0/1"&gt;Subhiksha Mukuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parab_M/0/1/0/all/0/1"&gt;Meghana D. Parab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_M/0/1/0/all/0/1"&gt;Malvika Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Meredith Berger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethumadhavan_A/0/1/0/all/0/1"&gt;Arathi Sethumadhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiman_A/0/1/0/all/0/1"&gt;Ashish Jaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Enhanced Rotation Prediction for Self-Supervised Learning. (arXiv:1912.11603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.11603</id>
        <link href="http://arxiv.org/abs/1912.11603"/>
        <updated>2021-06-07T03:06:16.176Z</updated>
        <summary type="html"><![CDATA[The rotation prediction (Rotation) is a simple pretext-task for
self-supervised learning (SSL), where models learn useful representations for
target vision tasks by solving pretext-tasks. Although Rotation captures
information of object shapes, it hardly captures information of textures. To
tackle this problem, we introduce a novel pretext-task called image enhanced
rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and
another pretext-task based on image enhancement (e.g., sharpening and
solarizing) while maintaining simplicity. Through the simultaneous prediction
of rotation and image enhancement, models learn representations to capture the
information of not only object shapes but also textures. Our experimental
results show that IE-Rot models outperform Rotation on various standard
benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO
detection/segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shioda_T/0/1/0/all/0/1"&gt;Tetsuya Shioda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10897</id>
        <link href="http://arxiv.org/abs/2103.10897"/>
        <updated>2021-06-07T03:06:16.169Z</updated>
        <summary type="html"><![CDATA[This work introduces Bilinear Classes, a new structural framework, which
permit generalization in reinforcement learning in a wide variety of settings
through the use of function approximation. The framework incorporates nearly
all existing models in which a polynomial sample complexity is achievable, and,
notably, also includes new models, such as the Linear $Q^*/V^*$ model in which
both the optimal $Q$-function and the optimal $V$-function are linear in some
known feature space. Our main result provides an RL algorithm which has
polynomial sample complexity for Bilinear Classes; notably, this sample
complexity is stated in terms of a reduction to the generalization error of an
underlying supervised learning sub-problem. These bounds nearly match the best
known sample complexity bounds for existing models. Furthermore, this framework
also extends to the infinite dimensional (RKHS) setting: for the the Linear
$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample
complexities that have no explicit dependence on the explicit feature dimension
(which could be infinite), but instead depends only on information theoretic
quantities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1"&gt;Shachar Lovett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1"&gt;Gaurav Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruosong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Prediction, Generalization, and Recursion: An Entropic-Innovations Perspective. (arXiv:2001.03813v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03813</id>
        <link href="http://arxiv.org/abs/2001.03813"/>
        <updated>2021-06-07T03:06:16.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the fundamental performance limits of prediction,
with or without side information. More specifically, we derive generic lower
bounds on the $\mathcal{L}_p$ norms of the prediction errors that are valid for
any prediction algorithms and for any data distributions. Meanwhile, we combine
the entropic analysis from information theory and the innovations approach from
prediction/estimation theory to characterize the conditions (in terms of, e.g.,
directed information or mutual information) to achieve the bounds. We also
investigate the implications of the results in analyzing the fundamental limits
of generalization in fitting (learning) problems from the perspective of
prediction with side information, as well as the fundamental limits of
recursive algorithms by viewing them as generalized prediction problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-06-07T03:06:16.150Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Difficulty of Unbiased Alpha Divergence Minimization. (arXiv:2010.09541v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09541</id>
        <link href="http://arxiv.org/abs/2010.09541"/>
        <updated>2021-06-07T03:06:16.132Z</updated>
        <summary type="html"><![CDATA[Several approximate inference algorithms have been proposed to minimize an
alpha-divergence between an approximating distribution and a target
distribution. Many of these algorithms introduce bias, the magnitude of which
becomes problematic in high dimensions. Other algorithms are unbiased. These
often seem to suffer from high variance, but little is rigorously known. In
this work we study unbiased methods for alpha-divergence minimization through
the Signal-to-Noise Ratio (SNR) of the gradient estimator. We study several
representative scenarios where strong analytical results are possible, such as
fully-factorized or Gaussian distributions. We find that when alpha is not
zero, the SNR worsens exponentially in the dimensionality of the problem. This
casts doubt on the practicality of these methods. We empirically confirm these
theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Geffner_T/0/1/0/all/0/1"&gt;Tomas Geffner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Domke_J/0/1/0/all/0/1"&gt;Justin Domke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Iterative Graph Matching. (arXiv:2106.02206v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02206</id>
        <link href="http://arxiv.org/abs/2106.02206"/>
        <updated>2021-06-07T03:06:16.126Z</updated>
        <summary type="html"><![CDATA[Recent works leveraging Graph Neural Networks to approach graph matching
tasks have shown promising results. Recent progress in learning discrete
distributions poses new opportunities for learning graph matching models. In
this work, we propose a new model, Stochastic Iterative Graph MAtching (SIGMA),
to address the graph matching problem. Our model defines a distribution of
matchings for a graph pair so the model can explore a wide range of possible
matchings. We further introduce a novel multi-step matching procedure, which
learns how to refine a graph pair's matching results incrementally. The model
also includes dummy nodes so that the model does not have to find matchings for
nodes without correspondence. We fit this model to data via scalable stochastic
optimization. We conduct extensive experiments across synthetic graph datasets
as well as biochemistry and computer vision applications. Across all tasks, our
results show that SIGMA can produce significantly improved graph matching
results compared to state-of-the-art models. Ablation studies verify that each
of our components (stochastic training, iterative matching, and dummy nodes)
offers noticeable improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linfeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1"&gt;Soha Hassoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li-Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02549</id>
        <link href="http://arxiv.org/abs/2106.02549"/>
        <updated>2021-06-07T03:06:16.120Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1"&gt;Thorben Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCA Initialization for Approximate Message Passing in Rotationally Invariant Models. (arXiv:2106.02356v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02356</id>
        <link href="http://arxiv.org/abs/2106.02356"/>
        <updated>2021-06-07T03:06:16.112Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating a rank-$1$ signal in the presence of
rotationally invariant noise-a class of perturbations more general than
Gaussian noise. Principal Component Analysis (PCA) provides a natural
estimator, and sharp results on its performance have been obtained in the
high-dimensional regime. Recently, an Approximate Message Passing (AMP)
algorithm has been proposed as an alternative estimator with the potential to
improve the accuracy of PCA. However, the existing analysis of AMP requires an
initialization that is both correlated with the signal and independent of the
noise, which is often unrealistic in practice. In this work, we combine the two
methods, and propose to initialize AMP with PCA. Our main result is a rigorous
asymptotic characterization of the performance of this estimator. Both the AMP
algorithm and its analysis differ from those previously derived in the Gaussian
setting: at every iteration, our AMP algorithm requires a specific term to
account for PCA initialization, while in the Gaussian case, PCA initialization
affects only the first iteration of AMP. The proof is based on a two-phase
artificial AMP that first approximates the PCA estimator and then mimics the
true AMP. Our numerical simulations show an excellent agreement between AMP
results and theoretical predictions, and suggest an interesting open direction
on achieving Bayes-optimal performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1"&gt;Marco Mondelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1"&gt;Ramji Venkataramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Streaming Linear System Identification with Reverse Experience Replay. (arXiv:2103.05896v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05896</id>
        <link href="http://arxiv.org/abs/2103.05896"/>
        <updated>2021-06-07T03:06:16.106Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating a linear time-invariant (LTI) dynamical
system from a single trajectory via streaming algorithms, which is encountered
in several applications including reinforcement learning (RL) and time-series
analysis. While the LTI system estimation problem is well-studied in the {\em
offline} setting, the practically important streaming/online setting has
received little attention. Standard streaming methods like stochastic gradient
descent (SGD) are unlikely to work since streaming points can be highly
correlated. In this work, we propose a novel streaming algorithm, SGD with
Reverse Experience Replay ($\mathsf{SGD}-\mathsf{RER}$), that is inspired by
the experience replay (ER) technique popular in the RL literature.
$\mathsf{SGD}-\mathsf{RER}$ divides data into small buffers and runs SGD
backwards on the data stored in the individual buffers. We show that this
algorithm exactly deconstructs the dependency structure and obtains information
theoretically optimal guarantees for both parameter error and prediction error.
Thus, we provide the first -- to the best of our knowledge -- optimal SGD-style
algorithm for the classical problem of linear system identification with a
first order oracle. Furthermore, $\mathsf{SGD}-\mathsf{RER}$ can be applied to
more general settings like sparse LTI identification with known sparsity
pattern, and non-linear dynamical systems. Our work demonstrates that the
knowledge of data dependency structure can aid us in designing statistically
and computationally efficient algorithms which can "decorrelate" streaming
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning phylogenetic trees as hyperbolic point configurations. (arXiv:2104.11430v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11430</id>
        <link href="http://arxiv.org/abs/2104.11430"/>
        <updated>2021-06-07T03:06:16.086Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for the inference of phylogenetic trees that
utilises point configurations on hyperbolic space as its optimisation
landscape. Each taxon corresponds to a point of the point configuration, while
the evolutionary distance between taxa is represented by the geodesic distance
between their corresponding points. The point configuration is iteratively
modified to increase an objective function that additively combines pairwise
log-likelihood terms. After convergence, the final tree is derived from the
inter-point distances using a standard distance-based method. The objective
function, which is shown to mimic the log-likelihood on tree space, is a
differentiable function on a Riemannian manifold. Thus gradient-based
optimisation techniques can be applied, avoiding the need for combinatorial
rearrangements of tree topology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1"&gt;Benjamin Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sobolev Norm Learning Rates for Conditional Mean Embeddings. (arXiv:2105.07446v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07446</id>
        <link href="http://arxiv.org/abs/2105.07446"/>
        <updated>2021-06-07T03:06:16.080Z</updated>
        <summary type="html"><![CDATA[We develop novel learning rates for conditional mean embeddings by applying
the theory of interpolation for reproducing kernel Hilbert spaces (RKHS). We
derive explicit, adaptive convergence rates for the sample estimator under the
misspecifed setting, where the target operator is not Hilbert-Schmidt or
bounded with respect to the input/output RKHSs. We demonstrate that in certain
parameter regimes, we can achieve uniform convergence rates in the output RKHS.
We hope our analyses will allow the much broader application of conditional
mean embeddings to more complex ML/RL settings involving infinite dimensional
RKHSs and continuous state spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Talwai_P/0/1/0/all/0/1"&gt;Prem Talwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shameli_A/0/1/0/all/0/1"&gt;Ali Shameli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation formulas and pointwise properties for Barron functions. (arXiv:2006.05982v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05982</id>
        <link href="http://arxiv.org/abs/2006.05982"/>
        <updated>2021-06-07T03:06:16.074Z</updated>
        <summary type="html"><![CDATA[We study the natural function space for infinitely wide two-layer neural
networks with ReLU activation (Barron space) and establish different
representation formulae. In two cases, we describe the space explicitly up to
isomorphism.

Using a convenient representation, we study the pointwise properties of
two-layer networks and show that functions whose singular set is fractal or
curved (for example distance functions from smooth submanifolds) cannot be
represented by infinitely wide two-layer networks with finite path-norm. We use
this structure theorem to show that the only $C^1$-diffeomorphisms which Barron
space are affine.

Furthermore, we show that every Barron function can be decomposed as the sum
of a bounded and a positively one-homogeneous function and that there exist
Barron functions which decay rapidly at infinity and are globally
Lebesgue-integrable. This result suggests that two-layer neural networks may be
able to approximate a greater variety of functions than commonly believed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Be Considerate: Objectives, Side Effects, and Deciding How to Act. (arXiv:2106.02617v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02617</id>
        <link href="http://arxiv.org/abs/2106.02617"/>
        <updated>2021-06-07T03:06:16.068Z</updated>
        <summary type="html"><![CDATA[Recent work in AI safety has highlighted that in sequential decision making,
objectives are often underspecified or incomplete. This gives discretion to the
acting agent to realize the stated objective in ways that may result in
undesirable outcomes. We contend that to learn to act safely, a reinforcement
learning (RL) agent should include contemplation of the impact of its actions
on the wellbeing and agency of others in the environment, including other
acting agents and reactive processes. We endow RL agents with the ability to
contemplate such impact by augmenting their reward based on expectation of
future return by others in the environment, providing different criteria for
characterizing impact. We further endow these agents with the ability to
differentially factor this impact into their decision making, manifesting
behavior that ranges from self-centred to self-less, as demonstrated by
experiments in gridworld environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamdari_P/0/1/0/all/0/1"&gt;Parand Alizadeh Alamdari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klassen_T/0/1/0/all/0/1"&gt;Toryn Q. Klassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icarte_R/0/1/0/all/0/1"&gt;Rodrigo Toro Icarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1"&gt;Sheila A. McIlraith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators. (arXiv:2106.02205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02205</id>
        <link href="http://arxiv.org/abs/2106.02205"/>
        <updated>2021-06-07T03:06:16.062Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel pre-trained language models (PLM) compression
approach based on the matrix product operator (short as MPO) from quantum
many-body physics. It can decompose an original matrix into central tensors
(containing the core information) and auxiliary tensors (with only a small
proportion of parameters). With the decomposed MPO structure, we propose a
novel fine-tuning strategy by only updating the parameters from the auxiliary
tensors, and design an optimization algorithm for MPO-based approximation over
stacked network architectures. Our approach can be applied to the original or
the compressed PLMs in a general way, which derives a lighter network and
significantly reduces the parameters to be fine-tuned. Extensive experiments
have demonstrated the effectiveness of the proposed approach in model
compression, especially the reduction in finetuning parameters (91% reduction
on average).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Ze-Feng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Z.Y. Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhong-Yi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model. (arXiv:2106.02566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02566</id>
        <link href="http://arxiv.org/abs/2106.02566"/>
        <updated>2021-06-07T03:06:16.056Z</updated>
        <summary type="html"><![CDATA[The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
`active level' of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. It is also more accurate, faster, and with a smaller memory footprint
than usual neural attention modules. Extensive experiments showcase more
comprehensive visual explanations compared to the state-of-the-art
visualization model across multiple tasks including few-shot classification,
person re-identification, fine-grained image classification. The proposed
visualization model sheds imperative light on how neural networks `pay their
attention' differently in different tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1"&gt;Tristan Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Suiyi Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1"&gt;Thomas Fr&amp;#xe9;our&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1"&gt;Harold Mouch&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Driven Road Maintenance Inspection. (arXiv:2106.02567v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02567</id>
        <link href="http://arxiv.org/abs/2106.02567"/>
        <updated>2021-06-07T03:06:16.030Z</updated>
        <summary type="html"><![CDATA[Road infrastructure maintenance inspection is typically a labour-intensive
and critical task to ensure the safety of all the road users. In this work, we
propose a detailed methodology to use state-of-the-art techniques in artificial
intelligence and computer vision to automate a sizeable portion of the
maintenance inspection subtasks and reduce the labour costs. The proposed
methodology uses state-of-the-art computer vision techniques such as object
detection and semantic segmentation to automate inspections on primary road
structures such as the road surface, markings, barriers (guardrails) and
traffic signs. The models are mostly trained on commercially viable datasets
and augmented with proprietary data. We demonstrate that our AI models can not
only automate and scale maintenance inspections on primary road structures but
also result in higher recall compared to traditional manual inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1"&gt;Ratnajit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haris Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badar_A/0/1/0/all/0/1"&gt;Ahmed Badar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouns_T/0/1/0/all/0/1"&gt;Terence Brouns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contracting Neural-Newton Solver. (arXiv:2106.02543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02543</id>
        <link href="http://arxiv.org/abs/2106.02543"/>
        <updated>2021-06-07T03:06:16.024Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have set the focus on neural networks (NNs)
that can successfully replace traditional numerical solvers in many
applications, achieving impressive computing gains. One such application is
time domain simulation, which is indispensable for the design, analysis and
operation of many engineering systems. Simulating dynamical systems with
implicit Newton-based solvers is a computationally heavy task, as it requires
the solution of a parameterized system of differential and algebraic equations
at each time step. A variety of NN-based methodologies have been shown to
successfully approximate the dynamical trajectories computed by numerical time
domain solvers at a fraction of the time. However, so far no previous NN-based
model has explicitly captured the fact that any predicted point on the time
domain trajectory also represents the fixed point of the numerical solver
itself. As we show, explicitly capturing this property can lead to
significantly increased NN accuracy and much smaller NN sizes. In this paper,
we model the Newton solver at the heart of an implicit Runge-Kutta integrator
as a contracting map iteratively seeking this fixed point. Our primary
contribution is to develop a recurrent NN simulation tool, termed the
Contracting Neural-Newton Solver (CoNNS), which explicitly captures the
contracting nature of these Newton iterations. To build CoNNS, we train a
feedforward NN and mimic this contraction behavior by embedding a series of
training constraints which guarantee the mapping provided by the NN satisfies
the Banach fixed-point theorem; thus, we are able to prove that successive
passes through the NN are guaranteed to converge to a unique, fixed point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chevalier_S/0/1/0/all/0/1"&gt;Samuel Chevalier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1"&gt;Jochen Stiasny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1"&gt;Spyros Chatzivasileiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Learning for Communication Networks: A Survey. (arXiv:2106.02533v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02533</id>
        <link href="http://arxiv.org/abs/2106.02533"/>
        <updated>2021-06-07T03:06:16.018Z</updated>
        <summary type="html"><![CDATA[Communication networks are important infrastructures in contemporary society.
There are still many challenges that are not fully solved and new solutions are
proposed continuously in this active research area. In recent years, to model
the network topology, graph-based deep learning has achieved state-of-the-art
performance in a series of problems in communication networks. In this survey,
we review the rapidly growing body of research using different graph-based deep
learning models, e.g. graph convolutional and graph attention networks, in
various problems from different communication networks, e.g. wireless networks,
wired networks, and software-defined networks. We also present a well-organized
list of the problem and solution for each study and identify future research
directions. To the best of our knowledge, this paper is the first survey that
focuses on the application of graph-based deep learning methods in
communication networks. To track the follow-up research, a public GitHub
repository is created, where the relevant papers will be updated continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwei Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning-based synthetic-CT generation in radiotherapy and PET: a review. (arXiv:2102.02734v2 [physics.med-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02734</id>
        <link href="http://arxiv.org/abs/2102.02734"/>
        <updated>2021-06-07T03:06:16.011Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning (DL)-based methods for the generation of synthetic
computed tomography (sCT) have received significant research attention as an
alternative to classical ones. We present here a systematic review of these
methods by grouping them into three categories, according to their clinical
applications: I) To replace CT in magnetic resonance (MR)-based treatment
planning. II) Facilitate cone-beam computed tomography (CBCT)-based
image-guided adaptive radiotherapy. III) Derive attenuation maps for the
correction of positron emission tomography (PET). Appropriate database
searching was performed on journal articles published between January 2014 and
December 2020. The DL methods' key characteristics were extracted from each
eligible study, and a comprehensive comparison among network architectures and
metrics was reported. A detailed review of each category was given,
highlighting essential contributions, identifying specific challenges, and
summarising the achievements. Lastly, the statistics of all the cited works
from various aspects were analysed, revealing the popularity and future trends,
and the potential of DL-based sCT generation. The current status of DL-based
sCT generation was evaluated, assessing the clinical readiness of the presented
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Spadea_M/0/1/0/all/0/1"&gt;Maria Francesca Spadea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Maspero_M/0/1/0/all/0/1"&gt;Matteo Maspero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zaffino_P/0/1/0/all/0/1"&gt;Paolo Zaffino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Seco_J/0/1/0/all/0/1"&gt;Joao Seco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bag of Tricks for Node Classification with Graph Neural Networks. (arXiv:2103.13355v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13355</id>
        <link href="http://arxiv.org/abs/2103.13355"/>
        <updated>2021-06-07T03:06:16.001Z</updated>
        <summary type="html"><![CDATA[Over the past few years, graph neural networks (GNN) and label
propagation-based methods have made significant progress in addressing node
classification tasks on graphs. However, in addition to their reliance on
elaborate architectures and algorithms, there are several key technical details
that are frequently overlooked, and yet nonetheless can play a vital role in
achieving satisfactory performance. In this paper, we first summarize a series
of existing tricks-of-the-trade, and then propose several new ones related to
label usage, loss function formulation, and model design that can significantly
improve various GNN architectures. We empirically evaluate their impact on
final node classification accuracy by conducting ablation studies and
demonstrate consistently-improved performance, often to an extent that
outweighs the gains from more dramatic changes in the underlying GNN
architecture. Notably, many of the top-ranked models on the Open Graph
Benchmark (OGB) leaderboard benefit from our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yangkun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jiarui Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness-Aware Unsupervised Feature Selection. (arXiv:2106.02216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02216</id>
        <link href="http://arxiv.org/abs/2106.02216"/>
        <updated>2021-06-07T03:06:15.981Z</updated>
        <summary type="html"><![CDATA[Feature selection is a prevalent data preprocessing paradigm for various
learning tasks. Due to the expensive cost of acquiring supervision information,
unsupervised feature selection sparks great interests recently. However,
existing unsupervised feature selection algorithms do not have fairness
considerations and suffer from a high risk of amplifying discrimination by
selecting features that are over associated with protected attributes such as
gender, race, and ethnicity. In this paper, we make an initial investigation of
the fairness-aware unsupervised feature selection problem and develop a
principled framework, which leverages kernel alignment to find a subset of
high-quality features that can best preserve the information in the original
feature space while being minimally correlated with protected attributes.
Specifically, different from the mainstream in-processing debiasing methods,
our proposed framework can be regarded as a model-agnostic debiasing strategy
that eliminates biases and discrimination before downstream learning algorithms
are involved. Experimental results on multiple real-world datasets demonstrate
that our framework achieves a good trade-off between utility maximization and
fairness promotion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1"&gt;Xiaoying Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Gradient Algorithms for Nonconvex C^{1+alpha} Cost Functions. (arXiv:2012.00628v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00628</id>
        <link href="http://arxiv.org/abs/2012.00628"/>
        <updated>2021-06-07T03:06:15.975Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with convergence of stochastic gradient algorithms
with momentum terms in the nonconvex setting. A class of stochastic momentum
methods, including stochastic gradient descent, heavy ball, and Nesterov's
accelerated gradient, is analyzed in a general framework under mild
assumptions. Based on the convergence result of expected gradients, we prove
the almost sure convergence by a detailed discussion of the effects of momentum
and the number of upcrossings. It is worth noting that there are not additional
restrictions imposed on the objective function and stepsize. Another
improvement over previous results is that the existing Lipschitz condition of
the gradient is relaxed into the condition of Holder continuity. As a
byproduct, we apply a localization procedure to extend our results to
stochastic stepsizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shanjian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Granger Causality from Irregularly Sampled Time Series. (arXiv:2106.02600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02600</id>
        <link href="http://arxiv.org/abs/2106.02600"/>
        <updated>2021-06-07T03:06:15.968Z</updated>
        <summary type="html"><![CDATA[Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Song Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Josef_C/0/1/0/all/0/1"&gt;Christopher S. Josef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamaleswaran_R/0/1/0/all/0/1"&gt;Rishikesan Kamaleswaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:15.958Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing. (arXiv:2106.02559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02559</id>
        <link href="http://arxiv.org/abs/2106.02559"/>
        <updated>2021-06-07T03:06:15.952Z</updated>
        <summary type="html"><![CDATA[Analysing whether neural language models encode linguistic information has
become popular in NLP. One method of doing so, which is frequently cited to
support the claim that models like BERT encode syntax, is called probing;
probes are small supervised models trained to extract linguistic information
from another model's output. If a probe is able to predict a particular
structure, it is argued that the model whose output it is trained on must have
implicitly learnt to encode it. However, drawing a generalisation about a
model's linguistic knowledge about a specific phenomena based on what a probe
is able to learn may be problematic: in this work, we show that semantic cues
in training data means that syntactic probes do not properly isolate syntax. We
generate a new corpus of semantically nonsensical but syntactically well-formed
Jabberwocky sentences, which we use to evaluate two probes trained on normal
data. We train the probes on several popular language models (BERT, GPT, and
RoBERTa), and find that in all settings they perform worse when evaluated on
these data, for one probe by an average of 15.4 UUAS points absolute. Although
in most cases they still outperform the baselines, their lead is reduced
substantially, e.g. by 53% in the case of BERT for one probe. This begs the
question: what empirical scores constitute knowing syntax?]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViViT: Curvature access through the generalized Gauss-Newton's low-rank structure. (arXiv:2106.02624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02624</id>
        <link href="http://arxiv.org/abs/2106.02624"/>
        <updated>2021-06-07T03:06:15.929Z</updated>
        <summary type="html"><![CDATA[Curvature in form of the Hessian or its generalized Gauss-Newton (GGN)
approximation is valuable for algorithms that rely on a local model for the
loss to train, compress, or explain deep networks. Existing methods based on
implicit multiplication via automatic differentiation or Kronecker-factored
block diagonal approximations do not consider noise in the mini-batch. We
present ViViT, a curvature model that leverages the GGN's low-rank structure
without further approximations. It allows for efficient computation of
eigenvalues, eigenvectors, as well as per-sample first- and second-order
directional derivatives. The representation is computed in parallel with
gradients in one backward pass and offers a fine-grained cost-accuracy
trade-off, which allows it to scale. As examples for ViViT's usefulness, we
investigate the directional gradients and curvatures during training, and how
noise information can be used to improve the stability of second-order methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dangel_F/0/1/0/all/0/1"&gt;Felix Dangel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatzel_L/0/1/0/all/0/1"&gt;Lukas Tatzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Multiplicative Weights Update: Learning to Learn using Projector-based Game Signatures. (arXiv:2106.02615v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.02615</id>
        <link href="http://arxiv.org/abs/2106.02615"/>
        <updated>2021-06-07T03:06:15.922Z</updated>
        <summary type="html"><![CDATA[Recently, Optimistic Multiplicative Weights Update (OMWU) was proven to be
the first constant step-size algorithm in the online no-regret framework to
enjoy last-iterate convergence to Nash Equilibria in the constrained zero-sum
bimatrix case, where weights represent the probabilities of playing pure
strategies. We introduce the second such algorithm, \textit{Consensus MWU}, for
which we prove local convergence and show empirically that it enjoys faster and
more robust convergence than OMWU. Our algorithm shows the importance of a new
object, the \textit{simplex Hessian}, as well as of the interaction of the game
with the (eigen)space of vectors summing to zero, which we believe future
research can build on. As for OMWU, CMWU has convergence guarantees in the
zero-sum case only, but Cheung and Piliouras (2020) recently showed that OMWU
and MWU display opposite convergence properties depending on whether the game
is zero-sum or cooperative. Inspired by this work and the recent literature on
learning to optimize for single functions, we extend CMWU to non zero-sum games
by introducing a new framework for online learning in games, where the update
rule's gradient and Hessian coefficients along a trajectory are learnt by a
reinforcement learning policy that is conditioned on the nature of the game:
\textit{the game signature}. We construct the latter using a new canonical
decomposition of two-player games into eight components corresponding to
commutative projection operators, generalizing and unifying recent game
concepts studied in the literature. We show empirically that our new learning
policy is able to exploit the game signature across a wide range of game types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vadori_N/0/1/0/all/0/1"&gt;Nelson Vadori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savani_R/0/1/0/all/0/1"&gt;Rahul Savani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1"&gt;Thomas Spooner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1"&gt;Sumitra Ganesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation. (arXiv:2004.03238v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03238</id>
        <link href="http://arxiv.org/abs/2004.03238"/>
        <updated>2021-06-07T03:06:15.914Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) models for reading comprehension have achieved
human-level accuracy on in-distribution test sets. However, they have been
demonstrated to lack robustness to challenge sets, whose distribution is
different from that of training sets. Existing data augmentation methods
mitigate this problem by simply augmenting training sets with synthetic
examples sampled from the same distribution as the challenge sets. However,
these methods assume that the distribution of a challenge set is known a
priori, making them less applicable to unseen challenge sets. In this study, we
focus on question-answer pair generation (QAG) to mitigate this problem. While
most existing QAG methods aim to improve the quality of synthetic examples, we
conjecture that diversity-promoting QAG can mitigate the sparsity of training
sets and lead to better robustness. We present a variational QAG model that
generates multiple diverse QA pairs from a paragraph. Our experiments show that
our method can improve the accuracy of 12 challenge sets, as well as the
in-distribution accuracy. Our code and data are available at
https://github.com/KazutoshiShinoda/VQAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1"&gt;Kazutoshi Shinoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Empirical Objective Functions for MCMC Proposal Optimization. (arXiv:2106.02104v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02104</id>
        <link href="http://arxiv.org/abs/2106.02104"/>
        <updated>2021-06-07T03:06:15.898Z</updated>
        <summary type="html"><![CDATA[We introduce and demonstrate a semi-empirical procedure for determining
approximate objective functions suitable for optimizing arbitrarily
parameterized proposal distributions in MCMC methods. Our proposed Ab Initio
objective functions consist of the weighted combination of functions following
constraints on their global optima and of coordinate invariance that we argue
should be upheld by general measures of MCMC efficiency for use in proposal
optimization. The coefficients of Ab Initio objective functions are determined
so as to recover the optimal MCMC behavior prescribed by established
theoretical analysis for chosen reference problems. Our experimental results
demonstrate that Ab Initio objective functions maintain favorable performance
and preferable optimization behavior compared to existing objective functions
for MCMC optimization when optimizing highly expressive proposal distributions.
We argue that Ab Initio objective functions are sufficiently robust to enable
the confident optimization of MCMC proposal distributions parameterized by deep
generative networks that extend beyond the traditional limitations of
individual MCMC schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cannella_C/0/1/0/all/0/1"&gt;Chris Cannella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Question Rewriting for Increasing Response Rate. (arXiv:2106.02257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02257</id>
        <link href="http://arxiv.org/abs/2106.02257"/>
        <updated>2021-06-07T03:06:15.889Z</updated>
        <summary type="html"><![CDATA[When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiayi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xilian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled change point detection via representation learning. (arXiv:2106.02602v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02602</id>
        <link href="http://arxiv.org/abs/2106.02602"/>
        <updated>2021-06-07T03:06:15.872Z</updated>
        <summary type="html"><![CDATA[Change points are abrupt alterations in the distribution of sequential data.
A change-point detection (CPD) model aims at quick detection of such changes.
Classic approaches perform poorly for semi-structured sequential data because
of the absence of adequate data representation learning. To deal with it, we
introduce a principled differentiable loss function that considers the
specificity of the CPD task. The theoretical results suggest that this function
approximates well classic rigorous solutions. For such loss function, we
propose an end-to-end method for the training of deep representation learning
CPD models. Our experiments provide evidence that the proposed approach
improves baseline results of change point detection for various data types,
including real-world videos and image sequences, and improve representations
for them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romanenkova_E/0/1/0/all/0/1"&gt;Evgenia Romanenkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zainulin_R/0/1/0/all/0/1"&gt;Ramil Zainulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_M/0/1/0/all/0/1"&gt;Matvey Morozov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning transition times in event sequences: the Event-Based Hidden Markov Model of disease progression. (arXiv:2011.01023v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01023</id>
        <link href="http://arxiv.org/abs/2011.01023"/>
        <updated>2021-06-07T03:06:15.865Z</updated>
        <summary type="html"><![CDATA[Progressive diseases worsen over time and are characterised by monotonic
change in features that track disease progression. Here we connect ideas from
two formerly separate methodologies -- event-based and hidden Markov modelling
-- to derive a new generative model of disease progression. Our model can
uniquely infer the most likely group-level sequence and timing of events
(natural history) from limited datasets. Moreover, it can infer and predict
individual-level trajectories (prognosis) even when data are missing, giving it
high clinical utility. Here we derive the model and provide an inference scheme
based on the expectation maximisation algorithm. We use clinical, imaging and
biofluid data from the Alzheimer's Disease Neuroimaging Initiative to
demonstrate the validity and utility of our model. First, we train our model to
uncover a new group-level sequence of feature changes in Alzheimer's disease
over a period of ${\sim}17.3$ years. Next, we demonstrate that our model
provides improved utility over a continuous time hidden Markov model by area
under the receiver operator characteristic curve ${\sim}0.23$. Finally, we
demonstrate that our model maintains predictive accuracy with up to $50\%$
missing data. These results support the clinical validity of our model and its
broader utility in resource-limited medical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wijeratne_P/0/1/0/all/0/1"&gt;Peter A. Wijeratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Graph Neural Networks. (arXiv:2102.07835v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07835</id>
        <link href="http://arxiv.org/abs/2102.07835"/>
        <updated>2021-06-07T03:06:15.851Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a powerful architecture for tackling graph
learning tasks, yet have been shown to be oblivious to eminent substructures,
such as cycles. We present TOGL, a novel layer that incorporates global
topological information of a graph using persistent homology. TOGL can be
easily integrated into any type of GNN and is strictly more expressive in terms
of the Weisfeiler--Lehman test of isomorphism. Augmenting GNNs with our layer
leads to beneficial predictive performance for graph and node classification
tasks, both on synthetic data sets, which can be classified by humans using
their topology but not by ordinary GNNs, and on real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1"&gt;Edward De Brouwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_Y/0/1/0/all/0/1"&gt;Yves Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homological Time Series Analysis of Sensor Signals from Power Plants. (arXiv:2106.02493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02493</id>
        <link href="http://arxiv.org/abs/2106.02493"/>
        <updated>2021-06-07T03:06:15.840Z</updated>
        <summary type="html"><![CDATA[In this paper, we use topological data analysis techniques to construct a
suitable neural network classifier for the task of learning sensor signals of
entire power plants according to their reference designation system. We use
representations of persistence diagrams to derive necessary preprocessing steps
and visualize the large amounts of data. We derive architectures with deep
one-dimensional convolutional layers combined with stacked long short-term
memories as residual networks suitable for processing the persistence features.
We combine three separate sub-networks, obtaining as input the time series
itself and a representation of the persistent homology for the zeroth and first
dimension. We give a mathematical derivation for most of the used
hyper-parameters. For validation, numerical experiments were performed with
sensor data from four power plants of the same construction type.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melodia_L/0/1/0/all/0/1"&gt;Luciano Melodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenz_R/0/1/0/all/0/1"&gt;Richard Lenz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Sliced Embedding Discrepancy for Incomparable Distributions. (arXiv:2106.02542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02542</id>
        <link href="http://arxiv.org/abs/2106.02542"/>
        <updated>2021-06-07T03:06:15.828Z</updated>
        <summary type="html"><![CDATA[Gromov-Wasserstein (GW) distance is a key tool for manifold learning and
cross-domain learning, allowing the comparison of distributions that do not
live in the same metric space. Because of its high computational complexity,
several approximate GW distances have been proposed based on entropy
regularization or on slicing, and one-dimensional GW computation. In this
paper, we propose a novel approach for comparing two incomparable
distributions, that hinges on the idea of distributional slicing, embeddings,
and on computing the closed-form Wasserstein distance between the sliced
distributions. We provide a theoretical analysis of this new divergence, called
distributional sliced embedding (DSE) discrepancy, and we show that it
preserves several interesting properties of GW distance including
rotation-invariance. We show that the embeddings involved in DSE can be
efficiently learned. Finally, we provide a large set of experiments
illustrating the behavior of DSE as a divergence in the context of generative
modeling and in query framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alaya_M/0/1/0/all/0/1"&gt;Mokhtar Z. Alaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1"&gt;Maxime Berar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1"&gt;Alain Rakotomamonjy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Guarantees for Online Receding Horizon Learning Control. (arXiv:2010.11327v12 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11327</id>
        <link href="http://arxiv.org/abs/2010.11327"/>
        <updated>2021-06-07T03:06:15.820Z</updated>
        <summary type="html"><![CDATA[In this paper we provide provable regret guarantees for an online
meta-learning receding horizon control algorithm in an iterative control
setting. We consider the setting where, in each iteration the system to be
controlled is a linear deterministic system that is different and unknown, the
cost for the controller in an iteration is a general additive cost function and
there are affine control input constraints. By analysing conditions under which
sub-linear regret is achievable, we prove that the online receding horizon
controller achieves a regret for the controller cost and constraint violation
that are $\tilde{O}(T^{3/4})$ with respect to the best policy that satisfies
the control input control constraints, when the preview of the cost functions
is limited to an interval and the interval size is doubled from one to the
next. We then show that the average of the regret for the controller cost and
constraint violation with respect to the same policy vary as
$\tilde{O}((1+1/\sqrt{N})T^{3/4})$ with the number of iterations $N$, under the
same setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Muthirayan_D/0/1/0/all/0/1"&gt;Deepan Muthirayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khargonekar_P/0/1/0/all/0/1"&gt;Pramod P. Khargonekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08360</id>
        <link href="http://arxiv.org/abs/2102.08360"/>
        <updated>2021-06-07T03:06:15.801Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increasingly been used as an auxiliary tool in
healthcare applications, due to their ability to improve performance of several
diagnosis tasks. However, these methods are not widely adopted in clinical
settings due to the practical limitations in the reliability, generalizability,
and interpretability of deep learning based systems. As a result, methods have
been developed that impose additional constraints during network training to
gain more control as well as improve interpretabilty, facilitating their
acceptance in healthcare community. In this work, we investigate the benefit of
using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases
from chest X-ray images. The OS constraint can be written as a simple
orthonormality term which is used in conjunction with the standard
cross-entropy loss during classification network training. Previous studies
have demonstrated significant benefits in applying such constraints to deep
learning models. Our findings corroborate these observations, indicating that
the orthonormality loss function effectively produces improved semantic
localization via GradCAM visualizations, enhanced classification performance,
and reduced model calibration error. Our approach achieves an improvement in
accuracy of 1.6% and 4.8% for two- and three-class classification,
respectively; similar results are found for models with data augmentation
applied. In addition to these findings, our work also presents a new
application of the OS regularizer in healthcare, increasing the post-hoc
interpretability and performance of deep learning models for COVID-19
classification to facilitate adoption of these methods in clinical settings. We
also identify the limitations of our strategy that can be explored for further
research in future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Ella Y. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1"&gt;Ankita Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Holistic Approach to Interpretability in Financial Lending: Models, Visualizations, and Summary-Explanations. (arXiv:2106.02605v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02605</id>
        <link href="http://arxiv.org/abs/2106.02605"/>
        <updated>2021-06-07T03:06:15.797Z</updated>
        <summary type="html"><![CDATA[Lending decisions are usually made with proprietary models that provide
minimally acceptable explanations to users. In a future world without such
secrecy, what decision support tools would one want to use for justified
lending decisions? This question is timely, since the economy has dramatically
shifted due to a pandemic, and a massive number of new loans will be necessary
in the short term. We propose a framework for such decisions, including a
globally interpretable machine learning model, an interactive visualization of
it, and several types of summaries and explanations for any given decision. The
machine learning model is a two-layer additive risk model, which resembles a
two-layer neural network, but is decomposable into subscales. In this model,
each node in the first (hidden) layer represents a meaningful subscale model,
and all of the nonlinearities are transparent. Our online visualization tool
allows exploration of this model, showing precisely how it came to its
conclusion. We provide three types of explanations that are simpler than, but
consistent with, the global model: case-based reasoning explanations that use
neighboring past cases, a set of features that were the most important for the
model's prediction, and summary-explanations that provide a customized sparse
explanation for any particular lending decision made by the model. Our
framework earned the FICO recognition award for the Explainable Machine
Learning Challenge, which was the first public challenge in the domain of
explainable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kangcheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaposhnik_Y/0/1/0/all/0/1"&gt;Yaron Shaposhnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Algorithms for Sparse Principal Component Analysis. (arXiv:2006.12748v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12748</id>
        <link href="http://arxiv.org/abs/2006.12748"/>
        <updated>2021-06-07T03:06:15.789Z</updated>
        <summary type="html"><![CDATA[Principal component analysis (PCA) is a widely used dimension reduction
technique in machine learning and multivariate statistics. To improve the
interpretability of PCA, various approaches to obtain sparse principal
direction loadings have been proposed, which are termed Sparse Principal
Component Analysis (SPCA). In this paper, we present thresholding as a provably
accurate, polynomial time, approximation algorithm for the SPCA problem,
without imposing any restrictive assumptions on the input covariance matrix.
Our first thresholding algorithm using the Singular Value Decomposition is
conceptually simple; is faster than current state-of-the-art; and performs well
in practice. On the negative side, our (novel) theoretical bounds do not
accurately predict the strong practical performance of this approach. The
second algorithm solves a well-known semidefinite programming relaxation and
then uses a novel, two step, deterministic thresholding scheme to compute a
sparse principal vector. It works very well in practice and, remarkably, this
solid practical performance is accurately predicted by our theoretical bounds,
which bridge the theory-practice gap better than current state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Agniva Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drineas_P/0/1/0/all/0/1"&gt;Petros Drineas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Samson Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners. (arXiv:2006.09264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09264</id>
        <link href="http://arxiv.org/abs/2006.09264"/>
        <updated>2021-06-07T03:06:15.769Z</updated>
        <summary type="html"><![CDATA[One-shot Neural Architecture Search (NAS) aims to minimize the computational
expense of discovering state-of-the-art models. However, in the past year
attention has been drawn to the comparable performance of naive random search
across the same search spaces used by leading NAS algorithms. To address this,
we explore the effects of drastically relaxing the NAS search space, and we
present Bonsai-Net, an efficient one-shot NAS method to explore our relaxed
search space. Bonsai-Net is built around a modified differential pruner and can
consistently discover state-of-the-art architectures that are significantly
better than random search with fewer parameters than other state-of-the-art
methods. Additionally, Bonsai-Net performs simultaneous model search and
training, dramatically reducing the total time it takes to generate
fully-trained models from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geada_R/0/1/0/all/0/1"&gt;Rob Geada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangle_D/0/1/0/all/0/1"&gt;Dennis Prangle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1"&gt;Andrew Stephen McGough&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02331</id>
        <link href="http://arxiv.org/abs/2106.02331"/>
        <updated>2021-06-07T03:06:15.758Z</updated>
        <summary type="html"><![CDATA[This paper presents a new deep clustering (DC) method called manifold-aware
DC (M-DC) that can enhance hyperspace utilization more effectively than the
original DC. The original DC has a limitation in that a pair of two speakers
has to be embedded having an orthogonal relationship due to its use of the
one-hot vector-based loss function, while our method derives a unique loss
function aimed at maximizing the target angle in the hyperspace based on the
nature of a regular simplex. Our proposed loss imposes a higher penalty than
the original DC when the speaker is assigned incorrectly. The change from DC to
M-DC can be easily achieved by rewriting just one term in the loss function of
DC, without any other modifications to the network architecture or model
parameters. As such, our method has high practicability because it does not
affect the original inference part. The experimental results show that the
proposed method improves the performances of the original DC and its expansion
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1"&gt;Keitaro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sawata_R/0/1/0/all/0/1"&gt;Ryosuke Sawata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Shusuke Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Online Mirror Descent. (arXiv:2106.02393v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02393</id>
        <link href="http://arxiv.org/abs/2106.02393"/>
        <updated>2021-06-07T03:06:15.741Z</updated>
        <summary type="html"><![CDATA[We introduce and analyze MT-OMD, a multitask generalization of Online Mirror
Descent (OMD) which operates by sharing updates between tasks. We prove that
the regret of MT-OMD is of order $\sqrt{1 + \sigma^2(N-1)}\sqrt{T}$, where
$\sigma^2$ is the task variance according to the geometry induced by the
regularizer, $N$ is the number of tasks, and $T$ is the time horizon. Whenever
tasks are similar, that is, $\sigma^2 \le 1$, this improves upon the
$\sqrt{NT}$ bound obtained by running independent OMDs on each task. Our
multitask extensions of Online Gradient Descent and Exponentiated Gradient, two
important instances of OMD, are shown to enjoy closed-form updates, making them
easy to use in practice. Finally, we provide numerical experiments on four
real-world datasets which support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1"&gt;Pierre Laforgue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1"&gt;Andrea Paudice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovery of Causal Additive Models in the Presence of Unobserved Variables. (arXiv:2106.02234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02234</id>
        <link href="http://arxiv.org/abs/2106.02234"/>
        <updated>2021-06-07T03:06:15.702Z</updated>
        <summary type="html"><![CDATA[Causal discovery from data affected by unobserved variables is an important
but difficult problem to solve. The effects that unobserved variables have on
the relationships between observed variables are more complex in nonlinear
cases than in linear cases. In this study, we focus on causal additive models
in the presence of unobserved variables. Causal additive models exhibit
structural equations that are additive in the variables and error terms. We
take into account the presence of not only unobserved common causes but also
unobserved intermediate variables. Our theoretical results show that, when the
causal relationships are nonlinear and there are unobserved variables, it is
not possible to identify all the causal relationships between observed
variables through regression and independence tests. However, our theoretical
results also show that it is possible to avoid incorrect inferences. We propose
a method to identify all the causal relationships that are theoretically
possible to identify without being biased by unobserved variables. The
empirical results using artificial data and simulated functional magnetic
resonance imaging (fMRI) data show that our method effectively infers causal
structures in the presence of unobserved variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1"&gt;Takashi Nicholas Maeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1"&gt;Shohei Shimizu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-$k$ Regularization for Supervised Feature Selection. (arXiv:2106.02197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02197</id>
        <link href="http://arxiv.org/abs/2106.02197"/>
        <updated>2021-06-07T03:06:15.657Z</updated>
        <summary type="html"><![CDATA[Feature selection identifies subsets of informative features and reduces
dimensions in the original feature space, helping provide insights into data
generation or a variety of domain problems. Existing methods mainly depend on
feature scoring functions or sparse regularizations; nonetheless, they have
limited ability to reconcile the representativeness and inter-correlations of
features. In this paper, we introduce a novel, simple yet effective
regularization approach, named top-$k$ regularization, to supervised feature
selection in regression and classification tasks. Structurally, the top-$k$
regularization induces a sub-architecture on the architecture of a learning
model to boost its ability to select the most informative features and model
complex nonlinear relationships simultaneously. Theoretically, we derive and
mathematically prove a uniform approximation error bound for using this
approach to approximate high-dimensional sparse functions. Extensive
experiments on a wide variety of benchmarking datasets show that the top-$k$
regularization is effective and stable for supervised feature selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinxing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qiang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Strict Generalisation Benefit for Invariance in Kernel Methods. (arXiv:2106.02346v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02346</id>
        <link href="http://arxiv.org/abs/2106.02346"/>
        <updated>2021-06-07T03:06:15.572Z</updated>
        <summary type="html"><![CDATA[It is a commonly held belief that enforcing invariance improves
generalisation. Although this approach enjoys widespread popularity, it is only
very recently that a rigorous theoretical demonstration of this benefit has
been established. In this work we build on the function space perspective of
Elesedy and Zaidi arXiv:2102.10333 to derive a strictly non-zero generalisation
benefit of incorporating invariance in kernel ridge regression when the target
is invariant to the action of a compact group. We study invariance enforced by
feature averaging and find that generalisation is governed by a notion of
effective dimension that arises from the interplay between the kernel and the
group. In building towards this result, we find that the action of the group
induces an orthogonal decomposition of both the reproducing kernel Hilbert
space and its kernel, which may be of interest in its own right.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Elesedy_B/0/1/0/all/0/1"&gt;Bryn Elesedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rates of (Locally) Differentially Private Heavy-tailed Multi-Armed Bandits. (arXiv:2106.02575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02575</id>
        <link href="http://arxiv.org/abs/2106.02575"/>
        <updated>2021-06-07T03:06:15.207Z</updated>
        <summary type="html"><![CDATA[In this paper we study the problem of stochastic multi-armed bandits (MAB) in
the (local) differential privacy (DP/LDP) model. Unlike the previous results
which need to assume bounded reward distributions, here we mainly focus on the
case the reward distribution of each arm only has $(1+v)$-th moment with some
$v\in (0, 1]$. In the first part, we study the problem in the central
$\epsilon$-DP model. We first provide a near-optimal result by developing a
private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the
result via a private and robust version of the Successive Elimination (SE)
algorithm. Finally, we show that the instance-dependent regret bound of our
improved algorithm is optimal by showing its lower bound. In the second part of
the paper, we study the problem in the $\epsilon$-LDP model. We propose an
algorithm which could be seen as locally private and robust version of the SE
algorithm, and show it could achieve (near) optimal rates for both
instance-dependent and instance-independent regrets. All of the above results
can also reveal the differences between the problem of private MAB with bounded
rewards and heavy-tailed rewards. To achieve these (near) optimal rates, we
develop several new hard instances and private robust estimators as byproducts,
which might could be used to other related problems. Finally, experimental
results also support our theoretical analysis and show the effectiveness of our
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Youming Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06175</id>
        <link href="http://arxiv.org/abs/2103.06175"/>
        <updated>2021-06-07T03:06:15.201Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junguang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yifei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yufeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide Network Learning with Differential Privacy. (arXiv:2103.01294v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01294</id>
        <link href="http://arxiv.org/abs/2103.01294"/>
        <updated>2021-06-07T03:06:15.194Z</updated>
        <summary type="html"><![CDATA[Despite intense interest and considerable effort, the current generation of
neural networks suffers a significant loss of accuracy under most practically
relevant privacy training regimes. One particularly challenging class of neural
networks are the wide ones, such as those deployed for NLP typeahead prediction
or recommender systems. Observing that these models share something in
common--an embedding layer that reduces the dimensionality of the input--we
focus on developing a general approach towards training these models that takes
advantage of the sparsity of the gradients. More abstractly, we address the
problem of differentially private empirical risk minimization (ERM) for models
that admit sparse gradients. We demonstrate that for non-convex ERM problems,
the loss is logarithmically dependent on the number of parameters, in contrast
with polynomial dependence for the general case. Following the same intuition,
we propose a novel algorithm for privately training neural networks. Finally,
we provide an empirical study of a DP wide neural network on a real-world
dataset, which has been rarely explored in the previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mironov_I/0/1/0/all/0/1"&gt;Ilya Mironov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hejazinia_M/0/1/0/all/0/1"&gt;Meisam Hejazinia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cybersecurity Threats in Connected and Automated Vehicles based Federated Learning Systems. (arXiv:2102.13256v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13256</id>
        <link href="http://arxiv.org/abs/2102.13256"/>
        <updated>2021-06-07T03:06:15.186Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a machine learning technique that aims at training
an algorithm across decentralized entities holding their local data private.
Wireless mobile networks allow users to communicate with other fixed or mobile
users. The road traffic network represents an infrastructure-based
configuration of a wireless mobile network where the Connected and Automated
Vehicles (CAV) represent the communicating entities. Applying FL in a wireless
mobile network setting gives rise to a new threat in the mobile environment
that is very different from the traditional fixed networks. The threat is due
to the intrinsic characteristics of the wireless medium and is caused by the
characteristics of the vehicular networks such as high node-mobility and
rapidly changing topology. Most cyber defense techniques depend on highly
reliable and connected networks. This paper explores falsified information
attacks, which target the FL process that is ongoing at the RSU. We identified
a number of attack strategies conducted by the malicious CAVs to disrupt the
training of the global model in vehicular networks. We show that the attacks
were able to increase the convergence time and decrease the accuracy the model.
We demonstrate that our attacks bypass FL defense strategies in their primary
form and highlight the need for novel poisoning resilience defense mechanisms
in the wireless mobile setting of the future road networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mallah_R/0/1/0/all/0/1"&gt;Ranwa Al Mallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badu_Marfo_G/0/1/0/all/0/1"&gt;Godwin Badu-Marfo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_B/0/1/0/all/0/1"&gt;Bilal Farooq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedCCEA : A Practical Approach of Client Contribution Evaluation for Federated Learning. (arXiv:2106.02310v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02310</id>
        <link href="http://arxiv.org/abs/2106.02310"/>
        <updated>2021-06-07T03:06:15.169Z</updated>
        <summary type="html"><![CDATA[Client contribution evaluation, also known as data valuation, is a crucial
approach in federated learning(FL) for client selection and incentive
allocation. However, due to restrictions of accessibility of raw data, only
limited information such as local weights and local data size of each client is
open for quantifying the client contribution. Using data size from available
information, we introduce an empirical evaluation method called Federated
Client Contribution Evaluation through Accuracy Approximation(FedCCEA). This
method builds the Accuracy Approximation Model(AAM), which estimates a
simulated test accuracy using inputs of sampled data size and extracts the
clients' data quality and data size to measure client contribution. FedCCEA
strengthens some advantages: (1) enablement of data size selection to the
clients, (2) feasible evaluation time regardless of the number of clients, and
(3) precise estimation in non-IID settings. We demonstrate the superiority of
FedCCEA compared to previous methods through several experiments: client
contribution distribution, client removal, and robustness test to partial
participation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shyn_S/0/1/0/all/0/1"&gt;Sung Kuk Shyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Donghee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwangsu Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Surrogate Models for Absorptivity and Emissivity Spectra of Multiple Elements. (arXiv:2106.02528v1 [physics.plasm-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02528</id>
        <link href="http://arxiv.org/abs/2106.02528"/>
        <updated>2021-06-07T03:06:15.163Z</updated>
        <summary type="html"><![CDATA[Simulations of high energy density physics are expensive in terms of
computational resources. In particular, the computation of opacities of
plasmas, which are needed to accurately compute radiation transport in the
non-local thermal equilibrium (NLTE) regime, are expensive to the point of
easily requiring multiple times the sum-total compute time of all other
components of the simulation. As such, there is great interest in finding ways
to accelerate NLTE computations. Previous work has demonstrated that a
combination of fully-connected autoencoders and a deep jointly-informed neural
network (DJINN) can successfully replace the standard NLTE calculations for the
opacity of krypton. This work expands this idea to multiple elements in
demonstrating that individual surrogate models can be also be generated for
other elements with the focus being on creating autoencoders that can
accurately encode and decode the absorptivity and emissivity spectra.
Furthermore, this work shows that multiple elements across a large range of
atomic numbers can be combined into a single autoencoder when using a
convolutional autoencoder while maintaining accuracy that is comparable to
individual fully-connected autoencoders. Lastly, it is demonstrated that DJINN
can effectively learn the latent space of a convolutional autoencoder that can
encode multiple elements allowing the combination to effectively function as a
surrogate model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wal_M/0/1/0/all/0/1"&gt;Michael D. Vander Wal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+McClarren_R/0/1/0/all/0/1"&gt;Ryan G. McClarren&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Humbird_K/0/1/0/all/0/1"&gt;Kelli D. Humbird&lt;/a&gt; (2) ((1) University of Notre Dame, (2) Lawrence Livermore National Laboratory)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the robustness of NMF algorithms. (arXiv:2106.02213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02213</id>
        <link href="http://arxiv.org/abs/2106.02213"/>
        <updated>2021-06-07T03:06:15.157Z</updated>
        <summary type="html"><![CDATA[We examine three non-negative matrix factorization techniques; L2-norm,
L1-norm, and L2,1-norm. Our aim is to establish the performance of these
different approaches, and their robustness in real-world applications such as
feature selection while managing computational complexity, sensitivity to noise
and more. We thoroughly examine each approach from a theoretical perspective,
and examine the performance of each using a series of experiments drawing on
both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors
(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria
under a range of simulated noise scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Local Model-Agnostic Explanations Using Ground Truth. (arXiv:2106.02488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02488</id>
        <link href="http://arxiv.org/abs/2106.02488"/>
        <updated>2021-06-07T03:06:15.152Z</updated>
        <summary type="html"><![CDATA[Explanation techniques are commonly evaluated using human-grounded methods,
limiting the possibilities for large-scale evaluations and rapid progress in
the development of new techniques. We propose a functionally-grounded
evaluation procedure for local model-agnostic explanation techniques. In our
approach, we generate ground truth for explanations when the black-box model is
Logistic Regression and Gaussian Naive Bayes and compare how similar each
explanation is to the extracted ground truth. In our empirical study,
explanations of Local Interpretable Model-agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Local Permutation Importance (LPI) are
compared in terms of how similar they are to the extracted ground truth. In the
case of Logistic Regression, we find that the performance of the explanation
techniques is highly dependent on the normalization of the data. In contrast,
Local Permutation Importance outperforms the other techniques on Naive Bayes,
irrespective of normalization. We hope that this work lays the foundation for
further research into functionally-grounded evaluation methods for explanation
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahnama_A/0/1/0/all/0/1"&gt;Amir Hossein Akhavan Rahnama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Butepage_J/0/1/0/all/0/1"&gt;Judith Butepage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geurts_P/0/1/0/all/0/1"&gt;Pierre Geurts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bostrom_H/0/1/0/all/0/1"&gt;Henrik Bostrom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:15.145Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Classification with Multi-step Machine Learning. (arXiv:2106.02301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02301</id>
        <link href="http://arxiv.org/abs/2106.02301"/>
        <updated>2021-06-07T03:06:15.138Z</updated>
        <summary type="html"><![CDATA[The usefulness and value of Multi-step Machine Learning (ML), where a task is
organized into connected sub-tasks with known intermediate inference goals, as
opposed to a single large model learned end-to-end without intermediate
sub-tasks, is presented. Pre-optimized ML models are connected and better
performance is obtained by re-optimizing the connected one. The selection of an
ML model from several small ML model candidates for each sub-task has been
performed by using the idea based on Neural Architecture Search (NAS). In this
paper, Differentiable Architecture Search (DARTS) and Single Path One-Shot NAS
(SPOS-NAS) are tested, where the construction of loss functions is improved to
keep all ML models smoothly learning. Using DARTS and SPOS-NAS as an
optimization and selection as well as the connections for multi-step machine
learning systems, we find that (1) such a system can quickly and successfully
select highly performant model combinations, and (2) the selected models are
consistent with baseline algorithms, such as grid search, and their outputs are
well controlled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_M/0/1/0/all/0/1"&gt;Masahiko Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kishimoto_T/0/1/0/all/0/1"&gt;Tomoe Kishimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaneta_Y/0/1/0/all/0/1"&gt;Yuya Kaneta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1"&gt;Taichi Itoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umeda_Y/0/1/0/all/0/1"&gt;Yoshiaki Umeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_J/0/1/0/all/0/1"&gt;Junichi Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iiyama_Y/0/1/0/all/0/1"&gt;Yutaro Iiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sawada_R/0/1/0/all/0/1"&gt;Ryu Sawada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terashi_K/0/1/0/all/0/1"&gt;Koji Terashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Equal Gender Representation in the Annotations of Toxic Language Detection. (arXiv:2106.02183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02183</id>
        <link href="http://arxiv.org/abs/2106.02183"/>
        <updated>2021-06-07T03:06:15.120Z</updated>
        <summary type="html"><![CDATA[Classifiers tend to propagate biases present in the data on which they are
trained. Hence, it is important to understand how the demographic identities of
the annotators of comments affect the fairness of the resulting model. In this
paper, we focus on the differences in the ways men and women annotate comments
for toxicity, investigating how these differences result in models that amplify
the opinions of male annotators. We find that the BERT model as-sociates toxic
comments containing offensive words with male annotators, causing the model to
predict 67.7% of toxic comments as having been annotated by men. We show that
this disparity between gender predictions can be mitigated by removing
offensive words and highly toxic comments from the training data. We then apply
the learned associations between gender and language to toxic language
classifiers, finding that models trained exclusively on female-annotated data
perform 1.8% better than those trained solely on male-annotated data and that
training models on data after removing all offensive words reduces bias in the
model by 55.5% while increasing the sensitivity by 0.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Excell_E/0/1/0/all/0/1"&gt;Elizabeth Excell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. (arXiv:2106.02073v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02073</id>
        <link href="http://arxiv.org/abs/2106.02073"/>
        <updated>2021-06-07T03:06:15.112Z</updated>
        <summary type="html"><![CDATA[Recent work [Papyan, Han, and Donoho, 2020] discovered a phenomenon called
Neural Collapse (NC) that occurs pervasively in today's deep net training
paradigm of driving cross-entropy loss towards zero. In this phenomenon, the
last-layer features collapse to their class-means, both the classifiers and
class-means collapse to the same Simplex Equiangular Tight Frame (ETF), and the
behavior of the last-layer classifier converges to that of the
nearest-class-mean decision rule. Since then, follow-ups-such as Mixon et al.
[2020] and Poggio and Liao [2020a,b]-formally analyzed this inductive bias by
replacing the hard-to-study cross-entropy by the more tractable mean squared
error (MSE) loss. But, these works stopped short of demonstrating the empirical
reality of MSE-NC on benchmark datasets and canonical networks-as had been done
in Papyan, Han, and Donoho [2020] for the cross-entropy loss. In this work, we
establish the empirical reality of MSE-NC by reporting experimental
observations for three prototypical networks and five canonical datasets with
code for reproducing NC. Following this, we develop three main contributions
inspired by MSE-NC. Firstly, we show a new theoretical decomposition of the MSE
loss into (A) a term assuming the last-layer classifier is exactly the
least-squares or Webb and Lowe [1990] classifier and (B) a term capturing the
deviation from this least-squares classifier. Secondly, we exhibit experiments
on canonical datasets and networks demonstrating that, during training,
term-(B) is negligible. This motivates a new theoretical construct: the central
path, where the linear classifier stays MSE-optimal-for the given feature
activations-throughout the dynamics. Finally, through our study of continually
renormalized gradient flow along the central path, we produce closed-form
dynamics that predict full Neural Collapse in an unconstrained features model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;X.Y. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papyan_V/0/1/0/all/0/1"&gt;Vardan Papyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donoho_D/0/1/0/all/0/1"&gt;David L. Donoho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can convolutional ResNets approximately preserve input distances? A frequency analysis perspective. (arXiv:2106.02469v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02469</id>
        <link href="http://arxiv.org/abs/2106.02469"/>
        <updated>2021-06-07T03:06:15.106Z</updated>
        <summary type="html"><![CDATA[ResNets constrained to be bi-Lipschitz, that is, approximately distance
preserving, have been a crucial component of recently proposed techniques for
deterministic uncertainty quantification in neural models. We show that
theoretical justifications for recent regularisation schemes trying to enforce
such a constraint suffer from a crucial flaw -- the theoretical link between
the regularisation scheme used and bi-Lipschitzness is only valid under
conditions which do not hold in practice, rendering existing theory of limited
use, despite the strong empirical performance of these models. We provide a
theoretical explanation for the effectiveness of these regularisation schemes
using a frequency analysis perspective, showing that under mild conditions
these schemes will enforce a lower Lipschitz bound on the low-frequency
projection of images. We then provide empirical evidence supporting our
theoretical claims, and perform further experiments which demonstrate that our
broader conclusions appear to hold when some of the mathematical assumptions of
our proof are relaxed, corresponding to the setup used in prior work. In
addition, we present a simple constructive algorithm to search for counter
examples to the distance preservation condition, and discuss possible
implications of our theory for future model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Lewis Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Transportation Systems to Mitigate Road Traffic Congestion. (arXiv:2106.02315v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02315</id>
        <link href="http://arxiv.org/abs/2106.02315"/>
        <updated>2021-06-07T03:06:15.100Z</updated>
        <summary type="html"><![CDATA[Intelligent transport systems have efficiently and effectively proved
themselves in settling up the problem of traffic congestion around the world.
The multi-agent based transportation system is one of the most important
intelligent transport systems, which represents an interaction among the
neighbouring vehicles, drivers, roads, infrastructure and vehicles. In this
paper, two traffic management models have been created to mitigate congestion
and to ensure that emergency vehicles arrive as quickly as possible. A
tool-chain SUMO-JADE is employed to create a microscopic simulation symbolizing
the interactions of traffic. The simulation model has showed a significant
reduction of at least 50% in the average time delay and thus a real improvement
in the entire journey time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamadeh_N/0/1/0/all/0/1"&gt;Nizar Hamadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karouni_A/0/1/0/all/0/1"&gt;Ali Karouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Farhat_Z/0/1/0/all/0/1"&gt;Zeinab Farhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghor_H/0/1/0/all/0/1"&gt;Hussein El Ghor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghor_M/0/1/0/all/0/1"&gt;Mohamad El Ghor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katea_I/0/1/0/all/0/1"&gt;Israa Katea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Visual Exploration of Relations in Data Sets. (arXiv:1905.02515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.02515</id>
        <link href="http://arxiv.org/abs/1905.02515"/>
        <updated>2021-06-07T03:06:15.084Z</updated>
        <summary type="html"><![CDATA[Efficient explorative data analysis systems must take into account both what
a user knows and wants to know. This paper proposes a principled framework for
interactive visual exploration of relations in data, through views most
informative given the user's current knowledge and objectives. The user can
input pre-existing knowledge of relations in the data and also formulate
specific exploration interests, which are then taken into account in the
exploration. The idea is to steer the exploration process towards the interests
of the user, instead of showing uninteresting or already known relations. The
user's knowledge is modelled by a distribution over data sets parametrised by
subsets of rows and columns of data, called tile constraints. We provide a
computationally efficient implementation of this concept based on constrained
randomisation. Furthermore, we describe a novel dimensionality reduction method
for finding the views most informative to the user, which at the limit of no
background knowledge and with generic objectives reduces to PCA. We show that
the method is suitable for interactive use and is robust to noise, outperforms
standard projection pursuit visualisation methods, and gives understandable and
useful results in analysis of real-world data. We provide an open-source
implementation of the framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Puolamaki_K/0/1/0/all/0/1"&gt;Kai Puolam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oikarinen_E/0/1/0/all/0/1"&gt;Emilia Oikarinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henelius_A/0/1/0/all/0/1"&gt;Andreas Henelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable Regularization of Probabilistic Circuits. (arXiv:2106.02264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02264</id>
        <link href="http://arxiv.org/abs/2106.02264"/>
        <updated>2021-06-07T03:06:15.078Z</updated>
        <summary type="html"><![CDATA[Probabilistic Circuits (PCs) are a promising avenue for probabilistic
modeling. They combine advantages of probabilistic graphical models (PGMs) with
those of neural networks (NNs). Crucially, however, they are tractable
probabilistic models, supporting efficient and exact computation of many
probabilistic inference queries, such as marginals and MAP. Further, since PCs
are structured computation graphs, they can take advantage of
deep-learning-style parameter updates, which greatly improves their
scalability. However, this innovation also makes PCs prone to overfitting,
which has been observed in many standard benchmarks. Despite the existence of
abundant regularization techniques for both PGMs and NNs, they are not
effective enough when applied to PCs. Instead, we re-think regularization for
PCs and propose two intuitive techniques, data softening and entropy
regularization, that both take advantage of PCs' tractability and still have an
efficient implementation as a computation graph. Specifically, data softening
provides a principled way to add uncertainty in datasets in closed form, which
implicitly regularizes PC parameters. To learn parameters from a softened
dataset, PCs only need linear time by virtue of their tractability. In entropy
regularization, the exact entropy of the distribution encoded by a PC can be
regularized directly, which is again infeasible for most other density
estimation models. We show that both methods consistently improve the
generalization performance of a wide variety of PCs. Moreover, when paired with
a simple PC structure, we achieved state-of-the-art results on 10 out of 20
standard discrete density estimation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation for LiDAR Perception. (arXiv:2106.02377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02377</id>
        <link href="http://arxiv.org/abs/2106.02377"/>
        <updated>2021-06-07T03:06:15.072Z</updated>
        <summary type="html"><![CDATA[Scalable systems for automated driving have to reliably cope with an
open-world setting. This means, the perception systems are exposed to drastic
domain shifts, like changes in weather conditions, time-dependent aspects, or
geographic regions. Covering all domains with annotated data is impossible
because of the endless variations of domains and the time-consuming and
expensive annotation process. Furthermore, fast development cycles of the
system additionally introduce hardware changes, such as sensor types and
vehicle setups, and the required knowledge transfer from simulation. To enable
scalable automated driving, it is therefore crucial to address these domain
shifts in a robust and efficient manner. Over the last years, a vast amount of
different domain adaptation techniques evolved. There already exists a number
of survey papers for domain adaptation on camera images, however, a survey for
LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated
driving that provides detailed 3D scans of the vehicle's surroundings. To
stimulate future research, this paper presents a comprehensive review of recent
progress in domain adaptation methods and formulates interesting research
questions specifically targeted towards LiDAR perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1"&gt;Larissa T. Triess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1"&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1"&gt;Christoph B. Rist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1"&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Deep Networks with Semantic Data Augmentation. (arXiv:2007.10538v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10538</id>
        <link href="http://arxiv.org/abs/2007.10538"/>
        <updated>2021-06-07T03:06:15.065Z</updated>
        <summary type="html"><![CDATA[Data augmentation is widely known as a simple yet surprisingly effective
technique for regularizing deep networks. Conventional data augmentation
schemes, e.g., flipping, translation or rotation, are low-level,
data-independent and class-agnostic operations, leading to limited diversity
for augmented samples. To this end, we propose a novel semantic data
augmentation algorithm to complement traditional approaches. The proposed
method is inspired by the intriguing property that deep networks are effective
in learning linearized features, i.e., certain directions in the deep feature
space correspond to meaningful semantic transformations, e.g., changing the
background or view angle of an object. Based on this observation, translating
training samples along many such directions in the feature space can
effectively augment the dataset for more diversity. To implement this idea, we
first introduce a sampling based method to obtain semantically meaningful
directions efficiently. Then, an upper bound of the expected cross-entropy (CE)
loss on the augmented training set is derived by assuming the number of
augmented samples goes to infinity, yielding a highly efficient algorithm. In
fact, we show that the proposed implicit semantic data augmentation (ISDA)
algorithm amounts to minimizing a novel robust CE loss, which adds minimal
extra computational cost to a normal training procedure. In addition to
supervised learning, ISDA can be applied to semi-supervised learning tasks
under the consistency regularization framework, where ISDA amounts to
minimizing the upper bound of the expected KL-divergence between the augmented
features and the original features. Although being simple, ISDA consistently
improves the generalization performance of popular deep models (e.g., ResNets
and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,
ImageNet, and Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yitong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privately Learning Mixtures of Axis-Aligned Gaussians. (arXiv:2106.02162v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02162</id>
        <link href="http://arxiv.org/abs/2106.02162"/>
        <updated>2021-06-07T03:06:15.059Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning mixtures of Gaussians under the
constraint of approximate differential privacy. We prove that
$\widetilde{O}(k^2 d \log^{3/2}(1/\delta) / \alpha^2 \varepsilon)$ samples are
sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\mathbb{R}^d$
to within total variation distance $\alpha$ while satisfying $(\varepsilon,
\delta)$-differential privacy. This is the first result for privately learning
mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If
the covariance matrices of each of the Gaussians is the identity matrix, we
show that $\widetilde{O}(kd/\alpha^2 + kd \log(1/\delta) / \alpha \varepsilon)$
samples are sufficient.

Recently, the "local covering" technique of Bun, Kamath, Steinke, and Wu has
been successfully used for privately learning high-dimensional Gaussians with a
known covariance matrix and extended to privately learning general
high-dimensional Gaussians by Aden-Ali, Ashtiani, and Kamath. Given these
positive results, this approach has been proposed as a promising direction for
privately learning mixtures of Gaussians. Unfortunately, we show that this is
not possible.

We design a new technique for privately learning mixture distributions. A
class of distributions $\mathcal{F}$ is said to be list-decodable if there is
an algorithm that, given "heavily corrupted" samples from $f\in \mathcal{F}$,
outputs a list of distributions, $\widehat{\mathcal{F}}$, such that one of the
distributions in $\widehat{\mathcal{F}}$ approximates $f$. We show that if
$\mathcal{F}$ is privately list-decodable, then we can privately learn mixtures
of distributions in $\mathcal{F}$. Finally, we show axis-aligned Gaussian
distributions are privately list-decodable, thereby proving mixtures of such
distributions are privately learnable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aden_Ali_I/0/1/0/all/0/1"&gt;Ishaq Aden-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashtiani_H/0/1/0/all/0/1"&gt;Hassan Ashtiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liaw_C/0/1/0/all/0/1"&gt;Christopher Liaw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02225</id>
        <link href="http://arxiv.org/abs/2106.02225"/>
        <updated>2021-06-07T03:06:15.053Z</updated>
        <summary type="html"><![CDATA[The adoption of machine learning in materials science has rapidly transformed
materials property prediction. Hurdles limiting full capitalization of recent
advancements in machine learning include the limited development of methods to
learn the underlying interactions of multiple elements, as well as the
relationships among multiple properties, to facilitate property prediction in
new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that
seamlessly integrates (i) prediction using only a material's composition, (ii)
learning and exploitation of correlations among target properties in
multi-target regression, and (iii) leveraging training data from tangential
domains via generative transfer learning. The model is demonstrated for
prediction of spectral optical absorption of complex metal oxides spanning 69
3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear
composition-property relationships in composition spaces for which no training
data is available, which broadens the purview of machine learning to the
discovery of materials with exceptional properties. This achievement results
from the principled integration of latent embedding learning, property
correlation learning, generative transfer learning, and attention models. The
best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))
wherein a generative adversarial network is trained on computational density of
states data and deployed in the target domain to augment prediction of optical
absorption from composition. H-CLMP(T) aggregates multiple knowledge sources
with a framework that is well-suited for multi-target regression across the
physical sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1"&gt;Shufeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1"&gt;Dan Guevarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1"&gt;John M. Gregoire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally coherent video anonymization through GAN inpainting. (arXiv:2106.02328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02328</id>
        <link href="http://arxiv.org/abs/2106.02328"/>
        <updated>2021-06-07T03:06:15.045Z</updated>
        <summary type="html"><![CDATA[This work tackles the problem of temporally coherent face anonymization in
natural video streams.We propose JaGAN, a two-stage system starting with
detecting and masking out faces with black image patches in all individual
frames of the video. The second stage leverages a privacy-preserving Video
Generative Adversarial Network designed to inpaint the missing image patches
with artificially generated faces. Our initial experiments reveal that image
based generative models are not capable of inpainting patches showing temporal
coherent appearance across neighboring video frames. To address this issue we
introduce a newly curated video collection, which is made publicly available
for the research community along with this paper. We also introduce the
Identity Invariance Score IdI as a means to quantify temporal coherency between
neighboring frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_T/0/1/0/all/0/1"&gt;Thangapavithraa Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blies_P/0/1/0/all/0/1"&gt;Patrick Blies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_G/0/1/0/all/0/1"&gt;Georg G&amp;#xf6;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsch_R/0/1/0/all/0/1"&gt;Raphael Mitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasserer_M/0/1/0/all/0/1"&gt;Marcel Wasserer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entity Concept-enhanced Few-shot Relation Extraction. (arXiv:2106.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02401</id>
        <link href="http://arxiv.org/abs/2106.02401"/>
        <updated>2021-06-07T03:06:15.039Z</updated>
        <summary type="html"><![CDATA[Few-shot relation extraction (FSRE) is of great importance in long-tail
distribution problem, especially in special domain with low-resource data. Most
existing FSRE algorithms fail to accurately classify the relations merely based
on the information of the sentences together with the recognized entity pairs,
due to limited samples and lack of knowledge. To address this problem, in this
paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction
scheme (ConceptFERE), which introduces the inherent concepts of entities to
provide clues for relation prediction and boost the relations classification
performance. Firstly, a concept-sentence attention module is developed to
select the most appropriate concept from multiple concepts of each entity by
calculating the semantic similarity between sentences and concepts. Secondly, a
self-attention based fusion module is presented to bridge the gap of concept
embedding and sentence embedding from different semantic spaces. Extensive
experiments on the FSRE benchmark dataset FewRel have demonstrated the
effectiveness and the superiority of the proposed ConceptFERE scheme as
compared to the state-of-the-art baselines. Code is available at
https://github.com/LittleGuoKe/ConceptFERE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qinghua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Graph Learning for Link Prediction. (arXiv:2106.02172v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02172</id>
        <link href="http://arxiv.org/abs/2106.02172"/>
        <updated>2021-06-07T03:06:15.009Z</updated>
        <summary type="html"><![CDATA[Learning to predict missing links is important for many graph-based
applications. Existing methods were designed to learn the observed association
between two sets of variables: (1) the observed graph structure and (2) the
existence of link between a pair of nodes. However, the causal relationship
between these variables was ignored and we visit the possibility of learning it
by simply asking a counterfactual question: "would the link exist or not if the
observed graph structure became different?" To answer this question by causal
inference, we consider the information of the node pair as context, global
graph structural properties as treatment, and link existence as outcome. In
this work, we propose a novel link prediction method that enhances graph
learning by the counterfactual inference. It creates counterfactual links from
the observed ones, and our method learns representations from both of them.
Experiments on a number of benchmark datasets show that our proposed method
achieves the state-of-the-art performance on link prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenhao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proving Equivalence Between Complex Expressions Using Graph-to-Sequence Neural Models. (arXiv:2106.02452v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2106.02452</id>
        <link href="http://arxiv.org/abs/2106.02452"/>
        <updated>2021-06-07T03:06:14.983Z</updated>
        <summary type="html"><![CDATA[We target the problem of provably computing the equivalence between two
complex expression trees. To this end, we formalize the problem of equivalence
between two such programs as finding a set of semantics-preserving rewrite
rules from one into the other, such that after the rewrite the two programs are
structurally identical, and therefore trivially equivalent.We then develop a
graph-to-sequence neural network system for program equivalence, trained to
produce such rewrite sequences from a carefully crafted automatic example
generation algorithm. We extensively evaluate our system on a rich multi-type
linear algebra expression language, using arbitrary combinations of 100+
graph-rewriting axioms of equivalence. Our machine learning system guarantees
correctness for all true negatives, and ensures 0 false positive by design. It
outputs via inference a valid proof of equivalence for 93% of the 10,000
equivalent expression pairs isolated for testing, using up to 50-term
expressions. In all cases, the validity of the sequence produced and therefore
the provable assertion of program equivalence is always computable, in
negligible time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barollet_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Barollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic gradient descent with noise of machine learning type. Part II: Continuous time analysis. (arXiv:2106.02588v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02588</id>
        <link href="http://arxiv.org/abs/2106.02588"/>
        <updated>2021-06-07T03:06:14.972Z</updated>
        <summary type="html"><![CDATA[The representation of functions by artificial neural networks depends on a
large number of parameters in a non-linear fashion. Suitable parameters of
these are found by minimizing a 'loss functional', typically by stochastic
gradient descent (SGD) or an advanced SGD-based algorithm.

In a continuous time model for SGD with noise that follows the 'machine
learning scaling', we show that in a certain noise regime, the optimization
algorithm prefers 'flat' minima of the objective function in a sense which is
different from the flat minimum selection of continuous time SGD with
homogeneous noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Signed Cumulative Distribution Transform for 1-D Signal Analysis and Classification. (arXiv:2106.02146v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02146</id>
        <link href="http://arxiv.org/abs/2106.02146"/>
        <updated>2021-06-07T03:06:14.964Z</updated>
        <summary type="html"><![CDATA[This paper presents a new mathematical signal transform that is especially
suitable for decoding information related to non-rigid signal displacements. We
provide a measure theoretic framework to extend the existing Cumulative
Distribution Transform [ACHA 45 (2018), no. 3, 616-641] to arbitrary (signed)
signals on $\overline{\mathbb{R}}$. We present both forward (analysis) and
inverse (synthesis) formulas for the transform, and describe several of its
properties including translation, scaling, convexity, linear separability and
others. Finally, we describe a metric in transform space, and demonstrate the
application of the transform in classifying (detecting) signals under random
displacements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldroubi_A/0/1/0/all/0/1"&gt;Akram Aldroubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Rocio Diaz Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medri_I/0/1/0/all/0/1"&gt;Ivan Medri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1"&gt;Gustavo K. Rohde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thareja_S/0/1/0/all/0/1"&gt;Sumati Thareja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fuzzy Clustering with Similarity Queries. (arXiv:2106.02212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02212</id>
        <link href="http://arxiv.org/abs/2106.02212"/>
        <updated>2021-06-07T03:06:14.945Z</updated>
        <summary type="html"><![CDATA[The fuzzy or soft $k$-means objective is a popular generalization of the
well-known $k$-means problem, extending the clustering capability of the
$k$-means to datasets that are uncertain, vague, and otherwise hard to cluster.
In this paper, we propose a semi-supervised active clustering framework, where
the learner is allowed to interact with an oracle (domain expert), asking for
the similarity between a certain set of chosen items. We study the query and
computational complexities of clustering in this framework. We prove that
having a few of such similarity queries enables one to get a polynomial-time
approximation algorithm to an otherwise conjecturally NP-hard problem. In
particular, we provide probabilistic algorithms for fuzzy clustering in this
setting that asks $O(\mathsf{poly}(k)\log n)$ similarity queries and run with
polynomial-time-complexity, where $n$ is the number of items. The fuzzy
$k$-means objective is nonconvex, with $k$-means as a special case, and is
equivalent to some other generic nonconvex problem such as non-negative matrix
factorization. The ubiquitous Lloyd-type algorithms (or,
expectation-maximization algorithm) can get stuck at a local minima. Our
results show that by making few similarity queries, the problem becomes easier
to solve. Finally, we test our algorithms over real-world datasets, showing
their effectiveness in real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huleihel_W/0/1/0/all/0/1"&gt;Wasim Huleihel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumyabrata Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Resource Reservation for Crowdsourced Live Video Streaming Applications in Geo-Distributed Cloud Environment. (arXiv:2106.02420v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02420</id>
        <link href="http://arxiv.org/abs/2106.02420"/>
        <updated>2021-06-07T03:06:14.933Z</updated>
        <summary type="html"><![CDATA[Crowdsourced live video streaming (livecast) services such as Facebook Live,
YouNow, Douyu and Twitch are gaining more momentum recently. Allocating the
limited resources in a cost-effective manner while maximizing the Quality of
Service (QoS) through real-time delivery and the provision of the appropriate
representations for all viewers is a challenging problem. In our paper, we
introduce a machine-learning based predictive resource allocation framework for
geo-distributed cloud sites, considering the delay and quality constraints to
guarantee the maximum QoS for viewers and the minimum cost for content
providers. First, we present an offline optimization that decides the required
transcoding resources in distributed regions near the viewers with a trade-off
between the QoS and the overall cost. Second, we use machine learning to build
forecasting models that proactively predict the approximate transcoding
resources to be reserved at each cloud site ahead of time. Finally, we develop
a Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource
allocation of real-time broadcasted videos on the rented resources. Extensive
simulations have shown that GNCA outperforms the state-of-the art resource
allocation approaches for crowdsourced live streaming by achieving more than
20% gain in terms of system cost while serving the viewers with relatively
lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1"&gt;Fatima Haouari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_K/0/1/0/all/0/1"&gt;Kashif Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Target Networks: Improving Deep $Q$-learning with Functional Regularization. (arXiv:2106.02613v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02613</id>
        <link href="http://arxiv.org/abs/2106.02613"/>
        <updated>2021-06-07T03:06:14.915Z</updated>
        <summary type="html"><![CDATA[Target networks are at the core of recent success in Reinforcement Learning.
They stabilize the training by using old parameters to estimate the $Q$-values,
but this also limits the propagation of newly-encountered rewards which could
ultimately slow down the training. In this work, we propose an alternative
training method based on functional regularization which does not have this
deficiency. Unlike target networks, our method uses up-to-date parameters to
estimate the target $Q$-values, thereby speeding up training while maintaining
stability. Surprisingly, in some cases, we can show that target networks are a
special, restricted type of functional regularizers. Using this approach, we
show empirical improvements in sample efficiency and performance across a range
of Atari and simulated robotics environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1"&gt;Alexandre Pich&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marconi_G/0/1/0/all/0/1"&gt;Gian Maria Marconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02190</id>
        <link href="http://arxiv.org/abs/2106.02190"/>
        <updated>2021-06-07T03:06:14.892Z</updated>
        <summary type="html"><![CDATA[We developed Distilled Graph Attention Policy Networks (DGAPNs), a
curiosity-driven reinforcement learning model to generate novel
graph-structured chemical representations that optimize user-defined objectives
by efficiently navigating a physically constrained domain. The framework is
examined on the task of generating molecules that are designed to bind,
noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial
Graph Attention Network (sGAT) that leverages self-attention over both node and
edge attributes as well as encoding spatial structure -- this capability is of
considerable interest in areas such as molecular and synthetic biology and drug
discovery. An attentional policy network is then introduced to learn decision
rules for a dynamic, fragment-based chemical environment, and state-of-the-art
policy gradient techniques are employed to train the network with enhanced
stability. Exploration is efficiently encouraged by incorporating innovation
reward bonuses learned and proposed by random network distillation. In
experiments, our framework achieved outstanding results compared to
state-of-the-art algorithms, while increasing the diversity of proposed
molecules and reducing the complexity of paths to chemical synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choma_N/0/1/0/all/0/1"&gt;Nicholas Choma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Andrew Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_M/0/1/0/all/0/1"&gt;Mikaela Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_E/0/1/0/all/0/1"&gt;&amp;#xc9;rica T. Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Manesh Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergara_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica G. Melesse Vergara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1"&gt;Thomas S. Brettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1"&gt;Wibe A. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Head_M/0/1/0/all/0/1"&gt;Martha S. Head&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugent_P/0/1/0/all/0/1"&gt;Peter Nugent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_D/0/1/0/all/0/1"&gt;Daniel A. Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of General-Purpose Embeddings for Code Changes. (arXiv:2106.02087v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.02087</id>
        <link href="http://arxiv.org/abs/2106.02087"/>
        <updated>2021-06-07T03:06:14.886Z</updated>
        <summary type="html"><![CDATA[A lot of problems in the field of software engineering - bug fixing, commit
message generation, etc. - require analyzing not only the code itself but
specifically code changes. Applying machine learning models to these tasks
requires us to create numerical representations of the changes, i.e.
embeddings. Recent studies demonstrate that the best way to obtain these
embeddings is to pre-train a deep neural network in an unsupervised manner on a
large volume of unlabeled data and then further fine-tune it for a specific
task.

In this work, we propose an approach for obtaining such embeddings of code
changes during pre-training and evaluate them on two different downstream tasks
- applying changes to code and commit message generation. The pre-training
consists of the model learning to apply the given change (an edit sequence) to
the code in a correct way, and therefore requires only the code change itself.
To increase the quality of the obtained embeddings, we only consider the
changed tokens in the edit sequence. In the task of applying code changes, our
model outperforms the model that uses full edit sequences by 5.9 percentage
points in accuracy. As for the commit message generation, our model
demonstrated the same results as supervised models trained for this specific
task, which indicates that it can encode code changes well and can be improved
in the future by pre-training on a larger dataset of easily gathered code
changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pravilov_M/0/1/0/all/0/1"&gt;Mikhail Pravilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1"&gt;Egor Bogomolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golubev_Y/0/1/0/all/0/1"&gt;Yaroslav Golubev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1"&gt;Timofey Bryksin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucination In Object Detection -- A Study In Visual Part Verification. (arXiv:2106.02523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02523</id>
        <link href="http://arxiv.org/abs/2106.02523"/>
        <updated>2021-06-07T03:06:14.880Z</updated>
        <summary type="html"><![CDATA[We show that object detectors can hallucinate and detect missing objects;
potentially even accurately localized at their expected, but non-existing,
position. This is particularly problematic for applications that rely on visual
part verification: detecting if an object part is present or absent. We show
how popular object detectors hallucinate objects in a visual part verification
task and introduce the first visual part verification dataset: DelftBikes,
which has 10,000 bike photographs, with 22 densely annotated parts per image,
where some parts may be missing. We explicitly annotated an extra object state
label for each part to reflect if a part is missing or intact. We propose to
evaluate visual part verification by relying on recall and compare popular
object detectors on DelftBikes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1"&gt;Osman Semih Kayhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vredebregt_B/0/1/0/all/0/1"&gt;Bart Vredebregt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying Reinforcement Learning Policies with $\mathcal{L}_1$ Adaptive Control. (arXiv:2106.02249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02249</id>
        <link href="http://arxiv.org/abs/2106.02249"/>
        <updated>2021-06-07T03:06:14.874Z</updated>
        <summary type="html"><![CDATA[A reinforcement learning (RL) policy trained in a nominal environment could
fail in a new/perturbed environment due to the existence of dynamic variations.
Existing robust methods try to obtain a fixed policy for all envisioned dynamic
variation scenarios through robust or adversarial training. These methods could
lead to conservative performance due to emphasis on the worst case, and often
involve tedious modifications to the training environment. We propose an
approach to robustifying a pre-trained non-robust RL policy with
$\mathcal{L}_1$ adaptive control. Leveraging the capability of an
$\mathcal{L}_1$ control law in the fast estimation of and active compensation
for dynamic variations, our approach can significantly improve the robustness
of an RL policy trained in a standard (i.e., non-robust) way, either in a
simulator or in the real world. Numerical experiments are provided to validate
the efficacy of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yikun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1"&gt;Manan Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1"&gt;Naira Hovakimyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-06-07T03:06:14.867Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via moral philosophy's definition of social good, propose a
framework to evaluate NLP tasks' direct and indirect real-world impact, and
adopt the methodology of global priorities research to identify priority causes
for NLP research. Finally, we use our theoretical framework to provide some
practical guidelines for future NLP research for social good. Our data and
codes are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Descent Optimization Pattern and Aliasing: Caveats of Noisy Labels. (arXiv:2106.02100v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02100</id>
        <link href="http://arxiv.org/abs/2106.02100"/>
        <updated>2021-06-07T03:06:14.849Z</updated>
        <summary type="html"><![CDATA[Optimization plays a key role in the training of deep neural networks.
Deciding when to stop training can have a substantial impact on the performance
of the network during inference. Under certain conditions, the generalization
error can display a double descent pattern during training: the learning curve
is non-monotonic and seemingly diverges before converging again after
additional epochs. This optimization pattern can lead to early stopping
procedures to stop training before the second convergence and consequently
select a suboptimal set of parameters for the network, with worse performance
during inference. In this work, in addition to confirming that double descent
occurs with small datasets and noisy labels as evidenced by others, we show
that noisy labels must be present both in the training and generalization sets
to observe a double descent pattern. We also show that the learning rate has an
influence on double descent, and study how different optimizers and optimizer
parameters influence the apparition of double descent. Finally, we show that
increasing the learning rate can create an aliasing effect that masks the
double descent pattern without suppressing it. We study this phenomenon through
extensive experiments on variants of CIFAR-10 and show that they translate to a
real world application: the forecast of seizure events in epileptic patients
from continuous electroencephalographic recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubost_F/0/1/0/all/0/1"&gt;Florian Dubost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1"&gt;Khaled Kamal Saab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_E/0/1/0/all/0/1"&gt;Erin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1"&gt;Daniel Yang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pike_M/0/1/0/all/0/1"&gt;Max Pike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Siddharth Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskhar_N/0/1/0/all/0/1"&gt;Nandita Bhaskhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Messer_C/0/1/0/all/0/1"&gt;Christopher Lee-Messer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms. (arXiv:2106.02126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02126</id>
        <link href="http://arxiv.org/abs/2106.02126"/>
        <updated>2021-06-07T03:06:14.842Z</updated>
        <summary type="html"><![CDATA[One of the key drivers of complexity in the classical (stochastic)
multi-armed bandit (MAB) problem is the difference between mean rewards in the
top two arms, also known as the instance gap. The celebrated Upper Confidence
Bound (UCB) policy is among the simplest optimism-based MAB algorithms that
naturally adapts to this gap: for a horizon of play n, it achieves optimal
O(log n) regret in instances with "large" gaps, and a near-optimal O(\sqrt{n
log n}) minimax regret when the gap can be arbitrarily "small." This paper
provides new results on the arm-sampling behavior of UCB, leading to several
important insights. Among these, it is shown that arm-sampling rates under UCB
are asymptotically deterministic, regardless of the problem complexity. This
discovery facilitates new sharp asymptotics and a novel alternative proof for
the O(\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also
provides the first complete process-level characterization of the MAB problem
under UCB in the conventional diffusion scaling. Among other things, the
"small" gap worst-case lens adopted in this paper also reveals profound
distinctions between the behavior of UCB and Thompson Sampling, such as an
"incomplete learning" phenomenon characteristic of the latter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalvit_A/0/1/0/all/0/1"&gt;Anand Kalvit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1"&gt;Assaf Zeevi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization. (arXiv:2106.02266v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02266</id>
        <link href="http://arxiv.org/abs/2106.02266"/>
        <updated>2021-06-07T03:06:14.835Z</updated>
        <summary type="html"><![CDATA[A major bottleneck in the real-world applications of machine learning models
is their failure in generalizing to unseen domains whose data distribution is
not i.i.d to the training domains. This failure often stems from learning
non-generalizable features in the training domains that are spuriously
correlated with the label of data. To address this shortcoming, there has been
a growing surge of interest in learning good explanations that are hard to
vary, which is studied under the notion of Out-of-Distribution (OOD)
Generalization. The search for good explanations that are \textit{invariant}
across different domains can be seen as finding local (global) minimas in the
loss landscape that hold true across all of the training domains. In this
paper, we propose a masking strategy, which determines a continuous weight
based on the agreement of gradients that flow in each edge of network, in order
to control the amount of update received by the edge in each step of
optimization. Particularly, our proposed technique referred to as "Smoothed-AND
(SAND)-masking", not only validates the agreement in the direction of gradients
but also promotes the agreement among their magnitudes to further ensure the
discovery of invariances across training domains. SAND-mask is validated over
the Domainbed benchmark for domain generalization and significantly improves
the state-of-the-art accuracy on the Colored MNIST dataset while providing
competitive results on other domain generalization datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1"&gt;Soroosh Shahtalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagnon_Audet_J/0/1/0/all/0/1"&gt;Jean-Christophe Gagnon-Audet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1"&gt;Touraj Laleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1"&gt;Mojtaba Faramarzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding and Fixing Spurious Patterns with Explanations. (arXiv:2106.02112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02112</id>
        <link href="http://arxiv.org/abs/2106.02112"/>
        <updated>2021-06-07T03:06:14.827Z</updated>
        <summary type="html"><![CDATA[Machine learning models often use spurious patterns such as "relying on the
presence of a person to detect a tennis racket," which do not generalize. In
this work, we present an end-to-end pipeline for identifying and mitigating
spurious patterns for image classifiers. We start by finding patterns such as
"the model's prediction for tennis racket changes 63% of the time if we hide
the people." Then, if a pattern is spurious, we mitigate it via a novel form of
data augmentation. We demonstrate that this approach identifies a diverse set
of spurious patterns and that it mitigates them by producing a model that is
both more accurate on a distribution where the spurious pattern is not helpful
and more robust to distribution shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1"&gt;Gregory Plumb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1"&gt;Marco Tulio Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits. (arXiv:2002.02518v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02518</id>
        <link href="http://arxiv.org/abs/2002.02518"/>
        <updated>2021-06-07T03:06:14.822Z</updated>
        <summary type="html"><![CDATA[Many of the recent triumphs in machine learning are dependent on well-tuned
hyperparameters. This is particularly prominent in reinforcement learning (RL)
where a small change in the configuration can lead to failure. Despite the
importance of tuning hyperparameters, it remains expensive and is often done in
a naive and laborious way. A recent solution to this problem is Population
Based Training (PBT) which updates both weights and hyperparameters in a single
training run of a population of agents. PBT has been shown to be particularly
effective in RL, leading to widespread use in the field. However, PBT lacks
theoretical guarantees since it relies on random heuristics to explore the
hyperparameter space. This inefficiency means it typically requires vast
computational resources, which is prohibitive for many small and medium sized
labs. In this work, we introduce the first provably efficient PBT-style
algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to
guide the search in an efficient way, making it possible to discover high
performing hyperparameter configurations with far fewer agents than typically
required by PBT. We show in a series of RL experiments that PB2 is able to
achieve high performance with a modest computational budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey. (arXiv:2106.02154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02154</id>
        <link href="http://arxiv.org/abs/2106.02154"/>
        <updated>2021-06-07T03:06:14.802Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper for nonlinear dimensionality and feature
extraction methods which are based on the Laplacian of graph of data. We first
introduce adjacency matrix, definition of Laplacian matrix, and the
interpretation of Laplacian. Then, we cover the cuts of graph and spectral
clustering which applies clustering in a subspace of data. Different
optimization variants of Laplacian eigenmap and its out-of-sample extension are
explained. Thereafter, we introduce the locality preserving projection and its
kernel variant as linear special cases of Laplacian eigenmap. Versions of graph
embedding are then explained which are generalized versions of Laplacian
eigenmap and locality preserving projection. Finally, diffusion map is
introduced which is a method based on Laplacian of data and random walks on the
data graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nara: Learning Network-Aware Resource Allocation Algorithms for Cloud Data Centres. (arXiv:2106.02412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02412</id>
        <link href="http://arxiv.org/abs/2106.02412"/>
        <updated>2021-06-07T03:06:14.795Z</updated>
        <summary type="html"><![CDATA[Data centres (DCs) underline many prominent future technological trends such
as distributed training of large scale machine learning models and
internet-of-things based platforms. DCs will soon account for over 3\% of
global energy demand, so efficient use of DC resources is essential. Robust DC
networks (DCNs) are essential to form the large scale systems needed to handle
this demand, but can bottleneck how efficiently DC-server resources can be used
when servers with insufficient connectivity between them cannot be jointly
allocated to a job. However, allocating servers' resources whilst accounting
for their inter-connectivity maps to an NP-hard combinatorial optimisation
problem, and so is often ignored in DC resource management schemes. We present
Nara, a framework based on reinforcement learning (RL) and graph neural
networks (GNN) to learn network-aware allocation policies that increase the
number of requests allocated over time compared to previous methods. Unique to
our solution is the use of a GNN to generate representations of server-nodes in
the DCN, which are then interpreted as actions by a RL policy-network which
chooses from which servers resources will be allocated to incoming requests.
Nara is agnostic to the topology size and shape and is trained end-to-end. The
method can accept up to 33\% more requests than the best baseline when deployed
on DCNs with up to the order of $10\times$ more compute nodes than the DCN seen
during training and is able to maintain its policy's performance on DCNs with
the order of $100\times$ more servers than seen during training. It also
generalises to unseen DCN topologies with varied network structure and unseen
request distributions without re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabka_Z/0/1/0/all/0/1"&gt;Zacharaya Shabka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zervas_G/0/1/0/all/0/1"&gt;Georgios Zervas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-06-07T03:06:14.788Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ambulatory blood pressure monitoring versus office blood pressure measurement: Are there sex differences?. (arXiv:2106.02392v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.02392</id>
        <link href="http://arxiv.org/abs/2106.02392"/>
        <updated>2021-06-07T03:06:14.782Z</updated>
        <summary type="html"><![CDATA[The accurate measurement of blood pressure (BP) is an important prerequisite
for the reliable diagnosis and efficient management of hypertension and other
medical conditions. Office Blood Pressure Measurement (OBP) is a technique
performed in-office with the sphygmomanometer, while Ambulatory Blood Pressure
Monitoring (ABPM) is a technique that measures blood pressure during 24h. The
BP fluctuations also depend on other factors such as physical activity,
temperature, mood, age, sex, any pathologies, a hormonal activity that may
intrinsically influence the differences between OBP and ABPM. The aim of this
study is to examine the possible influence of sex on the discrepancies between
OBP and ABPM in 872 subjects with known or suspected hypertension. A
significant correlation was observed between OBP and ABPM mean values
calculated during the day, night and 24h (ABPMday, ABPMnight, ABPM24h) in both
groups (p<0.0001). The main finding of this study is that no difference between
sexes was observed in the relation between OBP and mean ABMP values except
between systolic OBP and systolic ABPM during the night. In addition, this
study showed a moderate correlation between BPs obtained with the two
approaches with a great dispersion around the regression line which suggests
that the two approaches cannot be used interchangeably.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miladinovic_A/0/1/0/all/0/1"&gt;Aleksandar Miladinovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ajcevic_M/0/1/0/all/0/1"&gt;Milo&amp;#x161; Aj&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Siveri_G/0/1/0/all/0/1"&gt;Giulia Siveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liguori_L/0/1/0/all/0/1"&gt;Laura Liguori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pascazio_L/0/1/0/all/0/1"&gt;Lorenzo Pascazio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Accardo_A/0/1/0/all/0/1"&gt;Agostino Accardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching keyword spotters to spot new keywords with limited examples. (arXiv:2106.02443v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02443</id>
        <link href="http://arxiv.org/abs/2106.02443"/>
        <updated>2021-06-07T03:06:14.775Z</updated>
        <summary type="html"><![CDATA[Learning to recognize new keywords with just a few examples is essential for
personalizing keyword spotting (KWS) models to a user's choice of keywords.
However, modern KWS models are typically trained on large datasets and
restricted to a small vocabulary of keywords, limiting their transferability to
a broad range of unseen keywords. Towards easily customizable KWS models, we
present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained
on the task of recognizing a large number of keywords. Speech representations
offered by KeySEM are highly effective for learning new keywords from a limited
number of examples. Comparisons with a diverse range of related work across
several datasets show that our method achieves consistently superior
performance with fewer training examples. Although KeySEM was pre-trained only
on English utterances, the performance gains also extend to datasets from four
other languages indicating that KeySEM learns useful representations well
aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's
ability to learn new keywords sequentially without requiring to re-train on
previously learned keywords. Our experimental observations suggest that KeySEM
is well suited to on-device environments where post-deployment learning and
ease of customization are often desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kilgour_K/0/1/0/all/0/1"&gt;Kevin Kilgour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rom_H/0/1/0/all/0/1"&gt;Hassan Rom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Generalization in Kernel Regression. (arXiv:2106.02261v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02261</id>
        <link href="http://arxiv.org/abs/2106.02261"/>
        <updated>2021-06-07T03:06:14.755Z</updated>
        <summary type="html"><![CDATA[In real word applications, data generating process for training a machine
learning model often differs from what the model encounters in the test stage.
Understanding how and whether machine learning models generalize under such
distributional shifts have been a theoretical challenge. Here, we study
generalization in kernel regression when the training and test distributions
are different using methods from statistical physics. Using the replica method,
we derive an analytical formula for the out-of-distribution generalization
error applicable to any kernel and real datasets. We identify an overlap matrix
that quantifies the mismatch between distributions for a given kernel as a key
determinant of generalization performance under distribution shift. Using our
analytical expressions we elucidate various generalization phenomena including
possible improvement in generalization when there is a mismatch. We develop
procedures for optimizing training and test distributions for a given data
budget to find best and worst case generalizations under the shift. We present
applications of our theory to real and synthetic datasets and for many kernels.
We compare results of our theory applied to Neural Tangent Kernel with
simulations of wide networks and show agreement. We analyze linear regression
in further depth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Canatar_A/0/1/0/all/0/1"&gt;Abdulkadir Canatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1"&gt;Blake Bordelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals. (arXiv:2106.02118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02118</id>
        <link href="http://arxiv.org/abs/2106.02118"/>
        <updated>2021-06-07T03:06:14.749Z</updated>
        <summary type="html"><![CDATA[Importance: An artificial intelligence (AI)-based model to predict COVID-19
likelihood from chest x-ray (CXR) findings can serve as an important adjunct to
accelerate immediate clinical decision making and improve clinical decision
making. Despite significant efforts, many limitations and biases exist in
previously developed AI diagnostic models for COVID-19. Utilizing a large set
of local and international CXR images, we developed an AI model with high
performance on temporal and external validation.

Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,
but not replacement, for clinical decision support of COVID-19 diagnosis, which
largely hinges on exposure history, signs, and symptoms. While AI-based tools
have not yet reached full diagnostic potential in COVID-19, they may still
offer valuable information to clinicians taken into consideration along with
clinical signs and symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adila_D/0/1/0/all/0/1"&gt;Dyah Adila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zach Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ingraham_N/0/1/0/all/0/1"&gt;Nicholas Ingraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murray_E/0/1/0/all/0/1"&gt;Eric Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boley_D/0/1/0/all/0/1"&gt;Daniel Boley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Switzer_S/0/1/0/all/0/1"&gt;Sean Switzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allen_T/0/1/0/all/0/1"&gt;Tadashi Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steenburg_S/0/1/0/all/0/1"&gt;Scott D. Steenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kummerfeld_E/0/1/0/all/0/1"&gt;Erich Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher Tignanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation. (arXiv:2106.02267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02267</id>
        <link href="http://arxiv.org/abs/2106.02267"/>
        <updated>2021-06-07T03:06:14.742Z</updated>
        <summary type="html"><![CDATA[The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses
on the object and style like other artwork researches. Such study has benefited
from the renewed interest by the machine learning community in culturally
important topics, leading to interdisciplinary works including collections of
images, quantitative approaches, and machine learning-based creativities. They,
however, have several drawbacks, and it remains challenging to integrate these
works into a comprehensive view. To bridge this gap, we propose a holistic
approach We first present a large-scale Ukiyo-e dataset with coherent semantic
labels and geometric annotations, then show its value in a quantitative study
of Ukiyo-e paintings' object using these labels and annotations. We further
demonstrate the machine learning methods could help style study through soft
color decomposition of Ukiyo-e, and finally provides joint insights into object
and style by composing sketches and colors using colorization. Dataset
available at https://github.com/rois-codh/arc-ukiyoe-faces]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingtao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1"&gt;Chikahiko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Dense Multi-Cable Knots. (arXiv:2106.02252v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.02252</id>
        <link href="http://arxiv.org/abs/2106.02252"/>
        <updated>2021-06-07T03:06:14.737Z</updated>
        <summary type="html"><![CDATA[Disentangling two or more cables requires many steps to remove crossings
between and within cables. We formalize the problem of disentangling multiple
cables and present an algorithm, Iterative Reduction Of Non-planar Multiple
cAble kNots (IRON-MAN), that outputs robot actions to remove crossings from
multi-cable knotted structures. We instantiate this algorithm with a learned
perception system, inspired by prior work in single-cable untying that given an
image input, can disentangle two-cable twists, three-cable braids, and knots of
two or three cables, such as overhand, square, carrick bend, sheet bend, crown,
and fisherman's knots. IRON-MAN keeps track of task-relevant keypoints
corresponding to target cable endpoints and crossings and iteratively
disentangles the cables by identifying and undoing crossings that are critical
to knot structure. Using a da Vinci surgical robot, we experimentally evaluate
the effectiveness of IRON-MAN on untangling multi-cable knots of types that
appear in the training data, as well as generalizing to novel classes of
multi-cable knots. Results suggest that IRON-MAN is effective in disentangling
knots involving up to three cables with 80.5% success and generalizing to knot
types that are not present during training, with cables of both distinct or
identical colors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_V/0/1/0/all/0/1"&gt;Vainavi Viswanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grannen_J/0/1/0/all/0/1"&gt;Jennifer Grannen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1"&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novoseller_E/0/1/0/all/0/1"&gt;Ellen Novoseller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeffrey Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1"&gt;Michael Laskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series Forecasting with Regime Switching. (arXiv:2106.02329v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02329</id>
        <link href="http://arxiv.org/abs/2106.02329"/>
        <updated>2021-06-07T03:06:14.730Z</updated>
        <summary type="html"><![CDATA[We propose a deep switching state space model (DS$^3$M) for efficient
inference and forecasting of nonlinear time series with irregularly switching
among various regimes. The switching among regimes is captured by both discrete
and continuous latent variables with recurrent neural networks. The model is
estimated with variational inference using a reparameterization trick. We test
the approach on a variety of simulated and real datasets. In all cases, DS$^3$M
achieves competitive performance compared to several state-of-the-art methods
(e.g. GRU, SRNN, DSARF, SNLDS), with superior forecasting accuracy, convincing
interpretability of the discrete latent variables, and powerful representation
of the continuous latent variables for different kinds of time series.
Specifically, the MAPE values increase by 0.09\% to 15.71\% against the
second-best performing alternative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiuqin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions. (arXiv:2106.02436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02436</id>
        <link href="http://arxiv.org/abs/2106.02436"/>
        <updated>2021-06-07T03:06:14.724Z</updated>
        <summary type="html"><![CDATA[We study the stochastic Multi-Armed Bandit (MAB) problem with random delays
in the feedback received by the algorithm. We consider two settings: the
reward-dependent delay setting, where realized delays may depend on the
stochastic rewards, and the reward-independent delay setting. Our main
contribution is algorithms that achieve near-optimal regret in each of the
settings, with an additional additive dependence on the quantiles of the delay
distribution. Our results do not make any assumptions on the delay
distributions: in particular, we do not assume they come from any parametric
family of distributions and allow for unbounded support and expectation; we
further allow for infinite delays where the algorithm might occasionally not
observe any feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1"&gt;Tal Lancewicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1"&gt;Shahar Segal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02285</id>
        <link href="http://arxiv.org/abs/2106.02285"/>
        <updated>2021-06-07T03:06:14.684Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02229</id>
        <link href="http://arxiv.org/abs/2106.02229"/>
        <updated>2021-06-07T03:06:14.659Z</updated>
        <summary type="html"><![CDATA[We introduce RL-DARTS, one of the first applications of Differentiable
Architecture Search (DARTS) in reinforcement learning (RL) to search for
convolutional cells, applied to the Procgen benchmark. We outline the initial
difficulties of applying neural architecture search techniques in RL, and
demonstrate that by simply replacing the image encoder with a DARTS supernet,
our search method is sample-efficient, requires minimal extra compute
resources, and is also compatible with off-policy and on-policy RL algorithms,
needing only minor changes in preexisting code. Surprisingly, we find that the
supernet can be used as an actor for inference to generate replay data in
standard RL training loops, and thus train end-to-end. Throughout this training
process, we show that the supernet gradually learns better cells, leading to
alternative architectures which can be highly competitive against manually
designed policies, but also verify previous design choices for RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yingjie Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Summer Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1"&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Learning-based Optimal Market Bidding Strategy for Price-Maker Energy Storage. (arXiv:2106.02396v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02396</id>
        <link href="http://arxiv.org/abs/2106.02396"/>
        <updated>2021-06-07T03:06:14.610Z</updated>
        <summary type="html"><![CDATA[Load serving entities with storage units reach sizes and performances that
can significantly impact clearing prices in electricity markets. Nevertheless,
price endogeneity is rarely considered in storage bidding strategies and
modeling the electricity market is a challenging task. Meanwhile, model-free
reinforcement learning such as the Actor-Critic are becoming increasingly
popular for designing energy system controllers. Yet implementation frequently
requires lengthy, data-intense, and unsafe trial-and-error training. To fill
these gaps, we implement an online Supervised Actor-Critic (SAC) algorithm,
supervised with a model-based controller -- Model Predictive Control (MPC). The
energy storage agent is trained with this algorithm to optimally bid while
learning and adjusting to its impact on the market clearing prices. We compare
the supervised Actor-Critic algorithm with the MPC algorithm as a supervisor,
finding that the former reaps higher profits via learning. Our contribution,
thus, is an online and safe SAC algorithm that outperforms the current
model-based state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Badoual_M/0/1/0/all/0/1"&gt;Mathilde D. Badoual&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moura_S/0/1/0/all/0/1"&gt;Scott J. Moura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02466</id>
        <link href="http://arxiv.org/abs/2106.02466"/>
        <updated>2021-06-07T03:06:14.601Z</updated>
        <summary type="html"><![CDATA[The self-supervised learning (SSL) paradigm is an essential exploration area,
which tries to eliminate the need for expensive data labeling. Despite the
great success of SSL methods in computer vision and natural language
processing, most of them employ contrastive learning objectives that require
negative samples, which are hard to define. This becomes even more challenging
in the case of graphs and is a bottleneck for achieving robust representations.
To overcome such limitations, we propose a framework for self-supervised graph
representation learning -- Graph Barlow Twins, which utilizes a
cross-correlation-based loss function instead of negative samples. Moreover, it
does not rely on non-symmetric neural network architectures -- in contrast to
state-of-the-art self-supervised graph representation learning method BGRL. We
show that our method achieves as competitive results as BGRL, best
self-supervised methods, and fully supervised ones while requiring
substantially fewer hyperparameters and converging in an order of magnitude
training steps earlier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bielak_P/0/1/0/all/0/1"&gt;Piotr Bielak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1"&gt;Tomasz Kajdanowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1"&gt;Nitesh V. Chawla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning. (arXiv:2106.02584v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02584</id>
        <link href="http://arxiv.org/abs/2106.02584"/>
        <updated>2021-06-07T03:06:14.434Z</updated>
        <summary type="html"><![CDATA[We challenge a common assumption underlying most supervised deep learning:
that a model makes a prediction depending only on its parameters and the
features of a single input. To this end, we introduce a general-purpose deep
learning architecture that takes as input the entire dataset instead of
processing one datapoint at a time. Our approach uses self-attention to reason
about relationships between datapoints explicitly, which can be seen as
realizing non-parametric models using parametric attention mechanisms. However,
unlike conventional non-parametric models, we let the model learn end-to-end
from the data how to make use of other datapoints for prediction. Empirically,
our models solve cross-datapoint lookup and complex reasoning tasks unsolvable
by traditional deep learning models. We show highly competitive results on
tabular data, early results on CIFAR-10, and give insight into how the model
makes use of the interactions between points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kossen_J/0/1/0/all/0/1"&gt;Jannik Kossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyle_C/0/1/0/all/0/1"&gt;Clare Lyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Aidan N. Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRIBO: Robust Deep Reinforcement Learning via Multi-View Information Bottleneck. (arXiv:2102.13268v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13268</id>
        <link href="http://arxiv.org/abs/2102.13268"/>
        <updated>2021-06-07T03:06:14.355Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning (DRL) agents are often sensitive to visual
changes that were unseen in their training environments. To address this
problem, we leverage the sequential nature of RL to learn robust
representations that encode only task-relevant information from observations
based on the unsupervised multi-view setting. Specifically, we introduce an
auxiliary objective based on the multi-view in-formation bottleneck (MIB)
principle which quantifies the amount of task-irrelevant information and
encourages learning representations that are both predictive of the future and
less sensitive to task-irrelevant distractions. This enables us to train
high-performance policies that are robust to visual distractions and can
generalize to unseen environments. We demonstrate that our approach can achieve
SOTA performance on diverse visual control tasks on the DeepMind Control Suite,
even when the background is replaced with natural videos. In addition, we show
that our approach outperforms well-established baselines for generalization to
unseen environments on the Procgen benchmark. Our code is open-sourced and
available at https://github.com/JmfanBU/DRIBO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiameng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenchao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions. (arXiv:2106.02619v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02619</id>
        <link href="http://arxiv.org/abs/2106.02619"/>
        <updated>2021-06-07T03:06:14.330Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are among the most successful models
for learning high-complexity, real-world distributions. However, in theory, due
to the highly non-convex, non-concave landscape of the minmax training
objective, GAN remains one of the least understood deep learning models. In
this work, we formally study how GANs can efficiently learn certain
hierarchically generated distributions that are close to the distribution of
images in practice. We prove that when a distribution has a structure that we
refer to as Forward Super-Resolution, then simply training generative
adversarial networks using gradient descent ascent (GDA) can indeed learn this
distribution efficiently, both in terms of sample and time complexities. We
also provide concrete empirical evidence that not only our assumption "forward
super-resolution" is very natural in practice, but also the underlying learning
mechanisms that we study in this paper (to allow us efficiently train GAN via
GDA in theory) simulates the actual learning process of GANs in practice on
real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1"&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02246</id>
        <link href="http://arxiv.org/abs/2106.02246"/>
        <updated>2021-06-07T03:06:14.321Z</updated>
        <summary type="html"><![CDATA[Spatial context is central to understanding health and disease. Yet reference
protein interaction networks lack such contextualization, thereby limiting the
study of where protein interactions likely occur in the human body.
Contextualized protein interactions could better characterize genes with
disease-specific interactions and elucidate diseases' manifestation in specific
cell types. Here, we introduce AWARE, a graph neural message passing approach
to inject cellular and tissue context into protein embeddings. AWARE optimizes
for a multi-scale embedding space, whose structure reflects the topology of
cell type specific networks. We construct a multi-scale network of the Human
Cell Atlas and apply AWARE to learn protein, cell type, and tissue embeddings
that uphold cell type and tissue hierarchies. We demonstrate AWARE on the novel
task of predicting whether a gene is associated with a disease and where it
most likely manifests in the human body. AWARE embeddings outperform global
embeddings by at least 12.5%, highlighting the importance of contextual
learners for protein networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Michelle M. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAFLOW: Conditional Autoregressive Flows. (arXiv:2106.02531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02531</id>
        <link href="http://arxiv.org/abs/2106.02531"/>
        <updated>2021-06-07T03:06:14.264Z</updated>
        <summary type="html"><![CDATA[We introduce CAFLOW, a new diverse image-to-image translation model that
simultaneously leverages the power of auto-regressive modeling and the modeling
efficiency of conditional normalizing flows. We transform the conditioning
image into a sequence of latent encodings using a multi-scale normalizing flow
and repeat the process for the conditioned image. We model the conditional
distribution of the latent encodings by modeling the auto-regressive
distributions with an efficient multi-scale normalizing flow, where each
conditioning factor affects image synthesis at its respective resolution scale.
Our proposed framework performs well on a range of image-to-image translation
tasks. It outperforms former designs of conditional flows because of its
expressive auto-regressive structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1"&gt;Marcello Carioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etmann_C/0/1/0/all/0/1"&gt;Christian Etmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afyouni_S/0/1/0/all/0/1"&gt;Soroosh Afyouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1"&gt;Zoe Kourtzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Text Modeling through Short Run Inference. (arXiv:2106.02513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02513</id>
        <link href="http://arxiv.org/abs/2106.02513"/>
        <updated>2021-06-07T03:06:14.221Z</updated>
        <summary type="html"><![CDATA[Latent variable models for text, when trained successfully, accurately model
the data distribution and capture global semantic and syntactic features of
sentences. The prominent approach to train such models is variational
autoencoders (VAE). It is nevertheless challenging to train and often results
in a trivial local optimum where the latent variable is ignored and its
posterior collapses into the prior, an issue known as posterior collapse.
Various techniques have been proposed to mitigate this issue. Most of them
focus on improving the inference model to yield latent codes of higher quality.
The present work proposes a short run dynamics for inference. It is initialized
from the prior distribution of the latent variable and then runs a small number
(e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The
major advantage of our method is that it does not require a separate inference
model or assume simple geometry of the posterior distribution, thus rendering
an automatic, natural and flexible inference engine. We show that the models
trained with short run dynamics more accurately model the data, compared to
strong language model and VAE baselines, and exhibit no sign of posterior
collapse. Analyses of the latent space show that interpolation in the latent
space is able to generate coherent sentences with smooth transition and
demonstrate improved classification over strong baselines with latent features
from unsupervised pretraining. These results together expose a well-structured
latent space of our generative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1"&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence of cognitive, geographical, and collaborative proximity on knowledge production of Canadian nanotechnology. (arXiv:2106.02110v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02110</id>
        <link href="http://arxiv.org/abs/2106.02110"/>
        <updated>2021-06-07T03:06:14.212Z</updated>
        <summary type="html"><![CDATA[Incorporating existing knowledge is vital for innovating, discovering, and
generating new ideas. Knowledge production through research and invention is
the key to scientific and technological development. As an emerging technology,
nanotechnology has already proved its great potential for the global economy,
attracting considerable federal investments. Canada is reported as one of the
major players in producing nanotechnology research. In this paper, we focused
on the main drivers of knowledge production and diffusion by analyzing Canadian
nanotechnology researchers. We hypothesized that knowledge production in
Canadian nanotechnology is influenced by three key proximity factors, namely
cognitive, geographical, and collaborative. Using statistical analysis, social
network analysis, and machine learning techniques we comprehensively assessed
the influence of the proximity factors on academic knowledge production. Our
results not only prove a significant impact of the three key proximity factors
but also their predictive potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Neira_E/0/1/0/all/0/1"&gt;Elva Luz Crespo Neira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Beaudry_C/0/1/0/all/0/1"&gt;Catherine Beaudry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schiffauerova_A/0/1/0/all/0/1"&gt;Andrea Schiffauerova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Adversarial and Conditional State Space Model for Imitation Learning. (arXiv:2001.11628v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.11628</id>
        <link href="http://arxiv.org/abs/2001.11628"/>
        <updated>2021-06-07T03:06:14.203Z</updated>
        <summary type="html"><![CDATA[State representation learning (SRL) in partially observable Markov decision
processes has been studied to learn abstract features of data useful for robot
control tasks. For SRL, acquiring domain-agnostic states is essential for
achieving efficient imitation learning. Without these states, imitation
learning is hampered by domain-dependent information useless for control.
However, existing methods fail to remove such disturbances from the states when
the data from experts and agents show large domain shifts. To overcome this
issue, we propose a domain-adversarial and conditional state space model
(DAC-SSM) that enables control systems to obtain domain-agnostic and task- and
dynamics-aware states. DAC-SSM jointly optimizes the state inference,
observation reconstruction, forward dynamics, and reward models. To remove
domain-dependent information from the states, the model is trained with domain
discriminators in an adversarial manner, and the reconstruction is conditioned
on domain labels. We experimentally evaluated the model predictive control
performance via imitation learning for continuous control of sparse reward
tasks in simulators and compared it with the performance of the existing SRL
method. The agents from DAC-SSM achieved performance comparable to experts and
more than twice the baselines. We conclude domain-agnostic states are essential
for imitation learning that has large domain shifts and can be obtained using
DAC-SSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okumura_R/0/1/0/all/0/1"&gt;Ryo Okumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1"&gt;Masashi Okada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1"&gt;Tadahiro Taniguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization and Reparameterization Avoid Vanishing Gradients in Sigmoid-Type Networks. (arXiv:2106.02260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02260</id>
        <link href="http://arxiv.org/abs/2106.02260"/>
        <updated>2021-06-07T03:06:14.197Z</updated>
        <summary type="html"><![CDATA[Deep learning requires several design choices, such as the nodes' activation
functions and the widths, types, and arrangements of the layers. One
consideration when making these choices is the vanishing-gradient problem,
which is the phenomenon of algorithms getting stuck at suboptimal points due to
small gradients. In this paper, we revisit the vanishing-gradient problem in
the context of sigmoid-type activation. We use mathematical arguments to
highlight two different sources of the phenomenon, namely large individual
parameters and effects across layers, and to illustrate two simple remedies,
namely regularization and rescaling. We then demonstrate the effectiveness of
the two remedies in practice. In view of the vanishing-gradient problem being a
main reason why tanh and other sigmoid-type activation has become much less
popular than relu-type activation, our results bring sigmoid-type activation
back to the table.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ven_L/0/1/0/all/0/1"&gt;Leni Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1"&gt;Johannes Lederer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.02522</id>
        <link href="http://arxiv.org/abs/2106.02522"/>
        <updated>2021-06-07T03:06:14.186Z</updated>
        <summary type="html"><![CDATA[Stock prediction, with the purpose of forecasting the future price trends of
stocks, is crucial for maximizing profits from stock investments. While great
research efforts have been devoted to exploiting deep neural networks for
improved stock prediction, the existing studies still suffer from two major
issues. First, the long-range dependencies in time series are not sufficiently
captured. Second, the chaotic property of financial time series fundamentally
lowers prediction performance. In this study, we propose a novel framework to
address both issues regarding stock prediction. Specifically, in terms of
transforming time series into complex networks, we convert market price series
into graphs. Then, structural information, referring to associations among
temporal points and the node weights, is extracted from the mapped graphs to
resolve the problems regarding long-range dependencies and the chaotic
property. We take graph embeddings to represent the associations among temporal
points as the prediction model inputs. Node weights are used as a priori
knowledge to enhance the learning of temporal attention. The effectiveness of
our proposed framework is validated using real-world stock data, and our
approach obtains the best performance among several state-of-the-art
benchmarks. Moreover, in the conducted trading simulations, our framework
further obtains the highest cumulative profits. Our results supplement the
existing applications of complex network methods in the financial realm and
provide insightful implications for investment applications regarding decision
support in financial markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1"&gt;Shangzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jichang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adam in Private: Secure and Fast Training of Deep Neural Networks with Adaptive Moment Estimation. (arXiv:2106.02203v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02203</id>
        <link href="http://arxiv.org/abs/2106.02203"/>
        <updated>2021-06-07T03:06:14.163Z</updated>
        <summary type="html"><![CDATA[Privacy-preserving machine learning (PPML) aims at enabling machine learning
(ML) algorithms to be used on sensitive data. We contribute to this line of
research by proposing a framework that allows efficient and secure evaluation
of full-fledged state-of-the-art ML algorithms via secure multi-party
computation (MPC). This is in contrast to most prior works, which substitute ML
algorithms with approximated "MPC-friendly" variants. A drawback of the latter
approach is that fine-tuning of the combined ML and MPC algorithms is required,
which might lead to less efficient algorithms or inferior quality ML. This is
an issue for secure deep neural networks (DNN) training in particular, as this
involves arithmetic algorithms thought to be "MPC-unfriendly", namely, integer
division, exponentiation, inversion, and square root. In this work, we propose
secure and efficient protocols for the above seemingly MPC-unfriendly
computations. Our protocols are three-party protocols in the honest-majority
setting, and we propose both passively secure and actively secure with abort
variants. A notable feature of our protocols is that they simultaneously
provide high accuracy and efficiency. This framework enables us to efficiently
and securely compute modern ML algorithms such as Adam and the softmax function
"as is", without resorting to approximations. As a result, we obtain secure DNN
training that outperforms state-of-the-art three-party systems; our full
training is up to 6.7 times faster than just the online phase of the recently
proposed FALCON@PETS'21 on a standard benchmark network. We further perform
measurements on real-world DNNs, AlexNet and VGG16. The performance of our
framework is up to a factor of about 12-14 faster for AlexNet and 46-48 faster
for VGG16 to achieve an accuracy of 70% and 75%, respectively, when compared to
FALCON.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Attrapadung_N/0/1/0/all/0/1"&gt;Nuttapong Attrapadung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamada_K/0/1/0/all/0/1"&gt;Koki Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikarashi_D/0/1/0/all/0/1"&gt;Dai Ikarashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikuchi_R/0/1/0/all/0/1"&gt;Ryo Kikuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Takahiro Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishina_I/0/1/0/all/0/1"&gt;Ibuki Mishina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morita_H/0/1/0/all/0/1"&gt;Hiraku Morita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuldt_J/0/1/0/all/0/1"&gt;Jacob C. N. Schuldt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition. (arXiv:2106.02302v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02302</id>
        <link href="http://arxiv.org/abs/2106.02302"/>
        <updated>2021-06-07T03:06:14.152Z</updated>
        <summary type="html"><![CDATA[Integrating external language models (LMs) into end-to-end (E2E) models
remains a challenging task for domain-adaptive speech recognition. Recently,
internal language model estimation (ILME)-based LM fusion has shown significant
word error rate (WER) reduction from Shallow Fusion by subtracting a weighted
internal LM score from an interpolation of E2E model and external LM scores
during beam search. However, on different test sets, the optimal LM
interpolation weights vary over a wide range and have to be tuned extensively
on well-matched validation sets. In this work, we perform LM fusion in the
minimum WER (MWER) training of an E2E model to obviate the need for LM weights
tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),
we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based
fusion is conducted to generate N-best hypotheses and their posteriors.
Additional gradient is induced when internal LM is engaged in MWER-ILME loss
computation. During inference, LM weights pre-determined in MWER training
enable robust LM integrations on test sets from different domains. Experimented
with 30K-hour trained transformer transducers, MWER-ILME achieves on average
8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,
respectively, on 6 different test sets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guoli Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. (arXiv:2012.05420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05420</id>
        <link href="http://arxiv.org/abs/2012.05420"/>
        <updated>2021-06-07T03:06:14.139Z</updated>
        <summary type="html"><![CDATA[A recent numerical study observed that neural network classifiers enjoy a
large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$
where $A$ is a linear map and $f$ is the output of the penultimate layer of the
network (after activation), then all data points $x_{i, 1}, \dots, x_{i, N_i}$
in a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$
are located at the vertices of a regular $k-1$-dimensional standard simplex in
a high-dimensional Euclidean space.

We explain this observation analytically in toy models for highly expressive
deep neural networks. In complementary examples, we demonstrate rigorously that
even the final output of the classifier $h$ is not uniform over data samples
from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not
bring the data samples into a convenient geometric configuration).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02395</id>
        <link href="http://arxiv.org/abs/2106.02395"/>
        <updated>2021-06-07T03:06:14.132Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have shown to perform very well on large scale
object recognition problems and lead to widespread use for real-world
applications, including situations where DNN are implemented as "black boxes".
A promising approach to secure their use is to accept decisions that are likely
to be correct while discarding the others. In this work, we propose DOCTOR, a
simple method that aims to identify whether the prediction of a DNN classifier
should (or should not) be trusted so that, consequently, it would be possible
to accept it or to reject it. Two scenarios are investigated: Totally Black Box
(TBB) where only the soft-predictions are available and Partially Black Box
(PBB) where gradient-propagation to perform input pre-processing is allowed.
Empirically, we show that DOCTOR outperforms all state-of-the-art methods on
various well-known images and sentiment analysis datasets. In particular, we
observe a reduction of up to $4\%$ of the false rejection rate (FRR) in the PBB
scenario. DOCTOR can be applied to any pre-trained model, it does not require
prior information about the underlying dataset and is as simple as the simplest
available methods in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1"&gt;Federica Granese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1"&gt;Marco Romanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1"&gt;Daniele Gorla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Epidemic Forecasting and Community Risk Evaluation of COVID-19. (arXiv:2106.02094v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.02094</id>
        <link href="http://arxiv.org/abs/2106.02094"/>
        <updated>2021-06-07T03:06:14.126Z</updated>
        <summary type="html"><![CDATA[Pandemic control measures like lock-down, restrictions on restaurants and
gatherings, social-distancing have shown to be effective in curtailing the
spread of COVID-19. However, their sustained enforcement has negative economic
effects. To craft strategies and policies that reduce the hardship on the
people and the economy while being effective against the pandemic, authorities
need to understand the disease dynamics at the right geo-spatial granularity.
Considering factors like the hospitals' ability to handle the fluctuating
demands, evaluating various reopening scenarios, and accurate forecasting of
cases are vital to decision making. Towards this end, we present a flexible
end-to-end solution that seamlessly integrates public health data with tertiary
client data to accurately estimate the risk of reopening a community. At its
core lies a state-of-the-art prediction model that auto-captures changing
trends in transmission and mobility. Benchmarking against various published
baselines confirm the superiority of our forecasting algorithm. Combined with
the ability to extend to multiple client-specific requirements and perform
deductive reasoning through counter-factual analysis, this solution provides
actionable insights to multiple client domains ranging from government to
educational institutions, hospitals, and commercial establishments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_V/0/1/0/all/0/1"&gt;Vishrawas Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navalekar_S/0/1/0/all/0/1"&gt;Sayali Navalekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooley_R/0/1/0/all/0/1"&gt;Ryan Hooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_R/0/1/0/all/0/1"&gt;Raman Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Ajay Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianco_S/0/1/0/all/0/1"&gt;Simone Bianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufman_J/0/1/0/all/0/1"&gt;James H. Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using latent space regression to analyze and leverage compositionality in GANs. (arXiv:2103.10426v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10426</id>
        <link href="http://arxiv.org/abs/2103.10426"/>
        <updated>2021-06-07T03:06:14.106Z</updated>
        <summary type="html"><![CDATA[In recent years, Generative Adversarial Networks have become ubiquitous in
both research and public perception, but how GANs convert an unstructured
latent code to a high quality output is still an open question. In this work,
we investigate regression into the latent space as a probe to understand the
compositional properties of GANs. We find that combining the regressor and a
pretrained generator provides a strong image prior, allowing us to create
composite images from a collage of random image parts at inference time while
maintaining global consistency. To compare compositional properties across
different generators, we measure the trade-offs between reconstruction of the
unrealistic input and image quality of the regenerated samples. We find that
the regression approach enables more localized editing of individual image
parts compared to direct editing in the latent space, and we conduct
experiments to quantify this independence effect. Our method is agnostic to the
semantics of edits, and does not require labels or predefined concepts during
training. Beyond image composition, our method extends to a number of related
applications, such as image inpainting or example-based image editing, which we
demonstrate on several GANs and datasets, and because it uses only a single
forward pass, it can operate in real-time. Code is available on our project
page: https://chail.github.io/latent-composition/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Lucy Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1"&gt;Jonas Wulff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adiabatic Quantum Feature Selection for Sparse Linear Regression. (arXiv:2106.02357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02357</id>
        <link href="http://arxiv.org/abs/2106.02357"/>
        <updated>2021-06-07T03:06:14.099Z</updated>
        <summary type="html"><![CDATA[Linear regression is a popular machine learning approach to learn and predict
real valued outputs or dependent variables from independent variables or
features. In many real world problems, its beneficial to perform sparse linear
regression to identify important features helpful in predicting the dependent
variable. It not only helps in getting interpretable results but also avoids
overfitting when the number of features is large, and the amount of data is
small. The most natural way to achieve this is by using `best subset selection'
which penalizes non-zero model parameters by adding $\ell_0$ norm over
parameters to the least squares loss. However, this makes the objective
function non-convex and intractable even for a small number of features. This
paper aims to address the intractability of sparse linear regression with
$\ell_0$ norm using adiabatic quantum computing, a quantum computing paradigm
that is particularly useful for solving optimization problems faster. We
formulate the $\ell_0$ optimization problem as a Quadratic Unconstrained Binary
Optimization (QUBO) problem and solve it using the D-Wave adiabatic quantum
computer. We study and compare the quality of QUBO solution on synthetic and
real world datasets. The results demonstrate the effectiveness of the proposed
adiabatic quantum computing approach in finding the optimal solution. The QUBO
solution matches the optimal solution for a wide range of sparsity penalty
values across the datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desu_S/0/1/0/all/0/1"&gt;Surya Sai Teja Desu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1"&gt;P.K. Srijith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1"&gt;M.V. Panduranga Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1"&gt;Naveen Sivadasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes. (arXiv:2106.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02524</id>
        <link href="http://arxiv.org/abs/2106.02524"/>
        <updated>2021-06-07T03:06:14.093Z</updated>
        <summary type="html"><![CDATA[Continuity of care is crucial to ensuring positive health outcomes for
patients discharged from an inpatient hospital setting, and improved
information sharing can help. To share information, caregivers write discharge
notes containing action items to share with patients and their future
caregivers, but these action items are easily lost due to the lengthiness of
the documents. In this work, we describe our creation of a dataset of clinical
action items annotated over MIMIC-III, the largest publicly available dataset
of real clinical notes. This dataset, which we call CLIP, is annotated by
physicians and covers 718 documents representing 100K sentences. We describe
the task of extracting the action items from these documents as multi-aspect
extractive summarization, with each aspect representing a type of action to be
taken. We evaluate several machine learning models on this task, and show that
the best models exploit in-domain language model pre-training on 59K
unannotated documents, and incorporate context from neighboring sentences. We
also propose an approach to pre-training data selection that allows us to
explore the trade-off between size and domain-specificity of pre-training
datasets for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1"&gt;James Mullenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Sean Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seale_J/0/1/0/all/0/1"&gt;Jennifer Seale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1"&gt;Jordan Swartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKelvey_T/0/1/0/all/0/1"&gt;T. Greg McKelvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online reinforcement learning with sparse rewards through an active inference capsule. (arXiv:2106.02390v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02390</id>
        <link href="http://arxiv.org/abs/2106.02390"/>
        <updated>2021-06-07T03:06:14.085Z</updated>
        <summary type="html"><![CDATA[Intelligent agents must pursue their goals in complex environments with
partial information and often limited computational capacity. Reinforcement
learning methods have achieved great success by creating agents that optimize
engineered reward functions, but which often struggle to learn in sparse-reward
environments, generally require many environmental interactions to perform
well, and are typically computationally very expensive. Active inference is a
model-based approach that directs agents to explore uncertain states while
adhering to a prior model of their goal behaviour. This paper introduces an
active inference agent which minimizes the novel free energy of the expected
future. Our model is capable of solving sparse-reward problems with a very high
sample efficiency due to its objective function, which encourages directed
exploration of uncertain states. Moreover, our model is computationally very
light and can operate in a fully online manner while achieving comparable
performance to offline RL methods. We showcase the capabilities of our model by
solving the mountain car problem, where we demonstrate its superior exploration
properties and its robustness to observation noise, which in fact improves
performance. We also introduce a novel method for approximating the prior model
from the reward function, which simplifies the expression of complex objectives
and improves performance over previous active inference approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noel_A/0/1/0/all/0/1"&gt;Alejandro Daniel Noel&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hoof_C/0/1/0/all/0/1"&gt;Charel van Hoof&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1"&gt;Beren Millidge&lt;/a&gt; (2) ((1) Delft University of Technology, (2) University of Oxford)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Earth Mover's Pinball Loss: Quantiles for Histogram-Valued Regression. (arXiv:2106.02051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02051</id>
        <link href="http://arxiv.org/abs/2106.02051"/>
        <updated>2021-06-07T03:06:14.079Z</updated>
        <summary type="html"><![CDATA[Although ubiquitous in the sciences, histogram data have not received much
attention by the Deep Learning community. Whilst regression and classification
tasks for scalar and vector data are routinely solved by neural networks, a
principled approach for estimating histogram labels as a function of an input
vector or image is lacking in the literature. We present a dedicated method for
Deep Learning-based histogram regression, which incorporates cross-bin
information and yields distributions over possible histograms, expressed by
$\tau$-quantiles of the cumulative histogram in each bin. The crux of our
approach is a new loss function obtained by applying the pinball loss to the
cumulative histogram, which for 1D histograms reduces to the Earth Mover's
distance (EMD) in the special case of the median ($\tau = 0.5$), and
generalizes it to arbitrary quantiles. We validate our method with an
illustrative toy example, a football-related task, and an astrophysical
computer vision problem. We show that with our loss function, the accuracy of
the predicted median histograms is very similar to the standard EMD case (and
higher than for per-bin loss functions such as cross-entropy), while the
predictions become much more informative at almost no additional computational
cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+List_F/0/1/0/all/0/1"&gt;Florian List&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to select and use tools? : Active Perception of Target Objects Using Multimodal Deep Learning. (arXiv:2106.02445v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.02445</id>
        <link href="http://arxiv.org/abs/2106.02445"/>
        <updated>2021-06-07T03:06:14.061Z</updated>
        <summary type="html"><![CDATA[Selection of appropriate tools and use of them when performing daily tasks is
a critical function for introducing robots for domestic applications. In
previous studies, however, adaptability to target objects was limited, making
it difficult to accordingly change tools and adjust actions. To manipulate
various objects with tools, robots must both understand tool functions and
recognize object characteristics to discern a tool-object-action relation. We
focus on active perception using multimodal sensorimotor data while a robot
interacts with objects, and allow the robot to recognize their extrinsic and
intrinsic characteristics. We construct a deep neural networks (DNN) model that
learns to recognize object characteristics, acquires tool-object-action
relations, and generates motions for tool selection and handling. As an example
tool-use situation, the robot performs an ingredients transfer task, using a
turner or ladle to transfer an ingredient from a pot to a bowl. The results
confirm that the robot recognizes object characteristics and servings even when
the target ingredients are unknown. We also examine the contributions of
images, force, and tactile data and show that learning a variety of multimodal
information results in rich perception for tool use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_N/0/1/0/all/0/1"&gt;Namiko Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1"&gt;Tetsuya Ogata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funabashi_S/0/1/0/all/0/1"&gt;Satoshi Funabashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_H/0/1/0/all/0/1"&gt;Hiroki Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugano_S/0/1/0/all/0/1"&gt;Shigeki Sugano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Perceptron Revisited: Computational-Statistical Tradeoffs. (arXiv:2106.02496v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02496</id>
        <link href="http://arxiv.org/abs/2106.02496"/>
        <updated>2021-06-07T03:06:14.055Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning algorithms could provide significant speed-ups over
their classical counterparts; however, whether they could also achieve good
generalization remains unclear. Recently, two quantum perceptron models which
give a quadratic improvement over the classical perceptron algorithm using
Grover's search have been proposed by Wiebe et al. arXiv:1602.04799 . While the
first model reduces the complexity with respect to the size of the training
set, the second one improves the bound on the number of mistakes made by the
perceptron. In this paper, we introduce a hybrid quantum-classical perceptron
algorithm with lower complexity and better generalization ability than the
classical perceptron. We show a quadratic improvement over the classical
perceptron in both the number of samples and the margin of the data. We derive
a bound on the expected error of the hypothesis returned by our algorithm,
which compares favorably to the one obtained with the classical online
perceptron. We use numerical experiments to illustrate the trade-off between
computational complexity and statistical accuracy in quantum perceptron
learning and discuss some of the key practical issues surrounding the
implementation of quantum perceptron models into near-term quantum devices,
whose practical implementation represents a serious challenge due to inherent
noise. However, the potential benefits make correcting this worthwhile.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Roget_M/0/1/0/all/0/1"&gt;Mathieu Roget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Molfetta_G/0/1/0/all/0/1"&gt;Giuseppe Di Molfetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kadri_H/0/1/0/all/0/1"&gt;Hachem Kadri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-06-07T03:06:14.049Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,
CelebA, and ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing a First-order Heuristic for Approximate Bi-level Optimization. (arXiv:2106.02487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02487</id>
        <link href="http://arxiv.org/abs/2106.02487"/>
        <updated>2021-06-07T03:06:14.041Z</updated>
        <summary type="html"><![CDATA[Approximate bi-level optimization (ABLO) consists of (outer-level)
optimization problems, involving numerical (inner-level) optimization loops.
While ABLO has many applications across deep learning, it suffers from time and
memory complexity proportional to the length $r$ of its inner optimization
loop. To address this complexity, an earlier first-order method (FOM) was
proposed as a heuristic that omits second derivative terms, yielding
significant speed gains and requiring only constant memory. Despite FOM's
popularity, there is a lack of theoretical understanding of its convergence
properties. We contribute by theoretically characterizing FOM's gradient bias
under mild assumptions. We further demonstrate a rich family of examples where
FOM-based SGD does not converge to a stationary point of the ABLO objective. We
address this concern by proposing an unbiased FOM (UFOM) enjoying constant
memory complexity as a function of $r$. We characterize the introduced
time-variance tradeoff, demonstrate convergence bounds, and find an optimal
UFOM for a given ABLO problem. Finally, we propose an efficient adaptive UFOM
scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-07T03:06:14.035Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fre-GAN: Adversarial Frequency-consistent Audio Synthesis. (arXiv:2106.02297v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02297</id>
        <link href="http://arxiv.org/abs/2106.02297"/>
        <updated>2021-06-07T03:06:14.028Z</updated>
        <summary type="html"><![CDATA[Although recent works on neural vocoder have improved the quality of
synthesized audio, there still exists a gap between generated and ground-truth
audio in frequency space. This difference leads to spectral artifacts such as
hissing noise or robotic sound, and thus degrades the sample quality. In this
paper, we propose Fre-GAN which achieves frequency-consistent audio synthesis
with highly improved generation quality. Specifically, we first present
resolution-connected generator and resolution-wise discriminators, which help
learn various scales of spectral distributions over multiple frequency bands.
Additionally, to reproduce high-frequency components accurately, we leverage
discrete wavelet transform in the discriminators. From our experiments, Fre-GAN
achieves high-fidelity waveform generation with a gap of only 0.03 MOS compared
to ground-truth audio while outperforming standard models in quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1"&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Noisy Short Signal Camouflage in Multi-Domain Environment Decision-Making. (arXiv:2106.02044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02044</id>
        <link href="http://arxiv.org/abs/2106.02044"/>
        <updated>2021-06-07T03:06:14.010Z</updated>
        <summary type="html"><![CDATA[Data transmission between two or more digital devices in industry and
government demands secure and agile technology. Digital information
distribution often requires deployment of Internet of Things (IoT) devices and
Data Fusion techniques which have also gained popularity in both, civilian and
military environments, such as, emergence of Smart Cities and Internet of
Battlefield Things (IoBT). This usually requires capturing and consolidating
data from multiple sources. Because datasets do not necessarily originate from
identical sensors, fused data typically results in a complex Big Data problem.
Due to potentially sensitive nature of IoT datasets, Blockchain technology is
used to facilitate secure sharing of IoT datasets, which allows digital
information to be distributed, but not copied. However, blockchain has several
limitations related to complexity, scalability, and excessive energy
consumption. We propose an approach to hide information (sensor signal) by
transforming it to an image or an audio signal. In one of the latest attempts
to the military modernization, we investigate sensor fusion approach by
investigating the challenges of enabling an intelligent identification and
detection operation and demonstrates the feasibility of the proposed Deep
Learning and Anomaly Detection models that can support future application for
specific hand gesture alert system from wearable devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next Generation Multitarget Trackers: Random Finite Set Methods vs Transformer-based Deep Learning. (arXiv:2104.00734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00734</id>
        <link href="http://arxiv.org/abs/2104.00734"/>
        <updated>2021-06-07T03:06:14.004Z</updated>
        <summary type="html"><![CDATA[Multitarget Tracking (MTT) is the problem of tracking the states of an
unknown number of objects using noisy measurements, with important applications
to autonomous driving, surveillance, robotics, and others. In the model-based
Bayesian setting, there are conjugate priors that enable us to express the
multi-object posterior in closed form, which could theoretically provide
Bayes-optimal estimates. However, the posterior involves a super-exponential
growth of the number of hypotheses over time, forcing state-of-the-art methods
to resort to approximations for remaining tractable, which can impact their
performance in complex scenarios. Model-free methods based on deep-learning
provide an attractive alternative, as they can, in principle, learn the optimal
filter from data, but to the best of our knowledge were never compared to
current state-of-the-art Bayesian filters, specially not in contexts where
accurate models are available. In this paper, we propose a high-performing
deep-learning method for MTT based on the Transformer architecture and compare
it to two state-of-the-art Bayesian filters, in a setting where we assume the
correct model is provided. Although this gives an edge to the model-based
filters, it also allows us to generate unlimited training data. We show that
the proposed model outperforms state-of-the-art Bayesian filters in complex
scenarios, while matching their performance in simpler cases, which validates
the applicability of deep-learning also in the model-based regime. The code for
all our implementations is made available at
https://github.com/JulianoLagana/MT3 .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1"&gt;Juliano Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1"&gt;Georg Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ljungbergh_W/0/1/0/all/0/1"&gt;William Ljungbergh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yuxuan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1"&gt;Lennart Svensson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wymeersch_H/0/1/0/all/0/1"&gt;Henk Wymeersch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Confidence Sequences for Bounded Random Variables. (arXiv:2006.05022v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05022</id>
        <link href="http://arxiv.org/abs/2006.05022"/>
        <updated>2021-06-07T03:06:13.996Z</updated>
        <summary type="html"><![CDATA[Many inference problems, such as sequential decision problems like A/B
testing, adaptive sampling schemes like bandit selection, are often online in
nature. The fundamental problem for online inference is to provide a sequence
of confidence intervals that are valid uniformly over the growing-into-infinity
sample sizes. To address this question, we provide a near-optimal confidence
sequence for bounded random variables by utilizing Bentkus' concentration
results. We show that it improves on the existing approaches that use the
Cram{\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett
inequalities. The resulting confidence sequence is confirmed to be favorable in
both synthetic coverage problems and an application to adaptive stopping
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kuchibhotla_A/0/1/0/all/0/1"&gt;Arun Kumar Kuchibhotla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qinqing Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-07T03:06:13.988Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% in various state-of-the-art adversarially trained models on the
AutoAttack benchmark, where every small margin of improvement is significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Musical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal of Emotions Through Machine Learning. (arXiv:2106.02556v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.02556</id>
        <link href="http://arxiv.org/abs/2106.02556"/>
        <updated>2021-06-07T03:06:13.980Z</updated>
        <summary type="html"><![CDATA[The task of classifying emotions within a musical track has received
widespread attention within the Music Information Retrieval (MIR) community.
Music emotion recognition has traditionally relied on the use of acoustic
features, verbal features, and metadata-based filtering. The role of musical
prosody remains under-explored despite several studies demonstrating a strong
connection between prosody and emotion. In this study, we restrict the input of
traditional machine learning algorithms to the features of musical prosody.
Furthermore, our proposed approach builds upon the prior by classifying
emotions under an expanded emotional taxonomy, using the Geneva Wheel of
Emotion. We utilize a methodology for individual data collection from
vocalists, and personal ground truth labeling by the artist themselves. We
found that traditional machine learning algorithms when limited to the features
of musical prosody (1) achieve high accuracies for a single singer, (2)
maintain high accuracy when the dataset is expanded to multiple singers, and
(3) achieve high accuracies when trained on a reduced subset of the total
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_F/0/1/0/all/0/1"&gt;Farris Nicholas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brian_M/0/1/0/all/0/1"&gt;Model Brian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_S/0/1/0/all/0/1"&gt;Savery Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_W/0/1/0/all/0/1"&gt;Weinberg Gil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategyproof Learning: Building Trustworthy User-Generated Datasets. (arXiv:2106.02398v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02398</id>
        <link href="http://arxiv.org/abs/2106.02398"/>
        <updated>2021-06-07T03:06:13.964Z</updated>
        <summary type="html"><![CDATA[Today's large-scale machine learning algorithms harness massive amounts of
user-generated data to train large models. However, especially in the context
of content recommendation with enormous social, economical and political
incentives to promote specific views, products or ideologies, strategic users
might be tempted to fabricate or mislabel data in order to bias algorithms in
their favor. Unfortunately, today's learning schemes strongly incentivize such
strategic data misreporting. This is a major concern, as it endangers the
trustworthiness of the entire training datasets, and questions the safety of
any algorithm trained on such datasets. In this paper, we show that, perhaps
surprisingly, incentivizing data misreporting is not a fatality. We propose the
first personalized collaborative learning framework, Licchavi, with provable
strategyproofness guarantees through a careful design of the underlying loss
function. Interestingly, we also prove that Licchavi is Byzantine resilient: it
tolerates a minority of users that provide arbitrary data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farhadkhani_S/0/1/0/all/0/1"&gt;Sadegh Farhadkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea;-Nguy&amp;#xea;n Hoang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Adaptivity in Federated Learning: Convergence and Consistency. (arXiv:2106.02305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02305</id>
        <link href="http://arxiv.org/abs/2106.02305"/>
        <updated>2021-06-07T03:06:13.958Z</updated>
        <summary type="html"><![CDATA[The federated learning (FL) framework trains a machine learning model using
decentralized data stored at edge client devices by periodically aggregating
locally trained models. Popular optimization algorithms of FL use vanilla
(stochastic) gradient descent for both local updates at clients and global
updates at the aggregating server. Recently, adaptive optimization methods such
as AdaGrad have been studied for server updates. However, the effect of using
adaptive optimization methods for local updates at clients is not yet
understood. We show in both theory and practice that while local adaptive
methods can accelerate convergence, they can cause a non-vanishing solution
bias, where the final converged solution may be different from the stationary
point of the global objective function. We propose correction techniques to
overcome this inconsistency and complement the local adaptive methods for FL.
Extensive experiments on realistic federated training tasks show that the
proposed algorithms can achieve faster convergence and higher test accuracy
than the baselines without local adaptivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Luyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COLD: Concurrent Loads Disaggregator for Non-Intrusive Load Monitoring. (arXiv:2106.02352v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.02352</id>
        <link href="http://arxiv.org/abs/2106.02352"/>
        <updated>2021-06-07T03:06:13.951Z</updated>
        <summary type="html"><![CDATA[The modern artificial intelligence techniques show the outstanding
performances in the field of Non-Intrusive Load Monitoring (NILM). However, the
problem related to the identification of a large number of appliances working
simultaneously is underestimated. One of the reasons is the absence of a
specific data. In this research we propose the Synthesizer of Normalized
Signatures (SNS) algorithm to simulate the aggregated consumption with up to 10
concurrent loads. The results show that the synthetic data provides the models
with at least as a powerful identification accuracy as the real-world
measurements. We have developed the neural architecture named Concurrent Loads
Disaggregator (COLD) which is relatively simple and easy to understand in
comparison to the previous approaches. Our model allows identifying from 1 to
10 appliances working simultaneously with mean F1-score 78.95%. The source code
of the experiments performed is available at
https://github.com/arx7ti/cold-nilm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kamyshev_I/0/1/0/all/0/1"&gt;Ilia Kamyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kriukov_D/0/1/0/all/0/1"&gt;Dmitrii Kriukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gryazina_E/0/1/0/all/0/1"&gt;Elena Gryazina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution. (arXiv:2106.02295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02295</id>
        <link href="http://arxiv.org/abs/2106.02295"/>
        <updated>2021-06-07T03:06:13.945Z</updated>
        <summary type="html"><![CDATA[Model quantization is challenging due to many tedious hyper-parameters such
as precision (bitwidth), dynamic range (minimum and maximum discrete values)
and stepsize (interval between discrete values). Unlike prior arts that
carefully tune these values, we present a fully differentiable approach to
learn all of them, named Differentiable Dynamic Quantization (DDQ), which has
several benefits. (1) DDQ is able to quantize challenging lightweight
architectures like MobileNets, where different layers prefer different
quantization parameters. (2) DDQ is hardware-friendly and can be easily
implemented using low-precision matrix-vector multiplication, making it capable
in many hardware such as ARM. (3) Extensive experiments show that DDQ
outperforms prior arts on many networks and benchmarks, especially when models
are already efficient and compact. e.g., DDQ is the first approach that
achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhaoyang_Z/0/1/0/all/0/1"&gt;Zhang Zhaoyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenqi_S/0/1/0/all/0/1"&gt;Shao Wenqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jinwei_G/0/1/0/all/0/1"&gt;Gu Jinwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaogang_W/0/1/0/all/0/1"&gt;Wang Xiaogang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_L/0/1/0/all/0/1"&gt;Luo Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation. (arXiv:2102.01678v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01678</id>
        <link href="http://arxiv.org/abs/2102.01678"/>
        <updated>2021-06-07T03:06:13.938Z</updated>
        <summary type="html"><![CDATA[Suboptimal generalization of machine learning models on unseen data is a key
challenge which hampers the clinical applicability of such models to medical
imaging. Although various methods such as domain adaptation and domain
generalization have evolved to combat this challenge, learning robust and
generalizable representations is core to medical image understanding, and
continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation
for histoPathology), a form of data augmentation based on random style transfer
from non-medical style source such as artistic paintings, for learning
domain-agnostic visual representations in computational pathology. Style
transfer replaces the low-level texture content of an image with the
uninformative style of randomly selected style source image, while preserving
the original high-level semantic content. This improves robustness to domain
shift and can be used as a simple yet powerful tool for learning
domain-agnostic representations. We demonstrate that STRAP leads to
state-of-the-art performance, particularly in the presence of domain shifts, on
two particular classification tasks in computational pathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamashita_R/0/1/0/all/0/1"&gt;Rikiya Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banda_S/0/1/0/all/0/1"&gt;Snikitha Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jeanne Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable and Distributed User Association Policies for 5G and Beyond Networks. (arXiv:2106.02540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02540</id>
        <link href="http://arxiv.org/abs/2106.02540"/>
        <updated>2021-06-07T03:06:13.932Z</updated>
        <summary type="html"><![CDATA[We study the problem of user association, namely finding the optimal
assignment of user equipment to base stations to achieve a targeted network
performance. In this paper, we focus on the knowledge transferability of
association policies. Indeed, traditional non-trivial user association schemes
are often scenario-specific or deployment-specific and require a policy
re-design or re-learning when the number or the position of the users change.
In contrast, transferability allows to apply a single user association policy,
devised for a specific scenario, to other distinct user deployments, without
needing a substantial re-learning or re-design phase and considerably reducing
its computational and management complexity. To achieve transferability, we
first cast user association as a multi-agent reinforcement learning problem.
Then, based on a neural attention mechanism that we specifically conceived for
this context, we propose a novel distributed policy network architecture, which
is transferable among users with zero-shot generalization capability i.e.,
without requiring additional training.Numerical results show the effectiveness
of our solution in terms of overall network communication rate, outperforming
centralized benchmarks even when the number of users doubles with respect to
the initial training point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sana_M/0/1/0/all/0/1"&gt;Mohamed Sana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietro_N/0/1/0/all/0/1"&gt;Nicola di Pietro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strinati_E/0/1/0/all/0/1"&gt;Emilio Calvanese Strinati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence. (arXiv:2106.02096v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02096</id>
        <link href="http://arxiv.org/abs/2106.02096"/>
        <updated>2021-06-07T03:06:13.913Z</updated>
        <summary type="html"><![CDATA[We introduce a linear dimensionality reduction technique preserving
topological features via persistent homology. The method is designed to find
linear projection $L$ which preserves the persistent diagram of a point cloud
$\mathbb{X}$ via simulated annealing. The projection $L$ induces a set of
canonical simplicial maps from the Rips (or \v{C}ech) filtration of
$\mathbb{X}$ to that of $L\mathbb{X}$. In addition to the distance between
persistent diagrams, the projection induces a map between filtrations, called
filtration homomorphism. Using the filtration homomorphism, one can measure the
difference between shapes of two filtrations directly comparing simplicial
complexes with respect to quasi-isomorphism $\mu_{\operatorname{quasi-iso}}$ or
strong homotopy equivalence $\mu_{\operatorname{equiv}}$. These
$\mu_{\operatorname{quasi-iso}}$ and $\mu_{\operatorname{equiv}}$ measures how
much portion of corresponding simplicial complexes is quasi-isomorphic or
homotopy equivalence respectively. We validate the effectiveness of our
framework with simple examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Byeongsu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+You_K/0/1/0/all/0/1"&gt;Kisung You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.02601</id>
        <link href="http://arxiv.org/abs/2106.02601"/>
        <updated>2021-06-07T03:06:13.902Z</updated>
        <summary type="html"><![CDATA[Optimization problems are ubiquitous in our societies and are present in
almost every segment of the economy. Most of these optimization problems are
NP-hard and computationally demanding, often requiring approximate solutions
for large-scale instances. Machine learning frameworks that learn to
approximate solutions to such hard optimization problems are a potentially
promising avenue to address these difficulties, particularly when many closely
related problem instances must be solved repeatedly. Supervised learning
frameworks can train a model using the outputs of pre-solved instances.
However, when the outputs are themselves approximations, when the optimization
problem has symmetric solutions, and/or when the solver uses randomization,
solutions to closely related instances may exhibit large differences and the
learning task can become inherently more difficult. This paper demonstrates
this critical challenge, connects the volatility of the training data to the
ability of a model to approximate it, and proposes a method for producing
(exact or approximate) solutions to optimization problems that are more
amenable to supervised learning tasks. The effectiveness of the method is
tested on hard non-linear nonconvex and discrete combinatorial problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1"&gt;James Kotary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Slice-Aware Representations with Mixture of Attentions. (arXiv:2106.02363v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02363</id>
        <link href="http://arxiv.org/abs/2106.02363"/>
        <updated>2021-06-07T03:06:13.894Z</updated>
        <summary type="html"><![CDATA[Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Han Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1"&gt;Ruhi Sarikaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fluctuation-dissipation Type Theorem in Stochastic Linear Learning. (arXiv:2106.02220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02220</id>
        <link href="http://arxiv.org/abs/2106.02220"/>
        <updated>2021-06-07T03:06:13.888Z</updated>
        <summary type="html"><![CDATA[The fluctuation-dissipation theorem (FDT) is a simple yet powerful
consequence of the first-order differential equation governing the dynamics of
systems subject simultaneously to dissipative and stochastic forces. The linear
learning dynamics, in which the input vector maps to the output vector by a
linear matrix whose elements are the subject of learning, has a stochastic
version closely mimicking the Langevin dynamics when a full-batch gradient
descent scheme is replaced by that of stochastic gradient descent. We derive a
generalized FDT for the stochastic linear learning dynamics and verify its
validity among the well-known machine learning data sets such as MNIST,
CIFAR-10 and EMNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1"&gt;Manhyung Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jeonghyeok Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taewoong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jung Hoon Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning. (arXiv:2106.02097v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02097</id>
        <link href="http://arxiv.org/abs/2106.02097"/>
        <updated>2021-06-07T03:06:13.871Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end, model-based deep reinforcement learning agent which
dynamically attends to relevant parts of its state, in order to plan and to
generalize better out-of-distribution. The agent's architecture uses a set
representation and a bottleneck mechanism, forcing the number of entities to
which the agent attends at each planning step to be small. In experiments with
customized MiniGrid environments with different dynamics, we observe that the
design allows agents to learn to plan effectively, by attending to the relevant
objects, leading to better out-of-distribution generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingde Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luan_S/0/1/0/all/0/1"&gt;Sitao Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search via Bregman Iterations. (arXiv:2106.02479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02479</id>
        <link href="http://arxiv.org/abs/2106.02479"/>
        <updated>2021-06-07T03:06:13.865Z</updated>
        <summary type="html"><![CDATA[We propose a novel strategy for Neural Architecture Search (NAS) based on
Bregman iterations. Starting from a sparse neural network our gradient-based
one-shot algorithm gradually adds relevant parameters in an inverse scale space
manner. This allows the network to choose the best architecture in the search
space which makes it well-designed for a given task, e.g., by adding neurons or
skip connections. We demonstrate that using our approach one can unveil, for
instance, residual autoencoders for denoising, deblurring, and classification
tasks. Code is available at https://github.com/TimRoith/BregmanLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bungert_L/0/1/0/all/0/1"&gt;Leon Bungert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roith_T/0/1/0/all/0/1"&gt;Tim Roith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenbrinck_D/0/1/0/all/0/1"&gt;Daniel Tenbrinck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1"&gt;Martin Burger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Little Robustness Goes a Long Way: Leveraging Universal Features for Targeted Transfer Attacks. (arXiv:2106.02105v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02105</id>
        <link href="http://arxiv.org/abs/2106.02105"/>
        <updated>2021-06-07T03:06:13.858Z</updated>
        <summary type="html"><![CDATA[Adversarial examples for neural network image classifiers are known to be
transferable: examples optimized to be misclassified by a source classifier are
often misclassified as well by classifiers with different architectures.
However, targeted adversarial examples -- optimized to be classified as a
chosen target class -- tend to be less transferable between architectures.
While prior research on constructing transferable targeted attacks has focused
on improving the optimization procedure, in this work we examine the role of
the source classifier. Here, we show that training the source classifier to be
"slightly robust" -- that is, robust to small-magnitude adversarial examples --
substantially improves the transferability of targeted attacks, even between
architectures as different as convolutional neural networks and transformers.
We argue that this result supports a non-intuitive hypothesis: on the spectrum
from non-robust (standard) to highly robust classifiers, those that are only
slightly robust exhibit the most universal features -- ones that tend to
overlap with the features learned by other classifiers trained on the same
dataset. The results we present provide insight into the nature of adversarial
examples as well as the mechanisms underlying so-called "robust" classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1"&gt;Jacob M. Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1"&gt;Garrett T. Kenyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Celebrating Diversity in Shared Multi-Agent Reinforcement Learning. (arXiv:2106.02195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02195</id>
        <link href="http://arxiv.org/abs/2106.02195"/>
        <updated>2021-06-07T03:06:13.852Z</updated>
        <summary type="html"><![CDATA[Recently, deep multi-agent reinforcement learning (MARL) has shown the
promise to solve complex cooperative tasks. Its success is partly because of
parameter sharing among agents. However, such sharing may lead agents to behave
similarly and limit their coordination capacity. In this paper, we aim to
introduce diversity in both optimization and representation of shared
multi-agent reinforcement learning. Specifically, we propose an
information-theoretical regularization to maximize the mutual information
between agents' identities and their trajectories, encouraging extensive
exploration and diverse individualized behaviors. In representation, we
incorporate agent-specific modules in the shared neural network architecture,
which are regularized by L1-norm to promote learning sharing among agents while
keeping necessary diversity. Empirical results show that our method achieves
state-of-the-art performance on Google Research Football and super hard
StarCraft II micromanagement tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenghao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+WU_C/0/1/0/all/0/1"&gt;Chengjie WU&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qianchuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Surprising Power of Graph Neural Networks with Random Node Initialization. (arXiv:2010.01179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01179</id>
        <link href="http://arxiv.org/abs/2010.01179"/>
        <updated>2021-06-07T03:06:13.844Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are effective models for representation learning
on relational data. However, standard GNNs are limited in their expressive
power, as they cannot distinguish graphs beyond the capability of the
Weisfeiler-Leman graph isomorphism heuristic. In order to break this
expressiveness barrier, GNNs have been enhanced with random node initialization
(RNI), where the idea is to train and run the models with randomized initial
node features. In this work, we analyze the expressive power of GNNs with RNI,
and prove that these models are universal, a first such result for GNNs not
relying on computationally demanding higher-order properties. This universality
result holds even with partially randomized initial node features, and
preserves the invariance properties of GNNs in expectation. We then empirically
analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our
empirical findings support the superior performance of GNNs with RNI over
standard GNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abboud_R/0/1/0/all/0/1"&gt;Ralph Abboud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1"&gt;&amp;#x130;smail &amp;#x130;lkan Ceylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1"&gt;Martin Grohe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10201</id>
        <link href="http://arxiv.org/abs/2103.10201"/>
        <updated>2021-06-07T03:06:13.837Z</updated>
        <summary type="html"><![CDATA[Context: Software engineering researchers have undertaken many experiments
investigating the potential of software defect prediction algorithms.
Unfortunately, some widely used performance metrics are known to be
problematic, most notably F1, but nevertheless F1 is widely used.

Objective: To investigate the potential impact of using F1 on the validity of
this large body of research.

Method: We undertook a systematic review to locate relevant experiments and
then extract all pairwise comparisons of defect prediction performance using F1
and the un-biased Matthews correlation coefficient (MCC).

Results: We found a total of 38 primary studies. These contain 12,471 pairs
of results. Of these, 21.95% changed direction when the MCC metric is used
instead of the biased F1 metric. Unfortunately, we also found evidence
suggesting that F1 remains widely used in software defect prediction research.

Conclusions: We reiterate the concerns of statisticians that the F1 is a
problematic metric outside of an information retrieval context, since we are
concerned about both classes (defect-prone and not defect-prone units). This
inappropriate usage has led to a substantial number (more than one fifth) of
erroneous (in terms of direction) results. Therefore we urge researchers to (i)
use an unbiased metric and (ii) publish detailed results including confusion
matrices such that alternative analyses become possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jingxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1"&gt;Martin Shepperd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Semi-supervised Framework for Call Center Agent Malpractice Detection via Neural Feature Learning. (arXiv:2106.02433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02433</id>
        <link href="http://arxiv.org/abs/2106.02433"/>
        <updated>2021-06-07T03:06:13.820Z</updated>
        <summary type="html"><![CDATA[This work presents a practical solution to the problem of call center agent
malpractice. A semi-supervised framework comprising of non-linear power
transformation, neural feature learning and k-means clustering is outlined. We
put these building blocks together and tune the parameters so that the best
performance was obtained. The data used in the experiments is obtained from our
in-house call center. It is made up of recorded agent-customer conversations
which have been annotated using a convolutional neural network based segmenter.
The methods provided a means of tuning the parameters of the neural network to
achieve a desirable result. We show that, using our proposed framework, it is
possible to significantly reduce the malpractice classification error of a
k-means-only clustering model which would serve the same purpose. Additionally,
by presenting the amount of silence per call as a key performance indicator, we
show that the proposed system has enhanced agents performance at our call
center since deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iheme_L/0/1/0/all/0/1"&gt;Leonardo Obinna Iheme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Misinformation from Website Screenshots. (arXiv:2102.07849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07849</id>
        <link href="http://arxiv.org/abs/2102.07849"/>
        <updated>2021-06-07T03:06:13.813Z</updated>
        <summary type="html"><![CDATA[Can the look and the feel of a website give information about the
trustworthiness of an article? In this paper, we propose to use a promising,
yet neglected aspect in detecting the misinformativeness: the overall look of
the domain webpage. To capture this overall look, we take screenshots of news
articles served by either misinformative or trustworthy web domains and
leverage a tensor decomposition based semi-supervised classification technique.
The proposed approach i.e., VizFake is insensitive to a number of image
transformations such as converting the image to grayscale, vectorizing the
image and losing some parts of the screenshots. VizFake leverages a very small
amount of known labels, mirroring realistic and practical scenarios, where
labels (especially for known misinformative articles), are scarce and quickly
become dated. The F1 score of VizFake on a dataset of 50k screenshots of news
articles spanning more than 500 domains is roughly 85% using only 5% of ground
truth labels. Furthermore, tensor representations of VizFake, obtained in an
unsupervised manner, allow for exploratory analysis of the data that provides
valuable insights into the problem. Finally, we compare VizFake with deep
transfer learning, since it is a very popular black-box approach for image
classification and also well-known text text-based methods. VizFake achieves
competitive accuracy with deep transfer learning models while being two orders
of magnitude faster and not requiring laborious hyper-parameter tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1"&gt;Sara Abdali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurav_R/0/1/0/all/0/1"&gt;Rutuja Gurav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1"&gt;Siddharth Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_D/0/1/0/all/0/1"&gt;Daniel Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Entezari_N/0/1/0/all/0/1"&gt;Negin Entezari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1"&gt;Evangelos E. Papalexakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. (arXiv:2106.02170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02170</id>
        <link href="http://arxiv.org/abs/2106.02170"/>
        <updated>2021-06-07T03:06:13.788Z</updated>
        <summary type="html"><![CDATA[Automatic detection of phoneme or word-like units is one of the core
objectives in zero-resource speech processing. Recent attempts employ
self-supervised training methods, such as contrastive predictive coding (CPC),
where the next frame is predicted given past context. However, CPC only looks
at the audio signal's frame-level structure. We overcome this limitation with a
segmental contrastive predictive coding (SCPC) framework that can model the
signal structure at a higher level e.g. at the phoneme level. In this
framework, a convolutional neural network learns frame-level representation
from the raw waveform via noise-contrastive estimation (NCE). A differentiable
boundary detector finds variable-length segments, which are then used to
optimize a segment encoder via NCE to learn segment representations. The
differentiable boundary detector allows us to train frame-level and
segment-level encoders jointly. Typically, phoneme and word segmentation are
treated as separate tasks. We unify them and experimentally show that our
single model outperforms existing phoneme and word segmentation methods on
TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and
when is the right time to include the segmental loss in the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhati_S/0/1/0/all/0/1"&gt;Saurabhchand Bhati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Villalba_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Villalba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\ell_2$-norm Flow Diffusion in Near-Linear Time. (arXiv:2105.14629v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14629</id>
        <link href="http://arxiv.org/abs/2105.14629"/>
        <updated>2021-06-07T03:06:13.772Z</updated>
        <summary type="html"><![CDATA[Diffusion is a fundamental graph procedure and has been a basic building
block in a wide range of theoretical and empirical applications such as graph
partitioning and semi-supervised learning on graphs. In this paper, we study
computationally efficient diffusion primitives beyond random walk.

We design an $\widetilde{O}(m)$-time randomized algorithm for the
$\ell_2$-norm flow diffusion problem, a recently proposed diffusion model based
on network flow with demonstrated graph clustering related applications both in
theory and in practice. Examples include finding locally-biased low conductance
cuts. Using a known connection between the optimal dual solution of the flow
diffusion problem and the local cut structure, our algorithm gives an
alternative approach for finding such cuts in nearly linear time.

From a technical point of view, our algorithm contributes a novel way of
dealing with inequality constraints in graph optimization problems. It adapts
the high-level algorithmic framework of nearly linear time Laplacian system
solvers, but requires several new tools: vertex elimination under constraints,
a new family of graph ultra-sparsifiers, and accelerated proximal gradient
methods with inexact proximal mapping computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1"&gt;Richard Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme sparsity gives rise to functional specialization. (arXiv:2106.02626v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.02626</id>
        <link href="http://arxiv.org/abs/2106.02626"/>
        <updated>2021-06-07T03:06:13.761Z</updated>
        <summary type="html"><![CDATA[Modularity of neural networks -- both biological and artificial -- can be
thought of either structurally or functionally, and the relationship between
these is an open question. We show that enforcing structural modularity via
sparse connectivity between two dense sub-networks which need to communicate to
solve the task leads to functional specialization of the sub-networks, but only
at extreme levels of sparsity. With even a moderate number of interconnections,
the sub-networks become functionally entangled. Defining functional
specialization is in itself a challenging problem without a universally agreed
solution. To address this, we designed three different measures of
specialization (based on weight masks, retraining and correlation) and found
them to qualitatively agree. Our results have implications in both neuroscience
and machine learning. For neuroscience, it shows that we cannot conclude that
there is functional modularity simply by observing moderate levels of
structural modularity: knowing the brain's connectome is not sufficient for
understanding how it breaks down into functional modules. For machine learning,
using structure to promote functional modularity -- which may be important for
robustness and generalization -- may require extremely narrow bottlenecks
between modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bena_G/0/1/0/all/0/1"&gt;Gabriel B&amp;#xe9;na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Goodman_D/0/1/0/all/0/1"&gt;Dan F. M. Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete Graph Representation and Learning via Partial Graph Neural Networks. (arXiv:2003.10130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10130</id>
        <link href="http://arxiv.org/abs/2003.10130"/>
        <updated>2021-06-07T03:06:13.648Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are gaining increasing attention on graph data
learning tasks in recent years. However, in many applications, graph may be
coming in an incomplete form where attributes of graph nodes are partially
unknown/missing. Existing GNNs are generally designed on complete graphs which
can not deal with attribute-incomplete graph data directly. To address this
problem, we develop a novel partial aggregation based GNNs, named Partial Graph
Neural Networks (PaGNNs), for attribute-incomplete graph representation and
learning. Our work is motivated by the observation that the neighborhood
aggregation function in standard GNNs can be equivalently viewed as the
neighborhood reconstruction formulation. Based on it, we define two novel
partial aggregation (reconstruction) functions on incomplete graph and derive
PaGNNs for incomplete graph data learning. Extensive experiments on several
datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL. (arXiv:2106.02193v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02193</id>
        <link href="http://arxiv.org/abs/2106.02193"/>
        <updated>2021-06-07T03:06:13.624Z</updated>
        <summary type="html"><![CDATA[A highly desirable property of a reinforcement learning (RL) agent -- and a
major difficulty for deep RL approaches -- is the ability to generalize
policies learned on a few tasks over a high-dimensional observation space to
similar tasks not seen during training. Many promising approaches to this
challenge consider RL as a process of training two functions simultaneously: a
complex nonlinear encoder that maps high-dimensional observations to a latent
representation space, and a simple linear policy over this space. We posit that
a superior encoder for zero-shot generalization in RL can be trained by using
solely an auxiliary SSL objective if the training process encourages the
encoder to map behaviorally similar observations to similar representations, as
reward-based signal can cause overfitting in the encoder (Raileanu et al.,
2021). We propose Cross-Trajectory Representation Learning (CTRL), a method
that runs within an RL agent and conditions its encoder to recognize behavioral
similarity in observations by applying a novel SSL objective to pairs of
trajectories from the agent's policies. CTRL can be viewed as having the same
effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use
of rewards and associated overfitting risks. Our experiments ablate various
components of CTRL and demonstrate that in combination with PPO it achieves
better generalization performance on the challenging Procgen benchmark suite
(Cobbe et al., 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazoure_B/0/1/0/all/0/1"&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Ahmed M. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacAlpine_P/0/1/0/all/0/1"&gt;Patrick MacAlpine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1"&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1"&gt;Andrey Kolobov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using latent space regression to analyze and leverage compositionality in GANs. (arXiv:2103.10426v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10426</id>
        <link href="http://arxiv.org/abs/2103.10426"/>
        <updated>2021-06-07T03:06:13.443Z</updated>
        <summary type="html"><![CDATA[In recent years, Generative Adversarial Networks have become ubiquitous in
both research and public perception, but how GANs convert an unstructured
latent code to a high quality output is still an open question. In this work,
we investigate regression into the latent space as a probe to understand the
compositional properties of GANs. We find that combining the regressor and a
pretrained generator provides a strong image prior, allowing us to create
composite images from a collage of random image parts at inference time while
maintaining global consistency. To compare compositional properties across
different generators, we measure the trade-offs between reconstruction of the
unrealistic input and image quality of the regenerated samples. We find that
the regression approach enables more localized editing of individual image
parts compared to direct editing in the latent space, and we conduct
experiments to quantify this independence effect. Our method is agnostic to the
semantics of edits, and does not require labels or predefined concepts during
training. Beyond image composition, our method extends to a number of related
applications, such as image inpainting or example-based image editing, which we
demonstrate on several GANs and datasets, and because it uses only a single
forward pass, it can operate in real-time. Code is available on our project
page: https://chail.github.io/latent-composition/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Lucy Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1"&gt;Jonas Wulff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08360</id>
        <link href="http://arxiv.org/abs/2102.08360"/>
        <updated>2021-06-07T03:06:13.428Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increasingly been used as an auxiliary tool in
healthcare applications, due to their ability to improve performance of several
diagnosis tasks. However, these methods are not widely adopted in clinical
settings due to the practical limitations in the reliability, generalizability,
and interpretability of deep learning based systems. As a result, methods have
been developed that impose additional constraints during network training to
gain more control as well as improve interpretabilty, facilitating their
acceptance in healthcare community. In this work, we investigate the benefit of
using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases
from chest X-ray images. The OS constraint can be written as a simple
orthonormality term which is used in conjunction with the standard
cross-entropy loss during classification network training. Previous studies
have demonstrated significant benefits in applying such constraints to deep
learning models. Our findings corroborate these observations, indicating that
the orthonormality loss function effectively produces improved semantic
localization via GradCAM visualizations, enhanced classification performance,
and reduced model calibration error. Our approach achieves an improvement in
accuracy of 1.6% and 4.8% for two- and three-class classification,
respectively; similar results are found for models with data augmentation
applied. In addition to these findings, our work also presents a new
application of the OS regularizer in healthcare, increasing the post-hoc
interpretability and performance of deep learning models for COVID-19
classification to facilitate adoption of these methods in clinical settings. We
also identify the limitations of our strategy that can be explored for further
research in future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Ella Y. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1"&gt;Ankita Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14862</id>
        <link href="http://arxiv.org/abs/2103.14862"/>
        <updated>2021-06-07T03:06:13.409Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object localization (WSOL) is a challenging problem when
given image category labels but requires to learn object localization models.
Optimizing a convolutional neural network (CNN) for classification tends to
activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue. In this paper, we argue that partial
activation is caused by the intrinsic characteristics of CNN, where the
convolution operations produce local receptive fields and experience difficulty
to capture long-range feature dependency among pixels. We introduce the token
semantic coupled attention map (TS-CAM) to take full advantage of the
self-attention mechanism in visual transformer for long-range dependency
extraction. TS-CAM first splits an image into a sequence of patch tokens for
spatial embedding, which produce attention maps of long-range visual dependency
to avoid partial activation. TS-CAM then re-allocates category-related
semantics for patch tokens, enabling each of them to be aware of object
categories. TS-CAM finally couples the patch tokens with the semantic-agnostic
attention map to achieve semantic-aware localization. Experiments on the
ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM
counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1"&gt;Fang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good Practices and A Strong Baseline for Traffic Anomaly Detection. (arXiv:2105.03827v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03827</id>
        <link href="http://arxiv.org/abs/2105.03827"/>
        <updated>2021-06-07T03:06:13.374Z</updated>
        <summary type="html"><![CDATA[The detection of traffic anomalies is a critical component of the intelligent
city transportation management system. Previous works have proposed a variety
of notable insights and taken a step forward in this field, however, dealing
with the complex traffic environment remains a challenge. Moreover, the lack of
high-quality data and the complexity of the traffic scene, motivate us to study
this problem from a hand-crafted perspective. In this paper, we propose a
straightforward and efficient framework that includes pre-processing, a dynamic
track module, and post-processing. With video stabilization, background
modeling, and vehicle detection, the pro-processing phase aims to generate
candidate anomalies. The dynamic tracking module seeks and locates the start
time of anomalies by utilizing vehicle motion patterns and spatiotemporal
status. Finally, we use post-processing to fine-tune the temporal boundary of
anomalies. Not surprisingly, our proposed framework was ranked $1^{st}$ in the
NVIDIA AI CITY 2021 leaderboard for traffic anomaly detection. The code is
available at: https://github.com/Endeavour10020/AICity2021-Anomaly-Detection .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yue He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Knowledge Distillation via Knowledge Alignment and Correlation. (arXiv:2012.00573v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00573</id>
        <link href="http://arxiv.org/abs/2012.00573"/>
        <updated>2021-06-07T03:06:13.351Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation (KD) has become an important technique for model
compression and knowledge transfer. In this work, we first perform a
comprehensive analysis of the knowledge transferred by different KD methods. We
demonstrate that traditional KD methods, which minimize the KL divergence of
softmax outputs between networks, are related to the knowledge alignment of an
individual sample only. Meanwhile, recent contrastive learning-based KD methods
mainly transfer relational knowledge between different samples, namely,
knowledge correlation. While it is important to transfer the full knowledge
from teacher to student, we introduce the Multi-level Knowledge Distillation
(MLKD) by effectively considering both knowledge alignment and correlation.
MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge
from supervised or self-supervised pretrained teachers. We show that MLKD can
improve the reliability and transferability of learned representations.
Experiments demonstrate that MLKD outperforms other state-of-the-art methods on
a large number of experimental settings including different (a) pretraining
strategies (b) network architectures (c) datasets (d) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hongxin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1"&gt;Venkat Krovi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editing Conditional Radiance Fields. (arXiv:2105.06466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06466</id>
        <link href="http://arxiv.org/abs/2105.06466"/>
        <updated>2021-06-07T03:06:13.345Z</updated>
        <summary type="html"><![CDATA[A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richard Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"&gt;Bryan Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Deep Networks with Semantic Data Augmentation. (arXiv:2007.10538v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10538</id>
        <link href="http://arxiv.org/abs/2007.10538"/>
        <updated>2021-06-07T03:06:13.323Z</updated>
        <summary type="html"><![CDATA[Data augmentation is widely known as a simple yet surprisingly effective
technique for regularizing deep networks. Conventional data augmentation
schemes, e.g., flipping, translation or rotation, are low-level,
data-independent and class-agnostic operations, leading to limited diversity
for augmented samples. To this end, we propose a novel semantic data
augmentation algorithm to complement traditional approaches. The proposed
method is inspired by the intriguing property that deep networks are effective
in learning linearized features, i.e., certain directions in the deep feature
space correspond to meaningful semantic transformations, e.g., changing the
background or view angle of an object. Based on this observation, translating
training samples along many such directions in the feature space can
effectively augment the dataset for more diversity. To implement this idea, we
first introduce a sampling based method to obtain semantically meaningful
directions efficiently. Then, an upper bound of the expected cross-entropy (CE)
loss on the augmented training set is derived by assuming the number of
augmented samples goes to infinity, yielding a highly efficient algorithm. In
fact, we show that the proposed implicit semantic data augmentation (ISDA)
algorithm amounts to minimizing a novel robust CE loss, which adds minimal
extra computational cost to a normal training procedure. In addition to
supervised learning, ISDA can be applied to semi-supervised learning tasks
under the consistency regularization framework, where ISDA amounts to
minimizing the upper bound of the expected KL-divergence between the augmented
features and the original features. Although being simple, ISDA consistently
improves the generalization performance of popular deep models (e.g., ResNets
and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,
ImageNet, and Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yitong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete Graph Representation and Learning via Partial Graph Neural Networks. (arXiv:2003.10130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10130</id>
        <link href="http://arxiv.org/abs/2003.10130"/>
        <updated>2021-06-07T03:06:13.317Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are gaining increasing attention on graph data
learning tasks in recent years. However, in many applications, graph may be
coming in an incomplete form where attributes of graph nodes are partially
unknown/missing. Existing GNNs are generally designed on complete graphs which
can not deal with attribute-incomplete graph data directly. To address this
problem, we develop a novel partial aggregation based GNNs, named Partial Graph
Neural Networks (PaGNNs), for attribute-incomplete graph representation and
learning. Our work is motivated by the observation that the neighborhood
aggregation function in standard GNNs can be equivalently viewed as the
neighborhood reconstruction formulation. Based on it, we define two novel
partial aggregation (reconstruction) functions on incomplete graph and derive
PaGNNs for incomplete graph data learning. Extensive experiments on several
datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Attention for Visual Question Answering. (arXiv:2011.02164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02164</id>
        <link href="http://arxiv.org/abs/2011.02164"/>
        <updated>2021-06-07T03:06:13.310Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tanzila Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-06-07T03:06:13.303Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-interactive Dual-decoder for RGB-thermal Salient Object Detection. (arXiv:2005.02315v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02315</id>
        <link href="http://arxiv.org/abs/2005.02315"/>
        <updated>2021-06-07T03:06:13.294Z</updated>
        <summary type="html"><![CDATA[RGB-thermal salient object detection (SOD) aims to segment the common
prominent regions of visible image and corresponding thermal infrared image
that we call it RGBT SOD. Existing methods don't fully explore and exploit the
potentials of complementarity of different modalities and multi-type cues of
image contents, which play a vital role in achieving accurate results. In this
paper, we propose a multi-interactive dual-decoder to mine and model the
multi-type interactions for accurate RGBT SOD. In specific, we first encode two
modalities into multi-level multi-modal feature representations. Then, we
design a novel dual-decoder to conduct the interactions of multi-level
features, two modalities and global contexts. With these interactions, our
method works well in diversely challenging scenarios even in the presence of
invalid modality. Finally, we carry out extensive experiments on public RGBT
and RGBD SOD datasets, and the results show that the proposed method achieves
the outstanding performance against state-of-the-art algorithms. The source
code has been released
at:https://github.com/lz118/Multi-interactive-Dual-decoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_Y/0/1/0/all/0/1"&gt;Yang Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation. (arXiv:2102.01678v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01678</id>
        <link href="http://arxiv.org/abs/2102.01678"/>
        <updated>2021-06-07T03:06:13.277Z</updated>
        <summary type="html"><![CDATA[Suboptimal generalization of machine learning models on unseen data is a key
challenge which hampers the clinical applicability of such models to medical
imaging. Although various methods such as domain adaptation and domain
generalization have evolved to combat this challenge, learning robust and
generalizable representations is core to medical image understanding, and
continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation
for histoPathology), a form of data augmentation based on random style transfer
from non-medical style source such as artistic paintings, for learning
domain-agnostic visual representations in computational pathology. Style
transfer replaces the low-level texture content of an image with the
uninformative style of randomly selected style source image, while preserving
the original high-level semantic content. This improves robustness to domain
shift and can be used as a simple yet powerful tool for learning
domain-agnostic representations. We demonstrate that STRAP leads to
state-of-the-art performance, particularly in the presence of domain shifts, on
two particular classification tasks in computational pathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamashita_R/0/1/0/all/0/1"&gt;Rikiya Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banda_S/0/1/0/all/0/1"&gt;Snikitha Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jeanne Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing CNNs and statistical indicators to improve image classification. (arXiv:2012.11049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11049</id>
        <link href="http://arxiv.org/abs/2012.11049"/>
        <updated>2021-06-07T03:06:13.271Z</updated>
        <summary type="html"><![CDATA[Convolutional Networks have dominated the field of computer vision for the
last ten years, exhibiting extremely powerful feature extraction capabilities
and outstanding classification performance. The main strategy to prolong this
trend relies on further upscaling networks in size. However, costs increase
rapidly while performance improvements may be marginal. We hypothesise that
adding heterogeneous sources of information may be more cost-effective to a CNN
than building a bigger network. In this paper, an ensemble method is proposed
for accurate image classification, fusing automatically detected features
through Convolutional Neural Network architectures with a set of manually
defined statistical indicators. Through a combination of the predictions of a
CNN and a secondary classifier trained on statistical features, better
classification performance can be cheaply achieved. We test multiple learning
algorithms and CNN architectures on a diverse number of datasets to validate
our proposal, making public all our code and data via GitHub. According to our
results, the inclusion of additional indicators and an ensemble classification
approach helps to increase the performance in 8 of 9 datasets, with a
remarkable increase of more than 10% precision in two of them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1"&gt;Javier Huertas-Tato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1"&gt;Alejandro Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Juli&amp;#xe1;n Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1"&gt;David Camacho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction or Comparison: Toward Interpretable Qualitative Reasoning. (arXiv:2106.02399v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02399</id>
        <link href="http://arxiv.org/abs/2106.02399"/>
        <updated>2021-06-07T03:06:13.263Z</updated>
        <summary type="html"><![CDATA[Qualitative relationships illustrate how changing one property (e.g., moving
velocity) affects another (e.g., kinetic energy) and constitutes a considerable
portion of textual knowledge. Current approaches use either semantic parsers to
transform natural language inputs into logical expressions or a "black-box"
model to solve them in one step. The former has a limited application range,
while the latter lacks interpretability. In this work, we categorize
qualitative reasoning tasks into two types: prediction and comparison. In
particular, we adopt neural network modules trained in an end-to-end manner to
simulate the two reasoning processes. Experiments on two qualitative reasoning
question answering datasets, QuaRTz and QuaRel, show our methods' effectiveness
and generalization capability, and the intermediate outputs provided by the
modules make the reasoning process interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Enhanced Rotation Prediction for Self-Supervised Learning. (arXiv:1912.11603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.11603</id>
        <link href="http://arxiv.org/abs/1912.11603"/>
        <updated>2021-06-07T03:06:13.256Z</updated>
        <summary type="html"><![CDATA[The rotation prediction (Rotation) is a simple pretext-task for
self-supervised learning (SSL), where models learn useful representations for
target vision tasks by solving pretext-tasks. Although Rotation captures
information of object shapes, it hardly captures information of textures. To
tackle this problem, we introduce a novel pretext-task called image enhanced
rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and
another pretext-task based on image enhancement (e.g., sharpening and
solarizing) while maintaining simplicity. Through the simultaneous prediction
of rotation and image enhancement, models learn representations to capture the
information of not only object shapes but also textures. Our experimental
results show that IE-Rot models outperform Rotation on various standard
benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO
detection/segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shioda_T/0/1/0/all/0/1"&gt;Tetsuya Shioda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04960</id>
        <link href="http://arxiv.org/abs/2009.04960"/>
        <updated>2021-06-07T03:06:13.249Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a challenging task, which aims to learn a classifier for
novel classes with few examples. Pre-training based meta-learning methods
effectively tackle the problem by pre-training a feature extractor and then
fine-tuning it through the nearest centroid based meta-learning. However,
results show that the fine-tuning step makes very marginal improvements. In
this paper, 1) we figure out the key reason, i.e., in the pre-trained feature
space, the base classes already form compact clusters while novel classes
spread as groups with large variances, which implies that fine-tuning the
feature extractor is less meaningful; 2) instead of fine-tuning the feature
extractor, we focus on estimating more representative prototypes during
meta-learning. Consequently, we propose a novel prototype completion based
meta-learning framework. This framework first introduces primitive knowledge
(i.e., class-level part or attribute annotations) and extracts representative
attribute features as priors. Then, we design a prototype completion network to
learn to complete prototypes with these priors. To avoid the prototype
completion error caused by primitive knowledge noises or class differences, we
further develop a Gaussian based prototype fusion strategy that combines the
mean-based and completed prototypes by exploiting the unlabeled samples.
Extensive experiments show that our method: (i) can obtain more accurate
prototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of
classification accuracy. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhichao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lisai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-06-07T03:06:13.231Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,
CelebA, and ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss. (arXiv:2011.07189v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07189</id>
        <link href="http://arxiv.org/abs/2011.07189"/>
        <updated>2021-06-07T03:06:13.225Z</updated>
        <summary type="html"><![CDATA[RGBT tracking has attracted increasing attention since RGB and thermal
infrared data have strong complementary advantages, which could make trackers
all-day and all-weather work. However, how to effectively represent RGBT data
for visual tracking remains unstudied well. Existing works usually focus on
extracting modality-shared or modality-specific information, but the potentials
of these two cues are not well explored and exploited in RGBT tracking. In this
paper, we propose a novel multi-adapter network to jointly perform
modality-shared, modality-specific and instance-aware target representation
learning for RGBT tracking. To this end, we design three kinds of adapters
within an end-to-end deep learning framework. In specific, we use the modified
VGG-M as the generality adapter to extract the modality-shared target
representations.To extract the modality-specific features while reducing the
computational complexity, we design a modality adapter, which adds a small
block to the generality adapter in each layer and each modality in a parallel
manner. Such a design could learn multilevel modality-specific representations
with a modest number of parameters as the vast majority of parameters are
shared with the generality adapter. We also design instance adapter to capture
the appearance properties and temporal variations of a certain target.
Moreover, to enhance the shared and specific features, we employ the loss of
multiple kernel maximum mean discrepancy to measure the distribution divergence
of different modal features and integrate it into each layer for more robust
representation learning. Extensive experiments on two RGBT tracking benchmark
datasets demonstrate the outstanding performance of the proposed tracker
against the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1"&gt;Andong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuqing Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1"&gt;Bin Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reactive Human-to-Robot Handovers of Arbitrary Objects. (arXiv:2011.08961v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08961</id>
        <link href="http://arxiv.org/abs/2011.08961"/>
        <updated>2021-06-07T03:06:13.218Z</updated>
        <summary type="html"><![CDATA[Human-robot object handovers have been an actively studied area of robotics
over the past decade; however, very few techniques and systems have addressed
the challenge of handing over diverse objects with arbitrary appearance, size,
shape, and rigidity. In this paper, we present a vision-based system that
enables reactive human-to-robot handovers of unknown objects. Our approach
combines closed-loop motion planning with real-time, temporally-consistent
grasp generation to ensure reactivity and motion smoothness. Our system is
robust to different object positions and orientations, and can grasp both rigid
and non-rigid objects. We demonstrate the generalizability, usability, and
robustness of our approach on a novel benchmark set of 26 diverse household
objects, a user study with naive users (N=6) handing over a subset of 15
objects, and a systematic evaluation examining different ways of handing
objects. More results and videos can be found at
https://sites.google.com/nvidia.com/handovers-of-arbitrary-objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1"&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cakmak_M/0/1/0/all/0/1"&gt;Maya Cakmak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:13.212Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images. (arXiv:2008.05742v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05742</id>
        <link href="http://arxiv.org/abs/2008.05742"/>
        <updated>2021-06-07T03:06:13.206Z</updated>
        <summary type="html"><![CDATA[This paper focuses on the challenging task of learning 3D object surface
reconstructions from RGB images. Existingmethods achieve varying degrees of
success by using different surface representations. However, they all have
their own drawbacks,and cannot properly reconstruct the surface shapes of
complex topologies, arguably due to a lack of constraints on the
topologicalstructures in their learning frameworks. To this end, we propose to
learn and use the topology-preserved, skeletal shape representationto assist
the downstream task of object surface reconstruction from RGB images.
Technically, we propose the novelSkeletonNetdesign that learns a volumetric
representation of a skeleton via a bridged learning of a skeletal point set,
where we use paralleldecoders each responsible for the learning of points on 1D
skeletal curves and 2D skeletal sheets, as well as an efficient module
ofglobally guided subvolume synthesis for a refined, high-resolution skeletal
volume; we present a differentiablePoint2Voxellayer tomake SkeletonNet
end-to-end and trainable. With the learned skeletal volumes, we propose two
models, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the
Skeleton-Regularized Deep Implicit Surface Network (SkeDISN), which
respectivelybuild upon and improve over the existing frameworks of explicit
mesh deformation and implicit field learning for the downstream
surfacereconstruction task. We conduct thorough experiments that verify the
efficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing
methods as well, and they have their own merits when measured by different
metrics. Additional results ingeneralized task settings further demonstrate the
usefulness of our proposed methods. We have made both our implementation
codeand the ShapeNet-Skeleton dataset publicly available at ble at
https://github.com/tangjiapeng/SkeletonNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiapeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Semi-Supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency. (arXiv:2012.07042v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07042</id>
        <link href="http://arxiv.org/abs/2012.07042"/>
        <updated>2021-06-07T03:06:13.197Z</updated>
        <summary type="html"><![CDATA[Gross Target Volume (GTV) segmentation plays an irreplaceable role in
radiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that
Convolutional Neural Networks (CNN) have achieved good performance for this
task, they rely on a large set of labeled images for training, which is
expensive and time-consuming to acquire. In this paper, we propose a novel
framework with Uncertainty Rectified Pyramid Consistency (URPC) regularization
for semi-supervised NPC GTV segmentation. Concretely, we extend a backbone
segmentation network to produce pyramid predictions at different scales. The
pyramid predictions network (PPNet) is supervised by the ground truth of
labeled images and a multi-scale consistency loss for unlabeled images,
motivated by the fact that prediction at different scales for the same input
should be similar and consistent. However, due to the different resolution of
these predictions, encouraging them to be consistent at each pixel directly has
low robustness and may lose some fine details. To address this problem, we
further design a novel uncertainty rectifying module to enable the framework to
gradually learn from meaningful and reliable consensual regions at different
scales. Experimental results on a dataset with 258 NPC MR images showed that
with only 10% or 20% images labeled, our method largely improved the
segmentation performance by leveraging the unlabeled images, and it also
outperformed five state-of-the-art semi-supervised segmentation methods.
Moreover, when only 50% images labeled, URPC achieved an average Dice score of
82.74% that was close to fully supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wenjun Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jieneng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1"&gt;Tao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nianyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guotai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06175</id>
        <link href="http://arxiv.org/abs/2103.06175"/>
        <updated>2021-06-07T03:06:13.180Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junguang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yifei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yufeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape Modeling and Reconstruction from Raw Point Clouds. (arXiv:2012.07498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07498</id>
        <link href="http://arxiv.org/abs/2012.07498"/>
        <updated>2021-06-07T03:06:13.173Z</updated>
        <summary type="html"><![CDATA[Shape modeling and reconstruction from raw point clouds of objects stand as a
fundamental challenge in vision and graphics research. Classical methods
consider analytic shape priors; however, their performance degraded when the
scanned points deviate from the ideal conditions of cleanness and completeness.
Important progress has been recently made by data-driven approaches, which
learn global and/or local models of implicit surface representations from
auxiliary sets of training shapes. Motivated from a universal phenomenon that
self-similar shape patterns of local surface patches repeat across the entire
surface of an object, we aim to push forward the data-driven strategies and
propose to learn a local implicit surface network for a shared, adaptive
modeling of the entire surface for a direct surface reconstruction from raw
point cloud; we also enhance the leveraging of surface self-similarities by
improving correlations among the optimized latent codes of individual surface
patches. Given that orientations of raw points could be unavailable or noisy,
we extend sign agnostic learning into our local implicit model, which enables
our recovery of signed implicit fields of local surfaces from the unsigned
inputs. We term our framework as Sign-Agnostic Implicit Learning of Surface
Self-Similarities (SAIL-S3). With a global post-optimization of local sign
flipping, SAIL-S3 is able to directly model raw, un-oriented point clouds and
reconstruct high-quality object surfaces. Experiments show its superiority over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenbin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jiabao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yuxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Training of Convolutional Neural Networks with Low-bitwidth Weights and Activations. (arXiv:1908.04680v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.04680</id>
        <link href="http://arxiv.org/abs/1908.04680"/>
        <updated>2021-06-07T03:06:13.167Z</updated>
        <summary type="html"><![CDATA[This paper tackles the problem of training a deep convolutional neural
network of both low-bitwidth weights and activations. Optimizing a
low-precision network is very challenging due to the non-differentiability of
the quantizer, which may result in substantial accuracy loss. To address this,
we propose three practical approaches, including (i) progressive quantization;
(ii) stochastic precision; and (iii) joint knowledge distillation to improve
the network training. First, for progressive quantization, we propose two
schemes to progressively find good local minima. Specifically, we propose to
first optimize a net with quantized weights and subsequently quantize
activations. This is in contrast to the traditional methods which optimize them
simultaneously. Furthermore, we propose a second progressive quantization
scheme which gradually decreases the bit-width from high-precision to
low-precision during training. Second, to alleviate the excessive training
burden due to the multi-round training stages, we further propose a one-stage
stochastic precision strategy to randomly sample and quantize sub-networks
while keeping other parts in full-precision. Finally, we adopt a novel learning
scheme to jointly train a full-precision model alongside the low-precision one.
By doing so, the full-precision model provides hints to guide the low-precision
model training and significantly improves the performance of the low-precision
network. Extensive experiments on various datasets (e.g., CIFAR-100, ImageNet)
show the effectiveness of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signed Distance Function Computation from an Implicit Surface. (arXiv:2104.08057v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08057</id>
        <link href="http://arxiv.org/abs/2104.08057"/>
        <updated>2021-06-07T03:06:13.160Z</updated>
        <summary type="html"><![CDATA[We describe in this short note a technique to convert an implicit surface
into a Signed Distance Function (SDF) while exactly preserving the zero
level-set of the implicit. The proposed approach relies on embedding the input
implicit in the final layer of a neural network, which is trained to minimize a
loss function characterizing the SDF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fayolle_P/0/1/0/all/0/1"&gt;Pierre-Alain Fayolle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly- and Semi-Supervised Probabilistic Segmentation and Quantification of Ultrasound Needle-Reverberation Artifacts to Allow Better AI Understanding of Tissue Beneath Needles. (arXiv:2011.11958v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11958</id>
        <link href="http://arxiv.org/abs/2011.11958"/>
        <updated>2021-06-07T03:06:13.154Z</updated>
        <summary type="html"><![CDATA[Ultrasound image quality has continually been improving. However, when
needles or other metallic objects are operating inside the tissue, the
resulting reverberation artifacts can severely corrupt the surrounding image
quality. Such effects are challenging for existing computer vision algorithms
for medical image analysis. Needle reverberation artifacts can be hard to
identify at times and affect various pixel values to different degrees. The
boundaries of such artifacts are ambiguous, leading to disagreement among human
experts labeling the artifacts. We propose a weakly- and semi-supervised,
probabilistic needle-and-reverberation-artifact segmentation algorithm to
separate the desired tissue-based pixel values from the superimposed artifacts.
Our method models the intensity decay of artifact intensities and is designed
to minimize the human labeling error. We demonstrate the applicability of the
approach and compare it against other segmentation algorithms. Our method is
capable of differentiating between the reverberations from artifact-free
patches as well as of modeling the intensity fall-off in the artifacts. Our
method matches state-of-the-art artifact segmentation performance and sets a
new standard in estimating the per-pixel contributions of artifact vs
underlying anatomy, especially in the immediately adjacent regions between
reverberation lines. Our algorithm is also able to improve the performance
downstream image analysis algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1"&gt;Alex Ling Yu Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1"&gt;Edward Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1"&gt;John Galeotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Descriptor Enhancement via Self-Labelling Triplets for Visual Data Association. (arXiv:2011.10471v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10471</id>
        <link href="http://arxiv.org/abs/2011.10471"/>
        <updated>2021-06-07T03:06:13.136Z</updated>
        <summary type="html"><![CDATA[Object-level data association is central to robotic applications such as
tracking-by-detection and object-level simultaneous localization and mapping.
While current learned visual data association methods outperform hand-crafted
algorithms, many rely on large collections of domain-specific training examples
that can be difficult to obtain without prior knowledge. Additionally, such
methods often remain fixed during inference-time and do not harness observed
information to better their performance. We propose a self-supervised method
for incrementally refining visual descriptors to improve performance in the
task of object-level visual data association. Our method optimizes deep
descriptor generators online, by continuously training a widely available image
classification network pre-trained with domain-independent data. We show that
earlier layers in the network outperform later-stage layers for the data
association task while also allowing for a 94% reduction in the number of
parameters, enabling the online optimization. We show that self-labelling
challenging triplets--choosing positive examples separated by large temporal
distances and negative examples close in the descriptor space--improves the
quality of the learned descriptors for the multi-object tracking task. Finally,
we demonstrate that our approach surpasses other visual data-association
methods applied to a tracking-by-detection task, and show that it provides
better performance-gains when compared to other methods that attempt to adapt
to observed information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaoul_Y/0/1/0/all/0/1"&gt;Yorai Shaoul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Katherine Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ok_K/0/1/0/all/0/1"&gt;Kyel Ok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1"&gt;Nicholas Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepActsNet: Spatial and Motion features from Face, Hands, and Body Combined with Convolutional and Graph Networks for Improved Action Recognition. (arXiv:2009.09818v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09818</id>
        <link href="http://arxiv.org/abs/2009.09818"/>
        <updated>2021-06-07T03:06:13.130Z</updated>
        <summary type="html"><![CDATA[Existing action recognition methods mainly focus on joint and bone
information in human body skeleton data due to its robustness to complex
backgrounds and dynamic characteristics of the environments. In this paper, we
combine body skeleton data with spatial and motion features from face and two
hands, and present "Deep Action Stamps (DeepActs)", a novel data representation
to encode actions from video sequences. We also present "DeepActsNet", a deep
learning based ensemble model which learns convolutional and structural
features from Deep Action Stamps for highly accurate action recognition.
Experiments on three challenging action recognition datasets (NTU60, NTU120,
and SYSU) show that the proposed model trained using Deep Action Stamps produce
considerable improvements in the action recognition accuracy with less
computational cost compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asif_U/0/1/0/all/0/1"&gt;Umar Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1"&gt;Deval Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallar_S/0/1/0/all/0/1"&gt;Stefan von Cavallar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jianbin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrer_S/0/1/0/all/0/1"&gt;Stefan Harrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction. (arXiv:2006.03340v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03340</id>
        <link href="http://arxiv.org/abs/2006.03340"/>
        <updated>2021-06-07T03:06:13.124Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles are expected to drive in complex scenarios with several
independent non cooperating agents. Path planning for safely navigating in such
environments can not just rely on perceiving present location and motion of
other agents. It requires instead to predict such variables in a far enough
future. In this paper we address the problem of multimodal trajectory
prediction exploiting a Memory Augmented Neural Network. Our method learns past
and future trajectory embeddings using recurrent neural networks and exploits
an associative external memory to store and retrieve such embeddings.
Trajectory prediction is then performed by decoding in-memory future encodings
conditioned with the observed past. We incorporate scene knowledge in the
decoding state by learning a CNN on top of semantic scene maps. Memory growth
is limited by learning a writing controller based on the predictive capability
of existing embeddings. We show that our method is able to natively perform
multi-modal trajectory prediction obtaining state-of-the art results on three
datasets. Moreover, thanks to the non-parametric nature of the memory module,
we show how once trained our system can continuously improve by ingesting novel
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchetti_F/0/1/0/all/0/1"&gt;Francesco Marchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1"&gt;Federico Becattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1"&gt;Lorenzo Seidenari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Domain Invariant Features for Depth Estimation. (arXiv:2106.02594v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02594</id>
        <link href="http://arxiv.org/abs/2106.02594"/>
        <updated>2021-06-07T03:06:13.117Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of unsupervised synthetic-to-realistic domain
adaptation for single image depth estimation. An essential building block of
single image depth estimation is an encoder-decoder task network that takes RGB
images as input and produces depth maps as output. In this paper, we propose a
novel training strategy to force the task network to learn domain invariant
representations in a self-supervised manner. Specifically, we extend
self-supervised learning from traditional representation learning, which works
on images from a single domain, to domain invariant representation learning,
which works on images from two different domains by utilizing an image-to-image
translation network. Firstly, we use our bidirectional image-to-image
translation network to transfer domain-specific styles between synthetic and
real domains. This style transfer operation allows us to obtain similar images
from the different domains. Secondly, we jointly train our task network and
Siamese network with the same images from the different domains to obtain
domain invariance for the task network. Finally, we fine-tune the task network
using labeled synthetic and unlabeled real-world data. Our training strategy
yields improved generalization capability in the real-world domain. We carry
out an extensive evaluation on two popular datasets for depth estimation, KITTI
and Make3D. The results demonstrate that our proposed method outperforms the
state-of-the-art both qualitatively and quantitatively. The source code and
model weights will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akada_H/0/1/0/all/0/1"&gt;Hiroyasu Akada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhashim_I/0/1/0/all/0/1"&gt;Ibraheem Alhashim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model. (arXiv:2106.02566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02566</id>
        <link href="http://arxiv.org/abs/2106.02566"/>
        <updated>2021-06-07T03:06:13.109Z</updated>
        <summary type="html"><![CDATA[The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
`active level' of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. It is also more accurate, faster, and with a smaller memory footprint
than usual neural attention modules. Extensive experiments showcase more
comprehensive visual explanations compared to the state-of-the-art
visualization model across multiple tasks including few-shot classification,
person re-identification, fine-grained image classification. The proposed
visualization model sheds imperative light on how neural networks `pay their
attention' differently in different tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1"&gt;Tristan Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Suiyi Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1"&gt;Thomas Fr&amp;#xe9;our&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1"&gt;Harold Mouch&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedded Deep Regularized Block HSIC Thermomics for Early Diagnosis of Breast Cancer. (arXiv:2106.02106v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02106</id>
        <link href="http://arxiv.org/abs/2106.02106"/>
        <updated>2021-06-07T03:06:13.084Z</updated>
        <summary type="html"><![CDATA[Thermography has been used extensively as a complementary diagnostic tool in
breast cancer detection. Among thermographic methods matrix factorization (MF)
techniques show an unequivocal capability to detect thermal patterns
corresponding to vasodilation in cancer cases. One of the biggest challenges in
such techniques is selecting the best representation of the thermal basis. In
this study, an embedding method is proposed to address this problem and
Deep-semi-nonnegative matrix factorization (Deep-SemiNMF) for thermography is
introduced, then tested for 208 breast cancer screening cases. First, we apply
Deep-SemiNMF to infrared images to extract low-rank thermal representations for
each case. Then, we embed low-rank bases to obtain one basis for each patient.
After that, we extract 300 thermal imaging features, called thermomics, to
decode imaging information for the automatic diagnostic model. We reduced the
dimensionality of thermomics by spanning them onto Hilbert space using RBF
kernel and select the three most efficient features using the block Hilbert
Schmidt Independence Criterion Lasso (block HSIC Lasso). The preserved thermal
heterogeneity successfully classified asymptomatic versus symptomatic patients
applying a random forest model (cross-validated accuracy of 71.36%
(69.42%-73.3%)).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yousefi_B/0/1/0/all/0/1"&gt;Bardia Yousefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharifipour_H/0/1/0/all/0/1"&gt;Hossein Memarzadeh Sharifipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maldague_X/0/1/0/all/0/1"&gt;Xavier P.V. Maldague&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Methods for Vessel Trajectory Prediction based on Recurrent Neural Networks. (arXiv:2101.02486v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02486</id>
        <link href="http://arxiv.org/abs/2101.02486"/>
        <updated>2021-06-07T03:06:13.075Z</updated>
        <summary type="html"><![CDATA[Data-driven methods open up unprecedented possibilities for maritime
surveillance using Automatic Identification System (AIS) data. In this work, we
explore deep learning strategies using historical AIS observations to address
the problem of predicting future vessel trajectories with a prediction horizon
of several hours. We propose novel sequence-to-sequence vessel trajectory
prediction models based on encoder-decoder recurrent neural networks (RNNs)
that are trained on historical trajectory data to predict future trajectory
samples given previous observations. The proposed architecture combines Long
Short-Term Memory (LSTM) RNNs for sequence modeling to encode the observed data
and generate future predictions with different intermediate aggregation layers
to capture space-time dependencies in sequential data. Experimental results on
vessel trajectories from an AIS dataset made freely available by the Danish
Maritime Authority show the effectiveness of deep-learning methods for
trajectory prediction based on sequence-to-sequence neural networks, which
achieve better performance than baseline approaches based on linear regression
or on the Multi-Layer Perceptron (MLP) architecture. The comparative evaluation
of results shows: i) the superiority of attention pooling over static pooling
for the specific application, and ii) the remarkable performance improvement
that can be obtained with labeled trajectories, i.e., when predictions are
conditioned on a low-level context representation encoded from the sequence of
past observations, as well as on additional inputs (e.g., port of departure or
arrival) about the vessel's high-level intention, which may be available from
AIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Capobianco_S/0/1/0/all/0/1"&gt;Samuele Capobianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millefiori_L/0/1/0/all/0/1"&gt;Leonardo M. Millefiori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forti_N/0/1/0/all/0/1"&gt;Nicola Forti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braca_P/0/1/0/all/0/1"&gt;Paolo Braca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_P/0/1/0/all/0/1"&gt;Peter Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Fixed-Points in Recurrent Neural Networks. (arXiv:2106.02417v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02417</id>
        <link href="http://arxiv.org/abs/2106.02417"/>
        <updated>2021-06-07T03:06:13.067Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks are widely used in speech and language processing.
Due to dependency on the past, standard algorithms for training these models,
such as back-propagation through time (BPTT), cannot be efficiently
parallelised. Furthermore, applying these models to more complex structures
than sequences requires inference time approximations, which introduce
inconsistency between inference and training. This paper shows that recurrent
neural networks can be reformulated as fixed-points of non-linear equation
systems. These fixed-points can be computed using an iterative algorithm
exactly and in as many iterations as the length of any given sequence. Each
iteration of this algorithm adds one additional Markovian-like order of
dependencies such that upon termination all dependencies modelled by the
recurrent neural networks have been incorporated. Although exact fixed-points
inherit the same parallelization and inconsistency issues, this paper shows
that approximate fixed-points can be computed in parallel and used consistently
in training and inference including tasks such as lattice rescoring.
Experimental validation is performed in two tasks, Penn Tree Bank and
WikiText-2, and shows that approximate fixed-points yield competitive
prediction performance to recurrent neural networks trained using the BPTT
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ragni_A/0/1/0/all/0/1"&gt;Anton Ragni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation for LiDAR Perception. (arXiv:2106.02377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02377</id>
        <link href="http://arxiv.org/abs/2106.02377"/>
        <updated>2021-06-07T03:06:13.061Z</updated>
        <summary type="html"><![CDATA[Scalable systems for automated driving have to reliably cope with an
open-world setting. This means, the perception systems are exposed to drastic
domain shifts, like changes in weather conditions, time-dependent aspects, or
geographic regions. Covering all domains with annotated data is impossible
because of the endless variations of domains and the time-consuming and
expensive annotation process. Furthermore, fast development cycles of the
system additionally introduce hardware changes, such as sensor types and
vehicle setups, and the required knowledge transfer from simulation. To enable
scalable automated driving, it is therefore crucial to address these domain
shifts in a robust and efficient manner. Over the last years, a vast amount of
different domain adaptation techniques evolved. There already exists a number
of survey papers for domain adaptation on camera images, however, a survey for
LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated
driving that provides detailed 3D scans of the vehicle's surroundings. To
stimulate future research, this paper presents a comprehensive review of recent
progress in domain adaptation methods and formulates interesting research
questions specifically targeted towards LiDAR perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1"&gt;Larissa T. Triess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1"&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1"&gt;Christoph B. Rist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1"&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation. (arXiv:2004.03238v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03238</id>
        <link href="http://arxiv.org/abs/2004.03238"/>
        <updated>2021-06-07T03:06:13.054Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) models for reading comprehension have achieved
human-level accuracy on in-distribution test sets. However, they have been
demonstrated to lack robustness to challenge sets, whose distribution is
different from that of training sets. Existing data augmentation methods
mitigate this problem by simply augmenting training sets with synthetic
examples sampled from the same distribution as the challenge sets. However,
these methods assume that the distribution of a challenge set is known a
priori, making them less applicable to unseen challenge sets. In this study, we
focus on question-answer pair generation (QAG) to mitigate this problem. While
most existing QAG methods aim to improve the quality of synthetic examples, we
conjecture that diversity-promoting QAG can mitigate the sparsity of training
sets and lead to better robustness. We present a variational QAG model that
generates multiple diverse QA pairs from a paragraph. Our experiments show that
our method can improve the accuracy of 12 challenge sets, as well as the
in-distribution accuracy. Our code and data are available at
https://github.com/KazutoshiShinoda/VQAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1"&gt;Kazutoshi Shinoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals. (arXiv:2106.02118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02118</id>
        <link href="http://arxiv.org/abs/2106.02118"/>
        <updated>2021-06-07T03:06:13.030Z</updated>
        <summary type="html"><![CDATA[Importance: An artificial intelligence (AI)-based model to predict COVID-19
likelihood from chest x-ray (CXR) findings can serve as an important adjunct to
accelerate immediate clinical decision making and improve clinical decision
making. Despite significant efforts, many limitations and biases exist in
previously developed AI diagnostic models for COVID-19. Utilizing a large set
of local and international CXR images, we developed an AI model with high
performance on temporal and external validation.

Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,
but not replacement, for clinical decision support of COVID-19 diagnosis, which
largely hinges on exposure history, signs, and symptoms. While AI-based tools
have not yet reached full diagnostic potential in COVID-19, they may still
offer valuable information to clinicians taken into consideration along with
clinical signs and symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adila_D/0/1/0/all/0/1"&gt;Dyah Adila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zach Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ingraham_N/0/1/0/all/0/1"&gt;Nicholas Ingraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murray_E/0/1/0/all/0/1"&gt;Eric Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boley_D/0/1/0/all/0/1"&gt;Daniel Boley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Switzer_S/0/1/0/all/0/1"&gt;Sean Switzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allen_T/0/1/0/all/0/1"&gt;Tadashi Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steenburg_S/0/1/0/all/0/1"&gt;Scott D. Steenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kummerfeld_E/0/1/0/all/0/1"&gt;Erich Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher Tignanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching keyword spotters to spot new keywords with limited examples. (arXiv:2106.02443v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02443</id>
        <link href="http://arxiv.org/abs/2106.02443"/>
        <updated>2021-06-07T03:06:13.022Z</updated>
        <summary type="html"><![CDATA[Learning to recognize new keywords with just a few examples is essential for
personalizing keyword spotting (KWS) models to a user's choice of keywords.
However, modern KWS models are typically trained on large datasets and
restricted to a small vocabulary of keywords, limiting their transferability to
a broad range of unseen keywords. Towards easily customizable KWS models, we
present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained
on the task of recognizing a large number of keywords. Speech representations
offered by KeySEM are highly effective for learning new keywords from a limited
number of examples. Comparisons with a diverse range of related work across
several datasets show that our method achieves consistently superior
performance with fewer training examples. Although KeySEM was pre-trained only
on English utterances, the performance gains also extend to datasets from four
other languages indicating that KeySEM learns useful representations well
aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's
ability to learn new keywords sequentially without requiring to re-train on
previously learned keywords. Our experimental observations suggest that KeySEM
is well suited to on-device environments where post-deployment learning and
ease of customization are often desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kilgour_K/0/1/0/all/0/1"&gt;Kevin Kilgour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rom_H/0/1/0/all/0/1"&gt;Hassan Rom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00164</id>
        <link href="http://arxiv.org/abs/2105.00164"/>
        <updated>2021-06-07T03:06:13.015Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) systems have been proven to be vulnerable
to backdoor attacks, whereby hidden features (backdoors) are trained into a
language model and may only be activated by specific inputs (called triggers),
to trick the model into producing unexpected behaviors. In this paper, we
create covert and natural triggers for textual backdoor attacks, \textit{hidden
backdoors}, where triggers can fool both modern language models and human
inspection. We deploy our hidden backdoors through two state-of-the-art trigger
embedding methods. The first approach via homograph replacement, embeds the
trigger into deep neural networks through the visual spoofing of lookalike
character replacement. The second approach uses subtle differences between text
generated by language models and real natural text to produce trigger sentences
with correct grammar and high fluency. We demonstrate that the proposed hidden
backdoors can be effective across three downstream security-critical NLP tasks,
representative of modern human-centric NLP systems, including toxic comment
detection, neural machine translation (NMT), and question answering (QA). Our
two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at
least $97\%$ with an injection rate of only $3\%$ in toxic comment detection,
$95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$
ASR against QA updated with only 27 poisoning data samples on a model
previously trained with 92,024 samples (0.029\%). We are able to demonstrate
the adversary's high success rate of attacks, while maintaining functionality
for regular users, with triggers inconspicuous by the human administrators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaofeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1"&gt;Tian Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Benjamin Zi Hao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Minhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Haojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jialiang Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Procedural World Generation Framework for Systematic Evaluation of Continual Learning. (arXiv:2106.02585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02585</id>
        <link href="http://arxiv.org/abs/2106.02585"/>
        <updated>2021-06-07T03:06:13.009Z</updated>
        <summary type="html"><![CDATA[Several families of continual learning techniques have been proposed to
alleviate catastrophic interference in deep neural network training on
non-stationary data. However, a comprehensive comparison and analysis of
limitations remains largely open due to the inaccessibility to suitable
datasets. Empirical examination not only varies immensely between individual
works, it further currently relies on contrived composition of benchmarks
through subdivision and concatenation of various prevalent static vision
datasets. In this work, our goal is to bridge this gap by introducing a
computer graphics simulation framework that repeatedly renders only upcoming
urban scene fragments in an endless real-time procedural world generation
process. At its core lies a modular parametric generative model with adaptable
generative factors. The latter can be used to flexibly compose data streams,
which significantly facilitates a detailed analysis and allows for effortless
investigation of various continual learning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Timm Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1"&gt;Martin Mundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1"&gt;Iuliia Pliushch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Visvanathan Ramesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02581</id>
        <link href="http://arxiv.org/abs/2106.02581"/>
        <updated>2021-06-07T03:06:13.003Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis can provide a suitable lead for the tools used in software
engineering along with the API recommendation systems and relevant libraries to
be used. In this context, the existing tools like SentiCR, SentiStrength-SE,
etc. exhibited low f1-scores that completely defeats the purpose of deployment
of such strategies, thereby there is enough scope of performance improvement.
Recent advancements show that transformer based pre-trained models (e.g., BERT,
RoBERTa, ALBERT, etc.) have displayed better results in the text classification
task. Following this context, the present research explores different
BERT-based models to analyze the sentences in GitHub comments, Jira comments,
and Stack Overflow posts. The paper presents three different strategies to
analyse BERT based model for sentiment analysis, where in the first strategy
the BERT based pre-trained models are fine-tuned; in the second strategy an
ensemble model is developed from BERT variants; and in the third strategy a
compressed model (Distil BERT) is used. The experimental results show that the
BERT based ensemble approach and the compressed BERT model attain improvements
by 6-12% over prevailing tools for the F1 measure on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1"&gt;Himanshu Batra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:12.981Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Driven Road Maintenance Inspection. (arXiv:2106.02567v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02567</id>
        <link href="http://arxiv.org/abs/2106.02567"/>
        <updated>2021-06-07T03:06:12.953Z</updated>
        <summary type="html"><![CDATA[Road infrastructure maintenance inspection is typically a labour-intensive
and critical task to ensure the safety of all the road users. In this work, we
propose a detailed methodology to use state-of-the-art techniques in artificial
intelligence and computer vision to automate a sizeable portion of the
maintenance inspection subtasks and reduce the labour costs. The proposed
methodology uses state-of-the-art computer vision techniques such as object
detection and semantic segmentation to automate inspections on primary road
structures such as the road surface, markings, barriers (guardrails) and
traffic signs. The models are mostly trained on commercially viable datasets
and augmented with proprietary data. We demonstrate that our AI models can not
only automate and scale maintenance inspections on primary road structures but
also result in higher recall compared to traditional manual inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1"&gt;Ratnajit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haris Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badar_A/0/1/0/all/0/1"&gt;Ahmed Badar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouns_T/0/1/0/all/0/1"&gt;Terence Brouns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose and Semantic Map Based Probabilistic Forecast of Vulnerable Road Users' Trajectories. (arXiv:2106.02598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02598</id>
        <link href="http://arxiv.org/abs/2106.02598"/>
        <updated>2021-06-07T03:06:12.938Z</updated>
        <summary type="html"><![CDATA[In this article, an approach for probabilistic trajectory forecasting of
vulnerable road users (VRUs) is presented, which considers past movements and
the surrounding scene. Past movements are represented by 3D poses reflecting
the posture and movements of individual body parts. The surrounding scene is
modeled in the form of semantic maps showing, e.g., the course of streets,
sidewalks, and the occurrence of obstacles. The forecasts are generated in
grids discretizing the space and in the form of arbitrary discrete probability
distributions. The distributions are evaluated in terms of their reliability,
sharpness, and positional accuracy. We compare our method with an approach that
provides forecasts in the form of Gaussian distributions and discuss the
respective advantages and disadvantages. Thereby, we investigate the impact of
using poses and semantic maps. With a technique called spatial label smoothing,
our approach achieves reliable forecasts. Overall, the poses have a positive
impact on the forecasts. The semantic maps offer the opportunity to adapt the
probability distributions to the individual situation, although at the
considered forecasted time horizon of 2.52 s they play a minor role compared to
the past movements of the VRU. Our method is evaluated on a dataset recorded in
inner-city traffic using a research vehicle. The dataset is made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1"&gt;Viktor Kress&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeske_F/0/1/0/all/0/1"&gt;Fabian Jeske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1"&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1"&gt;Konrad Doll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks. (arXiv:2106.02599v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02599</id>
        <link href="http://arxiv.org/abs/2106.02599"/>
        <updated>2021-06-07T03:06:12.928Z</updated>
        <summary type="html"><![CDATA[There is a growing demand for high-resolution (HR) medical images in both the
clinical and research applications. Image quality is inevitably traded off with
the acquisition time for better patient comfort, lower examination costs, dose,
and fewer motion-induced artifacts. For many image-based tasks, increasing the
apparent resolution in the perpendicular plane to produce multi-planar
reformats or 3D images is commonly used. Single image super-resolution (SR) is
a promising technique to provide HR images based on unsupervised learning to
increase resolution of a 2D image, but there are few reports on 3D SR. Further,
perceptual loss is proposed in the literature to better capture the textual
details and edges than using pixel-wise loss functions, by comparing the
semantic distances in the high-dimensional feature space of a pre-trained 2D
network (e.g., VGG). However, it is not clear how one should generalize it to
3D medical images, and the attendant implications are still unclear. In this
paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using
Perceptual-tuned Generative Adversarial Network (GAN), in order to produce
thinner slice (e.g., high resolution in the 'Z' plane) medical images with
anti-aliasing and deblurring. The proposed method outperforms other
conventional resolution-enhancement methods and previous SR work on medical
images upon both qualitative and quantitative comparisons. Specifically, we
examine the model in terms of its generalization for various SR ratios and
imaging modalities. By addressing those limitations, our model shows promise as
a novel 3D SR interpolation technique, providing potential applications in both
clinical and research settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haoji Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Philbrick_K/0/1/0/all/0/1"&gt;Kenneth Philbrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1"&gt;Gian Marco Conte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sobek_J/0/1/0/all/0/1"&gt;Joseph D. Sobek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rouzrokh_P/0/1/0/all/0/1"&gt;Pouria Rouzrokh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Erickson_B/0/1/0/all/0/1"&gt;Bradley J. Erickson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking. (arXiv:2106.02495v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02495</id>
        <link href="http://arxiv.org/abs/2106.02495"/>
        <updated>2021-06-07T03:06:12.845Z</updated>
        <summary type="html"><![CDATA[Prior correlation filter (CF)-based tracking methods for unmanned aerial
vehicles (UAVs) have virtually focused on tracking in the daytime. However,
when the night falls, the trackers will encounter more harsh scenes, which can
easily lead to tracking failure. In this regard, this work proposes a novel
tracker with anti-dark function (ADTrack). The proposed method integrates an
efficient and effective low-light image enhancer into a CF-based tracker.
Besides, a target-aware mask is simultaneously generated by virtue of image
illumination variation. The target-aware mask can be applied to jointly train a
target-focused filter that assists the context filter for robust tracking.
Specifically, ADTrack adopts dual regression, where the context filter and the
target-focused filter restrict each other for dual filter learning. Exhaustive
experiments are conducted on typical dark sceneries benchmark, consisting of 37
typical night sequences from authoritative benchmarks, i.e., UAVDark, and our
newly constructed benchmark UAVDark70. The results have shown that ADTrack
favorably outperforms other state-of-the-art trackers and achieves a real-time
speed of 34 frames/s on a single CPU, greatly extending robust UAV tracking to
night scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAFLOW: Conditional Autoregressive Flows. (arXiv:2106.02531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02531</id>
        <link href="http://arxiv.org/abs/2106.02531"/>
        <updated>2021-06-07T03:06:12.818Z</updated>
        <summary type="html"><![CDATA[We introduce CAFLOW, a new diverse image-to-image translation model that
simultaneously leverages the power of auto-regressive modeling and the modeling
efficiency of conditional normalizing flows. We transform the conditioning
image into a sequence of latent encodings using a multi-scale normalizing flow
and repeat the process for the conditioned image. We model the conditional
distribution of the latent encodings by modeling the auto-regressive
distributions with an efficient multi-scale normalizing flow, where each
conditioning factor affects image synthesis at its respective resolution scale.
Our proposed framework performs well on a range of image-to-image translation
tasks. It outperforms former designs of conditional flows because of its
expressive auto-regressive structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1"&gt;Marcello Carioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etmann_C/0/1/0/all/0/1"&gt;Christian Etmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afyouni_S/0/1/0/all/0/1"&gt;Soroosh Afyouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1"&gt;Zoe Kourtzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Image Local Autoregressive Transformer. (arXiv:2106.02514v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02514</id>
        <link href="http://arxiv.org/abs/2106.02514"/>
        <updated>2021-06-07T03:06:12.805Z</updated>
        <summary type="html"><![CDATA[Recently, AutoRegressive (AR) models for the whole image generation empowered
by transformers have achieved comparable or even better performance to
Generative Adversarial Networks (GANs). Unfortunately, directly applying such
AR models to edit/change local image regions, may suffer from the problems of
missing global information, slow inference speed, and information leakage of
local guidance. To address these limitations, we propose a novel model -- image
Local Autoregressive Transformer (iLAT), to better facilitate the locally
guided image synthesis. Our iLAT learns the novel local discrete
representations, by the newly proposed local autoregressive (LA) transformer of
the attention mask and convolution mechanism. Thus iLAT can efficiently
synthesize the local image regions by key guidance information. Our iLAT is
evaluated on various locally guided image syntheses, such as pose-guided person
image synthesis and face editing. Both the quantitative and qualitative results
show the efficacy of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1"&gt;Chenjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yuxin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;XiangYang Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Pretraining for Detection via Object-Level Contrastive Learning. (arXiv:2106.02637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02637</id>
        <link href="http://arxiv.org/abs/2106.02637"/>
        <updated>2021-06-07T03:06:12.731Z</updated>
        <summary type="html"><![CDATA[Image-level contrastive representation learning has proven to be highly
effective as a generic model for transfer learning. Such generality for
transfer learning, however, sacrifices specificity if we are interested in a
certain downstream task. We argue that this could be sub-optimal and thus
advocate a design principle which encourages alignment between the
self-supervised pretext task and the downstream task. In this paper, we follow
this principle with a pretraining method specifically designed for the task of
object detection. We attain alignment in the following three aspects: 1)
object-level representations are introduced via selective search bounding boxes
as object proposals; 2) the pretraining network architecture incorporates the
same dedicated modules used in the detection pipeline (e.g. FPN); 3) the
pretraining is equipped with object detection properties such as object-level
translation invariance and scale invariance. Our method, called Selective
Object COntrastive learning (SoCo), achieves state-of-the-art results for
transfer performance on COCO detection using a Mask R-CNN framework. Code and
models will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOLQ: Segmenting Objects by Learning Queries. (arXiv:2106.02351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02351</id>
        <link href="http://arxiv.org/abs/2106.02351"/>
        <updated>2021-06-07T03:06:12.724Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end framework for instance segmentation.
Based on the recently introduced DETR [1], our method, termed SOLQ, segments
objects by learning unified queries. In SOLQ, each query represents one object
and has multiple representations: class, location and mask. The object queries
learned perform classification, box regression and mask encoding simultaneously
in an unified vector form. During training phase, the mask vectors encoded are
supervised by the compression coding of raw spatial masks. In inference time,
mask vectors produced can be directly transformed to spatial masks by the
inverse process of compression coding. Experimental results show that SOLQ can
achieve state-of-the-art performance, surpassing most of existing approaches.
Moreover, the joint learning of unified query representation can greatly
improve the detection performance of original DETR. We hope our SOLQ can serve
as a strong baseline for the Transformer-based instance segmentation. Code is
available at https://github.com/megvii-research/SOLQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Fangao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiancai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yichen Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:12.717Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid attention network based on progressive embedding scale-context for crowd counting. (arXiv:2106.02324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02324</id>
        <link href="http://arxiv.org/abs/2106.02324"/>
        <updated>2021-06-07T03:06:12.699Z</updated>
        <summary type="html"><![CDATA[The existing crowd counting methods usually adopted attention mechanism to
tackle background noise, or applied multi-level features or multi-scales
context fusion to tackle scale variation. However, these approaches deal with
these two problems separately. In this paper, we propose a Hybrid Attention
Network (HAN) by employing Progressive Embedding Scale-context (PES)
information, which enables the network to simultaneously suppress noise and
adapt head scale variation. We build the hybrid attention mechanism through
paralleling spatial attention and channel attention module, which makes the
network to focus more on the human head area and reduce the interference of
background objects. Besides, we embed certain scale-context to the hybrid
attention along the spatial and channel dimensions for alleviating these
counting errors caused by the variation of perspective and head scale. Finally,
we propose a progressive learning strategy through cascading multiple hybrid
attention modules with embedding different scale-context, which can gradually
integrate different scale-context information into the current feature map from
global to local. Ablation experiments provides that the network architecture
can gradually learn multi-scale features and suppress background noise.
Extensive experiments demonstrate that HANet obtain state-of-the-art counting
performance on four mainstream datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fusen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1"&gt;Jun Sang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongyuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02473</id>
        <link href="http://arxiv.org/abs/2106.02473"/>
        <updated>2021-06-07T03:06:12.678Z</updated>
        <summary type="html"><![CDATA[GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total
of 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,
120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to
realize the function of valuating image classification. In order to prove that
the methods of different periods in the field of image classification have
discrepancies on GasHisSDB, we select a variety of classifiers for evaluation.
Seven classical machine learning classifiers, three CNN classifiers and a novel
transformer-based classifier are selected for testing on image classification
tasks. GasHisSDB is available at the
URL:https://github.com/NEUhwm/GasHisSDB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling False Positive/Negative Rates for Deep-Learning-Based Prostate Cancer Detection on Multiparametric MR images. (arXiv:2106.02385v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02385</id>
        <link href="http://arxiv.org/abs/2106.02385"/>
        <updated>2021-06-07T03:06:12.649Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is one of the leading causes of death for men
worldwide. Multi-parametric magnetic resonance (mpMR) imaging has emerged as a
non-invasive diagnostic tool for detecting and localising prostate tumours by
specialised radiologists. These radiological examinations, for example, for
differentiating malignant lesions from benign prostatic hyperplasia in
transition zones and for defining the boundaries of clinically significant
cancer, remain challenging and highly skill-and-experience-dependent. We first
investigate experimental results in developing object detection neural networks
that are trained to predict the radiological assessment, using these
high-variance labels. We further argue that such a computer-assisted diagnosis
(CAD) system needs to have the ability to control the false-positive rate (FPR)
or false-negative rate (FNR), in order to be usefully deployed in a clinical
workflow, informing clinical decisions without further human intervention. This
work proposes a novel PCa detection network that incorporates a lesion-level
cost-sensitive loss and an additional slice-level loss based on a
lesion-to-slice mapping function, to manage the lesion- and slice-level costs,
respectively. Our experiments based on 290 clinical patients concludes that 1)
The lesion-level FNR was effectively reduced from 0.19 to 0.10 and the
lesion-level FPR was reduced from 1.03 to 0.66 by changing the lesion-level
cost; 2) The slice-level FNR was reduced from 0.19 to 0.00 by taking into
account the slice-level cost; (3) Both lesion-level and slice-level FNRs were
reduced with lower FP/FPR by changing the lesion-level or slice-level costs,
compared with post-training threshold adjustment using networks without the
proposed cost-aware training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Min_Z/0/1/0/all/0/1"&gt;Zhe Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bianco_F/0/1/0/all/0/1"&gt;Fernando J. Bianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodell_R/0/1/0/all/0/1"&gt;Rachael Rodell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1"&gt;Wen Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1"&gt;Dean Barratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-Adversarial Visual Question Answering. (arXiv:2106.02280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02280</id>
        <link href="http://arxiv.org/abs/2106.02280"/>
        <updated>2021-06-07T03:06:12.620Z</updated>
        <summary type="html"><![CDATA[Performance on the most commonly used Visual Question Answering dataset (VQA
v2) is starting to approach human accuracy. However, in interacting with
state-of-the-art VQA models, it is clear that the problem is far from being
solved. In order to stress test VQA models, we benchmark them against
human-adversarial examples. Human subjects interact with a state-of-the-art VQA
model, and for each image in the dataset, attempt to find a question where the
model's predicted answer is incorrect. We find that a wide range of
state-of-the-art models perform poorly when evaluated on these examples. We
conduct an extensive analysis of the collected adversarial examples and provide
guidance on future research directions. We hope that this Adversarial VQA
(AdVQA) benchmark can help drive progress in the field and advance the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1"&gt;Sasha Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1"&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magana_J/0/1/0/all/0/1"&gt;Jose Alberto Lopez Magana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution. (arXiv:2106.02299v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02299</id>
        <link href="http://arxiv.org/abs/2106.02299"/>
        <updated>2021-06-07T03:06:12.614Z</updated>
        <summary type="html"><![CDATA[Reference-based image super-resolution (RefSR) has shown promising success in
recovering high-frequency details by utilizing an external reference image
(Ref). In this task, texture details are transferred from the Ref image to the
low-resolution (LR) image according to their point- or patch-wise
correspondence. Therefore, high-quality correspondence matching is critical. It
is also desired to be computationally efficient. Besides, existing RefSR
methods tend to ignore the potential large disparity in distributions between
the LR and Ref images, which hurts the effectiveness of the information
utilization. In this paper, we propose the MASA network for RefSR, where two
novel modules are designed to address these problems. The proposed Match &
Extraction Module significantly reduces the computational cost by a
coarse-to-fine correspondence matching scheme. The Spatial Adaptation Module
learns the difference of distribution between the LR and Ref images, and remaps
the distribution of Ref features to that of LR features in a spatially adaptive
way. This scheme makes the network robust to handle different reference images.
Extensive quantitative and qualitative experiments validate the effectiveness
of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1"&gt;Xin Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiangbo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Correspondence with Transformers. (arXiv:2106.02520v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02520</id>
        <link href="http://arxiv.org/abs/2106.02520"/>
        <updated>2021-06-07T03:06:12.593Z</updated>
        <summary type="html"><![CDATA[We propose a novel cost aggregation network, called Cost Aggregation with
Transformers (CATs), to find dense correspondences between semantically similar
images with additional challenges posed by large intra-class appearance and
geometric variations. Compared to previous hand-crafted or CNN-based methods
addressing the cost aggregation stage, which either lack robustness to severe
deformations or inherit the limitation of CNNs that fail to discriminate
incorrect matches due to limited receptive fields, CATs explore global
consensus among initial correlation map with the help of some architectural
designs that allow us to exploit full potential of self-attention mechanism.
Specifically, we include appearance affinity modelling to disambiguate the
initial correlation maps and multi-level aggregation to benefit from
hierarchical feature representations within Transformer-based aggregator, and
combine with swapping self-attention and residual connections not only to
enforce consistent matching, but also to ease the learning process. We conduct
experiments to demonstrate the effectiveness of the proposed model over the
latest methods and provide extensive ablation studies. Code and trained models
will be made available at https://github.com/SunghwanHong/CATs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seokju Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sunghwan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1"&gt;Sangryul Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucination In Object Detection -- A Study In Visual Part Verification. (arXiv:2106.02523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02523</id>
        <link href="http://arxiv.org/abs/2106.02523"/>
        <updated>2021-06-07T03:06:12.586Z</updated>
        <summary type="html"><![CDATA[We show that object detectors can hallucinate and detect missing objects;
potentially even accurately localized at their expected, but non-existing,
position. This is particularly problematic for applications that rely on visual
part verification: detecting if an object part is present or absent. We show
how popular object detectors hallucinate objects in a visual part verification
task and introduce the first visual part verification dataset: DelftBikes,
which has 10,000 bike photographs, with 22 densely annotated parts per image,
where some parts may be missing. We explicitly annotated an extra object state
label for each part to reflect if a part is missing or intact. We propose to
evaluate visual part verification by relying on recall and compare popular
object detectors on DelftBikes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1"&gt;Osman Semih Kayhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vredebregt_B/0/1/0/all/0/1"&gt;Bart Vredebregt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covering Polygons is Even Harder. (arXiv:2106.02335v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2106.02335</id>
        <link href="http://arxiv.org/abs/2106.02335"/>
        <updated>2021-06-07T03:06:12.579Z</updated>
        <summary type="html"><![CDATA[In the MINIMUM CONVEX COVER (MCC) problem, we are given a simple polygon
$\mathcal P$ and an integer $k$, and the question is if there exist $k$ convex
polygons whose union is $\mathcal P$. It is known that MCC is
$\mathsf{NP}$-hard [Culberson & Reckhow: Covering polygons is hard, FOCS
1988/Journal of Algorithms 1994] and in $\exists\mathbb{R}$ [O'Rourke: The
complexity of computing minimum convex covers for polygons, Allerton 1982]. We
prove that MCC is $\exists\mathbb{R}$-hard, and the problem is thus
$\exists\mathbb{R}$-complete. In other words, the problem is equivalent to
deciding whether a system of polynomial equations and inequalities with integer
coefficients has a real solution.

If a cover for our constructed polygon exists, then so does a cover
consisting entirely of triangles. As a byproduct, we therefore also establish
that it is $\exists\mathbb{R}$-complete to decide whether $k$ triangles cover a
given polygon.

The issue that it was not known if finding a minimum cover is in
$\mathsf{NP}$ has repeatedly been raised in the literature, and it was
mentioned as a "long-standing open question" already in 2001 [Eidenbenz &
Widmayer: An approximation algorithm for minimum convex cover with logarithmic
performance guarantee, ESA 2001/SIAM Journal on Computing 2003]. We prove that
assuming the widespread belief that $\mathsf{NP}\neq\exists\mathbb{R}$, the
problem is not in $\mathsf{NP}$.

An implication of the result is that many natural approaches to finding small
covers are bound to give suboptimal solutions in some cases, since irrational
coordinates of arbitrarily high algebraic degree can be needed for the corners
of the pieces in an optimal solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamsen_M/0/1/0/all/0/1"&gt;Mikkel Abrahamsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoadMap: A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving. (arXiv:2106.02527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02527</id>
        <link href="http://arxiv.org/abs/2106.02527"/>
        <updated>2021-06-07T03:06:12.572Z</updated>
        <summary type="html"><![CDATA[Accurate localization is of crucial importance for autonomous driving tasks.
Nowadays, we have seen a lot of sensor-rich vehicles (e.g. Robo-taxi) driving
on the street autonomously, which rely on high-accurate sensors (e.g. Lidar and
RTK GPS) and high-resolution map. However, low-cost production cars cannot
afford such high expenses on sensors and maps. How to reduce costs? How do
sensor-rich vehicles benefit low-cost cars? In this paper, we proposed a
light-weight localization solution, which relies on low-cost cameras and
compact visual semantic maps. The map is easily produced and updated by
sensor-rich vehicles in a crowd-sourced way. Specifically, the map consists of
several semantic elements, such as lane line, crosswalk, ground sign, and stop
line on the road surface. We introduce the whole framework of on-vehicle
mapping, on-cloud maintenance, and user-end localization. The map data is
collected and preprocessed on vehicles. Then, the crowd-sourced data is
uploaded to a cloud server. The mass data from multiple vehicles are merged on
the cloud so that the semantic map is updated in time. Finally, the semantic
map is compressed and distributed to production cars, which use this map for
localization. We validate the performance of the proposed map in real-world
experiments and compare it against other algorithms. The average size of the
semantic map is $36$ kb/km. We highlight that this framework is a reliable and
practical localization solution for autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tong Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tongqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1"&gt;Qing Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally coherent video anonymization through GAN inpainting. (arXiv:2106.02328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02328</id>
        <link href="http://arxiv.org/abs/2106.02328"/>
        <updated>2021-06-07T03:06:12.565Z</updated>
        <summary type="html"><![CDATA[This work tackles the problem of temporally coherent face anonymization in
natural video streams.We propose JaGAN, a two-stage system starting with
detecting and masking out faces with black image patches in all individual
frames of the video. The second stage leverages a privacy-preserving Video
Generative Adversarial Network designed to inpaint the missing image patches
with artificially generated faces. Our initial experiments reveal that image
based generative models are not capable of inpainting patches showing temporal
coherent appearance across neighboring video frames. To address this issue we
introduce a newly curated video collection, which is made publicly available
for the research community along with this paper. We also introduce the
Identity Invariance Score IdI as a means to quantify temporal coherency between
neighboring frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_T/0/1/0/all/0/1"&gt;Thangapavithraa Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blies_P/0/1/0/all/0/1"&gt;Patrick Blies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_G/0/1/0/all/0/1"&gt;Georg G&amp;#xf6;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsch_R/0/1/0/all/0/1"&gt;Raphael Mitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasserer_M/0/1/0/all/0/1"&gt;Marcel Wasserer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Visual Classification of Plant Species In The Wild: Object Detection as A Reinforced Means of Attention. (arXiv:2106.02141v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02141</id>
        <link href="http://arxiv.org/abs/2106.02141"/>
        <updated>2021-06-07T03:06:12.546Z</updated>
        <summary type="html"><![CDATA[Plant species identification in the wild is a difficult problem in part due
to the high variability of the input data, but also because of complications
induced by the long-tail effects of the datasets distribution. Inspired by the
most recent fine-grained visual classification approaches which are based on
attention to mitigate the effects of data variability, we explore the idea of
using object detection as a form of attention. We introduce a bottom-up
approach based on detecting plant organs and fusing the predictions of a
variable number of organ-based species classifiers. We also curate a new
dataset with a long-tail distribution for evaluating plant organ detection and
organ-based species identification, which is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keaton_M/0/1/0/all/0/1"&gt;Matthew R. Keaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaveri_R/0/1/0/all/0/1"&gt;Ram J. Zaveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovur_M/0/1/0/all/0/1"&gt;Meghana Kovur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_C/0/1/0/all/0/1"&gt;Cole Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1"&gt;Donald A. Adjeroh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1"&gt;Gianfranco Doretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the robustness of NMF algorithms. (arXiv:2106.02213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02213</id>
        <link href="http://arxiv.org/abs/2106.02213"/>
        <updated>2021-06-07T03:06:12.539Z</updated>
        <summary type="html"><![CDATA[We examine three non-negative matrix factorization techniques; L2-norm,
L1-norm, and L2,1-norm. Our aim is to establish the performance of these
different approaches, and their robustness in real-world applications such as
feature selection while managing computational complexity, sensitivity to noise
and more. We thoroughly examine each approach from a theoretical perspective,
and examine the performance of each using a series of experiments drawing on
both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors
(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria
under a range of simulated noise scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02285</id>
        <link href="http://arxiv.org/abs/2106.02285"/>
        <updated>2021-06-07T03:06:12.531Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02395</id>
        <link href="http://arxiv.org/abs/2106.02395"/>
        <updated>2021-06-07T03:06:12.525Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have shown to perform very well on large scale
object recognition problems and lead to widespread use for real-world
applications, including situations where DNN are implemented as "black boxes".
A promising approach to secure their use is to accept decisions that are likely
to be correct while discarding the others. In this work, we propose DOCTOR, a
simple method that aims to identify whether the prediction of a DNN classifier
should (or should not) be trusted so that, consequently, it would be possible
to accept it or to reject it. Two scenarios are investigated: Totally Black Box
(TBB) where only the soft-predictions are available and Partially Black Box
(PBB) where gradient-propagation to perform input pre-processing is allowed.
Empirically, we show that DOCTOR outperforms all state-of-the-art methods on
various well-known images and sentiment analysis datasets. In particular, we
observe a reduction of up to $4\%$ of the false rejection rate (FRR) in the PBB
scenario. DOCTOR can be applied to any pre-trained model, it does not require
prior information about the underlying dataset and is as simple as the simplest
available methods in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1"&gt;Federica Granese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1"&gt;Marco Romanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1"&gt;Daniele Gorla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02638</id>
        <link href="http://arxiv.org/abs/2106.02638"/>
        <updated>2021-06-07T03:06:12.518Z</updated>
        <summary type="html"><![CDATA[This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computing resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
uniformly. In detail, AOT employs an identification mechanism to associate
multiple targets into the same high-dimensional embedding space. Thus, we can
simultaneously process the matching and segmentation decoding of multiple
objects as efficiently as processing a single object. For sufficiently modeling
multi-object association, a Long Short-Term Transformer is designed for
constructing hierarchical matching and propagation. We conduct extensive
experiments on both multi-object and single-object benchmarks to examine AOT
variant networks with different complexities. Particularly, our AOT-L
outperforms all the state-of-the-art competitors on three popular benchmarks,
i.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),
while keeping better multi-object efficiency. Meanwhile, our AOT-T can maintain
real-time multi-object speed on above benchmarks. We ranked 1st in the 3rd
Large-scale Video Object Segmentation Challenge. The code will be publicly
available at https://github.com/z-x-yang/AOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02320</id>
        <link href="http://arxiv.org/abs/2106.02320"/>
        <updated>2021-06-07T03:06:12.511Z</updated>
        <summary type="html"><![CDATA[Few-shot segmentation aims to train a segmentation model that can fast adapt
to novel classes with few exemplars. The conventional training paradigm is to
learn to make predictions on query images conditioned on the features from
support images. Previous methods only utilized the semantic-level prototypes of
support images as the conditional information. These methods cannot utilize all
pixel-wise support information for the query predictions, which is however
critical for the segmentation task. In this paper, we focus on utilizing
pixel-wise relationships between support and target images to facilitate the
few-shot semantic segmentation task. We design a novel Cycle-Consistent
Transformer (CyCTR) module to aggregate pixel-wise support features into query
ones. CyCTR performs cross-attention between features from different images,
i.e. support and query images. We observe that there may exist unexpected
irrelevant pixel-level support features. Directly performing cross-attention
may aggregate these features from support to query and bias the query features.
Thus, we propose using a novel cycle-consistent attention mechanism to filter
out possible harmful support features and encourage query features to attend to
the most informative pixels from support images. Experiments on all few-shot
segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable
improvement compared to previous state-of-the-art methods. Specifically, on
Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for
5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1"&gt;Guoliang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection. (arXiv:2106.02426v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02426</id>
        <link href="http://arxiv.org/abs/2106.02426"/>
        <updated>2021-06-07T03:06:12.494Z</updated>
        <summary type="html"><![CDATA[Non-Maximum Suppression (NMS) is essential for object detection and affects
the evaluation results by incorporating False Positives (FP) and False
Negatives (FN), especially in crowd occlusion scenes. In this paper, we raise
the problem of weak connection between the training targets and the evaluation
metrics caused by NMS and propose a novel NMS-Loss making the NMS procedure can
be trained end-to-end without any additional network parameters. Our NMS-Loss
punishes two cases when FP is not suppressed and FN is wrongly eliminated by
NMS. Specifically, we propose a pull loss to pull predictions with the same
target close to each other, and a push loss to push predictions with different
targets away from each other. Experimental results show that with the help of
NMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss
Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are
both better than state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zekun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Sixiao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Glance-and-Gaze Vision Transformer. (arXiv:2106.02277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02277</id>
        <link href="http://arxiv.org/abs/2106.02277"/>
        <updated>2021-06-07T03:06:12.486Z</updated>
        <summary type="html"><![CDATA[Recently, there emerges a series of vision Transformers, which show superior
performance with a more compact model size than conventional convolutional
neural networks, thanks to the strong ability of Transformers to model
long-range dependencies. However, the advantages of vision Transformers also
come with a price: Self-attention, the core part of Transformer, has a
quadratic complexity to the input sequence length. This leads to a dramatic
increase of computation and memory cost with the increase of sequence length,
thus introducing difficulties when applying Transformers to the vision tasks
that require dense predictions based on high-resolution feature maps. In this
paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer
(GG-Transformer), to address the aforementioned issues. It is motivated by the
Glance and Gaze behavior of human beings when recognizing objects in natural
scenes, with the ability to efficiently model both long-range dependencies and
local context. In GG-Transformer, the Glance and Gaze behavior is realized by
two parallel branches: The Glance branch is achieved by performing
self-attention on the adaptively-dilated partitions of the input, which leads
to a linear complexity while still enjoying a global receptive field; The Gaze
branch is implemented by a simple depth-wise convolutional layer, which
compensates local image context to the features obtained by the Glance
mechanism. We empirically demonstrate our method achieves consistently superior
performance over previous state-of-the-art Transformers on various vision tasks
and benchmarks. The codes and models will be made available at
https://github.com/yucornetto/GG-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qihang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingda Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yutong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yongyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation. (arXiv:2106.02267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02267</id>
        <link href="http://arxiv.org/abs/2106.02267"/>
        <updated>2021-06-07T03:06:12.479Z</updated>
        <summary type="html"><![CDATA[The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses
on the object and style like other artwork researches. Such study has benefited
from the renewed interest by the machine learning community in culturally
important topics, leading to interdisciplinary works including collections of
images, quantitative approaches, and machine learning-based creativities. They,
however, have several drawbacks, and it remains challenging to integrate these
works into a comprehensive view. To bridge this gap, we propose a holistic
approach We first present a large-scale Ukiyo-e dataset with coherent semantic
labels and geometric annotations, then show its value in a quantitative study
of Ukiyo-e paintings' object using these labels and annotations. We further
demonstrate the machine learning methods could help style study through soft
color decomposition of Ukiyo-e, and finally provides joint insights into object
and style by composing sketches and colors using colorization. Dataset
available at https://github.com/rois-codh/arc-ukiyoe-faces]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingtao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1"&gt;Chikahiko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Networks with Mixed Hierarchical Structures for Natural Language Processing. (arXiv:2106.02562v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02562</id>
        <link href="http://arxiv.org/abs/2106.02562"/>
        <updated>2021-06-07T03:06:12.473Z</updated>
        <summary type="html"><![CDATA[Hierarchical structures exist in both linguistics and Natural Language
Processing (NLP) tasks. How to design RNNs to learn hierarchical
representations of natural languages remains a long-standing challenge. In this
paper, we define two different types of boundaries referred to as static and
dynamic boundaries, respectively, and then use them to construct a multi-layer
hierarchical structure for document classification tasks. In particular, we
focus on a three-layer hierarchical structure with static word- and sentence-
layers and a dynamic phrase-layer. LSTM cells and two boundary detectors are
used to implement the proposed structure, and the resulting network is called
the {\em Recurrent Neural Network with Mixed Hierarchical Structures}
(MHS-RNN). We further add three layers of attention mechanisms to the MHS-RNN
model. Incorporating attention mechanisms allows our model to use more
important content to construct document representation and enhance its
performance on document classification tasks. Experiments on five different
datasets show that the proposed architecture outperforms previous methods on
all the five tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhaoxin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Michael Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeChat AI & ICT's Submission for DSTC9 Interactive Dialogue Evaluation Track. (arXiv:2101.07947v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07947</id>
        <link href="http://arxiv.org/abs/2101.07947"/>
        <updated>2021-06-07T03:06:12.456Z</updated>
        <summary type="html"><![CDATA[We participate in the DSTC9 Interactive Dialogue Evaluation Track (Gunasekara
et al. 2020) sub-task 1 (Knowledge Grounded Dialogue) and sub-task 2
(Interactive Dialogue). In sub-task 1, we employ a pre-trained language model
to generate topic-related responses and propose a response ensemble method for
response selection. In sub-task2, we propose a novel Dialogue Planning Model
(DPM) to capture conversation flow in the interaction with humans. We also
design an integrated open-domain dialogue system containing pre-process,
dialogue model, scoring model, and post-process, which can generate fluent,
coherent, consistent, and humanlike responses. We tie 1st on human ratings and
also get the highest Meteor, and Bert-score in sub-task 1, and rank 3rd on
interactive human evaluation in sub-task 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zongjia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On (co-lex) Ordering Automata. (arXiv:2106.02309v1 [cs.FL])]]></title>
        <id>http://arxiv.org/abs/2106.02309</id>
        <link href="http://arxiv.org/abs/2106.02309"/>
        <updated>2021-06-07T03:06:12.450Z</updated>
        <summary type="html"><![CDATA[The states of a deterministic finite automaton A can be identified with
collections of words in Pf(L(A)) -- the set of prefixes of words belonging to
the regular language accepted by A. But words can be ordered and among the many
possible orders a very natural one is the co-lexicographic one. Such
naturalness stems from the fact that it suggests a transfer of the order from
words to the automaton's states. In a number of papers automata admitting a
total ordering of states coherent with the ordering of the set of words
reaching them have been proposed. Such class of ordered automata -- the Wheeler
automata -- turned out to be efficiently stored/searched using an index.
Unfortunately not all automata can be totally ordered as previously outlined.
However, automata can always be partially ordered and an intrinsic measure of
their complexity can be defined and effectively determined, as the minimum
width of one of their admissible partial orders. As shown in previous works,
this new concept of width of an automaton has useful consequences in the fields
of graph compression, indexing data structures, and automata theory. In this
paper we prove that a canonical, minimum-width, partially-ordered automaton
accepting a language L -- dubbed the Hasse automaton H of L -- can be
exhibited. H provides, in a precise sense, the best possible way to (partially)
order the states of any automaton accepting L, as long as we want to maintain
an operational link with the (co-lexicographic) order of Pf(L(A)). Using H we
prove that the width of the language can be effectively computed from the
minimum automaton recognizing the language. Finally, we explore the
relationship between two (often conflicting) objectives: minimizing the width
and minimizing the number of states of an automaton.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAgostino_G/0/1/0/all/0/1"&gt;Giovanna D&amp;#x27;Agostino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotumaccio_N/0/1/0/all/0/1"&gt;Nicola Cotumaccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Policriti_A/0/1/0/all/0/1"&gt;Alberto Policriti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prezza_N/0/1/0/all/0/1"&gt;Nicola Prezza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings. (arXiv:2106.02490v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02490</id>
        <link href="http://arxiv.org/abs/2106.02490"/>
        <updated>2021-06-07T03:06:12.443Z</updated>
        <summary type="html"><![CDATA[Artificial Neural networks are mathematical models at their core. This
truismpresents some fundamental difficulty when networks are tasked with
Natural Language Processing. A key problem lies in measuring the similarity or
distance among vectors in NLP embedding space, since the mathematical concept
of distance does not always agree with the linguistic concept. We suggest that
the best way to measure linguistic distance among vectors is by employing the
Language Model (LM) that created them. We introduce Language Model Distance
(LMD) for measuring accuracy of vector transformations based on the
Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric
by applying it to a simple neural network learning the Procrustes algorithm for
bilingual word mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conley_T/0/1/0/all/0/1"&gt;Thomas Conley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[History Encoding Representation Design for Human Intention Inference. (arXiv:2106.02222v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02222</id>
        <link href="http://arxiv.org/abs/2106.02222"/>
        <updated>2021-06-07T03:06:12.437Z</updated>
        <summary type="html"><![CDATA[In this extended abstract, we investigate the design of learning
representation for human intention inference. In our designed human intention
prediction task, we propose a history encoding representation that is both
interpretable and effective for prediction. Through extensive experiments, we
show our prediction framework with a history encoding representation design is
successful on the human intention prediction problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02229</id>
        <link href="http://arxiv.org/abs/2106.02229"/>
        <updated>2021-06-07T03:06:12.430Z</updated>
        <summary type="html"><![CDATA[We introduce RL-DARTS, one of the first applications of Differentiable
Architecture Search (DARTS) in reinforcement learning (RL) to search for
convolutional cells, applied to the Procgen benchmark. We outline the initial
difficulties of applying neural architecture search techniques in RL, and
demonstrate that by simply replacing the image encoder with a DARTS supernet,
our search method is sample-efficient, requires minimal extra compute
resources, and is also compatible with off-policy and on-policy RL algorithms,
needing only minor changes in preexisting code. Surprisingly, we find that the
supernet can be used as an actor for inference to generate replay data in
standard RL training loops, and thus train end-to-end. Throughout this training
process, we show that the supernet gradually learns better cells, leading to
alternative architectures which can be highly competitive against manually
designed policies, but also verify previous design choices for RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yingjie Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Summer Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1"&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-volution: On the unification of convolution and self-attention. (arXiv:2106.02253v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02253</id>
        <link href="http://arxiv.org/abs/2106.02253"/>
        <updated>2021-06-07T03:06:12.423Z</updated>
        <summary type="html"><![CDATA[Convolution and self-attention are acting as two fundamental building blocks
in deep neural networks, where the former extracts local image features in a
linear way while the latter non-locally encodes high-order contextual
relationships. Though essentially complementary to each other, i.e.,
first-/high-order, stat-of-the-art architectures, i.e., CNNs or transformers
lack a principled way to simultaneously apply both operations in a single
computational module, due to their heterogeneous computing pattern and
excessive burden of global dot-product for visual tasks. In this work, we
theoretically derive a global self-attention approximation scheme, which
approximates a self-attention via the convolution operation on transformed
features. Based on the approximated scheme, we establish a multi-branch
elementary module composed of both convolution and self-attention operation,
capable of unifying both local and non-local feature interaction. Importantly,
once trained, this multi-branch module could be conditionally converted into a
single standard convolution operation via structural re-parameterization,
rendering a pure convolution styled operator named X-volution, ready to be
plugged into any modern networks as an atomic operation. Extensive experiments
demonstrate that the proposed X-volution, achieves highly competitive visual
understanding improvements (+1.2% top-1 accuracy on ImageNet classification,
+1.7 box AP and +1.5 mask AP on COCO detection and segmentation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNNs and GANs in MRI-based cross-modality medical image estimation. (arXiv:2106.02198v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02198</id>
        <link href="http://arxiv.org/abs/2106.02198"/>
        <updated>2021-06-07T03:06:12.417Z</updated>
        <summary type="html"><![CDATA[Cross-modality image estimation involves the generation of images of one
medical imaging modality from that of another modality. Convolutional neural
networks (CNNs) have been shown to be useful in identifying, characterising and
extracting image patterns. Generative adversarial networks (GANs) use CNNs as
generators and estimated images are discriminated as true or false based on an
additional network. CNNs and GANs within the image estimation framework may be
considered more generally as deep learning approaches, since imaging data tends
to be large, leading to a larger number of network weights. Almost all research
in the CNN/GAN image estimation literature has involved the use of MRI data
with the other modality primarily being PET or CT. This review provides an
overview of the use of CNNs and GANs for MRI-based cross-modality medical image
estimation. We outline the neural networks implemented, and detail network
constructs employed for CNN and GAN image-to-image estimators. Motivations
behind cross-modality image estimation are provided as well. GANs appear to
provide better utility in cross-modality image estimation in comparison with
CNNs, a finding drawn based on our analysis involving metrics comparing
estimated and actual images. Our final remarks highlight key challenges faced
by the cross-modality medical image estimation field, and suggestions for
future research are outlined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fard_A/0/1/0/all/0/1"&gt;Azin Shokraei Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Reutens_D/0/1/0/all/0/1"&gt;David C. Reutens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vegh_V/0/1/0/all/0/1"&gt;Viktor Vegh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Question Rewriting for Increasing Response Rate. (arXiv:2106.02257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02257</id>
        <link href="http://arxiv.org/abs/2106.02257"/>
        <updated>2021-06-07T03:06:12.399Z</updated>
        <summary type="html"><![CDATA[When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiayi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xilian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency. (arXiv:2106.02342v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02342</id>
        <link href="http://arxiv.org/abs/2106.02342"/>
        <updated>2021-06-07T03:06:12.392Z</updated>
        <summary type="html"><![CDATA[We study self-supervised video representation learning, which is a
challenging task due to 1) a lack of labels for explicit supervision and 2)
unstructured and noisy visual information. Existing methods mainly use
contrastive loss with video clips as the instances and learn visual
representation by discriminating instances from each other, but they require
careful treatment of negative pairs by relying on large batch sizes, memory
banks, extra modalities, or customized mining strategies, inevitably including
noisy data. In this paper, we observe that the consistency between positive
samples is the key to learn robust video representations. Specifically, we
propose two tasks to learn the appearance and speed consistency, separately.
The appearance consistency task aims to maximize the similarity between two
clips of the same video with different playback speeds. The speed consistency
task aims to maximize the similarity between two clips with the same playback
speed but different appearance information. We show that joint optimization of
the two tasks consistently improves the performance on downstream tasks, e.g.,
action recognition and video retrieval. Remarkably, for action recognition on
the UCF-101 dataset, we achieve 90.8% accuracy without using any additional
modalities or negative pairs for unsupervised pretraining, outperforming the
ImageNet supervised pre-trained model. Codes and models will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Deng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhihua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiangmiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Adversarial Learning for Deep Semi-Supervised Facial Action Unit Recognition. (arXiv:2106.02258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02258</id>
        <link href="http://arxiv.org/abs/2106.02258"/>
        <updated>2021-06-07T03:06:12.385Z</updated>
        <summary type="html"><![CDATA[Current works formulate facial action unit (AU) recognition as a supervised
learning problem, requiring fully AU-labeled facial images during training. It
is challenging if not impossible to provide AU annotations for large numbers of
facial images. Fortunately, AUs appear on all facial images, whether manually
labeled or not, satisfy the underlying anatomic mechanisms and human behavioral
habits. In this paper, we propose a deep semi-supervised framework for facial
action unit recognition from partially AU-labeled facial images. Specifically,
the proposed deep semi-supervised AU recognition approach consists of a deep
recognition network and a discriminator D. The deep recognition network R
learns facial representations from large-scale facial images and AU classifiers
from limited ground truth AU labels. The discriminator D is introduced to
enforce statistical similarity between the AU distribution inherent in ground
truth AU labels and the distribution of the predicted AU labels from labeled
and unlabeled facial images. The deep recognition network aims to minimize
recognition loss from the labeled facial images, to faithfully represent
inherent AU distribution for both labeled and unlabeled facial images, and to
confuse the discriminator. During training, the deep recognition network R and
the discriminator D are optimized alternately. Thus, the inherent AU
distributions caused by underlying anatomic mechanisms are leveraged to
construct better feature representations and AU classifiers from partially
AU-labeled data during training. Experiments on two benchmark databases
demonstrate that the proposed approach successfully captures AU distributions
through adversarial learning and outperforms state-of-the-art AU recognition
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shangfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yanan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1"&gt;Guozhu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bowen Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does CTC result in peaky behavior?. (arXiv:2105.14849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14849</id>
        <link href="http://arxiv.org/abs/2105.14849"/>
        <updated>2021-06-07T03:06:12.367Z</updated>
        <summary type="html"><![CDATA[The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes. (arXiv:2106.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02524</id>
        <link href="http://arxiv.org/abs/2106.02524"/>
        <updated>2021-06-07T03:06:12.360Z</updated>
        <summary type="html"><![CDATA[Continuity of care is crucial to ensuring positive health outcomes for
patients discharged from an inpatient hospital setting, and improved
information sharing can help. To share information, caregivers write discharge
notes containing action items to share with patients and their future
caregivers, but these action items are easily lost due to the lengthiness of
the documents. In this work, we describe our creation of a dataset of clinical
action items annotated over MIMIC-III, the largest publicly available dataset
of real clinical notes. This dataset, which we call CLIP, is annotated by
physicians and covers 718 documents representing 100K sentences. We describe
the task of extracting the action items from these documents as multi-aspect
extractive summarization, with each aspect representing a type of action to be
taken. We evaluate several machine learning models on this task, and show that
the best models exploit in-domain language model pre-training on 59K
unannotated documents, and incorporate context from neighboring sentences. We
also propose an approach to pre-training data selection that allows us to
explore the trade-off between size and domain-specificity of pre-training
datasets for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1"&gt;James Mullenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Sean Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seale_J/0/1/0/all/0/1"&gt;Jennifer Seale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1"&gt;Jordan Swartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKelvey_T/0/1/0/all/0/1"&gt;T. Greg McKelvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion. (arXiv:2004.13631v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13631</id>
        <link href="http://arxiv.org/abs/2004.13631"/>
        <updated>2021-06-07T03:06:12.354Z</updated>
        <summary type="html"><![CDATA[A comprehensive knowledge graph (KG) contains an instance-level entity graph
and an ontology-level concept graph. The two-view KG provides a testbed for
models to "simulate" human's abilities on knowledge abstraction,
concretization, and completion (KACC), which are crucial for human to recognize
the world and manage learned knowledge. Existing studies mainly focus on
partial aspects of KACC. In order to promote thorough analyses for KACC
abilities of models, we propose a unified KG benchmark by improving existing
benchmarks in terms of dataset scale, task coverage, and difficulty.
Specifically, we collect new datasets that contain larger concept graphs,
abundant cross-view links as well as dense entity graphs. Based on the
datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),
multi-hop knowledge concretization (MKC) and then design a comprehensive
benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical
triples as harder samples. The experimental results of existing methods
demonstrate the challenges of our benchmark. The resource is available at
https://github.com/thunlp/KACC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02561</id>
        <link href="http://arxiv.org/abs/2106.02561"/>
        <updated>2021-06-07T03:06:12.347Z</updated>
        <summary type="html"><![CDATA[Broad-coverage meaning representations in NLP mostly focus on explicitly
expressed content. More importantly, the scarcity of datasets annotating
diverse implicit roles limits empirical studies into their linguistic nuances.
For example, in the web review "Great service!", the provider and consumer are
implicit arguments of different types. We examine an annotated corpus of
fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully
re-annotating it, resolving several inconsistencies. Subsequently, we present
the first transition-based neural parser that can handle implicit arguments
dynamically, and experiment with two different transition systems on the
improved dataset. We find that certain types of implicit arguments are more
difficult to parse than others and that the simpler system is more accurate in
recovering implicit arguments, despite having a lower overall parsing score,
attesting current reasoning limitations of NLP models. This work will
facilitate a better understanding of implicit and underspecified language, by
incorporating it holistically into meaning representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural semi-Markov CRF for Monolingual Word Alignment. (arXiv:2106.02569v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02569</id>
        <link href="http://arxiv.org/abs/2106.02569"/>
        <updated>2021-06-07T03:06:12.337Z</updated>
        <summary type="html"><![CDATA[Monolingual word alignment is important for studying fine-grained editing
operations (i.e., deletion, addition, and substitution) in text-to-text
generation tasks, such as paraphrase generation, text simplification,
neutralizing biased language, etc. In this paper, we present a novel neural
semi-Markov CRF alignment model, which unifies word and phrase alignments
through variable-length spans. We also create a new benchmark with human
annotations that cover four different text genres to evaluate monolingual word
alignment models in more realistic settings. Experimental results show that our
proposed model outperforms all previous approaches for monolingual word
alignment as well as a competitive QA-based baseline, which was previously only
applied to bilingual data. Our model demonstrates good generalizability to
three out-of-domain datasets and shows great utility in two downstream
applications: automatic text simplification and sentence pair classification
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1"&gt;Wuwei Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specular reflections removal in colposcopic images based on neural networks: Supervised training with no ground truth previous knowledge. (arXiv:2106.02221v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02221</id>
        <link href="http://arxiv.org/abs/2106.02221"/>
        <updated>2021-06-07T03:06:12.330Z</updated>
        <summary type="html"><![CDATA[Cervical cancer is a malignant tumor that seriously threatens women's health,
and is one of the most common that affects women worldwide. For its early
detection, colposcopic images of the cervix are used for searching for possible
injuries or abnormalities. An inherent characteristic of these images is the
presence of specular reflections (brightness) that make it difficult to observe
some regions, which might imply a misdiagnosis. In this paper, a new strategy
based on neural networks is introduced for eliminating specular reflections and
estimating the unobserved anatomical cervix portion under the bright zones. We
present a supervised learning method, despite not knowing the ground truth from
the beginning, based on training a neural network to learn how to restore any
hidden region of colposcopic images. Once the specular reflections are
identified, they are removed from the image and the previously trained network
is used to fulfill these deleted areas. The quality of the processed images was
evaluated quantitatively and qualitatively. In 21 of the 22 evaluated images,
the detected specular reflections were totally eliminated, whereas, in the
remaining one, these reflections were almost completely eliminated. The
distribution of the colors and the content of the restored images are similar
to those of the originals. The evaluation carried out by a specialist in Cervix
Pathology concluded that, after eliminating the specular reflections, the
anatomical and physiological elements of the cervix are observable in the
restored images, which facilitates the medical diagnosis of cervical
pathologies. Our method has the potential to improve the early detection of
cervical cancer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jimenez_Martin_L/0/1/0/all/0/1"&gt;Lauren Jimenez-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perez_D/0/1/0/all/0/1"&gt;Daniel A. Vald&amp;#xe9;s P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asteasuainzarra_A/0/1/0/all/0/1"&gt;Ana M. Solares Asteasuainzarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leonard_L/0/1/0/all/0/1"&gt;Ludwig Leonard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_Romanach_M/0/1/0/all/0/1"&gt;Marta L. Baguer D&amp;#xed;az-Roma&amp;#xf1;ach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02288</id>
        <link href="http://arxiv.org/abs/2106.02288"/>
        <updated>2021-06-07T03:06:12.301Z</updated>
        <summary type="html"><![CDATA[Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging
task. The recordings are mostly sparse and contain only small objects. In this
work, we propose a simple tiling method that improves the detection capability
in the remote sensing case without modifying the model itself. By reducing the
background bias and enabling the usage of higher image resolutions during
training, our method can improve the performance of models substantially. The
procedure was validated on three different data sets and outperformed similar
approaches in performance and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1"&gt;Leon Amadeus Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[cs60075_team2 at SemEval-2021 Task 1 : Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora. (arXiv:2106.02340v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02340</id>
        <link href="http://arxiv.org/abs/2106.02340"/>
        <updated>2021-06-07T03:06:12.205Z</updated>
        <summary type="html"><![CDATA[This paper describes the performance of the team cs60075_team2 at SemEval
2021 Task 1 - Lexical Complexity Prediction. The main contribution of this
paper is to fine-tune transformer-based language models pre-trained on several
text corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the
corpora from which the CompLex Dataset was extracted, and others being from
other specific domains such as Finance, Law, etc. We perform ablation studies
on selecting the transformer models and how their individual complexity scores
are aggregated to get the resulting complexity scores. Our method achieves a
best Pearson Correlation of $0.784$ in sub-task 1 (single word) and $0.836$ in
sub-task 2 (multiple word expressions).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Abhilash Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adak_S/0/1/0/all/0/1"&gt;Sayantan Adak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halder_T/0/1/0/all/0/1"&gt;Tanurima Halder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokala_S/0/1/0/all/0/1"&gt;Sai Mahesh Pokala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v1 [cs.CC])]]></title>
        <id>http://arxiv.org/abs/2106.02397</id>
        <link href="http://arxiv.org/abs/2106.02397"/>
        <updated>2021-06-07T03:06:12.165Z</updated>
        <summary type="html"><![CDATA[A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, >, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.

We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.

We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1"&gt;Tillmann Miltzow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1"&gt;Reinier F. Schmiermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Contextualized Knowledge Structures for Commonsense Reasoning. (arXiv:2010.12873v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12873</id>
        <link href="http://arxiv.org/abs/2010.12873"/>
        <updated>2021-06-07T03:06:12.143Z</updated>
        <summary type="html"><![CDATA[Recently, knowledge graph (KG) augmented models have achieved noteworthy
success on various commonsense reasoning tasks. However, KG edge (fact)
sparsity and noisy edge extraction/generation often hinder models from
obtaining useful knowledge to reason over. To address these issues, we propose
a new KG-augmented model: Hybrid Graph Network (HGN). Unlike prior methods, HGN
learns to jointly contextualize extracted and generated knowledge by reasoning
over both within a unified graph structure. Given the task input context and an
extracted KG subgraph, HGN is trained to generate embeddings for the subgraph's
missing edges to form a "hybrid" graph, then reason over the hybrid graph while
filtering out context-irrelevant edges. We demonstrate HGN's effectiveness
through considerable performance gains across four commonsense reasoning
benchmarks, plus a user study on edge validness and helpfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1"&gt;Mrigank Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Aaron Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Ryan Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Handong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1"&gt;Nedim Lipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports. (arXiv:2010.14584v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14584</id>
        <link href="http://arxiv.org/abs/2010.14584"/>
        <updated>2021-06-07T03:06:12.123Z</updated>
        <summary type="html"><![CDATA[The task of text and sentence classification is associated with the need for
large amounts of labelled training data. The acquisition of high volumes of
labelled datasets can be expensive or unfeasible, especially for
highly-specialised domains for which documents are hard to obtain. Research on
the application of supervised classification based on small amounts of training
data is limited. In this paper, we address the combination of state-of-the-art
deep learning and classification methods and provide an insight into what
combination of methods fit the needs of small, domain-specific, and
terminologically-rich corpora. We focus on a real-world scenario related to a
collection of safeguarding reports comprising learning experiences and
reflections on tackling serious incidents involving children and vulnerable
adults. The relatively small volume of available reports and their use of
highly domain-specific terminology makes the application of automated
approaches difficult. We focus on the problem of automatically identifying the
main themes in a safeguarding report using supervised classification
approaches. Our results show the potential of deep learning models to simulate
subject-expert behaviour even for complex tasks with limited labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1"&gt;Aleksandra Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribaupierre_H/0/1/0/all/0/1"&gt;H&amp;#xe9;l&amp;#xe8;ne de Ribaupierre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Draw: Emergent Communication through Sketching. (arXiv:2106.02067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02067</id>
        <link href="http://arxiv.org/abs/2106.02067"/>
        <updated>2021-06-07T03:06:12.054Z</updated>
        <summary type="html"><![CDATA[Evidence that visual communication preceded written language and provided a
basis for it goes back to prehistory, in forms such as cave and rock paintings
depicting traces of our distant ancestors. Emergent communication research has
sought to explore how agents can learn to communicate in order to
collaboratively solve tasks. Existing research has focused on language, with a
learned communication channel transmitting sequences of discrete tokens between
the agents. In this work, we explore a visual communication channel between
agents that are allowed to draw with simple strokes. Our agents are
parameterised by deep neural networks, and the drawing procedure is
differentiable, allowing for end-to-end training. In the framework of a
referential communication game, we demonstrate that agents can not only
successfully learn to communicate by drawing, but with appropriate inductive
biases, can do so in a fashion that humans can interpret. We hope to encourage
future research to consider visual communication as a more flexible and
directly interpretable alternative of training collaborative agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negation in Cognitive Reasoning. (arXiv:2012.12641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12641</id>
        <link href="http://arxiv.org/abs/2012.12641"/>
        <updated>2021-06-07T03:06:12.042Z</updated>
        <summary type="html"><![CDATA[Negation is both an operation in formal logic and in natural language by
which a proposition is replaced by one stating the opposite, as by the addition
of "not" or another negation cue. Treating negation in an adequate way is
required for cognitive reasoning, which aims at modeling the human ability to
draw meaningful conclusions despite incomplete and inconsistent knowledge. One
task of cognitive reasoning is answering questions given by sentences in
natural language. There are tools based on discourse representation theory to
convert sentences automatically into a formal logic representation, and
additional knowledge can be added using the predicate names in the formula and
knowledge databases. However, the knowledge in logic databases in practice
always is incomplete. Hence, forward reasoning of automated reasoning systems
alone does not suffice to derive answers to questions because, instead of
complete proofs, often only partial positive knowledge can be derived, while
negative knowledge is used only during the reasoning process. In consequence,
we aim at eliminating syntactic negation, strictly speaking, the negated event
or property. In this paper, we describe an effective procedure to determine the
negated event or property in order to replace it by its inverse. This lays the
basis of cognitive reasoning, employing both logic and machine learning for
general question answering. We evaluate our procedure by several benchmarks and
demonstrate its practical usefulness in our cognitive reasoning system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schon_C/0/1/0/all/0/1"&gt;Claudia Schon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_S/0/1/0/all/0/1"&gt;Sophie Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1"&gt;Frieder Stolzenburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entity Concept-enhanced Few-shot Relation Extraction. (arXiv:2106.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02401</id>
        <link href="http://arxiv.org/abs/2106.02401"/>
        <updated>2021-06-07T03:06:12.027Z</updated>
        <summary type="html"><![CDATA[Few-shot relation extraction (FSRE) is of great importance in long-tail
distribution problem, especially in special domain with low-resource data. Most
existing FSRE algorithms fail to accurately classify the relations merely based
on the information of the sentences together with the recognized entity pairs,
due to limited samples and lack of knowledge. To address this problem, in this
paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction
scheme (ConceptFERE), which introduces the inherent concepts of entities to
provide clues for relation prediction and boost the relations classification
performance. Firstly, a concept-sentence attention module is developed to
select the most appropriate concept from multiple concepts of each entity by
calculating the semantic similarity between sentences and concepts. Secondly, a
self-attention based fusion module is presented to bridge the gap of concept
embedding and sentence embedding from different semantic spaces. Extensive
experiments on the FSRE benchmark dataset FewRel have demonstrated the
effectiveness and the superiority of the proposed ConceptFERE scheme as
compared to the state-of-the-art baselines. Code is available at
https://github.com/LittleGuoKe/ConceptFERE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qinghua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing the No-match: Entity Alignment with Dangling Cases. (arXiv:2106.02248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02248</id>
        <link href="http://arxiv.org/abs/2106.02248"/>
        <updated>2021-06-07T03:06:11.954Z</updated>
        <summary type="html"><![CDATA[This paper studies a new problem setting of entity alignment for knowledge
graphs (KGs). Since KGs possess different sets of entities, there could be
entities that cannot find alignment across them, leading to the problem of
dangling entities. As the first attempt to this problem, we construct a new
dataset and design a multi-task learning framework for both entity alignment
and dangling entity detection. The framework can opt to abstain from predicting
alignment for the detected dangling entities. We propose three techniques for
dangling entity detection that are based on the distribution of
nearest-neighbor distances, i.e., nearest neighbor classification, marginal
ranking and background ranking. After detecting and removing dangling entities,
an incorporated entity alignment model in our framework can provide more robust
alignment for remaining entities. Comprehensive experiments and analyses
demonstrate the effectiveness of our framework. We further discover that the
dangling entity detection module can, in turn, improve alignment learning and
the final performance. The contributed resource is publicly available to foster
further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zequn Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barcode Method for Generative Model Evaluation driven by Topological Data Analysis. (arXiv:2106.02207v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02207</id>
        <link href="http://arxiv.org/abs/2106.02207"/>
        <updated>2021-06-07T03:06:11.936Z</updated>
        <summary type="html"><![CDATA[Evaluating the performance of generative models in image synthesis is a
challenging task. Although the Fr\'echet Inception Distance is a widely
accepted evaluation metric, it integrates different aspects (e.g., fidelity and
diversity) of synthesized images into a single score and assumes the normality
of embedded vectors. Recent methods such as precision-and-recall and its
variants such as density-and-coverage have been developed to separate fidelity
and diversity based on k-nearest neighborhood methods. In this study, we
propose an algorithm named barcode, which is inspired by the topological data
analysis and is almost free of assumption and hyperparameter selections. In
extensive experiments on real-world datasets as well as theoretical approach on
high-dimensional normal samples, it was found that the 'usual' normality
assumption of embedded vectors has several drawbacks. The experimental results
demonstrate that barcode outperforms other methods in evaluating fidelity and
diversity of GAN outputs. Official codes can be found in
https://github.com/minjeekim00/Barcode.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_R/0/1/0/all/0/1"&gt;Ryoungwoo Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minjee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_D/0/1/0/all/0/1"&gt;Da-in Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyungjin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Jiyeon Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1"&gt;Namkug Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey. (arXiv:2106.02154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02154</id>
        <link href="http://arxiv.org/abs/2106.02154"/>
        <updated>2021-06-07T03:06:11.929Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper for nonlinear dimensionality and feature
extraction methods which are based on the Laplacian of graph of data. We first
introduce adjacency matrix, definition of Laplacian matrix, and the
interpretation of Laplacian. Then, we cover the cuts of graph and spectral
clustering which applies clustering in a subspace of data. Different
optimization variants of Laplacian eigenmap and its out-of-sample extension are
explained. Thereafter, we introduce the locality preserving projection and its
kernel variant as linear special cases of Laplacian eigenmap. Versions of graph
embedding are then explained which are generalized versions of Laplacian
eigenmap and locality preserving projection. Finally, diffusion map is
introduced which is a method based on Laplacian of data and random walks on the
data graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-07T03:06:11.921Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% in various state-of-the-art adversarially trained models on the
AutoAttack benchmark, where every small margin of improvement is significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02317</id>
        <link href="http://arxiv.org/abs/2106.02317"/>
        <updated>2021-06-07T03:06:11.913Z</updated>
        <summary type="html"><![CDATA[Dialogue policy learning, a subtask that determines the content of system
response generation and then the degree of task completion, is essential for
task-oriented dialogue systems. However, the unbalanced distribution of system
actions in dialogue datasets often causes difficulty in learning to generate
desired actions and responses. In this paper, we propose a
retrieve-and-memorize framework to enhance the learning of system actions.
Specially, we first design a neural context-aware retrieval module to retrieve
multiple candidate system actions from the training set given a dialogue
context. Then, we propose a memory-augmented multi-decoder network to generate
the system actions conditioned on the candidate actions, which allows the
network to adaptively select key information in the candidate actions and
ignore noises. We conduct experiments on the large-scale multi-domain
task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1.~Experimental
results show that our method achieves competitive performance among several
state-of-the-art models in the context-to-response generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jianxing Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-07T03:06:11.906Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:11.887Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation Curricula to Implicitly Train Non-Expert Annotators. (arXiv:2106.02382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02382</id>
        <link href="http://arxiv.org/abs/2106.02382"/>
        <updated>2021-06-07T03:06:11.880Z</updated>
        <summary type="html"><![CDATA[Annotation studies often require annotators to familiarize themselves with
the task, its annotation scheme, and the data domain. This can be overwhelming
in the beginning, mentally taxing, and induce errors into the resulting
annotations; especially in citizen science or crowd sourcing scenarios where
domain expertise is not required and only annotation guidelines are provided.
To alleviate these issues, we propose annotation curricula, a novel approach to
implicitly train annotators. Our goal is to gradually introduce annotators into
the task by ordering instances that are annotated according to a learning
curriculum. To do so, we first formalize annotation curricula for sentence- and
paragraph-level annotation tasks, define an ordering strategy, and identify
well-performing heuristics and interactively trained models on three existing
English datasets. We then conduct a user study with 40 voluntary participants
who are asked to identify the most fitting misconception for English tweets
about the Covid-19 pandemic. Our results show that using a simple heuristic to
order instances can already significantly reduce the total annotation time
while preserving a high annotation quality. Annotation curricula thus can
provide a novel way to improve data collection. To facilitate future research,
we further share our code and data consisting of 2,400 annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Ung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1"&gt;Jan-Christoph Klie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. (arXiv:2012.13048v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13048</id>
        <link href="http://arxiv.org/abs/2012.13048"/>
        <updated>2021-06-07T03:06:11.872Z</updated>
        <summary type="html"><![CDATA[Transformers have been shown to emulate logical deduction over natural
language theories (logical rules expressed in natural language), reliably
assigning true/false labels to candidate implications. However, their ability
to generate implications of a theory has not yet been demonstrated, and methods
for reconstructing proofs of answers are imperfect. In this work we show that a
generative model, called ProofWriter, can reliably generate both implications
of a theory and the natural language proof(s) that support them. In particular,
iterating a 1-step implication generator results in proofs that are highly
reliable, and represent actual model decisions (rather than post-hoc
rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's
proofs exceed previous methods by +9% absolute, and in a way that generalizes
to proof depths unseen in training and on out-of-domain problems. We also show
that generative techniques can perform a type of abduction with high precision:
Given a theory and an unprovable conclusion, identify a missing fact that
allows the conclusion to be proved, along with a proof. These results
significantly improve the viability of neural methods for systematically
reasoning over natural language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1"&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1"&gt;Bhavana Dalvi Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1"&gt;Peter Clark&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing. (arXiv:2106.02559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02559</id>
        <link href="http://arxiv.org/abs/2106.02559"/>
        <updated>2021-06-07T03:06:11.866Z</updated>
        <summary type="html"><![CDATA[Analysing whether neural language models encode linguistic information has
become popular in NLP. One method of doing so, which is frequently cited to
support the claim that models like BERT encode syntax, is called probing;
probes are small supervised models trained to extract linguistic information
from another model's output. If a probe is able to predict a particular
structure, it is argued that the model whose output it is trained on must have
implicitly learnt to encode it. However, drawing a generalisation about a
model's linguistic knowledge about a specific phenomena based on what a probe
is able to learn may be problematic: in this work, we show that semantic cues
in training data means that syntactic probes do not properly isolate syntax. We
generate a new corpus of semantically nonsensical but syntactically well-formed
Jabberwocky sentences, which we use to evaluate two probes trained on normal
data. We train the probes on several popular language models (BERT, GPT, and
RoBERTa), and find that in all settings they perform worse when evaluated on
these data, for one probe by an average of 15.4 UUAS points absolute. Although
in most cases they still outperform the baselines, their lead is reduced
substantially, e.g. by 53% in the case of BERT for one probe. This begs the
question: what empirical scores constitute knowing syntax?]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Democracy: Using Deep Learning to Identify and Prevent Misinformation. (arXiv:2106.02607v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02607</id>
        <link href="http://arxiv.org/abs/2106.02607"/>
        <updated>2021-06-07T03:06:11.859Z</updated>
        <summary type="html"><![CDATA[The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhm_A/0/1/0/all/0/1"&gt;Alyssa Suhm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahankal_P/0/1/0/all/0/1"&gt;Prathamesh Mahankal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuntharaj_S/0/1/0/all/0/1"&gt;Subhiksha Mukuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parab_M/0/1/0/all/0/1"&gt;Meghana D. Parab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_M/0/1/0/all/0/1"&gt;Malvika Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Meredith Berger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethumadhavan_A/0/1/0/all/0/1"&gt;Arathi Sethumadhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiman_A/0/1/0/all/0/1"&gt;Ashish Jaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Attention for Visual Question Answering. (arXiv:2011.02164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02164</id>
        <link href="http://arxiv.org/abs/2011.02164"/>
        <updated>2021-06-07T03:06:11.841Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tanzila Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding. (arXiv:2106.02318v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02318</id>
        <link href="http://arxiv.org/abs/2106.02318"/>
        <updated>2021-06-07T03:06:11.834Z</updated>
        <summary type="html"><![CDATA[Automatic extraction of product attribute values is an important enabling
technology in e-Commerce platforms. This task is usually modeled using sequence
labeling architectures, with several extensions to handle multi-attribute
extraction. One line of previous work constructs attribute-specific models,
through separate decoders or entirely separate models. However, this approach
constrains knowledge sharing across different attributes. Other contributions
use a single multi-attribute model, with different techniques to embed
attribute information. But sharing the entire network parameters across all
attributes can limit the model's capacity to capture attribute-specific
characteristics. In this paper we present AdaTag, which uses adaptive decoding
to handle extraction. We parameterize the decoder with pretrained attribute
embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This
allows for separate, but semantically correlated, decoders to be generated on
the fly for different attributes. This approach facilitates knowledge sharing,
while maintaining the specificity of each attribute. Our experiments on a
real-world e-Commerce dataset show marked improvements over previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1"&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1"&gt;Christan Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Computer Generated Dialog with Auxiliary Loss Functions and Custom Evaluation Metrics. (arXiv:2106.02516v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02516</id>
        <link href="http://arxiv.org/abs/2106.02516"/>
        <updated>2021-06-07T03:06:11.827Z</updated>
        <summary type="html"><![CDATA[Although people have the ability to engage in vapid dialogue without effort,
this may not be a uniquely human trait. Since the 1960's researchers have been
trying to create agents that can generate artificial conversation. These
programs are commonly known as chatbots. With increasing use of neural networks
for dialog generation, some conclude that this goal has been achieved. This
research joins the quest by creating a dialog generating Recurrent Neural
Network (RNN) and by enhancing the ability of this network with auxiliary loss
functions and a beam search. Our custom loss functions achieve better cohesion
and coherence by including calculations of Maximum Mutual Information (MMI) and
entropy. We demonstrate the effectiveness of this system by using a set of
custom evaluation metrics inspired by an abundance of previous research and
based on tried-and-true principles of Natural Language Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conley_T/0/1/0/all/0/1"&gt;Thomas Conley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clair_J/0/1/0/all/0/1"&gt;Jack St. Clair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-06-07T03:06:11.820Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via moral philosophy's definition of social good, propose a
framework to evaluate NLP tasks' direct and indirect real-world impact, and
adopt the methodology of global priorities research to identify priority causes
for NLP research. Finally, we use our theoretical framework to provide some
practical guidelines for future NLP research for social good. Our data and
codes are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Scaling for Universal Suggested Replies Model. (arXiv:2106.02232v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02232</id>
        <link href="http://arxiv.org/abs/2106.02232"/>
        <updated>2021-06-07T03:06:11.814Z</updated>
        <summary type="html"><![CDATA[We consider the problem of scaling automated suggested replies for Outlook
email system to multiple languages. Faced with increased compute requirements
and low resources for language expansion, we build a single universal model for
improving the quality and reducing run-time costs of our production system.
However, restricted data movement across regional centers prevents joint
training across languages. To this end, we propose a multi-task continual
learning framework, with auxiliary tasks and language adapters to learn
universal language representation across regions. The experimental results show
positive cross-lingual transfer across languages while reducing catastrophic
forgetting across regions. Our online results on real user traffic show
significant gains in CTR and characters saved, as well as 65% training cost
reduction compared with per-language models. As a consequence, we have scaled
the feature in multiple languages including low-resource markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1"&gt;Qianlan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1"&gt;Payal Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bojia Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Compress Once: Towards Effective and Elastic BERT Compression via Exploit-Explore Stochastic Nature Gradient. (arXiv:2106.02435v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02435</id>
        <link href="http://arxiv.org/abs/2106.02435"/>
        <updated>2021-06-07T03:06:11.775Z</updated>
        <summary type="html"><![CDATA[Despite superior performance on various natural language processing tasks,
pre-trained models such as BERT are challenged by deploying on
resource-constraint devices. Most existing model compression approaches require
re-compression or fine-tuning across diverse constraints to accommodate various
hardware deployments. This practically limits the further application of model
compression. Moreover, the ineffective training and searching process of
existing elastic compression paradigms[4,27] prevents the direct migration to
BERT compression. Motivated by the necessity of efficient inference across
various constraints on BERT, we propose a novel approach, YOCO-BERT, to achieve
compress once and deploy everywhere. Specifically, we first construct a huge
search space with 10^13 architectures, which covers nearly all configurations
in BERT model. Then, we propose a novel stochastic nature gradient optimization
method to guide the generation of optimal candidate architecture which could
keep a balanced trade-off between explorations and exploitation. When a certain
resource constraint is given, a lightweight distribution optimization approach
is utilized to obtain the optimal network for target deployment without
fine-tuning. Compared with state-of-the-art algorithms, YOCO-BERT provides more
compact models, yet achieving 2.1%-4.5% average accuracy improvement on the
GLUE benchmark. Besides, YOCO-BERT is also more effective, e.g.,the training
complexity is O(1)for N different devices. Code is
availablehttps://github.com/MAC-AutoML/YOCO-BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaokun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion. (arXiv:2106.02497v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02497</id>
        <link href="http://arxiv.org/abs/2106.02497"/>
        <updated>2021-06-07T03:06:11.767Z</updated>
        <summary type="html"><![CDATA[Despite recent successes of large pre-trained language models in solving
reasoning tasks, their inference capabilities remain opaque. We posit that such
models can be made more interpretable by explicitly generating interim
inference rules, and using them to guide the generation of task-specific
textual outputs. In this paper we present COINS, a recursive inference
framework that i) iteratively reads context sentences, ii) dynamically
generates contextualized inference rules, encodes them, and iii) uses them to
guide task-specific output generation. We apply COINS to a Narrative Story
Completion task that asks a model to complete a story with missing sentences,
to produce a coherent story with plausible logical connections, causal
relationships, and temporal dependencies. By modularizing inference and
sentence generation steps in a recurrent model, we aim to make reasoning steps
and their effects on next sentence generation transparent. Our automatic and
manual evaluations show that the model generates better story sentences than
SOTA baselines, especially in terms of coherence. We further demonstrate
improved performance over strong pre-trained LMs in generating commonsense
inference rules. The recursive nature of COINS holds the potential for
controlled generation of longer sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debjit Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model. (arXiv:2106.02596v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.02596</id>
        <link href="http://arxiv.org/abs/2106.02596"/>
        <updated>2021-06-07T03:06:11.759Z</updated>
        <summary type="html"><![CDATA[Stereotypical language expresses widely-held beliefs about different social
categories. Many stereotypes are overtly negative, while others may appear
positive on the surface, but still lead to negative consequences. In this work,
we present a computational approach to interpreting stereotypes in text through
the Stereotype Content Model (SCM), a comprehensive causal theory from social
psychology. The SCM proposes that stereotypes can be understood along two
primary dimensions: warmth and competence. We present a method for defining
warmth and competence axes in semantic embedding space, and show that the four
quadrants defined by this subspace accurately represent the warmth and
competence concepts, according to annotated lexicons. We then apply our
computational SCM model to textual stereotype data and show that it compares
favourably with survey-based studies in the psychological literature.
Furthermore, we explore various strategies to counter stereotypical beliefs
with anti-stereotypes. It is known that countering stereotypes with
anti-stereotypical examples is one of the most effective ways to reduce biased
thinking, yet the problem of generating anti-stereotypes has not been
previously studied. Thus, a better understanding of how to generate realistic
and effective anti-stereotypes can contribute to addressing pressing societal
concerns of stereotyping, prejudice, and discrimination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1"&gt;Kathleen C. Fraser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1"&gt;Isar Nejadgholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1"&gt;Svetlana Kiritchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene. (arXiv:2106.02327v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02327</id>
        <link href="http://arxiv.org/abs/2106.02327"/>
        <updated>2021-06-07T03:06:11.749Z</updated>
        <summary type="html"><![CDATA[The major paradigm of applying a pre-trained language model to downstream
tasks is to fine-tune it on labeled task data, which often suffers instability
and low performance when the labeled examples are scarce.~One way to alleviate
this problem is to apply post-training on unlabeled task data before
fine-tuning, adapting the pre-trained model to target domains by contrastive
learning that considers either token-level or sequence-level similarity.
Inspired by the success of sequence masking, we argue that both token-level and
sequence-level similarities can be captured with a pair of masked
sequences.~Therefore, we propose complementary random masking (CRM) to generate
a pair of masked sequences from an input sequence for sequence-level
contrastive learning and then develop contrastive masked language modeling
(CMLM) for post-training to integrate both token-level and sequence-level
contrastive learnings.~Empirical results show that CMLM surpasses several
recent post-training methods in few-shot settings without the need for data
augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Ruikun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guanhuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERICA: An Empathetic Android Companion for Covid-19 Quarantine. (arXiv:2106.02325v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02325</id>
        <link href="http://arxiv.org/abs/2106.02325"/>
        <updated>2021-06-07T03:06:11.742Z</updated>
        <summary type="html"><![CDATA[Over the past year, research in various domains, including Natural Language
Processing (NLP), has been accelerated to fight against the COVID-19 pandemic,
yet such research has just started on dialogue systems. In this paper, we
introduce an end-to-end dialogue system which aims to ease the isolation of
people under self-quarantine. We conduct a control simulation experiment to
assess the effects of the user interface, a web-based virtual agent called Nora
vs. the android ERICA via a video call. The experimental results show that the
android offers a more valuable user experience by giving the impression of
being more empathetic and engaging in the conversation due to its nonverbal
information, such as facial expressions and body gestures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1"&gt;Samuel Cahyawijaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lala_D/0/1/0/all/0/1"&gt;Divesh Lala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER. (arXiv:2106.02300v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02300</id>
        <link href="http://arxiv.org/abs/2106.02300"/>
        <updated>2021-06-07T03:06:11.684Z</updated>
        <summary type="html"><![CDATA[Neural methods have been shown to achieve high performance in Named Entity
Recognition (NER), but rely on costly high-quality labeled data for training,
which is not always available across languages. While previous works have shown
that unlabeled data in a target language can be used to improve cross-lingual
model performance, we propose a novel adversarial approach (AdvPicker) to
better leverage such data and further improve results. We design an adversarial
learning framework in which an encoder learns entity domain knowledge from
labeled source-language data and better shared features are captured via
adversarial training - where a discriminator selects less language-dependent
target-language data via similarity to the source language. Experimental
results on standard benchmark datasets well demonstrate that the proposed
method benefits strongly from this data selection process and outperforms
existing state-of-the-art methods; without requiring any additional external
resources (e.g., gazetteers or via machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weile Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huiqiang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qianhui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1"&gt;B&amp;#xf6;rje F. Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yi Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Slice-Aware Representations with Mixture of Attentions. (arXiv:2106.02363v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02363</id>
        <link href="http://arxiv.org/abs/2106.02363"/>
        <updated>2021-06-07T03:06:11.669Z</updated>
        <summary type="html"><![CDATA[Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Han Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1"&gt;Ruhi Sarikaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-language Sentence Selection via Data Augmentation and Rationale Training. (arXiv:2106.02293v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02293</id>
        <link href="http://arxiv.org/abs/2106.02293"/>
        <updated>2021-06-07T03:06:11.661Z</updated>
        <summary type="html"><![CDATA[This paper proposes an approach to cross-language sentence selection in a
low-resource setting. It uses data augmentation and negative sampling
techniques on noisy parallel sentence data to directly learn a cross-lingual
embedding-based query relevance model. Results show that this approach performs
as well as or better than multiple state-of-the-art machine translation +
monolingual retrieval systems trained on the same parallel data. Moreover, when
a rationale training secondary objective is applied to encourage the model to
match word alignment hints from a phrase-based statistical machine translation
model, consistent improvements are seen across three language pairs
(English-Somali, English-Swahili and English-Tagalog) over a variety of
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1"&gt;Chris Kedzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1"&gt;Petra Galu&amp;#x161;&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1"&gt;Kathleen McKeown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling the Unigram Distribution. (arXiv:2106.02289v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02289</id>
        <link href="http://arxiv.org/abs/2106.02289"/>
        <updated>2021-06-07T03:06:11.515Z</updated>
        <summary type="html"><![CDATA[The unigram distribution is the non-contextual probability of finding a
specific word form in a corpus. While of central importance to the study of
language, it is commonly approximated by each word's sample frequency in the
corpus. This approach, being highly dependent on sample size, assigns zero
probability to any out-of-vocabulary (oov) word form. As a result, it produces
negatively biased probabilities for any oov word form, while positively biased
probabilities to in-corpus words. In this work, we argue in favor of properly
modeling the unigram distribution -- claiming it should be a central task in
natural language processing. With this in mind, we present a novel model for
estimating it in a language (a neuralization of Goldwater et al.'s (2011)
model) and show it produces much better estimates across a diverse set of 7
languages than the na\"ive use of neural character-level language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikkarinen_I/0/1/0/all/0/1"&gt;Irene Nikkarinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1"&gt;Dami&amp;#xe1;n E. Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgreeSum: Agreement-Oriented Multi-Document Summarization. (arXiv:2106.02278v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02278</id>
        <link href="http://arxiv.org/abs/2106.02278"/>
        <updated>2021-06-07T03:06:11.483Z</updated>
        <summary type="html"><![CDATA[We aim to renew interest in a particular multi-document summarization (MDS)
task which we call AgreeSum: agreement-oriented multi-document summarization.
Given a cluster of articles, the goal is to provide abstractive summaries that
represent information common and faithful to all input articles. Given the lack
of existing datasets, we create a dataset for AgreeSum, and provide annotations
on article-summary entailment relations for a subset of the clusters in the
dataset. We aim to create strong baselines for the task by applying the
top-performing pretrained single-document summarization model PEGASUS onto
AgreeSum, leveraging both annotated clusters by supervised losses, and
unannotated clusters by T5-based entailment-related and language-related
losses. Compared to other baselines, both automatic evaluation and human
evaluation show better article-summary and cluster-summary entailment in
generated summaries. On a separate note, we hope that our article-summary
entailment annotations contribute to the community's effort in improving
abstractive summarization faithfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1"&gt;Adam D. Lelkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression. (arXiv:2106.02241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02241</id>
        <link href="http://arxiv.org/abs/2106.02241"/>
        <updated>2021-06-07T03:06:11.369Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) such as BERT adopt a training paradigm
which first pretrain the model in general data and then finetune the model on
task-specific data, and have recently achieved great success. However, PLMs are
notorious for their enormous parameters and hard to be deployed on real-life
applications. Knowledge distillation has been prevailing to address this
problem by transferring knowledge from a large teacher to a much smaller
student over a set of data. We argue that the selection of thee three key
components, namely teacher, training data, and learning objective, is crucial
to the effectiveness of distillation. We, therefore, propose a four-stage
progressive distillation framework ERNIE-Tiny to compress PLM, which varies the
three components gradually from general level to task-specific level.
Specifically, the first stage, General Distillation, performs distillation with
guidance from pretrained teacher, gerenal data and latent distillation loss.
Then, General-Enhanced Distillation changes teacher model from pretrained
teacher to finetuned teacher. After that, Task-Adaptive Distillation shifts
training data from general data to task-specific data. In the end,
Task-Specific Distillation, adds two additional losses, namely Soft-Label and
Hard-Label loss onto the last stage. Empirical results demonstrate the
effectiveness of our framework and generalization gain brought by ERNIE-Tiny.In
particular, experiments show that a 4-layer ERNIE-Tiny maintains over
98.0%performance of its 12-layer teacher BERT base on GLUE benchmark,
surpassing state-of-the-art (SOTA) by 1.0% GLUE score with the same amount of
parameters. Moreover, ERNIE-Tiny achieves a new compression SOTA on five
Chinese NLP tasks, outperforming BERT base by 0.4% accuracy with 7.5x fewer
parameters and9.4x faster inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weiyue Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shikun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1"&gt;Hao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition. (arXiv:2106.02302v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02302</id>
        <link href="http://arxiv.org/abs/2106.02302"/>
        <updated>2021-06-07T03:06:11.362Z</updated>
        <summary type="html"><![CDATA[Integrating external language models (LMs) into end-to-end (E2E) models
remains a challenging task for domain-adaptive speech recognition. Recently,
internal language model estimation (ILME)-based LM fusion has shown significant
word error rate (WER) reduction from Shallow Fusion by subtracting a weighted
internal LM score from an interpolation of E2E model and external LM scores
during beam search. However, on different test sets, the optimal LM
interpolation weights vary over a wide range and have to be tuned extensively
on well-matched validation sets. In this work, we perform LM fusion in the
minimum WER (MWER) training of an E2E model to obviate the need for LM weights
tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),
we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based
fusion is conducted to generate N-best hypotheses and their posteriors.
Additional gradient is induced when internal LM is engaged in MWER-ILME loss
computation. During inference, LM weights pre-determined in MWER training
enable robust LM integrations on test sets from different domains. Experimented
with 30K-hour trained transformer transducers, MWER-ILME achieves on average
8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,
respectively, on 6 different test sets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guoli Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Adapt Your Pretrained Multilingual Model to 1600 Languages. (arXiv:2106.02124v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02124</id>
        <link href="http://arxiv.org/abs/2106.02124"/>
        <updated>2021-06-07T03:06:11.353Z</updated>
        <summary type="html"><![CDATA[Pretrained multilingual models (PMMs) enable zero-shot learning via
cross-lingual transfer, performing best for languages seen during pretraining.
While methods exist to improve performance for unseen languages, they have
almost exclusively been evaluated using amounts of raw text only available for
a small fraction of the world's languages. In this paper, we evaluate the
performance of existing methods to adapt PMMs to new languages using a resource
available for over 1600 languages: the New Testament. This is challenging for
two reasons: (1) the small corpus size, and (2) the narrow domain. While
performance drops for all approaches, we surprisingly still see gains of up to
$17.69\%$ accuracy for part-of-speech tagging and $6.29$ F1 for NER on average
over all languages as compared to XLM-R. Another unexpected finding is that
continued pretraining, the simplest approach, performs best. Finally, we
perform a case study to disentangle the effects of domain and size and to shed
light on the influence of the finetuning source language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_A/0/1/0/all/0/1"&gt;Abteen Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1"&gt;Katharina Kann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer. (arXiv:2106.02210v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02210</id>
        <link href="http://arxiv.org/abs/2106.02210"/>
        <updated>2021-06-07T03:06:11.343Z</updated>
        <summary type="html"><![CDATA[Autoregressive models have been widely used in unsupervised text style
transfer. Despite their success, these models still suffer from the content
preservation problem that they usually ignore part of the source sentence and
generate some irrelevant words with strong styles. In this paper, we propose a
Non-Autoregressive generator for unsupervised text Style Transfer (NAST), which
alleviates the problem from two aspects. First, we observe that most words in
the transferred sentence can be aligned with related words in the source
sentence, so we explicitly model word alignments to suppress irrelevant words.
Second, existing models trained with the cycle loss align sentences in two
stylistic text spaces, which lacks fine-grained control at the word level. The
proposed non-autoregressive generator focuses on the connections between
aligned words, which learns the word-level transfer between styles. For
experiments, we integrate the proposed generator into two base models and
evaluate them on two style transfer tasks. The results show that NAST can
significantly improve the overall performance and provide explainable word
alignments. Moreover, the non-autoregressive generator achieves over 10x
speedups at inference. Our codes are available at
https://github.com/thu-coai/NAST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zikai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Henry Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qihan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dutch Named Entity Recognition and De-identification Methods for the Human Resource Domain. (arXiv:2106.02287v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02287</id>
        <link href="http://arxiv.org/abs/2106.02287"/>
        <updated>2021-06-07T03:06:11.323Z</updated>
        <summary type="html"><![CDATA[The human resource (HR) domain contains various types of privacy-sensitive
textual data, such as e-mail correspondence and performance appraisal. Doing
research on these documents brings several challenges, one of them
anonymisation. In this paper, we evaluate the current Dutch text
de-identification methods for the HR domain in four steps. First, by updating
one of these methods with the latest named entity recognition (NER) models. The
result is that the NER model based on the CoNLL 2002 corpus in combination with
the BERTje transformer give the best combination for suppressing persons
(recall 0.94) and locations (recall 0.82). For suppressing gender, DEDUCE is
performing best (recall 0.53). Second NER evaluation is based on both strict
de-identification of entities (a person must be suppressed as a person) and
third evaluation on a loose sense of de-identification (no matter what how a
person is suppressed, as long it is suppressed). In the fourth and last step a
new kind of NER dataset is tested for recognising job titles in texts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toledo_C/0/1/0/all/0/1"&gt;Cha&amp;#xef;m van Toledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_F/0/1/0/all/0/1"&gt;Friso van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spruit_M/0/1/0/all/0/1"&gt;Marco Spruit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-Adversarial Visual Question Answering. (arXiv:2106.02280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02280</id>
        <link href="http://arxiv.org/abs/2106.02280"/>
        <updated>2021-06-07T03:06:11.315Z</updated>
        <summary type="html"><![CDATA[Performance on the most commonly used Visual Question Answering dataset (VQA
v2) is starting to approach human accuracy. However, in interacting with
state-of-the-art VQA models, it is clear that the problem is far from being
solved. In order to stress test VQA models, we benchmark them against
human-adversarial examples. Human subjects interact with a state-of-the-art VQA
model, and for each image in the dataset, attempt to find a question where the
model's predicted answer is incorrect. We find that a wide range of
state-of-the-art models perform poorly when evaluated on these examples. We
conduct an extensive analysis of the collected adversarial examples and provide
guidance on future research directions. We hope that this Adversarial VQA
(AdVQA) benchmark can help drive progress in the field and advance the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1"&gt;Sasha Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1"&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magana_J/0/1/0/all/0/1"&gt;Jose Alberto Lopez Magana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances. (arXiv:2106.02227v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02227</id>
        <link href="http://arxiv.org/abs/2106.02227"/>
        <updated>2021-06-07T03:06:11.305Z</updated>
        <summary type="html"><![CDATA[Nowadays, open-domain dialogue models can generate acceptable responses
according to the historical context based on the large-scale pre-trained
language models. However, they generally concatenate the dialogue history
directly as the model input to predict the response, which we named as the flat
pattern and ignores the dynamic information flow across dialogue utterances. In
this work, we propose the DialoFlow model, in which we introduce a dynamic flow
mechanism to model the context flow, and design three training objectives to
capture the information dynamics across dialogue utterances by addressing the
semantic influence brought about by each utterance in large-scale pre-training.
Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset
demonstrate that our DialoFlow significantly outperforms the DialoGPT on the
dialogue generation task. Besides, we propose the Flow score, an effective
automatic metric for evaluating interactive human-bot conversation quality
based on the pre-trained DialoFlow, which presents high chatbot-level
correlation ($r=0.9$) with human ratings among 11 chatbots. Code and
pre-trained models will be public.
\footnote{\url{https://github.com/ictnlp/DialoFlow}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1"&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn Text-to-SQL. (arXiv:2106.02282v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02282</id>
        <link href="http://arxiv.org/abs/2106.02282"/>
        <updated>2021-06-07T03:06:11.294Z</updated>
        <summary type="html"><![CDATA[Recently, Text-to-SQL for multi-turn dialogue has attracted great interest.
Here, the user input of the current turn is parsed into the corresponding SQL
query of the appropriate database, given all previous dialogue history. Current
approaches mostly employ end-to-end models and consequently face two
challenges. First, dialogue history modeling and Text-to-SQL parsing are
implicitly combined, hence it is hard to carry out interpretable analysis and
obtain targeted improvement. Second, SQL annotation of multi-turn dialogue is
very expensive, leading to training data sparsity. In this paper, we propose a
novel decoupled multi-turn Text-to-SQL framework, where an utterance rewrite
model first explicitly solves completion of dialogue context, and then a
single-turn Text-to-SQL parser follows. A dual learning approach is also
proposed for the utterance rewrite model to address the data sparsity problem.
Compared with end-to-end approaches, the proposed decoupled method can achieve
excellent performance without any annotated in-domain data. With just a few
annotated rewrite cases, the decoupled method outperforms the released
state-of-the-art end-to-end models on both SParC and CoSQL datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Chen Hanqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1"&gt;Ruisheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Da Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mengyue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syntax-augmented Multilingual BERT for Cross-lingual Transfer. (arXiv:2106.02134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02134</id>
        <link href="http://arxiv.org/abs/2106.02134"/>
        <updated>2021-06-07T03:06:11.281Z</updated>
        <summary type="html"><![CDATA[In recent years, we have seen a colossal effort in pre-training multilingual
text encoders using large-scale corpora in many languages to facilitate
cross-lingual transfer learning. However, due to typological differences across
languages, the cross-lingual transfer is challenging. Nevertheless, language
syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous
works have shown that pre-trained multilingual encoders, such as mBERT
\cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual
transfer. This work shows that explicitly providing language syntax and
training mBERT using an auxiliary objective to encode the universal dependency
tree structure helps cross-lingual transfer. We perform rigorous experiments on
four NLP tasks, including text classification, question answering, named entity
recognition, and task-oriented semantic parsing. The experiment results show
that syntax-augmented mBERT improves cross-lingual transfer on popular
benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across
all languages. In the \emph{generalized} transfer setting, the performance
boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02242</id>
        <link href="http://arxiv.org/abs/2106.02242"/>
        <updated>2021-06-07T03:06:11.262Z</updated>
        <summary type="html"><![CDATA[Transformer has been widely adopted in Neural Machine Translation (NMT)
because of its large capacity and parallel training of sequence generation.
However, the deployment of Transformer is challenging because different
scenarios require models of different complexities and scales. Naively training
multiple Transformers is redundant in terms of both computation and memory. In
this paper, we propose a novel scalable Transformers, which naturally contains
sub-Transformers of different scales and have shared parameters. Each
sub-Transformer can be easily obtained by cropping the parameters of the
largest Transformer. A three-stage training scheme is proposed to tackle the
difficulty of training the scalable Transformers, which introduces additional
supervisions from word-level and sequence-level self-distillation. Extensive
experiments were conducted on WMT EN-De and En-Fr to validate our proposed
scalable Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijie Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. (arXiv:2106.02170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02170</id>
        <link href="http://arxiv.org/abs/2106.02170"/>
        <updated>2021-06-07T03:06:11.254Z</updated>
        <summary type="html"><![CDATA[Automatic detection of phoneme or word-like units is one of the core
objectives in zero-resource speech processing. Recent attempts employ
self-supervised training methods, such as contrastive predictive coding (CPC),
where the next frame is predicted given past context. However, CPC only looks
at the audio signal's frame-level structure. We overcome this limitation with a
segmental contrastive predictive coding (SCPC) framework that can model the
signal structure at a higher level e.g. at the phoneme level. In this
framework, a convolutional neural network learns frame-level representation
from the raw waveform via noise-contrastive estimation (NCE). A differentiable
boundary detector finds variable-length segments, which are then used to
optimize a segment encoder via NCE to learn segment representations. The
differentiable boundary detector allows us to train frame-level and
segment-level encoders jointly. Typically, phoneme and word segmentation are
treated as separate tasks. We unify them and experimentally show that our
single model outperforms existing phoneme and word segmentation methods on
TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and
when is the right time to include the segmental loss in the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhati_S/0/1/0/all/0/1"&gt;Saurabhchand Bhati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Villalba_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Villalba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTTune: Fine-Tuning Neural Machine Translation with BERTScore. (arXiv:2106.02208v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02208</id>
        <link href="http://arxiv.org/abs/2106.02208"/>
        <updated>2021-06-07T03:06:11.245Z</updated>
        <summary type="html"><![CDATA[Neural machine translation models are often biased toward the limited
translation references seen during training. To amend this form of overfitting,
in this paper we propose fine-tuning the models with a novel training objective
based on the recently-proposed BERTScore evaluation metric. BERTScore is a
scoring function based on contextual embeddings that overcomes the typical
limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing
translations that are different from the references, yet close in the
contextual embedding space, to be treated as substantially correct. To be able
to use BERTScore as a training objective, we propose three approaches for
generating soft predictions, allowing the network to remain completely
differentiable end-to-end. Experiments carried out over four, diverse language
pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up
to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1"&gt;Inigo Jauregi Unanue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnell_J/0/1/0/all/0/1"&gt;Jacob Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1"&gt;Massimo Piccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Embeddings for Typology and Cross-lingual Transfer Learning. (arXiv:2106.02082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02082</id>
        <link href="http://arxiv.org/abs/2106.02082"/>
        <updated>2021-06-07T03:06:11.236Z</updated>
        <summary type="html"><![CDATA[Cross-lingual language tasks typically require a substantial amount of
annotated data or parallel translation data. We explore whether language
representations that capture relationships among languages can be learned and
subsequently leveraged in cross-lingual tasks without the use of parallel data.
We generate dense embeddings for 29 languages using a denoising autoencoder,
and evaluate the embeddings using the World Atlas of Language Structures (WALS)
and two extrinsic tasks in a zero-shot setting: cross-lingual dependency
parsing and cross-lingual natural language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Taiqi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1"&gt;Kenji Sagae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding 'Grounding' in NLP. (arXiv:2106.02192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02192</id>
        <link href="http://arxiv.org/abs/2106.02192"/>
        <updated>2021-06-07T03:06:11.226Z</updated>
        <summary type="html"><![CDATA[The NLP community has seen substantial recent interest in grounding to
facilitate interaction between language technologies and the world. However, as
a community, we use the term broadly to reference any linking of text to data
or non-textual modality. In contrast, Cognitive Science more formally defines
"grounding" as the process of establishing what mutual information is required
for successful communication between two interlocutors -- a definition which
might implicitly capture the NLP usage but differs in intent and scope. We
investigate the gap between these definitions and seek answers to the following
questions: (1) What aspects of grounding are missing from NLP tasks? Here we
present the dimensions of coordination, purviews and constraints. (2) How is
the term "grounding" used in the current research? We study the trends in
datasets, domains, and tasks introduced in recent NLP conferences. And finally,
(3) How to advance our current definition to bridge the gap with Cognitive
Science? We present ways to both create new tasks or repurpose existing ones to
make advancements towards achieving a more complete sense of grounding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1"&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan W Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Inquiries about History: An Efficient and Practical Framework for Evaluating Open-domain Chatbot Consistency. (arXiv:2106.02228v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02228</id>
        <link href="http://arxiv.org/abs/2106.02228"/>
        <updated>2021-06-07T03:06:11.204Z</updated>
        <summary type="html"><![CDATA[A good open-domain chatbot should avoid presenting contradictory responses
about facts or opinions in a conversational session, known as its consistency
capacity. However, evaluating the consistency capacity of a chatbot is still
challenging. Employing human judges to interact with chatbots on purpose to
check their capacities is costly and low-efficient, and difficult to get rid of
subjective bias. In this paper, we propose the Addressing Inquiries about
History (AIH), an efficient and practical framework for the consistency
evaluation. At the conversation stage, AIH attempts to address appropriate
inquiries about the dialogue history to induce the chatbot to redeclare the
historical facts or opinions. We carry out the conversation between chatbots,
which is more efficient than the human-bot interaction and can also alleviate
the subjective bias. In this way, we manage to rapidly obtain a dialog session
that contains responses with high contradiction possibilities. At the
contradiction recognition stage, we can either employ human judges or a natural
language inference (NLI) model to recognize whether the answers to the
inquiries are contradictory with history. Finally, we are able to rank chatbots
according to the contradiction statistics. Experiments on open-domain chatbots
show that our approach can efficiently and reliably assess the consistency
capacity of chatbots and achieve a high ranking correlation with the human
evaluation. We release the framework and hope to help improve the consistency
capacity of chatbots. \footnote{\url{https://github.com/ictnlp/AIH}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1"&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-07T03:06:11.194Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nmT5 -- Is parallel data still relevant for pre-training massively multilingual language models?. (arXiv:2106.02171v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02171</id>
        <link href="http://arxiv.org/abs/2106.02171"/>
        <updated>2021-06-07T03:06:11.185Z</updated>
        <summary type="html"><![CDATA[Recently, mT5 - a massively multilingual version of T5 - leveraged a unified
text-to-text format to attain state-of-the-art results on a wide variety of
multilingual NLP tasks. In this paper, we investigate the impact of
incorporating parallel data into mT5 pre-training. We find that multi-tasking
language modeling with objectives such as machine translation during
pre-training is a straightforward way to improve performance on downstream
multilingual and cross-lingual tasks. However, the gains start to diminish as
the model capacity increases, suggesting that parallel data might not be as
essential for larger models. At the same time, even at larger model sizes, we
find that pre-training with parallel data still provides benefits in the
limited labelled data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1"&gt;Aditya Siddhant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Melvin Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1"&gt;Rami Al-Rfou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Linting Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. (arXiv:2005.12964v9 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12964</id>
        <link href="http://arxiv.org/abs/2005.12964"/>
        <updated>2021-06-07T03:06:11.174Z</updated>
        <summary type="html"><![CDATA[Deep candidate generation (DCG) that narrows down the collection of relevant
items from billions to hundreds via representation learning has become
prevalent in industrial recommender systems. Standard approaches approximate
maximum likelihood estimation (MLE) through sampling for better scalability and
address the problem of DCG in a way similar to language modeling. However, live
recommender systems face severe exposure bias and have a vocabulary several
orders of magnitude larger than that of natural language, implying that MLE
will preserve and even exacerbate the exposure bias in the long run in order to
faithfully fit the observed samples. In this paper, we theoretically prove that
a popular choice of contrastive loss is equivalent to reducing the exposure
bias via inverse propensity weighting, which provides a new perspective for
understanding the effectiveness of contrastive learning. Based on the
theoretical discovery, we design CLRec, a contrastive learning method to
improve DCG in terms of fairness, effectiveness and efficiency in recommender
systems with extremely large candidate size. We further improve upon CLRec and
propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our
methods have been successfully deployed in Taobao, where at least four-month
online A/B tests and offline analyses demonstrate its substantial improvements,
including a dramatic reduction in the Matthew effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in Chatbots. (arXiv:2106.02076v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.02076</id>
        <link href="http://arxiv.org/abs/2106.02076"/>
        <updated>2021-06-07T03:06:11.122Z</updated>
        <summary type="html"><![CDATA[Chatbots are popular machine partners for task-oriented and social
interactions. Human-human computer-mediated communication research has explored
how people express their gender and sexuality in online social interactions,
but little is known about whether and in what way chatbots do the same. We
conducted semi-structured interviews with 5 text-based conversational agents to
explore this topic Through these interviews, we identified 6 common themes
around the expression of gender and sexual identity: identity description,
identity formation, peer acceptance, positive reflection, uncomfortable
feelings and off-topic responses. Chatbots express gender and sexuality
explicitly and through relation of experience and emotions, mimicking the human
language on which they are trained. It is nevertheless evident that chatbots
differ from human dialogue partners as they lack the flexibility and
understanding enabled by lived human experience. While chatbots are proficient
in using language to express identity, they also display a lack of authentic
experiences of gender and sexuality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_J/0/1/0/all/0/1"&gt;Justin Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_L/0/1/0/all/0/1"&gt;Leigh Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrone_A/0/1/0/all/0/1"&gt;Allison Perrone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A diachronic evaluation of gender asymmetry in euphemism. (arXiv:2106.02083v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02083</id>
        <link href="http://arxiv.org/abs/2106.02083"/>
        <updated>2021-06-07T03:06:11.095Z</updated>
        <summary type="html"><![CDATA[The use of euphemisms is a known driver of language change. It has been
proposed that women use euphemisms more than men. Although there have been
several studies investigating gender differences in language, the claim about
euphemism usage has not been tested comprehensively through time. If women do
use euphemisms more, this could mean that women also lead the formation of new
euphemisms and language change over time. Using four large diachronic text
corpora of English, we evaluate the claim that women use euphemisms more than
men through a quantitative analysis. We assembled a list of 106 euphemism-taboo
pairs to analyze their relative use through time by each gender in the corpora.
Contrary to the existing belief, our results show that women do not use
euphemisms with a higher proportion than men. We repeated the analysis using
different subsets of the euphemism-taboo pairs list and found that our result
was robust. Our study indicates that in a broad range of settings involving
both speech and writing, and with varying degrees of formality, women do not
use or form euphemisms more than men.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapron_King_A/0/1/0/all/0/1"&gt;Anna Kapron-King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Equal Gender Representation in the Annotations of Toxic Language Detection. (arXiv:2106.02183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02183</id>
        <link href="http://arxiv.org/abs/2106.02183"/>
        <updated>2021-06-07T03:06:11.070Z</updated>
        <summary type="html"><![CDATA[Classifiers tend to propagate biases present in the data on which they are
trained. Hence, it is important to understand how the demographic identities of
the annotators of comments affect the fairness of the resulting model. In this
paper, we focus on the differences in the ways men and women annotate comments
for toxicity, investigating how these differences result in models that amplify
the opinions of male annotators. We find that the BERT model as-sociates toxic
comments containing offensive words with male annotators, causing the model to
predict 67.7% of toxic comments as having been annotated by men. We show that
this disparity between gender predictions can be mitigated by removing
offensive words and highly toxic comments from the training data. We then apply
the learned associations between gender and language to toxic language
classifiers, finding that models trained exclusively on female-annotated data
perform 1.8% better than those trained solely on male-annotated data and that
training models on data after removing all offensive words reduces bias in the
model by 55.5% while increasing the sensitivity by 0.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Excell_E/0/1/0/all/0/1"&gt;Elizabeth Excell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:10.740Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Insights into Metric Optimization for Ranking-based Recommendation. (arXiv:2106.02545v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02545</id>
        <link href="http://arxiv.org/abs/2106.02545"/>
        <updated>2021-06-07T03:06:10.712Z</updated>
        <summary type="html"><![CDATA[Direct optimization of IR metrics has often been adopted as an approach to
devise and develop ranking-based recommender systems. Most methods following
this approach aim at optimizing the same metric being used for evaluation,
under the assumption that this will lead to the best performance. A number of
studies of this practice bring this assumption, however, into question. In this
paper, we dig deeper into this issue in order to learn more about the effects
of the choice of the metric to optimize on the performance of a ranking-based
recommender system. We present an extensive experimental study conducted on
different datasets in both pairwise and listwise learning-to-rank scenarios, to
compare the relative merit of four popular IR metrics, namely RR, AP, nDCG and
RBP, when used for optimization and assessment of recommender systems in
various combinations. For the first three, we follow the practice of loss
function formulation available in literature. For the fourth one, we propose
novel loss functions inspired by RBP for both the pairwise and listwise
scenario. Our results confirm that the best performance is indeed not
necessarily achieved when optimizing the same metric being used for evaluation.
In fact, we find that RBP-inspired losses perform at least as well as other
metrics in a consistent way, and offer clear benefits in several cases.
Interesting to see is that RBP-inspired losses, while improving the
recommendation performance for all uses, may lead to an individual performance
gain that is correlated with the activity level of a user in interacting with
items. The more active the users, the more they benefit. Overall, our results
challenge the assumption behind the current research practice of optimizing and
evaluating the same metric, and point to RBP-based optimization instead as a
promising alternative when learning to rank in the recommendation context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Roger Zhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urbano_J/0/1/0/all/0/1"&gt;Juli&amp;#xe1;n Urbano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanjalic_A/0/1/0/all/0/1"&gt;Alan Hanjalic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search. (arXiv:2004.09424v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09424</id>
        <link href="http://arxiv.org/abs/2004.09424"/>
        <updated>2021-06-07T03:06:10.697Z</updated>
        <summary type="html"><![CDATA[Product search has been a crucial entry point to serve people shopping
online. Most existing personalized product models follow the paradigm of
representing and matching user intents and items in the semantic space, where
finer-grained matching is totally discarded and the ranking of an item cannot
be explained further than just user/item level similarity. In addition, while
some models in existing studies have created dynamic user representations based
on search context, their representations for items are static across all search
sessions. This makes every piece of information about the item always equally
important in representing the item during matching with various user intents.
Aware of the above limitations, we propose a review-based transformer model
(RTM) for personalized product search, which encodes the sequence of query,
user reviews, and item reviews with a transformer architecture. RTM conducts
review-level matching between the user and item, where each review has a
dynamic effect according to the context in the sequence. This makes it possible
to identify useful reviews to explain the scoring. Experimental results show
that RTM significantly outperforms state-of-the-art personalized product search
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1"&gt;Keping Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1"&gt;W. Bruce Croft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:10.657Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Elastic Embeddings for Customizing On-Device Recommenders. (arXiv:2106.02223v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02223</id>
        <link href="http://arxiv.org/abs/2106.02223"/>
        <updated>2021-06-07T03:06:10.644Z</updated>
        <summary type="html"><![CDATA[In today's context, deploying data-driven services like recommendation on
edge devices instead of cloud servers becomes increasingly attractive due to
privacy and network latency concerns. A common practice in building compact
on-device recommender systems is to compress their embeddings which are
normally the cause of excessive parameterization. However, despite the vast
variety of devices and their associated memory constraints, existing
memory-efficient recommender systems are only specialized for a fixed memory
budget in every design and training life cycle, where a new model has to be
retrained to obtain the optimal performance while adapting to a smaller/larger
memory budget. In this paper, we present a novel lightweight recommendation
paradigm that allows a well-trained recommender to be customized for arbitrary
device-specific memory constraints without retraining. The core idea is to
compose elastic embeddings for each item, where an elastic embedding is the
concatenation of a set of embedding blocks that are carefully chosen by an
automated search function. Correspondingly, we propose an innovative approach,
namely recommendation with universally learned elastic embeddings (RULE). To
ensure the expressiveness of all candidate embedding blocks, RULE enforces a
diversity-driven regularization when learning different embedding blocks. Then,
a performance estimator-based evolutionary search function is designed,
allowing for efficient specialization of elastic embeddings under any memory
constraint for on-device recommendation. Extensive experiments on real-world
datasets reveal the superior performance of RULE under tight memory budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yujia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Social Media Background to Improve Cold-start Recommendation Deep Models. (arXiv:2106.02256v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02256</id>
        <link href="http://arxiv.org/abs/2106.02256"/>
        <updated>2021-06-07T03:06:10.401Z</updated>
        <summary type="html"><![CDATA[In recommender systems, a cold-start problem occurs when there is no past
interaction record associated with the user or item. Typical solutions to the
cold-start problem make use of contextual information, such as user demographic
attributes or product descriptions. A group of works have shown that social
media background can help predicting temporal phenomenons such as product sales
and stock price movements. In this work, our goal is to investigate whether
social media background can be used as extra contextual information to improve
recommendation models. Based on an existing deep neural network model, we
proposed a method to represent temporal social media background as embeddings
and fuse them as an extra component in the model. We conduct experimental
evaluations on a real-world e-commerce dataset and a Twitter dataset. The
results show that our method of fusing social media background with the
existing model does generally improve recommendation performance. In some cases
the recommendation accuracy measured by hit-rate@K doubles after fusing with
social media background. Our findings can be beneficial for future recommender
system designs that consider complex temporal information representing social
interests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maekawa_T/0/1/0/all/0/1"&gt;Takuya Maekawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1"&gt;Takahiro Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facade-X: an opinionated approach to SPARQL anything. (arXiv:2106.02361v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.02361</id>
        <link href="http://arxiv.org/abs/2106.02361"/>
        <updated>2021-06-07T03:06:10.383Z</updated>
        <summary type="html"><![CDATA[The Semantic Web research community understood since its beginning how
crucial it is to equip practitioners with methods to transform non-RDF
resources into RDF. Proposals focus on either engineering content
transformations or accessing non-RDF resources with SPARQL. Existing solutions
require users to learn specific mapping languages (e.g. RML), to know how to
query and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to
combine multiple languages (e.g. SPARQL Generate). In this paper, we explore an
alternative solution and contribute a general-purpose meta-model for converting
non-RDF resources into RDF: Facade-X. Our approach can be implemented by
overriding the SERVICE operator and does not require to extend the SPARQL
syntax. We compare our approach with the state of art methods RML and SPARQL
Generate and show how our solution has lower learning demands and cognitive
complexity, and it is cheaper to implement and maintain, while having
comparable extensibility and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daga_E/0/1/0/all/0/1"&gt;Enrico Daga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1"&gt;Luigi Asprino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulholland_P/0/1/0/all/0/1"&gt;Paul Mulholland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1"&gt;Aldo Gangemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A General Method for Event Detection on Social Media. (arXiv:2106.02250v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02250</id>
        <link href="http://arxiv.org/abs/2106.02250"/>
        <updated>2021-06-07T03:06:10.358Z</updated>
        <summary type="html"><![CDATA[Event detection on social media has attracted a number of researches, given
the recent availability of large volumes of social media discussions. Previous
works on social media event detection either assume a specific type of event,
or assume certain behavior of observed variables. In this paper, we propose a
general method for event detection on social media that makes few assumptions.
The main assumption we make is that when an event occurs, affected semantic
aspects will behave differently from its usual behavior. We generalize the
representation of time units based on word embeddings of social media text, and
propose an algorithm to detect events in time series in a general sense. In the
experimental evaluation, we use a novel setting to test if our method and
baseline methods can exhaustively catch all real-world news in the test period.
The evaluation results show that when the event is quite unusual with regard to
the base social media discussion, it can be captured more effectively with our
method. Our method can be easily implemented and can be treated as a starting
point for more specific applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirakawa_M/0/1/0/all/0/1"&gt;Masumi Shirakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1"&gt;Takahiro Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-language Sentence Selection via Data Augmentation and Rationale Training. (arXiv:2106.02293v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02293</id>
        <link href="http://arxiv.org/abs/2106.02293"/>
        <updated>2021-06-07T03:06:10.316Z</updated>
        <summary type="html"><![CDATA[This paper proposes an approach to cross-language sentence selection in a
low-resource setting. It uses data augmentation and negative sampling
techniques on noisy parallel sentence data to directly learn a cross-lingual
embedding-based query relevance model. Results show that this approach performs
as well as or better than multiple state-of-the-art machine translation +
monolingual retrieval systems trained on the same parallel data. Moreover, when
a rationale training secondary objective is applied to encourage the model to
match word alignment hints from a phrase-based statistical machine translation
model, consistent improvements are seen across three language pairs
(English-Somali, English-Swahili and English-Tagalog) over a variety of
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1"&gt;Chris Kedzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1"&gt;Petra Galu&amp;#x161;&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1"&gt;Kathleen McKeown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Resource Reservation for Crowdsourced Live Video Streaming Applications in Geo-Distributed Cloud Environment. (arXiv:2106.02420v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02420</id>
        <link href="http://arxiv.org/abs/2106.02420"/>
        <updated>2021-06-07T03:06:10.260Z</updated>
        <summary type="html"><![CDATA[Crowdsourced live video streaming (livecast) services such as Facebook Live,
YouNow, Douyu and Twitch are gaining more momentum recently. Allocating the
limited resources in a cost-effective manner while maximizing the Quality of
Service (QoS) through real-time delivery and the provision of the appropriate
representations for all viewers is a challenging problem. In our paper, we
introduce a machine-learning based predictive resource allocation framework for
geo-distributed cloud sites, considering the delay and quality constraints to
guarantee the maximum QoS for viewers and the minimum cost for content
providers. First, we present an offline optimization that decides the required
transcoding resources in distributed regions near the viewers with a trade-off
between the QoS and the overall cost. Second, we use machine learning to build
forecasting models that proactively predict the approximate transcoding
resources to be reserved at each cloud site ahead of time. Finally, we develop
a Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource
allocation of real-time broadcasted videos on the rented resources. Extensive
simulations have shown that GNCA outperforms the state-of-the art resource
allocation approaches for crowdsourced live streaming by achieving more than
20% gain in terms of system cost while serving the viewers with relatively
lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1"&gt;Fatima Haouari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_K/0/1/0/all/0/1"&gt;Kashif Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding. (arXiv:2104.07070v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07070</id>
        <link href="http://arxiv.org/abs/2104.07070"/>
        <updated>2021-06-04T01:12:31.962Z</updated>
        <summary type="html"><![CDATA[In recent years self-supervised learning has emerged as a promising candidate
for unsupervised representation learning. In the visual domain its applications
are mostly studied in the context of images of natural scenes. However, its
applicability is especially interesting in specific areas, like remote sensing
and medicine, where it is hard to obtain huge amounts of labeled data. In this
work, we conduct an extensive analysis of the applicability of self-supervised
learning in remote sensing image classification. We analyze the influence of
the number and domain of images used for self-supervised pre-training on the
performance on downstream tasks. We show that, for the downstream task of
remote sensing image classification, using self-supervised pre-training on
remote sensing images can give better results than using supervised
pre-training on images of natural scenes. Besides, we also show that
self-supervised pre-training can be easily extended to multispectral images
producing even better results on our downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stojnic_V/0/1/0/all/0/1"&gt;Vladan Stojni&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Risojevic_V/0/1/0/all/0/1"&gt;Vladimir Risojevi&amp;#x107;&lt;/a&gt; (1) ((1) Faculty of Electrical Engineering, University of Banja Luka, Bosnia and Herzegovina)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Never Cluster Alone. (arXiv:2106.01908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01908</id>
        <link href="http://arxiv.org/abs/2106.01908"/>
        <updated>2021-06-04T01:12:31.953Z</updated>
        <summary type="html"><![CDATA[Recent advances in self-supervised learning with instance-level contrastive
objectives facilitate unsupervised clustering. However, a standalone datum is
not perceiving the context of the holistic cluster, and may undergo sub-optimal
assignment. In this paper, we extend the mainstream contrastive learning
paradigm to a cluster-level scheme, where all the data subjected to the same
cluster contribute to a unified representation that encodes the context of each
data group. Contrastive learning with this representation then rewards the
assignment of each datum. To implement this vision, we propose twin-contrast
clustering (TCC). We define a set of categorical variables as clustering
assignment confidence, which links the instance-level learning track with the
cluster-level one. On one hand, with the corresponding assignment variables
being the weight, a weighted aggregation along the data points implements the
set representation of a cluster. We further propose heuristic cluster
augmentation equivalents to enable cluster-level contrastive learning. On the
other hand, we derive the evidence lower-bound of the instance-level
contrastive objective with the assignments. By reparametrizing the assignment
variables, TCC is trained end-to-end, requiring no alternating steps. Extensive
experiments show that TCC outperforms the state-of-the-art on challenging
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Ziyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Menghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models. (arXiv:2106.01950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01950</id>
        <link href="http://arxiv.org/abs/2106.01950"/>
        <updated>2021-06-04T01:12:31.948Z</updated>
        <summary type="html"><![CDATA[Mechanisms for encoding positional information are central for
transformer-based language models. In this paper, we analyze the position
embeddings of existing language models, finding strong evidence of translation
invariance, both for the embeddings themselves and for their effect on
self-attention. The degree of translation invariance increases during training
and correlates positively with model performance. Our findings lead us to
propose translation-invariant self-attention (TISA), which accounts for the
relative position between tokens in an interpretable fashion without needing
conventional position embeddings. Our proposal has several theoretical
advantages over existing position-representation approaches. Experiments show
that it improves on regular ALBERT on GLUE tasks, while only adding orders of
magnitude less positional parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wennberg_U/0/1/0/all/0/1"&gt;Ulme Wennberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1"&gt;Gustav Eje Henter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Spatio-Temporal Person Re-identification: Algorithm and Benchmark. (arXiv:2105.15076v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15076</id>
        <link href="http://arxiv.org/abs/2105.15076"/>
        <updated>2021-06-04T01:12:31.942Z</updated>
        <summary type="html"><![CDATA[Person re-identification (re-ID) in the scenario with large spatial and
temporal spans has not been fully explored. This is partially because that,
existing benchmark datasets were mainly collected with limited spatial and
temporal ranges, e.g., using videos recorded in a few days by cameras in a
specific region of the campus. Such limited spatial and temporal ranges make it
hard to simulate the difficulties of person re-ID in real scenarios. In this
work, we contribute a novel Large-scale Spatio-Temporal (LaST) person re-ID
dataset, including 10,860 identities with more than 224k images. Compared with
existing datasets, LaST presents more challenging and high-diversity reID
settings, and significantly larger spatial and temporal ranges. For instance,
each person can appear in different cities or countries, and in various time
slots from daytime to night, and in different seasons from spring to winter. To
our best knowledge, LaST is a novel person re-ID dataset with the largest
spatiotemporal ranges. Based on LaST, we verified its challenge by conducting a
comprehensive performance evaluation of 14 re-ID algorithms. We further propose
an easy-to-implement baseline that works well on such challenging re-ID
setting. We also verified that models pre-trained on LaST can generalize well
on existing datasets with short-term and cloth-changing scenarios. We expect
LaST to inspire future works toward more realistic and challenging re-ID tasks.
More information about the dataset is available at
https://github.com/shuxjweb/last.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiujun Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]. (arXiv:2105.15034v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15034</id>
        <link href="http://arxiv.org/abs/2105.15034"/>
        <updated>2021-06-04T01:12:31.924Z</updated>
        <summary type="html"><![CDATA[In their recent paper titled "Large Associative Memory Problem in
Neurobiology and Machine Learning" [arXiv:2008.06996] the authors gave a
biologically plausible microscopic theory from which one can recover many dense
associative memory models discussed in the literature. We show that the layers
of the recent "MLP-mixer" [arXiv:2105.01601] as well as the essentially
equivalent model in [arXiv:2105.02723] are amongst them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Michael Kopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02019</id>
        <link href="http://arxiv.org/abs/2106.02019"/>
        <updated>2021-06-04T01:12:31.919Z</updated>
        <summary type="html"><![CDATA[We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1"&gt;Marc Habermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1"&gt;Viktor Rudnev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1"&gt;Kripasindhu Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Multilabel System for Automatic Music Emotion Recognition. (arXiv:1905.12629v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12629</id>
        <link href="http://arxiv.org/abs/1905.12629"/>
        <updated>2021-06-04T01:12:31.913Z</updated>
        <summary type="html"><![CDATA[Achieving advancements in automatic recognition of emotions that music can
induce require considering multiplicity and simultaneity of emotions.
Comparison of different machine learning algorithms performing multilabel and
multiclass classification is the core of our work. The study analyzes the
implementation of the Geneva Emotional Music Scale 9 in the Emotify music
dataset and investigates its adoption from a machine-learning perspective. We
approach the scenario of emotions expression/induction through music as a
multilabel and multiclass problem, where multiple emotion labels can be adopted
for the same music track by each annotator (multilabel), and each emotion can
be identified or not in the music (multiclass). The aim is the automatic
recognition of induced emotions through music.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paolizzo_F/0/1/0/all/0/1"&gt;Fabio Paolizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pichierri_N/0/1/0/all/0/1"&gt;Natalia Pichierri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casali_D/0/1/0/all/0/1"&gt;Daniele Casali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giardino_D/0/1/0/all/0/1"&gt;Daniele Giardino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matta_M/0/1/0/all/0/1"&gt;Marco Matta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costantini_G/0/1/0/all/0/1"&gt;Giovanni Costantini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipative Video Transformer. (arXiv:2106.02036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02036</id>
        <link href="http://arxiv.org/abs/2106.02036"/>
        <updated>2021-06-04T01:12:31.904Z</updated>
        <summary type="html"><![CDATA[We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation. (arXiv:2105.04447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04447</id>
        <link href="http://arxiv.org/abs/2105.04447"/>
        <updated>2021-06-04T01:12:31.898Z</updated>
        <summary type="html"><![CDATA[We propose a novel scene flow estimation approach to capture and infer 3D
motions from point clouds. Estimating 3D motions for point clouds is
challenging, since a point cloud is unordered and its density is significantly
non-uniform. Such unstructured data poses difficulties in matching
corresponding points between point clouds, leading to inaccurate flow
estimation. We propose a novel architecture named Sparse
Convolution-Transformer Network (SCTN) that equips the sparse convolution with
the transformer. Specifically, by leveraging the sparse convolution, SCTN
transfers irregular point cloud into locally consistent flow features for
estimating continuous and consistent motions within an object/local object
part. We further propose to explicitly learn point relations using a point
transformer module, different from exiting methods. We show that the learned
relation-based contextual information is rich and helpful for matching
corresponding points, benefiting scene flow estimation. In addition, a novel
loss function is proposed to adaptively encourage flow consistency according to
feature similarity. Extensive experiments demonstrate that our proposed
approach achieves a new state of the art in scene flow estimation. Our approach
achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene
Flow respectively, which significantly outperforms previous methods by large
margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Cheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1"&gt;Silvio Giancola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CFPNet: Channel-wise Feature Pyramid for Real-Time Semantic Segmentation. (arXiv:2103.12212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12212</id>
        <link href="http://arxiv.org/abs/2103.12212"/>
        <updated>2021-06-04T01:12:31.876Z</updated>
        <summary type="html"><![CDATA[Real-time semantic segmentation is playing a more important role in computer
vision, due to the growing demand for mobile devices and autonomous driving.
Therefore, it is very important to achieve a good trade-off among performance,
model size and inference speed. In this paper, we propose a Channel-wise
Feature Pyramid (CFP) module to balance those factors. Based on the CFP module,
we built CFPNet for real-time semantic segmentation which applied a series of
dilated convolution channels to extract effective features. Experiments on
Cityscapes and CamVid datasets show that the proposed CFPNet achieves an
effective combination of those factors. For the Cityscapes test dataset, CFPNet
achieves 70.1% class-wise mIoU with only 0.55 million parameters and 2.5 MB
memory. The inference speed can reach 30 FPS on a single RTX 2080Ti GPU with a
1024x2048-pixel image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1"&gt;Ange Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic radiomics: a new methodology to extract quantitative time-related features from tomographic images. (arXiv:2011.00454v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00454</id>
        <link href="http://arxiv.org/abs/2011.00454"/>
        <updated>2021-06-04T01:12:31.854Z</updated>
        <summary type="html"><![CDATA[The feature extraction methods of radiomics are mainly based on static
tomographic images at a certain moment, while the occurrence and development of
disease is a dynamic process that cannot be fully reflected by only static
characteristics. This study proposes a new dynamic radiomics feature extraction
workflow that uses time-dependent tomographic images of the same patient,
focuses on the changes in image features over time, and then quantifies them as
new dynamic features for diagnostic or prognostic evaluation. We first define
the mathematical paradigm of dynamic radiomics and introduce three specific
methods that can describe the transformation process of features over time.
Three different clinical problems are used to validate the performance of the
proposed dynamic feature with conventional 2D and 3D static features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Che_F/0/1/0/all/0/1"&gt;Fengying Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_R/0/1/0/all/0/1"&gt;Ruichuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weixing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaoyu Cui&lt;/a&gt; (Member, IEEE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Panoramic annular SLAM with loop closure and global optimization. (arXiv:2102.13400v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13400</id>
        <link href="http://arxiv.org/abs/2102.13400"/>
        <updated>2021-06-04T01:12:31.798Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose panoramic annular simultaneous localization and
mapping (PA-SLAM), a visual SLAM system based on panoramic annular lens. A
hybrid point selection strategy is put forward in the tracking front-end, which
ensures repeatability of keypoints and enables loop closure detection based on
the bag-of-words approach. Every detected loop candidate is verified
geometrically and the $Sim(3)$ relative pose constraint is estimated to perform
pose graph optimization and global bundle adjustment in the back-end. A
comprehensive set of experiments on real-world datasets demonstrates that the
hybrid point selection strategy allows reliable loop closure detection, and the
accumulated error and scale drift have been significantly reduced via global
optimization, enabling PA-SLAM to reach state-of-the-art accuracy while
maintaining high robustness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weijian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1"&gt;Jian Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaiwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints. (arXiv:2102.12827v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12827</id>
        <link href="http://arxiv.org/abs/2102.12827"/>
        <updated>2021-06-04T01:12:31.792Z</updated>
        <summary type="html"><![CDATA[Evaluating adversarial robustness amounts to finding the minimum perturbation
needed to have an input sample misclassified. The inherent complexity of the
underlying optimization requires current gradient-based attacks to be carefully
tuned, initialized, and possibly executed for many computationally-demanding
iterations, even if specialized to a given perturbation model. In this work, we
overcome these limitations by proposing a fast minimum-norm (FMN) attack that
works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$),
is robust to hyperparameter choices, does not require adversarial starting
points, and converges within few lightweight steps. It works by iteratively
finding the sample misclassified with maximum confidence within an
$\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to
minimize the distance of the current sample to the decision boundary. Extensive
experiments show that FMN significantly outperforms existing attacks in terms
of convergence speed and computation time, while reporting comparable or even
smaller perturbation sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Equilibrium Architectures for Inverse Problems in Imaging. (arXiv:2102.07944v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07944</id>
        <link href="http://arxiv.org/abs/2102.07944"/>
        <updated>2021-06-04T01:12:31.740Z</updated>
        <summary type="html"><![CDATA[Recent efforts on solving inverse problems in imaging via deep neural
networks use architectures inspired by a fixed number of iterations of an
optimization method. The number of iterations is typically quite small due to
difficulties in training networks corresponding to more iterations; the
resulting solvers cannot be run for more iterations at test time without
incurring significant errors. This paper describes an alternative approach
corresponding to an infinite number of iterations, yielding a consistent
improvement in reconstruction accuracy above state-of-the-art alternatives and
where the computational budget can be selected at test time to optimize
context-dependent trade-offs between accuracy and computation. The proposed
approach leverages ideas from Deep Equilibrium Models, where the fixed-point
iteration is constructed to incorporate a known forward model and insights from
classical optimization-based reconstruction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gilton_D/0/1/0/all/0/1"&gt;Davis Gilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ongie_G/0/1/0/all/0/1"&gt;Gregory Ongie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TryOnGAN: Body-Aware Try-On via Layered Interpolation. (arXiv:2101.02285v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02285</id>
        <link href="http://arxiv.org/abs/2101.02285"/>
        <updated>2021-06-04T01:12:31.718Z</updated>
        <summary type="html"><![CDATA[Given a pair of images-target person and garment on another person-we
automatically generate the target person in the given garment. Previous methods
mostly focused on texture transfer via paired data training, while overlooking
body shape deformations, skin color, and seamless blending of garment with the
person. This work focuses on those three components, while also not requiring
paired data training. We designed a pose conditioned StyleGAN2 architecture
with a clothing segmentation branch that is trained on images of people wearing
garments. Once trained, we propose a new layered latent space interpolation
method that allows us to preserve and synthesize skin color and target body
shape while transferring the garment from a different person. We demonstrate
results on high resolution 512x512 images, and extensively compare to state of
the art in try-on on both latent space generated and real images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1"&gt;Kathleen M Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varadharajan_S/0/1/0/all/0/1"&gt;Srivatsan Varadharajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1"&gt;Ira Kemelmacher-Shlizerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergent Graph Solvers. (arXiv:2106.01680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01680</id>
        <link href="http://arxiv.org/abs/2106.01680"/>
        <updated>2021-06-04T01:12:31.710Z</updated>
        <summary type="html"><![CDATA[We propose the convergent graph solver (CGS), a deep learning method that
learns iterative mappings to predict the properties of a graph system at its
stationary state (fixed point) with guaranteed convergence. CGS systematically
computes the fixed points of a target graph system and decodes them to estimate
the stationary properties of the system without the prior knowledge of existing
solvers or intermediate solutions. The forward propagation of CGS proceeds in
three steps: (1) constructing the input dependent linear contracting iterative
maps, (2) computing the fixed-points of the linear maps, and (3) decoding the
fixed-points to estimate the properties. The contractivity of the constructed
linear maps guarantees the existence and uniqueness of the fixed points
following the Banach fixed point theorem. To train CGS efficiently, we also
derive a tractable analytical expression for its gradient by leveraging the
implicit function theorem. We evaluate the performance of CGS by applying it to
various network-analytic and graph benchmark problems. The results indicate
that CGS has competitive capabilities for predicting the stationary properties
of graph systems, irrespective of whether the target systems are linear or
non-linear. CGS also shows high performance for graph classification problems
where the existence or the meaning of a fixed point is hard to be clearly
defined, which highlights the potential of CGS as a general graph neural
network architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jinhyun Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.06519</id>
        <link href="http://arxiv.org/abs/1903.06519"/>
        <updated>2021-06-04T01:12:31.704Z</updated>
        <summary type="html"><![CDATA[The most realistic information about the transparent sample such as a live
cell can be obtained only using bright-field light microscopy. At
high-intensity pulsing LED illumination, we captured a primary
12-bit-per-channel (bpc) response froman observed sample using a bright-field
wide-field microscope equipped with a high-resolution (4872x3248) image sensor.
In order to suppress data distortions originating from the light interactions
with elements in the optical path, poor sensor reproduction (geometrical
defects of the camera sensor and some peculiarities of sensor sensitivity),
this uncompressed 12-bpc data underwent a kind of correction after simultaneous
calibration of all the parts of the experimental arrangement. Moreover, the
final intensities of the corrected images are proportional to the photon fluxes
detected by a camera sensor. It can be visualized in 8-bpc intensity depth
after the Least Information Loss compression [Lect. Notes Bioinform. 9656, 527
(2016)].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1"&gt;Ganna Platonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1"&gt;Dalibor Stys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1"&gt;Pavel Soucek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1"&gt;Kirill Lonhus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1"&gt;Jan Valenta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1"&gt;Renata Rychtarikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification. (arXiv:2106.01649v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01649</id>
        <link href="http://arxiv.org/abs/2106.01649"/>
        <updated>2021-06-04T01:12:31.697Z</updated>
        <summary type="html"><![CDATA[Modern models for event causality identification (ECI) are mainly based on
supervised learning, which are prone to the data lacking problem.
Unfortunately, the existing NLP-related augmentation methods cannot directly
produce the available data required for this task. To solve the data lacking
problem, we introduce a new approach to augment training data for event
causality identification, by iteratively generating new examples and
classifying event causality in a dual learning framework. On the one hand, our
approach is knowledge-guided, which can leverage existing knowledge bases to
generate well-formed new sentences. On the other hand, our approach employs a
dual mechanism, which is a learnable augmentation framework and can
interactively adjust the generation process to generate task-related sentences.
Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show
that 1) our method can augment suitable task-related training data for ECI; 2)
our method outperforms previous methods on EventStoryLine and Causal-TimeBank
(+2.5 and +2.1 points on F1 value respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinyu Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pengfei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Weihua Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuguang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement. (arXiv:2106.01654v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01654</id>
        <link href="http://arxiv.org/abs/2106.01654"/>
        <updated>2021-06-04T01:12:31.690Z</updated>
        <summary type="html"><![CDATA[Current models for event causality identification (ECI) mainly adopt a
supervised framework, which heavily rely on labeled data for training.
Unfortunately, the scale of current annotated datasets is relatively limited,
which cannot provide sufficient support for models to capture useful indicators
from causal statements, especially for handing those new, unseen cases. To
alleviate this problem, we propose a novel approach, shortly named CauSeRL,
which leverages external causal statements for event causality identification.
First of all, we design a self-supervised framework to learn context-specific
causal patterns from external causal statements. Then, we adopt a contrastive
transfer strategy to incorporate the learned context-specific causal patterns
into the target ECI model. Experimental results show that our method
significantly outperforms previous methods on EventStoryLine and
Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinyu Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pengfei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Weihua Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuguang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints. (arXiv:2011.14858v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14858</id>
        <link href="http://arxiv.org/abs/2011.14858"/>
        <updated>2021-06-04T01:12:31.672Z</updated>
        <summary type="html"><![CDATA[The world is going through one of the most dangerous pandemics of all time
with the rapid spread of the novel coronavirus (COVID-19). According to the
World Health Organisation, the most effective way to thwart the transmission of
coronavirus is to wear medical face masks. Monitoring the use of face masks in
public places has been a challenge because manual monitoring could be unsafe.
This paper proposes an architecture for detecting medical face masks for
deployment on resource-constrained endpoints having extremely low memory
footprints. A small development board with an ARM Cortex-M7 microcontroller
clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for
the deployment of the model. Using the TensorFlow Lite framework, the model is
quantized to further reduce its size. The proposed model is 138 KB post
quantization and runs at the inference speed of 30 FPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_P/0/1/0/all/0/1"&gt;Puranjay Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirania_A/0/1/0/all/0/1"&gt;Abhay Chirania&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia. (arXiv:2106.01601v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01601</id>
        <link href="http://arxiv.org/abs/2106.01601"/>
        <updated>2021-06-04T01:12:31.666Z</updated>
        <summary type="html"><![CDATA[Human activities can be seen as sequences of events, which are crucial to
understanding societies. Disproportional event distribution for different
demographic groups can manifest and amplify social stereotypes, and potentially
jeopardize the ability of members in some groups to pursue certain goals. In
this paper, we present the first event-centric study of gender biases in a
Wikipedia corpus. To facilitate the study, we curate a corpus of career and
personal life descriptions with demographic information consisting of 7,854
fragments from 10,412 celebrities. Then we detect events with a
state-of-the-art event detection model, calibrate the results using
strategically generated templates, and extract events that have asymmetric
associations with genders. Our study discovers that the Wikipedia pages tend to
intermingle personal life events with professional events for females but not
for males, which calls for the awareness of the Wikipedia community to
formalize guidelines and train the editors to mind the implicit biases that
contributors carry. Our work also lays the foundation for future works on
quantifying and discovering event biases at the corpus level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corporate core values and social responsibility: What really matters to whom. (arXiv:2106.01644v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01644</id>
        <link href="http://arxiv.org/abs/2106.01644"/>
        <updated>2021-06-04T01:12:31.660Z</updated>
        <summary type="html"><![CDATA[This study uses an innovative measure, the Semantic Brand Score, to assess
the interest of stakeholders in different company core values. Among others, we
focus on corporate social responsibility (CSR) core value statements, and on
the attention they receive from five categories of stakeholders (customers,
company communication teams, employees, associations and media). Combining big
data methods and tools of Social Network Analysis and Text Mining, we analyzed
about 58,000 Italian tweets and found that different stakeholders have
different prevailing interests. CSR gets much less attention than expected.
Core values related to customers and employees are in the foreground.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barchiesi_M/0/1/0/all/0/1"&gt;M. A. Barchiesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Reference-based Super-Resolution via C2-Matching. (arXiv:2106.01863v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01863</id>
        <link href="http://arxiv.org/abs/2106.01863"/>
        <updated>2021-06-04T01:12:31.654Z</updated>
        <summary type="html"><![CDATA[Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising
paradigm to enhance a low-resolution (LR) input image by introducing an
additional high-resolution (HR) reference image. Existing Ref-SR methods mostly
rely on implicit correspondence matching to borrow HR textures from reference
images to compensate for the information loss in input images. However,
performing local transfer is difficult because of two gaps between input and
reference images: the transformation gap (e.g. scale and rotation) and the
resolution gap (e.g. HR and LR). To tackle these challenges, we propose
C2-Matching in this work, which produces explicit robust matching crossing
transformation and resolution. 1) For the transformation gap, we propose a
contrastive correspondence network, which learns transformation-robust
correspondences using augmented views of the input image. 2) For the resolution
gap, we adopt a teacher-student correlation distillation, which distills
knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR
matching. 3) Finally, we design a dynamic aggregation module to address the
potential misalignment issue. In addition, to faithfully evaluate the
performance of Ref-SR under a realistic setting, we contribute the
Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario.
Extensive experiments demonstrate that our proposed C2-Matching significantly
outperforms state of the arts by over 1dB on the standard CUFED5 benchmark.
Notably, it also shows great generalizability on WR-SR dataset as well as
robustness across large scale and rotation transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Person Detection in 2D Range Data using a Calibrated Camera. (arXiv:2012.08890v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08890</id>
        <link href="http://arxiv.org/abs/2012.08890"/>
        <updated>2021-06-04T01:12:31.647Z</updated>
        <summary type="html"><![CDATA[Deep learning is the essential building block of state-of-the-art person
detectors in 2D range data. However, only a few annotated datasets are
available for training and testing these deep networks, potentially limiting
their performance when deployed in new environments or with different LiDAR
models. We propose a method, which uses bounding boxes from an image-based
detector (e.g. Faster R-CNN) on a calibrated camera to automatically generate
training labels (called pseudo-labels) for 2D LiDAR-based person detectors.
Through experiments on the JackRabbot dataset with two detector models, DROW3
and DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned
with pseudo-labels, outperform detectors trained only on a different dataset.
Combined with robust training techniques, the self-supervised detectors reach a
performance close to the ones trained using manual annotations of the target
dataset. Our method is an effective way to improve person detectors during
deployment without any additional labeling effort, and we release our source
code to support relevant robotic applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1"&gt;Dan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinweg_M/0/1/0/all/0/1"&gt;Mats Steinweg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermans_A/0/1/0/all/0/1"&gt;Alexander Hermans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1"&gt;Bastian Leibe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing Backdoors in Robust Machine Learning Models. (arXiv:2003.00865v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00865</id>
        <link href="http://arxiv.org/abs/2003.00865"/>
        <updated>2021-06-04T01:12:31.627Z</updated>
        <summary type="html"><![CDATA[The introduction of robust optimisation has pushed the state-of-the-art in
defending against adversarial attacks. However, the behaviour of such
optimisation has not been studied in the light of a fundamentally different
class of attacks called backdoors. In this paper, we demonstrate that
adversarially robust models are susceptible to backdoor attacks. Subsequently,
we observe that backdoors are reflected in the feature representation of such
models. Then, this observation is leveraged to detect backdoor-infected models
via a detection technique called AEGIS. Specifically, AEGIS uses feature
clustering to effectively detect backdoor-infected robust Deep Neural Networks
(DNNs). In our evaluation of several visible and hidden backdoor triggers on
major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS
effectively detects robust DNNs infected with backdoors. AEGIS detects a
backdoor-infected model with 91.6% accuracy, without any false positives.
Furthermore, AEGIS detects the targeted class in the backdoor-infected model
with a reasonably low (11.1%) false positive rate. Our investigation reveals
that salient features of adversarially robust DNNs break the stealthy nature of
backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1"&gt;Ezekiel Soremekun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1"&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation. (arXiv:2106.01597v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01597</id>
        <link href="http://arxiv.org/abs/2106.01597"/>
        <updated>2021-06-04T01:12:31.621Z</updated>
        <summary type="html"><![CDATA[Despite the recent advancement in NLP research, cross-lingual transfer for
natural language generation is relatively understudied. In this work, we
transfer supervision from high resource language (HRL) to multiple low-resource
languages (LRLs) for natural language generation (NLG). We consider four NLG
tasks (text summarization, question generation, news headline generation, and
distractor generation) and three syntactically diverse languages, i.e.,
English, Hindi, and Japanese. We propose an unsupervised cross-lingual language
generation framework (called ZmBART) that does not use any parallel or
pseudo-parallel/back-translated data. In this framework, we further pre-train
mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task
using monolingual data of three languages. The objective function of the
auxiliary task is close to the target tasks which enriches the multi-lingual
latent representation of mBART and provides good initialization for target
tasks. Then, this model is fine-tuned with task-specific supervised English
data and directly evaluated with low-resource languages in the Zero-shot
setting. To overcome catastrophic forgetting and spurious correlation issues,
we applied freezing model component and data argumentation approaches
respectively. This simple modeling approach gave us promising results.We
experimented with few-shot training (with 1000 supervised data points) which
boosted the model performance further. We performed several ablations and
cross-lingual transferability analyses to demonstrate the robustness of ZmBART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1"&gt;Kaushal Kumar Maurya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1"&gt;Maunendra Sankar Desarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1"&gt;Yoshinobu Kano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1"&gt;Kumari Deepshikha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Complex Navigational Instructions Using Scene Graphs. (arXiv:2106.01607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01607</id>
        <link href="http://arxiv.org/abs/2106.01607"/>
        <updated>2021-06-04T01:12:31.615Z</updated>
        <summary type="html"><![CDATA[Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1"&gt;Michiel de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anuva Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction. (arXiv:2106.01609v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01609</id>
        <link href="http://arxiv.org/abs/2106.01609"/>
        <updated>2021-06-04T01:12:31.609Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of Chinese Grammatical Error Correction (CGEC) and
present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive
sequence prediction to address the deep issues hidden in CGEC. Considering that
most tokens are correct and can be conveyed directly from source to target, and
the error positions can be estimated and corrected based on the bidirectional
context information, thus we employ a BERT-initialized Transformer Encoder as
the backbone model to conduct information modeling and conveying. Considering
that only relying on the same position substitution cannot handle the
variable-length correction cases, various operations such substitution,
deletion, insertion, and local paraphrasing are required jointly. Therefore, a
Conditional Random Fields (CRF) layer is stacked on the up tail to conduct
non-autoregressive sequence prediction by modeling the token dependencies.
Since most tokens are correct and easily to be predicted/conveyed to the
target, then the models may suffer from a severe class imbalance issue. To
alleviate this problem, focal loss penalty strategies are integrated into the
loss functions. Moreover, besides the typical fix-length error correction
datasets, we also construct a variable-length corpus to conduct experiments.
Experimental results on standard datasets, especially on the variable-length
datasets, demonstrate the effectiveness of TtT in terms of sentence-level
Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and
Correction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Acoustic-based Approaches for Alzheimer's Disease Detection. (arXiv:2106.01555v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01555</id>
        <link href="http://arxiv.org/abs/2106.01555"/>
        <updated>2021-06-04T01:12:31.603Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the performance and generalizability of three
approaches for AD detection from speech on the recent ADReSSo challenge
dataset: 1) using conventional acoustic features 2) using novel pre-trained
acoustic embeddings 3) combining acoustic features and embeddings. We find that
while feature-based approaches have a higher precision, classification
approaches relying on the combination of embeddings and features prove to have
a higher, and more balanced performance across multiple metrics of performance.
Our best model, using such a combined approach, outperforms the acoustic
baseline in the challenge by 2.8\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balagopalan_A/0/1/0/all/0/1"&gt;Aparna Balagopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1"&gt;Jekaterina Novikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Few-Shot Image Classification. (arXiv:2010.04525v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04525</id>
        <link href="http://arxiv.org/abs/2010.04525"/>
        <updated>2021-06-04T01:12:31.597Z</updated>
        <summary type="html"><![CDATA[Few-shot image classification learns to recognize new categories from limited
labelled data. Metric learning based approaches have been widely investigated,
where a query sample is classified by finding the nearest prototype from the
support set based on their feature similarities. A neural network has different
uncertainties on its calculated similarities of different pairs. Understanding
and modeling the uncertainty on the similarity could promote the exploitation
of limited samples in few-shot optimization. In this work, we propose
Uncertainty-Aware Few-Shot framework for image classification by modeling
uncertainty of the similarities of query-support pairs and performing
uncertainty-aware optimization. Particularly, we exploit such uncertainty by
converting observed similarities to probabilistic representations and
incorporate them to the loss for more effective optimization. In order to
jointly consider the similarities between a query and the prototypes in a
support set, a graph-based model is utilized to estimate the uncertainty of the
pairs. Extensive experiments show our proposed method brings significant
improvements on top of a strong baseline and achieves the state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shih-Fu Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Complex Navigational Instructions Using Scene Graphs. (arXiv:2106.01607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01607</id>
        <link href="http://arxiv.org/abs/2106.01607"/>
        <updated>2021-06-04T01:12:31.579Z</updated>
        <summary type="html"><![CDATA[Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1"&gt;Michiel de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anuva Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically Detecting Cyberbullying Comments on Online Game Forums. (arXiv:2106.01598v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01598</id>
        <link href="http://arxiv.org/abs/2106.01598"/>
        <updated>2021-06-04T01:12:31.573Z</updated>
        <summary type="html"><![CDATA[Online game forums are popular to most of game players. They use it to
communicate and discuss the strategy of the game, or even to make friends.
However, game forums also contain abusive and harassment speech, disturbing and
threatening players. Therefore, it is necessary to automatically detect and
remove cyberbullying comments to keep the game forum clean and friendly. We use
the Cyberbullying dataset collected from World of Warcraft (WoW) and League of
Legends (LoL) forums and train classification models to automatically detect
whether a comment of a player is abusive or not. The result obtains 82.69% of
macro F1-score for LoL forum and 83.86% of macro F1-score for WoW forum by the
Toxic-BERT model on the Cyberbullying dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1"&gt;Hanh Hong-Phuc Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hieu Trung Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-06-04T01:12:31.567Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://github.com/fepegar/torchio. The package can be
installed from the Python Package Index running 'pip install torchio'. It
includes a command-line interface which allows users to apply transforms to
image files without using Python. Additionally, we provide a graphical
interface within a TorchIO extension in 3D Slicer to visualize the effects of
transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Chatbot's Self-Disclosure's Impact on User Trust, Affinity, and Recommendation Effectiveness. (arXiv:2106.01666v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01666</id>
        <link href="http://arxiv.org/abs/2106.01666"/>
        <updated>2021-06-04T01:12:31.560Z</updated>
        <summary type="html"><![CDATA[In recent years, chatbots have been empowered to engage in social
conversations with humans and have the potential to elicit people to disclose
their personal experiences, opinions, and emotions. However, how and to what
extent people respond to chabots' self-disclosure remain less known. In this
work, we designed a social chatbot with three self-disclosure levels that
conducted small talks and provided relevant recommendations to people. 372
MTurk participants were randomized to one of the four groups with different
self-disclosure levels to converse with the chatbot on two topics, movies, and
COVID-19. We found that people's self-disclosure level was strongly reciprocal
to a chatbot's self-disclosure level. Chatbots' self-disclosure also positively
impacted engagement and users' perception of the bot and led to a more
effective recommendation such that participants enjoyed and agreed more with
the recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kai-Hui Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weiyan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Yoojung Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models. (arXiv:2106.01623v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01623</id>
        <link href="http://arxiv.org/abs/2106.01623"/>
        <updated>2021-06-04T01:12:31.554Z</updated>
        <summary type="html"><![CDATA[This paper studies how to automatically generate a natural language text that
describes the facts in knowledge graph (KG). Considering the few-shot setting,
we leverage the excellent capacities of pretrained language models (PLMs) in
language understanding and generation. We make three major technical
contributions, namely representation alignment for bridging the semantic gap
between KG encodings and PLMs, relation-biased KG linearization for deriving
better input representations, and multi-task learning for learning the
correspondence between KG and text. Extensive experiments on three benchmark
datasets have demonstrated the effectiveness of our model on KG-to-text
generation task. In particular, our model outperforms all comparison methods on
both fully-supervised and few-shot settings. Our code and datasets are
available at https://github.com/RUCAIBox/Few-Shot-KG2Text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tianyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhicheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Nicholas Jing Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Reasoning for Document-level Relation Extraction. (arXiv:2106.01562v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01562</id>
        <link href="http://arxiv.org/abs/2106.01562"/>
        <updated>2021-06-04T01:12:31.536Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction (DocRE) models generally use graph
networks to implicitly model the reasoning skill (i.e., pattern recognition,
logical reasoning, coreference reasoning, etc.) related to the relation between
one entity pair in a document. In this paper, we propose a novel discriminative
reasoning framework to explicitly model the paths of these reasoning skills
between each entity pair in this document. Thus, a discriminative reasoning
network is designed to estimate the relation probability distribution of
different reasoning paths based on the constructed graph and vectorized
document contexts for each entity pair, thereby recognizing their relation.
Experimental results show that our method outperforms the previous
state-of-the-art performance on the large-scale DocRE dataset. The code is
publicly available at https://github.com/xwjim/DRN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kehai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiejun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Limitations of Limited Context for Constituency Parsing. (arXiv:2106.01580v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01580</id>
        <link href="http://arxiv.org/abs/2106.01580"/>
        <updated>2021-06-04T01:12:31.530Z</updated>
        <summary type="html"><![CDATA[Incorporating syntax into neural approaches in NLP has a multitude of
practical and scientific benefits. For instance, a language model that is
syntax-aware is likely to be able to produce better samples; even a
discriminative model like BERT with a syntax module could be used for core NLP
tasks like unsupervised syntactic parsing. Rapid progress in recent years was
arguably spurred on by the empirical success of the Parsing-Reading-Predict
architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM
of (Shen et al., 2019). Most notably, this is the first time neural approaches
were able to successfully perform unsupervised syntactic parsing (evaluated by
various metrics like F-1 score).

However, even heuristic (much less fully mathematical) understanding of why
and when these architectures work is lagging severely behind. In this work, we
answer representational questions raised by the architectures in (Shen et al.,
2018a, 2019), as well as some transition-based syntax-aware language models
(Dyer et al., 2016): what kind of syntactic structure can current neural
approaches to syntax represent? Concretely, we ground this question in the
sandbox of probabilistic context-free-grammars (PCFGs), and identify a key
aspect of the representational power of these approaches: the amount and
directionality of context that the predictor has access to when forced to make
parsing decision. We show that with limited context (either bounded, or
unidirectional), there are PCFGs, for which these approaches cannot represent
the max-likelihood parse; conversely, if the context is unlimited, they can
represent the max-likelihood parse of any PCFG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning. (arXiv:2106.01559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01559</id>
        <link href="http://arxiv.org/abs/2106.01559"/>
        <updated>2021-06-04T01:12:31.523Z</updated>
        <summary type="html"><![CDATA[Relational fact extraction aims to extract semantic triplets from
unstructured text. In this work, we show that all of the relational fact
extraction models can be organized according to a graph-oriented analytical
perspective. An efficient model, aDjacency lIst oRiented rElational faCT
(DIRECT), is proposed based on this analytical framework. To alleviate
challenges of error propagation and sub-task loss equilibrium, DIRECT employs a
novel adaptive multi-task learning strategy with dynamic sub-task loss
balancing. Extensive experiments are conducted on two benchmark datasets, and
results prove that the proposed model outperforms a series of state-of-the-art
(SoTA) models for relational triplet extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fubang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate, Prune, Select: A Pipeline for Counterspeech Generation against Online Hate Speech. (arXiv:2106.01625v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01625</id>
        <link href="http://arxiv.org/abs/2106.01625"/>
        <updated>2021-06-04T01:12:31.517Z</updated>
        <summary type="html"><![CDATA[Countermeasures to effectively fight the ever increasing hate speech online
without blocking freedom of speech is of great social interest. Natural
Language Generation (NLG), is uniquely capable of developing scalable
solutions. However, off-the-shelf NLG methods are primarily
sequence-to-sequence neural models and they are limited in that they generate
commonplace, repetitive and safe responses regardless of the hate speech (e.g.,
"Please refrain from using such language.") or irrelevant responses, making
them ineffective for de-escalating hateful conversations. In this paper, we
design a three-module pipeline approach to effectively improve the diversity
and relevance. Our proposed pipeline first generates various counterspeech
candidates by a generative model to promote diversity, then filters the
ungrammatical ones using a BERT model, and finally selects the most relevant
counterspeech response using a novel retrieval-based method. Extensive
Experiments on three representative datasets demonstrate the efficacy of our
approach in generating diverse and relevant counterspeech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wanzheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Suma Bhat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Investigation of KB-Text Embedding Alignment at Scale. (arXiv:2106.01586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01586</id>
        <link href="http://arxiv.org/abs/2106.01586"/>
        <updated>2021-06-04T01:12:31.500Z</updated>
        <summary type="html"><![CDATA[Knowledge bases (KBs) and text often contain complementary knowledge: KBs
store structured knowledge that can support long range reasoning, while text
stores more comprehensive and timely knowledge in an unstructured way.
Separately embedding the individual knowledge sources into vector spaces has
demonstrated tremendous successes in encoding the respective knowledge, but how
to jointly embed and reason with both knowledge sources to fully leverage the
complementary information is still largely an open problem. We conduct a
large-scale, systematic investigation of aligning KB and text embeddings for
joint reasoning. We set up a novel evaluation framework with two evaluation
tasks, few-shot link prediction and analogical reasoning, and evaluate an array
of KB-text embedding alignment methods. We also demonstrate how such alignment
can infuse textual information into KB embeddings for more accurate link
prediction on emerging entities and events, using COVID-19 as a case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1"&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahrami_M/0/1/0/all/0/1"&gt;Mehdi Bahrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yu Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations. (arXiv:2106.01548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01548</id>
        <link href="http://arxiv.org/abs/2106.01548"/>
        <updated>2021-06-04T01:12:31.333Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pretraining and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pretraining or strong data
augmentations. They also possess more perceptive attention maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSMD: Semi-Supervised Medical Image Detection with Adaptive Consistency and Heterogeneous Perturbation. (arXiv:2106.01544v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01544</id>
        <link href="http://arxiv.org/abs/2106.01544"/>
        <updated>2021-06-04T01:12:31.327Z</updated>
        <summary type="html"><![CDATA[Semi-Supervised classification and segmentation methods have been widely
investigated in medical image analysis. Both approaches can improve the
performance of fully-supervised methods with additional unlabeled data.
However, as a fundamental task, semi-supervised object detection has not gained
enough attention in the field of medical image analysis. In this paper, we
propose a novel Semi-Supervised Medical image Detector (SSMD). The motivation
behind SSMD is to provide free yet effective supervision for unlabeled data, by
regularizing the predictions at each position to be consistent. To achieve the
above idea, we develop a novel adaptive consistency cost function to regularize
different components in the predictions. Moreover, we introduce heterogeneous
perturbation strategies that work in both feature space and image space, so
that the proposed detector is promising to produce powerful image
representations and robust predictions. Extensive experimental results show
that the proposed SSMD achieves the state-of-the-art performance at a wide
range of settings. We also demonstrate the strength of each proposed module
with comprehensive ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haofeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weimin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalizing Pre-trained Models. (arXiv:2106.01499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01499</id>
        <link href="http://arxiv.org/abs/2106.01499"/>
        <updated>2021-06-04T01:12:31.320Z</updated>
        <summary type="html"><![CDATA[Self-supervised or weakly supervised models trained on large-scale datasets
have shown sample-efficient transfer to diverse datasets in few-shot settings.
We consider how upstream pretrained models can be leveraged for downstream
few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP
PERsonalized) uses image representations from CLIP, a large-scale image
representation learning model trained using weak natural language supervision.
We developed a technique, called Multi-label Weight Imprinting (MWI), for
multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image
representations from CLIP. We evaluated CLIPPER on 10 single-label and 5
multi-label datasets. Our model shows robust and competitive performance, and
we set new benchmarks for few-shot, multi-label, and continual learning. Our
lightweight technique is also compute-efficient and enables privacy-preserving
applications as the data is not sent to the upstream model for fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mina Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivatsa_P/0/1/0/all/0/1"&gt;P Srivatsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rane_A/0/1/0/all/0/1"&gt;Advait Rane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenniappa_S/0/1/0/all/0/1"&gt;Shriram Chenniappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazariwala_A/0/1/0/all/0/1"&gt;Asadali Hazariwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maes_P/0/1/0/all/0/1"&gt;Pattie Maes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable Adversarial Examples for Anchor Free Object Detection. (arXiv:2106.01618v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01618</id>
        <link href="http://arxiv.org/abs/2106.01618"/>
        <updated>2021-06-04T01:12:31.314Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been demonstrated to be vulnerable to adversarial
attacks: subtle perturbation can completely change prediction result. The
vulnerability has led to a surge of research in this direction, including
adversarial attacks on object detection networks. However, previous studies are
dedicated to attacking anchor-based object detectors. In this paper, we present
the first adversarial attack on anchor-free object detectors. It conducts
category-wise, instead of previously instance-wise, attacks on object
detectors, and leverages high-level semantic information to efficiently
generate transferable adversarial examples, which can also be transferred to
attack other object detectors, even anchor-based detectors such as Faster
R-CNN. Experimental results on two benchmark datasets demonstrate that our
proposed method achieves state-of-the-art performance and transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Quanyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_B/0/1/0/all/0/1"&gt;Bin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youbing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain First Person Audio-Visual Action Recognition through Relative Norm Alignment. (arXiv:2106.01689v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01689</id>
        <link href="http://arxiv.org/abs/2106.01689"/>
        <updated>2021-06-04T01:12:31.308Z</updated>
        <summary type="html"><![CDATA[First person action recognition is an increasingly researched topic because
of the growing popularity of wearable cameras. This is bringing to light
cross-domain issues that are yet to be addressed in this context. Indeed, the
information extracted from learned representations suffers from an intrinsic
environmental bias. This strongly affects the ability to generalize to unseen
scenarios, limiting the application of current methods in real settings where
trimmed labeled data are not available during training. In this work, we
propose to leverage over the intrinsic complementary nature of audio-visual
signals to learn a representation that works well on data seen during training,
while being able to generalize across different domains. To this end, we
introduce an audio-visual loss that aligns the contributions from the two
modalities by acting on the magnitude of their feature norm representations.
This new loss, plugged into a minimal multi-modal action recognition
architecture, leads to strong results in cross-domain first person action
recognition, as demonstrated by extensive experiments on the popular
EPIC-Kitchens dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1"&gt;Mirco Planamente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1"&gt;Chiara Plizzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alberti_E/0/1/0/all/0/1"&gt;Emanuele Alberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GMAIR: Unsupervised Object Detection Based on Spatial Attention and Gaussian Mixture. (arXiv:2106.01722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01722</id>
        <link href="http://arxiv.org/abs/2106.01722"/>
        <updated>2021-06-04T01:12:31.302Z</updated>
        <summary type="html"><![CDATA[Recent studies on unsupervised object detection based on spatial attention
have achieved promising results. Models, such as AIR and SPAIR, output "what"
and "where" latent variables that represent the attributes and locations of
objects in a scene, respectively. Most of the previous studies concentrate on
the "where" localization performance; however, we claim that acquiring "what"
object attributes is also essential for representation learning. This paper
presents a framework, GMAIR, for unsupervised object detection. It incorporates
spatial attention and a Gaussian mixture in a unified deep generative model.
GMAIR can locate objects in a scene and simultaneously cluster them without
supervision. Furthermore, we analyze the "what" latent variables and clustering
process. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and
show that GMAIR achieves competitive results on localization and clustering
compared to state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Weijin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Linfeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_L/0/1/0/all/0/1"&gt;Lizeth Patricia Aguirre Sanchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack. (arXiv:2106.01538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01538</id>
        <link href="http://arxiv.org/abs/2106.01538"/>
        <updated>2021-06-04T01:12:31.284Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural networks are sensitive to small input
perturbations. Since the discovery of this intriguing vulnerability, many
defence methods have been proposed that attempt to improve robustness to
adversarial noise. Fast and accurate attacks are required to compare various
defence methods. However, evaluating adversarial robustness has proven to be
extremely challenging. Existing norm minimisation adversarial attacks require
thousands of iterations (e.g. Carlini & Wagner attack), are limited to the
specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results
(e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast,
general and accurate, ignores the norm minimisation penalty and solves a
simpler perturbation-constrained problem. In this work, we introduce a fast,
general and accurate adversarial attack that optimises the original non-convex
constrained minimisation problem. We interpret optimising the Lagrangian of the
adversarial attack optimisation problem as a two-player game: the first player
minimises the Lagrangian wrt the adversarial noise; the second player maximises
the Lagrangian wrt the regularisation penalty. Our attack algorithm
simultaneously optimises primal and dual variables to find the minimal
adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,
such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual
proximal gradient descent attack. We show in the experiments that our attack
outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and
$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against
unregularised and adversarially trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matyasko_A/0/1/0/all/0/1"&gt;Alexander Matyasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconfounded Video Moment Retrieval with Causal Intervention. (arXiv:2106.01534v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01534</id>
        <link href="http://arxiv.org/abs/2106.01534"/>
        <updated>2021-06-04T01:12:31.278Z</updated>
        <summary type="html"><![CDATA[We tackle the task of video moment retrieval (VMR), which aims to localize a
specific moment in a video according to a textual query. Existing methods
primarily model the matching relationship between query and moment by complex
cross-modal interactions. Despite their effectiveness, current models mostly
exploit dataset biases while ignoring the video content, thus leading to poor
generalizability. We argue that the issue is caused by the hidden confounder in
VMR, {i.e., temporal location of moments}, that spuriously correlates the model
input and prediction. How to design robust matching models against the temporal
location biases is crucial but, as far as we know, has not been studied yet for
VMR.

To fill the research gap, we propose a causality-inspired VMR framework that
builds structural causal model to capture the true effect of query and video
content on the prediction. Specifically, we develop a Deconfounded Cross-modal
Matching (DCM) method to remove the confounding effects of moment location. It
first disentangles moment representation to infer the core feature of visual
content, and then applies causal intervention on the disentangled multimodal
input based on backdoor adjustment, which forces the model to fairly
incorporate each possible location of the target into consideration. Extensive
experiments clearly show that our approach can achieve significant improvement
over the state-of-the-art methods in terms of both accuracy and generalization
(Codes:
\color{blue}{\url{https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barbershop: GAN-based Image Compositing using Segmentation Masks. (arXiv:2106.01505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01505</id>
        <link href="http://arxiv.org/abs/2106.01505"/>
        <updated>2021-06-04T01:12:31.272Z</updated>
        <summary type="html"><![CDATA[Seamlessly blending features from multiple images is extremely challenging
because of complex relationships in lighting, geometry, and partial occlusion
which cause coupling between different parts of the image. Even though recent
work on GANs enables synthesis of realistic hair or faces, it remains difficult
to combine them into a single, coherent, and plausible image rather than a
disjointed set of image patches. We present a novel solution to image blending,
particularly for the problem of hairstyle transfer, based on GAN-inversion. We
propose a novel latent space for image blending which is better at preserving
detail and encoding spatial information, and propose a new GAN-embedding
algorithm which is able to slightly modify images to conform to a common
segmentation mask. Our novel representation enables the transfer of the visual
properties from multiple reference images including specific details such as
moles and wrinkles, and because we do image blending in a latent-space we are
able to synthesize images that are coherent. Our approach avoids blending
artifacts present in other approaches and finds a globally consistent image.
Our results demonstrate a significant improvement over the current state of the
art in a user study, with users preferring our blending solution over 95
percent of the time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Peihao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1"&gt;Rameen Abdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Femiani_J/0/1/0/all/0/1"&gt;John Femiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with Generic Representations. (arXiv:2106.01423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01423</id>
        <link href="http://arxiv.org/abs/2106.01423"/>
        <updated>2021-06-04T01:12:31.265Z</updated>
        <summary type="html"><![CDATA[The field of few-shot learning has made remarkable strides in developing
powerful models that can operate in the small data regime. Nearly all of these
methods assume every unlabeled instance encountered will belong to a handful of
known classes for which one has examples. This can be problematic for
real-world use cases where one routinely finds 'none-of-the-above' examples. In
this paper we describe this challenge of identifying what we term
'out-of-support' (OOS) examples. We describe how this problem is subtly
different from out-of-distribution detection and describe a new method of
identifying OOS examples within the Prototypical Networks framework using a
fixed point which we call the generic representation. We show that our method
outperforms other existing approaches in the literature as well as other
approaches that we propose in this paper. Finally, we investigate how the use
of such a generic point affects the geometry of a model's feature space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courts_N/0/1/0/all/0/1"&gt;Nico Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1"&gt;Lauren A. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buckheit_J/0/1/0/all/0/1"&gt;John Buckheit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+New_Z/0/1/0/all/0/1"&gt;Zachary New&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skomski_E/0/1/0/all/0/1"&gt;Elliott Skomski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung H. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1"&gt;Sandeep Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hibler_J/0/1/0/all/0/1"&gt;Jessica Hibler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corley_C/0/1/0/all/0/1"&gt;Courtney D. Corley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodas_N/0/1/0/all/0/1"&gt;Nathan O. Hodas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Memorization in Adversarial Training. (arXiv:2106.01606v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01606</id>
        <link href="http://arxiv.org/abs/2106.01606"/>
        <updated>2021-06-04T01:12:31.258Z</updated>
        <summary type="html"><![CDATA[It is well known that deep learning models have a propensity for fitting the
entire training set even with random labels, which requires memorization of
every training sample. In this paper, we investigate the memorization effect in
adversarial training (AT) for promoting a deeper understanding of capacity,
convergence, generalization, and especially robust overfitting of adversarially
trained classifiers. We first demonstrate that deep networks have sufficient
capacity to memorize adversarial examples of training data with completely
random labels, but not all AT algorithms can converge under the extreme
circumstance. Our study of AT with random labels motivates further analyses on
the convergence and generalization of AT. We find that some AT methods suffer
from a gradient instability issue, and the recently suggested complexity
measures cannot explain robust generalization by considering models trained on
random labels. Furthermore, we identify a significant drawback of memorization
in AT that it could result in robust overfitting. We then propose a new
mitigation algorithm motivated by detailed memorization analyses. Extensive
experiments on various datasets validate the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhijie Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NTIRE 2021 Challenge on High Dynamic Range Imaging: Dataset, Methods and Results. (arXiv:2106.01439v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01439</id>
        <link href="http://arxiv.org/abs/2106.01439"/>
        <updated>2021-06-04T01:12:31.241Z</updated>
        <summary type="html"><![CDATA[This paper reviews the first challenge on high-dynamic range (HDR) imaging
that was part of the New Trends in Image Restoration and Enhancement (NTIRE)
workshop, held in conjunction with CVPR 2021. This manuscript focuses on the
newly introduced dataset, the proposed methods and their results. The challenge
aims at estimating a HDR image from one or multiple respective low-dynamic
range (LDR) observations, which might suffer from under- or over-exposed
regions and different sources of noise. The challenge is composed by two
tracks: In Track 1 only a single LDR image is provided as input, whereas in
Track 2 three differently-exposed LDR images with inter-frame motion are
available. In both tracks, the ultimate goal is to achieve the best objective
HDR reconstruction in terms of PSNR with respect to a ground-truth image,
evaluated both directly and with a canonical tonemapping operation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1"&gt;Eduardo P&amp;#xe9;rez-Pellitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catley_Chandar_S/0/1/0/all/0/1"&gt;Sibi Catley-Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ale&amp;#x161; Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-04T01:12:31.235Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Complex Navigational Instructions Using Scene Graphs. (arXiv:2106.01607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01607</id>
        <link href="http://arxiv.org/abs/2106.01607"/>
        <updated>2021-06-04T01:12:31.229Z</updated>
        <summary type="html"><![CDATA[Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1"&gt;Michiel de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anuva Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Cognitive Regularizer for Language Modeling. (arXiv:2105.07144v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07144</id>
        <link href="http://arxiv.org/abs/2105.07144"/>
        <updated>2021-06-04T01:12:31.222Z</updated>
        <summary type="html"><![CDATA[The uniform information density (UID) hypothesis, which posits that speakers
behaving optimally tend to distribute information uniformly across a linguistic
signal, has gained traction in psycholinguistics as an explanation for certain
syntactic, morphological, and prosodic choices. In this work, we explore
whether the UID hypothesis can be operationalized as an inductive bias for
statistical language modeling. Specifically, we augment the canonical MLE
objective for training language models with a regularizer that encodes UID. In
experiments on ten languages spanning five language families, we find that
using UID regularization consistently improves perplexity in language models,
having a larger effect when training data is limited. Moreover, via an analysis
of generated sequences, we find that UID-regularized language models have other
desirable properties, e.g., they generate text that is more lexically diverse.
Our results not only suggest that UID is a reasonable inductive bias for
language modeling, but also provide an alternative validation of the UID
hypothesis using modern-day NLP tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperceptible Adversarial Examples for Fake Image Detection. (arXiv:2106.01615v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01615</id>
        <link href="http://arxiv.org/abs/2106.01615"/>
        <updated>2021-06-04T01:12:31.216Z</updated>
        <summary type="html"><![CDATA[Fooling people with highly realistic fake images generated with Deepfake or
GANs brings a great social disturbance to our society. Many methods have been
proposed to detect fake images, but they are vulnerable to adversarial
perturbations -- intentionally designed noises that can lead to the wrong
prediction. Existing methods of attacking fake image detectors usually generate
adversarial perturbations to perturb almost the entire image. This is redundant
and increases the perceptibility of perturbations. In this paper, we propose a
novel method to disrupt the fake image detection by determining key pixels to a
fake image detector and attacking only the key pixels, which results in the
$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of
existing works. Experiments on two public datasets with three fake image
detectors indicate that our proposed method achieves state-of-the-art
performance in both white-box and black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Quanyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuezun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_B/0/1/0/all/0/1"&gt;Bin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youbing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not All Knowledge Is Created Equal. (arXiv:2106.01489v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01489</id>
        <link href="http://arxiv.org/abs/2106.01489"/>
        <updated>2021-06-04T01:12:31.198Z</updated>
        <summary type="html"><![CDATA[Mutual knowledge distillation (MKD) improves a model by distilling knowledge
from another model. However, not all knowledge is certain and correct,
especially under adverse conditions. For example, label noise usually leads to
less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge
misleads the learning rather than helps. This problem can be handled by two
aspects: (i) improving the reliability of a model where the knowledge is from
(i.e., knowledge source's reliability); (ii) selecting reliable knowledge for
distillation. In the literature, making a model more reliable is widely studied
while selective MKD receives little attention. Therefore, we focus on studying
selective MKD and highlight its importance in this work.

Concretely, a generic MKD framework, Confident knowledge selection followed
by Mutual Distillation (CMD), is designed. The key component of CMD is a
generic knowledge selection formulation, making the selection threshold either
static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special
cases: zero knowledge and all knowledge, leading to a unified MKD framework. We
empirically find CMD-P performs better than CMD-S. The main reason is that a
model's knowledge upgrades and becomes confident as the training progresses.

Extensive experiments are present to demonstrate the effectiveness of CMD and
thoroughly justify the design of CMD. For example, CMD-P obtains new
state-of-the-art results in robustness against label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinshao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Di Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1"&gt;Neil M. Robertson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container: Context Aggregation Network. (arXiv:2106.01401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01401</id>
        <link href="http://arxiv.org/abs/2106.01401"/>
        <updated>2021-06-04T01:12:31.192Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are ubiquitous in computer vision, with
a myriad of effective and efficient variations. Recently, Transformers --
originally introduced in natural language processing -- have been increasingly
adopted in computer vision. While early adopters continue to employ CNN
backbones, the latest networks are end-to-end CNN-free Transformer solutions. A
recent surprising finding shows that a simple MLP based solution without any
traditional convolutional or Transformer components can produce effective
visual representations. While CNNs, Transformers and MLP-Mixers may be
considered as completely disparate architectures, we provide a unified view
showing that they are in fact special cases of a more general method to
aggregate spatial context in a neural network stack. We present the \model
(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head
context aggregation that can exploit long-range interactions \emph{a la}
Transformers while still exploiting the inductive bias of the local convolution
operation leading to faster convergence speeds, often seen in CNNs. In contrast
to Transformer-based methods that do not scale well to downstream tasks that
rely on larger input image resolutions, our efficient network, named
\modellight, can be employed in object detection and instance segmentation
networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive
detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large
improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50
backbone with a comparable compute and parameter size. Our method also achieves
promising results on self-supervised learning compared to DeiT on the DINO
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiasen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1"&gt;Aniruddha Kembhavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fingerprinting Fine-tuned Language Models in the Wild. (arXiv:2106.01703v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01703</id>
        <link href="http://arxiv.org/abs/2106.01703"/>
        <updated>2021-06-04T01:12:31.186Z</updated>
        <summary type="html"><![CDATA[There are concerns that the ability of language models (LMs) to generate high
quality synthetic text can be misused to launch spam, disinformation, or
propaganda. Therefore, the research community is actively working on developing
approaches to detect whether a given text is organic or synthetic. While this
is a useful first step, it is important to be able to further fingerprint the
author LM to attribute its origin. Prior work on fingerprinting LMs is limited
to attributing synthetic text generated by a handful (usually < 10) of
pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad
of ways (e.g., on a domain-specific text corpus) before being used to generate
synthetic text. It is challenging to fingerprinting fine-tuned LMs because the
universe of fine-tuned LMs is much larger in realistic scenarios. To address
this challenge, we study the problem of large-scale fingerprinting of
fine-tuned LMs in the wild. Using a real-world dataset of synthetic text
generated by 108 different fine-tuned LMs, we conduct comprehensive experiments
to demonstrate the limitations of existing fingerprinting approaches. Our
results show that fine-tuning itself is the most effective in attributing the
synthetic text generated by fine-tuned LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diwan_N/0/1/0/all/0/1"&gt;Nirav Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_T/0/1/0/all/0/1"&gt;Tanmoy Chakravorty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiq_Z/0/1/0/all/0/1"&gt;Zubair Shafiq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCompress: Efficient Point Cloud Geometry Compression. (arXiv:2106.01504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01504</id>
        <link href="http://arxiv.org/abs/2106.01504"/>
        <updated>2021-06-04T01:12:31.180Z</updated>
        <summary type="html"><![CDATA[Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killea_R/0/1/0/all/0/1"&gt;Ryan Killea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_S/0/1/0/all/0/1"&gt;Saeed Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLachlan_P/0/1/0/all/0/1"&gt;Paul McLachlan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CT-Net: Channel Tensorization Network for Video Classification. (arXiv:2106.01603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01603</id>
        <link href="http://arxiv.org/abs/2106.01603"/>
        <updated>2021-06-04T01:12:31.173Z</updated>
        <summary type="html"><![CDATA[3D convolution is powerful for video classification but often computationally
expensive, recent studies mainly focus on decomposing it on spatial-temporal
and/or channel dimensions. Unfortunately, most approaches fail to achieve a
preferable balance between convolutional efficiency and feature-interaction
sufficiency. For this reason, we propose a concise and novel Channel
Tensorization Network (CT-Net), by treating the channel dimension of input
feature as a multiplication of K sub-dimensions. On one hand, it naturally
factorizes convolution in a multiple dimension way, leading to a light
computation burden. On the other hand, it can effectively enhance feature
interaction from different channels, and progressively enlarge the 3D receptive
field of such interaction to boost classification accuracy. Furthermore, we
equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to
exploit spatial, temporal and channel attention in a high-dimensional manner,
to improve the cooperative power of all the feature dimensions in our
CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive
experiments are conducted on several challenging video benchmarks, e.g.,
Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of
recent SOTA approaches, in terms of accuracy and/or efficiency. The codes and
models will be available on https://github.com/Andy1621/CT-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kunchang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yali Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Doesn't Lie: Towards Universal Detection of Deep Inpainting. (arXiv:2106.01532v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01532</id>
        <link href="http://arxiv.org/abs/2106.01532"/>
        <updated>2021-06-04T01:12:31.166Z</updated>
        <summary type="html"><![CDATA[Deep image inpainting aims to restore damaged or missing regions in an image
with realistic contents. While having a wide range of applications such as
object removal and image recovery, deep inpainting techniques also have the
risk of being manipulated for image forgery. A promising countermeasure against
such forgeries is deep inpainting detection, which aims to locate the inpainted
regions in an image. In this paper, we make the first attempt towards universal
detection of deep inpainting, where the detection network can generalize well
when detecting different deep inpainting methods. To this end, we first propose
a novel data generation approach to generate a universal training dataset,
which imitates the noise discrepancies exist in real versus inpainted image
contents to train universal detectors. We then design a Noise-Image
Cross-fusion Network (NIX-Net) to effectively exploit the discriminative
information contained in both the images and their noise patterns. We
empirically show, on multiple benchmark datasets, that our approach outperforms
existing detection methods by a large margin and generalize well to unseen deep
inpainting techniques. Our universal training dataset can also significantly
boost the generalizability of existing detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1"&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xingjun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1"&gt;Haiqin Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Zhiyuan Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Feng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Select: A Fully Attentive Approach for Novel Object Captioning. (arXiv:2106.01424v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01424</id>
        <link href="http://arxiv.org/abs/2106.01424"/>
        <updated>2021-06-04T01:12:31.150Z</updated>
        <summary type="html"><![CDATA[Image captioning models have lately shown impressive results when applied to
standard datasets. Switching to real-life scenarios, however, constitutes a
challenge due to the larger variety of visual concepts which are not covered in
existing training sets. For this reason, novel object captioning (NOC) has
recently emerged as a paradigm to test captioning models on objects which are
unseen during the training phase. In this paper, we present a novel approach
for NOC that learns to select the most relevant objects of an image, regardless
of their adherence to the training set, and to constrain the generative process
of a language model accordingly. Our architecture is fully-attentive and
end-to-end trainable, also when incorporating constraints. We perform
experiments on the held-out COCO dataset, where we demonstrate improvements
over the state of the art, both in terms of adaptability to novel objects and
caption quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cagrandi_M/0/1/0/all/0/1"&gt;Marco Cagrandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection. (arXiv:2106.01483v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01483</id>
        <link href="http://arxiv.org/abs/2106.01483"/>
        <updated>2021-06-04T01:12:31.140Z</updated>
        <summary type="html"><![CDATA[The area of domain adaptation has been instrumental in addressing the domain
shift problem encountered by many applications. This problem arises due to the
difference between the distributions of source data used for training in
comparison with target data used during realistic testing scenarios. In this
paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)
framework that employs multiple domain adaptation paths and corresponding
domain classifiers at different scales of the recently introduced YOLOv4 object
detector to generate domain-invariant features. We train and test our proposed
method using popular datasets. Our experiments show significant improvements in
object detection performance when training YOLOv4 using the proposed MS-DAYOLO
and when tested on target data representing challenging weather conditions for
autonomous driving applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hnewa_M/0/1/0/all/0/1"&gt;Mazin Hnewa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1"&gt;Hayder Radha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes. (arXiv:2106.01487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01487</id>
        <link href="http://arxiv.org/abs/2106.01487"/>
        <updated>2021-06-04T01:12:31.133Z</updated>
        <summary type="html"><![CDATA[Learning binary representations of instances and classes is a classical
problem with several high potential applications. In modern settings, the
compression of high-dimensional neural representations to low-dimensional
binary codes is a challenging task and often require large bit-codes to be
accurate. In this work, we propose a novel method for Learning Low-dimensional
binary Codes (LLC) for instances as well as classes. Our method does not
require any side-information, like annotated attributes or label meta-data, and
learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The
learnt codes are super-efficient while still ensuring nearly optimal
classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the
learnt codes capture intrinsically important features in the data, by
discovering an intuitive taxonomy over classes. We further quantitatively
measure the quality of our codes by applying it to the efficient image
retrieval as well as out-of-distribution (OOD) detection problems. For
ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit
HashNet using only 10 bits and also are as accurate as 10 dimensional real
representations. Finally, our learnt binary codes can perform OOD detection,
out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune
its threshold, while we require none. Code and pre-trained models are available
at https://github.com/RAIVNLab/LLC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1"&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1"&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1"&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somani_R/0/1/0/all/0/1"&gt;Raghav Somani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields. (arXiv:2106.01553v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01553</id>
        <link href="http://arxiv.org/abs/2106.01553"/>
        <updated>2021-06-04T01:12:31.114Z</updated>
        <summary type="html"><![CDATA[Multilayer perceptrons (MLPs) have been successfully used to represent 3D
shapes implicitly and compactly, by mapping 3D coordinates to the corresponding
signed distance values or occupancy values. In this paper, we propose a novel
positional encoding scheme, called Spline Positional Encoding, to map the input
coordinates to a high dimensional space before passing them to MLPs, for
helping to recover 3D signed distance fields with fine-scale geometric details
from unorganized 3D point clouds. We verified the superiority of our approach
over other positional encoding schemes on tasks of 3D shape reconstruction from
input point clouds and shape space learning. The efficacy of our approach
extended to image reconstruction is also demonstrated and evaluated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng-Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu-Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Retrieval and Generation Training for Grounded Text Generation. (arXiv:2105.06597v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06597</id>
        <link href="http://arxiv.org/abs/2105.06597"/>
        <updated>2021-06-04T01:12:31.051Z</updated>
        <summary type="html"><![CDATA[Recent advances in large-scale pre-training such as GPT-3 allow seemingly
high quality text to be generated from a given prompt. However, such generation
systems often suffer from problems of hallucinated facts, and are not
inherently designed to incorporate useful external information. Grounded
generation models appear to offer remedies, but their training typically relies
on rarely-available parallel data where corresponding information-relevant
documents are provided for context. We propose a framework that alleviates this
data constraint by jointly training a grounded generator and document retriever
on the language model signal. The model learns to reward retrieval of the
documents with the highest utility in generation, and attentively combines them
using a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We
demonstrate that both generator and retriever can take advantage of this joint
training and work synergistically to produce more informative and relevant text
in both prose and dialogue generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Siqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1"&gt;Chris Brockett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1"&gt;Michel Galley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1"&gt;Bill Dolan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Societal Biases in Language Generation: Progress and Challenges. (arXiv:2105.04054v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04054</id>
        <link href="http://arxiv.org/abs/2105.04054"/>
        <updated>2021-06-04T01:12:30.986Z</updated>
        <summary type="html"><![CDATA[Technology for language generation has advanced rapidly, spurred by
advancements in pre-training large models on massive amounts of data and the
need for intelligent agents to communicate in a natural manner. While
techniques can effectively generate fluent text, they can also produce
undesirable societal biases that can have a disproportionately negative impact
on marginalized populations. Language generation presents unique challenges for
biases in terms of direct user interaction and the structure of decoding
techniques. To better understand these challenges, we present a survey on
societal biases in language generation, focusing on how data and techniques
contribute to biases and progress towards reducing biases. Motivated by a lack
of studies on biases from decoding techniques, we also conduct experiments to
quantify the effects of these techniques. By further discussing general trends
and open challenges, we call to attention promising directions for research and
the importance of fairness and inclusivity considerations for language
generation applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1"&gt;Emily Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1"&gt;Premkumar Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Busters: Outlier Dimensions that Disrupt Transformers. (arXiv:2105.06990v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06990</id>
        <link href="http://arxiv.org/abs/2105.06990"/>
        <updated>2021-06-04T01:12:30.980Z</updated>
        <summary type="html"><![CDATA[Multiple studies have shown that Transformers are remarkably robust to
pruning. Contrary to this received wisdom, we demonstrate that pre-trained
Transformer encoders are surprisingly fragile to the removal of a very small
number of features in the layer outputs (<0.0001% of model weights). In case of
BERT and other pre-trained encoder Transformers, the affected component is the
scaling factors and biases in the LayerNorm. The outliers are high-magnitude
normalization parameters that emerge early in pre-training and show up
consistently in the same dimensional position throughout the model. We show
that disabling them significantly degrades both the MLM loss and the downstream
task performance. This effect is observed across several BERT-family models and
other popular pre-trained Transformer architectures, including BART, XLNet and
ELECTRA; we also show a similar effect in GPT-2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovaleva_O/0/1/0/all/0/1"&gt;Olga Kovaleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulshreshtha_S/0/1/0/all/0/1"&gt;Saurabh Kulshreshtha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1"&gt;Anna Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1"&gt;Anna Rumshisky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts. (arXiv:2105.03023v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03023</id>
        <link href="http://arxiv.org/abs/2105.03023"/>
        <updated>2021-06-04T01:12:30.974Z</updated>
        <summary type="html"><![CDATA[Despite recent advances in natural language generation, it remains
challenging to control attributes of generated text. We propose DExperts:
Decoding-time Experts, a decoding-time method for controlled text generation
that combines a pretrained language model with "expert" LMs and/or
"anti-expert" LMs in a product of experts. Intuitively, under the ensemble,
tokens only get high probability if they are considered likely by the experts,
and unlikely by the anti-experts. We apply DExperts to language detoxification
and sentiment-controlled generation, where we outperform existing controllable
generation methods on both automatic and human evaluations. Moreover, because
DExperts operates only on the output of the pretrained LM, it is effective with
(anti-)experts of smaller size, including when operating on GPT-3. Our work
highlights the promise of tuning small LMs on text with (un)desirable
attributes for efficient decoding-time steering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alisa Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1"&gt;Maarten Sap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1"&gt;Swabha Swayamdipta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01547</id>
        <link href="http://arxiv.org/abs/2102.01547"/>
        <updated>2021-06-04T01:12:30.958Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an open source, production first, and production
ready speech recognition toolkit called WeNet in which a new two-pass approach
is implemented to unify streaming and non-streaming end-to-end (E2E) speech
recognition in a single model. The main motivation of WeNet is to close the gap
between the research and the production of E2E speechrecognition models. WeNet
provides an efficient way to ship ASR applications in several real-world
scenarios, which is the main difference and advantage to other open source E2E
speech recognition toolkits. In our toolkit, a new two-pass method is
implemented. Our method propose a dynamic chunk-based attention strategy of the
the transformer layers to allow arbitrary right context length modifies in
hybrid CTC/attention architecture. The inference latency could be easily
controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. Our experiments on
the AISHELL-1 dataset using WeNet show that, our model achieves 5.03\% relative
character error rate (CER) reduction in non-streaming ASR compared to a
standard non-streaming transformer. After model quantification, our model
perform reasonable RTF and latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhuoyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhendong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1"&gt;Xin Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems. (arXiv:2104.08570v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08570</id>
        <link href="http://arxiv.org/abs/2104.08570"/>
        <updated>2021-06-04T01:12:30.952Z</updated>
        <summary type="html"><![CDATA[In task-oriented dialogue (ToD), a user holds a conversation with an
artificial agent to complete a concrete task. Although this technology
represents one of the central objectives of AI and has been the focus of ever
more intense research and development efforts, it is currently limited to a few
narrow domains (e.g., food ordering, ticket booking) and a handful of languages
(e.g., English, Chinese). This work provides an extensive overview of existing
methods and resources in multilingual ToD as an entry point to this exciting
and emerging field. We find that the most critical factor preventing the
creation of truly multilingual ToD systems is the lack of datasets in most
languages for both training and evaluation. In fact, acquiring annotations or
human feedback for each component of modular systems or for data-hungry
end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches
to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer
from resource-rich languages (almost exclusively English), either by means of
machine translation or multilingual representations. These approaches are
currently viable only for typologically similar languages and languages with
parallel / monolingual corpora available. On the other hand, their
effectiveness beyond these boundaries is doubtful or hard to assess due to the
lack of linguistically diverse benchmarks (especially for natural language
generation and end-to-end evaluation). To overcome this limitation, we draw
parallels between components of the ToD pipeline and other NLP tasks, which can
inspire solutions for learning in low-resource scenarios. Finally, we list
additional challenges that multilinguality poses for related areas (such as
speech and human-centred evaluation), and indicate future directions that hold
promise to further expand language coverage and dialogue capabilities of
current ToD systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1"&gt;Evgeniia Razumovskaia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1"&gt;Goran Glava&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majewska_O/0/1/0/all/0/1"&gt;Olga Majewska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo M. Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1"&gt;Anna Korhonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations. (arXiv:2012.03929v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03929</id>
        <link href="http://arxiv.org/abs/2012.03929"/>
        <updated>2021-06-04T01:12:30.944Z</updated>
        <summary type="html"><![CDATA[Intent detection is a key component of modern goal-oriented dialog systems
that accomplish a user task by predicting the intent of users' text input.
There are three primary challenges in designing robust and accurate intent
detection models. First, typical intent detection models require a large amount
of labeled data to achieve high accuracy. Unfortunately, in practical scenarios
it is more common to find small, unbalanced, and noisy datasets. Secondly, even
with large training data, the intent detection models can see a different
distribution of test data when being deployed in the real world, leading to
poor accuracy. Finally, a practical intent detection model must be
computationally efficient in both training and single query inference so that
it can be used continuously and re-trained frequently. We benchmark intent
detection methods on a variety of datasets. Our results show that Watson
Assistant's intent detection model outperforms other commercial solutions and
is comparable to large pretrained language models while requiring only a
fraction of computational resources and training data. Watson Assistant
demonstrates a higher degree of robustness when the training and test
distributions differ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haode Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lin Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1"&gt;Atin Sood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1"&gt;Abhishek Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunc_L/0/1/0/all/0/1"&gt;Ladislav Kunc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1"&gt;Saloni Potdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of KB Queries in Task-Oriented Dialogs. (arXiv:2005.00123v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00123</id>
        <link href="http://arxiv.org/abs/2005.00123"/>
        <updated>2021-06-04T01:12:30.936Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialog (TOD) systems often need to formulate knowledge base
(KB) queries corresponding to the user intent and use the query results to
generate system responses. Existing approaches require dialog datasets to
explicitly annotate these KB queries -- these annotations can be time
consuming, and expensive. In response, we define the novel problems of
predicting the KB query and training the dialog agent, without explicit KB
query annotation. For query prediction, we propose a reinforcement learning
(RL) baseline, which rewards the generation of those queries whose KB results
cover the entities mentioned in subsequent dialog. Further analysis reveals
that correlation among query attributes in KB can significantly confuse memory
augmented policy optimization (MAPO), an existing state of the art RL agent. To
address this, we improve the MAPO baseline with simple but important
modifications suited to our task. To train the full TOD system for our setting,
we propose a pipelined approach: it independently predicts when to make a KB
query (query position predictor), then predicts a KB query at the predicted
position (query predictor), and uses the results of predicted query in
subsequent dialog (next response predictor). Overall, our work proposes first
solutions to our novel problem, and our analysis highlights the research
challenges in training TOD systems without query annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1"&gt;Dinesh Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LyricJam: A system for generating lyrics for live instrumental music. (arXiv:2106.01960v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01960</id>
        <link href="http://arxiv.org/abs/2106.01960"/>
        <updated>2021-06-04T01:12:30.910Z</updated>
        <summary type="html"><![CDATA[We describe a real-time system that receives a live audio stream from a jam
session and generates lyric lines that are congruent with the live music being
played. Two novel approaches are proposed to align the learned latent spaces of
audio and text representations that allow the system to generate novel lyric
lines matching live instrumental music. One approach is based on adversarial
alignment of latent representations of audio and lyrics, while the other
approach learns to transfer the topology from the music latent space to the
lyric latent space. A user study with music artists using the system showed
that the system was useful not only in lyric composition, but also encouraged
the artists to improvise and find new musical expressions. Another user study
demonstrated that users preferred the lines generated using the proposed
methods to the lines generated by a baseline model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1"&gt;Olga Vechtomova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1"&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1"&gt;Dhruv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shortformer: Better Language Modeling using Shorter Inputs. (arXiv:2012.15832v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15832</id>
        <link href="http://arxiv.org/abs/2012.15832"/>
        <updated>2021-06-04T01:12:30.899Z</updated>
        <summary type="html"><![CDATA[Increasing the input length has been a driver of progress in language
modeling with transformers. We identify conditions where shorter inputs are not
harmful, and achieve perplexity and efficiency improvements through two new
methods that decrease input length. First, we show that initially training a
model on short subsequences before moving on to longer ones both reduces
overall training time and, surprisingly, substantially improves perplexity.
Second, we show how to improve the efficiency of recurrence methods in
transformers, which let models condition on previously processed tokens when
generating sequences that exceed the maximal length the transformer can handle
at once. Existing methods require computationally expensive relative position
embeddings; we introduce a simple alternative of adding absolute position
embeddings to queries and keys instead of to word embeddings, which efficiently
produces superior results. We show that these recurrent models also benefit
from short input lengths. Combining these techniques speeds up training by a
factor of 1.65, reduces memory usage, and substantially improves perplexity on
WikiText-103, without adding any parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1"&gt;Ofir Press&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction. (arXiv:2106.01709v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01709</id>
        <link href="http://arxiv.org/abs/2106.01709"/>
        <updated>2021-06-04T01:12:30.890Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction has attracted much attention in recent
years. It is usually formulated as a classification problem that predicts
relations for all entity pairs in the document. However, previous works
indiscriminately represent intra- and inter-sentential relations in the same
way, confounding the different patterns for predicting them. Besides, they
create a document graph and use paths between entities on the graph as clues
for logical reasoning. However, not all entity pairs can be connected with a
path and have the correct logical reasoning paths in their graph. Thus many
cases of logical reasoning cannot be covered. This paper proposes an effective
architecture, SIRE, to represent intra- and inter-sentential relations in
different ways. We design a new and straightforward form of logical reasoning
module that can cover more logical reasoning chains. Experiments on the public
datasets show SIRE outperforms the previous state-of-the-art methods. Further
analysis shows that our predictions are reliable and explainable. Our code is
available at https://github.com/DreamInvoker/SIRE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuting Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations. (arXiv:2106.01978v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01978</id>
        <link href="http://arxiv.org/abs/2106.01978"/>
        <updated>2021-06-04T01:12:30.867Z</updated>
        <summary type="html"><![CDATA[Emotion Recognition in Conversations (ERC) has gained increasing attention
for developing empathetic machines. Recently, many approaches have been devoted
to perceiving conversational context by deep learning models. However, these
approaches are insufficient in understanding the context due to lacking the
ability to extract and integrate emotional clues. In this work, we propose
novel Contextual Reasoning Networks (DialogueCRN) to fully understand the
conversational context from a cognitive perspective. Inspired by the Cognitive
Theory of Emotion, we design multi-turn reasoning modules to extract and
integrate emotional clues. The reasoning module iteratively performs an
intuitive retrieving process and a conscious reasoning process, which imitates
human unique cognitive thinking. Extensive experiments on three public
benchmark datasets demonstrate the effectiveness and superiority of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dou Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huai_X/0/1/0/all/0/1"&gt;Xiaoyong Huai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reordering Examples Helps during Priming-based Few-Shot Learning. (arXiv:2106.01751v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01751</id>
        <link href="http://arxiv.org/abs/2106.01751"/>
        <updated>2021-06-04T01:12:30.851Z</updated>
        <summary type="html"><![CDATA[The ability to learn from limited data, or few-shot learning, is a desirable
and often critical requirement for NLP systems. While many existing methods do
poorly at learning from a handful of examples, large pretrained language models
have recently been shown to be efficient few-shot learners. One approach to
few-shot learning, which does not require finetuning of model parameters, is to
augment the language model's input with priming text which is typically
constructed using task specific descriptions and examples. In this work, we
further explore priming-based few-shot learning, with focus on using examples
as prompts. We show that presenting examples in the right order is key for
generalization. We introduce PERO (Prompting with Examples in the Right Order),
where we formulate few-shot learning as search over the set of permutations of
the training examples. We show that PERO can learn to generalize efficiently
using as few as 10 examples, in contrast to existing approaches. While the
newline token is a natural choice for separating the examples in the prompt, we
show that learning a new separator token can potentially provide further gains
in performance. We demonstrate the effectiveness of the proposed method on the
tasks of sentiment classification, natural language inference and fact
retrieval. Finally, we analyze the learned prompts to reveal novel insights,
including the idea that two training examples in the right order alone can
provide competitive performance for sentiment classification and natural
language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does BERT Solve Commonsense Task via Commonsense Knowledge?. (arXiv:2008.03945v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03945</id>
        <link href="http://arxiv.org/abs/2008.03945"/>
        <updated>2021-06-04T01:12:30.838Z</updated>
        <summary type="html"><![CDATA[BERT has been used for solving commonsense tasks such as CommonsenseQA. While
prior research has found that BERT does contain commonsense information to some
extent, there has been work showing that pre-trained models can rely on
spurious associations (e.g., data bias) rather than key cues in solving
sentiment classification and other problems. We quantitatively investigate the
presence of structural commonsense cues in BERT when solving commonsense tasks,
and the importance of such cues for the model prediction. Using two different
measures, we find that BERT does use relevant knowledge for solving the task,
and the presence of commonsense knowledge is positively correlated to the model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sijie Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling. (arXiv:2106.01925v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01925</id>
        <link href="http://arxiv.org/abs/2106.01925"/>
        <updated>2021-06-04T01:12:30.823Z</updated>
        <summary type="html"><![CDATA[Multi-intent SLU can handle multiple intents in an utterance, which has
attracted increasing attention. However, the state-of-the-art joint models
heavily rely on autoregressive approaches, resulting in two issues: slow
inference speed and information leakage. In this paper, we explore a
non-autoregressive model for joint multiple intent detection and slot filling,
achieving more fast and accurate. Specifically, we propose a Global-Locally
Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction
layer is proposed to model slot dependency for alleviating uncoordinated slots
problem while a global intent-slot graph interaction layer is introduced to
model the interaction between multiple intents and all slots in the utterance.
Experimental results on two public datasets show that our framework achieves
state-of-the-art performance while being 11.5 times faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Libo Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fuxuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianbao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection. (arXiv:2012.15761v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15761</id>
        <link href="http://arxiv.org/abs/2012.15761"/>
        <updated>2021-06-04T01:12:30.817Z</updated>
        <summary type="html"><![CDATA[We present a human-and-model-in-the-loop process for dynamically generating
datasets and training better performing and more robust hate detection models.
We provide a new dataset of ~40,000 entries, generated and labelled by trained
annotators over four rounds of dynamic data creation. It includes ~15,000
challenging perturbations and each hateful entry has fine-grained labels for
the type and target of hate. Hateful entries make up 54% of the dataset, which
is substantially higher than comparable datasets. We show that model
performance is substantially improved using this approach. Models trained on
later rounds of data collection perform better on test sets and are harder for
annotators to trick. They also perform better on HateCheck, a suite of
functional tests for online hate detection. We provide the code, dataset and
annotation guidelines for other researchers to use. Accepted at ACL 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1"&gt;Bertie Vidgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1"&gt;Tristan Thrush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study of Spanish Text Transformations for Twitter Sentiment Analysis. (arXiv:2106.02009v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02009</id>
        <link href="http://arxiv.org/abs/2106.02009"/>
        <updated>2021-06-04T01:12:30.810Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis is a text mining task that determines the polarity of a
given text, i.e., its positiveness or negativeness. Recently, it has received a
lot of attention given the interest in opinion mining in micro-blogging
platforms. These new forms of textual expressions present new challenges to
analyze text given the use of slang, orthographic and grammatical errors, among
others. Along with these challenges, a practical sentiment classifier should be
able to handle efficiently large workloads.

The aim of this research is to identify which text transformations
(lemmatization, stemming, entity removal, among others), tokenizers (e.g.,
words $n$-grams), and tokens weighting schemes impact the most the accuracy of
a classifier (Support Vector Machine) trained on two Spanish corpus. The
methodology used is to exhaustively analyze all the combinations of the text
transformations and their respective parameters to find out which
characteristics the best performing classifiers have in common. Furthermore,
among the different text transformations studied, we introduce a novel approach
based on the combination of word based $n$-grams and character based $q$-grams.
The results show that this novel combination of words and characters produces a
classifier that outperforms the traditional word based combination by $11.17\%$
and $5.62\%$ on the INEGI and TASS'15 dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1"&gt;Eric S. Tellez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miranda_Jimenez_S/0/1/0/all/0/1"&gt;Sabino Miranda-Jim&amp;#xe9;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1"&gt;Mario Graff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1"&gt;Daniela Moctezuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siodia_O/0/1/0/all/0/1"&gt;Oscar S. Siodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villasenor_E/0/1/0/all/0/1"&gt;Elio A. Villase&amp;#xf1;or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Refinement of Cross-Lingual Word Embeddings. (arXiv:1911.03070v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03070</id>
        <link href="http://arxiv.org/abs/1911.03070"/>
        <updated>2021-06-04T01:12:30.792Z</updated>
        <summary type="html"><![CDATA[Cross-lingual word embeddings transfer knowledge between languages: models
trained on high-resource languages can predict in low-resource languages. We
introduce CLIME, an interactive system to quickly refine cross-lingual word
embeddings for a given classification problem. First, CLIME ranks words by
their salience to the downstream task. Then, users mark similarity between
keywords and their nearest neighbors in the embedding space. Finally, CLIME
updates the embeddings using the annotations. We evaluate CLIME on identifying
health-related text in four low-resource languages: Ilocano, Sinhalese,
Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word
semantics and have higher test accuracy than the original embeddings. CLIME
often improves accuracy faster than an active learning baseline and can be
easily combined with active learning to improve results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Michelle Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1"&gt;Benjamin Van Durme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Findlater_L/0/1/0/all/0/1"&gt;Leah Findlater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1"&gt;Jordan Boyd-Graber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction. (arXiv:2106.01793v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01793</id>
        <link href="http://arxiv.org/abs/2106.01793"/>
        <updated>2021-06-04T01:12:30.785Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) is a more challenging task than
sentence RE as it often requires reasoning over multiple sentences. Yet, human
annotators usually use a small number of sentences to identify the relationship
between a given entity pair. In this paper, we present an embarrassingly simple
but effective method to heuristically select evidence sentences for
document-level RE, which can be easily combined with BiLSTM to achieve good
performance on benchmark datasets, even better than fancy graph neural network
based methods. We have released our code at
https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Quzhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shengqi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yansong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yuxuan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability. (arXiv:2106.02016v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02016</id>
        <link href="http://arxiv.org/abs/2106.02016"/>
        <updated>2021-06-04T01:12:30.779Z</updated>
        <summary type="html"><![CDATA[Recent advances in supervised, semi-supervised and self-supervised deep
learning algorithms have shown significant improvement in the performance of
automatic speech recognition(ASR) systems. The state-of-the-art systems have
achieved a word error rate (WER) less than 5%. However, in the past,
researchers have argued the non-suitability of the WER metric for the
evaluation of ASR systems for downstream tasks such as spoken language
understanding (SLU) and information retrieval. The reason is that the WER works
at the surface level and does not include any syntactic and semantic
knowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate
the ASR transcripts for downstream applications in general. The SWER can be
easily customized for any down-stream task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Somnath Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CCPM: A Chinese Classical Poetry Matching Dataset. (arXiv:2106.01979v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01979</id>
        <link href="http://arxiv.org/abs/2106.01979"/>
        <updated>2021-06-04T01:12:30.773Z</updated>
        <summary type="html"><![CDATA[Poetry is one of the most important art forms of human languages. Recently
many studies have focused on incorporating some linguistic features of poetry,
such as style and sentiment, into its understanding or generation system.
However, there is no focus on understanding or evaluating the semantics of
poetry. Therefore, we propose a novel task to assess a model's semantic
understanding of poetry by poem matching. Specifically, this task requires the
model to select one line of Chinese classical poetry among four candidates
according to the modern Chinese translation of a line of poetry. To construct
this dataset, we first obtain a set of parallel data of Chinese classical
poetry and modern Chinese translation. Then we retrieve similar lines of poetry
with the lines in a poetry corpus as negative choices. We name the dataset
Chinese Classical Poetry Matching Dataset (CCPM) and release it at
https://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance
the study on incorporating deep semantics into the understanding and generation
system of Chinese classical poetry. We also preliminarily run two variants of
BERT on this dataset as the baselines for this dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiarui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer. (arXiv:2106.01732v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01732</id>
        <link href="http://arxiv.org/abs/2106.01732"/>
        <updated>2021-06-04T01:12:30.767Z</updated>
        <summary type="html"><![CDATA[Multilingual pre-trained models have achieved remarkable transfer performance
by pre-trained on rich kinds of languages. Most of the models such as mBERT are
pre-trained on unlabeled corpora. The static and contextual embeddings from the
models could not be aligned very well. In this paper, we aim to improve the
zero-shot cross-lingual transfer performance by aligning the embeddings better.
We propose a pre-training task named Alignment Language Model (AlignLM), which
uses the statistical alignment information as the prior knowledge to guide
bilingual word prediction. We evaluate our method on multilingual machine
reading comprehension and natural language interface tasks. The results show
AlignLM can improve the zero-shot performance significantly on MLQA and XNLI
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziqing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wentao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jiani Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning. (arXiv:2106.01804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01804</id>
        <link href="http://arxiv.org/abs/2106.01804"/>
        <updated>2021-06-04T01:12:30.747Z</updated>
        <summary type="html"><![CDATA[Vision-language pre-training (VLP) on large-scale image-text pairs has
achieved huge success for the cross-modal downstream tasks. The most existing
pre-training methods mainly adopt a two-step training procedure, which firstly
employs a pre-trained object detector to extract region-based visual features,
then concatenates the image representation and text embedding as the input of
Transformer to train. However, these methods face problems of using
task-specific visual representation of the specific object detector for generic
cross-modal understanding, and the computation inefficiency of two-stage
pipeline. In this paper, we propose the first end-to-end vision-language
pre-trained model for both V+L understanding and generation, namely E2E-VLP,
where we build a unified Transformer framework to jointly learn visual
representation, and semantic alignments between image and text. We incorporate
the tasks of object detection and image captioning into pre-training with a
unified Transformer encoder-decoder architecture for enhancing visual learning.
An extensive set of experiments have been conducted on well-established
vision-language downstream tasks to demonstrate the effectiveness of this novel
VLP paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Songfang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenming Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmoDNN: Understanding emotions from short texts through a deep neural network ensemble. (arXiv:2106.01706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01706</id>
        <link href="http://arxiv.org/abs/2106.01706"/>
        <updated>2021-06-04T01:12:30.741Z</updated>
        <summary type="html"><![CDATA[The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representing Syntax and Composition with Geometric Transformations. (arXiv:2106.01904v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01904</id>
        <link href="http://arxiv.org/abs/2106.01904"/>
        <updated>2021-06-04T01:12:30.735Z</updated>
        <summary type="html"><![CDATA[The exploitation of syntactic graphs (SyGs) as a word's context has been
shown to be beneficial for distributional semantic models (DSMs), both at the
level of individual word representations and in deriving phrasal
representations via composition. However, notwithstanding the potential
performance benefit, the syntactically-aware DSMs proposed to date have huge
numbers of parameters (compared to conventional DSMs) and suffer from data
sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic
relations) has been largely limited to linear maps. The knowledge graphs'
literature, on the other hand, has proposed light-weight models employing
different geometric transformations (GTs) to encode edges in a knowledge graph
(KG). Our work explores the possibility of adopting this family of models to
encode SyGs. Furthermore, we investigate which GT better encodes syntactic
relations, so that these representations can be used to enhance phrase-level
composition via syntactic contextualisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertolini_L/0/1/0/all/0/1"&gt;Lorenzo Bertolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weeds_J/0/1/0/all/0/1"&gt;Julie Weeds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weir_D/0/1/0/all/0/1"&gt;David Weir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1"&gt;Qiwei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Hallucinated Content in Conditional Neural Sequence Generation. (arXiv:2011.02593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02593</id>
        <link href="http://arxiv.org/abs/2011.02593"/>
        <updated>2021-06-04T01:12:30.729Z</updated>
        <summary type="html"><![CDATA[Neural sequence models can generate highly fluent sentences, but recent
studies have also shown that they are also prone to hallucinate additional
content not supported by the input. These variety of fluent but wrong outputs
are particularly problematic, as it will not be possible for users to tell they
are being presented incorrect content. To detect these errors, we propose a
task to predict whether each token in the output sequence is hallucinated (not
contained in the input) and collect new manually annotated evaluation sets for
this task. We also introduce a method for learning to detect hallucinations
using pretrained language models fine tuned on synthetic data that includes
automatically inserted hallucinations Experiments on machine translation (MT)
and abstractive summarization demonstrate that our proposed approach
consistently outperforms strong baselines on all benchmark datasets. We further
demonstrate how to use the token-level hallucination labels to define a
fine-grained loss over the target sequence in low-resource MT and achieve
significant improvements over strong baseline methods. We also apply our method
to word-level quality estimation for MT and show its effectiveness in both
supervised and unsupervised settings. Codes and data available at
https://github.com/violet-zct/fairseq-detect-hallucination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chunting Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1"&gt;Mona Diab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_P/0/1/0/all/0/1"&gt;Paco Guzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1"&gt;Marjan Ghazvininejad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models. (arXiv:2106.01950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01950</id>
        <link href="http://arxiv.org/abs/2106.01950"/>
        <updated>2021-06-04T01:12:30.722Z</updated>
        <summary type="html"><![CDATA[Mechanisms for encoding positional information are central for
transformer-based language models. In this paper, we analyze the position
embeddings of existing language models, finding strong evidence of translation
invariance, both for the embeddings themselves and for their effect on
self-attention. The degree of translation invariance increases during training
and correlates positively with model performance. Our findings lead us to
propose translation-invariant self-attention (TISA), which accounts for the
relative position between tokens in an interpretable fashion without needing
conventional position embeddings. Our proposal has several theoretical
advantages over existing position-representation approaches. Experiments show
that it improves on regular ALBERT on GLUE tasks, while only adding orders of
magnitude less positional parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wennberg_U/0/1/0/all/0/1"&gt;Ulme Wennberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1"&gt;Gustav Eje Henter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Secure Generative Linguistic Steganography. (arXiv:2106.02011v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02011</id>
        <link href="http://arxiv.org/abs/2106.02011"/>
        <updated>2021-06-04T01:12:30.702Z</updated>
        <summary type="html"><![CDATA[Generative linguistic steganography mainly utilized language models and
applied steganographic sampling (stegosampling) to generate high-security
steganographic text (stegotext). However, previous methods generally lead to
statistical differences between the conditional probability distributions of
stegotext and natural text, which brings about security risks. In this paper,
to further ensure security, we present a novel provably secure generative
linguistic steganographic method ADG, which recursively embeds secret
information by Adaptive Dynamic Grouping of tokens according to their
probability given by an off-the-shelf language model. We not only prove the
security of ADG mathematically, but also conduct extensive experiments on three
public corpora to further verify its imperceptibility. The experimental results
reveal that the proposed method is able to generate stegotext with nearly
perfect security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Siyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhongliang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinshuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain. (arXiv:2106.01972v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01972</id>
        <link href="http://arxiv.org/abs/2106.01972"/>
        <updated>2021-06-04T01:12:30.696Z</updated>
        <summary type="html"><![CDATA[In the pursuit of natural language understanding, there has been a long
standing interest in tracking state changes throughout narratives. Impressive
progress has been made in modeling the state of transaction-centric dialogues
and procedural texts. However, this problem has been less intensively studied
in the realm of general discourse where ground truth descriptions of states may
be loosely defined and state changes are less densely distributed over
utterances. This paper proposes to turn to simplified, fully observable systems
that show some of these properties: Sports events. We curated 2,263 soccer
matches including time-stamped natural language commentary accompanied by
discrete events such as a team scoring goals, switching players or being
penalized with cards. We propose a new task formulation where, given paragraphs
of commentary of a game at different timestamps, the system is asked to
recognize the occurrence of in-game events. This domain allows for rich
descriptions of state while avoiding the complexities of many other real-world
settings. As an initial point of performance measurement, we include two
baseline methods from the perspectives of sentence classification with temporal
dependence and current state-of-the-art generative model, respectively, and
demonstrate that even sophisticated existing methods struggle on the state
tracking task when the definition of state broadens or non-event chatter
becomes prevalent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruochen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Distantly-Labeled Rationales in Neural Network Models. (arXiv:2106.01809v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01809</id>
        <link href="http://arxiv.org/abs/2106.01809"/>
        <updated>2021-06-04T01:12:30.690Z</updated>
        <summary type="html"><![CDATA[Recent studies strive to incorporate various human rationales into neural
networks to improve model performance, but few pay attention to the quality of
the rationales. Most existing methods distribute their models' focus to
distantly-labeled rationale words entirely and equally, while ignoring the
potential important non-rationale words and not distinguishing the importance
of different rationale words. In this paper, we propose two novel auxiliary
loss functions to make better use of distantly-labeled rationales, which
encourage models to maintain their focus on important words beyond labeled
rationales (PINs) and alleviate redundant training on non-helpful rationales
(NoIRs). Experiments on two representative classification tasks show that our
proposed methods can push a classification model to effectively learn crucial
clues from non-perfect rationales while maintaining the ability to spread its
focus to other unlabeled important words, thus significantly outperform
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Quzhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shengqi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yansong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fingerprinting Fine-tuned Language Models in the Wild. (arXiv:2106.01703v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01703</id>
        <link href="http://arxiv.org/abs/2106.01703"/>
        <updated>2021-06-04T01:12:30.684Z</updated>
        <summary type="html"><![CDATA[There are concerns that the ability of language models (LMs) to generate high
quality synthetic text can be misused to launch spam, disinformation, or
propaganda. Therefore, the research community is actively working on developing
approaches to detect whether a given text is organic or synthetic. While this
is a useful first step, it is important to be able to further fingerprint the
author LM to attribute its origin. Prior work on fingerprinting LMs is limited
to attributing synthetic text generated by a handful (usually < 10) of
pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad
of ways (e.g., on a domain-specific text corpus) before being used to generate
synthetic text. It is challenging to fingerprinting fine-tuned LMs because the
universe of fine-tuned LMs is much larger in realistic scenarios. To address
this challenge, we study the problem of large-scale fingerprinting of
fine-tuned LMs in the wild. Using a real-world dataset of synthetic text
generated by 108 different fine-tuned LMs, we conduct comprehensive experiments
to demonstrate the limitations of existing fingerprinting approaches. Our
results show that fine-tuning itself is the most effective in attributing the
synthetic text generated by fine-tuned LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diwan_N/0/1/0/all/0/1"&gt;Nirav Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_T/0/1/0/all/0/1"&gt;Tanmoy Chakravorty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiq_Z/0/1/0/all/0/1"&gt;Zubair Shafiq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages. (arXiv:2010.09322v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09322</id>
        <link href="http://arxiv.org/abs/2010.09322"/>
        <updated>2021-06-04T01:12:30.676Z</updated>
        <summary type="html"><![CDATA[This work presents a seemingly simple but effective technique to improve
low-resource ASR systems for phonetic languages. By identifying sets of
acoustically similar graphemes in these languages, we first reduce the output
alphabet of the ASR system using linguistically meaningful reductions and then
reconstruct the original alphabet using a standalone module. We demonstrate
that this lessens the burden and improves the performance of low-resource
end-to-end ASR systems (because only reduced-alphabet predictions are needed)
and that it is possible to design a very simple but effective reconstruction
module that recovers sequences in the original alphabet from sequences in the
reduced alphabet. We present a finite state transducer-based reconstruction
module that operates on the 1-best ASR hypothesis in the reduced alphabet. We
demonstrate the efficacy of our proposed technique using ASR systems for two
Indian languages, Gujarati and Telugu. With access to only 10 hrs of speech
data, we obtain relative WER reductions of up to 7% compared to systems that do
not use any reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Diwan_A/0/1/0/all/0/1"&gt;Anuj Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Template-Based Named Entity Recognition Using BART. (arXiv:2106.01760v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01760</id>
        <link href="http://arxiv.org/abs/2106.01760"/>
        <updated>2021-06-04T01:12:30.670Z</updated>
        <summary type="html"><![CDATA[There is a recent interest in investigating few-shot NER, where the
low-resource target domain has different label sets compared with a
resource-rich source domain. Existing methods use a similarity-based metric.
However, they cannot make full use of knowledge transfer in NER model
parameters. To address the issue, we propose a template-based method for NER,
treating NER as a language model ranking problem in a sequence-to-sequence
framework, where original sentences and statement templates filled by candidate
named entity span are regarded as the source sequence and the target sequence,
respectively. For inference, the model is required to classify each candidate
span based on the corresponding template scores. Our experiments demonstrate
that the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource
task), and significantly better than fine-tuning BERT 10.88%, 15.34%, and
11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS
(low-resource task), respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01810</id>
        <link href="http://arxiv.org/abs/2106.01810"/>
        <updated>2021-06-04T01:12:30.651Z</updated>
        <summary type="html"><![CDATA[The frustratingly fragile nature of neural network models make current
natural language generation (NLG) systems prone to backdoor attacks and
generate malicious sequences that could be sexist or offensive. Unfortunately,
little effort has been invested to how backdoor attacks can affect current NLG
models and how to defend against these attacks. In this work, we investigate
this problem on two important NLG tasks, machine translation and dialogue
generation. By giving a formal definition for backdoor attack and defense, and
developing corresponding benchmarks, we design methods to attack NLG models,
which achieve high attack success to ask NLG models to generate malicious
sequences. To defend against these attacks, we propose to detect the attack
trigger by examining the effect of deleting or replacing certain words on the
generation outputs, which we find successful for certain types of attacks. We
will discuss the limitation of this work, and hope this work can raise the
awareness of backdoor risks concealed in deep NLG systems. (Code and data are
available at https://github.com/ShannonAI/backdoor_nlg.)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Chun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoya Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yuxian Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiaofei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1"&gt;Xiang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization. (arXiv:2106.01890v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01890</id>
        <link href="http://arxiv.org/abs/2106.01890"/>
        <updated>2021-06-04T01:12:30.644Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a conceptually simple while empirically powerful
framework for abstractive summarization, SimCLS, which can bridge the gap
between the learning objective and evaluation metrics resulting from the
currently dominated sequence-to-sequence learning framework by formulating text
generation as a reference-free evaluation problem (i.e., quality estimation)
assisted by contrastive learning. Experimental results show that, with minor
modification over existing top-scoring systems, SimCLS can improve the
performance of existing top-performing models by a large margin. Particularly,
2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on
the CNN/DailyMail dataset, driving the state-of-the-art performance to a new
level. We have open-sourced our codes and results:
https://github.com/yixinL7/SimCLS. Results of our proposed models have been
deployed into ExplainaBoard platform, which allows researchers to understand
our systems in a more fine-grained way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNA-GCN: Graph convolutional networks for predicting DNA-protein binding. (arXiv:2106.01836v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2106.01836</id>
        <link href="http://arxiv.org/abs/2106.01836"/>
        <updated>2021-06-04T01:12:30.471Z</updated>
        <summary type="html"><![CDATA[Predicting DNA-protein binding is an important and classic problem in
bioinformatics. Convolutional neural networks have outperformed conventional
methods in modeling the sequence specificity of DNA-protein binding. However,
none of the studies has utilized graph convolutional networks for motif
inference. In this work, we propose to use graph convolutional networks for
motif inference. We build a sequence k-mer graph for the whole dataset based on
k-mer co-occurrence and k-mer sequence relationship and then learn DNA Graph
Convolutional Network (DNA-GCN) for the whole dataset. Our DNA-GCN is
initialized with a one-hot representation for all nodes, and it then jointly
learns the embeddings for both k-mers and sequences, as supervised by the known
labels of sequences. We evaluate our model on 50 datasets from ENCODE. DNA-GCN
shows its competitive performance compared with the baseline model. Besides, we
analyze our model and design several different architectures to help fit
different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuhang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Hand Pose Estimation via Regularized Graph Representation Learning. (arXiv:1912.01875v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01875</id>
        <link href="http://arxiv.org/abs/1912.01875"/>
        <updated>2021-06-04T01:12:30.464Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of 3D hand pose estimation from a monocular
RGB image. While previous methods have shown great success, the structure of
hands has not been fully exploited, which is critical in pose estimation. To
this end, we propose a regularized graph representation learning under a
conditional adversarial learning framework for 3D hand pose estimation, aiming
to capture structural inter-dependencies of hand joints. In particular, we
estimate an initial hand pose from a parametric hand model as a prior of hand
structure, which regularizes the inference of the structural deformation in the
prior pose for accurate graph representation learning via residual graph
convolution. To optimize the hand structure further, we propose two
bone-constrained loss functions, which characterize the morphable structure of
hand poses explicitly. Also, we introduce an adversarial learning framework
conditioned on the input image with a multi-source discriminator, which imposes
the structural constraints onto the distribution of generated 3D hand poses for
anthropomorphically valid hand poses. Extensive experiments demonstrate that
our model sets the new state-of-the-art in 3D hand pose estimation from a
monocular image on five standard benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yiming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Reference-based Super-Resolution via C2-Matching. (arXiv:2106.01863v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01863</id>
        <link href="http://arxiv.org/abs/2106.01863"/>
        <updated>2021-06-04T01:12:30.458Z</updated>
        <summary type="html"><![CDATA[Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising
paradigm to enhance a low-resolution (LR) input image by introducing an
additional high-resolution (HR) reference image. Existing Ref-SR methods mostly
rely on implicit correspondence matching to borrow HR textures from reference
images to compensate for the information loss in input images. However,
performing local transfer is difficult because of two gaps between input and
reference images: the transformation gap (e.g. scale and rotation) and the
resolution gap (e.g. HR and LR). To tackle these challenges, we propose
C2-Matching in this work, which produces explicit robust matching crossing
transformation and resolution. 1) For the transformation gap, we propose a
contrastive correspondence network, which learns transformation-robust
correspondences using augmented views of the input image. 2) For the resolution
gap, we adopt a teacher-student correlation distillation, which distills
knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR
matching. 3) Finally, we design a dynamic aggregation module to address the
potential misalignment issue. In addition, to faithfully evaluate the
performance of Ref-SR under a realistic setting, we contribute the
Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario.
Extensive experiments demonstrate that our proposed C2-Matching significantly
outperforms state of the arts by over 1dB on the standard CUFED5 benchmark.
Notably, it also shows great generalizability on WR-SR dataset as well as
robustness across large scale and rotation transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying language changes surrounding mental health on Twitter. (arXiv:2106.01481v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01481</id>
        <link href="http://arxiv.org/abs/2106.01481"/>
        <updated>2021-06-04T01:12:30.439Z</updated>
        <summary type="html"><![CDATA[Mental health challenges are thought to afflict around 10% of the global
population each year, with many going untreated due to stigma and limited
access to services. Here, we explore trends in words and phrases related to
mental health through a collection of 1- , 2-, and 3-grams parsed from a data
stream of roughly 10% of all English tweets since 2012. We examine temporal
dynamics of mental health language, finding that the popularity of the phrase
'mental health' increased by nearly two orders of magnitude between 2012 and
2018. We observe that mentions of 'mental health' spike annually and reliably
due to mental health awareness campaigns, as well as unpredictably in response
to mass shootings, celebrities dying by suicide, and popular fictional stories
portraying suicide. We find that the level of positivity of messages containing
'mental health', while stable through the growth period, has declined recently.
Finally, we use the ratio of original tweets to retweets to quantify the
fraction of appearances of mental health language due to social amplification.
Since 2015, mentions of mental health have become increasingly due to retweets,
suggesting that stigma associated with discussion of mental health on Twitter
has diminished with time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Stupinski_A/0/1/0/all/0/1"&gt;Anne Marie Stupinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Alshaabi_T/0/1/0/all/0/1"&gt;Thayer Alshaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Arnold_M/0/1/0/all/0/1"&gt;Michael V. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jane Lydia Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Minot_J/0/1/0/all/0/1"&gt;Joshua R. Minot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Price_M/0/1/0/all/0/1"&gt;Matthew Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dodds_P/0/1/0/all/0/1"&gt;Peter Sheridan Dodds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Danforth_C/0/1/0/all/0/1"&gt;Christopher M. Danforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01883</id>
        <link href="http://arxiv.org/abs/2106.01883"/>
        <updated>2021-06-04T01:12:30.432Z</updated>
        <summary type="html"><![CDATA[Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaojiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jirui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1"&gt;Qi Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02019</id>
        <link href="http://arxiv.org/abs/2106.02019"/>
        <updated>2021-06-04T01:12:30.426Z</updated>
        <summary type="html"><![CDATA[We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1"&gt;Marc Habermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1"&gt;Viktor Rudnev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1"&gt;Kripasindhu Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Conditional Density Estimation for Imputation and Classification of Incomplete Instances. (arXiv:2106.01708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01708</id>
        <link href="http://arxiv.org/abs/2106.01708"/>
        <updated>2021-06-04T01:12:30.420Z</updated>
        <summary type="html"><![CDATA[Incomplete instances with various missing attributes in many real-world
scenes have brought challenges to the classification task. There are some
missing values imputation methods to fill the missing values with substitute
values before classification. However, the separation between imputation and
classification may lead to inferior performance since label information are
ignored during imputation. Moreover, these imputation methods tend to
initialize these missing values with strong prior assumptions, while the
unreliability of such initialization is rarely considered. To tackle these
problems, a novel semi-supervised conditional normalizing flow (SSCFlow) is
proposed in this paper. SSCFlow explicitly utilizes the observed labels to
facilitate the imputation and classification simultaneously by employing a
semi-supervised algorithm to estimate the conditional probability density of
missing values. Moreover, SSCFlow takes the initialized missing values as
corrupted initial imputation and iteratively reconstructs their latent
representations with an overcomplete denoising autoencoder to approximate the
true conditional probability density of missing values. Experiments have been
conducted with real-world datasets to demonstrate the robustness and efficiency
of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Buliao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains. (arXiv:2106.01866v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01866</id>
        <link href="http://arxiv.org/abs/2106.01866"/>
        <updated>2021-06-04T01:12:30.413Z</updated>
        <summary type="html"><![CDATA[A robot working in human-centric environments needs to know which kind of
objects exist in the scene, where they are, and how to grasp and manipulate
various objects in different situations to help humans in everyday tasks.
Therefore, object recognition and grasping are two key functionalities for such
robots. Most state-of-the-art tackles object recognition and grasping as two
separate problems while both use visual input. Furthermore, the knowledge of
the robot is fixed after the training phase. In such cases, if the robot faces
new object categories, it must retrain from scratch to incorporate new
information without catastrophic interference. To address this problem, we
propose a deep learning architecture with augmented memory capacities to handle
open-ended object recognition and grasping simultaneously. In particular, our
approach takes multi-views of an object as input and jointly estimates
pixel-wise grasp configuration as well as a deep scale- and rotation-invariant
representation as outputs. The obtained representation is then used for
open-ended object recognition through a meta-active learning technique. We
demonstrate the ability of our approach to grasp never-seen-before objects and
to rapidly learn new object categories using very few examples on-site in both
simulation and real-world settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1"&gt;Hamidreza Kasaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Sha Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasso_R/0/1/0/all/0/1"&gt;Remo Sasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_M/0/1/0/all/0/1"&gt;Mohammadreza Kasaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Depth Estimation using Wavelet Decomposition. (arXiv:2106.02022v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02022</id>
        <link href="http://arxiv.org/abs/2106.02022"/>
        <updated>2021-06-04T01:12:30.393Z</updated>
        <summary type="html"><![CDATA[We present a novel method for predicting accurate depths from monocular
images with high efficiency. This optimal efficiency is achieved by exploiting
wavelet decomposition, which is integrated in a fully differentiable
encoder-decoder architecture. We demonstrate that we can reconstruct
high-fidelity depth maps by predicting sparse wavelet coefficients. In contrast
with previous works, we show that wavelet coefficients can be learned without
direct supervision on coefficients. Instead we supervise only the final depth
image that is reconstructed through the inverse wavelet transform. We
additionally show that wavelet coefficients can be learned in fully
self-supervised scenarios, without access to ground-truth depth. Finally, we
apply our method to different state-of-the-art monocular depth estimation
models, in each case giving similar or better results compared to the original
model, while requiring less than half the multiply-adds in the decoder network.
Code at https://github.com/nianticlabs/wavelet-monodepth]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramamonjisoa_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Ramamonjisoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1"&gt;Michael Firman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1"&gt;Jamie Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1"&gt;Vincent Lepetit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turmukhambetov_D/0/1/0/all/0/1"&gt;Daniyar Turmukhambetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in Classifying the Stages of Diabetic Retinopathy Using Convolutional Neural Networks in Low Memory Edge Devices. (arXiv:2106.01739v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01739</id>
        <link href="http://arxiv.org/abs/2106.01739"/>
        <updated>2021-06-04T01:12:30.387Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is a severe complication that may lead to retinal
vascular damage and is one of the leading causes of vision impairment and
blindness. DR broadly is classified into two stages - non-proliferative (NPDR),
where there are almost no symptoms, except a few microaneurysms, and
proliferative (PDR) involving a huge number of microaneurysms and hemorrhages,
soft and hard exudates, neo-vascularization, macular ischemia or a combination
of these, making it easier to detect. More specifically, DR is usually
classified into five levels, labeled 0-4, from 0 indicating no DR to 4 which is
most severe. This paper firstly presents a discussion on the risk factors of
the disease, then surveys the recent literature on the topic followed by
examining certain techniques which were found to be highly effective in
improving the prognosis accuracy. Finally, a convolutional neural network model
is proposed to detect all the stages of DR on a low-memory edge
microcontroller. The model has a size of just 5.9 MB, accuracy and F1 score
both of 94% and an inference speed of about 20 frames per second.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Based Analysis of Prostate Cancer from MP-MRI. (arXiv:2106.01835v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01835</id>
        <link href="http://arxiv.org/abs/2106.01835"/>
        <updated>2021-06-04T01:12:30.381Z</updated>
        <summary type="html"><![CDATA[The diagnosis of prostate cancer faces a problem with overdiagnosis that
leads to damaging side effects due to unnecessary treatment. Research has shown
that the use of multi-parametric magnetic resonance images to conduct biopsies
can drastically help to mitigate the overdiagnosis, thus reducing the side
effects on healthy patients. This study aims to investigate the use of deep
learning techniques to explore computer-aid diagnosis based on MRI as input.
Several diagnosis problems ranging from classification of lesions as being
clinically significant or not to the detection and segmentation of lesions are
addressed with deep learning based approaches.

This thesis tackled two main problems regarding the diagnosis of prostate
cancer. Firstly, XmasNet was used to conduct two large experiments on the
classification of lesions. Secondly, detection and segmentation experiments
were conducted, first on the prostate and afterward on the prostate cancer
lesions. The former experiments explored the lesions through a two-dimensional
space, while the latter explored models to work with three-dimensional inputs.
For this task, the 3D models explored were the 3D U-Net and a pretrained 3D
ResNet-18. A rigorous analysis of all these problems was conducted with a total
of two networks, two cropping techniques, two resampling techniques, two crop
sizes, five input sizes and data augmentations experimented for lesion
classification. While for segmentation two models, two input sizes and data
augmentations were experimented. However, while the binary classification of
the clinical significance of lesions and the detection and segmentation of the
prostate already achieve the desired results (0.870 AUC and 0.915 dice score
respectively), the classification of the PIRADS score and the segmentation of
lesions still have a large margin to improve (0.664 accuracy and 0.690 dice
score respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Neto_P/0/1/0/all/0/1"&gt;Pedro C. Neto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipative Video Transformer. (arXiv:2106.02036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02036</id>
        <link href="http://arxiv.org/abs/2106.02036"/>
        <updated>2021-06-04T01:12:30.375Z</updated>
        <summary type="html"><![CDATA[We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Shared Semantic Space for Speech-to-Text Translation. (arXiv:2105.03095v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03095</id>
        <link href="http://arxiv.org/abs/2105.03095"/>
        <updated>2021-06-04T01:12:30.368Z</updated>
        <summary type="html"><![CDATA[Having numerous potential applications and great impact, end-to-end speech
translation (ST) has long been treated as an independent task, failing to fully
draw strength from the rapid advances of its sibling - text machine translation
(MT). With text and audio inputs represented differently, the modality gap has
rendered MT data and its end-to-end models incompatible with their ST
counterparts. In observation of this obstacle, we propose to bridge this
representation gap with Chimera. By projecting audio and text features to a
common semantic representation, Chimera unifies MT and ST tasks and boosts the
performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new
state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,
improving the SOTA by a +1.9 BLEU margin. Further experimental analyses
demonstrate that the shared semantic space indeed conveys common knowledge
between these two tasks and thus paves a new way for augmenting training
resources across modalities. Code, data, and resources are available at
https://github.com/Glaciohound/Chimera-ST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising and Optical and SAR Image Classifications Based on Feature Extraction and Sparse Representation. (arXiv:2106.01896v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01896</id>
        <link href="http://arxiv.org/abs/2106.01896"/>
        <updated>2021-06-04T01:12:30.362Z</updated>
        <summary type="html"><![CDATA[Optical image data have been used by the Remote Sensing workforce to study
land use and cover since such data is easily interpretable. Synthetic Aperture
Radar (SAR) has the characteristic of obtaining images during all-day,
all-weather and provides object information that is different from visible and
infrared sensors. However, SAR images have more speckle noise and fewer
dimensions. This paper presents a method for denoising, feature extraction and
compares classifications of Optical and SAR images. The image was denoised
using K-Singular Value Decomposition (K-SVD) algorithm. A method to map the
extraordinary goal signatures to be had withinside the SAR or Optical image
using support vector machine (SVM) through offering given the enter facts to
the supervised classifier. Initially, the Gray Level Histogram (GLH) and Gray
Level Co-occurrence Matrix (GLCM) are used for feature extraction. Secondly,
the extracted feature vectors from the first step were combined using
correlation analysis to reduce the dimensionality of the feature spaces.
Thirdly, the Classification of SAR images was done in Sparse Representations
Classification (SRC). The above-mentioned classifications techniques were
developed and performance parameters are accuracy and Kappa Coefficient
calculated using MATLAB 2018a.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Balnarsaiah_B/0/1/0/all/0/1"&gt;Battula Balnarsaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajitha_G/0/1/0/all/0/1"&gt;G Rajitha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Baseline for Sentence-level Relation Extraction. (arXiv:2102.01373v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01373</id>
        <link href="http://arxiv.org/abs/2102.01373"/>
        <updated>2021-06-04T01:12:30.343Z</updated>
        <summary type="html"><![CDATA[Sentence-level relation extraction (RE) aims at identifying the relationship
between two entities in a sentence. Many efforts have been devoted to this
problem, while the best performing methods are still far from perfect. In this
paper, we revisit two problems that affect the performance of existing RE
models, namely entity representation and noisy or ill-defined labels. Our
improved baseline model, incorporated with entity representations with typed
markers, achieves an F1 of 74.6% on TACRED, significantly outperforms previous
SOTA methods. Furthermore, the presented new baseline achieves an F1 of 91.1%
on the refined Re-TACRED dataset, demonstrating that the pre-trained language
models achieve unexpectedly high performance on this task. We release our code
to the community for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. (arXiv:2106.02034v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02034</id>
        <link href="http://arxiv.org/abs/2106.02034"/>
        <updated>2021-06-04T01:12:30.337Z</updated>
        <summary type="html"><![CDATA[Attention is sparse in vision transformers. We observe the final prediction
in vision transformers is only based on a subset of most informative tokens,
which is sufficient for accurate image recognition. Based on this observation,
we propose a dynamic token sparsification framework to prune redundant tokens
progressively and dynamically based on the input. Specifically, we devise a
lightweight prediction module to estimate the importance score of each token
given the current features. The module is added to different layers to prune
redundant tokens hierarchically. To optimize the prediction module in an
end-to-end manner, we propose an attention masking strategy to differentiably
prune a token by blocking its interactions with other tokens. Benefiting from
the nature of self-attention, the unstructured sparse tokens are still hardware
friendly, which makes our framework easy to achieve actual speed-up. By
hierarchically pruning 66% of the input tokens, our method greatly reduces
31%~37% FLOPs and improves the throughput by over 40% while the drop of
accuracy is within 0.5% for various vision transformers. Equipped with the
dynamic token sparsification framework, DynamicViT models can achieve very
competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs
and vision transformers on ImageNet. Code is available at
https://github.com/raoyongming/DynamicViT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Graph Reasoning for Neural Network Regularization. (arXiv:2106.01805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01805</id>
        <link href="http://arxiv.org/abs/2106.01805"/>
        <updated>2021-06-04T01:12:30.330Z</updated>
        <summary type="html"><![CDATA[Regularizers helped deep neural networks prevent feature co-adaptations.
Dropout,as a commonly used regularization technique, stochastically disables
neuron ac-tivations during network optimization. However, such complete feature
disposal can affect the feature representation and network understanding.
Toward betterdescriptions of latent representations, we present DropGraph that
learns regularization function by constructing a stand-alone graph from the
backbone features. DropGraph first samples stochastic spatial feature vectors
and then incorporates graph reasoning methods to generate feature map
distortions. This add-on graph regularizes the network during training and can
be completely skipped during inference. We provide intuitions on the linkage
between graph reasoning andDropout with further discussions on how partial
graph reasoning method reduces feature correlations. To this end, we
extensively study the modeling of graphvertex dependencies and the utilization
of the graph for distorting backbone featuremaps. DropGraph was validated on
four tasks with a total of 7 different datasets.The experimental results show
that our method outperforms other state-of-the-art regularizers while leaving
the base model structure unmodified during inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tiange Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hongliang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing More About Questions Can Help: Improving Calibration in Question Answering. (arXiv:2106.01494v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01494</id>
        <link href="http://arxiv.org/abs/2106.01494"/>
        <updated>2021-06-04T01:12:30.324Z</updated>
        <summary type="html"><![CDATA[We study calibration in question answering, estimating whether model
correctly predicts answer for each question. Unlike prior work which mainly
rely on the model's confidence score, our calibrator incorporates information
about the input example (e.g., question and the evidence context). Together
with data augmentation via back translation, our simple approach achieves 5-10%
gains in calibration accuracy on reading comprehension benchmarks. Furthermore,
we present the first calibration study in the open retrieval setting, comparing
the calibration accuracy of retrieval-based span prediction models and answer
generation models. Here again, our approach shows consistent gains over
calibrators relying on the model confidence. Our simple and efficient
calibrator can be easily adapted to many tasks and model architectures, showing
robust gains in all settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shujian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chengyue Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Eunsol Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding. (arXiv:2106.01541v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01541</id>
        <link href="http://arxiv.org/abs/2106.01541"/>
        <updated>2021-06-04T01:12:30.318Z</updated>
        <summary type="html"><![CDATA[Recently, various neural models for multi-party conversation (MPC) have
achieved impressive improvements on a variety of tasks such as addressee
recognition, speaker identification and response prediction. However, these
existing methods on MPC usually represent interlocutors and utterances
individually and ignore the inherent complicated structure in MPC which may
provide crucial interlocutor and utterance semantics and would enhance the
conversation understanding process. To this end, we present MPC-BERT, a
pre-trained model for MPC understanding that considers learning who says what
to whom in a unified model with several elaborated self-supervised tasks.
Particularly, these tasks can be generally categorized into (1) interlocutor
structure modeling including reply-to utterance recognition, identical speaker
searching and pointer consistency distinction, and (2) utterance semantics
modeling including masked shared utterance restoration and shared node
detection. We evaluate MPC-BERT on three downstream tasks including addressee
recognition, speaker identification and response selection. Experimental
results show that MPC-BERT outperforms previous methods by large margins and
achieves new state-of-the-art performance on all three downstream tasks at two
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chongyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhen-Hua Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xiubo Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separated-Spectral-Distribution Estimation Based on Bayesian Inference with Single RGB Camera. (arXiv:2106.01861v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01861</id>
        <link href="http://arxiv.org/abs/2106.01861"/>
        <updated>2021-06-04T01:12:30.299Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel method for separately estimating spectral
distributions from images captured by a typical RGB camera. The proposed method
allows us to separately estimate a spectral distribution of illumination,
reflectance, or camera sensitivity, while recent hyperspectral cameras are
limited to capturing a joint spectral distribution from a scene. In addition,
the use of Bayesian inference makes it possible to take into account prior
information of both spectral distributions and image noise as probability
distributions. As a result, the proposed method can estimate spectral
distributions in a unified way, and it can enhance the robustness of the
estimation against noise, which conventional spectral-distribution estimation
methods cannot. The use of Bayesian inference also enables us to obtain the
confidence of estimation results. In an experiment, the proposed method is
shown not only to outperform conventional estimation methods in terms of RMSE
but also to be robust against noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Luna: Linear Unified Nested Attention. (arXiv:2106.01540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01540</id>
        <link href="http://arxiv.org/abs/2106.01540"/>
        <updated>2021-06-04T01:12:30.293Z</updated>
        <summary type="html"><![CDATA[The quadratic computational and memory complexities of the Transformer's
attention mechanism have limited its scalability for modeling long sequences.
In this paper, we propose Luna, a linear unified nested attention mechanism
that approximates softmax attention with two nested linear attention functions,
yielding only linear (as opposed to quadratic) time and space complexity.
Specifically, with the first attention function, Luna packs the input sequence
into a sequence of fixed length. Then, the packed sequence is unpacked using
the second attention function. As compared to a more traditional attention
mechanism, Luna introduces an additional sequence with a fixed length as input
and an additional corresponding output, which allows Luna to perform attention
operation linearly, while also storing adequate contextual information. We
perform extensive evaluations on three benchmarks of sequence modeling tasks:
long-context sequence modeling, neural machine translation and masked language
modeling for large-scale pretraining. Competitive or even better experimental
results demonstrate both the effectiveness and efficiency of Luna compared to a
variety]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuezhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sinong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chunting Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions. (arXiv:2106.01536v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01536</id>
        <link href="http://arxiv.org/abs/2106.01536"/>
        <updated>2021-06-04T01:12:30.287Z</updated>
        <summary type="html"><![CDATA[Many processes in psychology are complex, such as dyadic interactions between
two interacting partners (e.g. patient-therapist, intimate relationship
partners). Nevertheless, many basic questions about interactions are difficult
to investigate because dyadic processes can be within a person and between
partners, they are based on multimodal aspects of behavior and unfold rapidly.
Current analyses are mainly based on the behavioral coding method, whereby
human coders annotate behavior based on a coding schema. But coding is
labor-intensive, expensive, slow, focuses on few modalities. Current approaches
in psychology use LIWC for analyzing couples' interactions. However, advances
in natural language processing such as BERT could enable the development of
systems to potentially automate behavioral coding, which in turn could
substantially improve psychological research. In this work, we train machine
learning models to automatically predict positive and negative communication
behavioral codes of 368 German-speaking Swiss couples during an 8-minute
conflict interaction on a fine-grained scale (10-seconds sequences) using
linguistic features and paralinguistic features derived with openSMILE. Our
results show that both simpler TF-IDF features as well as more complex BERT
features performed better than LIWC, and that adding paralinguistic features
did not improve the performance. These results suggest it might be time to
consider modern alternatives to LIWC, the de facto linguistic features in
psychology, for prediction tasks in couples research. This work is a further
step towards the automated coding of couples' behavior which could enhance
couple research and therapy, and be utilized for other dyadic interactions as
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biggiogera_J/0/1/0/all/0/1"&gt;Jacopo Biggiogera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1"&gt;George Boateng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1"&gt;Peter Hilpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew Vowels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1"&gt;Guy Bodenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1"&gt;Mona Neysari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nussbeck_F/0/1/0/all/0/1"&gt;Fridtjof Nussbeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1"&gt;Tobias Kowatsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["You made me feel this way": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data. (arXiv:2106.01526v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01526</id>
        <link href="http://arxiv.org/abs/2106.01526"/>
        <updated>2021-06-04T01:12:30.280Z</updated>
        <summary type="html"><![CDATA[How romantic partners interact with each other during a conflict influences
how they feel at the end of the interaction and is predictive of whether the
partners stay together in the long term. Hence understanding the emotions of
each partner is important. Yet current approaches that are used include
self-reports which are burdensome and hence limit the frequency of this data
collection. Automatic emotion prediction could address this challenge. Insights
from psychology research indicate that partners' behaviors influence each
other's emotions in conflict interaction and hence, the behavior of both
partners could be considered to better predict each partner's emotion. However,
it is yet to be investigated how doing so compares to only using each partner's
own behavior in terms of emotion prediction performance. In this work, we used
BERT to extract linguistic features (i.e., what partners said) and openSMILE to
extract paralinguistic features (i.e., how they said it) from a data set of 368
German-speaking Swiss couples (N = 736 individuals) which were videotaped
during an 8-minutes conflict interaction in the laboratory. Based on those
features, we trained machine learning models to predict if partners feel
positive or negative after the conflict interaction. Our results show that
including the behavior of the other partner improves the prediction
performance. Furthermore, for men, considering how their female partners spoke
is most important and for women considering what their male partner said is
most important in getting better prediction performance. This work is a step
towards automatically recognizing each partners' emotion based on the behavior
of both, which would enable a better understanding of couples in research,
therapy, and the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1"&gt;George Boateng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1"&gt;Peter Hilpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1"&gt;Guy Bodenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1"&gt;Mona Neysari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1"&gt;Tobias Kowatsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effort-free Automated Skeletal Abnormality Detection of Rat Fetuses on Whole-body Micro-CT Scans. (arXiv:2106.01830v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01830</id>
        <link href="http://arxiv.org/abs/2106.01830"/>
        <updated>2021-06-04T01:12:30.274Z</updated>
        <summary type="html"><![CDATA[Machine Learning-based fast and quantitative automated screening plays a key
role in analyzing human bones on Computed Tomography (CT) scans. However,
despite the requirement in drug safety assessment, such research is rare on
animal fetus micro-CT scans due to its laborious data collection and
annotation. Therefore, we propose various bone feature engineering techniques
to thoroughly automate the skeletal localization/labeling/abnormality detection
of rat fetuses on whole-body micro-CT scans with minimum effort. Despite
limited training data of 49 fetuses, in skeletal labeling and abnormality
detection, we achieve accuracy of 0.900 and 0.810, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fukuda_A/0/1/0/all/0/1"&gt;Akihiro Fukuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hakamada_K/0/1/0/all/0/1"&gt;Kazumi Hakamada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Fine-Grained Entity Types with Box Embeddings. (arXiv:2101.00345v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00345</id>
        <link href="http://arxiv.org/abs/2101.00345"/>
        <updated>2021-06-04T01:12:30.254Z</updated>
        <summary type="html"><![CDATA[Neural entity typing models typically represent fine-grained entity types as
vectors in a high-dimensional space, but such spaces are not well-suited to
modeling these types' complex interdependencies. We study the ability of box
embeddings, which embed concepts as d-dimensional hyperrectangles, to capture
hierarchies of types even when these relationships are not defined explicitly
in the ontology. Our model represents both types and entity mentions as boxes.
Each mention and its context are fed into a BERT-based model to embed that
mention in our box space; essentially, this model leverages typological clues
present in the surface text to hypothesize a type representation for the
mention. Box containment can then be used to derive both the posterior
probability of a mention exhibiting a given type and the conditional
probability relations between types themselves. We compare our approach with a
vector-based typing model and observe state-of-the-art performance on several
entity typing benchmarks. In addition to competitive typing performance, our
box-based model shows better performance in prediction consistency (predicting
a supertype and a subtype together) and confidence (i.e., calibration),
demonstrating that the box-based model captures the latent type hierarchies
better than the vector-based model does.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1"&gt;Yasumasa Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1"&gt;Michael Boratko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Multilingual Pre-trained Language Model with Byte-level Subwords. (arXiv:2101.09469v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09469</id>
        <link href="http://arxiv.org/abs/2101.09469"/>
        <updated>2021-06-04T01:12:30.225Z</updated>
        <summary type="html"><![CDATA[The pre-trained language models have achieved great successes in various
natural language understanding (NLU) tasks due to its capacity to capture the
deep contextualized information in text by pre-training on large-scale corpora.
One of the fundamental components in pre-trained language models is the
vocabulary, especially for training multilingual models on many different
languages. In the technical report, we present our practices on training
multilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte
Pair Encoding). In the experiment, we adopted the architecture of NEZHA as the
underlying pre-trained language model and the results show that NEZHA trained
with byte-level subwords consistently outperforms Google multilingual BERT and
vanilla NEZHA by a notable margin in several multilingual NLU tasks. We release
the source code of our byte-level vocabulary building tools and the
multilingual pre-trained language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Junqiu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yinpeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution. (arXiv:2106.01518v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01518</id>
        <link href="http://arxiv.org/abs/2106.01518"/>
        <updated>2021-06-04T01:12:30.157Z</updated>
        <summary type="html"><![CDATA[Despite the prominence of neural abstractive summarization models, we know
little about how they actually form summaries and how to understand where their
decisions come from. We propose a two-step method to interpret summarization
model decisions. We first analyze the model's behavior by ablating the full
model to categorize each decoder decision into one of several generation modes:
roughly, is the model behaving like a language model, is it relying heavily on
the input, or is it somewhere in between? After isolating decisions that do
depend on the input, we explore interpreting these decisions using several
different attribution methods. We compare these techniques based on their
ability to select content and reconstruct the model's predicted token from
perturbations of the input, thus revealing whether highlighted attributions are
truly important for the generation of the next token. While this machinery can
be broadly useful even beyond summarization, we specifically demonstrate its
capability to identify phrases the summarization model has memorized and
determine where in the training pipeline this memorization happened, as well as
study complex generation phenomena like sentence fusion on a per-instance
basis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiacheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Efficacy of Summarization Evaluation across Languages. (arXiv:2106.01478v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01478</id>
        <link href="http://arxiv.org/abs/2106.01478"/>
        <updated>2021-06-04T01:12:30.126Z</updated>
        <summary type="html"><![CDATA[While automatic summarization evaluation methods developed for English are
routinely applied to other languages, this is the first attempt to
systematically quantify their panlinguistic efficacy. We take a summarization
corpus for eight different languages, and manually annotate generated summaries
for focus (precision) and coverage (recall). Based on this, we evaluate 19
summarization evaluation metrics, and find that using multilingual BERT within
BERTScore performs well across all languages, at a level above that for
English.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1"&gt;Fajri Koto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1"&gt;Jey Han Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1"&gt;Timothy Baldwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01444</id>
        <link href="http://arxiv.org/abs/2106.01444"/>
        <updated>2021-06-04T01:12:30.120Z</updated>
        <summary type="html"><![CDATA[The open-ended nature of visual captioning makes it a challenging area for
evaluation. The majority of proposed models rely on specialized training to
improve human-correlation, resulting in limited adoption, generalizability, and
explainabilty. We introduce "typicality", a new formulation of evaluation
rooted in information theory, which is uniquely suited for problems lacking a
definite ground truth. Typicality serves as our framework to develop a novel
semantic comparison, SPARCS, as well as referenceless fluency evaluation
metrics. Over the course of our analysis, two separate dimensions of fluency
naturally emerge: style, captured by metric SPURTS, and grammar, captured in
the form of grammatical outlier penalties. Through extensive experiments and
ablation studies on benchmark datasets, we show how these decomposed dimensions
of semantics and fluency provide greater system-level insight into captioner
differences. Our proposed metrics along with their combination, SMURF, achieve
state-of-the-art correlation with human judgment when compared with other
rule-based evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feinglass_J/0/1/0/all/0/1"&gt;Joshua Feinglass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yezhou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Contextual Language Model Adaptation for Speech Recognition. (arXiv:2106.01451v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01451</id>
        <link href="http://arxiv.org/abs/2106.01451"/>
        <updated>2021-06-04T01:12:30.097Z</updated>
        <summary type="html"><![CDATA[Language modeling (LM) for automatic speech recognition (ASR) does not
usually incorporate utterance level contextual information. For some domains
like voice assistants, however, additional context, such as the time at which
an utterance was spoken, provides a rich input signal. We introduce an
attention mechanism for training neural speech recognition language models on
both text and non-linguistic contextual data. When applied to a large
de-identified dataset of utterances collected by a popular voice assistant
platform, our method reduces perplexity by 7.0% relative over a standard LM
that does not incorporate contextual information. When evaluated on utterances
extracted from the long tail of the dataset, our method improves perplexity by
9.0% relative over a standard LM and by over 2.8% relative when compared to a
state-of-the-art model for contextual LM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_R/0/1/0/all/0/1"&gt;Richard Diehl Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotney_S/0/1/0/all/0/1"&gt;Scott Novotney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1"&gt;Ivan Bulyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1"&gt;Ankur Gandhe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Select: A Fully Attentive Approach for Novel Object Captioning. (arXiv:2106.01424v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01424</id>
        <link href="http://arxiv.org/abs/2106.01424"/>
        <updated>2021-06-04T01:12:30.088Z</updated>
        <summary type="html"><![CDATA[Image captioning models have lately shown impressive results when applied to
standard datasets. Switching to real-life scenarios, however, constitutes a
challenge due to the larger variety of visual concepts which are not covered in
existing training sets. For this reason, novel object captioning (NOC) has
recently emerged as a paradigm to test captioning models on objects which are
unseen during the training phase. In this paper, we present a novel approach
for NOC that learns to select the most relevant objects of an image, regardless
of their adherence to the training set, and to constrain the generative process
of a language model accordingly. Our architecture is fully-attentive and
end-to-end trainable, also when incorporating constraints. We perform
experiments on the held-out COCO dataset, where we demonstrate improvements
over the state of the art, both in terms of adaptability to novel objects and
caption quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cagrandi_M/0/1/0/all/0/1"&gt;Marco Cagrandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?. (arXiv:2106.01465v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01465</id>
        <link href="http://arxiv.org/abs/2106.01465"/>
        <updated>2021-06-04T01:12:30.075Z</updated>
        <summary type="html"><![CDATA[Is it possible to use natural language to intervene in a model's behavior and
alter its prediction in a desired way? We investigate the effectiveness of
natural language interventions for reading-comprehension systems, studying this
in the context of social stereotypes. Specifically, we propose a new language
understanding task, Linguistic Ethical Interventions (LEI), where the goal is
to amend a question-answering (QA) model's unethical behavior by communicating
context-specific principles of ethics and equity to it. To this end, we build
upon recent methods for quantifying a system's social stereotypes, augmenting
them with different kinds of ethical interventions and the desired model
behavior under such interventions. Our zero-shot evaluation finds that even
today's powerful neural language models are extremely poor ethical-advice
takers, that is, they respond surprisingly little to ethical interventions even
though these interventions are stated as simple sentences. Few-shot learning
improves model behavior but remains far from the desired outcome, especially
when evaluated for various types of generalization. Our new task thus poses a
novel language understanding challenge for the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1"&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]. (arXiv:2105.15034v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15034</id>
        <link href="http://arxiv.org/abs/2105.15034"/>
        <updated>2021-06-04T01:12:30.066Z</updated>
        <summary type="html"><![CDATA[In their recent paper titled "Large Associative Memory Problem in
Neurobiology and Machine Learning" [arXiv:2008.06996] the authors gave a
biologically plausible microscopic theory from which one can recover many dense
associative memory models discussed in the literature. We show that the layers
of the recent "MLP-mixer" [arXiv:2105.01601] as well as the essentially
equivalent model in [arXiv:2105.02723] are amongst them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Michael Kopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsharp Mask Guided Filtering. (arXiv:2106.01428v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01428</id>
        <link href="http://arxiv.org/abs/2106.01428"/>
        <updated>2021-06-04T01:12:30.059Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is guided image filtering, which emphasizes the
importance of structure transfer during filtering by means of an additional
guidance image. Where classical guided filters transfer structures using
hand-designed functions, recent guided filters have been considerably advanced
through parametric learning of deep networks. The state-of-the-art leverages
deep networks to estimate the two core coefficients of the guided filter. In
this work, we posit that simultaneously estimating both coefficients is
suboptimal, resulting in halo artifacts and structure inconsistencies. Inspired
by unsharp masking, a classical technique for edge enhancement that requires
only a single coefficient, we propose a new and simplified formulation of the
guided filter. Our formulation enjoys a filtering prior from a low-pass filter
and enables explicit structure transfer by estimating a single coefficient.
Based on our proposed formulation, we introduce a successive guided filtering
network, which provides multiple filtering results from a single network,
allowing for a trade-off between accuracy and efficiency. Extensive ablations,
comparisons and analysis show the effectiveness and efficiency of our
formulation and network, resulting in state-of-the-art results across filtering
tasks like upsampling, denoising, and cross-modality filtering. Code is
available at \url{https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zenglin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunlu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1"&gt;Pascal Mettes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G.M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination. (arXiv:2106.01970v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01970</id>
        <link href="http://arxiv.org/abs/2106.01970"/>
        <updated>2021-06-04T01:12:30.038Z</updated>
        <summary type="html"><![CDATA[We address the problem of recovering the shape and spatially-varying
reflectance of an object from posed multi-view images of the object illuminated
by one unknown lighting condition. This enables the rendering of novel views of
the object under arbitrary environment lighting and editing of the object's
material properties. The key to our approach, which we call Neural Radiance
Factorization (NeRFactor), is to distill the volumetric geometry of a Neural
Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object
into a surface representation and then jointly refine the geometry while
solving for the spatially-varying reflectance and the environment lighting.
Specifically, NeRFactor recovers 3D neural fields of surface normals, light
visibility, albedo, and Bidirectional Reflectance Distribution Functions
(BRDFs) without any supervision, using only a re-rendering loss, simple
smoothness priors, and a data-driven BRDF prior learned from real-world BRDF
measurements. By explicitly modeling light visibility, NeRFactor is able to
separate shadows from albedo and synthesize realistic soft or hard shadows
under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D
models for free-viewpoint relighting in this challenging and underconstrained
capture setup for both synthetic and real scenes. Qualitative and quantitative
experiments show that NeRFactor outperforms classic and deep learning-based
state of the art across various tasks. Our code and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1"&gt;Pratul P. Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1"&gt;Boyang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debevec_P/0/1/0/all/0/1"&gt;Paul Debevec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightweight Adapter Tuning for Multilingual Speech Translation. (arXiv:2106.01463v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01463</id>
        <link href="http://arxiv.org/abs/2106.01463"/>
        <updated>2021-06-04T01:12:30.030Z</updated>
        <summary type="html"><![CDATA[Adapter modules were recently introduced as an efficient alternative to
fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters
of a model and injecting lightweight modules between layers, resulting in the
addition of only a small number of task-specific trainable parameters. While
adapter tuning was investigated for multilingual neural machine translation,
this paper proposes a comprehensive analysis of adapters for multilingual
speech translation (ST). Starting from different pre-trained models (a
multilingual ST trained on parallel data or a multilingual BART (mBART) trained
on non-parallel multilingual data), we show that adapters can be used to: (a)
efficiently specialize ST to specific language pairs with a low extra cost in
terms of parameters, and (b) transfer from an automatic speech recognition
(ASR) task and an mBART pre-trained model to a multilingual ST task.
Experiments show that adapter tuning offer competitive results to full
fine-tuning, while being much more parameter-efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hang Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1"&gt;Didier Schwab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Person Image Synthesis with Spatially-Adaptive Warped Normalization. (arXiv:2105.14739v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14739</id>
        <link href="http://arxiv.org/abs/2105.14739"/>
        <updated>2021-06-04T01:12:30.023Z</updated>
        <summary type="html"><![CDATA[Controllable person image generation aims to produce realistic human images
with desirable attributes (e.g., the given pose, cloth textures or hair style).
However, the large spatial misalignment between the source and target images
makes the standard architectures for image-to-image translation not suitable
for this task. Most of the state-of-the-art architectures avoid the alignment
step during the generation, which causes many artifacts, especially for person
images with complex textures. To solve this problem, we introduce a novel
Spatially-Adaptive Warped Normalization (SAWN), which integrates a learned
flow-field to warp modulation parameters. This allows us to align person
spatial-adaptive styles with pose features efficiently. Moreover, we propose a
novel self-training part replacement strategy to refine the pretrained model
for the texture-transfer task, significantly improving the quality of the
generated cloth and the preservation ability of irrelevant regions. Our
experimental results on the widely used DeepFashion dataset demonstrate a
significant improvement of the proposed method over the state-of-the-art
methods on both pose-transfer and texture-transfer tasks. The source code is
available at https://github.com/zhangqianhui/Sawn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jichao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1"&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingjing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Wheat Head Dataset 2021: more diversity to improve the benchmarking of wheat head localization methods. (arXiv:2105.07660v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07660</id>
        <link href="http://arxiv.org/abs/2105.07660"/>
        <updated>2021-06-04T01:12:30.017Z</updated>
        <summary type="html"><![CDATA[The Global Wheat Head Detection (GWHD) dataset was created in 2020 and has
assembled 193,634 labelled wheat heads from 4,700 RGB images acquired from
various acquisition platforms and 7 countries/institutions. With an associated
competition hosted in Kaggle, GWHD has successfully attracted attention from
both the computer vision and agricultural science communities. From this first
experience in 2020, a few avenues for improvements have been identified,
especially from the perspective of data size, head diversity and label
reliability. To address these issues, the 2020 dataset has been reexamined,
relabeled, and augmented by adding 1,722 images from 5 additional countries,
allowing for 81,553 additional wheat heads to be added. We now release a new
version of the Global Wheat Head Detection (GWHD) dataset in 2021, which is
bigger, more diverse, and less noisy than the 2020 version. The GWHD 2021 is
now publicly available at this http URL and a new data challenge
has been organized on AIcrowd to make use of this updated dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1"&gt;Etienne David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serouart_M/0/1/0/all/0/1"&gt;Mario Serouart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1"&gt;Daniel Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madec_S/0/1/0/all/0/1"&gt;Simon Madec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velumani_K/0/1/0/all/0/1"&gt;Kaaviya Velumani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shouyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_F/0/1/0/all/0/1"&gt;Francisco Pinto Espinosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_S/0/1/0/all/0/1"&gt;Shahameh Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_I/0/1/0/all/0/1"&gt;Izzat S. A. Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsujimoto_H/0/1/0/all/0/1"&gt;Hisashi Tsujimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasuda_S/0/1/0/all/0/1"&gt;Shuhei Nasuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bangyou Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kichgessner_N/0/1/0/all/0/1"&gt;Norbert Kichgessner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aasen_H/0/1/0/all/0/1"&gt;Helge Aasen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hund_A/0/1/0/all/0/1"&gt;Andreas Hund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadhegi_Tehran_P/0/1/0/all/0/1"&gt;Pouria Sadhegi-Tehran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagasawa_K/0/1/0/all/0/1"&gt;Koichi Nagasawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishikawa_G/0/1/0/all/0/1"&gt;Goro Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dandrifosse_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Dandrifosse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlier_A/0/1/0/all/0/1"&gt;Alexis Carlier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mercatoris_B/0/1/0/all/0/1"&gt;Benoit Mercatoris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuroki_K/0/1/0/all/0/1"&gt;Ken Kuroki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haozhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1"&gt;Masanori Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badhon_M/0/1/0/all/0/1"&gt;Minhajul A. Badhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pozniak_C/0/1/0/all/0/1"&gt;Curtis Pozniak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1"&gt;David Shaner LeBauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lilimo_M/0/1/0/all/0/1"&gt;Morten Lilimo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poland_J/0/1/0/all/0/1"&gt;Jesse Poland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_S/0/1/0/all/0/1"&gt;Scott Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solan_B/0/1/0/all/0/1"&gt;Benoit de Solan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baret_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Baret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1"&gt;Ian Stavness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Palette: Guiding Scene Generation with Class Proportions. (arXiv:2106.01629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01629</id>
        <link href="http://arxiv.org/abs/2106.01629"/>
        <updated>2021-06-04T01:12:30.009Z</updated>
        <summary type="html"><![CDATA[Despite the recent progress of generative adversarial networks (GANs) at
synthesizing photo-realistic images, producing complex urban scenes remains a
challenging problem. Previous works break down scene generation into two
consecutive phases: unconditional semantic layout synthesis and image synthesis
conditioned on layouts. In this work, we propose to condition layout generation
as well for higher semantic control: given a vector of class proportions, we
generate layouts with matching composition. To this end, we introduce a
conditional framework with novel architecture designs and learning objectives,
which effectively accommodates class proportions to guide the scene generation
process. The proposed architecture also allows partial layout editing with
interesting applications. Thanks to the semantic control, we can produce
layouts close to the real distribution, helping enhance the whole scene
generation process. On different metrics and urban scene benchmarks, our models
outperform existing baselines. Moreover, we demonstrate the merit of our
approach for data augmentation: semantic segmenters trained on real
layout-image pairs along with additional ones generated by our approach
outperform models only trained on real pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1"&gt;Guillaume Le Moing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Tuan-Hung Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1"&gt;Himalaya Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1"&gt;Matthieu Cord&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Based Texture Analysis of Patella from X-Rays for Detecting Patellofemoral Osteoarthritis. (arXiv:2106.01700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01700</id>
        <link href="http://arxiv.org/abs/2106.01700"/>
        <updated>2021-06-04T01:12:29.989Z</updated>
        <summary type="html"><![CDATA[Objective is to assess the ability of texture features for detecting
radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view
radiographs. We used lateral view knee radiographs from MOST public use
datasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically
detected using landmark detection tool (BoneFinder). Hand-crafted features,
based on LocalBinary Patterns (LBP), were then extracted to describe the
patellar texture. First, a machine learning model (Gradient Boosting Machine)
was trained to detect radiographic PFOA from the LBP features. Furthermore, we
used end-to-end trained deep convolutional neural networks (CNNs) directly on
the texture patches for detecting the PFOA. The proposed classification models
were eventually compared with more conventional reference models that use
clinical assessments and participant characteristics such as age, sex, body
mass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL)
grade. Atlas-guided visual assessment of PFOA status by expert readers provided
in the MOST public use datasets was used as a classification outcome for the
models. Performance of prediction models was assessed using the area under the
receiver operating characteristic curve (ROC AUC), the area under the
precision-recall (PR) curve-average precision (AP)-, and Brier score in the
stratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had
PFOA. AUC and AP for the strongest reference model including age, sex, BMI,
WOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487,
respectively. Textural ROI classification using CNN significantly improved the
prediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study
that analyses patellar bone texture for diagnosing PFOA. Our results
demonstrates the potential of using texture features of patella to predict
PFOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bayramoglu_N/0/1/0/all/0/1"&gt;Neslihan Bayramoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nieminen_M/0/1/0/all/0/1"&gt;Miika T. Nieminen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saarakkala_S/0/1/0/all/0/1"&gt;Simo Saarakkala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. (arXiv:2010.11929v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11929</id>
        <link href="http://arxiv.org/abs/2010.11929"/>
        <updated>2021-06-04T01:12:29.982Z</updated>
        <summary type="html"><![CDATA[While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1"&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weissenborn_D/0/1/0/all/0/1"&gt;Dirk Weissenborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1"&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heigold_G/0/1/0/all/0/1"&gt;Georg Heigold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1"&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion. (arXiv:2106.01415v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01415</id>
        <link href="http://arxiv.org/abs/2106.01415"/>
        <updated>2021-06-04T01:12:29.976Z</updated>
        <summary type="html"><![CDATA[We propose a new paradigm for maintaining speaker identity in dysarthric
voice conversion (DVC). The poor quality of dysarthric speech can be greatly
improved by statistical VC, but as the normal speech utterances of a dysarthria
patient are nearly impossible to collect, previous work failed to recover the
individuality of the patient. In light of this, we suggest a novel, two-stage
approach for DVC, which is highly flexible in that no normal speech of the
patient is required. First, a powerful parallel sequence-to-sequence model
converts the input dysarthric speech into a normal speech of a reference
speaker as an intermediate product, and a nonparallel, frame-wise VC model
realized with a variational autoencoder then converts the speaker identity of
the reference speech back to that of the patient while assumed to be capable of
preserving the enhanced quality. We investigate several design options.
Experimental evaluation results demonstrate the potential of our approach to
improving the quality of the dysarthric speech while maintaining the speaker
identity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1"&gt;Kazuhiro Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yu-Huai Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ching-Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotic Inspection and 3D GPR-based Reconstruction for Underground Utilities. (arXiv:2106.01907v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01907</id>
        <link href="http://arxiv.org/abs/2106.01907"/>
        <updated>2021-06-04T01:12:29.969Z</updated>
        <summary type="html"><![CDATA[Ground Penetrating Radar (GPR) is an effective non-destructive evaluation
(NDE) device for inspecting and surveying subsurface objects (i.e., rebars,
utility pipes) in complex environments. However, the current practice for GPR
data collection requires a human inspector to move a GPR cart along pre-marked
grid lines and record the GPR data in both X and Y directions for
post-processing by 3D GPR imaging software. It is time-consuming and tedious
work to survey a large area. Furthermore, identifying the subsurface targets
depends on the knowledge of an experienced engineer, who has to make manual and
subjective interpretation that limits the GPR applications, especially in
large-scale scenarios. In addition, the current GPR imaging technology is not
intuitive, and not for normal users to understand, and not friendly to
visualize. To address the above challenges, this paper presents a novel robotic
system to collect GPR data, interpret GPR data, localize the underground
utilities, reconstruct and visualize the underground objects' dense point cloud
model in a user-friendly manner. This system is composed of three modules: 1) a
vision-aided Omni-directional robotic data collection platform, which enables
the GPR antenna to scan the target area freely with an arbitrary trajectory
while using a visual-inertial-based positioning module tags the GPR
measurements with positioning information; 2) a deep neural network (DNN)
migration module to interpret the raw GPR B-scan image into a cross-section of
object model; 3) a DNN-based 3D reconstruction method, i.e., GPRNet, to
generate underground utility model represented as fine 3D point cloud.
Comparative studies on synthetic and field GPR raw data with various
incompleteness and noise are performed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinglun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Biao_J/0/1/0/all/0/1"&gt;Jiang Biao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jizhong Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided Supervised Contrastive Learning for Semantic Segmentation. (arXiv:2106.01596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01596</id>
        <link href="http://arxiv.org/abs/2106.01596"/>
        <updated>2021-06-04T01:12:29.963Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has shown superior performance in embedding global and
spatial invariant features in computer vision (e.g., image classification).
However, its overall success of embedding local and spatial variant features is
still limited, especially for semantic segmentation. In a per-pixel prediction
task, more than one label can exist in a single image for segmentation (e.g.,
an image contains both cat, dog, and grass), thereby it is difficult to define
'positive' or 'negative' pairs in a canonical contrastive learning setting. In
this paper, we propose an attention-guided supervised contrastive learning
approach to highlight a single semantic object every time as the target. With
our design, the same image can be embedded to different semantic clusters with
semantic attention (i.e., coerce semantic masks) as an additional input
channel. To achieve such attention, a novel two-stage training strategy is
presented. We evaluate the proposed method on multi-organ medical image
segmentation task, as our major task, with both in-house data and BTCV 2015
datasets. Comparing with the supervised and semi-supervised training
state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields
substantial improvement of 5.53% and 6.09% in Dice score for both medical image
segmentation cohorts respectively. The performance of the proposed method on
natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%
substantial improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shunxing Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison for Anti-noise Robustness of Deep Learning Classification Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to Visual Transformer and Performer. (arXiv:2106.01927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01927</id>
        <link href="http://arxiv.org/abs/2106.01927"/>
        <updated>2021-06-04T01:12:29.945Z</updated>
        <summary type="html"><![CDATA[Image classification has achieved unprecedented advance with the the rapid
development of deep learning. However, the classification of tiny object images
is still not well investigated. In this paper, we first briefly review the
development of Convolutional Neural Network and Visual Transformer in deep
learning, and introduce the sources and development of conventional noises and
adversarial attacks. Then we use various models of Convolutional Neural Network
and Visual Transformer to conduct a series of experiments on the image dataset
of tiny objects (sperms and impurities), and compare various evaluation metrics
in the experimental results to obtain a model with stable performance. Finally,
we discuss the problems in the classification of tiny objects and make a
prospect for the classification of tiny objects in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Ao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1"&gt;Shuojia Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. (arXiv:2106.01635v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01635</id>
        <link href="http://arxiv.org/abs/2106.01635"/>
        <updated>2021-06-04T01:12:29.939Z</updated>
        <summary type="html"><![CDATA[In this paper we implement and compare 7 different data augmentation
strategies for the task of automatic scoring of children's ability to
understand others' thoughts, feelings, and desires (or "mindreading").

We recruit in-domain experts to re-annotate augmented samples and determine
to what extent each strategy preserves the original rating. We also carry out
multiple experiments to measure how much each augmentation strategy improves
the performance of automatic scoring systems. To determine the capabilities of
automatic systems to generalize to unseen data, we create UK-MIND-20 - a new
corpus of children's performance on tests of mindreading, consisting of 10,320
question-answer pairs.

We obtain a new state-of-the-art performance on the MIND-CA corpus, improving
macro-F1-score by 6 points. Results indicate that both the number of training
examples and the quality of the augmentation strategies affect the performance
of the systems. The task-specific augmentations generally outperform
task-agnostic augmentations. Automatic augmentations based on vectors (GloVe,
FastText) perform the worst.

We find that systems trained on MIND-CA generalize well to UK-MIND-20. We
demonstrate that data augmentation strategies also improve the performance on
unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1"&gt;Venelin Kovatchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_P/0/1/0/all/0/1"&gt;Phillip Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mark Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devine_R/0/1/0/all/0/1"&gt;Rory Devine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data. (arXiv:2106.01797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01797</id>
        <link href="http://arxiv.org/abs/2106.01797"/>
        <updated>2021-06-04T01:12:29.933Z</updated>
        <summary type="html"><![CDATA[Among ubiquitous multimodal data in the real world, text is the modality
generated by human, while image reflects the physical world honestly. In a
visual understanding application, machines are expected to understand images
like human. Inspired by this, we propose a novel self-supervised learning
method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual
representations by fully utilizing the naturally-existing multimodal data. Our
core idea of self-supervised learning is to maximize the mutual information
between features extracted from multiple views of a shared context to a
rational degree. Different from previous methods which only consider multiple
views from a single modality, our work produces multiple views from different
modalities, and jointly optimizes the mutual information for features pairs of
intra-modality and inter-modality. Considering the information gap between
inter-modality features pairs from data noise, we adopt a \emph{ranking-based}
contrastive learning to optimize the mutual information. During evaluation, we
directly use the pre-trained visual representations to complete various image
classification tasks. Experimental results show that, TVDIM significantly
outperforms previous visual self-supervised methods when processing the same
set of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1"&gt;Pengda Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset and Baselines for Multilingual Reply Suggestion. (arXiv:2106.02017v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02017</id>
        <link href="http://arxiv.org/abs/2106.02017"/>
        <updated>2021-06-04T01:12:29.927Z</updated>
        <summary type="html"><![CDATA[Reply suggestion models help users process emails and chats faster. Previous
work only studies English reply suggestion. Instead, we present MRS, a
multilingual reply suggestion dataset with ten languages. MRS can be used to
compare two families of models: 1) retrieval models that select the reply from
a fixed set and 2) generation models that produce the reply from scratch.
Therefore, MRS complements existing cross-lingual generalization benchmarks
that focus on classification and sequence labeling tasks. We build a generation
model and a retrieval model as baselines for MRS. The two models have different
strengths in the monolingual setting, and they require different strategies to
generalize across languages. MRS is publicly available at
https://github.com/zhangmozhi/mrs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data Using Deep Transfer Learning. (arXiv:2012.03129v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03129</id>
        <link href="http://arxiv.org/abs/2012.03129"/>
        <updated>2021-06-04T01:12:29.920Z</updated>
        <summary type="html"><![CDATA[Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1"&gt;Saeed Khaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lizhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?. (arXiv:2106.01561v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01561</id>
        <link href="http://arxiv.org/abs/2106.01561"/>
        <updated>2021-06-04T01:12:29.914Z</updated>
        <summary type="html"><![CDATA[Recent work has investigated the interesting question using pre-trained
language models (PLMs) as knowledge bases for answering open questions.
However, existing work is limited in using small benchmarks with high
test-train overlaps. We construct a new dataset of closed-book QA using SQuAD,
and investigate the performance of BART. Experiments show that it is
challenging for BART to remember training facts in high precision, and also
challenging to answer closed-book questions even if relevant knowledge is
retained. Some promising directions are found, including decoupling the
knowledge memorizing process and the QA finetune process, forcing the model to
recall relevant knowledge when question answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks. (arXiv:2011.14062v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14062</id>
        <link href="http://arxiv.org/abs/2011.14062"/>
        <updated>2021-06-04T01:12:29.897Z</updated>
        <summary type="html"><![CDATA[Spoken term discovery from untranscribed speech audio could be achieved via a
two-stage process. In the first stage, the unlabelled speech is decoded into a
sequence of subword units that are learned and modelled in an unsupervised
manner. In the second stage, partial sequence matching and clustering are
performed on the decoded subword sequences, resulting in a set of discovered
words or phrases. A limitation of this approach is that the results of subword
decoding could be erroneous, and the errors would impact the subsequent steps.
While Siamese/Triplet network is one approach to learn segment representations
that can improve the discovery process, the challenge in spoken term discovery
under a complete unsupervised scenario is that training examples are
unavailable. In this paper, we propose to generate training examples from
initial hypothesized sequence clusters. The Siamese/Triplet network is trained
on the hypothesized examples to measure the similarity between two speech
segments and hereby perform re-clustering of all hypothesized subword sequences
to achieve spoken term discovery. Experimental results show that the proposed
approach is effective in obtaining training examples for Siamese and Triplet
networks, improving the efficacy of spoken term discovery as compared with the
original two-stage method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sung_M/0/1/0/all/0/1"&gt;Man-Ling Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1"&gt;Tan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APES: Audiovisual Person Search in Untrimmed Video. (arXiv:2106.01667v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01667</id>
        <link href="http://arxiv.org/abs/2106.01667"/>
        <updated>2021-06-04T01:12:29.891Z</updated>
        <summary type="html"><![CDATA[Humans are arguably one of the most important subjects in video streams, many
real-world applications such as video summarization or video editing workflows
often require the automatic search and retrieval of a person of interest.
Despite tremendous efforts in the person reidentification and retrieval
domains, few works have developed audiovisual search strategies. In this paper,
we present the Audiovisual Person Search dataset (APES), a new dataset composed
of untrimmed videos whose audio (voices) and visual (faces) streams are densely
annotated. APES contains over 1.9K identities labeled along 36 hours of video,
making it the largest dataset available for untrimmed audiovisual person
search. A key property of APES is that it includes dense temporal annotations
that link faces to speech segments of the same identity. To showcase the
potential of our new dataset, we propose an audiovisual baseline and benchmark
for person retrieval. Our study shows that modeling audiovisual cues benefits
the recognition of people's identities. To enable reproducibility and promote
future research, the dataset annotations and baseline code are available at:
https://github.com/fuankarion/audiovisual-person-search]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1"&gt;Juan Leon Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1"&gt;Long Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perazzi_F/0/1/0/all/0/1"&gt;Federico Perazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joon-Young Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbelaez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1"&gt;Fabian Caba Heilbron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-tagging of Short Conversational Sentences using Transformer Methods. (arXiv:2106.01735v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01735</id>
        <link href="http://arxiv.org/abs/2106.01735"/>
        <updated>2021-06-04T01:12:29.885Z</updated>
        <summary type="html"><![CDATA[The problem of categorizing short speech sentences according to their
semantic features with high accuracy is a subject studied in natural language
processing. In this study, a data set created with samples classified in 46
different categories was used. Examples consist of sentences taken from chat
conversations between a company's customer representatives and the company's
website visitors. The primary purpose is to automatically tag questions and
requests from visitors in the most accurate way for 46 predetermined categories
for use in a chat application to generate meaningful answers to the questions
asked by the website visitors. For this, different BERT models and one GPT-2
model, pre-trained in Turkish, were preferred. The classification performances
of the relevant models were analyzed in detail and reported accordingly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1"&gt;Umut &amp;#xd6;zdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akca_M/0/1/0/all/0/1"&gt;M. Fatih Akca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmez_O/0/1/0/all/0/1"&gt;O&amp;#x11f;uzhan &amp;#xd6;lmez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulum_S/0/1/0/all/0/1"&gt;Semih G&amp;#xfc;l&amp;#xfc;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutal_S/0/1/0/all/0/1"&gt;Se&amp;#xe7;ilay Kutal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belhan_C/0/1/0/all/0/1"&gt;Ceren Belhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks. (arXiv:2106.01862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01862</id>
        <link href="http://arxiv.org/abs/2106.01862"/>
        <updated>2021-06-04T01:12:29.877Z</updated>
        <summary type="html"><![CDATA[Neuromorphic sensing and computing hold a promise for highly energy-efficient
and high-bandwidth-sensor processing. A major challenge for neuromorphic
computing is that learning algorithms for traditional artificial neural
networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due
to the discrete spikes and more complex neuronal dynamics. As a consequence,
SNNs have not yet been successfully applied to complex, large-scale tasks. In
this article, we focus on the self-supervised learning problem of optical flow
estimation from event-based camera inputs, and investigate the changes that are
necessary to the state-of-the-art ANN training pipeline in order to
successfully tackle it with SNNs. More specifically, we first modify the input
event representation to encode a much smaller time slice with minimal explicit
temporal information. Consequently, we make the network's neuronal dynamics and
recurrent connections responsible for integrating information over time.
Moreover, we reformulate the self-supervised loss function for event-based
optical flow to improve its convexity. We perform experiments with various
types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs,
we investigate the effects of elements such as parameter initialization and
optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We
find that initialization and surrogate gradient width play a crucial part in
enabling learning with sparse inputs, while the inclusion of adaptivity and
learnable neuronal parameters can improve performance. We show that the
performance of the proposed ANNs and SNNs are on par with that of the current
state-of-the-art ANNs trained in a self-supervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1"&gt;Federico Paredes-Vall&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagenaars_J/0/1/0/all/0/1"&gt;Jesse Hagenaars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attacking Text Classifiers via Sentence Rewriting Sampler. (arXiv:2104.08453v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08453</id>
        <link href="http://arxiv.org/abs/2104.08453"/>
        <updated>2021-06-04T01:12:29.860Z</updated>
        <summary type="html"><![CDATA[Most adversarial attack methods on text classification can change the
classifier's prediction by synonym substitution. We propose the adversarial
sentence rewriting sampler (ASRS), which rewrites the whole sentence to
generate more similar and higher-quality adversarial examples. Our method
achieves a better attack success rate on 4 out of 7 datasets, as well as
significantly better sentence quality on all 7 datasets. ASRS is an
indispensable supplement to the existing attack methods, because classifiers
cannot resist the attack from ASRS unless they are trained on adversarial
examples found by ASRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal. (arXiv:2106.01467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01467</id>
        <link href="http://arxiv.org/abs/2106.01467"/>
        <updated>2021-06-04T01:12:29.853Z</updated>
        <summary type="html"><![CDATA[Bringing empathy to a computerized system could significantly improve the
quality of human-computer communications, as soon as machines would be able to
understand customer intentions and better serve their needs. According to
different studies (Literature Review), visual information is one of the most
important channels of human interaction and contains significant behavioral
signals, that may be captured from facial expressions. Therefore, it is
consistent and natural that the research in the field of Facial Expression
Recognition (FER) has acquired increased interest over the past decade due to
having diverse application area including health-care, sociology, psychology,
driver-safety, virtual reality, cognitive sciences, security, entertainment,
marketing, etc. We propose a new architecture for the task of FER and examine
the impact of domain discrimination loss regularization on the learning
process. With regard to observations, including both classical training
conditions and unsupervised domain adaptation scenarios, important aspects of
the considered domain adaptation approach integration are traced. The results
may serve as a foundation for further research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhmetov_K/0/1/0/all/0/1"&gt;Kamil Akhmetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01444</id>
        <link href="http://arxiv.org/abs/2106.01444"/>
        <updated>2021-06-04T01:12:29.846Z</updated>
        <summary type="html"><![CDATA[The open-ended nature of visual captioning makes it a challenging area for
evaluation. The majority of proposed models rely on specialized training to
improve human-correlation, resulting in limited adoption, generalizability, and
explainabilty. We introduce "typicality", a new formulation of evaluation
rooted in information theory, which is uniquely suited for problems lacking a
definite ground truth. Typicality serves as our framework to develop a novel
semantic comparison, SPARCS, as well as referenceless fluency evaluation
metrics. Over the course of our analysis, two separate dimensions of fluency
naturally emerge: style, captured by metric SPURTS, and grammar, captured in
the form of grammatical outlier penalties. Through extensive experiments and
ablation studies on benchmark datasets, we show how these decomposed dimensions
of semantics and fluency provide greater system-level insight into captioner
differences. Our proposed metrics along with their combination, SMURF, achieve
state-of-the-art correlation with human judgment when compared with other
rule-based evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feinglass_J/0/1/0/all/0/1"&gt;Joshua Feinglass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yezhou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.01933</id>
        <link href="http://arxiv.org/abs/2106.01933"/>
        <updated>2021-06-04T01:12:29.832Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1"&gt;David Gaddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks. (arXiv:2106.01452v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01452</id>
        <link href="http://arxiv.org/abs/2106.01452"/>
        <updated>2021-06-04T01:12:29.826Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important blind spots of deep learning systems.
While word- and sentence-level attack scenarios mostly deal with finding
semantic paraphrases of the input that fool NLP models, character-level attacks
typically insert typos into the input stream. It is commonly thought that these
are easier to defend via spelling correction modules. In this work, we show
that both a standard spellchecker and the approach of Pruthi et al. (2019),
which trains to defend against insertions, deletions and swaps, perform poorly
on the character-level benchmark recently proposed in Eger and Benz (2020)
which includes more challenging attacks such as visual and phonetic
perturbations and missing word segmentations. In contrast, we show that an
untrained iterative approach which combines context-independent character-level
information with context-dependent information from BERT's masked language
modeling can perform on par with human crowd-workers from Amazon Mechanical
Turk (AMT) supervised via 3-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yannik Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackensen_J/0/1/0/all/0/1"&gt;Jan Mackensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attacks on Deep Regression Models for Neuroimaging. (arXiv:2105.02866v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02866</id>
        <link href="http://arxiv.org/abs/2105.02866"/>
        <updated>2021-06-04T01:12:29.754Z</updated>
        <summary type="html"><![CDATA[Ensuring the privacy of research participants is vital, even more so in
healthcare environments. Deep learning approaches to neuroimaging require large
datasets, and this often necessitates sharing data between multiple sites,
which is antithetical to the privacy objectives. Federated learning is a
commonly proposed solution to this problem. It circumvents the need for data
sharing by sharing parameters during the training process. However, we
demonstrate that allowing access to parameters may leak private information
even if data is never directly shared. In particular, we show that it is
possible to infer if a sample was used to train the model given only access to
the model prediction (black-box) or access to the model itself (white-box) and
some leaked samples from the training data distribution. Such attacks are
commonly referred to as Membership Inference attacks. We show realistic
Membership Inference attacks on deep learning models trained for 3D
neuroimaging tasks in a centralized as well as decentralized setup. We
demonstrate feasible attacks on brain age prediction models (deep learning
models that predict a person's age from their brain MRI scan). We correctly
identified whether an MRI scan was used in model training with a 60% to over
80% success rate depending on model complexity and security assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Gupta_U/0/1/0/all/0/1"&gt;Umang Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Stripelis_D/0/1/0/all/0/1"&gt;Dimitris Stripelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lam_P/0/1/0/all/0/1"&gt;Pradeep K. Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thompson_P/0/1/0/all/0/1"&gt;Paul M. Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ambite_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Luis Ambite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Steeg_G/0/1/0/all/0/1"&gt;Greg Ver Steeg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CitationIE: Leveraging the Citation Graph for Scientific Information Extraction. (arXiv:2106.01560v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.01560</id>
        <link href="http://arxiv.org/abs/2106.01560"/>
        <updated>2021-06-04T01:12:29.748Z</updated>
        <summary type="html"><![CDATA[Automatically extracting key information from scientific documents has the
potential to help scientists work more efficiently and accelerate the pace of
scientific progress. Prior work has considered extracting document-level entity
clusters and relations end-to-end from raw scientific text, which can improve
literature search and help identify methods and materials for a given problem.
Despite the importance of this task, most existing works on scientific
information extraction (SciIE) consider extraction solely based on the content
of an individual paper, without considering the paper's place in the broader
literature. In contrast to prior work, we augment our text representations by
leveraging a complementary source of document context: the citation graph of
referential links between citing and cited papers. On a test set of
English-language scientific documents, we show that simple ways of utilizing
the structure and content of the citation graph can each lead to significant
gains in different scientific information extraction tasks. When these tasks
are combined, we observe a sizable improvement in end-to-end information
extraction over the state-of-the-art, suggesting the potential for future work
along this direction. We release software tools to facilitate citation-aware
SciIE development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1"&gt;Vijay Viswanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Point or Not to Point: Understanding How Abstractive Summarizers Paraphrase Text. (arXiv:2106.01581v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01581</id>
        <link href="http://arxiv.org/abs/2106.01581"/>
        <updated>2021-06-04T01:12:29.731Z</updated>
        <summary type="html"><![CDATA[Abstractive neural summarization models have seen great improvements in
recent years, as shown by ROUGE scores of the generated summaries. But despite
these improved metrics, there is limited understanding of the strategies
different models employ, and how those strategies relate their understanding of
language. To understand this better, we run several experiments to characterize
how one popular abstractive model, the pointer-generator model of See et al.
(2017), uses its explicit copy/generation switch to control its level of
abstraction (generation) vs extraction (copying). On an extractive-biased
dataset, the model utilizes syntactic boundaries to truncate sentences that are
otherwise often copied verbatim. When we modify the copy/generation switch and
force the model to generate, only simple paraphrasing abilities are revealed
alongside factual inaccuracies and hallucinations. On an abstractive-biased
dataset, the model copies infrequently but shows similarly limited abstractive
abilities. In line with previous research, these results suggest that
abstractive summarization models lack the semantic understanding necessary to
generate paraphrases that are both abstractive and faithful to the source
document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_M/0/1/0/all/0/1"&gt;Matt Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timkey_W/0/1/0/all/0/1"&gt;William Timkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1"&gt;Marten Van Schijndel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support. (arXiv:2106.01702v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01702</id>
        <link href="http://arxiv.org/abs/2106.01702"/>
        <updated>2021-06-04T01:12:29.725Z</updated>
        <summary type="html"><![CDATA[Great research interests have been attracted to devise AI services that are
able to provide mental health support. However, the lack of corpora is a main
obstacle to this research, particularly in Chinese language. In this paper, we
propose PsyQA, a Chinese dataset of psychological health support in the form of
question and answer pair. PsyQA is crawled from a Chinese mental health service
platform, and contains 22K questions and 56K long and well-structured answers.
Based on the psychological counseling theories, we annotate a portion of answer
texts with typical strategies for providing support, and further present
in-depth analysis of both lexical features and strategy patterns in the
counseling answers. We also evaluate the performance of generating counseling
answers with the generative pretrained models. Results show that utilizing
strategies enhances the fluency and helpfulness of generated answers, but there
is still a large space for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhenru Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chujie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last iterate convergence of SGD for Least-Squares in the Interpolation regime. (arXiv:2102.03183v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03183</id>
        <link href="http://arxiv.org/abs/2102.03183"/>
        <updated>2021-06-04T01:12:29.705Z</updated>
        <summary type="html"><![CDATA[Motivated by the recent successes of neural networks that have the ability to
fit the data perfectly and generalize well, we study the noiseless model in the
fundamental least-squares setup. We assume that an optimum predictor fits
perfectly inputs and outputs $\langle \theta_* , \phi(X) \rangle = Y$, where
$\phi(X)$ stands for a possibly infinite dimensional non-linear feature map. To
solve this problem, we consider the estimator given by the last iterate of
stochastic gradient descent (SGD) with constant step-size. In this context, our
contribution is two fold: (i) from a (stochastic) optimization perspective, we
exhibit an archetypal problem where we can show explicitly the convergence of
SGD final iterate for a non-strongly convex problem with constant step-size
whereas usual results use some form of average and (ii) from a statistical
perspective, we give explicit non-asymptotic convergence rates in the
over-parameterized setting and leverage a fine-grained parameterization of the
problem to exhibit polynomial rates that can be faster than $O(1/T)$. The link
with reproducing kernel Hilbert spaces is established.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varre_A/0/1/0/all/0/1"&gt;Aditya Varre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1"&gt;Loucas Pillaud-Vivien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Distributed SVD via Local Power Iterations. (arXiv:2002.08014v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08014</id>
        <link href="http://arxiv.org/abs/2002.08014"/>
        <updated>2021-06-04T01:12:29.690Z</updated>
        <summary type="html"><![CDATA[We study distributed computing of the truncated singular value decomposition
problem. We develop an algorithm that we call \texttt{LocalPower} for improving
communication efficiency. Specifically, we uniformly partition the dataset
among $m$ nodes and alternate between multiple (precisely $p$) local power
iterations and one global aggregation. In the aggregation, we propose to weight
each local eigenvector matrix with orthogonal Procrustes transformation (OPT).
As a practical surrogate of OPT, sign-fixing, which uses a diagonal matrix with
$\pm 1$ entries as weights, has better computation complexity and stability in
experiments. We theoretically show that under certain assumptions
\texttt{LocalPower} lowers the required number of communications by a factor of
$p$ to reach a constant accuracy. We also show that the strategy of
periodically decaying $p$ helps obtain high-precision solutions. We conduct
experiments to demonstrate the effectiveness of \texttt{LocalPower}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shusen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihua Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Domain Adaptation. (arXiv:2106.01656v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01656</id>
        <link href="http://arxiv.org/abs/2106.01656"/>
        <updated>2021-06-04T01:12:29.684Z</updated>
        <summary type="html"><![CDATA[Many variants of unsupervised domain adaptation (UDA) problems have been
proposed and solved individually. Its side effect is that a method that works
for one variant is often ineffective for or not even applicable to another,
which has prevented practical applications. In this paper, we give a general
representation of UDA problems, named Generalized Domain Adaptation (GDA). GDA
covers the major variants as special cases, which allows us to organize them in
a comprehensive framework. Moreover, this generalization leads to a new
challenging setting where existing methods fail, such as when domain labels are
unknown, and class labels are only partially given to each domain. We propose a
novel approach to the new setting. The key to our approach is self-supervised
class-destructive learning, which enables the learning of class-invariant
representations and domain-adversarial classifiers without using any domain
labels. Extensive experiments using three benchmark datasets demonstrate that
our method outperforms the state-of-the-art UDA methods in the new setting and
that it is competitive in existing UDA variations as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitsuzumi_Y/0/1/0/all/0/1"&gt;Yu Mitsuzumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irie_G/0/1/0/all/0/1"&gt;Go Irie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikami_D/0/1/0/all/0/1"&gt;Daiki Ikami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1"&gt;Takashi Shibata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Labels are Treasure: Mean-Teacher-Assisted Confident Learning for Hepatic Vessel Segmentation. (arXiv:2106.01860v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01860</id>
        <link href="http://arxiv.org/abs/2106.01860"/>
        <updated>2021-06-04T01:12:29.677Z</updated>
        <summary type="html"><![CDATA[Manually segmenting the hepatic vessels from Computer Tomography (CT) is far
more expertise-demanding and laborious than other structures due to the
low-contrast and complex morphology of vessels, resulting in the extreme lack
of high-quality labeled data. Without sufficient high-quality annotations, the
usual data-driven learning-based approaches struggle with deficient training.
On the other hand, directly introducing additional data with low-quality
annotations may confuse the network, leading to undesirable performance
degradation. To address this issue, we propose a novel mean-teacher-assisted
confident learning framework to robustly exploit the noisy labeled data for the
challenging hepatic vessel segmentation task. Specifically, with the adapted
confident learning assisted by a third party, i.e., the weight-averaged teacher
model, the noisy labels in the additional low-quality dataset can be
transformed from "encumbrance" to "treasure" via progressive pixel-wise
soft-correction, thus providing productive guidance. Extensive experiments
using two public datasets demonstrate the superiority of the proposed framework
as well as the effectiveness of each component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1"&gt;Donghuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jagadeesan_J/0/1/0/all/0/1"&gt;Jayender Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Stochastic Moving-Average Estimators for Non-Convex Optimization. (arXiv:2104.14840v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14840</id>
        <link href="http://arxiv.org/abs/2104.14840"/>
        <updated>2021-06-04T01:12:29.670Z</updated>
        <summary type="html"><![CDATA[In this paper, we demonstrate the power of a widely used stochastic estimator
based on moving average (SEMA) on a range of stochastic non-convex optimization
problems, which only requires {\bf a general unbiased stochastic oracle}. We
analyze various stochastic methods (existing or newly proposed) based on the
{\bf variance recursion property} of SEMA for three families of non-convex
optimization, namely standard stochastic non-convex minimization, stochastic
non-convex strongly-concave min-max optimization, and stochastic bilevel
optimization. Our contributions include: (i) for standard stochastic non-convex
minimization, we present a simple and intuitive proof of convergence for a
family Adam-style methods (including Adam) with an increasing or large
"momentum" parameter for the first-order moment, which gives an alternative yet
more natural way to guarantee Adam converge; (ii) for stochastic non-convex
strongly-concave min-max optimization, we present a single-loop stochastic
gradient descent ascent method based on the moving average estimators and
establish its oracle complexity of $O(1/\epsilon^4)$ without using a large
mini-batch size, addressing a gap in the literature; (iii) for stochastic
bilevel optimization, we present a single-loop stochastic method based on the
moving average estimators and establish its oracle complexity of $\widetilde
O(1/\epsilon^4)$ without computing the inverse or SVD of the Hessian matrix,
improving state-of-the-art results. For all these problems, we also establish a
variance diminishing result for the used stochastic gradient estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhishuai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. (arXiv:2010.11929v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11929</id>
        <link href="http://arxiv.org/abs/2010.11929"/>
        <updated>2021-06-04T01:12:29.662Z</updated>
        <summary type="html"><![CDATA[While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1"&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weissenborn_D/0/1/0/all/0/1"&gt;Dirk Weissenborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1"&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heigold_G/0/1/0/all/0/1"&gt;Georg Heigold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1"&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning. (arXiv:2011.00382v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00382</id>
        <link href="http://arxiv.org/abs/2011.00382"/>
        <updated>2021-06-04T01:12:29.656Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge in multiagent reinforcement learning is to learn
beneficial behaviors in a shared environment with other simultaneously learning
agents. In particular, each agent perceives the environment as effectively
non-stationary due to the changing policies of other agents. Moreover, each
agent is itself constantly learning, leading to natural non-stationarity in the
distribution of experiences encountered. In this paper, we propose a novel
meta-multiagent policy gradient theorem that directly accounts for the
non-stationary policy dynamics inherent to multiagent learning settings. This
is achieved by modeling our gradient updates to consider both an agent's own
non-stationary policy dynamics and the non-stationary policy dynamics of other
agents in the environment. We show that our theoretically grounded approach
provides a general solution to the multiagent learning problem, which
inherently comprises all key aspects of previous state of the art approaches on
this topic. We test our method on a diverse suite of multiagent benchmarks and
demonstrate a more efficient ability to adapt to new agents as they learn than
baseline methods across the full spectrum of mixed incentive, competitive, and
cooperative domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1"&gt;Matthew Riemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chuangchuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1"&gt;Marwa Abdulhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habibi_G/0/1/0/all/0/1"&gt;Golnaz Habibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Cot_S/0/1/0/all/0/1"&gt;Sebastian Lopez-Cot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1"&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1"&gt;Jonathan P. How&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep neural network approximation of analytic functions. (arXiv:2104.02095v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02095</id>
        <link href="http://arxiv.org/abs/2104.02095"/>
        <updated>2021-06-04T01:12:29.648Z</updated>
        <summary type="html"><![CDATA[We provide an entropy bound for the spaces of neural networks with piecewise
linear activation functions, such as the ReLU and the absolute value functions.
This bound generalizes the known entropy bound for the space of linear
functions on $\mathbb{R}^d$ and it depends on the value at the point
$(1,1,...,1)$ of the networks obtained by taking the absolute values of all
parameters of original networks. Keeping this value together with the depth,
width and the parameters of the networks to have logarithmic dependence on
$1/\varepsilon$, we $\varepsilon$-approximate functions that are analytic on
certain regions of $\mathbb{C}^d$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards urban scenes understanding through polarization cues. (arXiv:2106.01717v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01717</id>
        <link href="http://arxiv.org/abs/2106.01717"/>
        <updated>2021-06-04T01:12:29.642Z</updated>
        <summary type="html"><![CDATA[Autonomous robotics is critically affected by the robustness of its scene
understanding algorithms. We propose a two-axis pipeline based on polarization
indices to analyze dynamic urban scenes. As robots evolve in unknown
environments, they are prone to encountering specular obstacles. Usually,
specular phenomena are rarely taken into account by algorithms which causes
misinterpretations and erroneous estimates. By exploiting all the light
properties, systems can greatly increase their robustness to events. In
addition to the conventional photometric characteristics, we propose to include
polarization sensing.

We demonstrate in this paper that the contribution of polarization
measurement increases both the performances of segmentation and the quality of
depth estimation. Our polarimetry-based approaches are compared here with other
state-of-the-art RGB-centric methods showing interest of using polarization
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanchon_M/0/1/0/all/0/1"&gt;Marc Blanchon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidibe_D/0/1/0/all/0/1"&gt;D&amp;#xe9;sir&amp;#xe9; Sidib&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morel_O/0/1/0/all/0/1"&gt;Olivier Morel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seulin_R/0/1/0/all/0/1"&gt;Ralph Seulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1"&gt;Fabrice Meriaudeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01981</id>
        <link href="http://arxiv.org/abs/2106.01981"/>
        <updated>2021-06-04T01:12:29.623Z</updated>
        <summary type="html"><![CDATA[Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1"&gt;Boris N. Oreshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1"&gt;Florent Bocquelet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1"&gt;F&amp;#xe9;lix H. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1"&gt;Bay Raitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1"&gt;Dominic Laflamme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOFA: Modular Factorial Design for Hyperparameter Optimization. (arXiv:2011.09545v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09545</id>
        <link href="http://arxiv.org/abs/2011.09545"/>
        <updated>2021-06-04T01:12:29.611Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel and lightweight hyperparameter optimization (HPO)
method, MOdular FActorial Design (MOFA). MOFA pursues several rounds of HPO,
where each round alternates between exploration of hyperparameter space by
factorial design and exploitation of evaluation results by factorial analysis.
Each round first explores the configuration space by constructing a
low-discrepancy set of hyperparameters that cover this space well while
de-correlating hyperparameters, and then exploits evaluation results through
factorial analysis that determines which hyperparameters should be further
explored and which should become fixed in the next round. We prove that the
inference of MOFA achieves higher confidence than other sampling schemes. Each
individual round is highly parallelizable and hence offers major improvements
of efficiency compared to model-based methods. Empirical results show that MOFA
achieves better effectiveness and efficiency compared with state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1"&gt;Bo Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yimin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hanrong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1"&gt;Steffen Staab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electrocardiogram synthesis. (arXiv:2103.00006v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00006</id>
        <link href="http://arxiv.org/abs/2103.00006"/>
        <updated>2021-06-04T01:12:29.603Z</updated>
        <summary type="html"><![CDATA[The electrocardiogram (ECG) records electrical signals in a non-invasive way
to observe the condition of the heart, typically looking at the heart from 12
different directions. Several types of the cardiac disease are diagnosed by
using 12-lead ECGs Recently, various wearable devices have enabled immediate
access to the ECG without the use of wieldy equipment. However, they only
provide ECGs with a couple of leads. This results in an inaccurate diagnosis of
cardiac disease due to lacking of required leads. We propose a deep generative
model for ECG synthesis from two asynchronous leads to ten leads. It first
represents a heart condition referring to two leads, and then generates ten
leads based on the represented heart condition. Both the rhythm and amplitude
of leads generated resemble those of the original ones, while the technique
removes noise and the baseline wander appearing in the original leads. As a
data augmentation method, our model improves the classification performance of
models compared with models using ECGs with only one or two leads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jo_Y/0/1/0/all/0/1"&gt;Yong-Yeon Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kwon_J/0/1/0/all/0/1"&gt;Joon-Myoung Kwon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Feature Aggregation by Cross-Scale Pixel-to-Region Relation Operation for Semantic Segmentation. (arXiv:2106.01744v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01744</id>
        <link href="http://arxiv.org/abs/2106.01744"/>
        <updated>2021-06-04T01:12:29.579Z</updated>
        <summary type="html"><![CDATA[Exploiting multi-scale features has shown great potential in tackling
semantic segmentation problems. The aggregation is commonly done with sum or
concatenation (concat) followed by convolutional (conv) layers. However, it
fully passes down the high-level context to the following hierarchy without
considering their interrelation. In this work, we aim to enable the low-level
feature to aggregate the complementary context from adjacent high-level feature
maps by a cross-scale pixel-to-region relation operation. We leverage
cross-scale context propagation to make the long-range dependency capturable
even by the high-resolution low-level features. To this end, we employ an
efficient feature pyramid network to obtain multi-scale features. We propose a
Relational Semantics Extractor (RSE) and Relational Semantics Propagator (RSP)
for context extraction and propagation respectively. Then we stack several RSP
into an RSP head to achieve the progressive top-down distribution of the
context. Experiment results on two challenging datasets Cityscapes and COCO
demonstrate that the RSP head performs competitively on both semantic
segmentation and panoptic segmentation with high efficiency. It outperforms
DeeplabV3 [1] by 0.7% with 75% fewer FLOPs (multiply-adds) in the semantic
segmentation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yechao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Lyuyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongliang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H. Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints. (arXiv:2102.12827v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12827</id>
        <link href="http://arxiv.org/abs/2102.12827"/>
        <updated>2021-06-04T01:12:29.562Z</updated>
        <summary type="html"><![CDATA[Evaluating adversarial robustness amounts to finding the minimum perturbation
needed to have an input sample misclassified. The inherent complexity of the
underlying optimization requires current gradient-based attacks to be carefully
tuned, initialized, and possibly executed for many computationally-demanding
iterations, even if specialized to a given perturbation model. In this work, we
overcome these limitations by proposing a fast minimum-norm (FMN) attack that
works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$),
is robust to hyperparameter choices, does not require adversarial starting
points, and converges within few lightweight steps. It works by iteratively
finding the sample misclassified with maximum confidence within an
$\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to
minimize the distance of the current sample to the decision boundary. Extensive
experiments show that FMN significantly outperforms existing attacks in terms
of convergence speed and computation time, while reporting comparable or even
smaller perturbation sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathology-Aware Generative Adversarial Networks for Medical Image Augmentation. (arXiv:2106.01915v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01915</id>
        <link href="http://arxiv.org/abs/2106.01915"/>
        <updated>2021-06-04T01:12:29.546Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) can play a key role in Medical Image
Analysis under large-scale annotated datasets. However, preparing such massive
dataset is demanding. In this context, Generative Adversarial Networks (GANs)
can generate realistic but novel samples, and thus effectively cover the real
image distribution. In terms of interpolation, the GAN-based medical image
augmentation is reliable because medical modalities can display the human
body's strong anatomical consistency at fixed position while clearly reflecting
inter-subject variability; thus, we propose to use noise-to-image GANs (e.g.,
random noise samples to diverse pathological images) for (i) medical Data
Augmentation (DA) and (ii) physician training. Regarding the DA, the
GAN-generated images can improve Computer-Aided Diagnosis based on supervised
learning. For the physician training, the GANs can display novel desired
pathological images and help train medical trainees despite
infrastructural/legal constraints. This thesis contains four GAN projects
aiming to present such novel applications' clinical relevance in collaboration
with physicians. Whereas the methods are more generally applicable, this thesis
only explores a few oncological applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement Prediction. (arXiv:2106.01920v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.01920</id>
        <link href="http://arxiv.org/abs/2106.01920"/>
        <updated>2021-06-04T01:12:29.530Z</updated>
        <summary type="html"><![CDATA[With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1"&gt;Kunal Bhardwaj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Truncated Log-concave Sampling with Reflective Hamiltonian Monte Carlo. (arXiv:2102.13068v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13068</id>
        <link href="http://arxiv.org/abs/2102.13068"/>
        <updated>2021-06-04T01:12:29.524Z</updated>
        <summary type="html"><![CDATA[We introduce Reflective Hamiltonian Monte Carlo (ReHMC), an HMC-based
algorithm, to sample from a log-concave distribution restricted to a convex
body. We prove that, starting from a warm start, the walk mixes to a
log-concave target distribution $\pi(x) \propto e^{-f(x)}$, where $f$ is
$L$-smooth and $m$-strongly-convex, within accuracy $\varepsilon$ after
$\widetilde O(\kappa d^2 \ell^2 \log (1 / \varepsilon))$ steps for a
well-rounded convex body where $\kappa = L / m$ is the condition number of the
negative log-density, $d$ is the dimension, $\ell$ is an upper bound on the
number of reflections, and $\varepsilon$ is the accuracy parameter. We also
developed an efficient open source implementation of ReHMC and we performed an
experimental study on various high-dimensional data-sets. The experiments
suggest that ReHMC outperfroms Hit-and-Run and Coordinate-Hit-and-Run regarding
the time it needs to produce an independent sample and introduces practical
truncated sampling in thousands of dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chalkis_A/0/1/0/all/0/1"&gt;Apostolos Chalkis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisikopoulos_V/0/1/0/all/0/1"&gt;Vissarion Fisikopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1"&gt;Marios Papachristou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsigaridas_E/0/1/0/all/0/1"&gt;Elias Tsigaridas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning an Inference Algorithm for Probabilistic Programs. (arXiv:2103.00737v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00737</id>
        <link href="http://arxiv.org/abs/2103.00737"/>
        <updated>2021-06-04T01:12:29.505Z</updated>
        <summary type="html"><![CDATA[We present a meta-algorithm for learning a posterior-inference algorithm for
restricted probabilistic programs. Our meta-algorithm takes a training set of
probabilistic programs that describe models with observations, and attempts to
learn an efficient method for inferring the posterior of a similar program. A
key feature of our approach is the use of what we call a white-box inference
algorithm that extracts information directly from model descriptions
themselves, given as programs. Concretely, our white-box inference algorithm is
equipped with multiple neural networks, one for each type of atomic command,
and computes an approximate posterior of a given probabilistic program by
analysing individual atomic commands in the program using these networks. The
parameters of these networks are then learnt from a training set by our
meta-algorithm. We empirically demonstrate that the learnt inference algorithm
generalises well to unseen programs in terms of both interpolation and
extrapolation, and report cases where our approach may be preferable to a
state-of-the-art inference algorithm such as HMC. The overall results show the
promise as well as remaining challenges of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_G/0/1/0/all/0/1"&gt;Gwonsoo Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongseok Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Sparse Sampling for Dense Reaction Predictions. (arXiv:2106.01764v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01764</id>
        <link href="http://arxiv.org/abs/2106.01764"/>
        <updated>2021-06-04T01:12:29.499Z</updated>
        <summary type="html"><![CDATA[Obtaining viewer responses from videos can be useful for creators and
streaming platforms to analyze the video performance and improve the future
user experience. In this report, we present our method for 2021 Evoked
Expression from Videos Challenge. In particular, our model utilizes both audio
and image modalities as inputs to predict emotion changes of viewers. To model
long-range emotion changes, we use a GRU-based model to predict one sparse
signal with 1Hz. We observe that the emotion changes are smooth. Therefore, the
final dense prediction is obtained via linear interpolating the signal, which
is robust to the prediction fluctuation. Albeit simple, the proposed method has
achieved pearson's correlation score of 0.04430 on the final private test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kezhou Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in Classifying the Stages of Diabetic Retinopathy Using Convolutional Neural Networks in Low Memory Edge Devices. (arXiv:2106.01739v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01739</id>
        <link href="http://arxiv.org/abs/2106.01739"/>
        <updated>2021-06-04T01:12:29.492Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is a severe complication that may lead to retinal
vascular damage and is one of the leading causes of vision impairment and
blindness. DR broadly is classified into two stages - non-proliferative (NPDR),
where there are almost no symptoms, except a few microaneurysms, and
proliferative (PDR) involving a huge number of microaneurysms and hemorrhages,
soft and hard exudates, neo-vascularization, macular ischemia or a combination
of these, making it easier to detect. More specifically, DR is usually
classified into five levels, labeled 0-4, from 0 indicating no DR to 4 which is
most severe. This paper firstly presents a discussion on the risk factors of
the disease, then surveys the recent literature on the topic followed by
examining certain techniques which were found to be highly effective in
improving the prognosis accuracy. Finally, a convolutional neural network model
is proposed to detect all the stages of DR on a low-memory edge
microcontroller. The model has a size of just 5.9 MB, accuracy and F1 score
both of 94% and an inference speed of about 20 frames per second.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection. (arXiv:2012.15761v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15761</id>
        <link href="http://arxiv.org/abs/2012.15761"/>
        <updated>2021-06-04T01:12:29.486Z</updated>
        <summary type="html"><![CDATA[We present a human-and-model-in-the-loop process for dynamically generating
datasets and training better performing and more robust hate detection models.
We provide a new dataset of ~40,000 entries, generated and labelled by trained
annotators over four rounds of dynamic data creation. It includes ~15,000
challenging perturbations and each hateful entry has fine-grained labels for
the type and target of hate. Hateful entries make up 54% of the dataset, which
is substantially higher than comparable datasets. We show that model
performance is substantially improved using this approach. Models trained on
later rounds of data collection perform better on test sets and are harder for
annotators to trick. They also perform better on HateCheck, a suite of
functional tests for online hate detection. We provide the code, dataset and
annotation guidelines for other researchers to use. Accepted at ACL 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1"&gt;Bertie Vidgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1"&gt;Tristan Thrush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Never Cluster Alone. (arXiv:2106.01908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01908</id>
        <link href="http://arxiv.org/abs/2106.01908"/>
        <updated>2021-06-04T01:12:29.480Z</updated>
        <summary type="html"><![CDATA[Recent advances in self-supervised learning with instance-level contrastive
objectives facilitate unsupervised clustering. However, a standalone datum is
not perceiving the context of the holistic cluster, and may undergo sub-optimal
assignment. In this paper, we extend the mainstream contrastive learning
paradigm to a cluster-level scheme, where all the data subjected to the same
cluster contribute to a unified representation that encodes the context of each
data group. Contrastive learning with this representation then rewards the
assignment of each datum. To implement this vision, we propose twin-contrast
clustering (TCC). We define a set of categorical variables as clustering
assignment confidence, which links the instance-level learning track with the
cluster-level one. On one hand, with the corresponding assignment variables
being the weight, a weighted aggregation along the data points implements the
set representation of a cluster. We further propose heuristic cluster
augmentation equivalents to enable cluster-level contrastive learning. On the
other hand, we derive the evidence lower-bound of the instance-level
contrastive objective with the assignments. By reparametrizing the assignment
variables, TCC is trained end-to-end, requiring no alternating steps. Extensive
experiments show that TCC outperforms the state-of-the-art on challenging
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Ziyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Menghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Self-Supervised Representation Ensembles for COVID-19 Cough Classification. (arXiv:2105.07566v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07566</id>
        <link href="http://arxiv.org/abs/2105.07566"/>
        <updated>2021-06-04T01:12:29.473Z</updated>
        <summary type="html"><![CDATA[The usage of smartphone-collected respiratory sound, trained with deep
learning models, for detecting and classifying COVID-19 becomes popular
recently. It removes the need for in-person testing procedures especially for
rural regions where related medical supplies, experienced workers, and
equipment are limited. However, existing sound-based diagnostic approaches are
trained in a fully supervised manner, which requires large scale well-labelled
data. It is critical to discover new methods to leverage unlabelled respiratory
data, which can be obtained more easily. In this paper, we propose a novel
self-supervised learning enabled framework for COVID-19 cough classification. A
contrastive pre-training phase is introduced to train a Transformer-based
feature encoder with unlabelled data. Specifically, we design a random masking
mechanism to learn robust representations of respiratory sounds. The
pre-trained feature encoder is then fine-tuned in the downstream phase to
perform cough classification. In addition, different ensembles with varied
random masking rates are also explored in the downstream phase. Through
extensive evaluations, we demonstrate that the proposed contrastive
pre-training, the random masking mechanism, and the ensemble architecture
contribute to improving cough classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially Adaptive Normalization for Single Domain Generalization. (arXiv:2106.01899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01899</id>
        <link href="http://arxiv.org/abs/2106.01899"/>
        <updated>2021-06-04T01:12:29.455Z</updated>
        <summary type="html"><![CDATA[Single domain generalization aims to learn a model that performs well on many
unseen domains with only one domain data for training. Existing works focus on
studying the adversarial domain augmentation (ADA) to improve the model's
generalization capability. The impact on domain generalization of the
statistics of normalization layers is still underinvestigated. In this paper,
we propose a generic normalization approach, adaptive standardization and
rescaling normalization (ASR-Norm), to complement the missing part in previous
works. ASR-Norm learns both the standardization and rescaling statistics via
neural networks. This new form of normalization can be viewed as a generic form
of the traditional normalizations. When trained with ADA, the statistics in
ASR-Norm are learned to be adaptive to the data coming from different domains,
and hence improves the model generalization performance across domains,
especially on the target domain with large discrepancy from the source domain.
The experimental results show that ASR-Norm can bring consistent improvement to
the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the
Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the
improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xinjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qifei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1"&gt;Junjie Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Fine-Grained Entity Types with Box Embeddings. (arXiv:2101.00345v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00345</id>
        <link href="http://arxiv.org/abs/2101.00345"/>
        <updated>2021-06-04T01:12:29.449Z</updated>
        <summary type="html"><![CDATA[Neural entity typing models typically represent fine-grained entity types as
vectors in a high-dimensional space, but such spaces are not well-suited to
modeling these types' complex interdependencies. We study the ability of box
embeddings, which embed concepts as d-dimensional hyperrectangles, to capture
hierarchies of types even when these relationships are not defined explicitly
in the ontology. Our model represents both types and entity mentions as boxes.
Each mention and its context are fed into a BERT-based model to embed that
mention in our box space; essentially, this model leverages typological clues
present in the surface text to hypothesize a type representation for the
mention. Box containment can then be used to derive both the posterior
probability of a mention exhibiting a given type and the conditional
probability relations between types themselves. We compare our approach with a
vector-based typing model and observe state-of-the-art performance on several
entity typing benchmarks. In addition to competitive typing performance, our
box-based model shows better performance in prediction consistency (predicting
a supertype and a subtype together) and confidence (i.e., calibration),
demonstrating that the box-based model captures the latent type hierarchies
better than the vector-based model does.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1"&gt;Yasumasa Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1"&gt;Michael Boratko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13962</id>
        <link href="http://arxiv.org/abs/2012.13962"/>
        <updated>2021-06-04T01:12:29.442Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
interdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1"&gt;Felix Leibfried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutordoir_V/0/1/0/all/0/1"&gt;Vincent Dutordoir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1"&gt;ST John&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrande_N/0/1/0/all/0/1"&gt;Nicolas Durrande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RetCL: A Selection-based Approach for Retrosynthesis via Contrastive Learning. (arXiv:2105.00795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00795</id>
        <link href="http://arxiv.org/abs/2105.00795"/>
        <updated>2021-06-04T01:12:29.434Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis, of which the goal is to find a set of reactants for
synthesizing a target product, is an emerging research area of deep learning.
While the existing approaches have shown promising results, they currently lack
the ability to consider availability (e.g., stability or purchasability) of the
reactants or generalize to unseen reaction templates (i.e., chemical reaction
rules). In this paper, we propose a new approach that mitigates the issues by
reformulating retrosynthesis into a selection problem of reactants from a
candidate set of commercially available molecules. To this end, we design an
efficient reactant selection framework, named RetCL (retrosynthesis via
contrastive learning), for enumerating all of the candidate molecules based on
selection scores computed by graph neural networks. For learning the score
functions, we also propose a novel contrastive training scheme with hard
negative mining. Extensive experiments demonstrate the benefits of the proposed
selection-based approach. For example, when all 671k reactants in the USPTO
{database} are given as candidates, our RetCL achieves top-1 exact match
accuracy of $71.3\%$ for the USPTO-50k benchmark, while a recent
transformer-based approach achieves $59.6\%$. We also demonstrate that RetCL
generalizes well to unseen templates in various settings in contrast to
template-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hankook Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Seung-Woo Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;You Young Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung-Ju Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UnitedQA: A Hybrid Approach for Open Domain Question Answering. (arXiv:2101.00178v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00178</id>
        <link href="http://arxiv.org/abs/2101.00178"/>
        <updated>2021-06-04T01:12:29.427Z</updated>
        <summary type="html"><![CDATA[To date, most of recent work under the retrieval-reader framework for
open-domain QA focuses on either extractive or generative reader exclusively.
In this paper, we study a hybrid approach for leveraging the strengths of both
models. We apply novel techniques to enhance both extractive and generative
readers built upon recent pretrained neural language models, and find that
proper training methods can provide large improvement over previous
state-of-the-art models. We demonstrate that a simple hybrid approach by
combining answers from both readers can efficiently take advantages of
extractive and generative answer inference strategies and outperforms single
models as well as homogeneous ensembles. Our approach outperforms previous
state-of-the-art models by 3.3 and 2.7 points in exact match on
NaturalQuestions and TriviaQA respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Robust Motion Planning with Guaranteed Stability: A Contraction Theory Approach. (arXiv:2102.12668v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12668</id>
        <link href="http://arxiv.org/abs/2102.12668"/>
        <updated>2021-06-04T01:12:29.406Z</updated>
        <summary type="html"><![CDATA[This paper presents Learning-based Autonomous Guidance with RObustness and
Stability guarantees (LAG-ROS), which provides machine learning-based nonlinear
motion planners with formal robustness and stability guarantees, by designing a
differential Lyapunov function using contraction theory. LAG-ROS utilizes a
neural network to model a robust tracking controller independently of a target
trajectory, for which we show that the Euclidean distance between the target
and controlled trajectories is exponentially bounded linearly in the learning
error, even under the existence of bounded external disturbances. We also
present a convex optimization approach that minimizes the steady-state bound of
the tracking error to construct the robust control law for neural network
training. In numerical simulations, it is demonstrated that the proposed method
indeed possesses superior properties of robustness and nonlinear stability
resulting from contraction theory, whilst retaining the computational
efficiency of existing learning-based motion planners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1"&gt;Hiroyasu Tsukamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soon-Jo Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes. (arXiv:2106.01577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01577</id>
        <link href="http://arxiv.org/abs/2106.01577"/>
        <updated>2021-06-04T01:12:29.399Z</updated>
        <summary type="html"><![CDATA[This paper presents the first {\em model-free}, {\em simulator-free}
reinforcement learning algorithm for Constrained Markov Decision Processes
(CMDPs) with sublinear regret and zero constraint violation. The algorithm is
named Triple-Q because it has three key components: a Q-function (also called
action-value function) for the cumulative reward, a Q-function for the
cumulative utility for the constraint, and a virtual-Queue that
(over)-estimates the cumulative constraint violation. Under Triple-Q, at each
step, an action is chosen based on the pseudo-Q-value that is a combination of
the three Q values. The algorithm updates the reward and utility Q-values with
learning rates that depend on the visit counts to the corresponding (state,
action) pairs and are periodically reset. In the episodic CMDP setting,
Triple-Q achieves $\tilde{\cal O}\left(\frac{1 }{\delta}H^4
S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$ regret, where $K$ is the
total number of episodes, $H$ is the number of steps in each episode, $S$ is
the number of states, $A$ is the number of actions, and $\delta$ is Slater's
constant. Furthermore, Triple-Q guarantees zero constraint violation when $K$
is sufficiently large. Finally, the computational complexity of Triple-Q is
similar to SARSA for unconstrained MDPs and is computationally efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Honghao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lei Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain. (arXiv:2106.01491v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01491</id>
        <link href="http://arxiv.org/abs/2106.01491"/>
        <updated>2021-06-04T01:12:29.393Z</updated>
        <summary type="html"><![CDATA[Crowdworker-constructed natural language inference (NLI) datasets have been
found to contain statistical artifacts associated with the annotation process
that allow hypothesis-only classifiers to achieve better-than-random
performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).
We investigate whether MedNLI, a physician-annotated dataset with premises
extracted from clinical notes, contains such artifacts (Romanov and Shivade,
2018). We find that entailed hypotheses contain generic versions of specific
concepts in the premise, as well as modifiers related to responsiveness,
duration, and probability. Neutral hypotheses feature conditions and behaviors
that co-occur with, or cause, the condition(s) in the premise. Contradiction
hypotheses feature explicit negation of the premise and implicit negation via
assertion of good health. Adversarial filtering demonstrates that performance
degrades when evaluated on the difficult subset. We provide partition
information and recommendations for alternative dataset construction strategies
for knowledge-intensive domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1"&gt;Christine Herlihy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1"&gt;Rachel Rudinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choose a Transformer: Fourier or Galerkin. (arXiv:2105.14995v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14995</id>
        <link href="http://arxiv.org/abs/2105.14995"/>
        <updated>2021-06-04T01:12:29.386Z</updated>
        <summary type="html"><![CDATA[In this paper, we apply the self-attention from the state-of-the-art
Transformer in Attention Is All You Need the first time to a data-driven
operator learning problem related to partial differential equations. We put
together an effort to explain the heuristics of, and improve the efficacy of
the self-attention by demonstrating that the softmax normalization in the
scaled dot-product attention is sufficient but not necessary, and have proved
the approximation capacity of a linear variant as a Petrov-Galerkin projection.
A new layer normalization scheme is proposed to allow a scaling to propagate
through attention layers, which helps the model achieve remarkable accuracy in
operator learning tasks with unnormalized data. Finally, we present three
operator learning experiments, including the viscid Burgers' equation, an
interface Darcy flow, and an inverse interface coefficient identification
problem. All experiments validate the improvements of the newly proposed simple
attention-based operator learner over their softmax-normalized counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shuhao Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast improvement of TEM image with low-dose electrons by deep learning. (arXiv:2106.01718v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01718</id>
        <link href="http://arxiv.org/abs/2106.01718"/>
        <updated>2021-06-04T01:12:29.380Z</updated>
        <summary type="html"><![CDATA[Low-electron-dose observation is indispensable for observing various samples
using a transmission electron microscope; consequently, image processing has
been used to improve transmission electron microscopy (TEM) images. To apply
such image processing to in situ observations, we here apply a convolutional
neural network to TEM imaging. Using a dataset that includes short-exposure
images and long-exposure images, we develop a pipeline for processed
short-exposure images, based on end-to-end training. The quality of images
acquired with a total dose of approximately 5 e- per pixel becomes comparable
to that of images acquired with a total dose of approximately 1000 e- per
pixel. Because the conversion time is approximately 8 ms, in situ observation
at 125 fps is possible. This imaging technique enables in situ observation of
electron-beam-sensitive specimens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Katsuno_H/0/1/0/all/0/1"&gt;Hiroyasu Katsuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kimura_Y/0/1/0/all/0/1"&gt;Yuki Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamazaki_T/0/1/0/all/0/1"&gt;Tomoya Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takigawa_I/0/1/0/all/0/1"&gt;Ichigaku Takigawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning. (arXiv:2106.01804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01804</id>
        <link href="http://arxiv.org/abs/2106.01804"/>
        <updated>2021-06-04T01:12:29.358Z</updated>
        <summary type="html"><![CDATA[Vision-language pre-training (VLP) on large-scale image-text pairs has
achieved huge success for the cross-modal downstream tasks. The most existing
pre-training methods mainly adopt a two-step training procedure, which firstly
employs a pre-trained object detector to extract region-based visual features,
then concatenates the image representation and text embedding as the input of
Transformer to train. However, these methods face problems of using
task-specific visual representation of the specific object detector for generic
cross-modal understanding, and the computation inefficiency of two-stage
pipeline. In this paper, we propose the first end-to-end vision-language
pre-trained model for both V+L understanding and generation, namely E2E-VLP,
where we build a unified Transformer framework to jointly learn visual
representation, and semantic alignments between image and text. We incorporate
the tasks of object detection and image captioning into pre-training with a
unified Transformer encoder-decoder architecture for enhancing visual learning.
An extensive set of experiments have been conducted on well-established
vision-language downstream tasks to demonstrate the effectiveness of this novel
VLP paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Songfang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenming Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Exploration is Near-Optimal for Tabular MDP. (arXiv:2102.09703v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09703</id>
        <link href="http://arxiv.org/abs/2102.09703"/>
        <updated>2021-06-04T01:12:29.351Z</updated>
        <summary type="html"><![CDATA[We study exploration using randomized value functions in Thompson Sampling
(TS)-like algorithms in reinforcement learning. This type of algorithms enjoys
appealing empirical performance. We show that when we use 1) a single random
seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a
worst-case $\widetilde{O}\left(H\sqrt{SAT}\right)$ regret bound for episodic
time-inhomogeneous Markov Decision Process where $S$ is the size of state
space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is
the number of interactions. This bound polynomially improves all existing
bounds for TS-like algorithms based on randomized value functions, and for the
first time, matches the $\Omega\left(H\sqrt{SAT}\right)$ lower bound up to
logarithmic factors. Our result highlights that randomized exploration can be
near-optimal, which was previously only achieved by optimistic algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1"&gt;Zhihan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1"&gt;Ruoqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation. (arXiv:2101.04108v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04108</id>
        <link href="http://arxiv.org/abs/2101.04108"/>
        <updated>2021-06-04T01:12:29.345Z</updated>
        <summary type="html"><![CDATA[Controlling bias in training datasets is vital for ensuring equal treatment,
or parity, between different groups in downstream applications. A naive
solution is to transform the data so that it is statistically independent of
group membership, but this may throw away too much information when a
reasonable compromise between fairness and accuracy is desired. Another common
approach is to limit the ability of a particular adversary who seeks to
maximize parity. Unfortunately, representations produced by adversarial
approaches may still retain biases as their efficacy is tied to the complexity
of the adversary used during training. To this end, we theoretically establish
that by limiting the mutual information between representations and protected
attributes, we can assuredly control the parity of any downstream classifier.
We demonstrate an effective method for controlling parity through mutual
information based on contrastive information estimators and show that they
outperform approaches that rely on variational bounds based on complex
generative models. We test our approach on UCI Adult and Heritage Health
datasets and demonstrate that our approach provides more informative
representations across a range of desired parity thresholds while providing
strong theoretical guarantees on the parity of any downstream algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1"&gt;Umang Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1"&gt;Aaron M Ferber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1"&gt;Greg Ver Steeg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Calibration and Out-of-domain Generalization. (arXiv:2102.10395v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10395</id>
        <link href="http://arxiv.org/abs/2102.10395"/>
        <updated>2021-06-04T01:12:29.333Z</updated>
        <summary type="html"><![CDATA[Out-of-domain (OOD) generalization is a significant challenge for machine
learning models. Many techniques have been proposed to overcome this challenge,
often focused on learning models with certain invariance properties. In this
work, we draw a link between OOD performance and model calibration, arguing
that calibration across multiple domains can be viewed as a special case of an
invariant representation leading to better OOD generalization. Specifically, we
show that under certain conditions, models which achieve \emph{multi-domain
calibration} are provably free of spurious correlations. This leads us to
propose multi-domain calibration as a measurable and trainable surrogate for
the OOD performance of a classifier. We therefore introduce methods that are
easy to apply and allow practitioners to improve multi-domain calibration by
training or modifying an existing model, leading to better performance on
unseen domains. Using five datasets from the recently proposed WILDS OOD
benchmark, as well as the Colored MNIST dataset, we demonstrate that training
or tuning models so they are calibrated across multiple domains leads to
significantly improved performance on unseen test domains. We believe this
intriguing connection between calibration and OOD generalization is promising
from both a practical and theoretical point of view.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1"&gt;Yoav Wald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenfeld_D/0/1/0/all/0/1"&gt;Daniel Greenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack. (arXiv:2106.01538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01538</id>
        <link href="http://arxiv.org/abs/2106.01538"/>
        <updated>2021-06-04T01:12:29.327Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural networks are sensitive to small input
perturbations. Since the discovery of this intriguing vulnerability, many
defence methods have been proposed that attempt to improve robustness to
adversarial noise. Fast and accurate attacks are required to compare various
defence methods. However, evaluating adversarial robustness has proven to be
extremely challenging. Existing norm minimisation adversarial attacks require
thousands of iterations (e.g. Carlini & Wagner attack), are limited to the
specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results
(e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast,
general and accurate, ignores the norm minimisation penalty and solves a
simpler perturbation-constrained problem. In this work, we introduce a fast,
general and accurate adversarial attack that optimises the original non-convex
constrained minimisation problem. We interpret optimising the Lagrangian of the
adversarial attack optimisation problem as a two-player game: the first player
minimises the Lagrangian wrt the adversarial noise; the second player maximises
the Lagrangian wrt the regularisation penalty. Our attack algorithm
simultaneously optimises primal and dual variables to find the minimal
adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,
such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual
proximal gradient descent attack. We show in the experiments that our attack
outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and
$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against
unregularised and adversarially trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matyasko_A/0/1/0/all/0/1"&gt;Alexander Matyasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dompteur: Taming Audio Adversarial Examples. (arXiv:2102.05431v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05431</id>
        <link href="http://arxiv.org/abs/2102.05431"/>
        <updated>2021-06-04T01:12:29.309Z</updated>
        <summary type="html"><![CDATA[Adversarial examples seem to be inevitable. These specifically crafted inputs
allow attackers to arbitrarily manipulate machine learning systems. Even worse,
they often seem harmless to human observers. In our digital society, this poses
a significant threat. For example, Automatic Speech Recognition (ASR) systems,
which serve as hands-free interfaces to many kinds of systems, can be attacked
with inputs incomprehensible for human listeners. The research community has
unsuccessfully tried several approaches to tackle this problem. In this paper
we propose a different perspective: We accept the presence of adversarial
examples against ASR systems, but we require them to be perceivable by human
listeners. By applying the principles of psychoacoustics, we can remove
semantically irrelevant information from the ASR input and train a model that
resembles human perception more closely. We implement our idea in a tool named
DOMPTEUR and demonstrate that our augmented system, in contrast to an
unmodified baseline, successfully focuses on perceptible ranges of the input
signal. This change forces adversarial examples into the audible range, while
using minimal computational overhead and preserving benign performance. To
evaluate our approach, we construct an adaptive attacker that actively tries to
avoid our augmentations and demonstrate that adversarial examples from this
attacker remain clearly perceivable. Finally, we substantiate our claims by
performing a hearing test with crowd-sourced human listeners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eisenhofer_T/0/1/0/all/0/1"&gt;Thorsten Eisenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonherr_L/0/1/0/all/0/1"&gt;Lea Sch&amp;#xf6;nherr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_J/0/1/0/all/0/1"&gt;Joel Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speckemeier_L/0/1/0/all/0/1"&gt;Lars Speckemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1"&gt;Dorothea Kolossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holz_T/0/1/0/all/0/1"&gt;Thorsten Holz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Abandoning and Pruning for Elastic Distances including Dynamic Time Warping. (arXiv:2102.05221v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05221</id>
        <link href="http://arxiv.org/abs/2102.05221"/>
        <updated>2021-06-04T01:12:29.297Z</updated>
        <summary type="html"><![CDATA[Nearest neighbor search under elastic distances is a key tool for time series
analysis, supporting many applications. However, straightforward
implementations of distances require $O(n^2)$ space and time complexities,
preventing these applications from scaling to long series. Much work has been
devoted to speeding up the NN search process, mostly with the development of
lower bounds, allowing to avoid costly distance computations when a given
threshold is exceeded. This threshold, provided by the similarity search
process, also allows to early abandon the computation of a distance itself.
Another approach, is to prune parts of the computation. All these techniques
are othogonal to each other. In this work, we develop a new generic strategy,
"EAPruned", that tightly integrates pruning with early abandoning. We apply it
to six elastic distance measures: DTW, CDTW, WDTW, ERP, MSM and TWE, showing
substantial speedup in NN search applications. Pruning alone also shows
substantial speedup for some distances, benefiting applications beyond the
scope of NN search (e.g. requiring all pairwise distances), and hence where
early abandoning is not applicable. We~release our implementation as part of a
new C++ library for time series classification, along with easy to use
Python/Numpy bindings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_M/0/1/0/all/0/1"&gt;Matthieu Herrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1"&gt;Geoffrey I. Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints. (arXiv:2011.14858v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14858</id>
        <link href="http://arxiv.org/abs/2011.14858"/>
        <updated>2021-06-04T01:12:29.202Z</updated>
        <summary type="html"><![CDATA[The world is going through one of the most dangerous pandemics of all time
with the rapid spread of the novel coronavirus (COVID-19). According to the
World Health Organisation, the most effective way to thwart the transmission of
coronavirus is to wear medical face masks. Monitoring the use of face masks in
public places has been a challenge because manual monitoring could be unsafe.
This paper proposes an architecture for detecting medical face masks for
deployment on resource-constrained endpoints having extremely low memory
footprints. A small development board with an ARM Cortex-M7 microcontroller
clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for
the deployment of the model. Using the TensorFlow Lite framework, the model is
quantized to further reduce its size. The proposed model is 138 KB post
quantization and runs at the inference speed of 30 FPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_P/0/1/0/all/0/1"&gt;Puranjay Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirania_A/0/1/0/all/0/1"&gt;Abhay Chirania&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory AMP. (arXiv:2012.10861v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10861</id>
        <link href="http://arxiv.org/abs/2012.10861"/>
        <updated>2021-06-04T01:12:29.185Z</updated>
        <summary type="html"><![CDATA[Approximate message passing (AMP) is a low-cost iterative
parameter-estimation technique for certain high-dimensional linear systems with
non-Gaussian distributions. However, AMP only applies to independent
identically distributed (IID) transform matrices, but may become unreliable
(e.g. perform poorly or even diverge) for other matrix ensembles, especially
for ill-conditioned ones. To handle this difficulty, orthogonal/vector AMP
(OAMP/VAMP) was proposed for general right-unitarily-invariant matrices.
However, the Bayes-optimal OAMP/VAMP requires high-complexity linear minimum
mean square error (MMSE) estimator. This limits the application of OAMP/VAMP to
large-scale systems.

To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory
AMP (MAMP), in which a long-memory matched filter is proposed for interference
suppression. The complexity of MAMP is comparable to AMP. The asymptotic
Gaussianity of estimation errors in MAMP is guaranteed by the orthogonality
principle. A state evolution is derived to asymptotically characterize the
performance of MAMP. Based on state evolution, the relaxation parameters and
damping vector in MAMP are optimized. For all right-unitarily-invariant
matrices, the optimized MAMP converges to the high-complexity OAMP/VAMP, and
thus is Bayes-optimal if it has a unique fixed point. Finally, simulations are
provided to verify the validity and accuracy of the theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shunqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1"&gt;Brian M. Kurkoski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph convolutions that can finally model local structure. (arXiv:2011.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15069</id>
        <link href="http://arxiv.org/abs/2011.15069"/>
        <updated>2021-06-04T01:12:29.168Z</updated>
        <summary type="html"><![CDATA[Despite quick progress in the last few years, recent studies have shown that
modern graph neural networks can still fail at very simple tasks, like
detecting small cycles. This hints at the fact that current networks fail to
catch information about the local structure, which is problematic if the
downstream task heavily relies on graph substructure analysis, as in the
context of chemistry. We propose a very simple correction to the now standard
GIN convolution that enables the network to detect small cycles with nearly no
cost in terms of computation time and number of parameters. Tested on real life
molecule property datasets, our model consistently improves performance on
large multi-tasked datasets over all baselines, both globally and on a per-task
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1"&gt;Oriel Frigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data Using Deep Transfer Learning. (arXiv:2012.03129v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03129</id>
        <link href="http://arxiv.org/abs/2012.03129"/>
        <updated>2021-06-04T01:12:29.160Z</updated>
        <summary type="html"><![CDATA[Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1"&gt;Saeed Khaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lizhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-UAV Path Planning for Wireless Data Harvesting with Deep Reinforcement Learning. (arXiv:2010.12461v3 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12461</id>
        <link href="http://arxiv.org/abs/2010.12461"/>
        <updated>2021-06-04T01:12:29.153Z</updated>
        <summary type="html"><![CDATA[Harvesting data from distributed Internet of Things (IoT) devices with
multiple autonomous unmanned aerial vehicles (UAVs) is a challenging problem
requiring flexible path planning methods. We propose a multi-agent
reinforcement learning (MARL) approach that, in contrast to previous work, can
adapt to profound changes in the scenario parameters defining the data
harvesting mission, such as the number of deployed UAVs, number, position and
data amount of IoT devices, or the maximum flying time, without the need to
perform expensive recomputations or relearn control policies. We formulate the
path planning problem for a cooperative, non-communicating, and homogeneous
team of UAVs tasked with maximizing collected data from distributed IoT sensor
nodes subject to flying time and collision avoidance constraints. The path
planning problem is translated into a decentralized partially observable Markov
decision process (Dec-POMDP), which we solve through a deep reinforcement
learning (DRL) approach, approximating the optimal UAV control policy without
prior knowledge of the challenging wireless channel characteristics in dense
urban environments. By exploiting a combination of centered global and local
map representations of the environment that are fed into convolutional layers
of the agents, we show that our proposed network architecture enables the
agents to cooperate effectively by carefully dividing the data collection task
among themselves, adapt to large complex environments and state spaces, and
make movement decisions that balance data collection goals, flight-time
efficiency, and navigation constraints. Finally, learning a control policy that
generalizes over the scenario parameter space enables us to analyze the
influence of individual parameters on collection performance and provide some
intuition about system-level benefits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayerlein_H/0/1/0/all/0/1"&gt;Harald Bayerlein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theile_M/0/1/0/all/0/1"&gt;Mirco Theile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caccamo_M/0/1/0/all/0/1"&gt;Marco Caccamo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesbert_D/0/1/0/all/0/1"&gt;David Gesbert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Structural-RNN for Robot Crowd Navigation with Deep Reinforcement Learning. (arXiv:2011.04820v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04820</id>
        <link href="http://arxiv.org/abs/2011.04820"/>
        <updated>2021-06-04T01:12:29.145Z</updated>
        <summary type="html"><![CDATA[Safe and efficient navigation through human crowds is an essential capability
for mobile robots. Previous work on robot crowd navigation assumes that the
dynamics of all agents are known and well-defined. In addition, the performance
of previous methods deteriorates in partially observable environments and
environments with dense crowds. To tackle these problems, we propose
decentralized structural-Recurrent Neural Network (DS-RNN), a novel network
that reasons about spatial and temporal relationships for robot decision making
in crowd navigation. We train our network with model-free deep reinforcement
learning without any expert supervision. We demonstrate that our model
outperforms previous methods in challenging crowd navigation scenarios. We
successfully transfer the policy learned in the simulator to a real-world
TurtleBot 2i.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuijing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1"&gt;Peixin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weihang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_N/0/1/0/all/0/1"&gt;Neeloy Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1"&gt;Katherine Driggs-Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Constraint-Based Algorithm for the Structural Learning of Continuous-Time Bayesian Networks. (arXiv:2007.03248v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03248</id>
        <link href="http://arxiv.org/abs/2007.03248"/>
        <updated>2021-06-04T01:12:29.138Z</updated>
        <summary type="html"><![CDATA[Dynamic Bayesian networks have been well explored in the literature as
discrete-time models: however, their continuous-time extensions have seen
comparatively little attention. In this paper, we propose the first
constraint-based algorithm for learning the structure of continuous-time
Bayesian networks. We discuss the different statistical tests and the
underlying hypotheses used by our proposal to establish conditional
independence. Furthermore, we analyze and discuss the computational complexity
of the best and worst cases for the proposed algorithm. Finally, we validate
its performance using synthetic data, and we discuss its strengths and
limitations comparing it with the score-based structure learning algorithm from
Nodelman et al. (2003). We find the latter to be more accurate in learning
networks with binary variables, while our constraint-based approach is more
accurate with variables assuming more than two values. Numerical experiments
confirm that score-based and constraint-based algorithms are comparable in
terms of computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bregoli_A/0/1/0/all/0/1"&gt;Alessandro Bregoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1"&gt;Marco Scutari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1"&gt;Fabio Stella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Window Data Augmentation Approach for Speech Emotion Recognition. (arXiv:2010.09895v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09895</id>
        <link href="http://arxiv.org/abs/2010.09895"/>
        <updated>2021-06-04T01:12:29.132Z</updated>
        <summary type="html"><![CDATA[We present a Multi-Window Data Augmentation (MWA-SER) approach for speech
emotion recognition. MWA-SER is a unimodal approach that focuses on two key
concepts; designing the speech augmentation method and building the deep
learning model to recognize the underlying emotion of an audio signal. Our
proposed multi-window augmentation approach generates additional data samples
from the speech signal by employing multiple window sizes in the audio feature
extraction process. We show that our augmentation method, combined with a deep
learning model, improves speech emotion recognition performance. We evaluate
the performance of our approach on three benchmark datasets: IEMOCAP, SAVEE,
and RAVDESS. We show that the multi-window model improves the SER performance
and outperforms a single-window model. The notion of finding the best window
size is an essential step in audio feature extraction. We perform extensive
experimental evaluations to find the best window choice and explore the
windowing effect for SER analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padi_S/0/1/0/all/0/1"&gt;Sarala Padi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sriram_R/0/1/0/all/0/1"&gt;Ram D.Sriram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Boosted Binary Histogram Ensemble for Large-scale Regression. (arXiv:2106.01986v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01986</id>
        <link href="http://arxiv.org/abs/2106.01986"/>
        <updated>2021-06-04T01:12:29.111Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a gradient boosting algorithm for large-scale
regression problems called \textit{Gradient Boosted Binary Histogram Ensemble}
(GBBHE) based on binary histogram partition and ensemble learning. From the
theoretical perspective, by assuming the H\"{o}lder continuity of the target
function, we establish the statistical convergence rate of GBBHE in the space
$C^{0,\alpha}$ and $C^{1,0}$, where a lower bound of the convergence rate for
the base learner demonstrates the advantage of boosting. Moreover, in the space
$C^{1,0}$, we prove that the number of iterations to achieve the fast
convergence rate can be reduced by using ensemble regressor as the base
learner, which improves the computational efficiency. In the experiments,
compared with other state-of-the-art algorithms such as gradient boosted
regression tree (GBRT), Breiman's forest, and kernel-based methods, our GBBHE
algorithm shows promising performance with less running time on large-scale
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hang_H/0/1/0/all/0/1"&gt;Hanyuan Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuchao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hanfang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gender Bias in Depression Detection Using Audio Features. (arXiv:2010.15120v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15120</id>
        <link href="http://arxiv.org/abs/2010.15120"/>
        <updated>2021-06-04T01:12:29.104Z</updated>
        <summary type="html"><![CDATA[Depression is a large-scale mental health problem and a challenging area for
machine learning researchers in detection of depression. Datasets such as
Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created
to aid research in this area. However, on top of the challenges inherent in
accurately detecting depression, biases in datasets may result in skewed
classification performance. In this paper we examine gender bias in the
DAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an
overreporting of performance. By different concepts from Fair Machine Learning,
such as data re-distribution, and using raw audio features, we can mitigate
against the harmful effects of bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bailey_A/0/1/0/all/0/1"&gt;Andrew Bailey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1"&gt;Mark D. Plumbley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. (arXiv:2106.02034v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02034</id>
        <link href="http://arxiv.org/abs/2106.02034"/>
        <updated>2021-06-04T01:12:29.096Z</updated>
        <summary type="html"><![CDATA[Attention is sparse in vision transformers. We observe the final prediction
in vision transformers is only based on a subset of most informative tokens,
which is sufficient for accurate image recognition. Based on this observation,
we propose a dynamic token sparsification framework to prune redundant tokens
progressively and dynamically based on the input. Specifically, we devise a
lightweight prediction module to estimate the importance score of each token
given the current features. The module is added to different layers to prune
redundant tokens hierarchically. To optimize the prediction module in an
end-to-end manner, we propose an attention masking strategy to differentiably
prune a token by blocking its interactions with other tokens. Benefiting from
the nature of self-attention, the unstructured sparse tokens are still hardware
friendly, which makes our framework easy to achieve actual speed-up. By
hierarchically pruning 66% of the input tokens, our method greatly reduces
31%~37% FLOPs and improves the throughput by over 40% while the drop of
accuracy is within 0.5% for various vision transformers. Equipped with the
dynamic token sparsification framework, DynamicViT models can achieve very
competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs
and vision transformers on ImageNet. Code is available at
https://github.com/raoyongming/DynamicViT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages. (arXiv:2010.09322v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09322</id>
        <link href="http://arxiv.org/abs/2010.09322"/>
        <updated>2021-06-04T01:12:29.090Z</updated>
        <summary type="html"><![CDATA[This work presents a seemingly simple but effective technique to improve
low-resource ASR systems for phonetic languages. By identifying sets of
acoustically similar graphemes in these languages, we first reduce the output
alphabet of the ASR system using linguistically meaningful reductions and then
reconstruct the original alphabet using a standalone module. We demonstrate
that this lessens the burden and improves the performance of low-resource
end-to-end ASR systems (because only reduced-alphabet predictions are needed)
and that it is possible to design a very simple but effective reconstruction
module that recovers sequences in the original alphabet from sequences in the
reduced alphabet. We present a finite state transducer-based reconstruction
module that operates on the 1-best ASR hypothesis in the reduced alphabet. We
demonstrate the efficacy of our proposed technique using ASR systems for two
Indian languages, Gujarati and Telugu. With access to only 10 hrs of speech
data, we obtain relative WER reductions of up to 7% compared to systems that do
not use any reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Diwan_A/0/1/0/all/0/1"&gt;Anuj Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies. (arXiv:2002.05120v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05120</id>
        <link href="http://arxiv.org/abs/2002.05120"/>
        <updated>2021-06-04T01:12:29.083Z</updated>
        <summary type="html"><![CDATA[Branch and Bound (B&B) is the exact tree search method typically used to
solve Mixed-Integer Linear Programming problems (MILPs). Learning branching
policies for MILP has become an active research area, with most works proposing
to imitate the strong branching rule and specialize it to distinct classes of
problems. We aim instead at learning a policy that generalizes across
heterogeneous MILPs: our main hypothesis is that parameterizing the state of
the B&B search tree can aid this type of generalization. We propose a novel
imitation learning framework, and introduce new input features and
architectures to represent branching. Experiments on MILP benchmark instances
clearly show the advantages of incorporating an explicit parameterization of
the state of the search tree to modulate the branching decisions, in terms of
both higher accuracy and smaller B&B trees. The resulting policies
significantly outperform the current state-of-the-art method for "learning to
branch" by effectively allowing generalization to generic unseen instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zarpellon_G/0/1/0/all/0/1"&gt;Giulia Zarpellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jason Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1"&gt;Andrea Lodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonconvex Low-Rank Tensor Completion from Noisy Data. (arXiv:1911.04436v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.04436</id>
        <link href="http://arxiv.org/abs/1911.04436"/>
        <updated>2021-06-04T01:12:29.065Z</updated>
        <summary type="html"><![CDATA[We study a noisy tensor completion problem of broad practical interest,
namely, the reconstruction of a low-rank tensor from highly incomplete and
randomly corrupted observations of its entries. While a variety of prior work
has been dedicated to this problem, prior algorithms either are computationally
too expensive for large-scale applications, or come with sub-optimal
statistical guarantees. Focusing on "incoherent" and well-conditioned tensors
of a constant CP rank, we propose a two-stage nonconvex algorithm -- (vanilla)
gradient descent following a rough initialization -- that achieves the best of
both worlds. Specifically, the proposed nonconvex algorithm faithfully
completes the tensor and retrieves all individual tensor factors within nearly
linear time, while at the same time enjoying near-optimal statistical
guarantees (i.e. minimal sample complexity and optimal estimation accuracy).
The estimation errors are evenly spread out across all entries, thus achieving
optimal $\ell_{\infty}$ statistical accuracy. We have also discussed how to
extend our approach to accommodate asymmetric tensors. The insight conveyed
through our analysis of nonconvex optimization might have implications for
other tensor estimation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Changxiao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning. (arXiv:2008.08198v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08198</id>
        <link href="http://arxiv.org/abs/2008.08198"/>
        <updated>2021-06-04T01:12:29.055Z</updated>
        <summary type="html"><![CDATA[X-ray diffraction based microscopy techniques such as High Energy Diffraction
Microscopy rely on knowledge of the position of diffraction peaks with high
precision. These positions are typically computed by fitting the observed
intensities in area detector data to a theoretical peak shape such as
pseudo-Voigt. As experiments become more complex and detector technologies
evolve, the computational cost of such peak detection and shape fitting becomes
the biggest hurdle to the rapid analysis required for real-time feedback during
in-situ experiments. To this end, we propose BraggNN, a deep learning-based
method that can determine peak positions much more rapidly than conventional
pseudo-Voigt peak fitting. When applied to a test dataset, BraggNN gives errors
of less than 0.29 and 0.57 pixels, relative to the conventional method, for 75%
and 95% of the peaks, respectively. When applied to a real experimental
dataset, a 3D reconstruction that used peak positions computed by BraggNN
yields 15% better results on average as compared to a reconstruction obtained
using peak positions determined using conventional 2D pseudo-Voigt fitting.
Recent advances in deep learning method implementations and special-purpose
model inference accelerators allow BraggNN to deliver enormous performance
improvements relative to the conventional method, running, for example, more
than 200 times faster than a conventional method on a consumer-class GPU card
with out-of-the-box software.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengchun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_H/0/1/0/all/0/1"&gt;Hemant Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1"&gt;Jun-Sang Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kenesei_P/0/1/0/all/0/1"&gt;Peter Kenesei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miceli_A/0/1/0/all/0/1"&gt;Antonino Miceli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Almer_J/0/1/0/all/0/1"&gt;Jonathan Almer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kettimuthu_R/0/1/0/all/0/1"&gt;Rajkumar Kettimuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Foster_I/0/1/0/all/0/1"&gt;Ian Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic tree ensembles for regularized nonlinear regression. (arXiv:2002.03375v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03375</id>
        <link href="http://arxiv.org/abs/2002.03375"/>
        <updated>2021-06-04T01:12:29.048Z</updated>
        <summary type="html"><![CDATA[This paper develops a novel stochastic tree ensemble method for nonlinear
regression, which we refer to as XBART, short for Accelerated Bayesian Additive
Regression Trees. By combining regularization and stochastic search strategies
from Bayesian modeling with computationally efficient techniques from recursive
partitioning approaches, the new method attains state-of-the-art performance:
in many settings it is both faster and more accurate than the widely-used
XGBoost algorithm. Via careful simulation studies, we demonstrate that our new
approach provides accurate point-wise estimates of the mean function and does
so faster than popular alternatives, such as BART, XGBoost and neural networks
(using Keras). We also prove a number of basic theoretical results about the
new algorithm, including consistency of the single tree version of the model
and stationarity of the Markov chain produced by the ensemble version.
Furthermore, we demonstrate that initializing standard Bayesian additive
regression trees Markov chain Monte Carlo (MCMC) at XBART-fitted trees
considerably improves credible interval coverage and reduces total run-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1"&gt;Jingyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hahn_P/0/1/0/all/0/1"&gt;P. Richard Hahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games. (arXiv:2106.01969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01969</id>
        <link href="http://arxiv.org/abs/2106.01969"/>
        <updated>2021-06-04T01:12:29.041Z</updated>
        <summary type="html"><![CDATA[Potential games are arguably one of the most important and widely studied
classes of normal form games. They define the archetypal setting of multi-agent
coordination as all agent utilities are perfectly aligned with each other via a
common potential function. Can this intuitive framework be transplanted in the
setting of Markov Games? What are the similarities and differences between
multi-agent coordination with and without state dependence? We present a novel
definition of Markov Potential Games (MPG) that generalizes prior attempts at
capturing complex stateful multi-agent coordination. Counter-intuitively,
insights from normal-form potential games do not carry over as MPGs can consist
of settings where state-games can be zero-sum games. In the opposite direction,
Markov games where every state-game is a potential game are not necessarily
MPGs. Nevertheless, MPGs showcase standard desirable properties such as the
existence of deterministic Nash policies. In our main technical result, we
prove fast convergence of independent policy gradient to Nash policies by
adapting recent gradient dominance property arguments developed for single
agent MDPs to multi-agent learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1"&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1"&gt;Will Overman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panageas_I/0/1/0/all/0/1"&gt;Ioannis Panageas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Matrix Approximation with Radial Basis Function Components. (arXiv:2106.02018v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02018</id>
        <link href="http://arxiv.org/abs/2106.02018"/>
        <updated>2021-06-04T01:12:29.034Z</updated>
        <summary type="html"><![CDATA[We introduce and investigate matrix approximation by decomposition into a sum
of radial basis function (RBF) components. An RBF component is a generalization
of the outer product between a pair of vectors, where an RBF function replaces
the scalar multiplication between individual vector elements. Even though the
RBF functions are positive definite, the summation across components is not
restricted to convex combinations and allows us to compute the decomposition
for any real matrix that is not necessarily symmetric or positive definite. We
formulate the problem of seeking such a decomposition as an optimization
problem with a nonlinear and non-convex loss function. Several modern versions
of the gradient descent method, including their scalable stochastic
counterparts, are used to solve this problem. We provide extensive empirical
evidence of the effectiveness of the RBF decomposition and that of the
gradient-based fitting algorithm. While being conceptually motivated by
singular value decomposition (SVD), our proposed nonlinear counterpart
outperforms SVD by drastically reducing the memory required to approximate a
data matrix with the same $L_2$-error for a wide range of matrix types. For
example, it leads to 2 to 10 times memory save for Gaussian noise, graph
adjacency matrices, and kernel matrices. Moreover, this proximity-based
decomposition can offer additional interpretability in applications that
involve, e.g., capturing the inner low-dimensional structure of the data,
retaining graph connectivity structure, and preserving the acutance of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1"&gt;Elizaveta Rebrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yu-Hang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits. (arXiv:2106.02029v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02029</id>
        <link href="http://arxiv.org/abs/2106.02029"/>
        <updated>2021-06-04T01:12:29.017Z</updated>
        <summary type="html"><![CDATA[It has become increasingly common for data to be collected adaptively, for
example using contextual bandits. Historical data of this type can be used to
evaluate other treatment assignment policies to guide future innovation or
experiments. However, policy evaluation is challenging if the target policy
differs from the one used to collect data, and popular estimators, including
doubly robust (DR) estimators, can be plagued by bias, excessive variance, or
both. In particular, when the pattern of treatment assignment in the collected
data looks little like the pattern generated by the policy to be evaluated, the
importance weights used in DR estimators explode, leading to excessive
variance.

In this paper, we improve the DR estimator by adaptively weighting
observations to control its variance. We show that a t-statistic based on our
improved estimator is asymptotically normal under certain conditions, allowing
us to form confidence intervals and test hypotheses. Using synthetic data and
public benchmarks, we provide empirical evidence for our estimator's improved
accuracy and inferential properties relative to existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhan_R/0/1/0/all/0/1"&gt;Ruohan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hadad_V/0/1/0/all/0/1"&gt;Vitor Hadad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hirshberg_D/0/1/0/all/0/1"&gt;David A. Hirshberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of KB Queries in Task-Oriented Dialogs. (arXiv:2005.00123v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00123</id>
        <link href="http://arxiv.org/abs/2005.00123"/>
        <updated>2021-06-04T01:12:29.010Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialog (TOD) systems often need to formulate knowledge base
(KB) queries corresponding to the user intent and use the query results to
generate system responses. Existing approaches require dialog datasets to
explicitly annotate these KB queries -- these annotations can be time
consuming, and expensive. In response, we define the novel problems of
predicting the KB query and training the dialog agent, without explicit KB
query annotation. For query prediction, we propose a reinforcement learning
(RL) baseline, which rewards the generation of those queries whose KB results
cover the entities mentioned in subsequent dialog. Further analysis reveals
that correlation among query attributes in KB can significantly confuse memory
augmented policy optimization (MAPO), an existing state of the art RL agent. To
address this, we improve the MAPO baseline with simple but important
modifications suited to our task. To train the full TOD system for our setting,
we propose a pipelined approach: it independently predicts when to make a KB
query (query position predictor), then predicts a KB query at the predicted
position (query predictor), and uses the results of predicted query in
subsequent dialog (next response predictor). Overall, our work proposes first
solutions to our novel problem, and our analysis highlights the research
challenges in training TOD systems without query annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1"&gt;Dinesh Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing Backdoors in Robust Machine Learning Models. (arXiv:2003.00865v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00865</id>
        <link href="http://arxiv.org/abs/2003.00865"/>
        <updated>2021-06-04T01:12:29.003Z</updated>
        <summary type="html"><![CDATA[The introduction of robust optimisation has pushed the state-of-the-art in
defending against adversarial attacks. However, the behaviour of such
optimisation has not been studied in the light of a fundamentally different
class of attacks called backdoors. In this paper, we demonstrate that
adversarially robust models are susceptible to backdoor attacks. Subsequently,
we observe that backdoors are reflected in the feature representation of such
models. Then, this observation is leveraged to detect backdoor-infected models
via a detection technique called AEGIS. Specifically, AEGIS uses feature
clustering to effectively detect backdoor-infected robust Deep Neural Networks
(DNNs). In our evaluation of several visible and hidden backdoor triggers on
major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS
effectively detects robust DNNs infected with backdoors. AEGIS detects a
backdoor-infected model with 91.6% accuracy, without any false positives.
Furthermore, AEGIS detects the targeted class in the backdoor-infected model
with a reasonably low (11.1%) false positive rate. Our investigation reveals
that salient features of adversarially robust DNNs break the stealthy nature of
backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1"&gt;Ezekiel Soremekun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1"&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset and Baselines for Multilingual Reply Suggestion. (arXiv:2106.02017v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02017</id>
        <link href="http://arxiv.org/abs/2106.02017"/>
        <updated>2021-06-04T01:12:28.996Z</updated>
        <summary type="html"><![CDATA[Reply suggestion models help users process emails and chats faster. Previous
work only studies English reply suggestion. Instead, we present MRS, a
multilingual reply suggestion dataset with ten languages. MRS can be used to
compare two families of models: 1) retrieval models that select the reply from
a fixed set and 2) generation models that produce the reply from scratch.
Therefore, MRS complements existing cross-lingual generalization benchmarks
that focus on classification and sequence labeling tasks. We build a generation
model and a retrieval model as baselines for MRS. The two models have different
strengths in the monolingual setting, and they require different strategies to
generalize across languages. MRS is publicly available at
https://github.com/zhangmozhi/mrs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Processes on Hypergraphs. (arXiv:2106.01982v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01982</id>
        <link href="http://arxiv.org/abs/2106.01982"/>
        <updated>2021-06-04T01:12:28.989Z</updated>
        <summary type="html"><![CDATA[We derive a Matern Gaussian process (GP) on the vertices of a hypergraph.
This enables estimation of regression models of observed or latent values
associated with the vertices, in which the correlation and uncertainty
estimates are informed by the hypergraph structure. We further present a
framework for embedding the vertices of a hypergraph into a latent space using
the hypergraph GP. Finally, we provide a scheme for identifying a small number
of representative inducing vertices that enables scalable inference through
sparse GPs. We demonstrate the utility of our framework on three challenging
real-world problems that concern multi-class classification for the political
party affiliation of legislators on the basis of voting behaviour,
probabilistic matrix factorisation of movie reviews, and embedding a hypergraph
of animals into a low-dimensional latent space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pinder_T/0/1/0/all/0/1"&gt;Thomas Pinder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Turnbull_K/0/1/0/all/0/1"&gt;Kathryn Turnbull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1"&gt;Christopher Nemeth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leslie_D/0/1/0/all/0/1"&gt;David Leslie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Variational State Estimation for Nonlinear State-Space Models. (arXiv:2002.02620v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02620</id>
        <link href="http://arxiv.org/abs/2002.02620"/>
        <updated>2021-06-04T01:12:28.971Z</updated>
        <summary type="html"><![CDATA[In this paper, the problem of state estimation, in the context of both
filtering and smoothing, for nonlinear state-space models is considered. Due to
the nonlinear nature of the models, the state estimation problem is generally
intractable as it involves integrals of general nonlinear functions and the
filtered and smoothed state distributions lack closed-form solutions. As such,
it is common to approximate the state estimation problem. In this paper, we
develop an assumed Gaussian solution based on variational inference, which
offers the key advantage of a flexible, but principled, mechanism for
approximating the required distributions. Our main contribution lies in a new
formulation of the state estimation problem as an optimisation problem, which
can then be solved using standard optimisation routines that employ exact
first- and second-order derivatives. The resulting state estimation approach
involves a minimal number of assumptions and applies directly to nonlinear
systems with both Gaussian and non-Gaussian probabilistic models. The
performance of our approach is demonstrated on several examples; a challenging
scalar system, a model of a simple robotic system, and a target tracking
problem using a von Mises-Fisher distribution and outperforms alternative
assumed Gaussian approaches to state estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Courts_J/0/1/0/all/0/1"&gt;Jarrad Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wills_A/0/1/0/all/0/1"&gt;Adrian Wills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-06-04T01:12:28.964Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://github.com/fepegar/torchio. The package can be
installed from the Python Package Index running 'pip install torchio'. It
includes a command-line interface which allows users to apply transforms to
image files without using Python. Additionally, we provide a graphical
interface within a TorchIO extension in 3D Slicer to visualize the effects of
transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning as One Big Sequence Modeling Problem. (arXiv:2106.02039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02039</id>
        <link href="http://arxiv.org/abs/2106.02039"/>
        <updated>2021-06-04T01:12:28.957Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is typically concerned with estimating
single-step policies or single-step models, leveraging the Markov property to
factorize the problem in time. However, we can also view RL as a sequence
modeling problem, with the goal being to predict a sequence of actions that
leads to a sequence of high rewards. Viewed in this way, it is tempting to
consider whether powerful, high-capacity sequence prediction models that work
well in other domains, such as natural-language processing, can also provide
simple and effective solutions to the RL problem. To this end, we explore how
RL can be reframed as "one big sequence modeling" problem, using
state-of-the-art Transformer architectures to model distributions over
sequences of states, actions, and rewards. Addressing RL as a sequence modeling
problem significantly simplifies a range of design decisions: we no longer
require separate behavior policy constraints, as is common in prior work on
offline model-free RL, and we no longer require ensembles or other epistemic
uncertainty estimators, as is common in prior work on model-based RL. All of
these roles are filled by the same Transformer sequence model. In our
experiments, we demonstrate the flexibility of this approach across
long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and
offline RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1"&gt;Michael Janner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Refinement of Cross-Lingual Word Embeddings. (arXiv:1911.03070v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03070</id>
        <link href="http://arxiv.org/abs/1911.03070"/>
        <updated>2021-06-04T01:12:28.950Z</updated>
        <summary type="html"><![CDATA[Cross-lingual word embeddings transfer knowledge between languages: models
trained on high-resource languages can predict in low-resource languages. We
introduce CLIME, an interactive system to quickly refine cross-lingual word
embeddings for a given classification problem. First, CLIME ranks words by
their salience to the downstream task. Then, users mark similarity between
keywords and their nearest neighbors in the embedding space. Finally, CLIME
updates the embeddings using the annotations. We evaluate CLIME on identifying
health-related text in four low-resource languages: Ilocano, Sinhalese,
Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word
semantics and have higher test accuracy than the original embeddings. CLIME
often improves accuracy faster than an active learning baseline and can be
easily combined with active learning to improve results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Michelle Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1"&gt;Benjamin Van Durme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Findlater_L/0/1/0/all/0/1"&gt;Leah Findlater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1"&gt;Jordan Boyd-Graber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MISIM: A Neural Code Semantics Similarity System Using the Context-Aware Semantics Structure. (arXiv:2006.05265v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05265</id>
        <link href="http://arxiv.org/abs/2006.05265"/>
        <updated>2021-06-04T01:12:28.943Z</updated>
        <summary type="html"><![CDATA[Code semantics similarity can be used for many tasks such as code
recommendation, automated software defect correction, and clone detection. Yet,
the accuracy of such systems has not yet reached a level of general purpose
reliability. To help address this, we present Machine Inferred Code Similarity
(MISIM), a neural code semantics similarity system consisting of two core
components: (i)MISIM uses a novel context-aware semantics structure, which was
purpose-built to lift semantics from code syntax; (ii)MISIM uses an extensible
neural code similarity scoring algorithm, which can be used for various neural
network architectures with learned parameters. We compare MISIM to four
state-of-the-art systems, including two additional hand-customized models, over
328K programs consisting of over 18 million lines of code. Our experiments show
that MISIM has 8.08% better accuracy (using MAP@R) compared to the next best
performing system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fangke Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shengtian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkat_A/0/1/0/all/0/1"&gt;Anand Venkat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcus_R/0/1/0/all/0/1"&gt;Ryan Marcus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1"&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tithi_J/0/1/0/all/0/1"&gt;Jesmin Jahan Tithi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1"&gt;Niranjan Hasabnis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1"&gt;Paul Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattson_T/0/1/0/all/0/1"&gt;Timothy Mattson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1"&gt;Tim Kraska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_P/0/1/0/all/0/1"&gt;Pradeep Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_V/0/1/0/all/0/1"&gt;Vivek Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1"&gt;Justin Gottschlich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum correlation alignment for unsupervised domain adaptation. (arXiv:2005.03355v4 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03355</id>
        <link href="http://arxiv.org/abs/2005.03355"/>
        <updated>2021-06-04T01:12:28.925Z</updated>
        <summary type="html"><![CDATA[Correlation alignment (CORAL), a representative domain adaptation (DA)
algorithm, decorrelates and aligns a labelled source domain dataset to an
unlabelled target domain dataset to minimize the domain shift such that a
classifier can be applied to predict the target domain labels. In this paper,
we implement the CORAL on quantum devices by two different methods. One method
utilizes quantum basic linear algebra subroutines (QBLAS) to implement the
CORAL with exponential speedup in the number and dimension of the given data
samples. The other method is achieved through a variational hybrid
quantum-classical procedure. In addition, the numerical experiments of the
CORAL with three different types of data sets, namely the synthetic data, the
synthetic-Iris data, the handwritten digit data, are presented to evaluate the
performance of our work. The simulation results prove that the variational
quantum correlation alignment algorithm (VQCORAL) can achieve competitive
performance compared with the classical CORAL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+He_X/0/1/0/all/0/1"&gt;Xi He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Optimal Transport for Machine Learning: Theory and Applications. (arXiv:2106.01963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01963</id>
        <link href="http://arxiv.org/abs/2106.01963"/>
        <updated>2021-06-04T01:12:28.917Z</updated>
        <summary type="html"><![CDATA[Optimal Transport (OT) theory has seen an increasing amount of attention from
the computer science community due to its potency and relevance in modeling and
machine learning. It introduces means that serve as powerful ways to compare
probability distributions with each other, as well as producing optimal
mappings to minimize cost functions. In this survey, we present a brief
introduction and history, a survey of previous work and propose directions of
future study. We will begin by looking at the history of optimal transport and
introducing the founders of this field. We then give a brief glance into the
algorithms related to OT. Then, we will follow up with a mathematical
formulation and the prerequisites to understand OT. These include Kantorovich
duality, entropic regularization, KL Divergence, and Wassertein barycenters.
Since OT is a computationally expensive problem, we then introduce the
entropy-regularized version of computing optimal mappings, which allowed OT
problems to become applicable in a wide range of machine learning problems. In
fact, the methods generated from OT theory are competitive with the current
state-of-the-art methods. We follow this up by breaking down research papers
that focus on image processing, graph learning, neural architecture search,
document representation, and domain adaptation. We close the paper with a small
section on future research. Of the recommendations presented, three main
problems are fundamental to allow OT to become widely applicable but rely
strongly on its mathematical formulation and thus are hardest to answer. Since
OT is a novel method, there is plenty of space for new research, and with more
and more competitive methods (either on an accuracy level or computational
speed level) being created, the future of applied optimal transport is bright
as it has become pervasive in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Torres_L/0/1/0/all/0/1"&gt;Luis Caicedo Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_L/0/1/0/all/0/1"&gt;Luiz Manella Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1"&gt;M. Hadi Amini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Active Dynamics Learning and Control: A Sequential Exploration-Exploitation Framework. (arXiv:2008.11700v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11700</id>
        <link href="http://arxiv.org/abs/2008.11700"/>
        <updated>2021-06-04T01:12:28.909Z</updated>
        <summary type="html"><![CDATA[Safe deployment of autonomous robots in diverse scenarios requires agents
that are capable of efficiently adapting to new environments while satisfying
constraints. In this work, we propose a practical and theoretically-justified
approach to maintaining safety in the presence of dynamics uncertainty. Our
approach leverages Bayesian meta-learning with last-layer adaptation: the
expressiveness of neural-network features trained offline, paired with
efficient last-layer online adaptation, enables the derivation of tight
confidence sets which contract around the true dynamics as the model adapts
online. We exploit these confidence sets to plan trajectories that guarantee
the safety of the system. Our approach handles problems with high dynamics
uncertainty where reaching the goal safely is initially infeasible by first
exploring to gather data and reduce uncertainty, before autonomously exploiting
the acquired information to safely perform the task. Under reasonable
assumptions, we prove that our framework has high-probability guarantees of
satisfying all constraints at all times jointly. This analysis also motivates
two regularizers of last-layer meta-learners that improve online adaptation
capabilities as well as performance by reducing the size of the confidence
sets. We extensively demonstrate our approach in simulation and on hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lew_T/0/1/0/all/0/1"&gt;Thomas Lew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Apoorva Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1"&gt;James Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bylard_A/0/1/0/all/0/1"&gt;Andrew Bylard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LyricJam: A system for generating lyrics for live instrumental music. (arXiv:2106.01960v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01960</id>
        <link href="http://arxiv.org/abs/2106.01960"/>
        <updated>2021-06-04T01:12:28.902Z</updated>
        <summary type="html"><![CDATA[We describe a real-time system that receives a live audio stream from a jam
session and generates lyric lines that are congruent with the live music being
played. Two novel approaches are proposed to align the learned latent spaces of
audio and text representations that allow the system to generate novel lyric
lines matching live instrumental music. One approach is based on adversarial
alignment of latent representations of audio and lyrics, while the other
approach learns to transfer the topology from the music latent space to the
lyric latent space. A user study with music artists using the system showed
that the system was useful not only in lyric composition, but also encouraged
the artists to improvise and find new musical expressions. Another user study
demonstrated that users preferred the lines generated using the proposed
methods to the lines generated by a baseline model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1"&gt;Olga Vechtomova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1"&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1"&gt;Dhruv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning and Variational Algorithms for Lattice Field Theory. (arXiv:2106.01975v1 [hep-lat])]]></title>
        <id>http://arxiv.org/abs/2106.01975</id>
        <link href="http://arxiv.org/abs/2106.01975"/>
        <updated>2021-06-04T01:12:28.895Z</updated>
        <summary type="html"><![CDATA[In lattice quantum field theory studies, parameters defining the lattice
theory must be tuned toward criticality to access continuum physics. Commonly
used Markov chain Monte Carlo (MCMC) methods suffer from critical slowing down
in this limit, restricting the precision of continuum extrapolations. Further
difficulties arise when measuring correlation functions of operators widely
separated in spacetime: for most correlation functions, an exponentially severe
signal-to-noise problem is encountered as the operators are taken to be widely
separated. This dissertation details two new techniques to address these
issues. First, we define a novel MCMC algorithm based on generative flow-based
models. Such models utilize machine learning methods to describe efficient
approximate samplers for distributions of interest. Independently drawn
flow-based samples are then used as proposals in an asymptotically exact
Metropolis-Hastings Markov chain. We address incorporating symmetries of
interest, including translational and gauge symmetries. We secondly introduce
an approach to "deform" Monte Carlo estimators based on contour deformations
applied to the domain of the path integral. The deformed estimators associated
with an observable give equivalent unbiased measurements of that observable,
but generically have different variances. We define families of deformed
manifolds for lattice gauge theories and introduce methods to efficiently
optimize the choice of manifold (the "observifold"), minimizing the deformed
observable variance. Finally, we demonstrate that flow-based MCMC can mitigate
critical slowing down and observifolds can exponentially reduce variance in
proof-of-principle applications to scalar $\phi^4$ theory and $\mathrm{U}(1)$
and $\mathrm{SU}(N)$ lattice gauge theories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1"&gt;Gurtej Kanwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Low-Rank Semidefinite Programming with Robust Loss Functions. (arXiv:1905.04629v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.04629</id>
        <link href="http://arxiv.org/abs/1905.04629"/>
        <updated>2021-06-04T01:12:28.875Z</updated>
        <summary type="html"><![CDATA[In real-world applications, it is important for machine learning algorithms
to be robust against data outliers or corruptions. In this paper, we focus on
improving the robustness of a large class of learning algorithms that are
formulated as low-rank semi-definite programming (SDP) problems. Traditional
formulations use square loss, which is notorious for being sensitive to
outliers. We propose to replace this with more robust noise models, including
the $\ell_1$-loss and other nonconvex losses. However, the resultant
optimization problem becomes difficult as the objective is no longer convex or
smooth. To alleviate this problem, we design an efficient algorithm based on
majorization-minimization. The crux is on constructing a good optimization
surrogate, and we show that this surrogate can be efficiently obtained by the
alternating direction method of multipliers (ADMM). By properly monitoring
ADMM's convergence, the proposed algorithm is empirically efficient and also
theoretically guaranteed to converge to a critical point. Extensive experiments
are performed on four machine learning applications using both synthetic and
real-world data sets. Results show that the proposed algorithm is not only fast
but also has better performance than the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Quanming Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hangsi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;En-Liang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1"&gt;James Kwok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01981</id>
        <link href="http://arxiv.org/abs/2106.01981"/>
        <updated>2021-06-04T01:12:28.860Z</updated>
        <summary type="html"><![CDATA[Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1"&gt;Boris N. Oreshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1"&gt;Florent Bocquelet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1"&gt;F&amp;#xe9;lix H. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1"&gt;Bay Raitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1"&gt;Dominic Laflamme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCompress: Efficient Point Cloud Geometry Compression. (arXiv:2106.01504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01504</id>
        <link href="http://arxiv.org/abs/2106.01504"/>
        <updated>2021-06-04T01:12:28.845Z</updated>
        <summary type="html"><![CDATA[Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killea_R/0/1/0/all/0/1"&gt;Ryan Killea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_S/0/1/0/all/0/1"&gt;Saeed Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLachlan_P/0/1/0/all/0/1"&gt;Paul McLachlan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplierless MP-Kernel Machine For Energy-efficient Edge Devices. (arXiv:2106.01958v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01958</id>
        <link href="http://arxiv.org/abs/2106.01958"/>
        <updated>2021-06-04T01:12:28.743Z</updated>
        <summary type="html"><![CDATA[We present a novel framework for designing multiplierless kernel machines
that can be used on resource-constrained platforms like intelligent edge
devices. The framework uses a piecewise linear (PWL) approximation based on a
margin propagation (MP) technique and uses only addition/subtraction, shift,
comparison, and register underflow/overflow operations. We propose a
hardware-friendly MP-based inference and online training algorithm that has
been optimized for a Field Programmable Gate Array (FPGA) platform. Our FPGA
implementation eliminates the need for DSP units and reduces the number of
LUTs. By reusing the same hardware for inference and training, we show that the
platform can overcome classification errors and local minima artifacts that
result from the MP approximation. Using the FPGA platform, we also show that
the proposed multiplierless MP-kernel machine demonstrates superior performance
in terms of power, performance, and area compared to other comparable
implementations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Abhishek Ramdas Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_P/0/1/0/all/0/1"&gt;Pallab Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabartty_S/0/1/0/all/0/1"&gt;Shantanu Chakrabartty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_C/0/1/0/all/0/1"&gt;Chetan Singh Thakur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. (arXiv:2106.01954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01954</id>
        <link href="http://arxiv.org/abs/2106.01954"/>
        <updated>2021-06-04T01:12:28.689Z</updated>
        <summary type="html"><![CDATA[Despite the recent popularity of neural network-based solvers for optimal
transport (OT), there is no standard quantitative way to evaluate their
performance. In this paper, we address this issue for quadratic-cost transport
-- specifically, computation of the Wasserstein-2 distance, a commonly-used
formulation of optimal transport in machine learning. To overcome the challenge
of computing ground truth transport maps between continuous measures needed to
assess these solvers, we use input-convex neural networks (ICNN) to construct
pairs of measures whose ground truth OT maps can be obtained analytically. This
strategy yields pairs of continuous benchmark measures in high-dimensional
spaces such as spaces of images. We thoroughly evaluate existing optimal
transport solvers using these benchmark measures. Even though these solvers
perform well in downstream tasks, many do not faithfully recover optimal
transport maps. To investigate the cause of this discrepancy, we further test
the solvers in a setting of image generation. Our study reveals crucial
limitations of existing solvers and shows that increased OT accuracy does not
necessarily correlate to better results downstream.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lingxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genevay_A/0/1/0/all/0/1"&gt;Aude Genevay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1"&gt;Justin Solomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement Learning. (arXiv:2106.01854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01854</id>
        <link href="http://arxiv.org/abs/2106.01854"/>
        <updated>2021-06-04T01:12:28.682Z</updated>
        <summary type="html"><![CDATA[Large sparse linear systems of equations are ubiquitous in science and
engineering, such as those arising from discretizations of partial differential
equations. Algebraic multigrid (AMG) methods are one of the most common methods
of solving such linear systems, with an extensive body of underlying
mathematical theory. A system of linear equations defines a graph on the set of
unknowns and each level of a multigrid solver requires the selection of an
appropriate coarse graph along with restriction and interpolation operators
that map to and from the coarse representation. The efficiency of the multigrid
solver depends critically on this selection and many selection methods have
been developed over the years. Recently, it has been demonstrated that it is
possible to directly learn the AMG interpolation and restriction operators,
given a coarse graph selection. In this paper, we consider the complementary
problem of learning to coarsen graphs for a multigrid solver. We propose a
method using a reinforcement learning (RL) agent based on graph neural networks
(GNNs), which can learn to perform graph coarsening on small training graphs
and then be applied to unstructured large graphs. We demonstrate that this
method can produce better coarse graphs than existing algorithms, even as the
graph size increases and other properties of the graph are varied. We also
propose an efficient inference procedure for performing graph coarsening that
results in linear time complexity in graph size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taghibakhshi_A/0/1/0/all/0/1"&gt;Ali Taghibakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacLachlan_S/0/1/0/all/0/1"&gt;Scott MacLachlan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olson_L/0/1/0/all/0/1"&gt;Luke Olson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_M/0/1/0/all/0/1"&gt;Matthew West&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning in Deep Networks: an Analysis of the Last Layer. (arXiv:2106.01834v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01834</id>
        <link href="http://arxiv.org/abs/2106.01834"/>
        <updated>2021-06-04T01:12:28.675Z</updated>
        <summary type="html"><![CDATA[We study how different output layer types of a deep neural network learn and
forget in continual learning settings. We describe the three factors affecting
catastrophic forgetting in the output layer: (1) weights modifications, (2)
interferences, and (3) projection drift. Our goal is to provide more insights
into how different types of output layers can address (1) and (2). We also
propose potential solutions and evaluate them on several benchmarks. We show
that the best-performing output layer type depends on the data distribution
drifts or the amount of data available. In particular, in some cases where a
standard linear layer would fail, it is sufficient to change the
parametrization and get significantly better performance while still training
with SGD. Our results and analysis shed light on the dynamics of the output
layer in continual learning scenarios and help select the best-suited output
layer for a given scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Lesort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1"&gt;Thomas George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced Convex-Concave Minimax Optimization. (arXiv:2106.01761v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.01761</id>
        <link href="http://arxiv.org/abs/2106.01761"/>
        <updated>2021-06-04T01:12:28.668Z</updated>
        <summary type="html"><![CDATA[This paper considers stochastic first-order algorithms for convex-concave
minimax problems of the form $\min_{\bf x}\max_{\bf y}f(\bf x, \bf y)$, where
$f$ can be presented by the average of $n$ individual components which are
$L$-average smooth. For $\mu_x$-strongly-convex-$\mu_y$-strongly-concave
setting, we propose a new method which could find a $\varepsilon$-saddle point
of the problem in $\tilde{\mathcal O}
\big(\sqrt{n(\sqrt{n}+\kappa_x)(\sqrt{n}+\kappa_y)}\log(1/\varepsilon)\big)$
stochastic first-order complexity, where $\kappa_x\triangleq L/\mu_x$ and
$\kappa_y\triangleq L/\mu_y$. This upper bound is near optimal with respect to
$\varepsilon$, $n$, $\kappa_x$ and $\kappa_y$ simultaneously. In addition, the
algorithm is easily implemented and works well in practical. Our methods can be
extended to solve more general unbalanced convex-concave minimax problems and
the corresponding upper complexity bounds are also near optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_L/0/1/0/all/0/1"&gt;Luo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guangzeng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihua Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical embedding: Beyond principal components. (arXiv:2106.01858v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01858</id>
        <link href="http://arxiv.org/abs/2106.01858"/>
        <updated>2021-06-04T01:12:28.662Z</updated>
        <summary type="html"><![CDATA[There has been an intense recent activity in embedding of very high
dimensional and nonlinear data structures, much of it in the data science and
machine learning literature. We survey this activity in four parts. In the
first part we cover nonlinear methods such as principal curves,
multidimensional scaling, local linear methods, ISOMAP, graph based methods and
kernel based methods. The second part is concerned with topological embedding
methods, in particular mapping topological properties into persistence
diagrams. Another type of data sets with a tremendous growth is very
high-dimensional network data. The task considered in part three is how to
embed such data in a vector space of moderate dimension to make the data
amenable to traditional techniques such as cluster and classification
techniques. The final part of the survey deals with embedding in
$\mathbb{R}^2$, which is visualization. Three methods are presented: $t$-SNE,
UMAP and LargeVis based on methods in parts one, two and three, respectively.
The methods are illustrated and compared on two simulated data sets; one
consisting of a triple of noisy Ranunculoid curves, and one consisting of
networks of increasing complexity and with two types of nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tjostheim_D/0/1/0/all/0/1"&gt;Dag Tj&amp;#xf8;stheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jullum_M/0/1/0/all/0/1"&gt;Martin Jullum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Loland_A/0/1/0/all/0/1"&gt;Anders L&amp;#xf8;land&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions. (arXiv:2106.01798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01798</id>
        <link href="http://arxiv.org/abs/2106.01798"/>
        <updated>2021-06-04T01:12:28.655Z</updated>
        <summary type="html"><![CDATA[Integrating discrete probability distributions and combinatorial optimization
problems into neural networks has numerous applications but poses several
challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a
framework for end-to-end learning of models combining discrete exponential
family distributions and differentiable neural components. I-MLE is widely
applicable: it only requires the ability to compute the most probable states;
and does not rely on smooth relaxations. The framework encompasses several
approaches, such as perturbation-based implicit differentiation and recent
methods to differentiate through black-box combinatorial solvers. We introduce
a novel class of noise distributions for approximating marginals via
perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood
estimation when used in some recently studied learning settings that involve
combinatorial solvers. Experiments on several datasets suggest that I-MLE is
competitive with and often outperforms existing approaches which rely on
problem-specific relaxations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1"&gt;Mathias Niepert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1"&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1"&gt;Luca Franceschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepOpt: Scalable Specification-based Falsification of Neural Networks using Black-Box Optimization. (arXiv:2106.01917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01917</id>
        <link href="http://arxiv.org/abs/2106.01917"/>
        <updated>2021-06-04T01:12:28.636Z</updated>
        <summary type="html"><![CDATA[Decisions made by deep neural networks (DNNs) have a tremendous impact on the
dependability of the systems that they are embedded into, which is of
particular concern in the realm of safety-critical systems. In this paper we
consider specification-based falsification of DNNs with the aim to support
debugging and repair. We propose DeepOpt, a falsification technique based on
black-box optimization, which generates counterexamples from a DNN in a
refinement loop. DeepOpt can analyze input-output specifications, which makes
it more general than falsification approaches that only support robustness
specifications. The key idea is to algebraically combine the DNN with the input
and output constraints derived from the specification. We have implemented
DeepOpt and evaluated it on DNNs of varying sizes and architectures.
Experimental comparisons demonstrate DeepOpt's precision and scalability; in
particular, DeepOpt requires very few queries to the DNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_Marquart_F/0/1/0/all/0/1"&gt;Fabian Bauer-Marquart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leue_S/0/1/0/all/0/1"&gt;Stefan Leue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1"&gt;Christian Schilling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Intervention Networks for Causal Effect Estimation. (arXiv:2106.01939v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01939</id>
        <link href="http://arxiv.org/abs/2106.01939"/>
        <updated>2021-06-04T01:12:28.630Z</updated>
        <summary type="html"><![CDATA[We address the estimation of conditional average treatment effects (CATEs)
when treatments are graph-structured (e.g., molecular graphs of drugs). Given a
weak condition on the effect, we propose a plug-in estimator that decomposes
CATE estimation into separate, simpler optimization problems. Our estimator (a)
isolates the causal estimands (reducing regularization bias), and (b) allows
one to plug in arbitrary models for learning. In experiments with small-world
and molecular graphs, we show that our approach outperforms prior approaches
and is robust to varying selection biases. Our implementation is online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1"&gt;Jean Kaddour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.01933</id>
        <link href="http://arxiv.org/abs/2106.01933"/>
        <updated>2021-06-04T01:12:28.624Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1"&gt;David Gaddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks. (arXiv:2106.01862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01862</id>
        <link href="http://arxiv.org/abs/2106.01862"/>
        <updated>2021-06-04T01:12:28.618Z</updated>
        <summary type="html"><![CDATA[Neuromorphic sensing and computing hold a promise for highly energy-efficient
and high-bandwidth-sensor processing. A major challenge for neuromorphic
computing is that learning algorithms for traditional artificial neural
networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due
to the discrete spikes and more complex neuronal dynamics. As a consequence,
SNNs have not yet been successfully applied to complex, large-scale tasks. In
this article, we focus on the self-supervised learning problem of optical flow
estimation from event-based camera inputs, and investigate the changes that are
necessary to the state-of-the-art ANN training pipeline in order to
successfully tackle it with SNNs. More specifically, we first modify the input
event representation to encode a much smaller time slice with minimal explicit
temporal information. Consequently, we make the network's neuronal dynamics and
recurrent connections responsible for integrating information over time.
Moreover, we reformulate the self-supervised loss function for event-based
optical flow to improve its convexity. We perform experiments with various
types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs,
we investigate the effects of elements such as parameter initialization and
optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We
find that initialization and surrogate gradient width play a crucial part in
enabling learning with sparse inputs, while the inclusion of adaptivity and
learnable neuronal parameters can improve performance. We show that the
performance of the proposed ANNs and SNNs are on par with that of the current
state-of-the-art ANNs trained in a self-supervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1"&gt;Federico Paredes-Vall&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagenaars_J/0/1/0/all/0/1"&gt;Jesse Hagenaars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathology-Aware Generative Adversarial Networks for Medical Image Augmentation. (arXiv:2106.01915v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01915</id>
        <link href="http://arxiv.org/abs/2106.01915"/>
        <updated>2021-06-04T01:12:28.600Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) can play a key role in Medical Image
Analysis under large-scale annotated datasets. However, preparing such massive
dataset is demanding. In this context, Generative Adversarial Networks (GANs)
can generate realistic but novel samples, and thus effectively cover the real
image distribution. In terms of interpolation, the GAN-based medical image
augmentation is reliable because medical modalities can display the human
body's strong anatomical consistency at fixed position while clearly reflecting
inter-subject variability; thus, we propose to use noise-to-image GANs (e.g.,
random noise samples to diverse pathological images) for (i) medical Data
Augmentation (DA) and (ii) physician training. Regarding the DA, the
GAN-generated images can improve Computer-Aided Diagnosis based on supervised
learning. For the physician training, the GANs can display novel desired
pathological images and help train medical trainees despite
infrastructural/legal constraints. This thesis contains four GAN projects
aiming to present such novel applications' clinical relevance in collaboration
with physicians. Whereas the methods are more generally applicable, this thesis
only explores a few oncological applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement Prediction. (arXiv:2106.01920v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.01920</id>
        <link href="http://arxiv.org/abs/2106.01920"/>
        <updated>2021-06-04T01:12:28.594Z</updated>
        <summary type="html"><![CDATA[With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1"&gt;Kunal Bhardwaj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Selection Bias in Evaluation of Prediction Performance of Causal Models. (arXiv:2106.01921v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01921</id>
        <link href="http://arxiv.org/abs/2106.01921"/>
        <updated>2021-06-04T01:12:28.588Z</updated>
        <summary type="html"><![CDATA[Causal models are notoriously difficult to validate because they make
untestable assumptions regarding confounding. New scientific experiments offer
the possibility of evaluating causal models using prediction performance.
Prediction performance measures are typically robust to violations in causal
assumptions. However prediction performance does depend on the selection of
training and test sets. In particular biased training sets can lead to
optimistic assessments of model performance. In this work, we revisit the
prediction performance of several recently proposed causal models tested on a
genetic perturbation data set of Kemmeren [Kemmeren et al., 2014]. We find that
sample selection bias is likely a key driver of model performance. We propose
using a less-biased evaluation set for assessing prediction performance on
Kemmeren and compare models on this new set. In this setting, the causal model
tested have similar performance to standard association based estimators such
as Lasso. Finally we compare the performance of causal estimators in simulation
studies which reproduce the Kemmeren structure of genetic knockout experiments
but without any sample selection bias. These results provide an improved
understanding of the performance of several causal models and offer guidance on
how future studies should use Kemmeren.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Long_J/0/1/0/all/0/1"&gt;James P. Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ha_M/0/1/0/all/0/1"&gt;Min Jin Ha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01741</id>
        <link href="http://arxiv.org/abs/2106.01741"/>
        <updated>2021-06-04T01:12:28.581Z</updated>
        <summary type="html"><![CDATA[A long-standing challenge in artificial intelligence is lifelong learning. In
lifelong learning, many tasks are presented in sequence and learners must
efficiently transfer knowledge between tasks while avoiding catastrophic
forgetting over long lifetimes. On these problems, policy reuse and other
multi-policy reinforcement learning techniques can learn many tasks. However,
they can generate many temporary or permanent policies, resulting in memory
issues. Consequently, there is a need for lifetime-scalable methods that
continually refine a policy library of a pre-defined size. This paper presents
a first approach to lifetime-scalable policy reuse. To pre-select the number of
policies, a notion of task capacity, the maximal number of tasks that a policy
can accurately solve, is proposed. To evaluate lifetime policy reuse using this
method, two state-of-the-art single-actor base-learners are compared: 1) a
value-based reinforcement learner, Deep Q-Network (DQN) or Deep Recurrent
Q-Network (DRQN); and 2) an actor-critic reinforcement learner, Proximal Policy
Optimisation (PPO) with or without Long Short-Term Memory layer. By selecting
the number of policies based on task capacity, D(R)QN achieves near-optimal
performance with 6 policies in a 27-task MDP domain and 9 policies in an
18-task POMDP domain; with fewer policies, catastrophic forgetting and negative
transfer are observed. Due to slow, monotonic improvement, PPO requires fewer
policies, 1 policy for the 27-task domain and 4 policies for the 18-task
domain, but it learns the tasks with lower accuracy than D(R)QN. These findings
validate lifetime-scalable policy reuse and suggest using D(R)QN for larger and
PPO for smaller library sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bossens_D/0/1/0/all/0/1"&gt;David M. Bossens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sobey_A/0/1/0/all/0/1"&gt;Adam J. Sobey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning models for DOTA 2 outcomes prediction. (arXiv:2106.01782v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01782</id>
        <link href="http://arxiv.org/abs/2106.01782"/>
        <updated>2021-06-04T01:12:28.575Z</updated>
        <summary type="html"><![CDATA[Prediction of the real-time multiplayer online battle arena (MOBA) games'
match outcome is one of the most important and exciting tasks in Esports
analytical research. This research paper predominantly focuses on building
predictive machine and deep learning models to identify the outcome of the Dota
2 MOBA game using the new method of multi-forward steps predictions. Three
models were investigated and compared: Linear Regression (LR), Neural Networks
(NN), and a type of recurrent neural network Long Short-Term Memory (LSTM). In
order to achieve the goals, we developed a data collecting python server using
Game State Integration (GSI) to track the real-time data of the players. Once
the exploratory feature analysis and tuning hyper-parameters were done, our
models' experiments took place on different players with dissimilar backgrounds
of playing experiences. The achieved accuracy scores depend on the
multi-forward prediction parameters, which for the worse case in linear
regression 69\% but on average 82\%, while in the deep learning models hit the
utmost accuracy of prediction on average 88\% for NN, and 93\% for LSTM models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhmedov_K/0/1/0/all/0/1"&gt;Kodirjon Akhmedov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_A/0/1/0/all/0/1"&gt;Anh Huy Phan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preparation of Many-body Ground States by Time Evolution with Variational Microscopic Magnetic Fields and Incomplete Interactions. (arXiv:2106.01779v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01779</id>
        <link href="http://arxiv.org/abs/2106.01779"/>
        <updated>2021-06-04T01:12:28.568Z</updated>
        <summary type="html"><![CDATA[State preparation is of fundamental importance in quantum physics, which can
be realized by constructing the quantum circuit as a unitary that transforms
the initial state to the target, or implementing a quantum control protocol to
evolve to the target state with a designed Hamiltonian. In this work, we study
the latter on quantum many-body systems by the time evolution with fixed
couplings and variational magnetic fields. In specific, we consider to prepare
the ground states of the Hamiltonians containing certain interactions that are
missing in the Hamiltonians for the time evolution. An optimization method is
proposed to optimize the magnetic fields by "fine-graining" the discretization
of time, in order to gain high precision and stability. The back propagation
technique is utilized to obtain the gradients of the fields against the
logarithmic fidelity. Our method is tested on preparing the ground state of
Heisenberg chain with the time evolution by the XY and Ising interactions, and
its performance surpasses two baseline methods that use local and global
optimization strategies, respectively. Our work can be applied and generalized
to other quantum models such as those defined on higher dimensional lattices.
It enlightens to reduce the complexity of the required interactions for
implementing quantum control or other tasks in quantum information and
computation by means of optimizing the magnetic fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Ying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yue-Min Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Peng-Fei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1"&gt;Shi-Ju Ran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-tagging of Short Conversational Sentences using Transformer Methods. (arXiv:2106.01735v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01735</id>
        <link href="http://arxiv.org/abs/2106.01735"/>
        <updated>2021-06-04T01:12:28.562Z</updated>
        <summary type="html"><![CDATA[The problem of categorizing short speech sentences according to their
semantic features with high accuracy is a subject studied in natural language
processing. In this study, a data set created with samples classified in 46
different categories was used. Examples consist of sentences taken from chat
conversations between a company's customer representatives and the company's
website visitors. The primary purpose is to automatically tag questions and
requests from visitors in the most accurate way for 46 predetermined categories
for use in a chat application to generate meaningful answers to the questions
asked by the website visitors. For this, different BERT models and one GPT-2
model, pre-trained in Turkish, were preferred. The classification performances
of the relevant models were analyzed in detail and reported accordingly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1"&gt;Umut &amp;#xd6;zdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akca_M/0/1/0/all/0/1"&gt;M. Fatih Akca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmez_O/0/1/0/all/0/1"&gt;O&amp;#x11f;uzhan &amp;#xd6;lmez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulum_S/0/1/0/all/0/1"&gt;Semih G&amp;#xfc;l&amp;#xfc;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutal_S/0/1/0/all/0/1"&gt;Se&amp;#xe7;ilay Kutal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belhan_C/0/1/0/all/0/1"&gt;Ceren Belhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Graph Reasoning for Neural Network Regularization. (arXiv:2106.01805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01805</id>
        <link href="http://arxiv.org/abs/2106.01805"/>
        <updated>2021-06-04T01:12:28.544Z</updated>
        <summary type="html"><![CDATA[Regularizers helped deep neural networks prevent feature co-adaptations.
Dropout,as a commonly used regularization technique, stochastically disables
neuron ac-tivations during network optimization. However, such complete feature
disposal can affect the feature representation and network understanding.
Toward betterdescriptions of latent representations, we present DropGraph that
learns regularization function by constructing a stand-alone graph from the
backbone features. DropGraph first samples stochastic spatial feature vectors
and then incorporates graph reasoning methods to generate feature map
distortions. This add-on graph regularizes the network during training and can
be completely skipped during inference. We provide intuitions on the linkage
between graph reasoning andDropout with further discussions on how partial
graph reasoning method reduces feature correlations. To this end, we
extensively study the modeling of graphvertex dependencies and the utilization
of the graph for distorting backbone featuremaps. DropGraph was validated on
four tasks with a total of 7 different datasets.The experimental results show
that our method outperforms other state-of-the-art regularizers while leaving
the base model structure unmodified during inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tiange Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hongliang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Normative Model of Classifier Fusion. (arXiv:2106.01770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01770</id>
        <link href="http://arxiv.org/abs/2106.01770"/>
        <updated>2021-06-04T01:12:28.538Z</updated>
        <summary type="html"><![CDATA[Combining the outputs of multiple classifiers or experts into a single
probabilistic classification is a fundamental task in machine learning with
broad applications from classifier fusion to expert opinion pooling. Here we
present a hierarchical Bayesian model of probabilistic classifier fusion based
on a new correlated Dirichlet distribution. This distribution explicitly models
positive correlations between marginally Dirichlet-distributed random vectors
thereby allowing normative modeling of correlations between base classifiers or
experts. The proposed model naturally accommodates the classic Independent
Opinion Pool and other independent fusion algorithms as special cases. It is
evaluated by uncertainty reduction and correctness of fusion on synthetic and
real-world data sets. We show that a change in performance of the fused
classifier due to uncertainty reduction can be Bayes optimal even for highly
correlated base classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trick_S/0/1/0/all/0/1"&gt;Susanne Trick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1"&gt;Constantin A. Rothkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effort-free Automated Skeletal Abnormality Detection of Rat Fetuses on Whole-body Micro-CT Scans. (arXiv:2106.01830v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01830</id>
        <link href="http://arxiv.org/abs/2106.01830"/>
        <updated>2021-06-04T01:12:28.531Z</updated>
        <summary type="html"><![CDATA[Machine Learning-based fast and quantitative automated screening plays a key
role in analyzing human bones on Computed Tomography (CT) scans. However,
despite the requirement in drug safety assessment, such research is rare on
animal fetus micro-CT scans due to its laborious data collection and
annotation. Therefore, we propose various bone feature engineering techniques
to thoroughly automate the skeletal localization/labeling/abnormality detection
of rat fetuses on whole-body micro-CT scans with minimum effort. Despite
limited training data of 49 fetuses, in skeletal labeling and abnormality
detection, we achieve accuracy of 0.900 and 0.810, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fukuda_A/0/1/0/all/0/1"&gt;Akihiro Fukuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hakamada_K/0/1/0/all/0/1"&gt;Kazumi Hakamada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01883</id>
        <link href="http://arxiv.org/abs/2106.01883"/>
        <updated>2021-06-04T01:12:28.514Z</updated>
        <summary type="html"><![CDATA[Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaojiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jirui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1"&gt;Qi Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal. (arXiv:2106.01467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01467</id>
        <link href="http://arxiv.org/abs/2106.01467"/>
        <updated>2021-06-04T01:12:28.507Z</updated>
        <summary type="html"><![CDATA[Bringing empathy to a computerized system could significantly improve the
quality of human-computer communications, as soon as machines would be able to
understand customer intentions and better serve their needs. According to
different studies (Literature Review), visual information is one of the most
important channels of human interaction and contains significant behavioral
signals, that may be captured from facial expressions. Therefore, it is
consistent and natural that the research in the field of Facial Expression
Recognition (FER) has acquired increased interest over the past decade due to
having diverse application area including health-care, sociology, psychology,
driver-safety, virtual reality, cognitive sciences, security, entertainment,
marketing, etc. We propose a new architecture for the task of FER and examine
the impact of domain discrimination loss regularization on the learning
process. With regard to observations, including both classical training
conditions and unsupervised domain adaptation scenarios, important aspects of
the considered domain adaptation approach integration are traced. The results
may serve as a foundation for further research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhmetov_K/0/1/0/all/0/1"&gt;Kamil Akhmetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drivers' Manoeuvre Modelling and Prediction for Safe HRI. (arXiv:2106.01730v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01730</id>
        <link href="http://arxiv.org/abs/2106.01730"/>
        <updated>2021-06-04T01:12:28.501Z</updated>
        <summary type="html"><![CDATA[As autonomous machines such as robots and vehicles start performing tasks
involving human users, ensuring a safe interaction between them becomes an
important issue. Translating methods from human-robot interaction (HRI) studies
to the interaction between humans and other highly complex machines (e.g.
semi-autonomous vehicles) could help advance the use of those machines in
scenarios requiring human interaction. One method involves understanding human
intentions and decision-making to estimate the human's present and near-future
actions whilst interacting with a robot. This idea originates from the
psychological concept of Theory of Mind, which has been broadly explored for
robotics and recently for autonomous and semi-autonomous vehicles. In this
work, we explored how to predict human intentions before an action is performed
by combining data from human-motion, vehicle-state and human inputs (e.g.
steering wheel, pedals). A data-driven approach based on Recurrent Neural
Network models was used to classify the current driving manoeuvre and to
predict the future manoeuvre to be performed. A state-transition model was used
with a fixed set of manoeuvres to label data recorded during the trials for
real-time applications. Models were trained and tested using drivers of
different seat preferences, driving expertise and arm-length; precision and
recall metrics over 95% for manoeuvre identification and 86% for manoeuvre
prediction were achieved, with prediction time-windows of up to 1 second for
both known and unknown test subjects. Compared to our previous results,
performance improved and manoeuvre prediction was possible for unknown test
subjects without knowing the current manoeuvre.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pulgarin_E/0/1/0/all/0/1"&gt;Erwin Jose Lopez Pulgarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_G/0/1/0/all/0/1"&gt;Guido Herrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonards_U/0/1/0/all/0/1"&gt;Ute Leonards&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood Inference from Sampled Trajectories. (arXiv:2106.01808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01808</id>
        <link href="http://arxiv.org/abs/2106.01808"/>
        <updated>2021-06-04T01:12:28.494Z</updated>
        <summary type="html"><![CDATA[Simulation-based inference enables learning the parameters of a model even
when its likelihood cannot be computed in practice. One class of methods uses
data simulated with different parameters to infer an amortized estimator for
the likelihood-to-evidence ratio, or equivalently the posterior function. We
show that this approach can be formulated in terms of mutual information
maximization between model parameters and simulated data. We use this
equivalence to reinterpret existing approaches for amortized inference, and
propose two new methods that rely on lower bounds of the mutual information. We
apply our framework to the inference of parameters of stochastic processes and
chaotic dynamical systems from sampled trajectories, using artificial neural
networks for posterior prediction. Our approach provides a unified framework
that leverages the power of mutual information estimators for inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isacchini_G/0/1/0/all/0/1"&gt;Giulio Isacchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spisak_N/0/1/0/all/0/1"&gt;Natanael Spisak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourmohammad_A/0/1/0/all/0/1"&gt;Armita Nourmohammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mora_T/0/1/0/all/0/1"&gt;Thierry Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walczak_A/0/1/0/all/0/1"&gt;Aleksandra M. Walczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LiMIIRL: Lightweight Multiple-Intent Inverse Reinforcement Learning. (arXiv:2106.01777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01777</id>
        <link href="http://arxiv.org/abs/2106.01777"/>
        <updated>2021-06-04T01:12:28.478Z</updated>
        <summary type="html"><![CDATA[Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a
reward function ensemble to rationalize demonstrations of different but
unlabelled intents. Within the popular expectation maximization (EM) framework
for learning probabilistic MI-IRL models, we present a warm-start strategy
based on up-front clustering of the demonstrations in feature space. Our
theoretical analysis shows that this warm-start solution produces a
near-optimal reward ensemble, provided the behavior modes satisfy mild
separation conditions. We also propose a MI-IRL performance metric that
generalizes the popular Expected Value Difference measure to directly assesses
learned rewards against the ground-truth reward ensemble. Our metric elegantly
addresses the difficulty of pairing up learned and ground truth rewards via a
min-cost flow formulation, and is efficiently computable. We also develop a
MI-IRL benchmark problem that allows for more comprehensive algorithmic
evaluations. On this problem, we find our MI-IRL warm-start strategy helps
avoid poor quality local minima reward ensembles, resulting in a significant
improvement in behavior clustering. Our extensive sensitivity analysis
demonstrates that the quality of the learned reward ensembles is improved under
various settings, including cases where our theoretical assumptions do not
necessarily hold. Finally, we demonstrate the effectiveness of our methods by
discovering distinct driving styles in a large real-world dataset of driver GPS
trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1"&gt;Aaron J. Snoswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Surya P. N. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nan Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmoDNN: Understanding emotions from short texts through a deep neural network ensemble. (arXiv:2106.01706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01706</id>
        <link href="http://arxiv.org/abs/2106.01706"/>
        <updated>2021-06-04T01:12:28.471Z</updated>
        <summary type="html"><![CDATA[The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction. (arXiv:2106.01709v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01709</id>
        <link href="http://arxiv.org/abs/2106.01709"/>
        <updated>2021-06-04T01:12:28.394Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction has attracted much attention in recent
years. It is usually formulated as a classification problem that predicts
relations for all entity pairs in the document. However, previous works
indiscriminately represent intra- and inter-sentential relations in the same
way, confounding the different patterns for predicting them. Besides, they
create a document graph and use paths between entities on the graph as clues
for logical reasoning. However, not all entity pairs can be connected with a
path and have the correct logical reasoning paths in their graph. Thus many
cases of logical reasoning cannot be covered. This paper proposes an effective
architecture, SIRE, to represent intra- and inter-sentential relations in
different ways. We design a new and straightforward form of logical reasoning
module that can cover more logical reasoning chains. Experiments on the public
datasets show SIRE outperforms the previous state-of-the-art methods. Further
analysis shows that our predictions are reliable and explainable. Our code is
available at https://github.com/DreamInvoker/SIRE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuting Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Minimization from Adaptively Collected Data: Guarantees for Supervised and Policy Learning. (arXiv:2106.01723v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01723</id>
        <link href="http://arxiv.org/abs/2106.01723"/>
        <updated>2021-06-04T01:12:28.387Z</updated>
        <summary type="html"><![CDATA[Empirical risk minimization (ERM) is the workhorse of machine learning,
whether for classification and regression or for off-policy policy learning,
but its model-agnostic guarantees can fail when we use adaptively collected
data, such as the result of running a contextual bandit algorithm. We study a
generic importance sampling weighted ERM algorithm for using adaptively
collected data to minimize the average of a loss function over a hypothesis
class and provide first-of-their-kind generalization guarantees and fast
convergence rates. Our results are based on a new maximal inequality that
carefully leverages the importance sampling structure to obtain rates with the
right dependence on the exploration rate in the data. For regression, we
provide fast rates that leverage the strong convexity of squared-error loss.
For policy learning, we provide rate-optimal regret guarantees that close an
open gap in the existing literature whenever exploration decays to zero, as is
the case for bandit-collected data. An empirical investigation validates our
theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bibaut_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bibaut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chambaz_A/0/1/0/all/0/1"&gt;Antoine Chambaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakopoulou_M/0/1/0/all/0/1"&gt;Maria Dimakopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kallus_N/0/1/0/all/0/1"&gt;Nathan Kallus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1"&gt;Mark van der Laan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization Variance: Exploring Generalization Properties of DNNs. (arXiv:2106.01714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01714</id>
        <link href="http://arxiv.org/abs/2106.01714"/>
        <updated>2021-06-04T01:12:28.352Z</updated>
        <summary type="html"><![CDATA[Unlike the conventional wisdom in statistical learning theory, the test error
of a deep neural network (DNN) often demonstrates double descent: as the model
complexity increases, it first follows a classical U-shaped curve and then
shows a second descent. Through bias-variance decomposition, recent studies
revealed that the bell-shaped variance is the major cause of model-wise double
descent (when the DNN is widened gradually). This paper investigates epoch-wise
double descent, i.e., the test error of a DNN also shows double descent as the
number of training epoches increases. By extending the bias-variance analysis
to epoch-wise double descent of the zero-one loss, we surprisingly find that
the variance itself, without the bias, varies consistently with the test error.
Inspired by this result, we propose a novel metric, optimization variance (OV),
to measure the diversity of model updates caused by the stochastic gradients of
random training batches drawn in the same iteration. OV can be estimated using
samples from the training set only but correlates well with the (unknown)
\emph{test} error, and hence early stopping may be achieved without using a
validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lymph Node Graph Neural Networks for Cancer Metastasis Prediction. (arXiv:2106.01711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01711</id>
        <link href="http://arxiv.org/abs/2106.01711"/>
        <updated>2021-06-04T01:12:28.334Z</updated>
        <summary type="html"><![CDATA[Predicting outcomes, such as survival or metastasis for individual cancer
patients is a crucial component of precision oncology. Machine learning (ML)
offers a promising way to exploit rich multi-modal data, including clinical
information and imaging to learn predictors of disease trajectory and help
inform clinical decision making. In this paper, we present a novel graph-based
approach to incorporate imaging characteristics of existing cancer spread to
local lymph nodes (LNs) as well as their connectivity patterns in a prognostic
ML model. We trained an edge-gated Graph Convolutional Network (Gated-GCN) to
accurately predict the risk of distant metastasis (DM) by propagating
information across the LN graph with the aid of soft edge attention mechanism.
In a cohort of 1570 head and neck cancer patients, the Gated-GCN achieves AUROC
of 0.757 for 2-year DM classification and $C$-index of 0.725 for lifetime DM
risk prediction, outperforming current prognostic factors as well as previous
approaches based on aggregated LN features. We also explored the importance of
graph structure and individual lymph nodes through ablation experiments and
interpretability studies, highlighting the importance of considering individual
LN characteristics as well as the relationships between regions of cancer
spread.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazmierski_M/0/1/0/all/0/1"&gt;Michal Kazmierski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haibe_Kains_B/0/1/0/all/0/1"&gt;Benjamin Haibe-Kains&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Representation over Dynamic Graph using Aggregation-Diffusion Mechanism. (arXiv:2106.01678v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01678</id>
        <link href="http://arxiv.org/abs/2106.01678"/>
        <updated>2021-06-04T01:12:28.326Z</updated>
        <summary type="html"><![CDATA[Representation learning on graphs that evolve has recently received
significant attention due to its wide application scenarios, such as
bioinformatics, knowledge graphs, and social networks. The propagation of
information in graphs is important in learning dynamic graph representations,
and most of the existing methods achieve this by aggregation. However, relying
only on aggregation to propagate information in dynamic graphs can result in
delays in information propagation and thus affect the performance of the
method. To alleviate this problem, we propose an aggregation-diffusion (AD)
mechanism that actively propagates information to its neighbor by diffusion
after the node updates its embedding through the aggregation mechanism. In
experiments on two real-world datasets in the dynamic link prediction task, the
AD mechanism outperforms the baseline models that only use aggregation to
propagate information. We further conduct extensive experiments to discuss the
influence of different factors in the AD mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhiying Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaofei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongjie Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Based Texture Analysis of Patella from X-Rays for Detecting Patellofemoral Osteoarthritis. (arXiv:2106.01700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01700</id>
        <link href="http://arxiv.org/abs/2106.01700"/>
        <updated>2021-06-04T01:12:28.305Z</updated>
        <summary type="html"><![CDATA[Objective is to assess the ability of texture features for detecting
radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view
radiographs. We used lateral view knee radiographs from MOST public use
datasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically
detected using landmark detection tool (BoneFinder). Hand-crafted features,
based on LocalBinary Patterns (LBP), were then extracted to describe the
patellar texture. First, a machine learning model (Gradient Boosting Machine)
was trained to detect radiographic PFOA from the LBP features. Furthermore, we
used end-to-end trained deep convolutional neural networks (CNNs) directly on
the texture patches for detecting the PFOA. The proposed classification models
were eventually compared with more conventional reference models that use
clinical assessments and participant characteristics such as age, sex, body
mass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL)
grade. Atlas-guided visual assessment of PFOA status by expert readers provided
in the MOST public use datasets was used as a classification outcome for the
models. Performance of prediction models was assessed using the area under the
receiver operating characteristic curve (ROC AUC), the area under the
precision-recall (PR) curve-average precision (AP)-, and Brier score in the
stratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had
PFOA. AUC and AP for the strongest reference model including age, sex, BMI,
WOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487,
respectively. Textural ROI classification using CNN significantly improved the
prediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study
that analyses patellar bone texture for diagnosing PFOA. Our results
demonstrates the potential of using texture features of patella to predict
PFOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bayramoglu_N/0/1/0/all/0/1"&gt;Neslihan Bayramoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nieminen_M/0/1/0/all/0/1"&gt;Miika T. Nieminen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saarakkala_S/0/1/0/all/0/1"&gt;Simo Saarakkala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression. (arXiv:2106.01682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01682</id>
        <link href="http://arxiv.org/abs/2106.01682"/>
        <updated>2021-06-04T01:12:28.288Z</updated>
        <summary type="html"><![CDATA[Gradient Boosting Machines (GBM) are hugely popular for solving tabular data
problems. However, practitioners are not only interested in point predictions,
but also in probabilistic predictions in order to quantify the uncertainty of
the predictions. Creating such probabilistic predictions is difficult with
existing GBM-based solutions: they either require training multiple models or
they become too computationally expensive to be useful for large-scale
settings. We propose Probabilistic Gradient Boosting Machines (PGBM), a method
to create probabilistic predictions with a single ensemble of decision trees in
a computationally efficient manner. PGBM approximates the leaf weights in a
decision tree as a random variable, and approximates the mean and variance of
each sample in a dataset via stochastic tree ensemble update equations. These
learned moments allow us to subsequently sample from a specified distribution
after training. We empirically demonstrate the advantages of PGBM compared to
existing state-of-the-art methods: (i) PGBM enables probabilistic estimates
without compromising on point performance in a single model, (ii) PGBM learns
probabilistic estimates via a single model only (and without requiring
multi-parameter boosting), and thereby offers a speedup of up to several orders
of magnitude over existing state-of-the-art methods on large datasets, and
(iii) PGBM achieves accurate probabilistic estimates in tasks with complex
differentiable loss functions, such as hierarchical time series problems, where
we observed up to 10\% improvement in point forecasting performance and up to
300\% improvement in probabilistic forecasting performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sprangers_O/0/1/0/all/0/1"&gt;Olivier Sprangers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schelter_S/0/1/0/all/0/1"&gt;Sebastian Schelter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. (arXiv:2106.01635v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01635</id>
        <link href="http://arxiv.org/abs/2106.01635"/>
        <updated>2021-06-04T01:12:28.281Z</updated>
        <summary type="html"><![CDATA[In this paper we implement and compare 7 different data augmentation
strategies for the task of automatic scoring of children's ability to
understand others' thoughts, feelings, and desires (or "mindreading").

We recruit in-domain experts to re-annotate augmented samples and determine
to what extent each strategy preserves the original rating. We also carry out
multiple experiments to measure how much each augmentation strategy improves
the performance of automatic scoring systems. To determine the capabilities of
automatic systems to generalize to unseen data, we create UK-MIND-20 - a new
corpus of children's performance on tests of mindreading, consisting of 10,320
question-answer pairs.

We obtain a new state-of-the-art performance on the MIND-CA corpus, improving
macro-F1-score by 6 points. Results indicate that both the number of training
examples and the quality of the augmentation strategies affect the performance
of the systems. The task-specific augmentations generally outperform
task-agnostic augmentations. Automatic augmentations based on vectors (GloVe,
FastText) perform the worst.

We find that systems trained on MIND-CA generalize well to UK-MIND-20. We
demonstrate that data augmentation strategies also improve the performance on
unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1"&gt;Venelin Kovatchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_P/0/1/0/all/0/1"&gt;Phillip Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mark Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devine_R/0/1/0/all/0/1"&gt;Rory Devine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Memorization in Adversarial Training. (arXiv:2106.01606v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01606</id>
        <link href="http://arxiv.org/abs/2106.01606"/>
        <updated>2021-06-04T01:12:28.273Z</updated>
        <summary type="html"><![CDATA[It is well known that deep learning models have a propensity for fitting the
entire training set even with random labels, which requires memorization of
every training sample. In this paper, we investigate the memorization effect in
adversarial training (AT) for promoting a deeper understanding of capacity,
convergence, generalization, and especially robust overfitting of adversarially
trained classifiers. We first demonstrate that deep networks have sufficient
capacity to memorize adversarial examples of training data with completely
random labels, but not all AT algorithms can converge under the extreme
circumstance. Our study of AT with random labels motivates further analyses on
the convergence and generalization of AT. We find that some AT methods suffer
from a gradient instability issue, and the recently suggested complexity
measures cannot explain robust generalization by considering models trained on
random labels. Furthermore, we identify a significant drawback of memorization
in AT that it could result in robust overfitting. We then propose a new
mitigation algorithm motivated by detailed memorization analyses. Extensive
experiments on various datasets validate the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhijie Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment. (arXiv:2106.01642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01642</id>
        <link href="http://arxiv.org/abs/2106.01642"/>
        <updated>2021-06-04T01:12:28.265Z</updated>
        <summary type="html"><![CDATA[In semi-supervised graph-based binary classifier learning, a subset of known
labels $\hat{x}_i$ are used to infer unknown labels, assuming that the label
signal $x$ is smooth with respect to a similarity graph specified by a
Laplacian matrix. When restricting labels $x_i$ to binary values, the problem
is NP-hard. While a conventional semi-definite programming (SDP) relaxation can
be solved in polynomial time using, for example, the alternating direction
method of multipliers (ADMM), the complexity of iteratively projecting a
candidate matrix $M$ onto the positive semi-definite (PSD) cone ($M \succeq 0$)
remains high. In this paper, leveraging a recent linear algebraic theory called
Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free
method by solving a sequence of linear programs (LP) instead. Specifically, we
first recast the SDP relaxation to its SDP dual, where a feasible solution $H
\succeq 0$ can be interpreted as a Laplacian matrix corresponding to a balanced
signed graph sans the last node. To achieve graph balance, we split the last
node into two that respectively contain the original positive and negative
edges, resulting in a new Laplacian $\bar{H}$. We repose the SDP dual for
solution $\bar{H}$, then replace the PSD cone constraint $\bar{H} \succeq 0$
with linear constraints derived from GDPA -- sufficient conditions to ensure
$\bar{H}$ is PSD -- so that the optimization becomes an LP per iteration.
Finally, we extract predicted labels from our converged LP solution $\bar{H}$.
Experiments show that our algorithm enjoyed a $40\times$ speedup on average
over the next fastest scheme while retaining comparable label prediction
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1"&gt;Gene Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Wai-tian Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning. (arXiv:2106.01613v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01613</id>
        <link href="http://arxiv.org/abs/2106.01613"/>
        <updated>2021-06-04T01:12:28.247Z</updated>
        <summary type="html"><![CDATA[Deployment of machine learning models in real high-risk settings (e.g.
healthcare) often depends not only on model's accuracy but also on its
fairness, robustness and interpretability. Generalized Additive Models (GAMs)
have a long history of use in these high-risk domains, but lack desirable
features of deep learning such as differentiability and scalability. In this
work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that
scale well to large datasets, while remaining interpretable and accurate. We
show that our proposed models have comparable accuracy to other
non-interpretable models, and outperform other GAMs on large datasets. We also
show that our models are more accurate in self-supervised learning setting when
access to labeled data is limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1"&gt;Rich Caruana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-04T01:12:28.239Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleeping Combinatorial Bandits. (arXiv:2106.01624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01624</id>
        <link href="http://arxiv.org/abs/2106.01624"/>
        <updated>2021-06-04T01:12:28.231Z</updated>
        <summary type="html"><![CDATA[In this paper, we study an interesting combination of sleeping and
combinatorial stochastic bandits. In the mixed model studied here, at each
discrete time instant, an arbitrary \emph{availability set} is generated from a
fixed set of \emph{base} arms. An algorithm can select a subset of arms from
the \emph{availability set} (sleeping bandits) and receive the corresponding
reward along with semi-bandit feedback (combinatorial bandits).

We adapt the well-known CUCB algorithm in the sleeping combinatorial bandits
setting and refer to it as \CSUCB. We prove -- under mild smoothness conditions
-- that the \CSUCB\ algorithm achieves an $O(\log (T))$ instance-dependent
regret guarantee. We further prove that (i) when the range of the rewards is
bounded, the regret guarantee of \CSUCB\ algorithm is $O(\sqrt{T \log (T)})$
and (ii) the instance-independent regret is $O(\sqrt[3]{T^2 \log(T)})$ in a
general setting. Our results are quite general and hold under general
environments -- such as non-additive reward functions, volatile arm
availability, a variable number of base-arms to be pulled -- arising in
practical applications. We validate the proven theoretical guarantees through
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1"&gt;Kumar Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghalme_G/0/1/0/all/0/1"&gt;Ganesh Ghalme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1"&gt;Sujit Gujar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narahari_Y/0/1/0/all/0/1"&gt;Yadati Narahari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu. (arXiv:2106.01674v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01674</id>
        <link href="http://arxiv.org/abs/2106.01674"/>
        <updated>2021-06-04T01:12:28.225Z</updated>
        <summary type="html"><![CDATA[In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiaochao Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hao Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangxing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guobao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiwei Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Daxiang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Phase Retrieval. (arXiv:2106.01660v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01660</id>
        <link href="http://arxiv.org/abs/2106.01660"/>
        <updated>2021-06-04T01:12:28.218Z</updated>
        <summary type="html"><![CDATA[We study a bandit version of phase retrieval where the learner chooses
actions $(A_t)_{t=1}^n$ in the $d$-dimensional unit ball and the expected
reward is $\langle A_t, \theta_\star\rangle^2$ where $\theta_\star \in \mathbb
R^d$ is an unknown parameter vector. We prove that the minimax cumulative
regret in this problem is $\smash{\tilde \Theta(d \sqrt{n})}$, which improves
on the best known bounds by a factor of $\smash{\sqrt{d}}$. We also show that
the minimax simple regret is $\smash{\tilde \Theta(d / \sqrt{n})}$ and that
this is only achievable by an adaptive algorithm. Our analysis shows that an
apparently convincing heuristic for guessing lower bounds can be misleading and
that uniform bounds on the information ratio for information-directed sampling
are not sufficient for optimal regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hao_B/0/1/0/all/0/1"&gt;Botao Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy student-teacher training for robust keyword spotting. (arXiv:2106.01604v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01604</id>
        <link href="http://arxiv.org/abs/2106.01604"/>
        <updated>2021-06-04T01:12:28.195Z</updated>
        <summary type="html"><![CDATA[We propose self-training with noisy student-teacher approach for streaming
keyword spotting, that can utilize large-scale unlabeled data and aggressive
data augmentation. The proposed method applies aggressive data augmentation
(spectral augmentation) on the input of both student and teacher and utilize
unlabeled data at scale, which significantly boosts the accuracy of student
against challenging conditions. Such aggressive augmentation usually degrades
model performance when used with supervised training with hard-labeled data.
Experiments show that aggressive spec augmentation on baseline supervised
training method degrades accuracy, while the proposed self-training with noisy
student-teacher training improves accuracy of some difficult-conditioned test
sets by as much as 60%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyun-Jin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Pai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subrahmanya_N/0/1/0/all/0/1"&gt;Niranjan Subrahmanya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cybersecurity Information Exchange with Privacy (CYBEX-P) and TAHOE -- A Cyberthreat Language. (arXiv:2106.01632v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.01632</id>
        <link href="http://arxiv.org/abs/2106.01632"/>
        <updated>2021-06-04T01:12:28.189Z</updated>
        <summary type="html"><![CDATA[Cybersecurity information sharing (CIS) is envisioned to protect
organizations more effectively from advanced cyber attacks. However, a
completely automated CIS platform is not widely adopted. The major challenges
are: (1) the absence of a robust cyber threat language (CTL) and (2) the
concerns over data privacy. This work introduces Cybersecurity Information
Exchangewith Privacy (CYBEX-P), as a CIS framework, to tackle these challenges.
CYBEX-P allows organizations to share heterogeneous data with granular,
attribute based privacy control. It correlates the data to automatically
generate intuitive reports and defensive rules. To achieve such versatility, we
have developed TAHOE - a graph based CTL. TAHOE is a structure for
storing,sharing and analyzing threat data. It also intrinsically correlates the
data. We have further developed a universal Threat Data Query Language (TDQL).
In this paper, we propose the system architecture for CYBEX-P. We then discuss
its scalability and privacy features along with a use case of CYBEX-P providing
Infrastructure as a Service (IaaS). We further introduce TAHOE& TDQL as better
alternatives to existing CTLs and formulate ThreatRank - an algorithm to detect
new malicious even]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadique_F/0/1/0/all/0/1"&gt;Farhan Sadique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astaburuaga_I/0/1/0/all/0/1"&gt;Ignacio Astaburuaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaul_R/0/1/0/all/0/1"&gt;Raghav Kaul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Shamik Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badsha_S/0/1/0/all/0/1"&gt;Shahriar Badsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnebly_J/0/1/0/all/0/1"&gt;James Schnebly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassell_A/0/1/0/all/0/1"&gt;Adam Cassell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1"&gt;Jeff Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latourrette_N/0/1/0/all/0/1"&gt;Nancy Latourrette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dascalu_S/0/1/0/all/0/1"&gt;Sergiu M. Dascalu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing Directed Acyclic Graph via Structural, Supervised and Generative Adversarial Learning. (arXiv:2106.01474v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01474</id>
        <link href="http://arxiv.org/abs/2106.01474"/>
        <updated>2021-06-04T01:12:28.182Z</updated>
        <summary type="html"><![CDATA[In this article, we propose a new hypothesis testing method for directed
acyclic graph (DAG). While there is a rich class of DAG estimation methods,
there is a relative paucity of DAG inference solutions. Moreover, the existing
methods often impose some specific model structures such as linear models or
additive models, and assume independent data observations. Our proposed test
instead allows the associations among the random variables to be nonlinear and
the data to be time-dependent. We build the test based on some highly flexible
neural networks learners. We establish the asymptotic guarantees of the test,
while allowing either the number of subjects or the number of time points for
each subject to diverge to infinity. We demonstrate the efficacy of the test
through simulations and a brain connectivity network analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengchun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yunzhe Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1"&gt;Lexin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperbolically-Discounted Reinforcement Learning on Reward-Punishment Framework. (arXiv:2106.01516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01516</id>
        <link href="http://arxiv.org/abs/2106.01516"/>
        <updated>2021-06-04T01:12:28.172Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new reinforcement learning with hyperbolic discounting.
Combining a new temporal difference error with the hyperbolic discounting in
recursive manner and reward-punishment framework, a new scheme to learn the
optimal policy is derived. In simulations, it is found that the proposal
outperforms the standard reinforcement learning, although the performance
depends on the design of reward and punishment. In addition, the averages of
discount factors w.r.t. reward and punishment are different from each other,
like a sign effect in animal behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1"&gt;Taisuke Kobayashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia. (arXiv:2106.01601v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01601</id>
        <link href="http://arxiv.org/abs/2106.01601"/>
        <updated>2021-06-04T01:12:28.166Z</updated>
        <summary type="html"><![CDATA[Human activities can be seen as sequences of events, which are crucial to
understanding societies. Disproportional event distribution for different
demographic groups can manifest and amplify social stereotypes, and potentially
jeopardize the ability of members in some groups to pursue certain goals. In
this paper, we present the first event-centric study of gender biases in a
Wikipedia corpus. To facilitate the study, we curate a corpus of career and
personal life descriptions with demographic information consisting of 7,854
fragments from 10,412 celebrities. Then we detect events with a
state-of-the-art event detection model, calibrate the results using
strategically generated templates, and extract events that have asymmetric
associations with genders. Our study discovers that the Wikipedia pages tend to
intermingle personal life events with professional events for females but not
for males, which calls for the awareness of the Wikipedia community to
formalize guidelines and train the editors to mind the implicit biases that
contributors carry. Our work also lays the foundation for future works on
quantifying and discovering event biases at the corpus level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discussion On the Validity of Manifold Learning. (arXiv:2106.01608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01608</id>
        <link href="http://arxiv.org/abs/2106.01608"/>
        <updated>2021-06-04T01:12:28.146Z</updated>
        <summary type="html"><![CDATA[Dimensionality reduction (DR) and manifold learning (ManL) have been applied
extensively in many machine learning tasks, including signal processing, speech
recognition, and neuroinformatics. However, the understanding of whether DR and
ManL models can generate valid learning results remains unclear. In this work,
we investigate the validity of learning results of some widely used DR and ManL
methods through the chart mapping function of a manifold. We identify a
fundamental problem of these methods: the mapping functions induced by these
methods violate the basic settings of manifolds, and hence they are not
learning manifold in the mathematical sense. To address this problem, we
provide a provably correct algorithm called fixed points Laplacian mapping
(FPLM), that has the geometric guarantee to find a valid manifold
representation (up to a homeomorphism). Combining one additional
condition(orientation preserving), we discuss a sufficient condition for an
algorithm to be bijective for any d-simplex decomposition result on a
d-manifold. However, constructing such a mapping function and its computational
method satisfying these conditions is still an open problem in mathematics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_A/0/1/0/all/0/1"&gt;Andi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junbin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Representation Learning for Markov Decision Processes. (arXiv:2106.01655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01655</id>
        <link href="http://arxiv.org/abs/2106.01655"/>
        <updated>2021-06-04T01:12:28.140Z</updated>
        <summary type="html"><![CDATA[In this paper we present a novel method for learning hierarchical
representations of Markov decision processes. Our method works by partitioning
the state space into subsets, and defines subtasks for performing transitions
between the partitions. We formulate the problem of partitioning the state
space as an optimization problem that can be solved using gradient descent
given a set of sampled trajectories, making our method suitable for
high-dimensional problems with large state spaces. We empirically validate the
method, by showing that it can successfully learn a useful hierarchical
representation in a navigation domain. Once learned, the hierarchical
representation can be used to solve different tasks in the given domain, thus
generalizing knowledge across tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Steccanella_L/0/1/0/all/0/1"&gt;Lorenzo Steccanella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Totaro_S/0/1/0/all/0/1"&gt;Simone Totaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1"&gt;Anders Jonsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation. (arXiv:2106.01597v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01597</id>
        <link href="http://arxiv.org/abs/2106.01597"/>
        <updated>2021-06-04T01:12:28.134Z</updated>
        <summary type="html"><![CDATA[Despite the recent advancement in NLP research, cross-lingual transfer for
natural language generation is relatively understudied. In this work, we
transfer supervision from high resource language (HRL) to multiple low-resource
languages (LRLs) for natural language generation (NLG). We consider four NLG
tasks (text summarization, question generation, news headline generation, and
distractor generation) and three syntactically diverse languages, i.e.,
English, Hindi, and Japanese. We propose an unsupervised cross-lingual language
generation framework (called ZmBART) that does not use any parallel or
pseudo-parallel/back-translated data. In this framework, we further pre-train
mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task
using monolingual data of three languages. The objective function of the
auxiliary task is close to the target tasks which enriches the multi-lingual
latent representation of mBART and provides good initialization for target
tasks. Then, this model is fine-tuned with task-specific supervised English
data and directly evaluated with low-resource languages in the Zero-shot
setting. To overcome catastrophic forgetting and spurious correlation issues,
we applied freezing model component and data argumentation approaches
respectively. This simple modeling approach gave us promising results.We
experimented with few-shot training (with 1000 supervised data points) which
boosted the model performance further. We performed several ablations and
cross-lingual transferability analyses to demonstrate the robustness of ZmBART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1"&gt;Kaushal Kumar Maurya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1"&gt;Maunendra Sankar Desarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1"&gt;Yoshinobu Kano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1"&gt;Kumari Deepshikha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided Supervised Contrastive Learning for Semantic Segmentation. (arXiv:2106.01596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01596</id>
        <link href="http://arxiv.org/abs/2106.01596"/>
        <updated>2021-06-04T01:12:28.127Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has shown superior performance in embedding global and
spatial invariant features in computer vision (e.g., image classification).
However, its overall success of embedding local and spatial variant features is
still limited, especially for semantic segmentation. In a per-pixel prediction
task, more than one label can exist in a single image for segmentation (e.g.,
an image contains both cat, dog, and grass), thereby it is difficult to define
'positive' or 'negative' pairs in a canonical contrastive learning setting. In
this paper, we propose an attention-guided supervised contrastive learning
approach to highlight a single semantic object every time as the target. With
our design, the same image can be embedded to different semantic clusters with
semantic attention (i.e., coerce semantic masks) as an additional input
channel. To achieve such attention, a novel two-stage training strategy is
presented. We evaluate the proposed method on multi-organ medical image
segmentation task, as our major task, with both in-house data and BTCV 2015
datasets. Comparing with the supervised and semi-supervised training
state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields
substantial improvement of 5.53% and 6.09% in Dice score for both medical image
segmentation cohorts respectively. The performance of the proposed method on
natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%
substantial improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shunxing Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes. (arXiv:2106.01487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01487</id>
        <link href="http://arxiv.org/abs/2106.01487"/>
        <updated>2021-06-04T01:12:28.120Z</updated>
        <summary type="html"><![CDATA[Learning binary representations of instances and classes is a classical
problem with several high potential applications. In modern settings, the
compression of high-dimensional neural representations to low-dimensional
binary codes is a challenging task and often require large bit-codes to be
accurate. In this work, we propose a novel method for Learning Low-dimensional
binary Codes (LLC) for instances as well as classes. Our method does not
require any side-information, like annotated attributes or label meta-data, and
learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The
learnt codes are super-efficient while still ensuring nearly optimal
classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the
learnt codes capture intrinsically important features in the data, by
discovering an intuitive taxonomy over classes. We further quantitatively
measure the quality of our codes by applying it to the efficient image
retrieval as well as out-of-distribution (OOD) detection problems. For
ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit
HashNet using only 10 bits and also are as accurate as 10 dimensional real
representations. Finally, our learnt binary codes can perform OOD detection,
out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune
its threshold, while we require none. Code and pre-trained models are available
at https://github.com/RAIVNLab/LLC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1"&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1"&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1"&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somani_R/0/1/0/all/0/1"&gt;Raghav Somani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain. (arXiv:2106.01491v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01491</id>
        <link href="http://arxiv.org/abs/2106.01491"/>
        <updated>2021-06-04T01:12:28.113Z</updated>
        <summary type="html"><![CDATA[Crowdworker-constructed natural language inference (NLI) datasets have been
found to contain statistical artifacts associated with the annotation process
that allow hypothesis-only classifiers to achieve better-than-random
performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).
We investigate whether MedNLI, a physician-annotated dataset with premises
extracted from clinical notes, contains such artifacts (Romanov and Shivade,
2018). We find that entailed hypotheses contain generic versions of specific
concepts in the premise, as well as modifiers related to responsiveness,
duration, and probability. Neutral hypotheses feature conditions and behaviors
that co-occur with, or cause, the condition(s) in the premise. Contradiction
hypotheses feature explicit negation of the premise and implicit negation via
assertion of good health. Adversarial filtering demonstrates that performance
degrades when evaluated on the difficult subset. We provide partition
information and recommendations for alternative dataset construction strategies
for knowledge-intensive domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1"&gt;Christine Herlihy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1"&gt;Rachel Rudinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Network Learning with Partially Aligned Graph Convolutional Networks. (arXiv:2106.01583v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01583</id>
        <link href="http://arxiv.org/abs/2106.01583"/>
        <updated>2021-06-04T01:12:28.094Z</updated>
        <summary type="html"><![CDATA[Graph neural networks have been widely used for learning representations of
nodes for many downstream tasks on graph data. Existing models were designed
for the nodes on a single graph, which would not be able to utilize information
across multiple graphs. The real world does have multiple graphs where the
nodes are often partially aligned. For examples, knowledge graphs share a
number of named entities though they may have different relation schema;
collaboration networks on publications and awarded projects share some
researcher nodes who are authors and investigators, respectively; people use
multiple web services, shopping, tweeting, rating movies, and some may register
the same email account across the platforms. In this paper, I propose partially
aligned graph convolutional networks to learn node representations across the
models. I investigate multiple methods (including model sharing,
regularization, and alignment reconstruction) as well as theoretical analysis
to positively transfer knowledge across the (small) set of partially aligned
nodes. Extensive experiments on real-world knowledge graphs and collaboration
networks show the superior performance of our proposed methods on relation
classification and link prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Heterogeneous Systems with AI Planning Heuristics and Machine Learning: A Performance and Energy Aware Approach. (arXiv:2106.01441v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.01441</id>
        <link href="http://arxiv.org/abs/2106.01441"/>
        <updated>2021-06-04T01:12:28.088Z</updated>
        <summary type="html"><![CDATA[Heterogeneous computing systems provide high performance and energy
efficiency. However, to optimally utilize such systems, solutions that
distribute the work across host CPUs and accelerating devices are needed. In
this paper, we present a performance and energy aware approach that combines AI
planning heuristics for parameter space exploration with a machine learning
model for performance and energy evaluation to determine a near-optimal system
configuration. For data-parallel applications our approach determines a
near-optimal host-device distribution of work, number of processing units
required and the corresponding scheduling strategy. We evaluate our approach
for various heterogeneous systems accelerated with GPU or the Intel Xeon Phi.
The experimental results demonstrate that our approach finds a near-optimal
system configuration by evaluating only about 7% of reasonable configurations.
Furthermore, the performance per Joule estimation of system configurations
using our machine learning model is more than 1000x faster compared to the
system evaluation by program execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memeti_S/0/1/0/all/0/1"&gt;Suejb Memeti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pllana_S/0/1/0/all/0/1"&gt;Sabri Pllana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not All Knowledge Is Created Equal. (arXiv:2106.01489v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01489</id>
        <link href="http://arxiv.org/abs/2106.01489"/>
        <updated>2021-06-04T01:12:28.038Z</updated>
        <summary type="html"><![CDATA[Mutual knowledge distillation (MKD) improves a model by distilling knowledge
from another model. However, not all knowledge is certain and correct,
especially under adverse conditions. For example, label noise usually leads to
less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge
misleads the learning rather than helps. This problem can be handled by two
aspects: (i) improving the reliability of a model where the knowledge is from
(i.e., knowledge source's reliability); (ii) selecting reliable knowledge for
distillation. In the literature, making a model more reliable is widely studied
while selective MKD receives little attention. Therefore, we focus on studying
selective MKD and highlight its importance in this work.

Concretely, a generic MKD framework, Confident knowledge selection followed
by Mutual Distillation (CMD), is designed. The key component of CMD is a
generic knowledge selection formulation, making the selection threshold either
static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special
cases: zero knowledge and all knowledge, leading to a unified MKD framework. We
empirically find CMD-P performs better than CMD-S. The main reason is that a
model's knowledge upgrades and becomes confident as the training progresses.

Extensive experiments are present to demonstrate the effectiveness of CMD and
thoroughly justify the design of CMD. For example, CMD-P obtains new
state-of-the-art results in robustness against label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinshao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Di Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1"&gt;Neil M. Robertson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines. (arXiv:2106.01506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01506</id>
        <link href="http://arxiv.org/abs/2106.01506"/>
        <updated>2021-06-04T01:12:28.029Z</updated>
        <summary type="html"><![CDATA[Despite their ubiquity in core AI fields like natural language processing,
the mechanics of deep attention-based neural networks like the Transformer
model are not fully understood. In this article, we present a new perspective
towards understanding how Transformers work. In particular, we show that the
"dot-product attention" that is the core of the Transformer's operation can be
characterized as a kernel learning method on a pair of Banach spaces. In
particular, the Transformer's kernel is characterized as having an infinite
feature dimension. Along the way we consider an extension of the standard
kernel learning problem to a binary setting, where data come from two input
domains and a response is defined for every cross-domain pair. We prove a new
representer theorem for these binary kernel machines with non-Mercer
(indefinite, asymmetric) kernels (implying that the functions learned are
elements of reproducing kernel Banach spaces rather than Hilbert spaces), and
also prove a new universal approximation theorem showing that the Transformer
calculation can learn any binary non-Mercer reproducing kernel Banach space
pair. We experiment with new kernels in Transformers, and obtain results that
suggest the infinite dimensionality of the standard Transformer kernel is
partially responsible for its performance. This paper's results provide a new
theoretical understanding of a very important but poorly understood model in
modern machine~learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_M/0/1/0/all/0/1"&gt;Matthew A. Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Luna: Linear Unified Nested Attention. (arXiv:2106.01540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01540</id>
        <link href="http://arxiv.org/abs/2106.01540"/>
        <updated>2021-06-04T01:12:28.018Z</updated>
        <summary type="html"><![CDATA[The quadratic computational and memory complexities of the Transformer's
attention mechanism have limited its scalability for modeling long sequences.
In this paper, we propose Luna, a linear unified nested attention mechanism
that approximates softmax attention with two nested linear attention functions,
yielding only linear (as opposed to quadratic) time and space complexity.
Specifically, with the first attention function, Luna packs the input sequence
into a sequence of fixed length. Then, the packed sequence is unpacked using
the second attention function. As compared to a more traditional attention
mechanism, Luna introduces an additional sequence with a fixed length as input
and an additional corresponding output, which allows Luna to perform attention
operation linearly, while also storing adequate contextual information. We
perform extensive evaluations on three benchmarks of sequence modeling tasks:
long-context sequence modeling, neural machine translation and masked language
modeling for large-scale pretraining. Competitive or even better experimental
results demonstrate both the effectiveness and efficiency of Luna compared to a
variety]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuezhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sinong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chunting Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Answering Over Temporal Knowledge Graphs. (arXiv:2106.01515v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01515</id>
        <link href="http://arxiv.org/abs/2106.01515"/>
        <updated>2021-06-04T01:12:28.000Z</updated>
        <summary type="html"><![CDATA[Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by
providing temporal scopes (start and end times) on each edge in the KG. While
Question Answering over KG (KGQA) has received some attention from the research
community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored
area. Lack of broad coverage datasets has been another factor limiting progress
in this area. We address this challenge by presenting CRONQUESTIONS, the
largest known Temporal KGQA dataset, clearly stratified into buckets of
structural complexity. CRONQUESTIONS expands the only known previous dataset by
a factor of 340x. We find that various state-of-the-art KGQA methods fall far
short of the desired performance on this new dataset. In response, we also
propose CRONKGQA, a transformer-based solution that exploits recent advances in
Temporal KG embeddings, and achieves performance superior to all baselines,
with an increase of 120% in accuracy over the next best performing method.
Through extensive experiments, we give detailed insights into the workings of
CRONKGQA, as well as situations where significant further improvements appear
possible. In addition to the dataset, we have released our code as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1"&gt;Apoorv Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1"&gt;Soumen Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIMLR: Machine Learning inside the SIR model for COVID-19 Forecasting. (arXiv:2106.01590v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01590</id>
        <link href="http://arxiv.org/abs/2106.01590"/>
        <updated>2021-06-04T01:12:27.972Z</updated>
        <summary type="html"><![CDATA[Accurate forecasts of the number of newly infected people during an epidemic
are critical for making effective timely decisions. This paper addresses this
challenge using the SIMLR model, which incorporates machine learning (ML) into
the epidemiological SIR model. For each region, SIMLR tracks the changes in the
policies implemented at the government level, which it uses to estimate the
time-varying parameters of an SIR model for forecasting the number of new
infections 1- to 4-weeks in advance.It also forecasts the probability of
changes in those government policies at each of these future times, which is
essential for the longer-range forecasts. We applied SIMLR to data from regions
in Canada and in the United States,and show that its MAPE (mean average
percentage error) performance is as good as SOTA forecasting models, with the
added advantage of being an interpretable model. We expect that this approach
will be useful not only for forecasting COVID-19 infections, but also in
predicting the evolution of other infectious diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vega_R/0/1/0/all/0/1"&gt;Roberto Vega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flores_L/0/1/0/all/0/1"&gt;Leonardo Flores&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greiner_R/0/1/0/all/0/1"&gt;Russell Greiner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Limitations of Limited Context for Constituency Parsing. (arXiv:2106.01580v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01580</id>
        <link href="http://arxiv.org/abs/2106.01580"/>
        <updated>2021-06-04T01:12:27.951Z</updated>
        <summary type="html"><![CDATA[Incorporating syntax into neural approaches in NLP has a multitude of
practical and scientific benefits. For instance, a language model that is
syntax-aware is likely to be able to produce better samples; even a
discriminative model like BERT with a syntax module could be used for core NLP
tasks like unsupervised syntactic parsing. Rapid progress in recent years was
arguably spurred on by the empirical success of the Parsing-Reading-Predict
architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM
of (Shen et al., 2019). Most notably, this is the first time neural approaches
were able to successfully perform unsupervised syntactic parsing (evaluated by
various metrics like F-1 score).

However, even heuristic (much less fully mathematical) understanding of why
and when these architectures work is lagging severely behind. In this work, we
answer representational questions raised by the architectures in (Shen et al.,
2018a, 2019), as well as some transition-based syntax-aware language models
(Dyer et al., 2016): what kind of syntactic structure can current neural
approaches to syntax represent? Concretely, we ground this question in the
sandbox of probabilistic context-free-grammars (PCFGs), and identify a key
aspect of the representational power of these approaches: the amount and
directionality of context that the predictor has access to when forced to make
parsing decision. We show that with limited context (either bounded, or
unidirectional), there are PCFGs, for which these approaches cannot represent
the max-likelihood parse; conversely, if the context is unlimited, they can
represent the max-likelihood parse of any PCFG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Assisted Learning. (arXiv:2106.01425v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01425</id>
        <link href="http://arxiv.org/abs/2106.01425"/>
        <updated>2021-06-04T01:12:27.944Z</updated>
        <summary type="html"><![CDATA[In distributed settings, collaborations between different entities, such as
financial institutions, medical centers, and retail markets, are crucial to
providing improved service and performance. However, the underlying entities
may have little interest in sharing their private data, proprietary models, and
objective functions. These privacy requirements have created new challenges for
collaboration. In this work, we propose Gradient Assisted Learning (GAL), a new
method for various entities to assist each other in supervised learning tasks
without sharing data, models, and objective functions. In this framework, all
participants collaboratively optimize the aggregate of local loss functions,
and each participant autonomously builds its own model by iteratively fitting
the gradients of the objective function. Experimental studies demonstrate that
Gradient Assisted Learning can achieve performance close to centralized
learning when all data, models, and objective functions are fully disclosed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_E/0/1/0/all/0/1"&gt;Enmao Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jie Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Investigation of KB-Text Embedding Alignment at Scale. (arXiv:2106.01586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01586</id>
        <link href="http://arxiv.org/abs/2106.01586"/>
        <updated>2021-06-04T01:12:27.937Z</updated>
        <summary type="html"><![CDATA[Knowledge bases (KBs) and text often contain complementary knowledge: KBs
store structured knowledge that can support long range reasoning, while text
stores more comprehensive and timely knowledge in an unstructured way.
Separately embedding the individual knowledge sources into vector spaces has
demonstrated tremendous successes in encoding the respective knowledge, but how
to jointly embed and reason with both knowledge sources to fully leverage the
complementary information is still largely an open problem. We conduct a
large-scale, systematic investigation of aligning KB and text embeddings for
joint reasoning. We set up a novel evaluation framework with two evaluation
tasks, few-shot link prediction and analogical reasoning, and evaluate an array
of KB-text embedding alignment methods. We also demonstrate how such alignment
can infuse textual information into KB embeddings for more accurate link
prediction on emerging entities and events, using COVID-19 as a case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1"&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahrami_M/0/1/0/all/0/1"&gt;Mehdi Bahrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yu Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IoT Solutions with Multi-Sensor Fusion and Signal-Image Encoding for Secure Data Transfer and Decision Making. (arXiv:2106.01497v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.01497</id>
        <link href="http://arxiv.org/abs/2106.01497"/>
        <updated>2021-06-04T01:12:27.919Z</updated>
        <summary type="html"><![CDATA[Deployment of Internet of Things (IoT) devices and Data Fusion techniques
have gained popularity in public and government domains. This usually requires
capturing and consolidating data from multiple sources. As datasets do not
necessarily originate from identical sensors, fused data typically results in a
complex data problem. Because military is investigating how heterogeneous IoT
devices can aid processes and tasks, we investigate a multi-sensor approach.
Moreover, we propose a signal to image encoding approach to transform
information (signal) to integrate (fuse) data from IoT wearable devices to an
image which is invertible and easier to visualize supporting decision making.
Furthermore, we investigate the challenge of enabling an intelligent
identification and detection operation and demonstrate the feasibility of the
proposed Deep Learning and Anomaly Detection models that can support future
application that utilizes hand gesture data from wearable devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dennison_M/0/1/0/all/0/1"&gt;Mark Dennison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raglin_A/0/1/0/all/0/1"&gt;Adrienne Raglin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations. (arXiv:2106.01548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01548</id>
        <link href="http://arxiv.org/abs/2106.01548"/>
        <updated>2021-06-04T01:12:27.912Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pretraining and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pretraining or strong data
augmentations. They also possess more perceptive attention maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins. (arXiv:2106.01501v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.01501</id>
        <link href="http://arxiv.org/abs/2106.01501"/>
        <updated>2021-06-04T01:12:27.903Z</updated>
        <summary type="html"><![CDATA[Structured data, or data that adheres to a pre-defined schema, can suffer
from fragmented context: information describing a single entity can be
scattered across multiple datasets or tables tailored for specific business
needs, with no explicit linking keys (e.g., primary key-foreign key
relationships or heuristic functions). Context enrichment, or rebuilding
fragmented context, using keyless joins is an implicit or explicit step in
machine learning (ML) pipelines over structured data sources. This process is
tedious, domain-specific, and lacks support in now-prevalent no-code ML systems
that let users create ML pipelines using just input data and high-level
configuration files. In response, we propose Ember, a system that abstracts and
automates keyless joins to generalize context enrichment. Our key insight is
that Ember can enable a general keyless join operator by constructing an index
populated with task-specific embeddings. Ember learns these embeddings by
leveraging Transformer-based representation learning techniques. We describe
our core architectural principles and operators when developing Ember, and
empirically demonstrate that Ember allows users to develop no-code pipelines
for five domains, including search, recommendation and question answering, and
can exceed alternatives by up to 39% recall, with as little as a single line
configuration change.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1"&gt;Sahaana Suri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1"&gt;Ihab F. Ilyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1"&gt;Theodoros Rekatsinas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Learning Creates a Fusion of Modeling Cultures. (arXiv:2106.01485v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01485</id>
        <link href="http://arxiv.org/abs/2106.01485"/>
        <updated>2021-06-04T01:12:27.895Z</updated>
        <summary type="html"><![CDATA[The past two decades have witnessed the great success of the algorithmic
modeling framework advocated by Breiman et al. (2001). Nevertheless, the
excellent prediction performance of these black-box models rely heavily on the
availability of strong supervision, i.e. a large set of accurate and exact
ground-truth labels. In practice, strong supervision can be unavailable or
expensive, which calls for modeling techniques under weak supervision. In this
comment, we summarize the key concepts in weakly supervised learning and
discuss some recent developments in the field. Using algorithmic modeling alone
under a weak supervision might lead to unstable and misleading results. A
promising direction would be integrating the data modeling culture into such a
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chengliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Gan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tian Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalizing Flows for Knockoff-free Controlled Feature Selection. (arXiv:2106.01528v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01528</id>
        <link href="http://arxiv.org/abs/2106.01528"/>
        <updated>2021-06-04T01:12:27.887Z</updated>
        <summary type="html"><![CDATA[The goal of controlled feature selection is to discover the features a
response depends on while limiting the proportion of false discoveries to a
predefined level. Recently, multiple methods have been proposed that use deep
learning to generate knockoffs for controlled feature selection through the
Model-X knockoff framework. We demonstrate, however, that these methods often
fail to control the false discovery rate (FDR). There are two reasons for this
shortcoming. First, these methods often learn inaccurate models of features.
Second, the "swap" property, which is required for knockoffs to be valid, is
often not well enforced. We propose a new procedure called FlowSelect that
remedies both of these problems. To more accurately model the features,
FlowSelect uses normalizing flows, the state-of-the-art method for density
estimation. To circumvent the need to enforce the swap property, FlowSelect
uses a novel MCMC-based procedure to directly compute p-values for each
feature. Asymptotically, FlowSelect controls the FDR exactly. Empirically,
FlowSelect controls the FDR well on both synthetic and semi-synthetic
benchmarks, whereas competing knockoff-based approaches fail to do so.
FlowSelect also demonstrates greater power on these benchmarks. Additionally,
using data from a genome-wide association study of soybeans, FlowSelect
correctly infers the genetic variants associated with specific soybean traits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hansen_D/0/1/0/all/0/1"&gt;Derek Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Manzo_B/0/1/0/all/0/1"&gt;Brian Manzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Regier_J/0/1/0/all/0/1"&gt;Jeffrey Regier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Black Hole Properties from Astronomical Multivariate Time Series with Bayesian Attentive Neural Processes. (arXiv:2106.01450v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2106.01450</id>
        <link href="http://arxiv.org/abs/2106.01450"/>
        <updated>2021-06-04T01:12:27.736Z</updated>
        <summary type="html"><![CDATA[Among the most extreme objects in the Universe, active galactic nuclei (AGN)
are luminous centers of galaxies where a black hole feeds on surrounding
matter. The variability patterns of the light emitted by an AGN contain
information about the physical properties of the underlying black hole.
Upcoming telescopes will observe over 100 million AGN in multiple broadband
wavelengths, yielding a large sample of multivariate time series with long gaps
and irregular sampling. We present a method that reconstructs the AGN time
series and simultaneously infers the posterior probability density distribution
(PDF) over the physical quantities of the black hole, including its mass and
luminosity. We apply this method to a simulated dataset of 11,000 AGN and
report precision and accuracy of 0.4 dex and 0.3 dex in the inferred black hole
mass. This work is the first to address probabilistic time series
reconstruction and parameter inference for AGN in an end-to-end fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Park_J/0/1/0/all/0/1"&gt;Ji Won Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Villar_A/0/1/0/all/0/1"&gt;Ashley Villar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yan-Fei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1"&gt;Shirley Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1"&gt;Joshua Yao-Yu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Marshall_P/0/1/0/all/0/1"&gt;Philip J. Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Roodman_A/0/1/0/all/0/1"&gt;Aaron Roodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Optimization with Smooth Algorithmic Adversaries. (arXiv:2106.01488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01488</id>
        <link href="http://arxiv.org/abs/2106.01488"/>
        <updated>2021-06-04T01:12:27.713Z</updated>
        <summary type="html"><![CDATA[This paper considers minimax optimization $\min_x \max_y f(x, y)$ in the
challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in
$y$. Though such optimization problems arise in many machine learning paradigms
including training generative adversarial networks (GANs) and adversarially
robust models, many fundamental issues remain in theory, such as the absence of
efficiently computable optimality notions, and cyclic or diverging behavior of
existing algorithms. Our framework sprouts from the practical consideration
that under a computational budget, the max-player can not fully maximize
$f(x,\cdot)$ since nonconcave maximization is NP-hard in general. So, we
propose a new algorithm for the min-player to play against smooth algorithms
deployed by the adversary (i.e., the max-player) instead of against full
maximization. Our algorithm is guaranteed to make monotonic progress (thus
having no limit cycles), and to find an appropriate "stationary point" in a
polynomial number of iterations. Our framework covers practical settings where
the smooth algorithms deployed by the adversary are multi-step stochastic
gradient ascent, and its accelerated version. We further provide complementing
experiments that confirm our theoretical findings and demonstrate the
effectiveness of the proposed approach in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fiez_T/0/1/0/all/0/1"&gt;Tanner Fiez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1"&gt;Lillian J. Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?. (arXiv:2106.01465v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01465</id>
        <link href="http://arxiv.org/abs/2106.01465"/>
        <updated>2021-06-04T01:12:27.700Z</updated>
        <summary type="html"><![CDATA[Is it possible to use natural language to intervene in a model's behavior and
alter its prediction in a desired way? We investigate the effectiveness of
natural language interventions for reading-comprehension systems, studying this
in the context of social stereotypes. Specifically, we propose a new language
understanding task, Linguistic Ethical Interventions (LEI), where the goal is
to amend a question-answering (QA) model's unethical behavior by communicating
context-specific principles of ethics and equity to it. To this end, we build
upon recent methods for quantifying a system's social stereotypes, augmenting
them with different kinds of ethical interventions and the desired model
behavior under such interventions. Our zero-shot evaluation finds that even
today's powerful neural language models are extremely poor ethical-advice
takers, that is, they respond surprisingly little to ethical interventions even
though these interventions are stated as simple sentences. Few-shot learning
improves model behavior but remains far from the desired outcome, especially
when evaluated for various types of generalization. Our new task thus poses a
novel language understanding challenge for the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1"&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Wrap: a Data-Efficient and Interpretable Extension to Image Classification Models. (arXiv:2106.01440v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01440</id>
        <link href="http://arxiv.org/abs/2106.01440"/>
        <updated>2021-06-04T01:12:27.692Z</updated>
        <summary type="html"><![CDATA[Due to their black-box and data-hungry nature, deep learning techniques are
not yet widely adopted for real-world applications in critical domains, like
healthcare and justice. This paper presents Memory Wrap, a plug-and-play
extension to any image classification model. Memory Wrap improves both
data-efficiency and model interpretability, adopting a content-attention
mechanism between the input and some memories of past training samples. We show
that Memory Wrap outperforms standard classifiers when it learns from a limited
set of data, and it reaches comparable performance when it learns from the full
dataset. We discuss how its structure and content-attention mechanisms make
predictions interpretable, compared to standard classifiers. To this end, we
both show a method to build explanations by examples and counterfactuals, based
on the memory content, and how to exploit them to get insights about its
decision process. We test our approach on image classification tasks using
several architectures on three different datasets, namely CIFAR10, SVHN, and
CINIC10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1"&gt;Biagio La Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capobianco_R/0/1/0/all/0/1"&gt;Roberto Capobianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1"&gt;Daniele Nardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks. (arXiv:2106.01452v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01452</id>
        <link href="http://arxiv.org/abs/2106.01452"/>
        <updated>2021-06-04T01:12:27.683Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important blind spots of deep learning systems.
While word- and sentence-level attack scenarios mostly deal with finding
semantic paraphrases of the input that fool NLP models, character-level attacks
typically insert typos into the input stream. It is commonly thought that these
are easier to defend via spelling correction modules. In this work, we show
that both a standard spellchecker and the approach of Pruthi et al. (2019),
which trains to defend against insertions, deletions and swaps, perform poorly
on the character-level benchmark recently proposed in Eger and Benz (2020)
which includes more challenging attacks such as visual and phonetic
perturbations and missing word segmentations. In contrast, we show that an
untrained iterative approach which combines context-independent character-level
information with context-dependent information from BERT's masked language
modeling can perform on par with human crowd-workers from Amazon Mechanical
Turk (AMT) supervised via 3-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yannik Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackensen_J/0/1/0/all/0/1"&gt;Jan Mackensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelizing Thompson Sampling. (arXiv:2106.01420v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01420</id>
        <link href="http://arxiv.org/abs/2106.01420"/>
        <updated>2021-06-04T01:12:27.674Z</updated>
        <summary type="html"><![CDATA[How can we make use of information parallelism in online decision making
problems while efficiently balancing the exploration-exploitation trade-off? In
this paper, we introduce a batch Thompson Sampling framework for two canonical
online decision making problems, namely, stochastic multi-arm bandit and linear
contextual bandit with finitely many arms. Over a time horizon $T$, our
\textit{batch} Thompson Sampling policy achieves the same (asymptotic) regret
bound of a fully sequential one while carrying out only $O(\log T)$ batch
queries. To achieve this exponential reduction, i.e., reducing the number of
interactions from $T$ to $O(\log T)$, our batch policy dynamically determines
the duration of each batch in order to balance the exploration-exploitation
trade-off. We also demonstrate experimentally that dynamic batch allocation
dramatically outperforms natural baselines such as static batch allocations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shadravan_M/0/1/0/all/0/1"&gt;Mohammad Shadravan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Bilevel Programming for Sparse Regularization. (arXiv:2106.01429v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01429</id>
        <link href="http://arxiv.org/abs/2106.01429"/>
        <updated>2021-06-04T01:12:27.655Z</updated>
        <summary type="html"><![CDATA[Iteratively reweighted least square (IRLS) is a popular approach to solve
sparsity-enforcing regression problems in machine learning. State of the art
approaches are more efficient but typically rely on specific coordinate pruning
schemes. In this work, we show how a surprisingly simple reparametrization of
IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is
able to achieve top performances on a wide range of sparsity (such as Lasso,
group Lasso and trace norm regularizations), regularization strength (including
hard constraints), and design matrices (ranging from correlated designs to
differential operators). Similarly to IRLS, our method only involves linear
systems resolutions, but in sharp contrast, corresponds to the minimization of
a smooth function. Despite being non-convex, we show that there is no spurious
minima and that saddle points are "ridable", so that there always exists a
descent direction. We thus advocate for the use of a BFGS quasi-Newton solver,
which makes our approach simple, robust and efficient. We perform a numerical
benchmark of the convergence speed of our algorithm against state of the art
solvers for Lasso, group Lasso, trace norm and linearly constrained problems.
These results highlight the versatility of our approach, removing the need to
use different solvers depending on the specificity of the ML problem under
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Poon_C/0/1/0/all/0/1"&gt;Clarice Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peyre_G/0/1/0/all/0/1"&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SemiFL: Communication Efficient Semi-Supervised Federated Learning with Unlabeled Clients. (arXiv:2106.01432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01432</id>
        <link href="http://arxiv.org/abs/2106.01432"/>
        <updated>2021-06-04T01:12:27.646Z</updated>
        <summary type="html"><![CDATA[Federated Learning allows training machine learning models by using the
computation and private data resources of a large number of distributed clients
such as smartphones and IoT devices. Most existing works on Federated Learning
(FL) assume the clients have ground-truth labels. However, in many practical
scenarios, clients may be unable to label task-specific data, e.g., due to lack
of expertise. In this work, we consider a server that hosts a labeled dataset,
and wishes to leverage clients with unlabeled data for supervised learning. We
propose a new Federated Learning framework referred to as SemiFL in order to
address the problem of Semi-Supervised Federated Learning (SSFL). In SemiFL,
clients have completely unlabeled data, while the server has a small amount of
labeled data. SemiFL is communication efficient since it separates the training
of server-side supervised data and client-side unsupervised data. We
demonstrate various efficient strategies of SemiFL that enhance learning
performance. Extensive empirical evaluations demonstrate that our communication
efficient method can significantly improve the performance of a labeled server
with unlabeled clients. Moreover, we demonstrate that SemiFL can outperform
many existing FL results trained with fully supervised data, and perform
competitively with the state-of-the-art centralized Semi-Supervised Learning
(SSL) methods. For instance, in standard communication efficient scenarios, our
method can perform 93% accuracy on the CIFAR10 dataset with only 4000 labeled
samples at the server. Such accuracy is only 2% away from the result trained
from 50000 fully labeled data, and it improves about 30% upon existing SSFL
methods in the communication efficient setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_E/0/1/0/all/0/1"&gt;Enmao Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jie Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour. (arXiv:2106.01434v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01434</id>
        <link href="http://arxiv.org/abs/2106.01434"/>
        <updated>2021-06-04T01:12:27.631Z</updated>
        <summary type="html"><![CDATA[Robots need to be able to work in multiple different environments. Even when
performing similar tasks, different behaviour should be deployed to best fit
the current environment. In this paper, We propose a new approach to
navigation, where it is treated as a multi-task learning problem. This enables
the robot to learn to behave differently in visual navigation tasks for
different environments while also learning shared expertise across
environments. We evaluated our approach in both simulated environments as well
as real-world data. Our method allows our system to converge with a 26%
reduction in training time, while also increasing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1"&gt;Xihan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1"&gt;Simon Hadfield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with Generic Representations. (arXiv:2106.01423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01423</id>
        <link href="http://arxiv.org/abs/2106.01423"/>
        <updated>2021-06-04T01:12:27.622Z</updated>
        <summary type="html"><![CDATA[The field of few-shot learning has made remarkable strides in developing
powerful models that can operate in the small data regime. Nearly all of these
methods assume every unlabeled instance encountered will belong to a handful of
known classes for which one has examples. This can be problematic for
real-world use cases where one routinely finds 'none-of-the-above' examples. In
this paper we describe this challenge of identifying what we term
'out-of-support' (OOS) examples. We describe how this problem is subtly
different from out-of-distribution detection and describe a new method of
identifying OOS examples within the Prototypical Networks framework using a
fixed point which we call the generic representation. We show that our method
outperforms other existing approaches in the literature as well as other
approaches that we propose in this paper. Finally, we investigate how the use
of such a generic point affects the geometry of a model's feature space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courts_N/0/1/0/all/0/1"&gt;Nico Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1"&gt;Lauren A. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buckheit_J/0/1/0/all/0/1"&gt;John Buckheit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+New_Z/0/1/0/all/0/1"&gt;Zachary New&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skomski_E/0/1/0/all/0/1"&gt;Elliott Skomski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung H. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1"&gt;Sandeep Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hibler_J/0/1/0/all/0/1"&gt;Jessica Hibler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corley_C/0/1/0/all/0/1"&gt;Courtney D. Corley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodas_N/0/1/0/all/0/1"&gt;Nathan O. Hodas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rectangular Flows for Manifold Learning. (arXiv:2106.01413v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01413</id>
        <link href="http://arxiv.org/abs/2106.01413"/>
        <updated>2021-06-04T01:12:27.611Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are invertible neural networks with tractable
change-of-volume terms, which allows optimization of their parameters to be
efficiently performed via maximum likelihood. However, data of interest is
typically assumed to live in some (often unknown) low-dimensional manifold
embedded in high-dimensional ambient space. The result is a modelling mismatch
since -- by construction -- the invertibility requirement implies
high-dimensional support of the learned distribution. Injective flows, mapping
from low- to high-dimensional space, aim to fix this discrepancy by learning
distributions on manifolds, but the resulting volume-change term becomes more
challenging to evaluate. Current approaches either avoid computing this term
entirely using various heuristics, or assume the manifold is known beforehand
and therefore are not widely applicable. Instead, we propose two methods to
tractably calculate the gradient of this term with respect to the parameters of
the model, relying on careful use of automatic differentiation and techniques
from numerical linear algebra. Both approaches perform end-to-end nonlinear
manifold learning and density estimation for data projected onto this manifold.
We study the trade-offs between our proposed methods, empirically verify that
we outperform approaches ignoring the volume-change term by more accurately
learning manifolds and the corresponding distributions on them, and show
promising results on out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Caterini_A/0/1/0/all/0/1"&gt;Anthony L. Caterini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1"&gt;Gabriel Loaiza-Ganem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cunningham_J/0/1/0/all/0/1"&gt;John P. Cunningham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning. (arXiv:2106.01404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01404</id>
        <link href="http://arxiv.org/abs/2106.01404"/>
        <updated>2021-06-04T01:12:27.492Z</updated>
        <summary type="html"><![CDATA[Learning to reach goal states and learning diverse skills through mutual
information (MI) maximization have been proposed as principled frameworks for
self-supervised reinforcement learning, allowing agents to acquire broadly
applicable multitask policies with minimal reward engineering. Starting from a
simple observation that the standard goal-conditioned RL (GCRL) is encapsulated
by the optimization objective of variational empowerment, we discuss how GCRL
and MI-based RL can be generalized into a single family of methods, which we
name variational GCRL (VGCRL), interpreting variational MI maximization, or
variational empowerment, as representation learning methods that acquire
functionally-aware state representations for goal reaching. This novel
perspective allows us to: (1) derive simple but unexplored variants of GCRL to
study how adding small representation capacity can already expand its
capabilities; (2) investigate how discriminator function capacity and
smoothness determine the quality of discovered skills, or latent goals, through
modifying latent dimensionality and applying spectral normalization; (3) adapt
techniques such as hindsight experience replay (HER) from GCRL to MI-based RL;
and lastly, (4) propose a novel evaluation metric, named latent goal reaching
(LGR), for comparing empowerment algorithms with different choices of latent
dimensionality and discriminator parameterization. Through principled
mathematical derivations and careful experimental studies, our work lays a
novel foundation from which to evaluate, analyze, and develop representation
learning techniques in goal-based RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jongwook Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Archit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shixiang Shane Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[q-RBFNN:A Quantum Calculus-based RBF Neural Network. (arXiv:2106.01370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01370</id>
        <link href="http://arxiv.org/abs/2106.01370"/>
        <updated>2021-06-04T01:12:27.482Z</updated>
        <summary type="html"><![CDATA[In this research a novel stochastic gradient descent based learning approach
for the radial basis function neural networks (RBFNN) is proposed. The proposed
method is based on the q-gradient which is also known as Jackson derivative. In
contrast to the conventional gradient, which finds the tangent, the q-gradient
finds the secant of the function and takes larger steps towards the optimal
solution. The proposed $q$-RBFNN is analyzed for its convergence performance in
the context of least square algorithm. In particular, a closed form expression
of the Wiener solution is obtained, and stability bounds of the learning rate
(step-size) is derived. The analytical results are validated through computer
simulation. Additionally, we propose an adaptive technique for the time-varying
$q$-parameter to improve convergence speed with no trade-offs in the steady
state performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1"&gt;Syed Saiq Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_T/0/1/0/all/0/1"&gt;Taha Hasan Masood Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_I/0/1/0/all/0/1"&gt;Imran Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Togneri_R/0/1/0/all/0/1"&gt;Roberto Togneri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-component gradient rules for variational quantum algorithms. (arXiv:2106.01388v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01388</id>
        <link href="http://arxiv.org/abs/2106.01388"/>
        <updated>2021-06-04T01:12:27.473Z</updated>
        <summary type="html"><![CDATA[Many near-term quantum computing algorithms are conceived as variational
quantum algorithms, in which parameterized quantum circuits are optimized in a
hybrid quantum-classical setup. Examples are variational quantum eigensolvers,
quantum approximate optimization algorithms as well as various algorithms in
the context of quantum-assisted machine learning. A common bottleneck of any
such algorithm is constituted by the optimization of the variational
parameters. A popular set of optimization methods work on the estimate of the
gradient, obtained by means of circuit evaluations. We will refer to the way in
which one can combine these circuit evaluations as gradient rules. This work
provides a comprehensive picture of the family of gradient rules that vary
parameters of quantum gates individually. The most prominent known members of
this family are the parameter shift rule and the finite differences method. To
unite this family, we propose a generalized parameter shift rule that expresses
all members of the aforementioned family as special cases, and discuss how all
of these can be seen as providing access to a linear combination of exact
first- and second-order derivatives. We further prove that a parameter shift
rule with one non-shifted evaluation and only one shifted circuit evaluation
can not exist does not exist, and introduce a novel perspective for approaching
new gradient rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hubregtsen_T/0/1/0/all/0/1"&gt;Thomas Hubregtsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wilde_F/0/1/0/all/0/1"&gt;Frederik Wilde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Qasim_S/0/1/0/all/0/1"&gt;Shozab Qasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eisert_J/0/1/0/all/0/1"&gt;Jens Eisert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Script E2E framework for Multilingual and Code-Switching ASR. (arXiv:2106.01400v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.01400</id>
        <link href="http://arxiv.org/abs/2106.01400"/>
        <updated>2021-06-04T01:12:27.461Z</updated>
        <summary type="html"><![CDATA[India is home to multiple languages, and training automatic speech
recognition (ASR) systems for languages is challenging. Over time, each
language has adopted words from other languages, such as English, leading to
code-mixing. Most Indian languages also have their own unique scripts, which
poses a major limitation in training multilingual and code-switching ASR
systems.

Inspired by results in text-to-speech synthesis, in this work, we use an
in-house rule-based phoneme-level common label set (CLS) representation to
train multilingual and code-switching ASR for Indian languages. We propose two
end-to-end (E2E) ASR systems. In the first system, the E2E model is trained on
the CLS representation, and we use a novel data-driven back-end to recover the
native language script. In the second system, we propose a modification to the
E2E model, wherein the CLS representation and the native language characters
are used simultaneously for training. We show our results on the multilingual
and code-switching tasks of the Indic ASR Challenge 2021. Our best results
achieve 6% and 5% improvement (approx) in word error rate over the baseline
system for the multilingual and code-switching tasks, respectively, on the
challenge development data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1"&gt;Mari Ganesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuriakose_J/0/1/0/all/0/1"&gt;Jom Kuriakose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thyagachandran_A/0/1/0/all/0/1"&gt;Anand Thyagachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+A_A/0/1/0/all/0/1"&gt;Arun Kumar A&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1"&gt;Ashish Seth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_L/0/1/0/all/0/1"&gt;Lodagala Durga Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaiswal_S/0/1/0/all/0/1"&gt;Saish Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Anusha Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murthy_H/0/1/0/all/0/1"&gt;Hema Murthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Undecidability of Learnability. (arXiv:2106.01382v1 [cs.CC])]]></title>
        <id>http://arxiv.org/abs/2106.01382</id>
        <link href="http://arxiv.org/abs/2106.01382"/>
        <updated>2021-06-04T01:12:27.447Z</updated>
        <summary type="html"><![CDATA[Machine learning researchers and practitioners steadily enlarge the multitude
of successful learning models. They achieve this through in-depth theoretical
analyses and experiential heuristics. However, there is no known
general-purpose procedure for rigorously evaluating whether newly proposed
models indeed successfully learn from data. We show that such a procedure
cannot exist. For PAC binary classification, uniform and universal online
learning, and exact learning through teacher-learner interactions, learnability
is in general undecidable, both in the sense of independence of the axioms in a
formal system and in the sense of uncomputability. Our proofs proceed via
computable constructions of function classes that encode the consistency
problem for formal systems and the halting problem for Turing machines into
complexity measures that characterize learnability. Our work shows that
undecidability appears in the theoretical foundations of machine learning:
There is no one-size-fits-all algorithm for deciding whether a machine learning
model can be successful. We cannot in general automatize the process of
assessing new learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caro_M/0/1/0/all/0/1"&gt;Matthias C. Caro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On using distributed representations of source code for the detection of C security vulnerabilities. (arXiv:2106.01367v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.01367</id>
        <link href="http://arxiv.org/abs/2106.01367"/>
        <updated>2021-06-04T01:12:27.425Z</updated>
        <summary type="html"><![CDATA[This paper presents an evaluation of the code representation model Code2vec
when trained on the task of detecting security vulnerabilities in C source
code. We leverage the open-source library astminer to extract path-contexts
from the abstract syntax trees of a corpus of labeled C functions. Code2vec is
trained on the resulting path-contexts with the task of classifying a function
as vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that
the accuracy of Code2vec for this task is comparable to simple
transformer-based methods such as pre-trained RoBERTa, and outperforms more
naive NLP-based methods. We achieved an accuracy of 61.43% while maintaining
low computational requirements relative to larger models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coimbra_D/0/1/0/all/0/1"&gt;David Coimbra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reis_S/0/1/0/all/0/1"&gt;Sofia Reis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreu_R/0/1/0/all/0/1"&gt;Rui Abreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1"&gt;Corina P&amp;#x103;s&amp;#x103;reanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogmus_H/0/1/0/all/0/1"&gt;Hakan Erdogmus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Assessment of the Design Quality of Python Programs with Personalized Feedback. (arXiv:2106.01399v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.01399</id>
        <link href="http://arxiv.org/abs/2106.01399"/>
        <updated>2021-06-04T01:12:27.402Z</updated>
        <summary type="html"><![CDATA[The assessment of program functionality can generally be accomplished with
straight-forward unit tests. However, assessing the design quality of a program
is a much more difficult and nuanced problem. Design quality is an important
consideration since it affects the readability and maintainability of programs.
Assessing design quality and giving personalized feedback is very time
consuming task for instructors and teaching assistants. This limits the scale
of giving personalized feedback to small class settings. Further, design
quality is nuanced and is difficult to concisely express as a set of rules. For
these reasons, we propose a neural network model to both automatically assess
the design of a program and provide personalized feedback to guide students on
how to make corrections. The model's effectiveness is evaluated on a corpus of
student programs written in Python. The model has an accuracy rate from 83.67%
to 94.27%, depending on the dataset, when predicting design scores as compared
to historical instructor assessment. Finally, we present a study where students
tried to improve the design of their programs based on the personalized
feedback produced by the model. Students who participated in the study improved
their program design scores by 19.58%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orr_J/0/1/0/all/0/1"&gt;J. Walker Orr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_N/0/1/0/all/0/1"&gt;Nathaniel Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Rankings for Recommendation in Matching Markets. (arXiv:2106.01941v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01941</id>
        <link href="http://arxiv.org/abs/2106.01941"/>
        <updated>2021-06-04T01:12:26.481Z</updated>
        <summary type="html"><![CDATA[Based on the success of recommender systems in e-commerce, there is growing
interest in their use in matching markets (e.g., labor). While this holds
potential for improving market fluidity and fairness, we show in this paper
that naively applying existing recommender systems to matching markets is
sub-optimal. Considering the standard process where candidates apply and then
get evaluated by employers, we present a new recommendation framework to model
this interaction mechanism and propose efficient algorithms for computing
personalized rankings in this setting. We show that the optimal rankings need
to not only account for the potentially divergent preferences of candidates and
employers, but they also need to account for capacity constraints. This makes
conventional ranking systems that merely rank by some local score (e.g.,
one-sided or reciprocal relevance) highly sub-optimal -- not only for an
individual user, but also for societal goals (e.g., low unemployment). To
address this shortcoming, we propose the first method for jointly optimizing
the rankings for all candidates in the market to explicitly maximize social
welfare. In addition to the theoretical derivation, we evaluate the method both
on simulated environments and on data from a real-world
networking-recommendation system that we built and fielded at a large computer
science conference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayoumi_M/0/1/0/all/0/1"&gt;Magd Bayoumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1"&gt;Thorsten Joachims&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmoDNN: Understanding emotions from short texts through a deep neural network ensemble. (arXiv:2106.01706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01706</id>
        <link href="http://arxiv.org/abs/2106.01706"/>
        <updated>2021-06-04T01:12:26.470Z</updated>
        <summary type="html"><![CDATA[The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Query Logs for Privacy Studies: On Deriving Search Queries from Questions. (arXiv:2004.02023v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02023</id>
        <link href="http://arxiv.org/abs/2004.02023"/>
        <updated>2021-06-04T01:12:26.458Z</updated>
        <summary type="html"><![CDATA[Translating verbose information needs into crisp search queries is a
phenomenon that is ubiquitous but hardly understood. Insights into this process
could be valuable in several applications, including synthesizing large
privacy-friendly query logs from public Web sources which are readily available
to the academic research community. In this work, we take a step towards
understanding query formulation by tapping into the rich potential of community
question answering (CQA) forums. Specifically, we sample natural language (NL)
questions spanning diverse themes from the Stack Exchange platform, and conduct
a large-scale conversion experiment where crowdworkers submit search queries
they would use when looking for equivalent information. We provide a careful
analysis of this data, accounting for possible sources of bias during
conversion, along with insights into user-specific linguistic patterns and
search behaviors. We release a dataset of 7,000 question-query pairs from this
study to facilitate further research on query understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1"&gt;Asia J. Biega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1"&gt;Jana Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu. (arXiv:2106.01674v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01674</id>
        <link href="http://arxiv.org/abs/2106.01674"/>
        <updated>2021-06-04T01:12:26.119Z</updated>
        <summary type="html"><![CDATA[In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiaochao Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hao Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangxing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guobao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiwei Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Daxiang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What and How long: Prediction of Mobile App Engagement. (arXiv:2106.01490v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01490</id>
        <link href="http://arxiv.org/abs/2106.01490"/>
        <updated>2021-06-04T01:12:26.099Z</updated>
        <summary type="html"><![CDATA[User engagement is crucial to the long-term success of a mobile app. Several
metrics, such as dwell time, have been used for measuring user engagement.
However, how to effectively predict user engagement in the context of mobile
apps is still an open research question. For example, do the mobile usage
contexts (e.g.,~time of day) in which users access mobile apps impact their
dwell time? Answers to such questions could help mobile operating system and
publishers to optimize advertising and service placement. In this paper, we
first conduct an empirical study for assessing how user characteristics,
temporal features, and the short/long-term contexts contribute to gains in
predicting users' app dwell time on the population level. The comprehensive
analysis is conducted on large app usage logs collected through a mobile
advertising company. The dataset covers more than 12K anonymous users and 1.3
million log events. Based on the analysis, we further investigate a novel
mobile app engagement prediction problem -- can we predict simultaneously what
app the user will use next and how long he/she will stay on that app? We
propose several strategies for this joint prediction problem and demonstrate
that our model can improve the performance significantly when compared with the
state-of-the-art baselines. Our work can help mobile system developers in
designing a better and more engagement-aware mobile app user experience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Ke Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelleg_D/0/1/0/all/0/1"&gt;Dan Pelleg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separated-Spectral-Distribution Estimation Based on Bayesian Inference with Single RGB Camera. (arXiv:2106.01861v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01861</id>
        <link href="http://arxiv.org/abs/2106.01861"/>
        <updated>2021-06-04T01:12:26.067Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel method for separately estimating spectral
distributions from images captured by a typical RGB camera. The proposed method
allows us to separately estimate a spectral distribution of illumination,
reflectance, or camera sensitivity, while recent hyperspectral cameras are
limited to capturing a joint spectral distribution from a scene. In addition,
the use of Bayesian inference makes it possible to take into account prior
information of both spectral distributions and image noise as probability
distributions. As a result, the proposed method can estimate spectral
distributions in a unified way, and it can enhance the robustness of the
estimation against noise, which conventional spectral-distribution estimation
methods cannot. The use of Bayesian inference also enables us to obtain the
confidence of estimation results. In an experiment, the proposed method is
shown not only to outperform conventional estimation methods in terms of RMSE
but also to be robust against noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipative Video Transformer. (arXiv:2106.02036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02036</id>
        <link href="http://arxiv.org/abs/2106.02036"/>
        <updated>2021-06-04T01:12:26.034Z</updated>
        <summary type="html"><![CDATA[We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
</feed>