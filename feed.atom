<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-08-24T01:40:31.088Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Towards large-scale, automated, accurate detection of CCTV camera objects using computer vision. Applications and implications for privacy, safety, and cybersecurity. (Preprint). (arXiv:2006.03870v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03870</id>
        <link href="http://arxiv.org/abs/2006.03870"/>
        <updated>2021-08-24T01:40:31.028Z</updated>
        <summary type="html"><![CDATA[In order to withstand the ever-increasing invasion of privacy by CCTV cameras
and technologies, on par CCTV-aware solutions must exist that provide privacy,
safety, and cybersecurity features. We argue that a first important step
towards such CCTV-aware solutions must be a mapping system (e.g., Google Maps,
OpenStreetMap) that provides both privacy and safety routing and navigation
options. However, this in turn requires that the mapping system contains
updated information on CCTV cameras' exact geo-location, coverage area, and
possibly other meta-data (e.g., resolution, facial recognition features,
operator). Such information is however missing from current mapping systems,
and there are several ways to fix this. One solution is to perform CCTV camera
detection on geo-location tagged images, e.g., street view imagery on various
platforms, user images publicly posted in image sharing platforms such as
Flickr. Unfortunately, to the best of our knowledge, there are no computer
vision models for CCTV camera object detection as well as no mapping system
that supports privacy and safety routing options.

To close these gaps, with this paper we introduce CCTVCV -- the first and
only computer vision MS COCO-compatible models that are able to accurately
detect CCTV and video surveillance cameras in images and video frames. To this
end, our best detectors were built using 8387 images that were manually
reviewed and annotated to contain 10419 CCTV camera instances, and achieve an
accuracy of up to 98.7%. Moreover, we build and evaluate multiple models,
present a comprehensive comparison of their performance, and outline core
challenges associated with such research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1"&gt;Hannu Turtiainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1"&gt;Andrei Costin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1"&gt;Tuomo Lahtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sintonen_L/0/1/0/all/0/1"&gt;Lauri Sintonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_T/0/1/0/all/0/1"&gt;Timo Hamalainen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crown Jewels Analysis using Reinforcement Learning with Attack Graphs. (arXiv:2108.09358v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09358</id>
        <link href="http://arxiv.org/abs/2108.09358"/>
        <updated>2021-08-24T01:40:31.010Z</updated>
        <summary type="html"><![CDATA[Cyber attacks pose existential threats to nations and enterprises. Current
practice favors piece-wise analysis using threat-models in the stead of
rigorous cyber terrain analysis and intelligence preparation of the
battlefield. Automated penetration testing using reinforcement learning offers
a new and promising approach for developing methodologies that are driven by
network structure and cyber terrain, that can be later interpreted in terms of
threat-models, but that are principally network-driven analyses. This paper
presents a novel method for crown jewel analysis termed CJA-RL that uses
reinforcement learning to identify key terrain and avenues of approach for
exploiting crown jewels. In our experiment, CJA-RL identified ideal entry
points, choke points, and pivots for exploiting a network with multiple crown
jewels, exemplifying how CJA-RL and reinforcement learning for penetration
testing generally can benefit computer network operations workflows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangupantulu_R/0/1/0/all/0/1"&gt;Rohit Gangupantulu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1"&gt;Tyler Cody&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1"&gt;Abdul Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redino_C/0/1/0/all/0/1"&gt;Christopher Redino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1"&gt;Ryan Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1"&gt;Paul Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift. (arXiv:2108.09671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09671</id>
        <link href="http://arxiv.org/abs/2108.09671"/>
        <updated>2021-08-24T01:40:30.992Z</updated>
        <summary type="html"><![CDATA[Recently proposed neural architecture search (NAS) methods co-train billions
of architectures in a supernet and estimate their potential accuracy using the
network weights detached from the supernet. However, the ranking correlation
between the architectures' predicted accuracy and their actual capability is
incorrect, which causes the existing NAS methods' dilemma. We attribute this
ranking correlation problem to the supernet training consistency shift,
including feature shift and parameter shift. Feature shift is identified as
dynamic input distributions of a hidden layer due to random path sampling. The
input distribution dynamic affects the loss descent and finally affects
architecture ranking. Parameter shift is identified as contradictory parameter
updates for a shared layer lay in different paths in different training steps.
The rapidly-changing parameter could not preserve architecture ranking. We
address these two shifts simultaneously using a nontrivial supernet-Pi model,
called Pi-NAS. Specifically, we employ a supernet-Pi model that contains
cross-path learning to reduce the feature consistency shift between different
paths. Meanwhile, we adopt a novel nontrivial mean teacher containing negative
samples to overcome parameter shift and model collision. Furthermore, our
Pi-NAS runs in an unsupervised manner, which can search for more transferable
architectures. Extensive experiments on ImageNet and a wide range of downstream
tasks (e.g., COCO 2017, ADE20K, and Cityscapes) demonstrate the effectiveness
and universality of our Pi-NAS compared to supervised NAS. See Codes:
https://github.com/Ernie1/Pi-NAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jiefeng Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition. (arXiv:2107.12213v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12213</id>
        <link href="http://arxiv.org/abs/2107.12213"/>
        <updated>2021-08-24T01:40:30.986Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) have been widely used and achieved
remarkable results in skeleton-based action recognition. In GCNs, graph
topology dominates feature aggregation and therefore is the key to extracting
representative features. In this work, we propose a novel Channel-wise Topology
Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies
and effectively aggregate joint features in different channels for
skeleton-based action recognition. The proposed CTR-GC models channel-wise
topologies through learning a shared topology as a generic prior for all
channels and refining it with channel-specific correlations for each channel.
Our refinement method introduces few extra parameters and significantly reduces
the difficulty of modeling channel-wise topologies. Furthermore, via
reformulating graph convolutions into a unified form, we find that CTR-GC
relaxes strict constraints of graph convolutions, leading to stronger
representation capability. Combining CTR-GC with temporal modeling modules, we
develop a powerful graph convolutional network named CTR-GCN which notably
outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and
NW-UCLA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chunfeng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Ying Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00166</id>
        <link href="http://arxiv.org/abs/2108.00166"/>
        <updated>2021-08-24T01:40:30.950Z</updated>
        <summary type="html"><![CDATA[Micro-expressions are spontaneous, unconscious facial movements that show
people's true inner emotions and have great potential in related fields of
psychological testing. Since the face is a 3D deformation object, the
occurrence of an expression can arouse spatial deformation of the face, but
limited by the available databases are 2D videos, lacking the description of 3D
spatial information of micro-expressions. Therefore, we proposed a new
micro-expression database containing 2D video sequences and 3D point clouds
sequences. The database includes 259 micro-expressions sequences, and these
samples were classified using the objective method based on facial action
coding system, as well as the non-objective method that combines video contents
and participants' self-reports. We extracted 2D and 3D features using the local
binary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,
respectively, and evaluated the classification accuracies of these two features
and their fusion results with leave-one-subject-out (LOSO) and 10-fold
cross-validation. Further, we performed various neural network algorithms for
database classification, the results show that classification accuracies are
improved by fusing 3D features than using only 2D features. The database offers
original and cropped micro-expression samples, which will facilitate the
exploration and research on 3D Spatio-temporal features of micro-expressions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fengping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1"&gt;Chun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1"&gt;Danmin Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Sketching of Polynomial Kernels of Polynomial Degree. (arXiv:2108.09420v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2108.09420</id>
        <link href="http://arxiv.org/abs/2108.09420"/>
        <updated>2021-08-24T01:40:30.920Z</updated>
        <summary type="html"><![CDATA[Kernel methods are fundamental in machine learning, and faster algorithms for
kernel approximation provide direct speedups for many core tasks in machine
learning. The polynomial kernel is especially important as other kernels can
often be approximated by the polynomial kernel via a Taylor series expansion.
Recent techniques in oblivious sketching reduce the dependence in the running
time on the degree $q$ of the polynomial kernel from exponential to polynomial,
which is useful for the Gaussian kernel, for which $q$ can be chosen to be
polylogarithmic. However, for more slowly growing kernels, such as the neural
tangent and arc-cosine kernels, $q$ needs to be polynomial, and previous work
incurs a polynomial factor slowdown in the running time. We give a new
oblivious sketch which greatly improves upon this running time, by removing the
dependence on $q$ in the leading order term. Combined with a novel sampling
scheme, we give the fastest algorithms for approximating a large family of
slow-growing kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lichen Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Network Embedding via Tensor Factorization. (arXiv:2108.09837v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09837</id>
        <link href="http://arxiv.org/abs/2108.09837"/>
        <updated>2021-08-24T01:40:30.900Z</updated>
        <summary type="html"><![CDATA[Representation learning on static graph-structured data has shown a
significant impact on many real-world applications. However, less attention has
been paid to the evolving nature of temporal networks, in which the edges are
often changing over time. The embeddings of such temporal networks should
encode both graph-structured information and the temporally evolving pattern.
Existing approaches in learning temporally evolving network representations
fail to capture the temporal interdependence. In this paper, we propose Toffee,
a novel approach for temporal network representation learning based on tensor
decomposition. Our method exploits the tensor-tensor product operator to encode
the cross-time information, so that the periodic changes in the evolving
networks can be captured. Experimental results demonstrate that Toffee
outperforms existing methods on multiple real-world temporal networks in
generating effective embeddings for the link prediction tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiuchen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Joyce C. Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Object Localization Using 2D Estimates for Computer Vision Applications. (arXiv:2009.11446v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.11446</id>
        <link href="http://arxiv.org/abs/2009.11446"/>
        <updated>2021-08-24T01:40:30.894Z</updated>
        <summary type="html"><![CDATA[A technique for object localization based on pose estimation and camera
calibration is presented. The 3-dimensional (3D) coordinates are estimated by
collecting multiple 2-dimensional (2D) images of the object and are utilized
for the calibration of the camera. The calibration steps involving a number of
parameter calculation including intrinsic and extrinsic parameters for the
removal of lens distortion, computation of object's size and camera's position
calculation are discussed. A transformation strategy to estimate the 3D pose
using the 2D images is presented. The proposed method is implemented on MATLAB
and validation experiments are carried out for both pose estimation and camera
calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_T/0/1/0/all/0/1"&gt;Taha Hasan Masood Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training. (arXiv:2108.09479v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.09479</id>
        <link href="http://arxiv.org/abs/2108.09479"/>
        <updated>2021-08-24T01:40:30.889Z</updated>
        <summary type="html"><![CDATA[Existing approaches to vision-language pre-training (VLP) heavily rely on an
object detector based on bounding boxes (regions), where salient objects are
first detected from images and then a Transformer-based model is used for
cross-modal fusion. Despite their superior performance, these approaches are
bounded by the capability of the object detector in terms of both effectiveness
and efficiency. Besides, the presence of object detection imposes unnecessary
constraints on model designs and makes it difficult to support end-to-end
training. In this paper, we revisit grid-based convolutional features for
vision-language pre-training, skipping the expensive region-related steps. We
propose a simple yet effective grid-based VLP method that works surprisingly
well with the grid features. By pre-training only with in-domain datasets, the
proposed Grid-VLP method can outperform most competitive region-based VLP
methods on three examined vision-language understanding tasks. We hope that our
findings help to further advance the state of the art of vision-language
pre-training, and provide a new direction towards effective and efficient VLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Junfeng Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1"&gt;Min Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Genetic Programming for Manifold Learning: Preserving Local Topology. (arXiv:2108.09914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09914</id>
        <link href="http://arxiv.org/abs/2108.09914"/>
        <updated>2021-08-24T01:40:30.884Z</updated>
        <summary type="html"><![CDATA[Manifold learning methods are an invaluable tool in today's world of
increasingly huge datasets. Manifold learning algorithms can discover a much
lower-dimensional representation (embedding) of a high-dimensional dataset
through non-linear transformations that preserve the most important structure
of the original data. State-of-the-art manifold learning methods directly
optimise an embedding without mapping between the original space and the
discovered embedded space. This makes interpretability - a key requirement in
exploratory data analysis - nearly impossible. Recently, genetic programming
has emerged as a very promising approach to manifold learning by evolving
functional mappings from the original space to an embedding. However, genetic
programming-based manifold learning has struggled to match the performance of
other approaches. In this work, we propose a new approach to using genetic
programming for manifold learning, which preserves local topology. This is
expected to significantly improve performance on tasks where local
neighbourhood structure (topology) is paramount. We compare our proposed
approach with various baseline manifold learning methods and find that it often
outperforms other methods, including a clear improvement over previous genetic
programming approaches. These results are particularly promising, given the
potential interpretability and reusability of the evolved mappings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lensen_A/0/1/0/all/0/1"&gt;Andrew Lensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1"&gt;Bing Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MimicBot: Combining Imitation and Reinforcement Learning to win in Bot Bowl. (arXiv:2108.09478v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.09478</id>
        <link href="http://arxiv.org/abs/2108.09478"/>
        <updated>2021-08-24T01:40:30.869Z</updated>
        <summary type="html"><![CDATA[This paper describe an hybrid agent trained to play in Fantasy Football AI
which participated in the Bot Bowl III competition. The agent, MimicBot, is
implemented using a specifically designed deep policy network and trained using
a combination of imitation and reinforcement learning. Previous attempts in
using a reinforcement learning approach in such context failed for a number of
reasons, e.g. due to the intrinsic randomness in the environment and the large
and uneven number of actions available, with a curriculum learning approach
failing to consistently beat a randomly paying agent. Currently no machine
learning approach can beat a scripted bot which makes use of the domain
knowledge on the game. Our solution, thanks to an imitation learning and a
hybrid decision-making process, consistently beat such scripted agents.
Moreover we shed lights on how to more efficiently train in a reinforcement
learning setting while drastically increasing sample efficiency. MimicBot is
the winner of the Bot Bowl III competition, and it is currently the
state-of-the-art solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pezzotti_N/0/1/0/all/0/1"&gt;Nicola Pezzotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks. (arXiv:2006.04270v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04270</id>
        <link href="http://arxiv.org/abs/2006.04270"/>
        <updated>2021-08-24T01:40:30.864Z</updated>
        <summary type="html"><![CDATA[Dropout is a well-known regularization method by sampling a sub-network from
a larger deep neural network and training different sub-networks on different
subsets of the data. Inspired by the dropout concept, we propose EDropout as an
energy-based framework for pruning neural networks in classification tasks. In
this approach, a set of binary pruning state vectors (population) represents a
set of corresponding sub-networks from an arbitrary provided original neural
network. An energy loss function assigns a scalar energy loss value to each
pruning state. The energy-based model stochastically evolves the population to
find states with lower energy loss. The best pruning state is then selected and
applied to the original network. Similar to dropout, the kept weights are
updated using backpropagation in a probabilistic model. The energy-based model
again searches for better pruning states and the cycle continuous. Indeed, this
procedure is in fact switching between the energy model, which manages the
pruning states, and the probabilistic model, which updates the temporarily
unpruned weights, in each iteration. The population can dynamically converge to
a pruning state. This can be interpreted as dropout leading to pruning the
network. From an implementation perspective, EDropout can prune typical neural
networks without modification of the network architecture. We evaluated the
proposed method on different flavours of ResNets, AlexNet, and SqueezeNet on
the Kuzushiji, Fashion, CIFAR-10, CIFAR-100, and Flowers datasets, and compared
the pruning rate and classification performance of the models. On average the
networks trained with EDropout achieved a pruning rate of more than $50\%$ of
the trainable parameters with approximately $<5\%$ and $<1\%$ drop of Top-1 and
Top-5 classification accuracy, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1"&gt;Hojjat Salehinejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valaee_S/0/1/0/all/0/1"&gt;Shahrokh Valaee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Quality of GAN Generated Images for Person Re-Identification. (arXiv:2108.09977v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09977</id>
        <link href="http://arxiv.org/abs/2108.09977"/>
        <updated>2021-08-24T01:40:30.858Z</updated>
        <summary type="html"><![CDATA[Recently, GAN based method has demonstrated strong effectiveness in
generating augmentation data for person re-identification (ReID), on account of
its ability to bridge the gap between domains and enrich the data variety in
feature space. However, most of the ReID works pick all the GAN generated data
as additional training samples or evaluate the quality of GAN generation at the
entire data set level, ignoring the image-level essential feature of data in
ReID task. In this paper, we analyze the in-depth characteristics of ReID
sample and solve the problem of "What makes a GAN-generated image good for
ReID". Specifically, we propose to examine each data sample with id-consistency
and diversity constraints by mapping image onto different spaces. With a
metric-based sampling method, we demonstrate that not every GAN-generated data
is beneficial for augmentation. Models trained with data filtered by our
quality evaluation outperform those trained with the full augmentation set by a
large margin. Extensive experiments show the effectiveness of our method on
both supervised ReID task and unsupervised domain adaptation ReID task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weihua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiaoyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A computational study on imputation methods for missing environmental data. (arXiv:2108.09500v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2108.09500</id>
        <link href="http://arxiv.org/abs/2108.09500"/>
        <updated>2021-08-24T01:40:30.853Z</updated>
        <summary type="html"><![CDATA[Data acquisition and recording in the form of databases are routine
operations. The process of collecting data, however, may experience
irregularities, resulting in databases with missing data. Missing entries might
alter analysis efficiency and, consequently, the associated decision-making
process. This paper focuses on databases collecting information related to the
natural environment. Given the broad spectrum of recorded activities, these
databases typically are of mixed nature. It is therefore relevant to evaluate
the performance of missing data processing methods considering this
characteristic. In this paper we investigate the performances of several
missing data imputation methods and their application to the problem of missing
data in environment. A computational study was performed to compare the method
missForest (MF) with two other imputation methods, namely Multivariate
Imputation by Chained Equations (MICE) and K-Nearest Neighbors (KNN). Tests
were made on 10 pretreated datasets of various types. Results revealed that MF
generally outperformed MICE and KNN in terms of imputation errors, with a more
pronounced performance gap for mixed typed databases where MF reduced the
imputation error up to 150%, when compared to the other methods. KNN was
usually the fastest method. MF was then successfully applied to a case study on
Quebec wastewater treatment plants performance monitoring. We believe that the
present study demonstrates the pertinence of using MF as imputation method when
dealing with missing environmental data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dixneuf_P/0/1/0/all/0/1"&gt;Paul Dixneuf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Errico_F/0/1/0/all/0/1"&gt;Fausto Errico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaus_M/0/1/0/all/0/1"&gt;Mathias Glaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegMix: Co-occurrence Driven Mixup for Semantic Segmentation and Adversarial Robustness. (arXiv:2108.09929v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09929</id>
        <link href="http://arxiv.org/abs/2108.09929"/>
        <updated>2021-08-24T01:40:30.848Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a strategy for training convolutional neural
networks to effectively resolve interference arising from competing hypotheses
relating to inter-categorical information throughout the network. The premise
is based on the notion of feature binding, which is defined as the process by
which activations spread across space and layers in the network are
successfully integrated to arrive at a correct inference decision. In our work,
this is accomplished for the task of dense image labelling by blending images
based on (i) categorical clustering or (ii) the co-occurrence likelihood of
categories. We then train a feature binding network which simultaneously
segments and separates the blended images. Subsequent feature denoising to
suppress noisy activations reveals additional desirable properties and high
degrees of successful predictions. Through this process, we reveal a general
mechanism, distinct from any prior methods, for boosting the performance of the
base segmentation and saliency network while simultaneously increasing
robustness to adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md Amirul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1"&gt;Matthew Kowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1"&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1"&gt;Neil D. B. Bruce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of CNN classifiers using Sugeno Fuzzy Integral Technique for Cervical Cytology Image Classification. (arXiv:2108.09460v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09460</id>
        <link href="http://arxiv.org/abs/2108.09460"/>
        <updated>2021-08-24T01:40:30.834Z</updated>
        <summary type="html"><![CDATA[Cervical cancer is the fourth most common category of cancer, affecting more
than 500,000 women annually, owing to the slow detection procedure. Early
diagnosis can help in treating and even curing cancer, but the tedious,
time-consuming testing process makes it impossible to conduct population-wise
screening. To aid the pathologists in efficient and reliable detection, in this
paper, we propose a fully automated computer-aided diagnosis tool for
classifying single-cell and slide images of cervical cancer. The main concern
in developing an automatic detection tool for biomedical image classification
is the low availability of publicly accessible data. Ensemble Learning is a
popular approach for image classification, but simplistic approaches that
leverage pre-determined weights to classifiers fail to perform satisfactorily.
In this research, we use the Sugeno Fuzzy Integral to ensemble the decision
scores from three popular pretrained deep learning models, namely, Inception
v3, DenseNet-161 and ResNet-34. The proposed Fuzzy fusion is capable of taking
into consideration the confidence scores of the classifiers for each sample,
and thus adaptively changing the importance given to each classifier, capturing
the complementary information supplied by each, thus leading to superior
classification performance. We evaluated the proposed method on three publicly
available datasets, the Mendeley Liquid Based Cytology (LBC) dataset, the
SIPaKMeD Whole Slide Image (WSI) dataset, and the SIPaKMeD Single Cell Image
(SCI) dataset, and the results thus yielded are promising. Analysis of the
approach using GradCAM-based visual representations and statistical tests, and
comparison of the method with existing and baseline models in literature
justify the efficacy of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_R/0/1/0/all/0/1"&gt;Rohit Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basak_H/0/1/0/all/0/1"&gt;Hritam Basak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koilada_A/0/1/0/all/0/1"&gt;Akhil Koilada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Soham Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Sukanta Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Nibaran Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-08-24T01:40:30.829Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11890</id>
        <link href="http://arxiv.org/abs/2011.11890"/>
        <updated>2021-08-24T01:40:30.823Z</updated>
        <summary type="html"><![CDATA[We present "Cross-Camera Convolutional Color Constancy" (C5), a
learning-based method, trained on images from multiple cameras, that accurately
estimates a scene's illuminant color from raw images captured by a new camera
previously unseen during training. C5 is a hypernetwork-like extension of the
convolutional color constancy (CCC) approach: C5 learns to generate the weights
of a CCC model that is then evaluated on the input image, with the CCC weights
dynamically adapted to different input content. Unlike prior cross-camera color
constancy models, which are usually designed to be agnostic to the spectral
properties of test-set images from unobserved cameras, C5 approaches this
problem through the lens of transductive inference: additional unlabeled images
are provided as input to the model at test time, which allows the model to
calibrate itself to the spectral properties of the test-set camera during
inference. C5 achieves state-of-the-art accuracy for cross-camera color
constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on
a GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a
practical solution to the problem of calibration-free automatic white balance
for mobile photography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1"&gt;Mahmoud Afifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1"&gt;Chloe LeGendre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Ta Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1"&gt;Francois Bleibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sarcasm Detection in Twitter -- Performance Impact when using Data Augmentation: Word Embeddings. (arXiv:2108.09924v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09924</id>
        <link href="http://arxiv.org/abs/2108.09924"/>
        <updated>2021-08-24T01:40:30.817Z</updated>
        <summary type="html"><![CDATA[Sarcasm is the use of words usually used to either mock or annoy someone, or
for humorous purposes. Sarcasm is largely used in social networks and
microblogging websites, where people mock or censure in a way that makes it
difficult even for humans to tell if what is said is what is meant. Failure to
identify sarcastic utterances in Natural Language Processing applications such
as sentiment analysis and opinion mining will confuse classification algorithms
and generate false results. Several studies on sarcasm detection have utilized
different learning algorithms. However, most of these learning models have
always focused on the contents of expression only, leaving the contextual
information in isolation. As a result, they failed to capture the contextual
information in the sarcastic expression. Moreover, some datasets used in
several studies have an unbalanced dataset which impacting the model result. In
this paper, we propose a contextual model for sarcasm identification in twitter
using RoBERTa, and augmenting the dataset by applying Global Vector
representation (GloVe) for the construction of word embedding and context
learning to generate more data and balancing the dataset. The effectiveness of
this technique is tested with various datasets and data augmentation settings.
In particular, we achieve performance gain by 3.2% in the iSarcasm dataset when
using data augmentation to increase 20% of data labeled as sarcastic, resulting
F-score of 40.4% compared to 37.2% without data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Handoyo_A/0/1/0/all/0/1"&gt;Alif Tri Handoyo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hidayaturrahman/0/1/0/all/0/1"&gt;Hidayaturrahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhartono_D/0/1/0/all/0/1"&gt;Derwin Suhartono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive confidence thresholding for monocular depth estimation. (arXiv:2009.12840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12840</id>
        <link href="http://arxiv.org/abs/2009.12840"/>
        <updated>2021-08-24T01:40:30.811Z</updated>
        <summary type="html"><![CDATA[Self-supervised monocular depth estimation has become an appealing solution
to the lack of ground truth labels, but its reconstruction loss often produces
over-smoothed results across object boundaries and is incapable of handling
occlusion explicitly. In this paper, we propose a new approach to leverage
pseudo ground truth depth maps of stereo images generated from self-supervised
stereo matching methods. The confidence map of the pseudo ground truth depth
map is estimated to mitigate performance degeneration by inaccurate pseudo
depth maps. To cope with the prediction error of the confidence map itself, we
also leverage the threshold network that learns the threshold dynamically
conditioned on the pseudo depth maps. The pseudo depth labels filtered out by
the thresholded confidence map are used to supervise the monocular depth
network. Furthermore, we propose the probabilistic framework that refines the
monocular depth map with the help of its uncertainty map through the
pixel-adaptive convolution (PAC) layer. Experimental results demonstrate
superior performance to state-of-the-art monocular depth estimation methods.
Lastly, we exhibit that the proposed threshold learning can also be used to
improve the performance of existing confidence estimation approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hyesong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hunsang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunkyung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1"&gt;Dongbo Min&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flikcer -- A Chrome Extension to Resolve Online Epileptogenic Visual Content with Real-Time Luminance Frequency Analysis. (arXiv:2108.09491v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.09491</id>
        <link href="http://arxiv.org/abs/2108.09491"/>
        <updated>2021-08-24T01:40:30.806Z</updated>
        <summary type="html"><![CDATA[Video content with fast luminance variations, or with spatial patterns of
high contrast - referred to as epileptogenic visual content - may induce
seizures on viewers with photosensitive epilepsy, and even cause discomfort in
users not affected by this disease. Flikcer is a web app in the form of a
website and chrome extension which aims to resolve epileptic content in videos.
It provides the number of possible triggers for a seizure. It also provides the
timestamps for these triggers along with a safer version of the video, free to
download. The algorithm is written in Python and uses machine learning and
computer vision. A key aspect of the algorithm is its computational efficiency,
allowing real time implementation for public users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_J/0/1/0/all/0/1"&gt;Jaisal Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Ashay Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Super-Resolution Networks with Local Attribution Maps. (arXiv:2011.11036v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11036</id>
        <link href="http://arxiv.org/abs/2011.11036"/>
        <updated>2021-08-24T01:40:30.791Z</updated>
        <summary type="html"><![CDATA[Image super-resolution (SR) techniques have been developing rapidly,
benefiting from the invention of deep networks and its successive
breakthroughs. However, it is acknowledged that deep learning and deep neural
networks are difficult to interpret. SR networks inherit this mysterious nature
and little works make attempt to understand them. In this paper, we perform
attribution analysis of SR networks, which aims at finding the input pixels
that strongly influence the SR results. We propose a novel attribution approach
called local attribution map (LAM), which inherits the integral gradient method
yet with two unique features. One is to use the blurred image as the baseline
input, and the other is to adopt the progressive blurring function as the path
function. Based on LAM, we show that: (1) SR networks with a wider range of
involved input pixels could achieve better performance. (2) Attention networks
and non-local networks extract features from a wider range of input pixels. (3)
Comparing with the range that actually contributes, the receptive field is
large enough for most deep networks. (4) For SR networks, textures with regular
stripes or grids are more likely to be noticed, while complex semantics are
difficult to utilize. Our work opens new directions for designing SR networks
and interpreting low-level vision deep models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets. (arXiv:2108.09640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09640</id>
        <link href="http://arxiv.org/abs/2108.09640"/>
        <updated>2021-08-24T01:40:30.785Z</updated>
        <summary type="html"><![CDATA[Due to the stochasticity of human behaviors, predicting the future
trajectories of road agents is challenging for autonomous driving. Recently,
goal-based multi-trajectory prediction methods are proved to be effective,
where they first score over-sampled goal candidates and then select a final set
from them. However, these methods usually involve goal predictions based on
sparse pre-defined anchors and heuristic goal selection algorithms. In this
work, we propose an anchor-free and end-to-end trajectory prediction model,
named DenseTNT, that directly outputs a set of trajectories from dense goal
candidates. In addition, we introduce an offline optimization-based technique
to provide multi-future pseudo-labels for our final online model. Experiments
show that DenseTNT achieves state-of-the-art performance, ranking 1st on the
Argoverse motion forecasting benchmark and being the 1st place winner of the
2021 Waymo Open Dataset Motion Prediction Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Junru Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification. (arXiv:2106.06133v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06133</id>
        <link href="http://arxiv.org/abs/2106.06133"/>
        <updated>2021-08-24T01:40:30.780Z</updated>
        <summary type="html"><![CDATA[Unsupervised object re-identification targets at learning discriminative
representations for object retrieval without any annotations. Clustering-based
methods conduct training with the generated pseudo labels and currently
dominate this research direction. However, they still suffer from the issue of
pseudo label noise. To tackle the challenge, we propose to properly estimate
pseudo label similarities between consecutive training generations with
clustering consensus and refine pseudo labels with temporally propagated and
ensembled pseudo labels. To the best of our knowledge, this is the first
attempt to leverage the spirit of temporal ensembling to improve classification
with dynamically changing classes over generations. The proposed pseudo label
refinery strategy is simple yet effective and can be seamlessly integrated into
existing clustering-based unsupervised re-identification methods. With our
proposed approach, state-of-the-art method can be further boosted with up to
8.8% mAP improvements on the challenging MSMT17 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yixiao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Image Coding for Machines: A Content-Adaptive Approach. (arXiv:2108.09992v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09992</id>
        <link href="http://arxiv.org/abs/2108.09992"/>
        <updated>2021-08-24T01:40:30.774Z</updated>
        <summary type="html"><![CDATA[Today, according to the Cisco Annual Internet Report (2018-2023), the
fastest-growing category of Internet traffic is machine-to-machine
communication. In particular, machine-to-machine communication of images and
videos represents a new challenge and opens up new perspectives in the context
of data compression. One possible solution approach consists of adapting
current human-targeted image and video coding standards to the use case of
machine consumption. Another approach consists of developing completely new
compression paradigms and architectures for machine-to-machine communications.
In this paper, we focus on image compression and present an inference-time
content-adaptive finetuning scheme that optimizes the latent representation of
an end-to-end learned image codec, aimed at improving the compression
efficiency for machine-consumption. The conducted experiments show that our
online finetuning brings an average bitrate saving (BD-rate) of -3.66% with
respect to our pretrained image codec. In particular, at low bitrate points,
our proposed method results in a significant bitrate saving of -9.85%. Overall,
our pretrained-and-then-finetuned system achieves -30.54% BD-rate over the
state-of-the-art image/video codec Versatile Video Coding (VVC).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1"&gt;Nam Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cricri_F/0/1/0/all/0/1"&gt;Francesco Cricri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1"&gt;Ramin Ghaznavi-Youvalari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tavakoli_H/0/1/0/all/0/1"&gt;Hamed Rezazadegan Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Can Increased Randomness in Stochastic Gradient Descent Improve Generalization?. (arXiv:2108.09507v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.09507</id>
        <link href="http://arxiv.org/abs/2108.09507"/>
        <updated>2021-08-24T01:40:30.752Z</updated>
        <summary type="html"><![CDATA[Recent works report that increasing the learning rate or decreasing the
minibatch size in stochastic gradient descent (SGD) can improve test set
performance. We argue this is expected under some conditions in models with a
loss function with multiple local minima. Our main contribution is an
approximate but analytical approach inspired by methods in Physics to study the
role of the SGD learning rate and batch size in generalization. We characterize
test set performance under a shift between the training and test data
distributions for loss functions with multiple minima. The shift can simply be
due to sampling, and is therefore typically present in practical applications.
We show that the resulting shift in local minima worsens test performance by
picking up curvature, implying that generalization improves by selecting wide
and/or little-shifted local minima. We then specialize to SGD, and study its
test performance under stationarity. Because obtaining the exact stationary
distribution of SGD is intractable, we derive a Fokker-Planck approximation of
SGD and obtain its stationary distribution instead. This process shows that the
learning rate divided by the minibatch size plays a role analogous to
temperature in statistical mechanics, and implies that SGD, including its
stationary distribution, is largely invariant to changes in learning rate or
batch size that leave its temperature constant. We show that increasing SGD
temperature encourages the selection of local minima with lower curvature, and
can enable better generalization. We provide experiments on CIFAR10
demonstrating the temperature invariance of SGD, improvement of the test loss
as SGD temperature increases, and quantifying the impact of sampling versus
domain shift in driving this effect. Finally, we present synthetic experiments
showing how our theory applies in a simplified loss with two local minima.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bradley_A/0/1/0/all/0/1"&gt;Arwen V. Bradley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gomez_Uribe_C/0/1/0/all/0/1"&gt;Carlos Alberto Gomez-Uribe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIDE: Center-based Stereo 3D Detector with Structure-aware Instance Depth Estimation. (arXiv:2108.09663v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09663</id>
        <link href="http://arxiv.org/abs/2108.09663"/>
        <updated>2021-08-24T01:40:30.746Z</updated>
        <summary type="html"><![CDATA[3D detection plays an indispensable role in environment perception. Due to
the high cost of commonly used LiDAR sensor, stereo vision based 3D detection,
as an economical yet effective setting, attracts more attention recently. For
these approaches based on 2D images, accurate depth information is the key to
achieve 3D detection, and most existing methods resort to a preliminary stage
for depth estimation. They mainly focus on the global depth and neglect the
property of depth information in this specific task, namely, sparsity and
locality, where exactly accurate depth is only needed for these 3D bounding
boxes. Motivated by this finding, we propose a stereo-image based anchor-free
3D detection method, called structure-aware stereo 3D detector (termed as
SIDE), where we explore the instance-level depth information via constructing
the cost volume from RoIs of each object. Due to the information sparsity of
local cost volume, we further introduce match reweighting and structure-aware
attention, to make the depth information more concentrated. Experiments
conducted on the KITTI dataset show that our method achieves the
state-of-the-art performance compared to existing methods without depth map
supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xidong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinge Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuexin Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Representation of Imbalanced Spatio-temporal Traffic Flow Data for Traffic Accident Detection. (arXiv:2108.09506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09506</id>
        <link href="http://arxiv.org/abs/2108.09506"/>
        <updated>2021-08-24T01:40:30.740Z</updated>
        <summary type="html"><![CDATA[Automatic detection of traffic accidents has a crucial effect on improving
transportation, public safety, and path planning. Many lives can be saved by
the consequent decrease in the time between when the accidents occur and when
rescue teams are dispatched, and much travelling time can be saved by notifying
drivers to select alternative routes. This problem is challenging mainly
because of the rareness of accidents and spatial heterogeneity of the
environment. This paper studies deep representation of loop detector data using
Long-Short Term Memory (LSTM) network for automatic detection of freeway
accidents. The LSTM-based framework increases class separability in the encoded
feature space while reducing the dimension of data. Our experiments on real
accident and loop detector data collected from the Twin Cities Metro freeways
of Minnesota demonstrate that deep representation of traffic flow data using
LSTM network has the potential to detect freeway accidents in less than 18
minutes with a true positive rate of 0.71 and a false positive rate of 0.25
which outperforms other competing methods in the same arrangement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehrannia_P/0/1/0/all/0/1"&gt;Pouya Mehrannia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagi_S/0/1/0/all/0/1"&gt;Shayan Shirahmad Gale Bagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshiri_B/0/1/0/all/0/1"&gt;Behzad Moshiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Basir_O/0/1/0/all/0/1"&gt;Otman Adam Al-Basir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Inspired Top-k Adversarial Perturbations. (arXiv:2006.15669v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15669</id>
        <link href="http://arxiv.org/abs/2006.15669"/>
        <updated>2021-08-24T01:40:30.735Z</updated>
        <summary type="html"><![CDATA[The brittleness of deep image classifiers to small adversarial input
perturbations have been extensively studied in the last several years. However,
the main objective of existing perturbations is primarily limited to change the
correctly predicted Top-$1$ class by an incorrect one, which does not intend
changing the Top-$k$ prediction. In many digital real-world scenarios Top-$k$
prediction is more relevant. In this work, we propose a fast and accurate
method of computing Top-$k$ adversarial examples as a simple multi-objective
optimization. We demonstrate its efficacy and performance by comparing it to
other adversarial example crafting techniques. Moreover, based on this method,
we propose Top-$k$ Universal Adversarial Perturbations, image-agnostic tiny
perturbations that cause the true class to be absent among the Top-$k$
prediction for the majority of natural images. We experimentally show that our
approach outperforms baseline methods and even improves existing techniques of
generating Universal Adversarial Perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tursynbek_N/0/1/0/all/0/1"&gt;Nurislam Tursynbek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1"&gt;Aleksandr Petiushko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1"&gt;Ivan Oseledets&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02353</id>
        <link href="http://arxiv.org/abs/2108.02353"/>
        <updated>2021-08-24T01:40:30.724Z</updated>
        <summary type="html"><![CDATA[The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply,
which usually manifests as that the images generated by generators tend to have
a high similarity amongst them, even though their corresponding latent vectors
have been very different. In this paper, we introduce a pluggable diversity
penalty module (DPM) to alleviate mode collapse of GANs. It reduces the
similarity of image pairs in feature space, i.e., if two latent vectors are
different, then we enforce the generator to generate two images with different
features. The normalized Gram matrix is used to measure the similarity. We
compare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN
(Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et
al. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results
show that the diversity penalty module can help GAN capture much more modes of
the data distribution. Further, in classification tasks, we apply this method
as image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the
classification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared
with WGAN GP (Gulrajani et al. 2017), respectively. In domain translation,
diversity penalty module can help StarGAN (Choi et al. 2018) generate more
accurate attention masks and accelarate the convergence process. Finally, we
quantitatively evaluate the proposed method with IS and FID on CelebA,
CIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity
penalty module gets much higher IS and lower FID compared with some SOTA GAN
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1"&gt;Sen Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Richard Yi Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1"&gt;Shiming Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1"&gt;Gaofeng Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EdgeConv with Attention Module for Monocular Depth Estimation. (arXiv:2106.08615v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08615</id>
        <link href="http://arxiv.org/abs/2106.08615"/>
        <updated>2021-08-24T01:40:30.717Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation is an especially important task in robotics and
autonomous driving, where 3D structural information is essential. However,
extreme lighting conditions and complex surface objects make it difficult to
predict depth in a single image. Therefore, to generate accurate depth maps, it
is important for the model to learn structural information about the scene. We
propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module
(EAM) to solve the difficulty of monocular depth estimation. The proposed
modules extract structural information by learning the relationship between
image patches close to each other in space using edge convolution. Our method
is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen
split, achieving state-of-the-art performance. We prove that the proposed model
predicts depth robustly in challenging scenes through various comparative
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sangwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chaewon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Breast Lesion Classification Using Cross-Attention Deep Networks. (arXiv:2108.09591v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09591</id>
        <link href="http://arxiv.org/abs/2108.09591"/>
        <updated>2021-08-24T01:40:30.706Z</updated>
        <summary type="html"><![CDATA[Accurate breast lesion risk estimation can significantly reduce unnecessary
biopsies and help doctors decide optimal treatment plans. Most existing
computer-aided systems rely solely on mammogram features to classify breast
lesions. While this approach is convenient, it does not fully exploit useful
information in clinical reports to achieve the optimal performance. Would
clinical features significantly improve breast lesion classification compared
to using mammograms alone? How to handle missing clinical information caused by
variation in medical practice? What is the best way to combine mammograms and
clinical features? There is a compelling need for a systematic study to address
these fundamental questions. This paper investigates several multimodal deep
networks based on feature concatenation, cross-attention, and co-attention to
combine mammograms and categorical clinical variables. We show that the
proposed architectures significantly increase the lesion classification
performance (average area under ROC curves from 0.89 to 0.94). We also evaluate
the model when clinical variables are missing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vo_H/0/1/0/all/0/1"&gt;Hung Q. Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_P/0/1/0/all/0/1"&gt;Pengyu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_T/0/1/0/all/0/1"&gt;Tiancheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_S/0/1/0/all/0/1"&gt;Stephen T.C. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hien V. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Pooling Driven Instance Segmentation Framework for Baggage Threat Recognition. (arXiv:2108.09603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09603</id>
        <link href="http://arxiv.org/abs/2108.09603"/>
        <updated>2021-08-24T01:40:30.694Z</updated>
        <summary type="html"><![CDATA[Automated systems designed for screening contraband items from the X-ray
imagery are still facing difficulties with high clutter, concealment, and
extreme occlusion. In this paper, we addressed this challenge using a novel
multi-scale contour instance segmentation framework that effectively identifies
the cluttered contraband data within the baggage X-ray scans. Unlike standard
models that employ region-based or keypoint-based techniques to generate
multiple boxes around objects, we propose to derive proposals according to the
hierarchy of the regions defined by the contours. The proposed framework is
rigorously validated on three public datasets, dubbed GDXray, SIXray, and
OPIXray, where it outperforms the state-of-the-art methods by achieving the
mean average precision score of 0.9779, 0.9614, and 0.8396, respectively.
Furthermore, to the best of our knowledge, this is the first contour instance
segmentation framework that leverages multi-scale information to recognize
cluttered and concealed contraband data from the colored and grayscale security
X-ray imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1"&gt;Taimur Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1"&gt;Samet Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1"&gt;Naoufel Werghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP-SLAM: Object Oriented SLAM with Deep Shape Priors. (arXiv:2108.09481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09481</id>
        <link href="http://arxiv.org/abs/2108.09481"/>
        <updated>2021-08-24T01:40:30.688Z</updated>
        <summary type="html"><![CDATA[We propose DSP-SLAM, an object-oriented SLAM system that builds a rich and
accurate joint map of dense 3D models for foreground objects, and sparse
landmark points to represent the background. DSP-SLAM takes as input the 3D
point cloud reconstructed by a feature-based SLAM system and equips it with the
ability to enhance its sparse map with dense reconstructions of detected
objects. Objects are detected via semantic instance segmentation, and their
shape and pose is estimated using category-specific deep shape embeddings as
priors, via a novel second order optimization. Our object-aware bundle
adjustment builds a pose-graph to jointly optimize camera poses, object
locations and feature points. DSP-SLAM can operate at 10 frames per second on 3
different input modalities: monocular, stereo, or stereo+LiDAR. We demonstrate
DSP-SLAM operating at almost frame rate on monocular-RGB sequences from the
Friburg and Redwood-OS datasets, and on stereo+LiDAR sequences on the KITTI
odometry dataset showing that it achieves high-quality full object
reconstructions, even from partial observations, while maintaining a consistent
global map. Our evaluation shows improvements in object pose and shape
reconstruction with respect to recent deep prior-based reconstruction methods
and reductions in camera tracking drift on the KITTI dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingwen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Runz_M/0/1/0/all/0/1"&gt;Martin R&amp;#xfc;nz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1"&gt;Lourdes Agapito&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Underwater Image Enhancement. (arXiv:2108.09650v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09650</id>
        <link href="http://arxiv.org/abs/2108.09650"/>
        <updated>2021-08-24T01:40:30.682Z</updated>
        <summary type="html"><![CDATA[Recently, learning-based algorithms have shown impressive performance in
underwater image enhancement. Most of them resort to training on synthetic data
and achieve outstanding performance. However, these methods ignore the
significant domain gap between the synthetic and real data (i.e., interdomain
gap), and thus the models trained on synthetic data often fail to generalize
well to real underwater scenarios. Furthermore, the complex and changeable
underwater environment also causes a great distribution gap among the real data
itself (i.e., intra-domain gap). However, almost no research focuses on this
problem and thus their techniques often produce visually unpleasing artifacts
and color distortions on various real images. Motivated by these observations,
we propose a novel Two-phase Underwater Domain Adaptation network (TUDA) to
simultaneously minimize the inter-domain and intra-domain gap. Concretely, a
new dual-alignment network is designed in the first phase, including a
translation part for enhancing realism of input images, followed by an
enhancement part. With performing image-level and feature-level adaptation in
two parts by jointly adversarial learning, the network can better build
invariance across domains and thus bridge the inter-domain gap. In the second
phase, we perform an easy-hard classification of real data according to the
assessed quality of enhanced images, where a rank-based underwater quality
assessment method is embedded. By leveraging implicit quality information
learned from rankings, this method can more accurately assess the perceptual
quality of enhanced images. Using pseudo labels from the easy part, an
easy-hard adaptation technique is then conducted to effectively decrease the
intra-domain gap between easy and hard samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengyong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Liquan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yufei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mai Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L3C-Stereo: Lossless Compression for Stereo Images. (arXiv:2108.09422v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09422</id>
        <link href="http://arxiv.org/abs/2108.09422"/>
        <updated>2021-08-24T01:40:30.669Z</updated>
        <summary type="html"><![CDATA[A large number of autonomous driving tasks need high-definition stereo
images, which requires a large amount of storage space. Efficiently executing
lossless compression has become a practical problem. Commonly, it is hard to
make accurate probability estimates for each pixel. To tackle this, we propose
L3C-Stereo, a multi-scale lossless compression model consisting of two main
modules: the warping module and the probability estimation module. The warping
module takes advantage of two view feature maps from the same domain to
generate a disparity map, which is used to reconstruct the right view so as to
improve the confidence of the probability estimate of the right view. The
probability estimation module provides pixel-wise logistic mixture
distributions for adaptive arithmetic coding. In the experiments, our method
outperforms the hand-crafted compression methods and the learning-based method
on all three datasets used. Then, we show that a better maximum disparity can
lead to a better compression effect. Furthermore, thanks to a compression
property of our model, it naturally generates a disparity map of an acceptable
quality for the subsequent stereo tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhe Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_F/0/1/0/all/0/1"&gt;Feng Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cichocki_A/0/1/0/all/0/1"&gt;Andrzej Cichocki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ruan_P/0/1/0/all/0/1"&gt;Peiying Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSR: Semi-supervised Soft Rasterizer for single-view 2D to 3D Reconstruction. (arXiv:2108.09593v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09593</id>
        <link href="http://arxiv.org/abs/2108.09593"/>
        <updated>2021-08-24T01:40:30.663Z</updated>
        <summary type="html"><![CDATA[Recent work has made significant progress in learning object meshes with weak
supervision. Soft Rasterization methods have achieved accurate 3D
reconstruction from 2D images with viewpoint supervision only. In this work, we
further reduce the labeling effort by allowing such 3D reconstruction methods
leverage unlabeled images. In order to obtain the viewpoints for these
unlabeled images, we propose to use a Siamese network that takes two images as
input and outputs whether they correspond to the same viewpoint. During
training, we minimize the cross entropy loss to maximize the probability of
predicting whether a pair of images belong to the same viewpoint or not. To get
the viewpoint of a new image, we compare it against different viewpoints
obtained from the training samples and select the viewpoint with the highest
matching probability. We finally label the unlabeled images with the most
confident predicted viewpoint and train a deep network that has a
differentiable rasterization layer. Our experiments show that even labeling
only two objects yields significant improvement in IoU for ShapeNet when
leveraging unlabeled examples. Code is available at
https://github.com/IssamLaradji/SSR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1"&gt;Issam Laradji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1"&gt;Pau Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1"&gt;David Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1"&gt;Derek Nowrouzezahrai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoundaryNet: An Attentive Deep Network with Fast Marching Distance Maps for Semi-automatic Layout Annotation. (arXiv:2108.09433v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09433</id>
        <link href="http://arxiv.org/abs/2108.09433"/>
        <updated>2021-08-24T01:40:30.658Z</updated>
        <summary type="html"><![CDATA[Precise boundary annotations of image regions can be crucial for downstream
applications which rely on region-class semantics. Some document collections
contain densely laid out, highly irregular and overlapping multi-class region
instances with large range in aspect ratio. Fully automatic boundary estimation
approaches tend to be data intensive, cannot handle variable-sized images and
produce sub-optimal results for aforementioned images. To address these issues,
we propose BoundaryNet, a novel resizing-free approach for high-precision
semi-automatic layout annotation. The variable-sized user selected region of
interest is first processed by an attention-guided skip network. The network
optimization is guided via Fast Marching distance maps to obtain a good quality
initial boundary estimate and an associated feature representation. These
outputs are processed by a Residual Graph Convolution Network optimized using
Hausdorff loss to obtain the final region boundary. Results on a challenging
image manuscript dataset demonstrate that BoundaryNet outperforms strong
baselines and produces high-quality semantic region boundaries. Qualitatively,
our approach generalizes across multiple document image datasets containing
different script systems and layouts, all without additional fine-tuning. We
integrate BoundaryNet into a document annotation system and show that it
provides high annotation throughput compared to manual and fully automatic
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Abhishek Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARAPReg: An As-Rigid-As Possible Regularization Loss for Learning Deformable Shape Generators. (arXiv:2108.09432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09432</id>
        <link href="http://arxiv.org/abs/2108.09432"/>
        <updated>2021-08-24T01:40:30.652Z</updated>
        <summary type="html"><![CDATA[This paper introduces an unsupervised loss for training parametric
deformation shape generators. The key idea is to enforce the preservation of
local rigidity among the generated shapes. Our approach builds on an
approximation of the as-rigid-as possible (or ARAP) deformation energy. We show
how to develop the unsupervised loss via a spectral decomposition of the
Hessian of the ARAP energy. Our loss nicely decouples pose and shape variations
through a robust norm. The loss admits simple closed-form expressions. It is
easy to train and can be plugged into any standard generation models, e.g.,
variational auto-encoder (VAE) and auto-decoder (AD). Experimental results show
that our approach outperforms existing shape generation approaches considerably
on public benchmark datasets of various shape categories such as human, animal
and bone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiangru Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bo Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zaiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junfeng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1"&gt;Chandrajit Bajaj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image coding for machines: an end-to-end learned approach. (arXiv:2108.09993v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09993</id>
        <link href="http://arxiv.org/abs/2108.09993"/>
        <updated>2021-08-24T01:40:30.640Z</updated>
        <summary type="html"><![CDATA[Over recent years, deep learning-based computer vision systems have been
applied to images at an ever-increasing pace, oftentimes representing the only
type of consumption for those images. Given the dramatic explosion in the
number of images generated per day, a question arises: how much better would an
image codec targeting machine-consumption perform against state-of-the-art
codecs targeting human-consumption? In this paper, we propose an image codec
for machines which is neural network (NN) based and end-to-end learned. In
particular, we propose a set of training strategies that address the delicate
problem of balancing competing loss functions, such as computer vision task
losses, image distortion losses, and rate loss. Our experimental results show
that our NN-based codec outperforms the state-of-the-art Versa-tile Video
Coding (VVC) standard on the object detection and instance segmentation tasks,
achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast
thanks to its compact size. To the best of our knowledge, this is the first
end-to-end learned machine-targeted image codec.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Nam Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cricri_F/0/1/0/all/0/1"&gt;Francesco Cricri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1"&gt;Ramin Ghaznavi-Youvalari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StarVQA: Space-Time Attention for Video Quality Assessment. (arXiv:2108.09635v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09635</id>
        <link href="http://arxiv.org/abs/2108.09635"/>
        <updated>2021-08-24T01:40:30.635Z</updated>
        <summary type="html"><![CDATA[The attention mechanism is blooming in computer vision nowadays. However, its
application to video quality assessment (VQA) has not been reported. Evaluating
the quality of in-the-wild videos is challenging due to the unknown of pristine
reference and shooting distortion. This paper presents a novel
\underline{s}pace-\underline{t}ime \underline{a}ttention network
fo\underline{r} the \underline{VQA} problem, named StarVQA. StarVQA builds a
Transformer by alternately concatenating the divided space-time attention. To
adapt the Transformer architecture for training, StarVQA designs a vectorized
regression loss by encoding the mean opinion score (MOS) to the probability
vector and embedding a special vectorized label token as the learnable
variable. To capture the long-range spatiotemporal dependencies of a video
sequence, StarVQA encodes the space-time position information of each patch to
the input of the Transformer. Various experiments are conducted on the de-facto
in-the-wild video datasets, including LIVE-VQC, KoNViD-1k, LSVQ, and
LSVQ-1080p. Experimental results demonstrate the superiority of the proposed
StarVQA over the state-of-the-art. Code and model will be available at:
https://github.com/DVL/StarVQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fengchuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuan-Gen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanpin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Leida Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guopu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network. (arXiv:2108.09661v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09661</id>
        <link href="http://arxiv.org/abs/2108.09661"/>
        <updated>2021-08-24T01:40:30.614Z</updated>
        <summary type="html"><![CDATA[In this paper, we abandon the dominant complex language model and rethink the
linguistic learning process in the scene text recognition. Different from
previous methods considering the visual and linguistic information in two
separate structures, we propose a Visual Language Modeling Network (VisionLAN),
which views the visual and linguistic information as a union by directly
enduing the vision model with language capability. Specially, we introduce the
text recognition of character-wise occluded feature maps in the training stage.
Such operation guides the vision model to use not only the visual texture of
characters, but also the linguistic information in visual context for
recognition when the visual cues are confused (e.g. occlusion, noise, etc.). As
the linguistic information is acquired along with visual features without the
need of extra language model, VisionLAN significantly improves the speed by 39%
and adaptively considers the linguistic information to enhance the visual
features for accurate recognition. Furthermore, an Occlusion Scene Text (OST)
dataset is proposed to evaluate the performance on the case of missing
character-wise visual cues. The state of-the-art results on several benchmarks
prove our effectiveness. Code and dataset are available at
https://github.com/wangyuxin87/VisionLAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Hongtao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shancheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shenggao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OSRM-CCTV: Open-source CCTV-aware routing and navigation system for privacy, anonymity and safety (Preprint). (arXiv:2108.09369v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09369</id>
        <link href="http://arxiv.org/abs/2108.09369"/>
        <updated>2021-08-24T01:40:30.243Z</updated>
        <summary type="html"><![CDATA[For the last several decades, the increased, widespread, unwarranted, and
unaccountable use of Closed-Circuit TeleVision (CCTV) cameras globally has
raised concerns about privacy risks. Additional recent features of many CCTV
cameras, such as Internet of Things (IoT) connectivity and Artificial
Intelligence (AI)-based facial recognition, only increase concerns among
privacy advocates. Therefore, on par \emph{CCTV-aware solutions} must exist
that provide privacy, safety, and cybersecurity features. We argue that an
important step forward is to develop solutions addressing privacy concerns via
routing and navigation systems (e.g., OpenStreetMap, Google Maps) that provide
both privacy and safety options for areas where cameras are known to be
present. However, at present no routing and navigation system, whether online
or offline, provide corresponding CCTV-aware functionality.

In this paper we introduce OSRM-CCTV -- the first and only CCTV-aware routing
and navigation system designed and built for privacy, anonymity and safety
applications. We validate and demonstrate the effectiveness and usability of
the system on a handful of synthetic and real-world examples. To help validate
our work as well as to further encourage the development and wide adoption of
the system, we release OSRM-CCTV as open-source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sintonen_L/0/1/0/all/0/1"&gt;Lauri Sintonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1"&gt;Hannu Turtiainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1"&gt;Andrei Costin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_T/0/1/0/all/0/1"&gt;Timo Hamalainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1"&gt;Tuomo Lahtinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Construction material classification on imbalanced datasets for construction monitoring automation using Vision Transformer (ViT) architecture. (arXiv:2108.09527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09527</id>
        <link href="http://arxiv.org/abs/2108.09527"/>
        <updated>2021-08-24T01:40:30.234Z</updated>
        <summary type="html"><![CDATA[Nowadays, automation is a critical topic due to its significant impacts on
the productivity of construction projects. Utilizing automation in this
industry brings about great results, such as remarkable improvements in the
efficiency, quality, and safety of construction activities. The scope of
automation in construction includes a wide range of stages, and monitoring
construction projects is no exception. Additionally, it is of great importance
in project management since an accurate and timely assessment of project
progress enables managers to quickly identify deviations from the schedule and
take the required actions at the right time. In this stage, one of the most
important tasks is to daily keep track of the project progress, which is very
time-consuming and labor-intensive, but automation has facilitated and
accelerated this task. It also eliminated or at least decreased the risk of
many dangerous tasks. In this way, the first step of construction automation is
to detect used materials in a project site automatically. In this paper, a
novel deep learning architecture is utilized, called Vision Transformer (ViT),
for detecting and classifying construction materials. To evaluate the
applicability and performance of the proposed method, it is trained and tested
on three large imbalanced datasets, namely Construction Material Library (CML)
and Building Material Dataset (BMD), used in the previous papers, as well as a
new dataset created by combining them. The achieved results revealed an
accuracy of 100 percent in all parameters and also in each material category.
It is believed that the proposed method provides a novel and robust tool for
detecting and classifying different material types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1"&gt;Maryam Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonyani_M/0/1/0/all/0/1"&gt;Mahdi Bonyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahami_H/0/1/0/all/0/1"&gt;Hadi Mahami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasirzadeh_F/0/1/0/all/0/1"&gt;Farnad Nasirzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BlockCopy: High-Resolution Video Processing with Block-Sparse Feature Propagation and Online Policies. (arXiv:2108.09376v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09376</id>
        <link href="http://arxiv.org/abs/2108.09376"/>
        <updated>2021-08-24T01:40:30.223Z</updated>
        <summary type="html"><![CDATA[In this paper we propose BlockCopy, a scheme that accelerates pretrained
frame-based CNNs to process video more efficiently, compared to standard
frame-by-frame processing. To this end, a lightweight policy network determines
important regions in an image, and operations are applied on selected regions
only, using custom block-sparse convolutions. Features of non-selected regions
are simply copied from the preceding frame, reducing the number of computations
and latency. The execution policy is trained using reinforcement learning in an
online fashion without requiring ground truth annotations. Our universal
framework is demonstrated on dense prediction tasks such as pedestrian
detection, instance segmentation and semantic segmentation, using both state of
the art (Center and Scale Predictor, MGAN, SwiftNet) and standard baseline
networks (Mask-RCNN, DeepLabV3+). BlockCopy achieves significant FLOPS savings
and inference speedup with minimal impact on accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verelst_T/0/1/0/all/0/1"&gt;Thomas Verelst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1"&gt;Tinne Tuytelaars&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Technical Survey and Evaluation of Traditional Point Cloud Clustering Methods for LiDAR Panoptic Segmentation. (arXiv:2108.09522v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09522</id>
        <link href="http://arxiv.org/abs/2108.09522"/>
        <updated>2021-08-24T01:40:30.210Z</updated>
        <summary type="html"><![CDATA[LiDAR panoptic segmentation is a newly proposed technical task for autonomous
driving. In contrast to popular end-to-end deep learning solutions, we propose
a hybrid method with an existing semantic segmentation network to extract
semantic information and a traditional LiDAR point cloud cluster algorithm to
split each instance object. We argue geometry-based traditional clustering
algorithms are worth being considered by showing a state-of-the-art performance
among all published end-to-end deep learning solutions on the panoptic
segmentation leaderboard of the SemanticKITTI dataset. To our best knowledge,
we are the first to attempt the point cloud panoptic segmentation with
clustering algorithms. Therefore, instead of working on new models, we give a
comprehensive technical survey in this paper by implementing four typical
cluster methods and report their performances on the benchmark. Those four
cluster methods are the most representative ones with real-time running speed.
They are implemented with C++ in this paper and then wrapped as a python
function for seamless integration with the existing deep learning frameworks.
We release our code for peer researchers who might be interested in this
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xinming Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-08-24T01:40:30.191Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from the human's visual and intuitive perspective. We
take the first step to bridge the gap by proposing a deep learning-based
technique to automatically classify road networks into four classes on a visual
basis. The method is implemented by generating an image of the street network
(Colored Road Hierarchy Diagram), which we introduce in this paper, and
classifying it using a deep convolutional neural network (ResNet-34). The model
achieves an overall classification accuracy of 0.875. Nine cities around the
world are selected as the study areas with their road networks acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: we apply
our method in a case study of urban vitality prediction. An advanced tree-based
regression model (LightGBM) is for the first time designated to establish the
relationship between morphological indices and vitality indicators. The effect
of road network classification is found to be small but positively associated
with urban vitality. This work expands the toolkit of quantitative urban
morphology study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synthesis-Based Approach for Thermal-to-Visible Face Verification. (arXiv:2108.09558v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09558</id>
        <link href="http://arxiv.org/abs/2108.09558"/>
        <updated>2021-08-24T01:40:30.174Z</updated>
        <summary type="html"><![CDATA[In recent years, visible-spectrum face verification systems have been shown
to match expert forensic examiner recognition performance. However, such
systems are ineffective in low-light and nighttime conditions. Thermal face
imagery, which captures body heat emissions, effectively augments the visible
spectrum, capturing discriminative facial features in scenes with limited
illumination. Due to the increased cost and difficulty of obtaining diverse,
paired thermal and visible spectrum datasets, algorithms and large-scale
benchmarks for low-light recognition are limited. This paper presents an
algorithm that achieves state-of-the-art performance on both the ARL-VTF and
TUFTS multi-spectral face datasets. Importantly, we study the impact of face
alignment, pixel-level correspondence, and identity classification with label
smoothing for multi-spectral face synthesis and verification. We show that our
proposed method is widely applicable, robust, and highly effective. In
addition, we show that the proposed method significantly outperforms face
frontalization methods on profile-to-frontal verification. Finally, we present
MILAB-VTF(B), a challenging multi-spectral face dataset that is composed of
paired thermal and visible videos. To the best of our knowledge, with face data
from 400 subjects, this dataset represents the most extensive collection of
publicly available indoor and long-range outdoor thermal-visible face imagery.
Lastly, we show that our end-to-end thermal-to-visible face verification system
provides strong performance on the MILAB-VTF(B) dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1"&gt;Neehar Peri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1"&gt;Joshua Gleason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1"&gt;Carlos D. Castillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bourlai_T/0/1/0/all/0/1"&gt;Thirimachos Bourlai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Palmira: A Deep Deformable Network for Instance Segmentation of Dense and Uneven Layouts in Handwritten Manuscripts. (arXiv:2108.09436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09436</id>
        <link href="http://arxiv.org/abs/2108.09436"/>
        <updated>2021-08-24T01:40:30.168Z</updated>
        <summary type="html"><![CDATA[Handwritten documents are often characterized by dense and uneven layout.
Despite advances, standard deep network based approaches for semantic layout
segmentation are not robust to complex deformations seen across semantic
regions. This phenomenon is especially pronounced for the low-resource Indic
palm-leaf manuscript domain. To address the issue, we first introduce
Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with
semantic layout annotations. Indiscapes2 contains documents from four different
historical collections and is 150% larger than its predecessor, Indiscapes. We
also propose a novel deep network Palmira for robust, deformation-aware
instance segmentation of regions in handwritten manuscripts. We also report
Hausdorff distance and its variants as a boundary-aware performance measure.
Our experiments demonstrate that Palmira provides robust layouts, outperforms
strong baseline approaches and ablative variants. We also include qualitative
results on Arabic, South-East Asian and Hebrew historical manuscripts to
showcase the generalization capability of Palmira.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharan_P/0/1/0/all/0/1"&gt;Prema Satish Sharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitha_S/0/1/0/all/0/1"&gt;Sowmya Aitha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Amandeep Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Abhishek Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augustine_A/0/1/0/all/0/1"&gt;Aaron Augustine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Ensembling Network for Unsupervised Domain Adaptation. (arXiv:2108.09473v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09473</id>
        <link href="http://arxiv.org/abs/2108.09473"/>
        <updated>2021-08-24T01:40:30.161Z</updated>
        <summary type="html"><![CDATA[Recently, in order to address the unsupervised domain adaptation (UDA)
problem, extensive studies have been proposed to achieve transferrable models.
Among them, the most prevalent method is adversarial domain adaptation, which
can shorten the distance between the source domain and the target domain.
Although adversarial learning is very effective, it still leads to the
instability of the network and the drawbacks of confusing category information.
In this paper, we propose a Robust Ensembling Network (REN) for UDA, which
applies a robust time ensembling teacher network to learn global information
for domain transfer. Specifically, REN mainly includes a teacher network and a
student network, which performs standard domain adaptation training and updates
weights of the teacher network. In addition, we also propose a dual-network
conditional adversarial loss to improve the ability of the discriminator.
Finally, for the purpose of improving the basic ability of the student network,
we utilize the consistency constraint to balance the error between the student
network and the teacher network. Extensive experimental results on several UDA
datasets have demonstrated the effectiveness of our model by comparing with
other state-of-the-art UDA algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Han Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Ningzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?. (arXiv:2108.09518v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09518</id>
        <link href="http://arxiv.org/abs/2108.09518"/>
        <updated>2021-08-24T01:40:30.155Z</updated>
        <summary type="html"><![CDATA[Deep learning-based methods for video pedestrian detection and tracking
require large volumes of training data to achieve good performance. However,
data acquisition in crowded public environments raises data privacy concerns --
we are not allowed to simply record and store data without the explicit consent
of all participants. Furthermore, the annotation of such data for computer
vision applications usually requires a substantial amount of manual effort,
especially in the video domain. Labeling instances of pedestrians in highly
crowded scenarios can be challenging even for human annotators and may
introduce errors in the training data. In this paper, we study how we can
advance different aspects of multi-person tracking using solely synthetic data.
To this end, we generate MOTSynth, a large, highly diverse synthetic dataset
for object detection and tracking using a rendering game engine. Our
experiments show that MOTSynth can be used as a replacement for real data on
tasks such as pedestrian detection, re-identification, segmentation, and
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fabbri_M/0/1/0/all/0/1"&gt;Matteo Fabbri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braso_G/0/1/0/all/0/1"&gt;Guillem Braso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maugeri_G/0/1/0/all/0/1"&gt;Gianluca Maugeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cetintas_O/0/1/0/all/0/1"&gt;Orcun Cetintas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasparini_R/0/1/0/all/0/1"&gt;Riccardo Gasparini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1"&gt;Aljosa Osep&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1"&gt;Simone Calderara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale Edge-based U-shape Network for Salient Object Detection. (arXiv:2108.09408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09408</id>
        <link href="http://arxiv.org/abs/2108.09408"/>
        <updated>2021-08-24T01:40:30.142Z</updated>
        <summary type="html"><![CDATA[Deep-learning based salient object detection methods achieve great
improvements. However, there are still problems existing in the predictions,
such as blurry boundary and inaccurate location, which is mainly caused by
inadequate feature extraction and integration. In this paper, we propose a
Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at
different scales to achieve better performance. To extract more useful
information for boundary prediction, U-shape Edge Network modules are embedded
in each decoder units. Besides, the additional down-sampling module alleviates
the location inaccuracy. Experimental results on four benchmark datasets
demonstrate the validity and reliability of the proposed method. Multi-scale
Edge based U-shape Network also shows its superiority when compared with 15
state-of-the-art salient object detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Han Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1"&gt;Yetong Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Ningzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Margin Maximization via Dual Acceleration. (arXiv:2107.00595v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00595</id>
        <link href="http://arxiv.org/abs/2107.00595"/>
        <updated>2021-08-24T01:40:30.120Z</updated>
        <summary type="html"><![CDATA[We present and analyze a momentum-based gradient method for training linear
classifiers with an exponentially-tailed loss (e.g., the exponential or
logistic loss), which maximizes the classification margin on separable data at
a rate of $\widetilde{\mathcal{O}}(1/t^2)$. This contrasts with a rate of
$\mathcal{O}(1/\log(t))$ for standard gradient descent, and $\mathcal{O}(1/t)$
for normalized gradient descent. This momentum-based method is derived via the
convex dual of the maximum-margin problem, and specifically by applying
Nesterov acceleration to this dual, which manages to result in a simple and
intuitive method in the primal. This dual view can also be used to derive a
stochastic variant, which performs adaptive non-uniform sampling via the dual
variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Ziwei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Telgarsky_M/0/1/0/all/0/1"&gt;Matus Telgarsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Systematic Clinical Evaluation of A Deep Learning Method for Medical Image Segmentation: Radiosurgery Application. (arXiv:2108.09535v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09535</id>
        <link href="http://arxiv.org/abs/2108.09535"/>
        <updated>2021-08-24T01:40:30.112Z</updated>
        <summary type="html"><![CDATA[We systematically evaluate a Deep Learning (DL) method in a 3D medical image
segmentation task. Our segmentation method is integrated into the radiosurgery
treatment process and directly impacts the clinical workflow. With our method,
we address the relative drawbacks of manual segmentation: high inter-rater
contouring variability and high time consumption of the contouring process. The
main extension over the existing evaluations is the careful and detailed
analysis that could be further generalized on other medical image segmentation
tasks. Firstly, we analyze the changes in the inter-rater detection agreement.
We show that the segmentation model reduces the ratio of detection
disagreements from 0.162 to 0.085 (p < 0.05). Secondly, we show that the model
improves the inter-rater contouring agreement from 0.845 to 0.871 surface Dice
Score (p < 0.05). Thirdly, we show that the model accelerates the delineation
process in between 1.6 and 2.0 times (p < 0.05). Finally, we design the setup
of the clinical experiment to either exclude or estimate the evaluation biases,
thus preserve the significance of the results. Besides the clinical evaluation,
we also summarize the intuitions and practical ideas for building an efficient
DL-based model for 3D medical image segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shirokikh_B/0/1/0/all/0/1"&gt;Boris Shirokikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalechina_A/0/1/0/all/0/1"&gt;Alexandra Dalechina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shevtsov_A/0/1/0/all/0/1"&gt;Alexey Shevtsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Krivov_E/0/1/0/all/0/1"&gt;Egor Krivov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kostjuchenko_V/0/1/0/all/0/1"&gt;Valery Kostjuchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Durgaryan_A/0/1/0/all/0/1"&gt;Amayak Durgaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Golanov_A/0/1/0/all/0/1"&gt;Andrey Golanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform. (arXiv:2108.09551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09551</id>
        <link href="http://arxiv.org/abs/2108.09551"/>
        <updated>2021-08-24T01:40:30.083Z</updated>
        <summary type="html"><![CDATA[We propose a versatile deep image compression network based on Spatial
Feature Transform (SFT arXiv:1804.02815), which takes a source image and a
corresponding quality map as inputs and produce a compressed image with
variable rates. Our model covers a wide range of compression rates using a
single model, which is controlled by arbitrary pixel-wise quality maps. In
addition, the proposed framework allows us to perform task-aware image
compressions for various tasks, e.g., classification, by efficiently estimating
optimized quality maps specific to target tasks for our encoding network. This
is even possible with a pretrained network without learning separate models for
individual tasks. Our algorithm achieves outstanding rate-distortion trade-off
compared to the approaches based on multiple models that are optimized
separately for several different target rates. At the same level of
compression, the proposed approach successfully improves performance on image
classification and text region quality preservation via task-aware quality map
estimation without additional model training. The code is available at the
project website: https://github.com/micmic123/QmapCompression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Song_M/0/1/0/all/0/1"&gt;Myungseo Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preferential Temporal Difference Learning. (arXiv:2106.06508v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06508</id>
        <link href="http://arxiv.org/abs/2106.06508"/>
        <updated>2021-08-24T01:40:30.078Z</updated>
        <summary type="html"><![CDATA[Temporal-Difference (TD) learning is a general and very useful tool for
estimating the value function of a given policy, which in turn is required to
find good policies. Generally speaking, TD learning updates states whenever
they are visited. When the agent lands in a state, its value can be used to
compute the TD-error, which is then propagated to other states. However, it may
be interesting, when computing updates, to take into account other information
than whether a state is visited or not. For example, some states might be more
important than others (such as states which are frequently seen in a successful
trajectory). Or, some states might have unreliable value estimates (for
example, due to partial observability or lack of data), making their values
less desirable as targets. We propose an approach to re-weighting states used
in TD updates, both when they are the input and when they provide the target
for the update. We prove that our approach converges with linear function
approximation and illustrate its desirable empirical behaviour compared to
other TD-style methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1"&gt;Nishanth Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early-exit deep neural networks for distorted images: providing an efficient edge offloading. (arXiv:2108.09343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09343</id>
        <link href="http://arxiv.org/abs/2108.09343"/>
        <updated>2021-08-24T01:40:30.073Z</updated>
        <summary type="html"><![CDATA[Edge offloading for deep neural networks (DNNs) can be adaptive to the
input's complexity by using early-exit DNNs. These DNNs have side branches
throughout their architecture, allowing the inference to end earlier in the
edge. The branches estimate the accuracy for a given input. If this estimated
accuracy reaches a threshold, the inference ends on the edge. Otherwise, the
edge offloads the inference to the cloud to process the remaining DNN layers.
However, DNNs for image classification deals with distorted images, which
negatively impact the branches' estimated accuracy. Consequently, the edge
offloads more inferences to the cloud. This work introduces expert side
branches trained on a particular distortion type to improve robustness against
image distortion. The edge detects the distortion type and selects appropriate
expert branches to perform the inference. This approach increases the estimated
accuracy on the edge, improving the offloading decisions. We validate our
proposal in a realistic scenario, in which the edge offloads DNN inference to
Amazon EC2 instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1"&gt;Roberto G. Pacheco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Fernanda D.V.R. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couto_R/0/1/0/all/0/1"&gt;Rodrigo S. Couto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Better Segment Objects from Unseen Classes with Unlabeled Videos. (arXiv:2104.12276v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12276</id>
        <link href="http://arxiv.org/abs/2104.12276"/>
        <updated>2021-08-24T01:40:30.052Z</updated>
        <summary type="html"><![CDATA[The ability to localize and segment objects from unseen classes would open
the door to new applications, such as autonomous object learning in active
vision. Nonetheless, improving the performance on unseen classes requires
additional training data, while manually annotating the objects of the unseen
classes can be labor-extensive and expensive. In this paper, we explore the use
of unlabeled video sequences to automatically generate training data for
objects of unseen classes. It is in principle possible to apply existing video
segmentation methods to unlabeled videos and automatically obtain object masks,
which can then be used as a training set even for classes with no manual labels
available. However, our experiments show that these methods do not perform well
enough for this purpose. We therefore introduce a Bayesian method that is
specifically designed to automatically create such a training set: Our method
starts from a set of object proposals and relies on (non-realistic)
analysis-by-synthesis to select the correct ones by performing an efficient
optimization over all the frames simultaneously. Through extensive experiments,
we show that our method can generate a high-quality training set which
significantly boosts the performance of segmenting objects of unseen classes.
We thus believe that our method could open the door for open-world instance
segmentation using abundant Internet videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuming Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1"&gt;Vincent Lepetit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence Selection for Active Learning. (arXiv:2108.09331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09331</id>
        <link href="http://arxiv.org/abs/2108.09331"/>
        <updated>2021-08-24T01:40:30.047Z</updated>
        <summary type="html"><![CDATA[The existing active learning methods select the samples by evaluating the
sample's uncertainty or its effect on the diversity of labeled datasets based
on different task-specific or model-specific criteria. In this paper, we
propose the Influence Selection for Active Learning(ISAL) which selects the
unlabeled samples that can provide the most positive Influence on model
performance. To obtain the Influence of the unlabeled sample in the active
learning scenario, we design the Untrained Unlabeled sample Influence
Calculation(UUIC) to estimate the unlabeled sample's expected gradient with
which we calculate its Influence. To prove the effectiveness of UUIC, we
provide both theoretical and experimental analyses. Since the UUIC just depends
on the model gradients, which can be obtained easily from any neural network,
our active learning algorithm is task-agnostic and model-agnostic. ISAL
achieves state-of-the-art performance in different active learning settings for
different tasks with different datasets. Compared with previous methods, our
method decreases the annotation cost at least by 12%, 13% and 16% on CIFAR10,
VOC2012 and COCO, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhuoming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Hao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Huaping Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weijia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Conghui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aegis: A Trusted, Automatic and Accurate Verification Framework for Vertical Federated Learning. (arXiv:2108.06958v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06958</id>
        <link href="http://arxiv.org/abs/2108.06958"/>
        <updated>2021-08-24T01:40:30.040Z</updated>
        <summary type="html"><![CDATA[Vertical federated learning (VFL) leverages various privacy-preserving
algorithms, e.g., homomorphic encryption or secret sharing based SecureBoost,
to ensure data privacy. However, these algorithms all require a semi-honest
secure definition, which raises concerns in real-world applications. In this
paper, we present Aegis, a trusted, automatic, and accurate verification
framework to verify the security of VFL jobs. Aegis is separated from local
parties to ensure the security of the framework. Furthermore, it automatically
adapts to evolving VFL algorithms by defining the VFL job as a finite state
machine to uniformly verify different algorithms and reproduce the entire job
to provide more accurate verification. We implement and evaluate Aegis with
different threat models on financial and medical datasets. Evaluation results
show that: 1) Aegis can detect 95% threat models, and 2) it provides
fine-grained verification results within 84% of the total VFL job time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cengguang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junxue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_D/0/1/0/all/0/1"&gt;Di Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07160</id>
        <link href="http://arxiv.org/abs/2106.07160"/>
        <updated>2021-08-24T01:40:30.034Z</updated>
        <summary type="html"><![CDATA[We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1"&gt;Kim Hammar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1"&gt;Rolf Stadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00700</id>
        <link href="http://arxiv.org/abs/2108.00700"/>
        <updated>2021-08-24T01:40:30.025Z</updated>
        <summary type="html"><![CDATA[The activation function is at the heart of a deep neural networks
nonlinearity; the choice of the function has great impact on the success of
training. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)
due to its simplicity and reliability, despite its few drawbacks. While most
previous functions proposed to supplant ReLU have been hand-designed, recent
work on learning the function during training has shown promising results. In
this paper we propose an adaptive piecewise linear activation function, the
Piecewise Linear Unit (PiLU), which can be learned independently for each
dimension of the neural network. We demonstrate how PiLU is a generalised
rectifier unit and note its similarities with the Adaptive Piecewise Linear
Units, namely adaptive and piecewise linear. Across a distribution of 30
experiments, we show that for the same model architecture, hyperparameters, and
pre-processing, PiLU significantly outperforms ReLU: reducing classification
error by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in
the number of neurons. Further work should be dedicated to exploring
generalised piecewise linear units, as well as verifying these results across
other challenging domains and larger problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1"&gt;Jordan Inturrisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1"&gt;Sui Yang Khoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1"&gt;Abbas Kouzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1"&gt;Riccardo Pagliarella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition. (arXiv:2108.09322v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09322</id>
        <link href="http://arxiv.org/abs/2108.09322"/>
        <updated>2021-08-24T01:40:30.010Z</updated>
        <summary type="html"><![CDATA[This paper presents a pure transformer-based approach, dubbed the Multi-Modal
Video Transformer (MM-ViT), for video action recognition. Different from other
schemes which solely utilize the decoded RGB frames, MM-ViT operates
exclusively in the compressed video domain and exploits all readily available
modalities, i.e., I-frames, motion vectors, residuals and audio waveform. In
order to handle the large number of spatiotemporal tokens extracted from
multiple modalities, we develop several scalable model variants which factorize
self-attention across the space, time and modality dimensions. In addition, to
further explore the rich inter-modal interactions and their effects, we develop
and compare three distinct cross-modal attention mechanisms that can be
seamlessly integrated into the transformer building block. Extensive
experiments on three public action recognition benchmarks (UCF-101,
Something-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the
state-of-the-art video transformers in both efficiency and accuracy, and
performs better or equally well to the state-of-the-art CNN counterparts with
computationally-heavy optical flow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1"&gt;Chiu Man Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity. (arXiv:2105.04854v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04854</id>
        <link href="http://arxiv.org/abs/2105.04854"/>
        <updated>2021-08-24T01:40:30.005Z</updated>
        <summary type="html"><![CDATA[Rationalizing which parts of a molecule drive the predictions of a molecular
graph convolutional neural network (GCNN) can be difficult. To help, we propose
two simple regularization techniques to apply during the training of GCNNs:
Batch Representation Orthonormalization (BRO) and Gini regularization. BRO,
inspired by molecular orbital theory, encourages graph convolution operations
to generate orthonormal node embeddings. Gini regularization is applied to the
weights of the output layer and constrains the number of dimensions the model
can use to make predictions. We show that Gini and BRO regularization can
improve the accuracy of state-of-the-art GCNN attribution methods on artificial
benchmark datasets. In a real-world setting, we demonstrate that medicinal
chemists significantly prefer explanations extracted from regularized models.
While we only study these regularizers in the context of GCNNs, both can be
applied to other types of neural networks]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Henderson_R/0/1/0/all/0/1"&gt;Ryan Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Clevert_D/0/1/0/all/0/1"&gt;Djork-Arn&amp;#xe9; Clevert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montanari_F/0/1/0/all/0/1"&gt;Floriane Montanari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Local Discrimination for Medical Images. (arXiv:2108.09440v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09440</id>
        <link href="http://arxiv.org/abs/2108.09440"/>
        <updated>2021-08-24T01:40:29.999Z</updated>
        <summary type="html"><![CDATA[Contrastive representation learning is an effective unsupervised method to
alleviate the demand for expensive annotated data in medical image processing.
Recent work mainly based on instance-wise discrimination to learn global
features, while neglect local details, which limit their application in
processing tiny anatomical structures, tissues and lesions. Therefore, we aim
to propose a universal local discrmination framework to learn local
discriminative features to effectively initialize medical models, meanwhile, we
systematacially investigate its practical medical applications. Specifically,
based on the common property of intra-modality structure similarity, i.e.
similar structures are shared among the same modality images, a systematic
local feature learning framework is proposed. Instead of making instance-wise
comparisons based on global embedding, our method makes pixel-wise embedding
and focuses on measuring similarity among patches and regions. The finer
contrastive rule makes the learnt representation more generalized for
segmentation tasks and outperform extensive state-of-the-art methods by wining
11 out of all 12 downstream tasks in color fundus and chest X-ray. Furthermore,
based on the property of inter-modality shape similarity, i.e. structures may
share similar shape although in different medical modalities, we joint
across-modality shape prior into region discrimination to realize unsupervised
segmentation. It shows the feaibility of segmenting target only based on shape
description from other modalities and inner pattern similarity provided by
region discrimination. Finally, we enhance the center-sensitive ability of
patch discrimination by introducing center-sensitive averaging to realize
one-shot landmark localization, this is an effective application for patch
discrimination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jieyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1"&gt;Qing Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lisheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polymorphic dynamic programming by algebraic shortcut fusion. (arXiv:2107.01752v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01752</id>
        <link href="http://arxiv.org/abs/2107.01752"/>
        <updated>2021-08-24T01:40:29.994Z</updated>
        <summary type="html"><![CDATA[Dynamic programming (DP) is a broadly applicable algorithmic design paradigm
for the efficient, exact solution of otherwise intractable, combinatorial
problems. However, the design of such algorithms is often presented informally
in an ad-hoc manner, and as a result is often difficult to apply correctly. In
this paper, we present a rigorous algebraic formalism for systematically
deriving novel DP algorithms, either from existing DP algorithms or from simple
functional recurrences. These derivations lead to algorithms which are provably
correct and polymorphic over any semiring, which means that they can be applied
to the full scope of combinatorial problems expressible in terms of semirings.
This includes, for example: optimization, optimal probability and Viterbi
decoding, probabilistic marginalization, logical inference, fuzzy sets,
differentiable softmax, and relational and provenance queries. The approach,
building on many ideas from the existing literature on constructive
algorithmics, exploits generic properties of (semiring) polymorphic functions,
tupling and formal sums (lifting), and algebraic simplifications arising from
constraint algebras. We demonstrate the effectiveness of this formalism for
some example applications arising in signal processing, bioinformatics and
reliability engineering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1"&gt;Max A. Little&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kayas_U/0/1/0/all/0/1"&gt;Ugur Kayas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Tracking: Using Deep Learning to Discover Novel Interactions in Biological Swarms. (arXiv:2108.09394v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2108.09394</id>
        <link href="http://arxiv.org/abs/2108.09394"/>
        <updated>2021-08-24T01:40:29.986Z</updated>
        <summary type="html"><![CDATA[Most deep-learning frameworks for understanding biological swarms are
designed to fit perceptive models of group behavior to individual-level data
(e.g., spatial coordinates of identified features of individuals) that have
been separately gathered from video observations. Despite considerable advances
in automated tracking, these methods are still very expensive or unreliable
when tracking large numbers of animals simultaneously. Moreover, this approach
assumes that the human-chosen features include sufficient features to explain
important patterns in collective behavior. To address these issues, we propose
training deep network models to predict system-level states directly from
generic graphical features from the entire view, which can be relatively
inexpensive to gather in a completely automated fashion. Because the resulting
predictive models are not based on human-understood predictors, we use
explanatory modules (e.g., Grad-CAM) that combine information hidden in the
latent variables of the deep-network model with the video data itself to
communicate to a human observer which aspects of observed individual behaviors
are most informative in predicting group behavior. This represents an example
of augmented intelligence in behavioral ecology -- knowledge co-creation in a
human-AI team. As proof of concept, we utilize a 20-day video recording of a
colony of over 50 Harpegnathos saltator ants to showcase that, without any
individual annotations provided, a trained model can generate an "importance
map" across the video frames to highlight regions of important behaviors, such
as dueling (which the AI has no a priori knowledge of), that play a role in the
resolution of reproductive-hierarchy re-formation. Based on the empirical
results, we also discuss the potential use and current challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1"&gt;Taeyeong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyenson_B/0/1/0/all/0/1"&gt;Benjamin Pyenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebig_J/0/1/0/all/0/1"&gt;Juergen Liebig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1"&gt;Theodore P. Pavlic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning. (arXiv:2108.09335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09335</id>
        <link href="http://arxiv.org/abs/2108.09335"/>
        <updated>2021-08-24T01:40:29.963Z</updated>
        <summary type="html"><![CDATA[Deep metric learning has been effectively used to learn distance metrics for
different visual tasks like image retrieval, clustering, etc. In order to aid
the training process, existing methods either use a hard mining strategy to
extract the most informative samples or seek to generate hard synthetics using
an additional network. Such approaches face different challenges and can lead
to biased embeddings in the former case, and (i) harder optimization (ii)
slower training speed (iii) higher model complexity in the latter case. In
order to overcome these challenges, we propose a novel approach that looks for
optimal hard negatives (LoOp) in the embedding space, taking full advantage of
each tuple by calculating the minimum distance between a pair of positives and
a pair of negatives. Unlike mining-based methods, our approach considers the
entire space between pairs of embeddings to calculate the optimal hard
negatives. Extensive experiments combining our approach and representative
metric learning losses reveal a significant boost in performance on three
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasudeva_B/0/1/0/all/0/1"&gt;Bhavya Vasudeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deora_P/0/1/0/all/0/1"&gt;Puneesh Deora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1"&gt;Sukalpa Chanda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End2End Occluded Face Recognition by Masking Corrupted Features. (arXiv:2108.09468v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09468</id>
        <link href="http://arxiv.org/abs/2108.09468"/>
        <updated>2021-08-24T01:40:29.957Z</updated>
        <summary type="html"><![CDATA[With the recent advancement of deep convolutional neural networks,
significant progress has been made in general face recognition. However, the
state-of-the-art general face recognition models do not generalize well to
occluded face images, which are exactly the common cases in real-world
scenarios. The potential reasons are the absences of large-scale occluded face
data for training and specific designs for tackling corrupted features brought
by occlusions. This paper presents a novel face recognition method that is
robust to occlusions based on a single end-to-end deep neural network. Our
approach, named FROM (Face Recognition with Occlusion Masks), learns to
discover the corrupted features from the deep convolutional neural networks,
and clean them by the dynamically learned masks. In addition, we construct
massive occluded face images to train FROM effectively and efficiently. FROM is
simple yet powerful compared to the existing methods that either rely on
external detectors to discover the occlusions or employ shallow models which
are less discriminative. Experimental results on the LFW, Megaface challenge 1,
RMF2, AR dataset and other simulated occluded/masked datasets confirm that FROM
dramatically improves the accuracy under occlusions, and generalizes well on
general face recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Haibo Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1"&gt;Dihong Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch2CAD: Patchwise Embedding Learning for In-the-Wild Shape Retrieval from a Single Image. (arXiv:2108.09368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09368</id>
        <link href="http://arxiv.org/abs/2108.09368"/>
        <updated>2021-08-24T01:40:29.949Z</updated>
        <summary type="html"><![CDATA[3D perception of object shapes from RGB image input is fundamental towards
semantic scene understanding, grounding image-based perception in our spatially
3-dimensional real-world environments. To achieve a mapping between image views
of objects and 3D shapes, we leverage CAD model priors from existing
large-scale databases, and propose a novel approach towards constructing a
joint embedding space between 2D images and 3D CAD models in a patch-wise
fashion -- establishing correspondences between patches of an image view of an
object and patches of CAD geometry. This enables part similarity reasoning for
retrieving similar CADs to a new image view without exact matches in the
database. Our patch embedding provides more robust CAD retrieval for shape
estimation in our end-to-end estimation of CAD model shape and pose for
detected objects in a single input image. Experiments on in-the-wild, complex
imagery from ScanNet show that our approach is more robust than state of the
art in real-world scenarios without any exact CAD matches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1"&gt;Weicheng Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1"&gt;Anelia Angelova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Angela Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Sensitive Concept Drift Detector with Constraint Embedding. (arXiv:2108.06980v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06980</id>
        <link href="http://arxiv.org/abs/2108.06980"/>
        <updated>2021-08-24T01:40:29.939Z</updated>
        <summary type="html"><![CDATA[Detecting drifts in data is essential for machine learning applications, as
changes in the statistics of processed data typically has a profound influence
on the performance of trained models. Most of the available drift detection
methods are either supervised and require access to the true labels during
inference time, or they are completely unsupervised and aim for changes in
distributions without taking label information into account. We propose a novel
task-sensitive semi-supervised drift detection scheme, which utilizes label
information while training the initial model, but takes into account that
supervised label information is no longer available when using the model during
inference. It utilizes a constrained low-dimensional embedding representation
of the input data. This way, it is best suited for the classification task. It
is able to detect real drift, where the drift affects the classification
performance, while it properly ignores virtual drift, where the classification
performance is not affected by the drift. In the proposed framework, the actual
method to detect a change in the statistics of incoming data samples can be
chosen freely. Experimental evaluation on nine benchmarks datasets, with
different types of drift, demonstrates that the proposed framework can reliably
detect drifts, and outperforms state-of-the-art unsupervised drift detection
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellani_A/0/1/0/all/0/1"&gt;Andrea Castellani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1"&gt;Sebastian Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Barbara Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting and Segmenting Adversarial Graphics Patterns from Images. (arXiv:2108.09383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09383</id>
        <link href="http://arxiv.org/abs/2108.09383"/>
        <updated>2021-08-24T01:40:29.931Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks pose a substantial threat to computer vision system
security, but the social media industry constantly faces another form of
"adversarial attack" in which the hackers attempt to upload inappropriate
images and fool the automated screening systems by adding artificial graphics
patterns. In this paper, we formulate the defense against such attacks as an
artificial graphics pattern segmentation problem. We evaluate the efficacy of
several segmentation algorithms and, based on observation of their performance,
propose a new method tailored to this specific problem. Extensive experiments
show that the proposed method outperforms the baselines and has a promising
generalization capability, which is the most crucial aspect in segmenting
artificial graphics patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiangyu Qu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt; (1) ((1) Purdue University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08462</id>
        <link href="http://arxiv.org/abs/2106.08462"/>
        <updated>2021-08-24T01:40:29.905Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Neural Ordinary Differential Equations (ODEs) can
serve as generative models of images using the perspective of Continuous
Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and
invertible generation/density estimation. In this work we introduce a
Multi-Resolution variant of such models (MRCNF), by characterizing the
conditional distribution over the additional information required to generate a
fine image that is consistent with the coarse image. We introduce a
transformation between resolutions that allows for no change in the log
likelihood. We show that this approach yields comparable likelihood values for
various image datasets, with improved performance at higher resolutions, with
fewer parameters, using only 1 GPU. Further, we examine the out-of-distribution
properties of (Multi-Resolution) Continuous Normalizing Flows, and find that
they are similar to those of other likelihood-based generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1"&gt;Chris Finlay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1"&gt;Adam Oberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATTACC the Quadratic Bottleneck of Attention Layers. (arXiv:2107.06419v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06419</id>
        <link href="http://arxiv.org/abs/2107.06419"/>
        <updated>2021-08-24T01:40:29.898Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms form the backbone of state-of-the-art machine learning
models for a variety of tasks. Deploying them on deep neural network (DNN)
accelerators, however, is prohibitively challenging especially under long
sequences. Operators in attention layers exhibit limited reuse and quadratic
growth in memory footprint, leading to severe memory-boundedness. This paper
introduces a new attention-tailored dataflow, termed FLAT, which leverages
operator fusion, loop-nest optimizations, and interleaved execution. It
increases the effective memory bandwidth by efficiently utilizing the
high-bandwidth, low-capacity on-chip buffer and thus achieves better run time
and compute resource utilization. We term FLAT-compatible accelerators ATTACC.
In our evaluation, ATTACC achieves 1.94x and 1.76x speedup and 49% and 42% of
energy reduction comparing to state-of-the-art edge and cloud accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1"&gt;Sheng-Chun Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1"&gt;Suvinay Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1"&gt;Gaurav Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1"&gt;Tushar Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse-shot Learning with Exclusive Cross-Entropy for Extremely Many Localisations. (arXiv:2104.10425v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10425</id>
        <link href="http://arxiv.org/abs/2104.10425"/>
        <updated>2021-08-24T01:40:29.887Z</updated>
        <summary type="html"><![CDATA[Object localisation, in the context of regular images, often depicts objects
like people or cars. In these images, there is typically a relatively small
number of objects per class, which usually is manageable to annotate. However,
outside the setting of regular images, we are often confronted with a different
situation. In computational pathology, digitised tissue sections are extremely
large images, whose dimensions quickly exceed 250'000x250'000 pixels, where
relevant objects, such as tumour cells or lymphocytes can quickly number in the
millions. Annotating them all is practically impossible and annotating sparsely
a few, out of many more, is the only possibility. Unfortunately, learning from
sparse annotations, or sparse-shot learning, clashes with standard supervised
learning because what is not annotated is treated as a negative. However,
assigning negative labels to what are true positives leads to confusion in the
gradients and biased learning. To this end, we present exclusive cross-entropy,
which slows down the biased learning by examining the second-order loss
derivatives in order to drop the loss terms corresponding to likely biased
terms. Experiments on nine datasets and two different localisation tasks,
detection with YOLLO and segmentation with Unet, show that we obtain
considerable improvements compared to cross-entropy or focal loss, while often
reaching the best possible performance for the model with only 10-40% of
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panteli_A/0/1/0/all/0/1"&gt;Andreas Panteli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teuwen_J/0/1/0/all/0/1"&gt;Jonas Teuwen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horlings_H/0/1/0/all/0/1"&gt;Hugo Horlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08649</id>
        <link href="http://arxiv.org/abs/2105.08649"/>
        <updated>2021-08-24T01:40:29.874Z</updated>
        <summary type="html"><![CDATA[User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models. Public codes are
available at https://github.com/zachstarkk/DCAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangtian Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1"&gt;Robert Pless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models. (arXiv:2107.03863v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03863</id>
        <link href="http://arxiv.org/abs/2107.03863"/>
        <updated>2021-08-24T01:40:29.868Z</updated>
        <summary type="html"><![CDATA[Describing the relationship between the variables in a study domain and
modelling the data generating mechanism is a fundamental problem in many
empirical sciences. Probabilistic graphical models are one common approach to
tackle the problem. Learning the graphical structure is computationally
challenging and a fervent area of current research with a plethora of
algorithms being developed. To facilitate the benchmarking of different
methods, we present a novel automated workflow, called benchpress for producing
scalable, reproducible, and platform-independent benchmarks of structure
learning algorithms for probabilistic graphical models. Benchpress is
interfaced via a simple JSON-file, which makes it accessible for all users,
while the code is designed in a fully modular fashion to enable researchers to
contribute additional methodologies. Benchpress currently provides an interface
to a large number of state-of-the-art algorithms from libraries such as
BDgraph, BiDAG, bnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and
trilearn as well as a variety of methods for data generating models and
performance evaluation. Alongside user-defined models and randomly generated
datasets, the software tool also includes a number of standard datasets and
graphical models from the literature, which may be included in a benchmarking
workflow. We demonstrate the applicability of this workflow for learning
Bayesian networks in four typical data scenarios. The source code and
documentation is publicly available from
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rios_F/0/1/0/all/0/1"&gt;Felix L. Rios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1"&gt;Giusi Moffa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1"&gt;Jack Kuipers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoundDet: Polyphonic Moving Sound Event Detection and Localization from Raw Waveform. (arXiv:2106.06969v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06969</id>
        <link href="http://arxiv.org/abs/2106.06969"/>
        <updated>2021-08-24T01:40:29.855Z</updated>
        <summary type="html"><![CDATA[We present a new framework SoundDet, which is an end-to-end trainable and
light-weight framework, for polyphonic moving sound event detection and
localization. Prior methods typically approach this problem by preprocessing
raw waveform into time-frequency representations, which is more amenable to
process with well-established image processing pipelines. Prior methods also
detect in segment-wise manner, leading to incomplete and partial detections.
SoundDet takes a novel approach and directly consumes the raw, multichannel
waveform and treats the spatio-temporal sound event as a complete
"sound-object" to be detected. Specifically, SoundDet consists of a backbone
neural network and two parallel heads for temporal detection and spatial
localization, respectively. Given the large sampling rate of raw waveform, the
backbone network first learns a set of phase-sensitive and frequency-selective
bank of filters to explicitly retain direction-of-arrival information, whilst
being highly computationally and parametrically efficient than standard 1D/2D
convolution. A dense sound event proposal map is then constructed to handle the
challenges of predicting events with large varying temporal duration.
Accompanying the dense proposal map are a temporal overlapness map and a motion
smoothness map that measure a proposal's confidence to be an event from
temporal detection accuracy and movement consistency perspective. Involving the
two maps guarantees SoundDet to be trained in a spatio-temporally unified
manner. Experimental results on the public DCASE dataset show the advantage of
SoundDet on both segment-based and our newly proposed event-based evaluation
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1"&gt;Niki Trigoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1"&gt;Andrew Markham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs. (arXiv:2105.12049v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12049</id>
        <link href="http://arxiv.org/abs/2105.12049"/>
        <updated>2021-08-24T01:40:29.841Z</updated>
        <summary type="html"><![CDATA[It is known that deep neural networks, trained for the classification of a
non-sensitive target attribute, can reveal some sensitive attributes of their
input data; through features of different granularity extracted by the
classifier. We take a step forward and show that deep classifiers can be
trained to secretly encode a sensitive attribute of users' input data into the
classifier's outputs for the target attribute, at inference time. This results
in an attack that works even if users have a full white-box view of the
classifier, and can keep all internal representations hidden except for the
classifier's outputs for the target attribute. We introduce an
information-theoretical formulation of such attacks and present efficient
empirical implementations for training honest-but-curious (HBC) classifiers
based on this formulation: classifiers that can be accurate in predicting their
target attribute, but can also exploit their outputs to secretly encode a
sensitive attribute. Our evaluations on several tasks in real-world datasets
show that a semi-trusted server can build a classifier that is not only
perfectly honest but also accurately curious. Our work highlights a
vulnerability that can be exploited by malicious machine learning service
providers to attack their user's privacy in several seemingly safe scenarios;
such as encrypted inferences, computations at the edge, or private knowledge
distillation. We conclude by showing the difficulties in distinguishing between
standard and HBC classifiers, discussing challenges in defending against this
vulnerability of deep classifiers, and enumerating related open directions for
future studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1"&gt;Mohammad Malekzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borovykh_A/0/1/0/all/0/1"&gt;Anastasia Borovykh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Segment Medical Images from Few-Shot Sparse Labels. (arXiv:2108.05476v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.05476</id>
        <link href="http://arxiv.org/abs/2108.05476"/>
        <updated>2021-08-24T01:40:29.836Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel approach for few-shot semantic segmentation
with sparse labeled images. We investigate the effectiveness of our method,
which is based on the Model-Agnostic Meta-Learning (MAML) algorithm, in the
medical scenario, where the use of sparse labeling and few-shot can alleviate
the cost of producing new annotated datasets. Our method uses sparse labels in
the meta-training and dense labels in the meta-test, thus making the model
learn to predict dense labels from sparse ones. We conducted experiments with
four Chest X-Ray datasets to evaluate two types of annotations (grid and
points). The results show that our method is the most suitable when the target
domain highly differs from source domains, achieving Jaccard scores comparable
to dense labels, using less than 2% of the pixels of an image with labels in
few-shot scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1"&gt;Pedro H. T. Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Static analysis of ReLU neural networks with tropical polyhedra. (arXiv:2108.00893v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00893</id>
        <link href="http://arxiv.org/abs/2108.00893"/>
        <updated>2021-08-24T01:40:29.830Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of range analysis for feedforward neural
networks, which is a basic primitive for applications such as robustness of
neural networks, compliance to specifications and reachability analysis of
neural-network feedback systems. Our approach focuses on ReLU (rectified linear
unit) feedforward neural nets that present specific difficulties: approaches
that exploit derivatives do not apply in general, the number of patterns of
neuron activations can be quite large even for small networks, and convex
approximations are generally too coarse. In this paper, we employ set-based
methods and abstract interpretation that have been very successful in coping
with similar difficulties in classical program verification. We present an
approach that abstracts ReLU feedforward neural networks using tropical
polyhedra. We show that tropical polyhedra can efficiently abstract ReLU
activation function, while being able to control the loss of precision due to
linear computations. We show how the connection between ReLU networks and
tropical rational functions can provide approaches for range analysis of ReLU
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1"&gt;Eric Goubault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palumby_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Palumby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putot_S/0/1/0/all/0/1"&gt;Sylvie Putot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rustenholz_L/0/1/0/all/0/1"&gt;Louis Rustenholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1"&gt;Sriram Sankaranarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection. (arXiv:2108.07135v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07135</id>
        <link href="http://arxiv.org/abs/2108.07135"/>
        <updated>2021-08-24T01:40:29.824Z</updated>
        <summary type="html"><![CDATA[Vehicle counting systems can help with vehicle analysis and traffic incident
detection. Unfortunately, most existing methods require some level of human
input to identify the Region of interest (ROI), movements of interest, or to
establish a reference point or line to count vehicles from traffic cameras.
This work introduces a method to count vehicles from traffic videos that
automatically identifies the ROI for the camera, as well as the driving
trajectories of the vehicles. This makes the method feasible to use with
Pan-Tilt-Zoom cameras, which are frequently used in developing countries.
Preliminary results indicate that the proposed method achieves an average
intersection over the union of 57.05% for the ROI and a mean absolute error of
just 17.44% at counting vehicles of the traffic video cameras tested.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1"&gt;Malolan Vasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreu_N/0/1/0/all/0/1"&gt;Nelson Abreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasquez_R/0/1/0/all/0/1"&gt;Raysa V&amp;#xe1;squez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1"&gt;Christian L&amp;#xf3;pez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-DARTS: Distributed Differentiable Architecture Search. (arXiv:2108.09306v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09306</id>
        <link href="http://arxiv.org/abs/2108.09306"/>
        <updated>2021-08-24T01:40:29.811Z</updated>
        <summary type="html"><![CDATA[Differentiable ARchiTecture Search (DARTS) is one of the most trending Neural
Architecture Search (NAS) methods, drastically reducing search cost by
resorting to Stochastic Gradient Descent (SGD) and weight-sharing. However, it
also greatly reduces the search space, thus excluding potential promising
architectures from being discovered. In this paper, we propose D-DARTS, a novel
solution that addresses this problem by nesting several neural networks at
cell-level instead of using weight-sharing to produce more diversified and
specialized architectures. Moreover, we introduce a novel algorithm which can
derive deeper architectures from a few trained cells, increasing performance
and saving computation time. Our solution is able to provide state-of-the-art
results on CIFAR-10, CIFAR-100 and ImageNet while using significantly less
parameters than previous baselines, resulting in more hardware-efficient neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heuillet_A/0/1/0/all/0/1"&gt;Alexandre Heuillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1"&gt;Hedi Tabia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arioui_H/0/1/0/all/0/1"&gt;Hichem Arioui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Youcef_Toumi_K/0/1/0/all/0/1"&gt;Kamal Youcef-Toumi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Mixtures of Plackett-Luce Models with Features from Top-$l$ Orders. (arXiv:2006.03869v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03869</id>
        <link href="http://arxiv.org/abs/2006.03869"/>
        <updated>2021-08-24T01:40:29.796Z</updated>
        <summary type="html"><![CDATA[Plackett-Luce model (PL) is one of the most popular models for preference
learning. In this paper, we consider PL with features and its mixture models,
where each alternative has a vector of features, possibly different across
agents. Such models significantly generalize the standard PL, but are not as
well investigated in the literature. We extend mixtures of PLs with features to
models that generate top-$l$ and characterize their identifiability. We further
prove that when PL with features is identifiable, its MLE is consistent with a
strictly concave objective function under mild assumptions, by characterizing a
bound on root-mean-square-error (RMSE), which naturally leads to a sample
complexity bound. Our experiments on synthetic data demonstrate the
effectiveness of MLE on PL with features with tradeoffs between statistical
efficiency and computational efficiency when $l$ takes different values. Our
experiments on real-world data show the prediction power of PL with features
and its mixtures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhibing Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Ao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1"&gt;Lirong Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fourier Neural Operator Networks: A Fast and General Solver for the Photoacoustic Wave Equation. (arXiv:2108.09374v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09374</id>
        <link href="http://arxiv.org/abs/2108.09374"/>
        <updated>2021-08-24T01:40:29.790Z</updated>
        <summary type="html"><![CDATA[Simulation tools for photoacoustic wave propagation have played a key role in
advancing photoacoustic imaging by providing quantitative and qualitative
insights into parameters affecting image quality. Classical methods for
numerically solving the photoacoustic wave equation relies on a fine
discretization of space and can become computationally expensive for large
computational grids. In this work, we apply Fourier Neural Operator (FNO)
networks as a fast data-driven deep learning method for solving the 2D
photoacoustic wave equation in a homogeneous medium. Comparisons between the
FNO network and pseudo-spectral time domain approach demonstrated that the FNO
network generated comparable simulations with small errors and was several
orders of magnitude faster. Moreover, the FNO network was generalizable and can
generate simulations not observed in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guan_S/0/1/0/all/0/1"&gt;Steven Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hsu_K/0/1/0/all/0/1"&gt;Ko-Tsung Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chitnis_P/0/1/0/all/0/1"&gt;Parag V. Chitnis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer. (arXiv:2108.06625v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06625</id>
        <link href="http://arxiv.org/abs/2108.06625"/>
        <updated>2021-08-24T01:40:29.785Z</updated>
        <summary type="html"><![CDATA[In order to model the evolution of user preference, we should learn user/item
embeddings based on time-ordered item purchasing sequences, which is defined as
Sequential Recommendation (SR) problem. Existing methods leverage sequential
patterns to model item transitions. However, most of them ignore crucial
temporal collaborative signals, which are latent in evolving user-item
interactions and coexist with sequential patterns. Therefore, we propose to
unify sequential patterns and temporal collaborative signals to improve the
quality of recommendation, which is rather challenging. Firstly, it is hard to
simultaneously encode sequential patterns and collaborative signals. Secondly,
it is non-trivial to express the temporal effects of collaborative signals.

Hence, we design a new framework Temporal Graph Sequential Recommender
(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel
Temporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the
self-attention mechanism by adopting a novel collaborative attention. TCT layer
can simultaneously capture collaborative signals from both users and items, as
well as considering temporal dynamics inside sequential patterns. We propagate
the information learned fromTCTlayerover the temporal graph to unify sequential
patterns and temporal collaborative signals. Empirical results on five datasets
show that TGSRec significantly outperforms other baselines, in average up to
22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Ziwei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Paced Contrastive Learning for Semi-supervised Medical Image Segmentation with Meta-labels. (arXiv:2107.13741v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13741</id>
        <link href="http://arxiv.org/abs/2107.13741"/>
        <updated>2021-08-24T01:40:29.759Z</updated>
        <summary type="html"><![CDATA[Pre-training a recognition model with contrastive learning on a large dataset
of unlabeled data has shown great potential to boost the performance of a
downstream task, e.g., image classification. However, in domains such as
medical imaging, collecting unlabeled data can be challenging and expensive. In
this work, we propose to adapt contrastive learning to work with meta-label
annotations, for improving the model's performance in medical image
segmentation even when no additional unlabeled data is available. Meta-labels
such as the location of a 2D slice in a 3D MRI scan or the type of device used,
often come for free during the acquisition process. We use the meta-labels for
pre-training the image encoder as well as to regularize a semi-supervised
training, in which a reduced set of annotated data is used for training.
Finally, to fully exploit the weak annotations, a self-paced learning approach
is used to help the learning and discriminate useful labels from noise. Results
on three different medical image segmentation datasets show that our approach:
i) highly boosts the performance of a model trained on a few scans, ii)
outperforms previous contrastive and semi-supervised approaches, and iii)
reaches close to the performance of a model trained on the full data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jizong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1"&gt;Chrisitian Desrosiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00139</id>
        <link href="http://arxiv.org/abs/2108.00139"/>
        <updated>2021-08-24T01:40:29.746Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (ReID) aims to match person images with
occlusion. It is fundamentally challenging because of the serious occlusion
which aggravates the misalignment problem between images. At the cost of
incorporating a pose estimator, many works introduce pose information to
alleviate the misalignment in both training and testing. To achieve high
accuracy while preserving low inference complexity, we propose a network named
Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the
pose information is exploited to regularize the learning of semantics aligned
features but is discarded in testing. PGFL-KD consists of a main branch (MB),
and two pose-guided branches, \ieno, a foreground-enhanced branch (FEB), and a
body part semantics aligned branch (SAB). The FEB intends to emphasise the
features of visible body parts while excluding the interference of obstructions
and background (\ieno, foreground feature alignment). The SAB encourages
different channel groups to focus on different body parts to have body part
semantics aligned representation. To get rid of the dependency on pose
information when testing, we regularize the MB to learn the merits of the FEB
and SAB through knowledge distillation and interaction-based training.
Extensive experiments on occluded, partial, and holistic ReID tasks show the
effectiveness of our proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relighting Images in the Wild with a Self-Supervised Siamese Auto-Encoder. (arXiv:2012.06444v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06444</id>
        <link href="http://arxiv.org/abs/2012.06444"/>
        <updated>2021-08-24T01:40:29.740Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised method for image relighting of single view
images in the wild. The method is based on an auto-encoder which deconstructs
an image into two separate encodings, relating to the scene illumination and
content, respectively. In order to disentangle this embedding information
without supervision, we exploit the assumption that some augmentation
operations do not affect the image content and only affect the direction of the
light. A novel loss function, called spherical harmonic loss, is introduced
that forces the illumination embedding to convert to a spherical harmonic
vector. We train our model on large-scale datasets such as Youtube 8M and
CelebA. Our experiments show that our method can correctly estimate scene
illumination and realistically re-light input images, without any supervision
or a prior shape model. Compared to supervised methods, our approach has
similar performance and avoids common lighting artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neophytou_A/0/1/0/all/0/1"&gt;Alexandros Neophytou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Sunando Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommerlade_E/0/1/0/all/0/1"&gt;Eric Sommerlade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Signed Distance Field for Multi-view Surface Reconstruction. (arXiv:2108.09964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09964</id>
        <link href="http://arxiv.org/abs/2108.09964"/>
        <updated>2021-08-24T01:40:29.731Z</updated>
        <summary type="html"><![CDATA[Recent works on implicit neural representations have shown promising results
for multi-view surface reconstruction. However, most approaches are limited to
relatively simple geometries and usually require clean object masks for
reconstructing complex and concave objects. In this work, we introduce a novel
neural surface reconstruction framework that leverages the knowledge of stereo
matching and feature consistency to optimize the implicit surface
representation. More specifically, we apply a signed distance field (SDF) and a
surface light field to represent the scene geometry and appearance
respectively. The SDF is directly supervised by geometry from stereo matching,
and is refined by optimizing the multi-view feature consistency and the
fidelity of rendered images. Our method is able to improve the robustness of
geometry estimation and support reconstruction of complex scene topologies.
Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples
datasets. Compared to previous state-of-the-art methods, our method achieves
better mesh reconstruction in wide open scenes without masks as input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1"&gt;Long Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Literature Review of Automated Query Reformulations in Source Code Search. (arXiv:2108.09646v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.09646</id>
        <link href="http://arxiv.org/abs/2108.09646"/>
        <updated>2021-08-24T01:40:29.726Z</updated>
        <summary type="html"><![CDATA[Software developers often fix critical bugs to ensure the reliability of
their software. They might also need to add new features to their software at a
regular interval to stay competitive in the market. These bugs and features are
reported as change requests (i.e., technical documents written by software
users). Developers consult these documents to implement the required changes in
the software code. As a part of change implementation, they often choose a few
important keywords from a change request as an ad hoc query. Then they execute
the query with a code search engine (e.g., Lucene) and attempt to find out the
exact locations within the software code that need to be changed.
Unfortunately, even experienced developers often fail to choose the right
queries. As a consequence, the developers often experience difficulties in
detecting the appropriate locations within the code and spend the majority of
their time in numerous trials and errors. There have been many studies that
attempt to support developers in constructing queries by automatically
reformulating their ad hoc queries. In this systematic literature review, we
carefully select 70 primary studies on query reformulations from 2,970
candidate studies, perform an in-depth qualitative analysis using the Grounded
Theory approach, and then answer six important research questions. Our
investigation has reported several major findings. First, to date, eight major
methodologies (e.g., term weighting, query-term co-occurrence analysis,
thesaurus lookup) have been adopted in query reformulation. Second, the
existing studies suffer from several major limitations (e.g., lack of
generalizability, vocabulary mismatch problem, weak evaluation, the extra
burden on the developers) that might prevent their wide adoption. Finally, we
discuss several open issues in search query reformulations and suggest multiple
future research opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mohammad Masudur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_C/0/1/0/all/0/1"&gt;Chanchal K. Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DTWSSE: Data Augmentation with a Siamese Encoder for Time Series. (arXiv:2108.09885v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09885</id>
        <link href="http://arxiv.org/abs/2108.09885"/>
        <updated>2021-08-24T01:40:29.720Z</updated>
        <summary type="html"><![CDATA[Access to labeled time series data is often limited in the real world, which
constrains the performance of deep learning models in the field of time series
analysis. Data augmentation is an effective way to solve the problem of small
sample size and imbalance in time series datasets. The two key factors of data
augmentation are the distance metric and the choice of interpolation method.
SMOTE does not perform well on time series data because it uses a Euclidean
distance metric and interpolates directly on the object. Therefore, we propose
a DTW-based synthetic minority oversampling technique using siamese encoder for
interpolation named DTWSSE. In order to reasonably measure the distance of the
time series, DTW, which has been verified to be an effective method forts, is
employed as the distance metric. To adapt the DTW metric, we use an autoencoder
trained in an unsupervised self-training manner for interpolation. The encoder
is a Siamese Neural Network for mapping the time series data from the DTW
hidden space to the Euclidean deep feature space, and the decoder is used to
map the deep feature space back to the DTW hidden space. We validate the
proposed methods on a number of different balanced or unbalanced time series
datasets. Experimental results show that the proposed method can lead to better
performance of the downstream deep learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinlan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yahui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Rongyi Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guiding Query Position and Performing Similar Attention for Transformer-Based Detection Heads. (arXiv:2108.09691v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09691</id>
        <link href="http://arxiv.org/abs/2108.09691"/>
        <updated>2021-08-24T01:40:29.705Z</updated>
        <summary type="html"><![CDATA[After DETR was proposed, this novel transformer-based detection paradigm
which performs several cross-attentions between object queries and feature maps
for predictions has subsequently derived a series of transformer-based
detection heads. These models iterate object queries after each
cross-attention. However, they don't renew the query position which indicates
object queries' position information. Thus model needs extra learning to figure
out the newest regions that query position should express and need more
attention. To fix this issue, we propose the Guided Query Position (GQPos)
method to embed the latest location information of object queries to query
position iteratively.

Another problem of such transformer-based detection heads is the high
complexity to perform attention on multi-scale feature maps, which hinders them
from improving detection performance at all scales. Therefore we propose a
novel fusion scheme named Similar Attention (SiA): besides the feature maps is
fused, SiA also fuse the attention weights maps to accelerate the learning of
high-resolution attention weight map by well-learned low-resolution attention
weight map.

Our experiments show that the proposed GQPos improves the performance of a
series of models, including DETR, SMCA, YoloS, and HoiTransformer and SiA
consistently improve the performance of multi-scale transformer-based detection
heads like DETR and HoiTransformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaohu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Ze Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ChunYuan/0/1/0/all/0/1"&gt;ChunYuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection and Localization of Multiple Image Splicing Using MobileNet V1. (arXiv:2108.09674v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09674</id>
        <link href="http://arxiv.org/abs/2108.09674"/>
        <updated>2021-08-24T01:40:29.700Z</updated>
        <summary type="html"><![CDATA[In modern society, digital images have become a prominent source of
information and medium of communication. They can, however, be simply altered
using freely available image editing software. Two or more images are combined
to generate a new image that can transmit information across social media
platforms to influence the people in the society. This information may have
both positive and negative consequences. Hence there is a need to develop a
technique that will detect and locates a multiple image splicing forgery in an
image. This research work proposes multiple image splicing forgery detection
using Mask R-CNN, with a backbone as a MobileNet V1. It also calculates the
percentage score of a forged region of multiple spliced images. The comparative
analysis of the proposed work with the variants of ResNet is performed. The
proposed model is trained and tested using our MISD (Multiple Image Splicing
Dataset), and it is observed that the proposed model outperforms the variants
of ResNet models (ResNet 51,101 and 151).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kadam_K/0/1/0/all/0/1"&gt;Kalyani Kadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahirrao_D/0/1/0/all/0/1"&gt;Dr. Swati Ahirrao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotecha_D/0/1/0/all/0/1"&gt;Dr. Ketan Kotecha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1"&gt;Sayan Sahu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fine-Grained Visual Attention Approach for Fingerspelling Recognition in the Wild. (arXiv:2105.07625v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07625</id>
        <link href="http://arxiv.org/abs/2105.07625"/>
        <updated>2021-08-24T01:40:29.695Z</updated>
        <summary type="html"><![CDATA[Fingerspelling in sign language has been the means of communicating technical
terms and proper nouns when they do not have dedicated sign language gestures.
Automatic recognition of fingerspelling can help resolve communication barriers
when interacting with deaf people. The main challenges prevalent in
fingerspelling recognition are the ambiguity in the gestures and strong
articulation of the hands. The automatic recognition model should address high
inter-class visual similarity and high intra-class variation in the gestures.
Most of the existing research in fingerspelling recognition has focused on the
dataset collected in a controlled environment. The recent collection of a
large-scale annotated fingerspelling dataset in the wild, from social media and
online platforms, captures the challenges in a real-world scenario. In this
work, we propose a fine-grained visual attention mechanism using the
Transformer model for the sequence-to-sequence prediction task in the wild
dataset. The fine-grained attention is achieved by utilizing the change in
motion of the video frames (optical flow) in sequential context-based attention
along with a Transformer encoder model. The unsegmented continuous video
dataset is jointly trained by balancing the Connectionist Temporal
Classification (CTC) loss and the maximum-entropy loss. The proposed approach
can capture better fine-grained attention in a single iteration. Experiment
evaluations show that it outperforms the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gajurel_K/0/1/0/all/0/1"&gt;Kamala Gajurel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Cuncong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-aware Clustering for Unsupervised Domain Adaptive Object Re-identification. (arXiv:2108.09682v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09682</id>
        <link href="http://arxiv.org/abs/2108.09682"/>
        <updated>2021-08-24T01:40:29.681Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptive (UDA) object re-identification (Re-ID) aims at
adapting a model trained on a labeled source domain to an unlabeled target
domain. State-of-the-art object Re-ID approaches adopt clustering algorithms to
generate pseudo-labels for the unlabeled target domain. However, the inevitable
label noise caused by the clustering procedure significantly degrades the
discriminative power of Re-ID model. To address this problem, we propose an
uncertainty-aware clustering framework (UCF) for UDA tasks. First, a novel
hierarchical clustering scheme is proposed to promote clustering quality.
Second, an uncertainty-aware collaborative instance selection method is
introduced to select images with reliable labels for model training. Combining
both techniques effectively reduces the impact of noisy labels. In addition, we
introduce a strong baseline that features a compact contrastive loss. Our UCF
method consistently achieves state-of-the-art performance in multiple UDA tasks
for object Re-ID, and significantly reduces the gap between unsupervised and
supervised Re-ID performance. In particular, the performance of our
unsupervised UCF method in the MSMT17$\to$Market1501 task is better than that
of the fully supervised setting on Market1501. The code of UCF is available at
https://github.com/Wang-pengfei/UCF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Changxing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Wentao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voxel-based Network for Shape Completion by Leveraging Edge Generation. (arXiv:2108.09936v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09936</id>
        <link href="http://arxiv.org/abs/2108.09936"/>
        <updated>2021-08-24T01:40:29.667Z</updated>
        <summary type="html"><![CDATA[Deep learning technique has yielded significant improvements in point cloud
completion with the aim of completing missing object shapes from partial
inputs. However, most existing methods fail to recover realistic structures due
to over-smoothing of fine-grained details. In this paper, we develop a
voxel-based network for point cloud completion by leveraging edge generation
(VE-PCN). We first embed point clouds into regular voxel grids, and then
generate complete objects with the help of the hallucinated shape edges. This
decoupled architecture together with a multi-scale grid feature learning is
able to generate more realistic on-surface details. We evaluate our model on
the publicly available completion datasets and show that it outperforms
existing state-of-the-art approaches quantitatively and qualitatively. Our
source code is available at https://github.com/xiaogangw/VE-PCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Embedding for Few-Shot Classification. (arXiv:2108.09666v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09666</id>
        <link href="http://arxiv.org/abs/2108.09666"/>
        <updated>2021-08-24T01:40:29.662Z</updated>
        <summary type="html"><![CDATA[We propose to address the problem of few-shot classification by meta-learning
"what to observe" and "where to attend" in a relational perspective. Our method
leverages relational patterns within and between images via self-correlational
representation (SCR) and cross-correlational attention (CCA). Within each
image, the SCR module transforms a base feature map into a self-correlation
tensor and learns to extract structural patterns from the tensor. Between the
images, the CCA module computes cross-correlation between two image
representations and learns to produce co-attention between them. Our Relational
Embedding Network (RENet) combines the two relational modules to learn
relational embedding in an end-to-end manner. In experimental evaluation, it
achieves consistent improvements over state-of-the-art methods on four widely
used few-shot classification benchmarks of miniImageNet, tieredImageNet,
CUB-200-2011, and CIFAR-FS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Dahyun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1"&gt;Heeseung Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1"&gt;Juhong Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;Minsu Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RaP-Net: A Region-wise and Point-wise Weighting Network to Extract Robust Features for Indoor Localization. (arXiv:2012.00234v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00234</id>
        <link href="http://arxiv.org/abs/2012.00234"/>
        <updated>2021-08-24T01:40:29.629Z</updated>
        <summary type="html"><![CDATA[Feature extraction plays an important role in visual localization. Unreliable
features on dynamic objects or repetitive regions will interfere with feature
matching and challenge indoor localization greatly. To address the problem, we
propose a novel network, RaP-Net, to simultaneously predict region-wise
invariability and point-wise reliability, and then extract features by
considering both of them. We also introduce a new dataset, named
OpenLORIS-Location, to train the proposed network. The dataset contains 1553
images from 93 indoor locations. Various appearance changes between images of
the same location are included and can help the model to learn the
invariability in typical indoor scenes. Experimental results show that the
proposed RaP-Net trained with OpenLORIS-Location dataset achieves excellent
performance in the feature matching task and significantly outperforms
state-of-the-arts feature algorithms in indoor localization. The RaP-Net code
and dataset are available at https://github.com/ivipsourcecode/RaP-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongjiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1"&gt;Jinyu Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xuesong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuxin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qiwei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1"&gt;Ping Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongfei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1"&gt;Haosong Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1"&gt;Qi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1"&gt;Fei Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Relational Metric Learning. (arXiv:2108.10026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10026</id>
        <link href="http://arxiv.org/abs/2108.10026"/>
        <updated>2021-08-24T01:40:29.614Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep relational metric learning (DRML) framework for
image clustering and retrieval. Most existing deep metric learning methods
learn an embedding space with a general objective of increasing interclass
distances and decreasing intraclass distances. However, the conventional losses
of metric learning usually suppress intraclass variations which might be
helpful to identify samples of unseen classes. To address this problem, we
propose to adaptively learn an ensemble of features that characterizes an image
from different aspects to model both interclass and intraclass distributions.
We further employ a relational module to capture the correlations among each
feature in the ensemble and construct a graph to represent an image. We then
perform relational inference on the graph to integrate the ensemble and obtain
a relation-aware embedding to measure the similarities. Extensive experiments
on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets
demonstrate that our framework improves existing deep metric learning methods
and achieves very competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Borui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpaired Depth Super-Resolution in the Wild. (arXiv:2105.12038v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12038</id>
        <link href="http://arxiv.org/abs/2105.12038"/>
        <updated>2021-08-24T01:40:29.598Z</updated>
        <summary type="html"><![CDATA[Depth maps captured with commodity sensors are often of low quality and
resolution; these maps need to be enhanced to be used in many applications.
State-of-the-art data-driven methods of depth map super-resolution rely on
registered pairs of low- and high-resolution depth maps of the same scenes.
Acquisition of real-world paired data requires specialized setups. Another
alternative, generating low-resolution maps from high-resolution maps by
subsampling, adding noise and other artificial degradation methods, does not
fully capture the characteristics of real-world low-resolution images. As a
consequence, supervised learning methods trained on such artificial paired data
may not perform well on real-world low-resolution inputs. We consider an
approach to depth super-resolution based on learning from unpaired data. While
many techniques for unpaired image-to-image translation have been proposed,
most fail to deliver effective hole-filling or reconstruct accurate surfaces
using depth maps. We propose an unpaired learning method for depth
super-resolution, which is based on a learnable degradation model, enhancement
component and surface normal estimates as features to produce more accurate
depth maps. We propose a benchmark for unpaired depth SR and demonstrate that
our method outperforms existing unpaired methods and performs on par with
paired.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safin_A/0/1/0/all/0/1"&gt;Aleksandr Safin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1"&gt;Maxim Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drobyshev_N/0/1/0/all/0/1"&gt;Nikita Drobyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1"&gt;Oleg Voynov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1"&gt;Alexey Artemov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1"&gt;Denis Zorin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection. (arXiv:2006.03184v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03184</id>
        <link href="http://arxiv.org/abs/2006.03184"/>
        <updated>2021-08-24T01:40:29.555Z</updated>
        <summary type="html"><![CDATA[Many recent studies have shown that deep neural models are vulnerable to
adversarial samples: images with imperceptible perturbations, for example, can
fool image classifiers. In this paper, we present the first type-specific
approach to generating adversarial examples for object detection, which entails
detecting bounding boxes around multiple objects present in the image and
classifying them at the same time, making it a harder task than against image
classification. We specifically aim to attack the widely used Faster R-CNN by
changing the predicted label for a particular object in an image: where prior
work has targeted one specific object (a stop sign), we generalise to arbitrary
objects, with the key challenge being the need to change the labels of all
bounding boxes for all instances of that object type. To do so, we propose a
novel method, named Pick-Object-Attack. Pick-Object-Attack successfully adds
perturbations only to bounding boxes for the targeted object, preserving the
labels of other detected objects in the image. In terms of perceptibility, the
perturbations induced by the method are very small. Furthermore, for the first
time, we examine the effect of adversarial attacks on object detection in terms
of a downstream task, image captioning; we show that where a method that can
modify all object types leads to very obvious changes in captions, the changes
from our constrained attack are much less apparent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nezami_O/0/1/0/all/0/1"&gt;Omid Mohamad Nezami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1"&gt;Akshay Chaturvedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1"&gt;Mark Dras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1"&gt;Utpal Garain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Medical Image Segmentation Based on Knowledge Distillation. (arXiv:2108.09987v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09987</id>
        <link href="http://arxiv.org/abs/2108.09987"/>
        <updated>2021-08-24T01:40:29.541Z</updated>
        <summary type="html"><![CDATA[Recent advances have been made in applying convolutional neural networks to
achieve more precise prediction results for medical image segmentation
problems. However, the success of existing methods has highly relied on huge
computational complexity and massive storage, which is impractical in the
real-world scenario. To deal with this problem, we propose an efficient
architecture by distilling knowledge from well-trained medical image
segmentation networks to train another lightweight network. This architecture
empowers the lightweight network to get a significant improvement on
segmentation capability while retaining its runtime efficiency. We further
devise a novel distillation module tailored for medical image segmentation to
transfer semantic region information from teacher to student network. It forces
the student network to mimic the extent of difference of representations
calculated from different tissue regions. This module avoids the ambiguous
boundary problem encountered when dealing with medical imaging but instead
encodes the internal information of each semantic region for transferring.
Benefited from our module, the lightweight network could receive an improvement
of up to 32.6% in our experiment while maintaining its portability in the
inference phase. The entire structure has been verified on two widely accepted
public CT datasets LiTS17 and KiTS19. We demonstrate that a lightweight network
distilled by our method has non-negligible value in the scenario which requires
relatively high operating speed and low storage usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qin_D/0/1/0/all/0/1"&gt;Dian Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jiajun Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jingjun Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhijua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1"&gt;Huifen Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Regulation for Semantic Segmentation. (arXiv:2108.09702v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09702</id>
        <link href="http://arxiv.org/abs/2108.09702"/>
        <updated>2021-08-24T01:40:29.533Z</updated>
        <summary type="html"><![CDATA[In this paper, we seek reasons for the two major failure cases in Semantic
Segmentation (SS): 1) missing small objects or minor object parts, and 2)
mislabeling minor parts of large objects as wrong classes. We have an
interesting finding that Failure-1 is due to the underuse of detailed features
and Failure-2 is due to the underuse of visual contexts. To help the model
learn a better trade-off, we introduce several Self-Regulation (SR) losses for
training SS neural networks. By "self", we mean that the losses are from the
model per se without using any additional data or supervision. By applying the
SR losses, the deep layer features are regulated by the shallow ones to
preserve more details; meanwhile, shallow layer classification logits are
regulated by the deep ones to capture more semantics. We conduct extensive
experiments on both weakly and fully supervised SS tasks, and the results show
that our approach consistently surpasses the baselines. We also validate that
SR losses are easy to implement in various state-of-the-art SS models, e.g.,
SPGNet and OCRNet, incurring little computational overhead during training and
none for testing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanwang_Z/0/1/0/all/0/1"&gt;Zhang Hanwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jinhui_T/0/1/0/all/0/1"&gt;Tang Jinhui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiansheng_H/0/1/0/all/0/1"&gt;Hua Xiansheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qianru_S/0/1/0/all/0/1"&gt;Sun Qianru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation. (arXiv:2108.09897v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09897</id>
        <link href="http://arxiv.org/abs/2108.09897"/>
        <updated>2021-08-24T01:40:29.528Z</updated>
        <summary type="html"><![CDATA[This paper addresses weakly supervised amodal instance segmentation, where
the goal is to segment both visible and occluded (amodal) object parts, while
training provides only ground-truth visible (modal) segmentations. Following
prior work, we use data manipulation to generate occlusions in training images
and thus train a segmenter to predict amodal segmentations of the manipulated
data. The resulting predictions on training images are taken as the
pseudo-ground truth for the standard training of Mask-RCNN, which we use for
amodal instance segmentation of test images. For generating the pseudo-ground
truth, we specify a new Amodal Segmenter based on Boundary Uncertainty
estimation (ASBU) and make two contributions. First, while prior work uses the
occluder's mask, our ASBU uses the occlusion boundary as input. Second, ASBU
estimates an uncertainty map of the prediction. The estimated uncertainty
regularizes learning such that lower segmentation loss is incurred on regions
with high uncertainty. ASBU achieves significant performance improvement
relative to the state of the art on the COCOA and KINS datasets in three tasks:
amodal instance segmentation, amodal completion, and ordering recovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1"&gt;Sinisa Todorovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment. (arXiv:2108.09980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09980</id>
        <link href="http://arxiv.org/abs/2108.09980"/>
        <updated>2021-08-24T01:40:29.523Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been widely used to train transformer-based
vision-language models for video-text alignment and multi-modal representation
learning. This paper presents a new algorithm called Token-Aware Cascade
contrastive learning (TACo) that improves contrastive learning using two novel
techniques. The first is the token-aware contrastive loss which is computed by
taking into account the syntactic classes of words. This is motivated by the
observation that for a video-text pair, the content words in the text, such as
nouns and verbs, are more likely to be aligned with the visual contents in the
video than the function words. Second, a cascade sampling method is applied to
generate a small set of hard negative examples for efficient loss estimation
for multi-modal fusion layers. To validate the effectiveness of TACo, in our
experiments we finetune pretrained models for a set of downstream tasks
including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video
action step localization (CrossTask), video action segmentation (COIN). The
results show that our models attain consistent improvements across different
experimental settings over previous methods, setting new state-of-the-art on
three public text-video retrieval benchmarks of YouCook2, MSR-VTT and
ActivityNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HeadGAN: One-shot Neural Head Synthesis and Editing. (arXiv:2012.08261v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08261</id>
        <link href="http://arxiv.org/abs/2012.08261"/>
        <updated>2021-08-24T01:40:29.509Z</updated>
        <summary type="html"><![CDATA[Recent attempts to solve the problem of head reenactment using a single
reference image have shown promising results. However, most of them either
perform poorly in terms of photo-realism, or fail to meet the identity
preservation problem, or do not fully transfer the driving pose and expression.
We propose HeadGAN, a novel system that conditions synthesis on 3D face
representations, which can be extracted from any driving video and adapted to
the facial geometry of any reference image, disentangling identity from
expression. We further improve mouth movements, by utilising audio features as
a complementary input. The 3D face representation enables HeadGAN to be further
used as an efficient method for compression and reconstruction and a tool for
expression and pose editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doukas_M/0/1/0/all/0/1"&gt;Michail Christos Doukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharmanska_V/0/1/0/all/0/1"&gt;Viktoriia Sharmanska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning for Deep Object Detection via Probabilistic Modeling. (arXiv:2103.16130v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16130</id>
        <link href="http://arxiv.org/abs/2103.16130"/>
        <updated>2021-08-24T01:40:29.503Z</updated>
        <summary type="html"><![CDATA[Active learning aims to reduce labeling costs by selecting only the most
informative samples on a dataset. Few existing works have addressed active
learning for object detection. Most of these methods are based on multiple
models or are straightforward extensions of classification methods, hence
estimate an image's informativeness using only the classification head. In this
paper, we propose a novel deep active learning approach for object detection.
Our approach relies on mixture density networks that estimate a probabilistic
distribution for each localization and classification head's output. We
explicitly estimate the aleatoric and epistemic uncertainty in a single forward
pass of a single model. Our method uses a scoring function that aggregates
these two types of uncertainties for both heads to obtain every image's
informativeness score. We demonstrate the efficacy of our approach in PASCAL
VOC and MS-COCO datasets. Our approach outperforms single-model based methods
and performs on par with multi-model based methods at a fraction of the
computing cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jiwoong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyuk-Jae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farabet_C/0/1/0/all/0/1"&gt;Clement Farabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Orthogonal Moments for Image Representation: Theory, Implementation, and Evaluation. (arXiv:2103.14799v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14799</id>
        <link href="http://arxiv.org/abs/2103.14799"/>
        <updated>2021-08-24T01:40:29.487Z</updated>
        <summary type="html"><![CDATA[Image representation is an important topic in computer vision and pattern
recognition. It plays a fundamental role in a range of applications towards
understanding visual contents. Moment-based image representation has been
reported to be effective in satisfying the core conditions of semantic
description due to its beneficial mathematical properties, especially geometric
invariance and independence. This paper presents a comprehensive survey of the
orthogonal moments for image representation, covering recent advances in
fast/accurate calculation, robustness/invariance optimization, definition
extension, and application. We also create a software package for a variety of
widely-used orthogonal moments and evaluate such methods in a same base. The
presented theory analysis, software implementation, and evaluation results can
support the community, particularly in developing novel techniques and
promoting real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1"&gt;Shuren Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaochun Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfographicVQA. (arXiv:2104.12756v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12756</id>
        <link href="http://arxiv.org/abs/2104.12756"/>
        <updated>2021-08-24T01:40:29.466Z</updated>
        <summary type="html"><![CDATA[Infographics are documents designed to effectively communicate information
using a combination of textual, graphical and visual elements. In this work, we
explore the automatic understanding of infographic images by using Visual
Question Answering technique.To this end, we present InfographicVQA, a new
dataset that comprises a diverse collection of infographics along with natural
language questions and answers annotations. The collected questions require
methods to jointly reason over the document layout, textual content, graphical
elements, and data visualizations. We curate the dataset with emphasis on
questions that require elementary reasoning and basic arithmetic skills.
Finally, we evaluate two strong baselines based on state of the art multi-modal
VQA models, and establish baseline performance for the new task. The dataset,
code and leaderboard will be made available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1"&gt;Minesh Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagal_V/0/1/0/all/0/1"&gt;Viraj Bagal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1"&gt;Rub&amp;#xe8;n P&amp;#xe9;rez Tito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1"&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1"&gt;Ernest Valveny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C.V Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiaSwap: Removing dataset bias with bias-tailored swapping augmentation. (arXiv:2108.10008v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10008</id>
        <link href="http://arxiv.org/abs/2108.10008"/>
        <updated>2021-08-24T01:40:29.451Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often make decisions based on the spurious correlations
inherent in the dataset, failing to generalize in an unbiased data
distribution. Although previous approaches pre-define the type of dataset bias
to prevent the network from learning it, recognizing the bias type in the real
dataset is often prohibitive. This paper proposes a novel bias-tailored
augmentation-based approach, BiaSwap, for learning debiased representation
without requiring supervision on the bias type. Assuming that the bias
corresponds to the easy-to-learn attributes, we sort the training images based
on how much a biased classifier can exploits them as shortcut and divide them
into bias-guiding and bias-contrary samples in an unsupervised manner.
Afterwards, we integrate the style-transferring module of the image translation
model with the class activation maps of such biased classifier, which enables
to primarily transfer the bias attributes learned by the classifier. Therefore,
given the pair of bias-guiding and bias-contrary, BiaSwap generates the
bias-swapped image which contains the bias attributes from the bias-contrary
images, while preserving bias-irrelevant ones in the bias-guiding images. Given
such augmented images, BiaSwap demonstrates the superiority in debiasing
against the existing baselines over both synthetic and real-world datasets.
Even without careful supervision on the bias, BiaSwap achieves a remarkable
performance on both unbiased and bias-guiding samples, implying the improved
generalization capability of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Eungyeup Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihyeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness Certification for Point Cloud Models. (arXiv:2103.16652v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16652</id>
        <link href="http://arxiv.org/abs/2103.16652"/>
        <updated>2021-08-24T01:40:29.440Z</updated>
        <summary type="html"><![CDATA[The use of deep 3D point cloud models in safety-critical applications, such
as autonomous driving, dictates the need to certify the robustness of these
models to real-world transformations. This is technically challenging, as it
requires a scalable verifier tailored to point cloud models that handles a wide
range of semantic 3D transformations. In this work, we address this challenge
and introduce 3DCertify, the first verifier able to certify the robustness of
point cloud models. 3DCertify is based on two key insights: (i) a generic
relaxation based on first-order Taylor approximations, applicable to any
differentiable transformation, and (ii) a precise relaxation for global feature
pooling, which is more complex than pointwise activations (e.g., ReLU or
sigmoid) but commonly employed in point cloud models. We demonstrate the
effectiveness of 3DCertify by performing an extensive evaluation on a wide
range of 3D transformations (e.g., rotation, twisting) for both classification
and part segmentation tasks. For example, we can certify robustness against
rotations by $\pm$60{\deg} for 95.7% of point clouds, and our max pool
relaxation increases certification by up to 15.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorenz_T/0/1/0/all/0/1"&gt;Tobias Lorenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1"&gt;Anian Ruoss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1"&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1"&gt;Martin Vechev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases. (arXiv:2104.06950v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06950</id>
        <link href="http://arxiv.org/abs/2104.06950"/>
        <updated>2021-08-24T01:40:29.434Z</updated>
        <summary type="html"><![CDATA[We propose a method for the unsupervised reconstruction of a
temporally-coherent sequence of surfaces from a sequence of time-evolving point
clouds, yielding dense, semantically meaningful correspondences between all
keyframes. We represent the reconstructed surface as an atlas, using a neural
network. Using canonical correspondences defined via the atlas, we encourage
the reconstruction to be as isometric as possible across frames, leading to
semantically-meaningful reconstruction. Through experiments and comparisons, we
empirically show that our method achieves results that exceed that state of the
art in the accuracy of unsupervised correspondences and accuracy of surface
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bednarik_J/0/1/0/all/0/1"&gt;Jan Bednarik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"&gt;Vladimir G. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1"&gt;Shaifali Parashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1"&gt;Noam Aigerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relating CNNs with brain: Challenges and findings. (arXiv:2108.09768v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09768</id>
        <link href="http://arxiv.org/abs/2108.09768"/>
        <updated>2021-08-24T01:40:29.416Z</updated>
        <summary type="html"><![CDATA[Conventional neural network models (CNN), loosely inspired by the primate
visual system, have been shown to predict neural responses in the visual
cortex. However, the relationship between CNNs and the visual system is
incomplete due to many reasons. On one hand state of the art CNN architecture
is very complex, yet can be fooled by imperceptibly small, explicitly crafted
perturbations which makes it hard difficult to map layers of the network with
the visual system and to understand what they are doing. On the other hand, we
don't know the exact mapping between feature space of the CNNs and the space
domain of the visual cortex, which makes it hard to accurately predict neural
responses. In this paper we review the challenges and the methods that have
been used to predict neural responses in the visual cortex and whole brain as
part of The Algonauts Project 2021 Challenge: "How the Human Brain Makes Sense
of a World in Motion".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdel_Salam_R/0/1/0/all/0/1"&gt;Reem Abdel-Salam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive Contour Saliency Blending Network for Accurate Salient Object Detection. (arXiv:2105.13865v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13865</id>
        <link href="http://arxiv.org/abs/2105.13865"/>
        <updated>2021-08-24T01:40:29.404Z</updated>
        <summary type="html"><![CDATA[Contour information plays a vital role in salient object detection. However,
excessive false positives remain in predictions from existing contour-based
models due to insufficient contour-saliency fusion. In this work, we designed a
network for better edge quality in salient object detection. We proposed a
contour-saliency blending module to exchange information between contour and
saliency. We adopted recursive CNN to increase contour-saliency fusion while
keeping the total trainable parameters the same. Furthermore, we designed a
stage-wise feature extraction module to help the model pick up the most helpful
features from previous intermediate saliency predictions. Besides, we proposed
two new loss functions, namely Dual Confinement Loss and Confidence Loss, for
our model to generate better boundary predictions. Evaluation results on five
common benchmark datasets reveal that our model achieves competitive
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1"&gt;Yi Ke Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsubono_T/0/1/0/all/0/1"&gt;Takahiro Tsubono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Transformer Networks for Curriculum Learning. (arXiv:2108.09696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09696</id>
        <link href="http://arxiv.org/abs/2108.09696"/>
        <updated>2021-08-24T01:40:29.391Z</updated>
        <summary type="html"><![CDATA[Curriculum learning is a bio-inspired training technique that is widely
adopted to machine learning for improved optimization and better training of
neural networks regarding the convergence rate or obtained accuracy. The main
concept in curriculum learning is to start the training with simpler tasks and
gradually increase the level of difficulty. Therefore, a natural question is
how to determine or generate these simpler tasks. In this work, we take
inspiration from Spatial Transformer Networks (STNs) in order to form an
easy-to-hard curriculum. As STNs have been proven to be capable of removing the
clutter from the input images and obtaining higher accuracy in image
classification tasks, we hypothesize that images processed by STNs can be seen
as easier tasks and utilized in the interest of curriculum learning. To this
end, we study multiple strategies developed for shaping the training
curriculum, using the data generated by STNs. We perform various experiments on
cluttered MNIST and Fashion-MNIST datasets, where on the former, we obtain an
improvement of $3.8$pp in classification accuracy compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azimi_F/0/1/0/all/0/1"&gt;Fatemeh Azimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nies_J/0/1/0/all/0/1"&gt;Jean-Francois Jacques Nicolas Nies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1"&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1"&gt;Federico Raue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Inpainting via Conditional Texture and Structure Dual Generation. (arXiv:2108.09760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09760</id>
        <link href="http://arxiv.org/abs/2108.09760"/>
        <updated>2021-08-24T01:40:29.364Z</updated>
        <summary type="html"><![CDATA[Deep generative approaches have recently made considerable progress in image
inpainting by introducing structure priors. Due to the lack of proper
interaction with image texture during structure reconstruction, however,
current solutions are incompetent in handling the cases with large corruptions,
and they generally suffer from distorted results. In this paper, we propose a
novel two-stream network for image inpainting, which models the
structure-constrained texture synthesis and texture-guided structure
reconstruction in a coupled manner so that they better leverage each other for
more plausible generation. Furthermore, to enhance the global consistency, a
Bi-directional Gated Feature Fusion (Bi-GFF) module is designed to exchange and
combine the structure and texture information and a Contextual Feature
Aggregation (CFA) module is developed to refine the generated contents by
region affinity learning and multi-scale feature aggregation. Qualitative and
quantitative experiments on the CelebA, Paris StreetView and Places2 datasets
demonstrate the superiority of the proposed method. Our code is available at
https://github.com/Xiefan-Guo/CTSDG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiefan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Holistic 3D Scene Understanding from a Single Image with Implicit Representation. (arXiv:2103.06422v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06422</id>
        <link href="http://arxiv.org/abs/2103.06422"/>
        <updated>2021-08-24T01:40:29.359Z</updated>
        <summary type="html"><![CDATA[We present a new pipeline for holistic 3D scene understanding from a single
image, which could predict object shapes, object poses, and scene layout. As it
is a highly ill-posed problem, existing methods usually suffer from inaccurate
estimation of both shapes and layout especially for the cluttered scene due to
the heavy occlusion between objects. We propose to utilize the latest deep
implicit representation to solve this challenge. We not only propose an
image-based local structured implicit network to improve the object shape
estimation, but also refine the 3D object pose and scene layout via a novel
implicit scene graph neural network that exploits the implicit local object
features. A novel physical violation loss is also proposed to avoid incorrect
context between objects. Extensive experiments demonstrate that our method
outperforms the state-of-the-art methods in terms of object shape, scene layout
estimation, and 3D object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhaopeng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yinda Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1"&gt;Bing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Pose Estimation with Spatial and Temporal Transformers. (arXiv:2103.10455v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10455</id>
        <link href="http://arxiv.org/abs/2103.10455"/>
        <updated>2021-08-24T01:40:29.353Z</updated>
        <summary type="html"><![CDATA[Transformer architectures have become the model of choice in natural language
processing and are now being introduced into computer vision tasks such as
image classification, object detection, and semantic segmentation. However, in
the field of human pose estimation, convolutional architectures still remain
dominant. In this work, we present PoseFormer, a purely transformer-based
approach for 3D human pose estimation in videos without convolutional
architectures involved. Inspired by recent developments in vision transformers,
we design a spatial-temporal transformer structure to comprehensively model the
human joint relations within each frame as well as the temporal correlations
across frames, then output an accurate 3D human pose of the center frame. We
quantitatively and qualitatively evaluate our method on two popular and
standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments
show that PoseFormer achieves state-of-the-art performance on both datasets.
Code is available at \url{https://github.com/zczcwh/PoseFormer}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Ce Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Sijie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1"&gt;Matias Mendieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Taojiannan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhengming Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safeguarded Dynamic Label Regression for Generalized Noisy Supervision. (arXiv:1903.02152v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.02152</id>
        <link href="http://arxiv.org/abs/1903.02152"/>
        <updated>2021-08-24T01:40:29.347Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels, which aims to reduce expensive labors on accurate
annotations, has become imperative in the Big Data era. Previous noise
transition based method has achieved promising results and presented a
theoretical guarantee on performance in the case of class-conditional noise.
However, this type of approaches critically depend on an accurate
pre-estimation of the noise transition, which is usually impractical.
Subsequent improvement adapts the pre-estimation along with the training
progress via a Softmax layer. However, the parameters in the Softmax layer are
highly tweaked for the fragile performance due to the ill-posed stochastic
approximation. To address these issues, we propose a Latent Class-Conditional
Noise model (LCCN) that naturally embeds the noise transition under a Bayesian
framework. By projecting the noise transition into a Dirichlet-distributed
space, the learning is constrained on a simplex based on the whole dataset,
instead of some ad-hoc parametric space. We then deduce a dynamic label
regression method for LCCN to iteratively infer the latent labels, to
stochastically train the classifier and to model the noise. Our approach
safeguards the bounded update of the noise transition, which avoids previous
arbitrarily tuning via a batch of samples. We further generalize LCCN for
open-set noisy labels and the semi-supervised setting. We perform extensive
experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and
the agnostic noise data sets, Clothing1M and WebVision17. The experimental
results have demonstrated that the proposed model outperforms several
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W. Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Video Deblurring with Blur-Invariant Motion Estimation and Pixel Volumes. (arXiv:2108.09982v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09982</id>
        <link href="http://arxiv.org/abs/2108.09982"/>
        <updated>2021-08-24T01:40:29.342Z</updated>
        <summary type="html"><![CDATA[For the success of video deblurring, it is essential to utilize information
from neighboring frames. Most state-of-the-art video deblurring methods adopt
motion compensation between video frames to aggregate information from multiple
frames that can help deblur a target frame. However, the motion compensation
methods adopted by previous deblurring methods are not blur-invariant, and
consequently, their accuracy is limited for blurry frames with different blur
amounts. To alleviate this problem, we propose two novel approaches to deblur
videos by effectively aggregating information from multiple video frames.
First, we present blur-invariant motion estimation learning to improve motion
estimation accuracy between blurry frames. Second, for motion compensation,
instead of aligning frames by warping with estimated motions, we use a pixel
volume that contains candidate sharp pixels to resolve motion estimation
errors. We combine these two processes to propose an effective recurrent video
deblurring network that fully exploits deblurred previous frames. Experiments
show that our method achieves the state-of-the-art performance both
quantitatively and qualitatively compared to recent methods that use deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1"&gt;Hyeongseok Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junyong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jonghyeop Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Sunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Video Reconstruction: A Coarse-to-fine Network and A Real-world Benchmark Dataset. (arXiv:2103.14943v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14943</id>
        <link href="http://arxiv.org/abs/2103.14943"/>
        <updated>2021-08-24T01:40:29.326Z</updated>
        <summary type="html"><![CDATA[High dynamic range (HDR) video reconstruction from sequences captured with
alternating exposures is a very challenging problem. Existing methods often
align low dynamic range (LDR) input sequence in the image space using optical
flow, and then merge the aligned images to produce HDR output. However,
accurate alignment and fusion in the image space are difficult due to the
missing details in the over-exposed regions and noise in the under-exposed
regions, resulting in unpleasing ghosting artifacts. To enable more accurate
alignment and HDR fusion, we introduce a coarse-to-fine deep learning framework
for HDR video reconstruction. Firstly, we perform coarse alignment and pixel
blending in the image space to estimate the coarse HDR video. Secondly, we
conduct more sophisticated alignment and temporal fusion in the feature space
of the coarse HDR video to produce better reconstruction. Considering the fact
that there is no publicly available dataset for quantitative and comprehensive
evaluation of HDR video reconstruction methods, we collect such a benchmark
dataset, which contains $97$ sequences of static scenes and 184 testing pairs
of dynamic scenes. Extensive experiments show that our method outperforms
previous state-of-the-art methods. Our dataset, code and model will be made
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guanying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhetong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;Kwan-Yee K. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Borrowing for Generalized Zero-Shot Learning. (arXiv:2102.04969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04969</id>
        <link href="http://arxiv.org/abs/2102.04969"/>
        <updated>2021-08-24T01:40:29.320Z</updated>
        <summary type="html"><![CDATA[Generalized zero-shot learning (GZSL) is one of the most realistic but
challenging problems due to the partiality of the classifier to supervised
classes, especially under the class-inductive instance-inductive (CIII)
training setting, where testing data are not available. Instance-borrowing
methods and synthesizing methods solve it to some extent with the help of
testing semantics, but therefore neither can be used under CIII. Besides, the
latter require the training process of a classifier after generating examples.
In contrast, a novel non-transductive regularization under CIII called Semantic
Borrowing (SB) for improving GZSL methods with compatibility metric learning is
proposed in this paper, which not only can be used for training linear models,
but also nonlinear ones such as artificial neural networks. This regularization
item in the loss function borrows similar semantics in the training set, so
that the classifier can model the relationship between the semantics of
zero-shot and supervised classes more accurately during training. In practice,
the information of semantics of unknown classes would not be available for
training while this approach does NOT need it. Extensive experiments on GZSL
benchmark datasets show that SB can reduce the partiality of the classifier to
supervised classes and improve the performance of generalized zero-shot
classification, surpassing inductive GZSL state of the arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaowei Chen&lt;/a&gt; (Sun Yat-sen University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Saliency Transformer. (arXiv:2104.12099v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12099</id>
        <link href="http://arxiv.org/abs/2104.12099"/>
        <updated>2021-08-24T01:40:29.313Z</updated>
        <summary type="html"><![CDATA[Existing state-of-the-art saliency detection methods heavily rely on
CNN-based architectures. Alternatively, we rethink this task from a
convolution-free sequence-to-sequence perspective and predict saliency by
modeling long-range dependencies, which can not be achieved by convolution.
Specifically, we develop a novel unified model based on a pure transformer,
namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient
object detection (SOD). It takes image patches as inputs and leverages the
transformer to propagate global contexts among image patches. Unlike
conventional architectures used in Vision Transformer (ViT), we leverage
multi-level token fusion and propose a new token upsampling method under the
transformer framework to get high-resolution detection results. We also develop
a token-based multi-task decoder to simultaneously perform saliency and
boundary detection by introducing task-related tokens and a novel
patch-task-attention mechanism. Experimental results show that our model
outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most
importantly, our whole framework not only provides a new perspective for the
SOD field but also shows a new paradigm for transformer-based dense prediction
models. Code is available at https://github.com/nnizhang/VST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ni Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1"&gt;Kaiyuan Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junwei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypercorrelation Squeeze for Few-Shot Segmentation. (arXiv:2104.01538v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01538</id>
        <link href="http://arxiv.org/abs/2104.01538"/>
        <updated>2021-08-24T01:40:29.307Z</updated>
        <summary type="html"><![CDATA[Few-shot semantic segmentation aims at learning to segment a target object
from a query image using only a few annotated support images of the target
class. This challenging task requires to understand diverse levels of visual
cues and analyze fine-grained correspondence relations between the query and
the support images. To address the problem, we propose Hypercorrelation Squeeze
Networks (HSNet) that leverages multi-level feature correlation and efficient
4D convolutions. It extracts diverse features from different levels of
intermediate convolutional layers and constructs a collection of 4D correlation
tensors, i.e., hypercorrelations. Using efficient center-pivot 4D convolutions
in a pyramidal architecture, the method gradually squeezes high-level semantic
and low-level geometric cues of the hypercorrelation into precise segmentation
masks in coarse-to-fine manner. The significant performance improvements on
standard few-shot segmentation benchmarks of PASCAL-5i, COCO-20i, and FSS-1000
verify the efficacy of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1"&gt;Juhong Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Dahyun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;Minsu Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust low-rank multilinear tensor approximation for a joint estimation of the multilinear rank and the loading matrices. (arXiv:1811.05863v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.05863</id>
        <link href="http://arxiv.org/abs/1811.05863"/>
        <updated>2021-08-24T01:40:29.302Z</updated>
        <summary type="html"><![CDATA[In order to compute the best low-rank tensor approximation using the
Multilinear Tensor Decomposition (MTD) model, it is essential to estimate the
rank of the underlying multilinear tensor from the noisy observation tensor. In
this paper, we propose a Robust MTD (R-MTD) method, which jointly estimates the
multilinear rank and the loading matrices. Based on the low-rank property and
an over-estimation of the core tensor, this joint estimation problem is solved
by promoting (group) sparsity of the over-estimated core tensor. Group sparsity
is promoted using mixed-norms. Then we establish a link between the mixed-norms
and the nuclear norm, showing that mixed-norms are better candidates for a
convex envelope of the rank. After several iterations of the Alternating
Direction Method of Multipliers (ADMM), the Minimum Description Length (MDL)
criterion computed from the eigenvalues of the unfolding matrices of the
estimated core tensor is minimized in order to estimate the multilinear rank.
The latter is then used to estimate more accurately the loading matrices. We
further develop another R-MTD method, called R-OMTD, by imposing an
orthonormality constraint on each loading matrix in order to decrease the
computation complexity. A series of simulated noisy tensor and real-world data
are used to show the effectiveness of the proposed methods compared with
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albera_L/0/1/0/all/0/1"&gt;Laurent Albera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kachenoura_A/0/1/0/all/0/1"&gt;Amar Kachenoura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1"&gt;Huazhong Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senhadji_L/0/1/0/all/0/1"&gt;Lotfi Senhadji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural Network. (arXiv:2104.09248v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09248</id>
        <link href="http://arxiv.org/abs/2104.09248"/>
        <updated>2021-08-24T01:40:29.286Z</updated>
        <summary type="html"><![CDATA[Being capable of estimating the pose of uncooperative objects in space has
been proposed as a key asset for enabling safe close-proximity operations such
as space rendezvous, in-orbit servicing and active debris removal. Usual
approaches for pose estimation involve classical computer vision-based
solutions or the application of Deep Learning (DL) techniques. This work
explores a novel DL-based methodology, using Convolutional Neural Networks
(CNNs), for estimating the pose of uncooperative spacecrafts. Contrary to other
approaches, the proposed CNN directly regresses poses without needing any prior
3D information. Moreover, bounding boxes of the spacecraft in the image are
predicted in a simple, yet efficient manner. The performed experiments show how
this work competes with the state-of-the-art in uncooperative spacecraft pose
estimation, including works which require 3D information as well as works which
predict bounding boxes through sophisticated CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1"&gt;Albert Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musallam_M/0/1/0/all/0/1"&gt;Mohamed Adel Musallam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaudilliere_V/0/1/0/all/0/1"&gt;Vincent Gaudilliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghorbel_E/0/1/0/all/0/1"&gt;Enjie Ghorbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ismaeil_K/0/1/0/all/0/1"&gt;Kassem Al Ismaeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1"&gt;Marcos Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1"&gt;Djamila Aouada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-based Distance Decomposition for Monocular 3D Object Detection. (arXiv:2104.03775v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03775</id>
        <link href="http://arxiv.org/abs/2104.03775"/>
        <updated>2021-08-24T01:40:29.281Z</updated>
        <summary type="html"><![CDATA[Monocular 3D object detection is of great significance for autonomous driving
but remains challenging. The core challenge is to predict the distance of
objects in the absence of explicit depth information. Unlike regressing the
distance as a single variable in most existing methods, we propose a novel
geometry-based distance decomposition to recover the distance by its factors.
The decomposition factors the distance of objects into the most representative
and stable variables, i.e. the physical height and the projected visual height
in the image plane. Moreover, the decomposition maintains the self-consistency
between the two heights, leading to robust distance prediction when both
predicted heights are inaccurate. The decomposition also enables us to trace
the causes of the distance uncertainty for different scenarios. Such
decomposition makes the distance prediction interpretable, accurate, and
robust. Our method directly predicts 3D bounding boxes from RGB images with a
compact architecture, making the training and inference simple and efficient.
The experimental results show that our method achieves the state-of-the-art
performance on the monocular 3D Object Detection and Birds Eye View tasks of
the KITTI dataset, and can generalize to images with different camera
intrinsics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xuepeng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaozhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuangrong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Tae-Kyun Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Quantitative Approach for Optimizing Convolutional Neural Networks. (arXiv:2009.05236v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05236</id>
        <link href="http://arxiv.org/abs/2009.05236"/>
        <updated>2021-08-24T01:40:29.275Z</updated>
        <summary type="html"><![CDATA[With the increasing popularity of deep learning, Convolutional Neural
Networks (CNNs) have been widely applied in various domains, such as image
classification and object detection, and achieve stunning success in terms of
their high accuracy over the traditional statistical methods. To exploit the
potential of CNN models, a huge amount of research and industry efforts have
been devoted to optimizing CNNs. Among these endeavors, CNN architecture design
has attracted tremendous attention because of its great potential of improving
model accuracy or reducing model complexity. However, existing work either
introduces repeated training overhead in the search process or lacks an
interpretable metric to guide the design. To clear these hurdles, we propose
Information Field (IF), an explainable and easy-to-compute metric, to estimate
the quality of a CNN architecture and guide the search process of designs. To
validate the effectiveness of IF, we build a static optimizer to improve the
CNN architectures at both the stage level and the kernel level. Our optimizer
not only provides a clear and reproducible procedure but also mitigates
unnecessary training efforts in the architecture search process. Extensive
experiments and studies show that the models generated by our optimizer can
achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction,
compared with state-of-the-art CNN structures like MobileNet and ResNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Boyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xueqiao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yufei Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations. (arXiv:2012.06440v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06440</id>
        <link href="http://arxiv.org/abs/2012.06440"/>
        <updated>2021-08-24T01:40:29.269Z</updated>
        <summary type="html"><![CDATA[This work proposes a weakly-supervised temporal action localization
framework, called D2-Net, which strives to temporally localize actions using
video-level supervision. Our main contribution is the introduction of a novel
loss formulation, which jointly enhances the discriminability of latent
embeddings and robustness of the output temporal class activations with respect
to foreground-background noise caused by weak supervision. The proposed
formulation comprises a discriminative and a denoising loss term for enhancing
temporal action localization. The discriminative term incorporates a
classification loss and utilizes a top-down attention mechanism to enhance the
separability of latent foreground-background embeddings. The denoising loss
term explicitly addresses the foreground-background noise in class activations
by simultaneously maximizing intra-video and inter-video mutual information
using a bottom-up attention mechanism. As a result, activations in the
foreground regions are emphasized whereas those in the background regions are
suppressed, thereby leading to more robust predictions. Comprehensive
experiments are performed on multiple benchmarks, including THUMOS14 and
ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing
methods on all datasets, achieving gains as high as 2.3% in terms of mAP at
IoU=0.5 on THUMOS14. Source code is available at
https://github.com/naraysa/D2-Net]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1"&gt;Sanath Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1"&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Surprising Efficiency of Committee-based Models. (arXiv:2012.01988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01988</id>
        <link href="http://arxiv.org/abs/2012.01988"/>
        <updated>2021-08-24T01:40:29.263Z</updated>
        <summary type="html"><![CDATA[Committee-based models, i.e., model ensembles or cascades, are underexplored
in recent work on developing efficient models. While committee-based models
themselves are not new, there lacks a systematic understanding of their
efficiency in comparison with single models. To fill this gap, we conduct a
comprehensive analysis of the efficiency of committee-based models. We find
that committee-based models provide a complementary paradigm to achieve
superior efficiency without tuning the architecture: even the most simplistic
method for building ensembles or cascades from existing pre-trained networks
can attain a significant speedup and higher accuracy over state-of-the-art
single models, and also outperforms sophisticated neural architecture search
methods. The superior efficiency of committee-based models holds true for
several tasks, including image classification, video classification, and
semantic segmentation, and various architecture families, such as EfficientNet,
ResNet, MobileNetV2, and X3D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaofang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1"&gt;Dan Kondratyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christiansen_E/0/1/0/all/0/1"&gt;Eric Christiansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris M. Kitani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Movshovitz_Attias_Y/0/1/0/all/0/1"&gt;Yair Movshovitz-Attias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1"&gt;Elad Eban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08462</id>
        <link href="http://arxiv.org/abs/2106.08462"/>
        <updated>2021-08-24T01:40:29.244Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Neural Ordinary Differential Equations (ODEs) can
serve as generative models of images using the perspective of Continuous
Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and
invertible generation/density estimation. In this work we introduce a
Multi-Resolution variant of such models (MRCNF), by characterizing the
conditional distribution over the additional information required to generate a
fine image that is consistent with the coarse image. We introduce a
transformation between resolutions that allows for no change in the log
likelihood. We show that this approach yields comparable likelihood values for
various image datasets, with improved performance at higher resolutions, with
fewer parameters, using only 1 GPU. Further, we examine the out-of-distribution
properties of (Multi-Resolution) Continuous Normalizing Flows, and find that
they are similar to those of other likelihood-based generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1"&gt;Chris Finlay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1"&gt;Adam Oberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-Box Test-Time Shape REFINEment for Single View 3D Reconstruction. (arXiv:2108.09911v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09911</id>
        <link href="http://arxiv.org/abs/2108.09911"/>
        <updated>2021-08-24T01:40:29.239Z</updated>
        <summary type="html"><![CDATA[Much recent progress has been made in reconstructing the 3D shape of an
object from an image of it, i.e. single view 3D reconstruction. However, it has
been suggested that current methods simply adopt a "nearest-neighbor" strategy,
instead of genuinely understanding the shape behind the input image. In this
paper, we rigorously show that for many state of the art methods, this issue
manifests as (1) inconsistencies between coarse reconstructions and input
images, and (2) inability to generalize across domains. We thus propose REFINE,
a postprocessing mesh refinement step that can be easily integrated into the
pipeline of any black-box method in the literature. At test time, REFINE
optimizes a network per mesh instance, to encourage consistency between the
mesh and the given object view. This, along with a novel combination of
regularizing losses, reduces the domain gap and achieves state of the art
performance. We believe that this novel paradigm is an important step towards
robust, accurate reconstructions, remaining relevant as new reconstruction
networks are introduced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leung_B/0/1/0/all/0/1"&gt;Brandon Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1"&gt;Chih-Hui Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1"&gt;Nuno Vasconcelos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ID-Reveal: Identity-aware DeepFake Video Detection. (arXiv:2012.02512v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02512</id>
        <link href="http://arxiv.org/abs/2012.02512"/>
        <updated>2021-08-24T01:40:29.233Z</updated>
        <summary type="html"><![CDATA[A major challenge in DeepFake forgery detection is that state-of-the-art
algorithms are mostly trained to detect a specific fake method. As a result,
these approaches show poor generalization across different types of facial
manipulations, e.g., from face swapping to facial reenactment. To this end, we
introduce ID-Reveal, a new approach that learns temporal facial features,
specific of how a person moves while talking, by means of metric learning
coupled with an adversarial training strategy. The advantage is that we do not
need any training data of fakes, but only train on real videos. Moreover, we
utilize high-level semantic features, which enables robustness to widespread
and disruptive forms of post-processing. We perform a thorough experimental
analysis on several publicly available benchmarks. Compared to state of the
art, our method improves generalization and is more robust to low-quality
videos, that are usually spread over social networks. In particular, we obtain
an average improvement of more than 15% in terms of accuracy for facial
reenactment on high compressed videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cozzolino_D/0/1/0/all/0/1"&gt;Davide Cozzolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossler_A/0/1/0/all/0/1"&gt;Andreas R&amp;#xf6;ssler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1"&gt;Justus Thies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1"&gt;Luisa Verdoliva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness-via-Synthesis: Robust Training with Generative Adversarial Perturbations. (arXiv:2108.09713v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09713</id>
        <link href="http://arxiv.org/abs/2108.09713"/>
        <updated>2021-08-24T01:40:29.228Z</updated>
        <summary type="html"><![CDATA[Upon the discovery of adversarial attacks, robust models have become
obligatory for deep learning-based systems. Adversarial training with
first-order attacks has been one of the most effective defenses against
adversarial perturbations to this day. The majority of the adversarial training
approaches focus on iteratively perturbing each pixel with the gradient of the
loss function with respect to the input image. However, the adversarial
training with gradient-based attacks lacks diversity and does not generalize
well to natural images and various attacks. This study presents a robust
training algorithm where the adversarial perturbations are automatically
synthesized from a random vector using a generator network. The classifier is
trained with cross-entropy loss regularized with the optimal transport distance
between the representations of the natural and synthesized adversarial samples.
Unlike prevailing generative defenses, the proposed one-step attack generation
framework synthesizes diverse perturbations without utilizing gradient of the
classifier's loss. Experimental results show that the proposed approach attains
comparable robustness with various gradient-based and generative robust
training techniques on CIFAR10, CIFAR100, and SVHN datasets. In addition,
compared to the baselines, the proposed robust training framework generalizes
well to the natural samples. Code and trained models will be made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baytas_I/0/1/0/all/0/1"&gt;Inci M. Baytas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_D/0/1/0/all/0/1"&gt;Debayan Deb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-08-24T01:40:29.221Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from the human's visual and intuitive perspective. We
take the first step to bridge the gap by proposing a deep learning-based
technique to automatically classify road networks into four classes on a visual
basis. The method is implemented by generating an image of the street network
(Colored Road Hierarchy Diagram), which we introduce in this paper, and
classifying it using a deep convolutional neural network (ResNet-34). The model
achieves an overall classification accuracy of 0.875. Nine cities around the
world are selected as the study areas with their road networks acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: we apply
our method in a case study of urban vitality prediction. An advanced tree-based
regression model (LightGBM) is for the first time designated to establish the
relationship between morphological indices and vitality indicators. The effect
of road network classification is found to be small but positively associated
with urban vitality. This work expands the toolkit of quantitative urban
morphology study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph2Pix: A Graph-Based Image to Image Translation Framework. (arXiv:2108.09752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09752</id>
        <link href="http://arxiv.org/abs/2108.09752"/>
        <updated>2021-08-24T01:40:29.197Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a graph-based image-to-image translation framework
for generating images. We use rich data collected from the popular creativity
platform Artbreeder (this http URL), where users interpolate multiple
GAN-generated images to create artworks. This unique approach of creating new
images leads to a tree-like structure where one can track historical data about
the creation of a particular image. Inspired by this structure, we propose a
novel graph-to-image translation model called Graph2Pix, which takes a graph
and corresponding images as input and generates a single image as output. Our
experiments show that Graph2Pix is able to outperform several image-to-image
translation frameworks on benchmark metrics, including LPIPS (with a 25%
improvement) and human perception studies (n=60), where users preferred the
images generated by our method 81.5% of the time. Our source code and dataset
are publicly available at https://github.com/catlab-team/graph2pix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gokay_D/0/1/0/all/0/1"&gt;Dilara Gokay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simsar_E/0/1/0/all/0/1"&gt;Enis Simsar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atici_E/0/1/0/all/0/1"&gt;Efehan Atici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmetoglu_A/0/1/0/all/0/1"&gt;Alper Ahmetoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuksel_A/0/1/0/all/0/1"&gt;Atif Emre Yuksel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1"&gt;Pinar Yanardag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Layered Image Decomposition into Object Prototypes. (arXiv:2104.14575v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14575</id>
        <link href="http://arxiv.org/abs/2104.14575"/>
        <updated>2021-08-24T01:40:29.192Z</updated>
        <summary type="html"><![CDATA[We present an unsupervised learning framework for decomposing images into
layers of automatically discovered object models. Contrary to recent approaches
that model image layers with autoencoder networks, we represent them as
explicit transformations of a small set of prototypical images. Our model has
three main components: (i) a set of object prototypes in the form of learnable
images with a transparency channel, which we refer to as sprites; (ii)
differentiable parametric functions predicting occlusions and transformation
parameters necessary to instantiate the sprites in a given image; (iii) a
layered image formation model with occlusion for compositing these instances
into complete images including background. By jointly learning the sprites and
occlusion/transformation predictors to reconstruct images, our approach not
only yields accurate layered image decompositions, but also identifies object
categories and instance parameters. We first validate our approach by providing
results on par with the state of the art on standard multi-object synthetic
benchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the
applicability of our model to real images in tasks that include clustering
(SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from
unfiltered social network images. To the best of our knowledge, our approach is
the first layered image decomposition algorithm that learns an explicit and
shared concept of object type, and is robust enough to be applied to real
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1"&gt;Tom Monnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1"&gt;Elliot Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1"&gt;Mathieu Aubry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECACL: A Holistic Framework for Semi-Supervised Domain Adaptation. (arXiv:2104.09136v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09136</id>
        <link href="http://arxiv.org/abs/2104.09136"/>
        <updated>2021-08-24T01:40:29.185Z</updated>
        <summary type="html"><![CDATA[This paper studies Semi-Supervised Domain Adaptation (SSDA), a practical yet
under-investigated research topic that aims to learn a model of good
performance using unlabeled samples and a few labeled samples in the target
domain, with the help of labeled samples from a source domain. Several SSDA
methods have been proposed recently, which however fail to fully exploit the
value of the few labeled target samples. In this paper, we propose Enhanced
Categorical Alignment and Consistency Learning (ECACL), a holistic SSDA
framework that incorporates multiple mutually complementary domain alignment
techniques. ECACL includes two categorical domain alignment techniques that
achieve class-level alignment, a strong data augmentation based technique that
enhances the model's generalizability and a consistency learning based
technique that forces the model to be robust with image perturbations. These
techniques are applied on one or multiple of the three inputs (labeled source,
unlabeled target, and labeled target) and align the domains from different
perspectives. ECACL unifies them together and achieves fairly comprehensive
domain alignments that are much better than the existing methods: For example,
ECACL raises the state-of-the-art accuracy from 68.4 to 81.1 on VisDA2017 and
from 45.5 to 53.4 on DomainNet for the 1-shot setting. Our code is available at
\url{https://github.com/kailigo/pacl}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Handong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yulun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Burst Imaging for Light-Constrained Structure-From-Motion. (arXiv:2108.09895v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.09895</id>
        <link href="http://arxiv.org/abs/2108.09895"/>
        <updated>2021-08-24T01:40:29.179Z</updated>
        <summary type="html"><![CDATA[Images captured under extremely low light conditions are noise-limited, which
can cause existing robotic vision algorithms to fail. In this paper we develop
an image processing technique for aiding 3D reconstruction from images acquired
in low light conditions. Our technique, based on burst photography, uses direct
methods for image registration within bursts of short exposure time images to
improve the robustness and accuracy of feature-based structure-from-motion
(SfM). We demonstrate improved SfM performance in challenging light-constrained
scenes, including quantitative evaluations that show improved feature
performance and camera pose estimates. Additionally, we show that our method
converges more frequently to correct reconstructions than the state-of-the-art.
Our method is a significant step towards allowing robots to operate in low
light conditions, with potential applications to robots operating in
environments such as underground mines and night time operation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravendran_A/0/1/0/all/0/1"&gt;Ahalya Ravendran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bryson_M/0/1/0/all/0/1"&gt;Mitch Bryson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dansereau_D/0/1/0/all/0/1"&gt;Donald G. Dansereau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PR-GCN: A Deep Graph Convolutional Network with Point Refinement for 6D Pose Estimation. (arXiv:2108.09916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09916</id>
        <link href="http://arxiv.org/abs/2108.09916"/>
        <updated>2021-08-24T01:40:29.174Z</updated>
        <summary type="html"><![CDATA[RGB-D based 6D pose estimation has recently achieved remarkable progress, but
still suffers from two major limitations: (1) ineffective representation of
depth data and (2) insufficient integration of different modalities. This paper
proposes a novel deep learning approach, namely Graph Convolutional Network
with Point Refinement (PR-GCN), to simultaneously address the issues above in a
unified way. It first introduces the Point Refinement Network (PRN) to polish
3D point clouds, recovering missing parts with noise removed. Subsequently, the
Multi-Modal Fusion Graph Convolutional Network (MMF-GCN) is presented to
strengthen RGB-D combination, which captures geometry-aware inter-modality
correlation through local information propagation in the graph convolutional
network. Extensive experiments are conducted on three widely used benchmarks,
and state-of-the-art performance is reached. Besides, it is also shown that the
proposed PRN and MMF-GCN modules are well generalized to other frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guangyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huiqun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The 2nd Anti-UAV Workshop & Challenge: Methods and Results. (arXiv:2108.09909v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09909</id>
        <link href="http://arxiv.org/abs/2108.09909"/>
        <updated>2021-08-24T01:40:29.159Z</updated>
        <summary type="html"><![CDATA[The 2nd Anti-UAV Workshop \& Challenge aims to encourage research in
developing novel and accurate methods for multi-scale object tracking. The
Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released.
There are two subsets in the dataset, $i.e.$, the test-dev subset and
test-challenge subset. Both subsets consist of 140 thermal infrared video
sequences, spanning multiple occurrences of multi-scale UAVs. Around 24
participating teams from the globe competed in the 2nd Anti-UAV Challenge. In
this paper, we provide a brief summary of the 2nd Anti-UAV Workshop \&
Challenge including brief introductions to the top three methods.The submission
leaderboard will be reopened for researchers that are interested in the
Anti-UAV challenge. The benchmark dataset and other information can be found
at: https://anti-uav.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1"&gt;Nana Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Min Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaojuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yong_T/0/1/0/all/0/1"&gt;Ting Yong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yafeng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yandong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shiming Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPRNet: Hierarchical Point Regression for Whole-Body Human Pose Estimation. (arXiv:2106.04269v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04269</id>
        <link href="http://arxiv.org/abs/2106.04269"/>
        <updated>2021-08-24T01:40:29.154Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new bottom-up one-stage method for whole-body
pose estimation, which we call "hierarchical point regression," or HPRNet for
short. In standard body pose estimation, the locations of $\sim 17$ major
joints on the human body are estimated. Differently, in whole-body pose
estimation, the locations of fine-grained keypoints (68 on face, 21 on each
hand and 3 on each foot) are estimated as well, which creates a scale variance
problem that needs to be addressed. To handle the scale variance among
different body parts, we build a hierarchical point representation of body
parts and jointly regress them. The relative locations of fine-grained
keypoints in each part (e.g. face) are regressed in reference to the center of
that part, whose location itself is estimated relative to the person center. In
addition, unlike the existing two-stage methods, our method predicts whole-body
pose in a constant time independent of the number of people in an image. On the
COCO WholeBody dataset, HPRNet significantly outperforms all previous bottom-up
methods on the keypoint detection of all whole-body parts (i.e. body, foot,
face and hand); it also achieves state-of-the-art results on face (75.4 AP) and
hand (50.4 AP) keypoint detection. Code and models are available at
\url{https://github.com/nerminsamet/HPRNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samet_N/0/1/0/all/0/1"&gt;Nermin Samet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CANet: A Context-Aware Network for Shadow Removal. (arXiv:2108.09894v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09894</id>
        <link href="http://arxiv.org/abs/2108.09894"/>
        <updated>2021-08-24T01:40:29.148Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel two-stage context-aware network named CANet
for shadow removal, in which the contextual information from non-shadow regions
is transferred to shadow regions at the embedded feature spaces. At Stage-I, we
propose a contextual patch matching (CPM) module to generate a set of potential
matching pairs of shadow and non-shadow patches. Combined with the potential
contextual relationships between shadow and non-shadow regions, our
well-designed contextual feature transfer (CFT) mechanism can transfer
contextual information from non-shadow to shadow regions at different scales.
With the reconstructed feature maps, we remove shadows at L and A/B channels
separately. At Stage-II, we use an encoder-decoder to refine current results
and generate the final shadow removal results. We evaluate our proposed CANet
on two benchmark datasets and some real-world shadow images with complex
scenes. Extensive experimental results strongly demonstrate the efficacy of our
proposed CANet and exhibit superior performance to state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zipei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Ling Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chunxia Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Dynamics of Facial Behavior for Mental Health Assessment. (arXiv:2108.09934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09934</id>
        <link href="http://arxiv.org/abs/2108.09934"/>
        <updated>2021-08-24T01:40:29.142Z</updated>
        <summary type="html"><![CDATA[Facial action unit (FAU) intensities are popular descriptors for the analysis
of facial behavior. However, FAUs are sparsely represented when only a few are
activated at a time. In this study, we explore the possibility of representing
the dynamics of facial expressions by adopting algorithms used for word
representation in natural language processing. Specifically, we perform
clustering on a large dataset of temporal facial expressions with 5.3M frames
before applying the Global Vector representation (GloVe) algorithm to learn the
embeddings of the facial clusters. We evaluate the usefulness of our learned
representations on two downstream tasks: schizophrenia symptom estimation and
depression severity regression. These experimental results show the potential
effectiveness of our approach for improving the assessment of mental health
symptoms over baseline models that use FAU intensities alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_E/0/1/0/all/0/1"&gt;Ellen Bradley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matvey_M/0/1/0/all/0/1"&gt;Michelle Matvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woolley_J/0/1/0/all/0/1"&gt;Joshua Woolley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1"&gt;Mohammad Soleymani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Neural Fusion for Full-frame Video Stabilization. (arXiv:2102.06205v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06205</id>
        <link href="http://arxiv.org/abs/2102.06205"/>
        <updated>2021-08-24T01:40:29.137Z</updated>
        <summary type="html"><![CDATA[Existing video stabilization methods often generate visible distortion or
require aggressive cropping of frame boundaries, resulting in smaller field of
views. In this work, we present a frame synthesis algorithm to achieve
full-frame video stabilization. We first estimate dense warp fields from
neighboring frames and then synthesize the stabilized frame by fusing the
warped contents. Our core technical novelty lies in the learning-based
hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy
and fast-moving objects. We validate the effectiveness of our method on the
NUS, selfie, and DeepStab video datasets. Extensive experiment results
demonstrate the merits of our approach over prior video stabilization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Lun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1"&gt;Wei-Sheng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yung-Yu Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching. (arXiv:2108.09770v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09770</id>
        <link href="http://arxiv.org/abs/2108.09770"/>
        <updated>2021-08-24T01:40:29.131Z</updated>
        <summary type="html"><![CDATA[Recent methods in stereo matching have continuously improved the accuracy
using deep models. This gain, however, is attained with a high increase in
computation cost, such that the network may not fit even on a moderate GPU.
This issue raises problems when the model needs to be deployed on
resource-limited devices. For this, we propose two light models for stereo
vision with reduced complexity and without sacrificing accuracy. Depending on
the dimension of cost volume, we design a 2D and a 3D model with
encoder-decoders built from 2D and 3D convolutions, respectively. To this end,
we leverage 2D MobileNet blocks and extend them to 3D for stereo vision
application. Besides, a new cost volume is proposed to boost the accuracy of
the 2D model, making it performing close to 3D networks. Experiments show that
the proposed 2D/3D networks effectively reduce the computational expense
(27%/95% and 72%/38% fewer parameters/operations in 2D and 3D models,
respectively) while upholding the accuracy. Our code is available at
https://github.com/cogsys-tuebingen/mobilestereonet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shamsafar_F/0/1/0/all/0/1"&gt;Faranak Shamsafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woerz_S/0/1/0/all/0/1"&gt;Samuel Woerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahim_R/0/1/0/all/0/1"&gt;Rafia Rahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition. (arXiv:2102.01063v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01063</id>
        <link href="http://arxiv.org/abs/2102.01063"/>
        <updated>2021-08-24T01:40:29.117Z</updated>
        <summary type="html"><![CDATA[Accuracy predictor is a key component in Neural Architecture Search (NAS) for
ranking architectures. Building a high-quality accuracy predictor usually costs
enormous computation. To address this issue, instead of using an accuracy
predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the
architectures. The Zen-Score represents the network expressivity and positively
correlates with the model accuracy. The calculation of Zen-Score only takes a
few forward inferences through a randomly initialized network, without training
network parameters. Built upon the Zen-Score, we further propose a new NAS
algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network
under given inference budgets. Within less than half GPU day, Zen-NAS is able
to directly search high performance architectures in a data-free style.
Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times
faster on multiple server-side and mobile-side GPU platforms with
state-of-the-art accuracy on ImageNet. Our source code and pre-trained models
are released on https://github.com/idstcv/ZenNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Ming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pichao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenhong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hesen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1"&gt;Qi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZS-SLR: Zero-Shot Sign Language Recognition from RGB-D Videos. (arXiv:2108.10059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10059</id>
        <link href="http://arxiv.org/abs/2108.10059"/>
        <updated>2021-08-24T01:40:29.110Z</updated>
        <summary type="html"><![CDATA[Sign Language Recognition (SLR) is a challenging research area in computer
vision. To tackle the annotation bottleneck in SLR, we formulate the problem of
Zero-Shot Sign Language Recognition (ZS-SLR) and propose a two-stream model
from two input modalities: RGB and Depth videos. To benefit from the vision
Transformer capabilities, we use two vision Transformer models, for human
detection and visual features representation. We configure a transformer
encoder-decoder architecture, as a fast and accurate human detection model, to
overcome the challenges of the current human detection models. Considering the
human keypoints, the detected human body is segmented into nine parts. A
spatio-temporal representation from human body is obtained using a vision
Transformer and a LSTM network. A semantic space maps the visual features to
the lingual embedding of the class labels via a Bidirectional Encoder
Representations from Transformers (BERT) model. We evaluated the proposed model
on four datasets, Montalbano II, MSR Daily Activity 3D, CAD-60, and NTU-60,
obtaining state-of-the-art results compared to state-of-the-art ZS-SLR models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1"&gt;Razieh Rastgoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1"&gt;Kourosh Kiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1"&gt;Sergio Escalera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Better Segment Objects from Unseen Classes with Unlabeled Videos. (arXiv:2104.12276v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12276</id>
        <link href="http://arxiv.org/abs/2104.12276"/>
        <updated>2021-08-24T01:40:29.104Z</updated>
        <summary type="html"><![CDATA[The ability to localize and segment objects from unseen classes would open
the door to new applications, such as autonomous object learning in active
vision. Nonetheless, improving the performance on unseen classes requires
additional training data, while manually annotating the objects of the unseen
classes can be labor-extensive and expensive. In this paper, we explore the use
of unlabeled video sequences to automatically generate training data for
objects of unseen classes. It is in principle possible to apply existing video
segmentation methods to unlabeled videos and automatically obtain object masks,
which can then be used as a training set even for classes with no manual labels
available. However, our experiments show that these methods do not perform well
enough for this purpose. We therefore introduce a Bayesian method that is
specifically designed to automatically create such a training set: Our method
starts from a set of object proposals and relies on (non-realistic)
analysis-by-synthesis to select the correct ones by performing an efficient
optimization over all the frames simultaneously. Through extensive experiments,
we show that our method can generate a high-quality training set which
significantly boosts the performance of segmenting objects of unseen classes.
We thus believe that our method could open the door for open-world instance
segmentation using abundant Internet videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuming Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1"&gt;Vincent Lepetit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Photo-Sketch Recognition Using Bidirectional Collaborative Synthesis Network. (arXiv:2108.09898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09898</id>
        <link href="http://arxiv.org/abs/2108.09898"/>
        <updated>2021-08-24T01:40:29.084Z</updated>
        <summary type="html"><![CDATA[This research features a deep-learning based framework to address the problem
of matching a given face sketch image against a face photo database. The
problem of photo-sketch matching is challenging because 1) there is large
modality gap between photo and sketch, and 2) the number of paired training
samples is insufficient to train deep learning based networks. To circumvent
the problem of large modality gap, our approach is to use an intermediate
latent space between the two modalities. We effectively align the distributions
of the two modalities in this latent space by employing a bidirectional (photo
-> sketch and sketch -> photo) collaborative synthesis network. A StyleGAN-like
architecture is utilized to make the intermediate latent space be equipped with
rich representation power. To resolve the problem of insufficient training
samples, we introduce a three-step training scheme. Extensive evaluation on
public composite face sketch database confirms superior performance of our
method compared to existing state-of-the-art methods. The proposed methodology
can be employed in matching other modality pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Seho Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Din_N/0/1/0/all/0/1"&gt;Nizam Ud Din&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunkyu Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Juneho Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIMAT: Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multi-modal Data. (arXiv:2104.03642v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03642</id>
        <link href="http://arxiv.org/abs/2104.03642"/>
        <updated>2021-08-24T01:40:29.079Z</updated>
        <summary type="html"><![CDATA[In medical applications, deep learning methods are built to automate
diagnostic tasks, often formulated as single-target classification problems.
However, a clinically relevant question that practitioners usually face, is how
to predict the future trajectory of a disease (prognosis). Current methods for
such a problem often require domain knowledge, and are complicated to apply. In
this paper, we formulate the prognosis prediction problem as a one-to-many
forecasting problem. Inspired by a clinical decision-making process with two
agents -- a radiologist and a general practitioner, we model a prognosis
prediction problem with two transformer-based components that share information
between each other. The first transformer in this model aims to analyze the
imaging data, and the second one leverages its internal states as inputs, also
fusing them with auxiliary patient data. We show the effectiveness of our
method in predicting the development of structural knee osteoarthritis changes,
and forecasting Alzheimer's disease clinical status. Our results show that the
proposed method outperforms the state-of-the-art baselines in terms of various
performance metrics, including calibration, which is desired from a medical
decision support system. An open source implementation of our method is made
publicly available at https://github.com/MIPT-Oulu/CLIMAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Huy Hoang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saarakkala_S/0/1/0/all/0/1"&gt;Simo Saarakkala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1"&gt;Matthew B. Blaschko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiulpin_A/0/1/0/all/0/1"&gt;Aleksei Tiulpin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEDI: Few-shot learning based on Earth Mover's Distance algorithm combined with deep residual network to identify diabetic retinopathy. (arXiv:2108.09711v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09711</id>
        <link href="http://arxiv.org/abs/2108.09711"/>
        <updated>2021-08-24T01:40:29.064Z</updated>
        <summary type="html"><![CDATA[Diabetic retinopathy(DR) is the main cause of blindness in diabetic patients.
However, DR can easily delay the occurrence of blindness through the diagnosis
of the fundus. In view of the reality, it is difficult to collect a large
amount of diabetic retina data in clinical practice. This paper proposes a
few-shot learning model of a deep residual network based on Earth Mover's
Distance algorithm to assist in diagnosing DR. We build training and validation
classification tasks for few-shot learning based on 39 categories of 1000
sample data, train deep residual networks, and obtain experience maximization
pre-training models. Based on the weights of the pre-trained model, the Earth
Mover's Distance algorithm calculates the distance between the images, obtains
the similarity between the images, and changes the model's parameters to
improve the accuracy of the training model. Finally, the experimental
construction of the small sample classification task of the test set to
optimize the model further, and finally, an accuracy of 93.5667% on the
3way10shot task of the diabetic retina test set. For the experimental code and
results, please refer to:
https://github.com/panliangrui/few-shot-learning-funds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1"&gt;Liangrui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_B/0/1/0/all/0/1"&gt;Boya Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1"&gt;Peng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chongcheawchamnan_M/0/1/0/all/0/1"&gt;Mitchai Chongcheawchamnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaoliang Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Relational Context for Multi-Task Dense Prediction. (arXiv:2104.13874v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13874</id>
        <link href="http://arxiv.org/abs/2104.13874"/>
        <updated>2021-08-24T01:40:29.059Z</updated>
        <summary type="html"><![CDATA[The timeline of computer vision research is marked with advances in learning
and utilizing efficient contextual representations. Most of them, however, are
targeted at improving model performance on a single downstream task. We
consider a multi-task environment for dense prediction tasks, represented by a
common backbone and independent task-specific heads. Our goal is to find the
most efficient way to refine each task prediction by capturing cross-task
contexts dependent on tasks' relations. We explore various attention-based
contexts, such as global and local, in the multi-task setting and analyze their
behavior when applied to refine each task independently. Empirical findings
confirm that different source-target task pairs benefit from different context
types. To automate the selection process, we propose an Adaptive
Task-Relational Context (ATRC) module, which samples the pool of all available
contexts for each task pair using neural architecture search and outputs the
optimal configuration for deployment. Our method achieves state-of-the-art
performance on two important multi-task benchmarks, namely NYUD-v2 and
PASCAL-Context. The proposed ATRC has a low computational toll and can be used
as a drop-in refinement module for any supervised multi-task architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1"&gt;David Bruggemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1"&gt;Menelaos Kanakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In-Place Scene Labelling and Understanding with Implicit Scene Representation. (arXiv:2103.15875v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15875</id>
        <link href="http://arxiv.org/abs/2103.15875"/>
        <updated>2021-08-24T01:40:29.047Z</updated>
        <summary type="html"><![CDATA[Semantic labelling is highly correlated with geometry and radiance
reconstruction, as scene entities with similar shape and appearance are more
likely to come from similar classes. Recent implicit neural reconstruction
techniques are appealing as they do not require prior training data, but the
same fully self-supervised approach is not possible for semantics because
labels are human-defined properties.

We extend neural radiance fields (NeRF) to jointly encode semantics with
appearance and geometry, so that complete and accurate 2D semantic labels can
be achieved using a small amount of in-place annotations specific to the scene.
The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
by enabling sparse labels to efficiently propagate. We show the benefit of this
approach when labels are either sparse or very noisy in room-scale scenes. We
demonstrate its advantageous properties in various interesting applications
such as an efficient scene labelling tool, novel semantic view synthesis, label
denoising, super-resolution, label interpolation and multi-view semantic label
fusion in visual semantic mapping systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_S/0/1/0/all/0/1"&gt;Shuaifeng Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1"&gt;Tristan Laidlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1"&gt;Stefan Leutenegger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1"&gt;Andrew J. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Collaborative Visual SLAM Framework for Service Robots. (arXiv:2102.03228v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03228</id>
        <link href="http://arxiv.org/abs/2102.03228"/>
        <updated>2021-08-24T01:40:29.033Z</updated>
        <summary type="html"><![CDATA[We present a collaborative visual simultaneous localization and mapping
(SLAM) framework for service robots. With an edge server maintaining a map
database and performing global optimization, each robot can register to an
existing map, update the map, or build new maps, all with a unified interface
and low computation and memory cost. We design an elegant communication
pipeline to enable real-time information sharing between robots. With a novel
landmark organization and retrieval method on the server, each robot can
acquire landmarks predicted to be in its view, to augment its local map. The
framework is general enough to support both RGB-D and monocular cameras, as
well as robots with multiple cameras, taking the rigid constraints between
cameras into consideration. The proposed framework has been fully implemented
and verified with public datasets and live experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_M/0/1/0/all/0/1"&gt;Ming Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xuesong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuxin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yingzhe Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dawei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhiqiang Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FVC: A New Framework towards Deep Video Compression in Feature Space. (arXiv:2105.09600v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09600</id>
        <link href="http://arxiv.org/abs/2105.09600"/>
        <updated>2021-08-24T01:40:28.968Z</updated>
        <summary type="html"><![CDATA[Learning based video compression attracts increasing attention in the past
few years. The previous hybrid coding approaches rely on pixel space operations
to reduce spatial and temporal redundancy, which may suffer from inaccurate
motion estimation or less effective motion compensation. In this work, we
propose a feature-space video coding network (FVC) by performing all major
operations (i.e., motion estimation, motion compression, motion compensation
and residual compression) in the feature space. Specifically, in the proposed
deformable compensation module, we first apply motion estimation in the feature
space to produce motion information (i.e., the offset maps), which will be
compressed by using the auto-encoder style network. Then we perform motion
compensation by using deformable convolution and generate the predicted
feature. After that, we compress the residual feature between the feature from
the current frame and the predicted feature from our deformable compensation
module. For better frame reconstruction, the reference features from multiple
previous reconstructed frames are also fused by using the non-local attention
mechanism in the multi-frame feature fusion module. Comprehensive experimental
results demonstrate that the proposed framework achieves the state-of-the-art
performance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhihao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lucas-Kanade Reloaded: End-to-End Super-Resolution from Raw Image Bursts. (arXiv:2104.06191v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06191</id>
        <link href="http://arxiv.org/abs/2104.06191"/>
        <updated>2021-08-24T01:40:28.962Z</updated>
        <summary type="html"><![CDATA[This presentation addresses the problem of reconstructing a high-resolution
image from multiple lower-resolution snapshots captured from slightly different
viewpoints in space and time. Key challenges for solving this problem include
(i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw
(noisy) images for maximal faithfulness to native camera data, and (iii)
designing/learning an image prior (regularizer) well suited to the task. We
address these three challenges with a hybrid algorithm building on the insight
from Wronski et al. that aliasing is an ally in this setting, with parameters
that can be learned end to end, while retaining the interpretability of
classical approaches to inverse problems. The effectiveness of our approach is
demonstrated on synthetic and real image bursts, setting a new state of the art
on several benchmarks and delivering excellent qualitative results on real raw
bursts captured by smartphones and prosumer cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lecouat_B/0/1/0/all/0/1"&gt;Bruno Lecouat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification. (arXiv:2103.14899v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14899</id>
        <link href="http://arxiv.org/abs/2103.14899"/>
        <updated>2021-08-24T01:40:28.954Z</updated>
        <summary type="html"><![CDATA[The recently developed vision transformer (ViT) has achieved promising
results on image classification compared to convolutional neural networks.
Inspired by this, in this paper, we study how to learn multi-scale feature
representations in transformer models for image classification. To this end, we
propose a dual-branch transformer to combine image patches (i.e., tokens in a
transformer) of different sizes to produce stronger image features. Our
approach processes small-patch and large-patch tokens with two separate
branches of different computational complexity and these tokens are then fused
purely by attention multiple times to complement each other. Furthermore, to
reduce computation, we develop a simple yet effective token fusion module based
on cross attention, which uses a single token for each branch as a query to
exchange information with other branches. Our proposed cross-attention only
requires linear time for both computational and memory complexity instead of
quadratic time otherwise. Extensive experiments demonstrate that our approach
performs better than or on par with several concurrent works on vision
transformer, in addition to efficient CNN models. For example, on the
ImageNet1K dataset, with some architectural changes, our approach outperforms
the recent DeiT by a large margin of 2\% with a small to moderate increase in
FLOPs and model parameters. Our source codes and models are available at
\url{https://github.com/IBM/CrossViT}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Quanfu Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$S^3$: Learnable Sparse Signal Superdensity for Guided Depth Estimation. (arXiv:2103.02396v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02396</id>
        <link href="http://arxiv.org/abs/2103.02396"/>
        <updated>2021-08-24T01:40:28.922Z</updated>
        <summary type="html"><![CDATA[Dense depth estimation plays a key role in multiple applications such as
robotics, 3D reconstruction, and augmented reality. While sparse signal, e.g.,
LiDAR and Radar, has been leveraged as guidance for enhancing dense depth
estimation, the improvement is limited due to its low density and imbalanced
distribution. To maximize the utility from the sparse source, we propose $S^3$
technique, which expands the depth value from sparse cues while estimating the
confidence of expanded region. The proposed $S^3$ can be applied to various
guided depth estimation approaches and trained end-to-end at different stages,
including input, cost volume and output. Extensive experiments demonstrate the
effectiveness, robustness, and flexibility of the $S^3$ technique on LiDAR and
Radar signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tsung-Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsou_T/0/1/0/all/0/1"&gt;Tsung-Lin Tsou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-An Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning of Visual Relations: The Devil is in the Tails. (arXiv:2108.09668v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09668</id>
        <link href="http://arxiv.org/abs/2108.09668"/>
        <updated>2021-08-24T01:40:28.906Z</updated>
        <summary type="html"><![CDATA[Significant effort has been recently devoted to modeling visual relations.
This has mostly addressed the design of architectures, typically by adding
parameters and increasing model complexity. However, visual relation learning
is a long-tailed problem, due to the combinatorial nature of joint reasoning
about groups of objects. Increasing model complexity is, in general, ill-suited
for long-tailed problems due to their tendency to overfit. In this paper, we
explore an alternative hypothesis, denoted the Devil is in the Tails. Under
this hypothesis, better performance is achieved by keeping the model simple but
improving its ability to cope with long-tailed distributions. To test this
hypothesis, we devise a new approach for training visual relationships models,
which is inspired by state-of-the-art long-tailed recognition literature. This
is based on an iterative decoupled training scheme, denoted Decoupled Training
for Devil in the Tails (DT2). DT2 employs a novel sampling approach,
Alternating Class-Balanced Sampling (ACBS), to capture the interplay between
the long-tailed entity and predicate distributions of visual relations. Results
show that, with an extremely simple architecture, DT2-ACBS significantly
outperforms much more complex state-of-the-art methods on scene graph
generation tasks. This suggests that the development of sophisticated models
must be considered in tandem with the long-tailed nature of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1"&gt;Alakh Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tz-Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Subarna Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1"&gt;Nuno Vasconcelos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn-Explain-Reinforce: Counterfactual Reasoning and Its Guidance to Reinforce an Alzheimer's Disease Diagnosis Model. (arXiv:2108.09451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09451</id>
        <link href="http://arxiv.org/abs/2108.09451"/>
        <updated>2021-08-24T01:40:28.756Z</updated>
        <summary type="html"><![CDATA[Existing studies on disease diagnostic models focus either on diagnostic
model learning for performance improvement or on the visual explanation of a
trained diagnostic model. We propose a novel learn-explain-reinforce (LEAR)
framework that unifies diagnostic model learning, visual explanation generation
(explanation unit), and trained diagnostic model reinforcement (reinforcement
unit) guided by the visual explanation. For the visual explanation, we generate
a counterfactual map that transforms an input sample to be identified as an
intended target label. For example, a counterfactual map can localize
hypothetical abnormalities within a normal brain image that may cause it to be
diagnosed with Alzheimer's disease (AD). We believe that the generated
counterfactual maps represent data-driven and model-induced knowledge about a
target task, i.e., AD diagnosis using structural MRI, which can be a vital
source of information to reinforce the generalization of the trained diagnostic
model. To this end, we devise an attention-based feature refinement module with
the guidance of the counterfactual maps. The explanation and reinforcement
units are reciprocal and can be operated iteratively. Our proposed approach was
validated via qualitative and quantitative analysis on the ADNI dataset. Its
comprehensibility and fidelity were demonstrated through ablation studies and
comparisons with existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1"&gt;Kwanseok Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jee Seok Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suk_H/0/1/0/all/0/1"&gt;Heung-Il Suk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integer-arithmetic-only Certified Robustness for Quantized Neural Networks. (arXiv:2108.09413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09413</id>
        <link href="http://arxiv.org/abs/2108.09413"/>
        <updated>2021-08-24T01:40:28.741Z</updated>
        <summary type="html"><![CDATA[Adversarial data examples have drawn significant attention from the machine
learning and security communities. A line of work on tackling adversarial
examples is certified robustness via randomized smoothing that can provide a
theoretical robustness guarantee. However, such a mechanism usually uses
floating-point arithmetic for calculations in inference and requires large
memory footprints and daunting computational costs. These defensive models
cannot run efficiently on edge devices nor be deployed on integer-only logical
units such as Turing Tensor Cores or integer-only ARM processors. To overcome
these challenges, we propose an integer randomized smoothing approach with
quantization to convert any classifier into a new smoothed classifier, which
uses integer-only arithmetic for certified robustness against adversarial
perturbations. We prove a tight robustness guarantee under L2-norm for the
proposed approach. We show our approach can obtain a comparable accuracy and
4x~5x speedup over floating-point arithmetic certified robust methods on
general-purpose CPUs and mobile devices on two distinct datasets (CIFAR-10 and
Caltech-101).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haowen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahabi_C/0/1/0/all/0/1"&gt;Cyrus Shahabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Survey on Graph Anomaly Detection with Deep Learning. (arXiv:2106.07178v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07178</id>
        <link href="http://arxiv.org/abs/2106.07178"/>
        <updated>2021-08-24T01:40:28.735Z</updated>
        <summary type="html"><![CDATA[Anomalies represent rare observations (e.g., data records or events) that are
deviating significantly from others. Over the last forty years, researches on
anomalies have received great interests because of their significance in many
disciplines (e.g., computer science, chemistry, and biology). Anomaly
detection, which aims to identify these rare observations, is among the most
vital tasks and has shown its power in preventing detrimental events, such as
financial fraud and network intrusion, from happening. The detection task is
typically solved by detecting outlying data points in the features space and
inherently overlooks the structural information in real-world data. Graphs have
been prevalently used to preserve the structural information, and this raises
the graph anomaly detection problem - identifying anomalous graph objects
(i.e., nodes, edges and sub-graphs). However, conventional anomaly detection
techniques cannot well solve this problem because of the complexity of graph
data (e.g., irregular structures, non-independent and large-scale). For the
aptitudes of deep learning in breaking these limitations, graph anomaly
detection with deep learning has received intensified studies recently. In this
survey, we aim to provide a systematic and comprehensive review of the
contemporary deep learning techniques for graph anomaly detection.
Specifically, our categorization follows a task-driven strategy and classifies
existing works according to the anomalous graph objects they can detect. We
especially focus on the motivations, key intuitions and technical details of
existing works. We also summarize open-sourced implementations, public
datasets, and commonly-used evaluation metrics for future studies. Finally, we
highlight twelve future research directions according to our survey results
covering emerging problems introduced by graph data, anomaly detection and real
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaoxiao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1"&gt;Shan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Quan Z. Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1"&gt;Leman Akoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multiple-View Geometric Model for Specularity Prediction on Non-Uniformly Curved Surfaces. (arXiv:2108.09378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09378</id>
        <link href="http://arxiv.org/abs/2108.09378"/>
        <updated>2021-08-24T01:40:28.729Z</updated>
        <summary type="html"><![CDATA[Specularity prediction is essential to many computer vision applications by
giving important visual cues that could be used in Augmented Reality (AR),
Simultaneous Localisation and Mapping (SLAM), 3D reconstruction and material
modeling, thus improving scene understanding. However, it is a challenging task
requiring numerous information from the scene including the camera pose, the
geometry of the scene, the light sources and the material properties. Our
previous work have addressed this task by creating an explicit model using an
ellipsoid whose projection fits the specularity image contours for a given
camera pose. These ellipsoid-based approaches belong to a family of models
called JOint-LIght MAterial Specularity (JOLIMAS), where we have attempted to
gradually remove assumptions on the scene such as the geometry of the specular
surfaces. However, our most recent approach is still limited to uniformly
curved surfaces. This paper builds upon these methods by generalising JOLIMAS
to any surface geometry while improving the quality of specularity prediction,
without sacrificing computation performances. The proposed method establishes a
link between surface curvature and specularity shape in order to lift the
geometric assumptions from previous work. Contrary to previous work, our new
model is built from a physics-based local illumination model namely
Torrance-Sparrow, providing a better model reconstruction. Specularity
prediction using our new model is tested against the most recent JOLIMAS
version on both synthetic and real sequences with objects of varying shape
curvatures. Our method outperforms previous approaches in specularity
prediction, including the real-time setup, as shown in the supplementary
material using videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morgand_A/0/1/0/all/0/1"&gt;Alexandre Morgand&lt;/a&gt; (1) &lt;a href="http://arxiv.org/find/cs/1/au:+Tamaazousti_M/0/1/0/all/0/1"&gt;Mohamed Tamaazousti&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Bartoli_A/0/1/0/all/0/1"&gt;Adrien Bartoli&lt;/a&gt; (3) ((1) SLAMcore ltd, London, UK (2) Universit&amp;#xe9; Paris Saclay, CEA, LIST, Gif-sur-Yvette, France (3) IP-UMR 6602 - CNRS/UCA/CHU, Clermont-Ferrand, France)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping. (arXiv:2107.05341v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05341</id>
        <link href="http://arxiv.org/abs/2107.05341"/>
        <updated>2021-08-24T01:40:28.723Z</updated>
        <summary type="html"><![CDATA[We explore the ability of overparameterized shallow neural networks to learn
Lipschitz regression functions with and without label noise when trained by
Gradient Descent (GD). To avoid the problem that in the presence of noisy
labels, neural networks trained to nearly zero training error are inconsistent
on this class, we propose an early stopping rule that allows us to show optimal
rates. This provides an alternative to the result of Hu et al. (2021) who
studied the performance of $\ell 2$ -regularized GD for training shallow
networks in nonparametric regression which fully relied on the infinite-width
network (Neural Tangent Kernel (NTK)) approximation. Here we present a simpler
analysis which is based on a partitioning argument of the input space (as in
the case of 1-nearest-neighbor rule) coupled with the fact that trained neural
networks are smooth with respect to their inputs when trained by GD. In the
noise-free case the proof does not rely on any kernelization and can be
regarded as a finite-width result. In the case of label noise, by slightly
modifying the proof, the noise is controlled using a technique of Yao, Rosasco,
and Caponnetto (2007).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1"&gt;Ilja Kuzborskij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alignment Knowledge Distillation for Online Streaming Attention-based Speech Recognition. (arXiv:2103.00422v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00422</id>
        <link href="http://arxiv.org/abs/2103.00422"/>
        <updated>2021-08-24T01:40:28.709Z</updated>
        <summary type="html"><![CDATA[This article describes an efficient training method for online streaming
attention-based encoder-decoder (AED) automatic speech recognition (ASR)
systems. AED models have achieved competitive performance in offline scenarios
by jointly optimizing all components. They have recently been extended to an
online streaming framework via models such as monotonic chunkwise attention
(MoChA). However, the elaborate attention calculation process is not robust for
long-form speech utterances. Moreover, the sequence-level training objective
and time-restricted streaming encoder cause a nonnegligible delay in token
emission during inference. To address these problems, we propose CTC
synchronous training (CTC-ST), in which CTC alignments are leveraged as a
reference for token boundaries to enable a MoChA model to learn optimal
monotonic input-output alignments. We formulate a purely end-to-end training
objective to synchronize the boundaries of MoChA to those of CTC. The CTC model
shares an encoder with the MoChA model to enhance the encoder representation.
Moreover, the proposed method provides alignment information learned in the CTC
branch to the attention-based decoder. Therefore, CTC-ST can be regarded as
self-distillation of alignment knowledge from CTC to MoChA. Experimental
evaluations on a variety of benchmark datasets show that the proposed method
significantly reduces recognition errors and emission latency simultaneously.
The robustness to long-form and noisy speech is also demonstrated. We compare
CTC-ST with several methods that distill alignment knowledge from a hybrid ASR
system and show that the CTC-ST can achieve a comparable tradeoff of accuracy
and latency without relying on external alignment information. The best MoChA
system shows recognition accuracy comparable to that of RNN-transducer (RNN-T)
while achieving lower emission latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1"&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorize, Factorize, or be Na\"ive: Learning Optimal Feature Interaction Methods for CTR Prediction. (arXiv:2108.01265v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01265</id>
        <link href="http://arxiv.org/abs/2108.01265"/>
        <updated>2021-08-24T01:40:28.702Z</updated>
        <summary type="html"><![CDATA[Click-through rate prediction is one of the core tasks in commercial
recommender systems. It aims to predict the probability of a user clicking a
particular item given user and item features. As feature interactions bring in
non-linearity, they are widely adopted to improve the performance of CTR
prediction models. Therefore, effectively modelling feature interactions has
attracted much attention in both the research and industry field. The current
approaches can generally be categorized into three classes: (1) na\"ive
methods, which do not model feature interactions and only use original
features; (2) memorized methods, which memorize feature interactions by
explicitly viewing them as new features and assigning trainable embeddings; (3)
factorized methods, which learn latent vectors for original features and
implicitly model feature interactions through factorization functions. Studies
have shown that modelling feature interactions by one of these methods alone
are suboptimal due to the unique characteristics of different feature
interactions. To address this issue, we first propose a general framework
called OptInter which finds the most suitable modelling method for each feature
interaction. Different state-of-the-art deep CTR models can be viewed as
instances of OptInter. To realize the functionality of OptInter, we also
introduce a learning algorithm that automatically searches for the optimal
modelling method. We conduct extensive experiments on four large datasets. Our
experiments show that OptInter improves the best performed state-of-the-art
baseline deep CTR models by up to 2.21%. Compared to the memorized method,
which also outperforms baselines, we reduce up to 91% parameters. In addition,
we conduct several ablation studies to investigate the influence of different
components of OptInter. Finally, we provide interpretable discussions on the
results of OptInter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1"&gt;Fuyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xue Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-Aware Query Auto-completion using Extreme Multi-label Ranking. (arXiv:2012.07654v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07654</id>
        <link href="http://arxiv.org/abs/2012.07654"/>
        <updated>2021-08-24T01:40:28.696Z</updated>
        <summary type="html"><![CDATA[Query auto-completion (QAC) is a fundamental feature in search engines where
the task is to suggest plausible completions of a prefix typed in the search
bar. Previous queries in the user session can provide useful context for the
user's intent and can be leveraged to suggest auto-completions that are more
relevant while adhering to the user's prefix. Such session-aware QACs can be
generated by recent sequence-to-sequence deep learning models; however, these
generative approaches often do not meet the stringent latency requirements of
responding to each user keystroke. Moreover, these generative approaches pose
the risk of showing nonsensical queries.

In this paper, we provide a solution to this problem: we take the novel
approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)
problem where the input is the previous query in the session and the user's
current prefix, while the output space is the set of tens of millions of
queries entered by users in the recent past. We adapt a popular XMR algorithm
for this purpose by proposing several modifications to the key steps in the
algorithm. The proposed modifications yield a 3.9x improvement in terms of Mean
Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs
dataset. We are able to maintain an inference latency of less than 10 ms while
still using session context. When compared against baseline models of
acceptable latency, we observed a 33% improvement in MRR for short prefixes of
up to 3 characters. Moreover, our model yielded a statistically significant
improvement of 2.81% over a production QAC system in terms of suggestion
acceptance rate, when deployed on the search bar of an online shopping store as
part of an A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nishant Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Daniel N. Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02180</id>
        <link href="http://arxiv.org/abs/2108.02180"/>
        <updated>2021-08-24T01:40:28.690Z</updated>
        <summary type="html"><![CDATA[We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1"&gt;Tom Braude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08248</id>
        <link href="http://arxiv.org/abs/2102.08248"/>
        <updated>2021-08-24T01:40:28.685Z</updated>
        <summary type="html"><![CDATA[Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1"&gt;Jakob D. Havtorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1"&gt;Lars Maal&amp;#xf8;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardness of Learning Halfspaces with Massart Noise. (arXiv:2012.09720v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09720</id>
        <link href="http://arxiv.org/abs/2012.09720"/>
        <updated>2021-08-24T01:40:28.678Z</updated>
        <summary type="html"><![CDATA[We study the complexity of PAC learning halfspaces in the presence of Massart
(bounded) noise. Specifically, given labeled examples $(x, y)$ from a
distribution $D$ on $\mathbb{R}^{n} \times \{ \pm 1\}$ such that the marginal
distribution on $x$ is arbitrary and the labels are generated by an unknown
halfspace corrupted with Massart noise at rate $\eta<1/2$, we want to compute a
hypothesis with small misclassification error. Characterizing the efficient
learnability of halfspaces in the Massart model has remained a longstanding
open problem in learning theory.

Recent work gave a polynomial-time learning algorithm for this problem with
error $\eta+\epsilon$. This error upper bound can be far from the
information-theoretically optimal bound of $\mathrm{OPT}+\epsilon$. More recent
work showed that {\em exact learning}, i.e., achieving error
$\mathrm{OPT}+\epsilon$, is hard in the Statistical Query (SQ) model. In this
work, we show that there is an exponential gap between the
information-theoretically optimal error and the best error that can be achieved
by a polynomial-time SQ algorithm. In particular, our lower bound implies that
no efficient SQ algorithm can approximate the optimal error within any
polynomial factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14185</id>
        <link href="http://arxiv.org/abs/2107.14185"/>
        <updated>2021-08-24T01:40:28.663Z</updated>
        <summary type="html"><![CDATA[Transferability of adversarial examples is of central importance for
attacking an unknown model, which facilitates adversarial attacks in more
practical scenarios, e.g., black-box attacks. Existing transferable attacks
tend to craft adversarial examples by indiscriminately distorting features to
degrade prediction accuracy in a source model without aware of intrinsic
features of objects in the images. We argue that such brute-force degradation
would introduce model-specific local optimum into adversarial examples, thus
limiting the transferability. By contrast, we propose the Feature
Importance-aware Attack (FIA), which disrupts important object-aware features
that dominate model decisions consistently. More specifically, we obtain
feature importance by introducing the aggregate gradient, which averages the
gradients with respect to feature maps of the source model, computed on a batch
of random transforms of the original clean image. The gradients will be highly
correlated to objects of interest, and such correlation presents invariance
across different models. Besides, the random transforms will preserve intrinsic
features of objects and suppress model-specific information. Finally, the
feature importance guides to search for adversarial examples towards disrupting
critical features, achieving stronger transferability. Extensive experimental
evaluation demonstrates the effectiveness and superior performance of the
proposed FIA, i.e., improving the success rate by 9.5% against normally trained
models and 12.8% against defense models as compared to the state-of-the-art
transferable attacks. Code is available at: https://github.com/hcguoO0/FIA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhibo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengchang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhifei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhan Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech. (arXiv:2107.00712v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00712</id>
        <link href="http://arxiv.org/abs/2107.00712"/>
        <updated>2021-08-24T01:40:28.657Z</updated>
        <summary type="html"><![CDATA[People communicate using both speech and non-verbal signals such as gestures,
face expression or body pose. Non-verbal signals impact the meaning of the
spoken utterance in an abundance of ways. An absence of non-verbal signals
impoverishes the process of communication. Yet, when users are represented as
avatars, it is difficult to translate non-verbal signals along with the speech
into the virtual world without specialized motion-capture hardware. In this
paper, we propose a novel, data-driven technique for generating gestures
directly from speech. Our approach is based on the application of Generative
Adversarial Neural Networks (GANs) to model the correlation rather than
causation between speech and gestures. This approach approximates neuroscience
findings on how non-verbal communication and speech are correlated. We create a
large dataset which consists of speech and corresponding gestures in a 3D human
pose format from which our model learns the speaker-specific correlation. We
evaluate the proposed technique in a user study that is inspired by the Turing
test. For the study, we animate the generated gestures on a virtual character.
We find that users are not able to distinguish between the generated and the
recorded gestures. Moreover, users are able to identify our synthesized
gestures as related or not related to a given utterance. Code and videos are
available at https://github.com/mrebol/Gestures-From-Speech]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebol_M/0/1/0/all/0/1"&gt;Manuel Rebol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutl_C/0/1/0/all/0/1"&gt;Christian G&amp;#xfc;tl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietroszek_K/0/1/0/all/0/1"&gt;Krzysztof Pietroszek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform. (arXiv:2107.11627v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11627</id>
        <link href="http://arxiv.org/abs/2107.11627"/>
        <updated>2021-08-24T01:40:28.652Z</updated>
        <summary type="html"><![CDATA[Fast and accurate simulation of imaging through atmospheric turbulence is
essential for developing turbulence mitigation algorithms. Recognizing the
limitations of previous approaches, we introduce a new concept known as the
phase-to-space (P2S) transform to significantly speed up the simulation. P2S is
build upon three ideas: (1) reformulating the spatially varying convolution as
a set of invariant convolutions with basis functions, (2) learning the basis
function via the known turbulence statistics models, (3) implementing the P2S
transform via a light-weight network that directly convert the phase
representation to spatial representation. The new simulator offers 300x --
1000x speed up compared to the mainstream split-step simulators while
preserving the essential turbulence statistics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhiyuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1"&gt;Nicholas Chimitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection. (arXiv:2108.07135v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07135</id>
        <link href="http://arxiv.org/abs/2108.07135"/>
        <updated>2021-08-24T01:40:28.638Z</updated>
        <summary type="html"><![CDATA[Vehicle counting systems can help with vehicle analysis and traffic incident
detection. Unfortunately, most existing methods require some level of human
input to identify the Region of interest (ROI), movements of interest, or to
establish a reference point or line to count vehicles from traffic cameras.
This work introduces a method to count vehicles from traffic videos that
automatically identifies the ROI for the camera, as well as the driving
trajectories of the vehicles. This makes the method feasible to use with
Pan-Tilt-Zoom cameras, which are frequently used in developing countries.
Preliminary results indicate that the proposed method achieves an average
intersection over the union of 57.05% for the ROI and a mean absolute error of
just 17.44% at counting vehicles of the traffic video cameras tested.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1"&gt;Malolan Vasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreu_N/0/1/0/all/0/1"&gt;Nelson Abreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasquez_R/0/1/0/all/0/1"&gt;Raysa V&amp;#xe1;squez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1"&gt;Christian L&amp;#xf3;pez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding. (arXiv:2103.04850v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04850</id>
        <link href="http://arxiv.org/abs/2103.04850"/>
        <updated>2021-08-24T01:40:28.633Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning conditional average treatment effects (CATE)
from high-dimensional, observational data with unobserved confounders.
Unobserved confounders introduce ignorance -- a level of unidentifiability --
about an individual's response to treatment by inducing bias in CATE estimates.
We present a new parametric interval estimator suited for high-dimensional
data, that estimates a range of possible CATE values when given a predefined
bound on the level of hidden confounding. Further, previous interval estimators
do not account for ignorance about the CATE associated with samples that may be
underrepresented in the original study, or samples that violate the overlap
assumption. Our interval estimator also incorporates model uncertainty so that
practitioners can be made aware of out-of-distribution data. We prove that our
estimator converges to tight bounds on CATE when there may be unobserved
confounding, and assess it using semi-synthetic, high-dimensional datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jesson_A/0/1/0/all/0/1"&gt;Andrew Jesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural Network. (arXiv:2104.09248v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09248</id>
        <link href="http://arxiv.org/abs/2104.09248"/>
        <updated>2021-08-24T01:40:28.628Z</updated>
        <summary type="html"><![CDATA[Being capable of estimating the pose of uncooperative objects in space has
been proposed as a key asset for enabling safe close-proximity operations such
as space rendezvous, in-orbit servicing and active debris removal. Usual
approaches for pose estimation involve classical computer vision-based
solutions or the application of Deep Learning (DL) techniques. This work
explores a novel DL-based methodology, using Convolutional Neural Networks
(CNNs), for estimating the pose of uncooperative spacecrafts. Contrary to other
approaches, the proposed CNN directly regresses poses without needing any prior
3D information. Moreover, bounding boxes of the spacecraft in the image are
predicted in a simple, yet efficient manner. The performed experiments show how
this work competes with the state-of-the-art in uncooperative spacecraft pose
estimation, including works which require 3D information as well as works which
predict bounding boxes through sophisticated CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1"&gt;Albert Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musallam_M/0/1/0/all/0/1"&gt;Mohamed Adel Musallam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaudilliere_V/0/1/0/all/0/1"&gt;Vincent Gaudilliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghorbel_E/0/1/0/all/0/1"&gt;Enjie Ghorbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ismaeil_K/0/1/0/all/0/1"&gt;Kassem Al Ismaeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1"&gt;Marcos Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1"&gt;Djamila Aouada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Machine Learning using Real, Synthetic and Augmented Fire Tests to Predict Fire Resistance and Spalling of RC Columns. (arXiv:2108.09862v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09862</id>
        <link href="http://arxiv.org/abs/2108.09862"/>
        <updated>2021-08-24T01:40:28.618Z</updated>
        <summary type="html"><![CDATA[This paper presents the development of systematic machine learning (ML)
approach to enable explainable and rapid assessment of fire resistance and
fire-induced spalling of reinforced concrete (RC) columns. The developed
approach comprises of an ensemble of three novel ML algorithms namely; random
forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL).
These algorithms are trained to account for a wide collection of geometric
characteristics and material properties, as well as loading conditions to
examine fire performance of normal and high strength RC columns by analyzing a
comprehensive database of fire tests comprising of over 494 observations. The
developed ensemble is also capable of presenting quantifiable insights to ML
predictions; thus, breaking free from the notion of 'blackbox' ML and
establishing a solid step towards transparent and explainable ML. Most
importantly, this work tackles the scarcity of available fire tests by
proposing new techniques to leverage the use of real, synthetic and augmented
fire test observations. The developed ML ensemble has been calibrated and
validated for standard and design fire exposures and for one, two, three and
four-sided fire exposures thus; covering a wide range of practical scenarios
present during fire incidents. When fully deployed, the developed ensemble can
analyze over 5,000 RC columns in under 60 seconds thus, providing an attractive
solution for researchers and practitioners. The presented approach can also be
easily extended for evaluating fire resistance and spalling of other structural
members and under varying fire scenarios and loading conditions and hence paves
the way to modernize the state of this research area and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naser_M/0/1/0/all/0/1"&gt;M.Z. Naser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodur_V/0/1/0/all/0/1"&gt;V.K. Kodur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Geometry and Classical Cram\'{e}r-Rao Type Inequalities. (arXiv:2104.01061v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01061</id>
        <link href="http://arxiv.org/abs/2104.01061"/>
        <updated>2021-08-24T01:40:28.599Z</updated>
        <summary type="html"><![CDATA[We examine the role of information geometry in the context of classical
Cram\'er-Rao (CR) type inequalities. In particular, we focus on Eguchi's theory
of obtaining dualistic geometric structures from a divergence function and then
applying Amari-Nagoaka's theory to obtain a CR type inequality. The classical
deterministic CR inequality is derived from Kullback-Leibler (KL)-divergence.
We show that this framework could be generalized to other CR type inequalities
through four examples: $\alpha$-version of CR inequality, generalized CR
inequality, Bayesian CR inequality, and Bayesian $\alpha$-CR inequality. These
are obtained from, respectively, $I_\alpha$-divergence (or relative
$\alpha$-entropy), generalized Csisz\'ar divergence, Bayesian KL divergence,
and Bayesian $I_\alpha$-divergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_K/0/1/0/all/0/1"&gt;Kumar Vijay Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Ashok Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZeroSARAH: Efficient Nonconvex Finite-Sum Optimization with Zero Full Gradient Computation. (arXiv:2103.01447v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01447</id>
        <link href="http://arxiv.org/abs/2103.01447"/>
        <updated>2021-08-24T01:40:28.593Z</updated>
        <summary type="html"><![CDATA[We propose ZeroSARAH -- a novel variant of the variance-reduced method SARAH
(Nguyen et al., 2017) -- for minimizing the average of a large number of
nonconvex functions $\frac{1}{n}\sum_{i=1}^{n}f_i(x)$. To the best of our
knowledge, in this nonconvex finite-sum regime, all existing variance-reduced
methods, including SARAH, SVRG, SAGA and their variants, need to compute the
full gradient over all $n$ data samples at the initial point $x^0$, and then
periodically compute the full gradient once every few iterations (for SVRG,
SARAH and their variants). Note that SVRG, SAGA and their variants typically
achieve weaker convergence results than variants of SARAH: $n^{2/3}/\epsilon^2$
vs. $n^{1/2}/\epsilon^2$. Thus we focus on the variant of SARAH. The proposed
ZeroSARAH and its distributed variant D-ZeroSARAH are the \emph{first}
variance-reduced algorithms which \emph{do not require any full gradient
computations}, not even for the initial point. Moreover, for both standard and
distributed settings, we show that ZeroSARAH and D-ZeroSARAH obtain new
state-of-the-art convergence results, which can improve the previous best-known
result (given by e.g., SPIDER, SARAH, and PAGE) in certain regimes. Avoiding
any full gradient computations (which are time-consuming steps) is important in
many applications as the number of data samples $n$ usually is very large.
Especially in the distributed setting, periodic computation of full gradient
over all data samples needs to periodically synchronize all
clients/devices/machines, which may be impossible or unaffordable. Thus, we
expect that ZeroSARAH/D-ZeroSARAH will have a practical impact in distributed
and federated learning where full device participation is impractical.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhize Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanzely_S/0/1/0/all/0/1"&gt;Slavom&amp;#xed;r Hanzely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the approximation of functions by tanh neural networks. (arXiv:2104.08938v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08938</id>
        <link href="http://arxiv.org/abs/2104.08938"/>
        <updated>2021-08-24T01:40:28.587Z</updated>
        <summary type="html"><![CDATA[We derive bounds on the error, in high-order Sobolev norms, incurred in the
approximation of Sobolev-regular as well as analytic functions by neural
networks with the hyperbolic tangent activation function. These bounds provide
explicit estimates on the approximation error with respect to the size of the
neural networks. We show that tanh neural networks with only two hidden layers
suffice to approximate functions at comparable or better rates than much deeper
ReLU neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1"&gt;Tim De Ryck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lanthaler_S/0/1/0/all/0/1"&gt;Samuel Lanthaler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Siddhartha Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness Certification for Point Cloud Models. (arXiv:2103.16652v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16652</id>
        <link href="http://arxiv.org/abs/2103.16652"/>
        <updated>2021-08-24T01:40:28.582Z</updated>
        <summary type="html"><![CDATA[The use of deep 3D point cloud models in safety-critical applications, such
as autonomous driving, dictates the need to certify the robustness of these
models to real-world transformations. This is technically challenging, as it
requires a scalable verifier tailored to point cloud models that handles a wide
range of semantic 3D transformations. In this work, we address this challenge
and introduce 3DCertify, the first verifier able to certify the robustness of
point cloud models. 3DCertify is based on two key insights: (i) a generic
relaxation based on first-order Taylor approximations, applicable to any
differentiable transformation, and (ii) a precise relaxation for global feature
pooling, which is more complex than pointwise activations (e.g., ReLU or
sigmoid) but commonly employed in point cloud models. We demonstrate the
effectiveness of 3DCertify by performing an extensive evaluation on a wide
range of 3D transformations (e.g., rotation, twisting) for both classification
and part segmentation tasks. For example, we can certify robustness against
rotations by $\pm$60{\deg} for 95.7% of point clouds, and our max pool
relaxation increases certification by up to 15.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorenz_T/0/1/0/all/0/1"&gt;Tobias Lorenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1"&gt;Anian Ruoss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1"&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1"&gt;Martin Vechev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks. (arXiv:2006.04270v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04270</id>
        <link href="http://arxiv.org/abs/2006.04270"/>
        <updated>2021-08-24T01:40:28.562Z</updated>
        <summary type="html"><![CDATA[Dropout is a well-known regularization method by sampling a sub-network from
a larger deep neural network and training different sub-networks on different
subsets of the data. Inspired by the dropout concept, we propose EDropout as an
energy-based framework for pruning neural networks in classification tasks. In
this approach, a set of binary pruning state vectors (population) represents a
set of corresponding sub-networks from an arbitrary provided original neural
network. An energy loss function assigns a scalar energy loss value to each
pruning state. The energy-based model stochastically evolves the population to
find states with lower energy loss. The best pruning state is then selected and
applied to the original network. Similar to dropout, the kept weights are
updated using backpropagation in a probabilistic model. The energy-based model
again searches for better pruning states and the cycle continuous. Indeed, this
procedure is in fact switching between the energy model, which manages the
pruning states, and the probabilistic model, which updates the temporarily
unpruned weights, in each iteration. The population can dynamically converge to
a pruning state. This can be interpreted as dropout leading to pruning the
network. From an implementation perspective, EDropout can prune typical neural
networks without modification of the network architecture. We evaluated the
proposed method on different flavours of ResNets, AlexNet, and SqueezeNet on
the Kuzushiji, Fashion, CIFAR-10, CIFAR-100, and Flowers datasets, and compared
the pruning rate and classification performance of the models. On average the
networks trained with EDropout achieved a pruning rate of more than $50\%$ of
the trainable parameters with approximately $<5\%$ and $<1\%$ drop of Top-1 and
Top-5 classification accuracy, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1"&gt;Hojjat Salehinejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valaee_S/0/1/0/all/0/1"&gt;Shahrokh Valaee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection. (arXiv:2006.03184v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03184</id>
        <link href="http://arxiv.org/abs/2006.03184"/>
        <updated>2021-08-24T01:40:28.549Z</updated>
        <summary type="html"><![CDATA[Many recent studies have shown that deep neural models are vulnerable to
adversarial samples: images with imperceptible perturbations, for example, can
fool image classifiers. In this paper, we present the first type-specific
approach to generating adversarial examples for object detection, which entails
detecting bounding boxes around multiple objects present in the image and
classifying them at the same time, making it a harder task than against image
classification. We specifically aim to attack the widely used Faster R-CNN by
changing the predicted label for a particular object in an image: where prior
work has targeted one specific object (a stop sign), we generalise to arbitrary
objects, with the key challenge being the need to change the labels of all
bounding boxes for all instances of that object type. To do so, we propose a
novel method, named Pick-Object-Attack. Pick-Object-Attack successfully adds
perturbations only to bounding boxes for the targeted object, preserving the
labels of other detected objects in the image. In terms of perceptibility, the
perturbations induced by the method are very small. Furthermore, for the first
time, we examine the effect of adversarial attacks on object detection in terms
of a downstream task, image captioning; we show that where a method that can
modify all object types leads to very obvious changes in captions, the changes
from our constrained attack are much less apparent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nezami_O/0/1/0/all/0/1"&gt;Omid Mohamad Nezami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1"&gt;Akshay Chaturvedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1"&gt;Mark Dras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1"&gt;Utpal Garain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DTW-Merge: A Novel Data Augmentation Technique for Time Series Classification. (arXiv:2103.01119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01119</id>
        <link href="http://arxiv.org/abs/2103.01119"/>
        <updated>2021-08-24T01:40:28.532Z</updated>
        <summary type="html"><![CDATA[In recent years, neural networks achieved much success in various
applications. The main challenge in training deep neural networks is the lack
of sufficient data to improve the model's generalization and avoid overfitting.
One of the solutions is to generate new training samples. This paper proposes a
novel data augmentation method for time series based on Dynamic Time Warping.
This method is inspired by the concept that warped parts of two time series
have similar temporal properties and therefore, exchanging them between the two
series generates a new training sample. The proposed method selects an element
of the optimal warping path randomly and then exchanges the segments that are
aligned together. Exploiting the proposed approach with recently introduced
ResNet reveals improved results on the 2018 UCR Time Series Classification
Archive. By employing Gradient-weighted Class Activation Mapping (Grad-CAM) and
Multidimensional Scaling (MDS), we manifest that our method extract more
discriminant features out of time series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyash_M/0/1/0/all/0/1"&gt;Mohammad Akyash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1"&gt;Hoda Mohammadzade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1"&gt;Hamid Behroozi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unleashing the Tiger: Inference Attacks on Split Learning. (arXiv:2012.02670v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02670</id>
        <link href="http://arxiv.org/abs/2012.02670"/>
        <updated>2021-08-24T01:40:28.509Z</updated>
        <summary type="html"><![CDATA[We investigate the security of Split Learning -- a novel collaborative
machine learning framework that enables peak performance by requiring minimal
resources consumption. In the present paper, we expose vulnerabilities of the
protocol and demonstrate its inherent insecurity by introducing general attack
strategies targeting the reconstruction of clients' private training sets. More
prominently, we show that a malicious server can actively hijack the learning
process of the distributed model and bring it into an insecure state that
enables inference attacks on clients' data. We implement different adaptations
of the attack and test them on various datasets as well as within realistic
threat scenarios. We demonstrate that our attack is able to overcome recently
proposed defensive techniques aimed at enhancing the security of the split
learning protocol. Finally, we also illustrate the protocol's insecurity
against malicious clients by extending previously devised attacks for Federated
Learning. To make our results reproducible, we made our code available at
https://github.com/pasquini-dario/SplitNN_FSHA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasquini_D/0/1/0/all/0/1"&gt;Dario Pasquini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ateniese_G/0/1/0/all/0/1"&gt;Giuseppe Ateniese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernaschi_M/0/1/0/all/0/1"&gt;Massimo Bernaschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survival regression with accelerated failure time model in XGBoost. (arXiv:2006.04920v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04920</id>
        <link href="http://arxiv.org/abs/2006.04920"/>
        <updated>2021-08-24T01:40:28.504Z</updated>
        <summary type="html"><![CDATA[Survival regression is used to estimate the relation between time-to-event
and feature variables, and is important in application domains such as
medicine, marketing, risk management and sales management. Nonlinear tree based
machine learning algorithms as implemented in libraries such as XGBoost,
scikit-learn, LightGBM, and CatBoost are often more accurate in practice than
linear models. However, existing state-of-the-art implementations of tree-based
models have offered limited support for survival regression. In this work, we
implement loss functions for learning accelerated failure time (AFT) models in
XGBoost, to increase the support for survival modeling for different kinds of
label censoring. We demonstrate with real and simulated experiments the
effectiveness of AFT in XGBoost with respect to a number of baselines, in two
respects: generalization performance and training speed. Furthermore, we take
advantage of the support for NVIDIA GPUs in XGBoost to achieve substantial
speedup over multi-core CPUs. To our knowledge, our work is the first
implementation of AFT that utilizes the processing power of NVIDIA GPUs.
Starting from the 1.2.0 release, the XGBoost package natively supports the AFT
model. The addition of AFT in XGBoost has had significant impact in the open
source community, and a few statistics packages now utilize the XGBoost AFT
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barnwal_A/0/1/0/all/0/1"&gt;Avinash Barnwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyunsu Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hocking_T/0/1/0/all/0/1"&gt;Toby Dylan Hocking&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Unitary Convolutional Neural Networks. (arXiv:2102.11855v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11855</id>
        <link href="http://arxiv.org/abs/2102.11855"/>
        <updated>2021-08-24T01:40:28.484Z</updated>
        <summary type="html"><![CDATA[Deep neural networks can suffer from the exploding and vanishing activation
problem, in which the networks fail to train properly because the neural
signals either amplify or attenuate across the layers and become saturated.
While other normalization methods aim to fix the stated problem, most of them
have inference speed penalties in those applications that require running
averages of the neural activations. Here we extend the unitary framework based
on Lie algebra to neural networks of any dimensionalities, overcoming the major
constraints of the prior arts that limit synaptic weights to be square
matrices. Our proposed unitary convolutional neural networks deliver up to 32%
faster inference speeds and up to 50% reduction in permanent hard disk space
while maintaining competitive prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hao-Yuan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang L. Wang&lt;/a&gt; (University of California, Los Angeles)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Video Game Player Burnout with the Use of Sensor Data and Machine Learning. (arXiv:2012.02299v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02299</id>
        <link href="http://arxiv.org/abs/2012.02299"/>
        <updated>2021-08-24T01:40:28.478Z</updated>
        <summary type="html"><![CDATA[Current research in eSports lacks the tools for proper game practising and
performance analytics. The majority of prior work relied only on in-game data
for advising the players on how to perform better. However, in-game mechanics
and trends are frequently changed by new patches limiting the lifespan of the
models trained exclusively on the in-game logs. In this article, we propose the
methods based on the sensor data analysis for predicting whether a player will
win the future encounter. The sensor data were collected from 10 participants
in 22 matches in League of Legends video game. We have trained machine learning
models including Transformer and Gated Recurrent Unit to predict whether the
player wins the encounter taking place after some fixed time in the future. For
10 seconds forecasting horizon Transformer neural network architecture achieves
ROC AUC score 0.706. This model is further developed into the detector capable
of predicting that a player will lose the encounter occurring in 10 seconds in
88.3% of cases with 73.5% accuracy. This might be used as a players' burnout or
fatigue detector, advising players to retreat. We have also investigated which
physiological features affect the chance to win or lose the next in-game
encounter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smerdov_A/0/1/0/all/0/1"&gt;Anton Smerdov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somov_A/0/1/0/all/0/1"&gt;Andrey Somov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1"&gt;Paul Lukowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN Vocoder: Multi-Resolution Discriminator Is All You Need. (arXiv:2103.05236v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05236</id>
        <link href="http://arxiv.org/abs/2103.05236"/>
        <updated>2021-08-24T01:40:28.452Z</updated>
        <summary type="html"><![CDATA[Several of the latest GAN-based vocoders show remarkable achievements,
outperforming autoregressive and flow-based competitors in both qualitative and
quantitative measures while synthesizing orders of magnitude faster. In this
work, we hypothesize that the common factor underlying their success is the
multi-resolution discriminating framework, not the minute details in
architecture, loss function, or training strategy. We experimentally test the
hypothesis by evaluating six different generators paired with one shared
multi-resolution discriminating framework. For all evaluative measures with
respect to text-to-speech syntheses and for all perceptual metrics, their
performances are not distinguishable from one another, which supports our
hypothesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jaeseong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dalhyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1"&gt;Gyuhyeon Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1"&gt;Geumbyeol Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chae_G/0/1/0/all/0/1"&gt;Gyeongsu Chae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Semi-supervised Learning. (arXiv:2103.00550v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00550</id>
        <link href="http://arxiv.org/abs/2103.00550"/>
        <updated>2021-08-24T01:40:28.433Z</updated>
        <summary type="html"><![CDATA[Deep semi-supervised learning is a fast-growing field with a range of
practical applications. This paper provides a comprehensive survey on both
fundamentals and recent advances in deep semi-supervised learning methods from
perspectives of model design and unsupervised loss functions. We first present
a taxonomy for deep semi-supervised learning that categorizes existing methods,
including deep generative methods, consistency regularization methods,
graph-based methods, pseudo-labeling methods, and hybrid methods. Then we
provide a comprehensive review of 52 representative methods and offer a
detailed comparison of these methods in terms of the type of losses,
contributions, and architecture differences. In addition to the progress in the
past few years, we further discuss some shortcomings of existing methods and
provide some tentative heuristic solutions for solving these open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiangli Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zixing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Representative Interpretations on Convolutional Neural Networks. (arXiv:2108.06384v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06384</id>
        <link href="http://arxiv.org/abs/2108.06384"/>
        <updated>2021-08-24T01:40:28.424Z</updated>
        <summary type="html"><![CDATA[Interpreting the decision logic behind effective deep convolutional neural
networks (CNN) on images complements the success of deep learning models.
However, the existing methods can only interpret some specific decision logic
on individual or a small number of images. To facilitate human
understandability and generalization ability, it is important to develop
representative interpretations that interpret common decision logics of a CNN
on a large group of similar images, which reveal the common semantics data
contributes to many closely related predictions. In this paper, we develop a
novel unsupervised approach to produce a highly representative interpretation
for a large number of similar images. We formulate the problem of finding
representative interpretations as a co-clustering problem, and convert it into
a submodular cost submodular cover problem based on a sample of the linear
decision boundaries of a CNN. We also present a visualization and similarity
ranking method. Our extensive experiments demonstrate the excellent performance
of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1"&gt;Peter Cho-Ho Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1"&gt;Lingyang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torgonskiy_M/0/1/0/all/0/1"&gt;Maxim Torgonskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lanjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIMAT: Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multi-modal Data. (arXiv:2104.03642v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03642</id>
        <link href="http://arxiv.org/abs/2104.03642"/>
        <updated>2021-08-24T01:40:28.418Z</updated>
        <summary type="html"><![CDATA[In medical applications, deep learning methods are built to automate
diagnostic tasks, often formulated as single-target classification problems.
However, a clinically relevant question that practitioners usually face, is how
to predict the future trajectory of a disease (prognosis). Current methods for
such a problem often require domain knowledge, and are complicated to apply. In
this paper, we formulate the prognosis prediction problem as a one-to-many
forecasting problem. Inspired by a clinical decision-making process with two
agents -- a radiologist and a general practitioner, we model a prognosis
prediction problem with two transformer-based components that share information
between each other. The first transformer in this model aims to analyze the
imaging data, and the second one leverages its internal states as inputs, also
fusing them with auxiliary patient data. We show the effectiveness of our
method in predicting the development of structural knee osteoarthritis changes,
and forecasting Alzheimer's disease clinical status. Our results show that the
proposed method outperforms the state-of-the-art baselines in terms of various
performance metrics, including calibration, which is desired from a medical
decision support system. An open source implementation of our method is made
publicly available at https://github.com/MIPT-Oulu/CLIMAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Huy Hoang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saarakkala_S/0/1/0/all/0/1"&gt;Simo Saarakkala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1"&gt;Matthew B. Blaschko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiulpin_A/0/1/0/all/0/1"&gt;Aleksei Tiulpin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focal Frequency Loss for Image Reconstruction and Synthesis. (arXiv:2012.12821v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12821</id>
        <link href="http://arxiv.org/abs/2012.12821"/>
        <updated>2021-08-24T01:40:28.409Z</updated>
        <summary type="html"><![CDATA[Image reconstruction and synthesis have witnessed remarkable progress thanks
to the development of generative models. Nonetheless, gaps could still exist
between the real and generated images, especially in the frequency domain. In
this study, we show that narrowing gaps in the frequency domain can ameliorate
image reconstruction and synthesis quality further. We propose a novel focal
frequency loss, which allows a model to adaptively focus on frequency
components that are hard to synthesize by down-weighting the easy ones. This
objective function is complementary to existing spatial losses, offering great
impedance against the loss of important frequency information due to the
inherent bias of neural networks. We demonstrate the versatility and
effectiveness of focal frequency loss to improve popular models, such as VAE,
pix2pix, and SPADE, in both perceptual quality and quantitative performance. We
further show its potential on StyleGAN2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wayne Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Attention for Grounded Image Captioning. (arXiv:2108.01056v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01056</id>
        <link href="http://arxiv.org/abs/2108.01056"/>
        <updated>2021-08-24T01:40:28.404Z</updated>
        <summary type="html"><![CDATA[We study the problem of weakly supervised grounded image captioning. That is,
given an image, the goal is to automatically generate a sentence describing the
context of the image with each noun word grounded to the corresponding region
in the image. This task is challenging due to the lack of explicit fine-grained
region word alignments as supervision. Previous weakly supervised methods
mainly explore various kinds of regularization schemes to improve attention
accuracy. However, their performances are still far from the fully supervised
ones. One main issue that has been ignored is that the attention for generating
visually groundable words may only focus on the most discriminate parts and can
not cover the whole object. To this end, we propose a simple yet effective
method to alleviate the issue, termed as partial grounding problem in our
paper. Specifically, we design a distributed attention mechanism to enforce the
network to aggregate information from multiple spatially different regions with
consistent semantics while generating the words. Therefore, the union of the
focused region proposals should form a visual region that encloses the object
of interest completely. Extensive experiments have demonstrated the superiority
of our proposed method compared with the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nenglun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runnan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhiwen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yuqiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Haolei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-mesh: Human Correspondence Matching with Mesh Convolutional Networks. (arXiv:2108.06695v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06695</id>
        <link href="http://arxiv.org/abs/2108.06695"/>
        <updated>2021-08-24T01:40:28.388Z</updated>
        <summary type="html"><![CDATA[The proliferation of 3D scanning technology has driven a need for methods to
interpret geometric data, particularly for human subjects. In this paper we
propose an elegant fusion of regression (bottom-up) and generative (top-down)
methods to fit a parametric template model to raw scan meshes.

Our first major contribution is an intrinsic convolutional mesh U-net
architecture that predicts pointwise correspondence to a template surface.
Soft-correspondence is formulated as coordinates in a newly-constructed
Cartesian space. Modeling correspondence as Euclidean proximity enables
efficient optimization, both for network training and for the next step of the
algorithm.

Our second contribution is a generative optimization algorithm that uses the
U-net correspondence predictions to guide a parametric Iterative Closest Point
registration. By employing pre-trained human surface parametric models we
maximally leverage domain-specific prior knowledge.

The pairing of a mesh-convolutional network with generative model fitting
enables us to predict correspondence for real human surface scans including
occlusions, partialities, and varying genus (e.g. from self-contact). We
evaluate the proposed method on the FAUST correspondence challenge where we
achieve 20% (33%) improvement over state of the art methods for inter- (intra-)
subject correspondence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Groisser_B/0/1/0/all/0/1"&gt;Benjamin Groisser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_A/0/1/0/all/0/1"&gt;Alon Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1"&gt;Ron Kimmel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KoDF: A Large-scale Korean DeepFake Detection Dataset. (arXiv:2103.10094v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10094</id>
        <link href="http://arxiv.org/abs/2103.10094"/>
        <updated>2021-08-24T01:40:28.343Z</updated>
        <summary type="html"><![CDATA[A variety of effective face-swap and face-reenactment methods have been
publicized in recent years, democratizing the face synthesis technology to a
great extent. Videos generated as such have come to be called deepfakes with a
negative connotation, for various social problems they have caused. Facing the
emerging threat of deepfakes, we have built the Korean DeepFake Detection
Dataset (KoDF), a large-scale collection of synthesized and real videos focused
on Korean subjects. In this paper, we provide a detailed description of methods
used to construct the dataset, experimentally show the discrepancy between the
distributions of KoDF and existing deepfake detection datasets, and underline
the importance of using multiple datasets for real-world generalization. KoDF
is publicly available at https://moneybrain-research.github.io/kodf in its
entirety (i.e. real clips, synthesized clips, clips with adversarial attack,
and metadata).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_P/0/1/0/all/0/1"&gt;Patrick Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jaeseong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1"&gt;Gyuhyeon Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chae_G/0/1/0/all/0/1"&gt;Gyeongsu Chae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse-shot Learning with Exclusive Cross-Entropy for Extremely Many Localisations. (arXiv:2104.10425v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10425</id>
        <link href="http://arxiv.org/abs/2104.10425"/>
        <updated>2021-08-24T01:40:28.336Z</updated>
        <summary type="html"><![CDATA[Object localisation, in the context of regular images, often depicts objects
like people or cars. In these images, there is typically a relatively small
number of objects per class, which usually is manageable to annotate. However,
outside the setting of regular images, we are often confronted with a different
situation. In computational pathology, digitised tissue sections are extremely
large images, whose dimensions quickly exceed 250'000x250'000 pixels, where
relevant objects, such as tumour cells or lymphocytes can quickly number in the
millions. Annotating them all is practically impossible and annotating sparsely
a few, out of many more, is the only possibility. Unfortunately, learning from
sparse annotations, or sparse-shot learning, clashes with standard supervised
learning because what is not annotated is treated as a negative. However,
assigning negative labels to what are true positives leads to confusion in the
gradients and biased learning. To this end, we present exclusive cross-entropy,
which slows down the biased learning by examining the second-order loss
derivatives in order to drop the loss terms corresponding to likely biased
terms. Experiments on nine datasets and two different localisation tasks,
detection with YOLLO and segmentation with Unet, show that we obtain
considerable improvements compared to cross-entropy or focal loss, while often
reaching the best possible performance for the model with only 10-40% of
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panteli_A/0/1/0/all/0/1"&gt;Andreas Panteli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teuwen_J/0/1/0/all/0/1"&gt;Jonas Teuwen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horlings_H/0/1/0/all/0/1"&gt;Hugo Horlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08248</id>
        <link href="http://arxiv.org/abs/2102.08248"/>
        <updated>2021-08-24T01:40:28.321Z</updated>
        <summary type="html"><![CDATA[Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1"&gt;Jakob D. Havtorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1"&gt;Lars Maal&amp;#xf8;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Lane Detection via Expanded Self Attention. (arXiv:2102.07037v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07037</id>
        <link href="http://arxiv.org/abs/2102.07037"/>
        <updated>2021-08-24T01:40:28.315Z</updated>
        <summary type="html"><![CDATA[The image-based lane detection algorithm is one of the key technologies in
autonomous vehicles. Modern deep learning methods achieve high performance in
lane detection, but it is still difficult to accurately detect lanes in
challenging situations such as congested roads and extreme lighting conditions.
To be robust on these challenging situations, it is important to extract global
contextual information even from limited visual cues. In this paper, we propose
a simple but powerful self-attention mechanism optimized for lane detection
called the Expanded Self Attention (ESA) module. Inspired by the simple
geometric structure of lanes, the proposed method predicts the confidence of a
lane along the vertical and horizontal directions in an image. The prediction
of the confidence enables estimating occluded locations by extracting global
contextual information. ESA module can be easily implemented and applied to any
encoder-decoder-based model without increasing the inference time. The
performance of our method is evaluated on three popular lane detection
benchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art
performance in CULane and BDD100K and distinct improvement on TuSimple dataset.
The experimental results show that our approach is robust to occlusion and
extreme lighting conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junhyeop Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dogyoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1"&gt;Woojin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sangwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Object Detection via Instance-Level Temporal Cycle Confusion. (arXiv:2104.08381v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08381</id>
        <link href="http://arxiv.org/abs/2104.08381"/>
        <updated>2021-08-24T01:40:28.295Z</updated>
        <summary type="html"><![CDATA[Building reliable object detectors that are robust to domain shifts, such as
various changes in context, viewpoint, and object appearances, is critical for
real-world applications. In this work, we study the effectiveness of auxiliary
self-supervised tasks to improve the out-of-distribution generalization of
object detectors. Inspired by the principle of maximum entropy, we introduce a
novel self-supervised task, instance-level temporal cycle confusion (CycConf),
which operates on the region features of the object detectors. For each object,
the task is to find the most different object proposals in the adjacent frame
in a video and then cycle back to itself for self-supervision. CycConf
encourages the object detector to explore invariant structures across instances
under various motions, which leads to improved model robustness in unseen
domains at test time. We observe consistent out-of-domain performance
improvements when training object detectors in tandem with self-supervised
tasks on large-scale video datasets (BDD100K and Waymo open data). The joint
training framework also establishes a new state-of-the-art on standard
unsupervised domain adaptative detection benchmarks (Cityscapes, Foggy
Cityscapes, and Sim10K). The code and models are available at
https://github.com/xinw1012/cycle-confusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Thomas E. Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video. (arXiv:2012.12247v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12247</id>
        <link href="http://arxiv.org/abs/2012.12247"/>
        <updated>2021-08-24T01:40:28.290Z</updated>
        <summary type="html"><![CDATA[We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and
novel view synthesis approach for general non-rigid dynamic scenes. Our
approach takes RGB images of a dynamic scene as input (e.g., from a monocular
video recording), and creates a high-quality space-time geometry and appearance
representation. We show that a single handheld consumer-grade camera is
sufficient to synthesize sophisticated renderings of a dynamic scene from novel
virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles
the dynamic scene into a canonical volume and its deformation. Scene
deformation is implemented as ray bending, where straight rays are deformed
non-rigidly. We also propose a novel rigidity network to better constrain rigid
regions of the scene, leading to more stable results. The ray bending and
rigidity network are trained without explicit supervision. Our formulation
enables dense correspondence estimation across views and time, and compelling
video editing applications such as motion exaggeration. Our code will be open
sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tretschk_E/0/1/0/all/0/1"&gt;Edgar Tretschk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ayush Tewari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1"&gt;Michael Zollh&amp;#xf6;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1"&gt;Christoph Lassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Processing Challenges and Examples for {\it in-situ} Transmission Electron Microscopy. (arXiv:2104.08688v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08688</id>
        <link href="http://arxiv.org/abs/2104.08688"/>
        <updated>2021-08-24T01:40:28.281Z</updated>
        <summary type="html"><![CDATA[Transmission Electron Microscopy (TEM) is a powerful tool for imaging
material structure and characterizing material chemistry. Recent advances in
data collection technology for TEM have enabled high-volume and high-resolution
data collection at a microsecond frame rate. Taking advantage of these advances
in data collection rates requires the development and application of data
processing tools, including image analysis, feature extraction, and streaming
data processing techniques. In this paper, we highlight a few areas in
materials science that have benefited from combining signal processing and
statistical analysis with data collection capabilities in TEM and present a
future outlook on opportunities of integrating signal processing with automated
TEM data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kacher_J/0/1/0/all/0/1"&gt;Josh Kacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voigt_S/0/1/0/all/0/1"&gt;Sven P. Voigt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shixiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuchi_H/0/1/0/all/0/1"&gt;Henry Yuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Key_J/0/1/0/all/0/1"&gt;Jordan Key&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalidindi_S/0/1/0/all/0/1"&gt;Surya R. Kalidindi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency. (arXiv:2108.09891v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09891</id>
        <link href="http://arxiv.org/abs/2108.09891"/>
        <updated>2021-08-24T01:40:28.275Z</updated>
        <summary type="html"><![CDATA[The success of deep neural networks (DNNs) haspromoted the widespread
applications of person re-identification (ReID). However, ReID systems inherit
thevulnerability of DNNs to malicious attacks of visually in-conspicuous
adversarial perturbations. Detection of adver-sarial attacks is, therefore, a
fundamental requirement forrobust ReID systems. In this work, we propose a
Multi-Expert Adversarial Attack Detection (MEAAD) approach toachieve this goal
by checking context inconsistency, whichis suitable for any DNN-based ReID
systems. Specifically,three kinds of context inconsistencies caused by
adversar-ial attacks are employed to learn a detector for distinguish-ing the
perturbed examples, i.e., a) the embedding distancesbetween a perturbed query
person image and its top-K re-trievals are generally larger than those between
a benignquery image and its top-K retrievals, b) the embedding dis-tances among
the top-K retrievals of a perturbed query im-age are larger than those of a
benign query image, c) thetop-K retrievals of a benign query image obtained
with mul-tiple expert ReID models tend to be consistent, which isnot preserved
when attacks are present. Extensive exper-iments on the Market1501 and
DukeMTMC-ReID datasetsshow that, as the first adversarial attack detection
approachfor ReID,MEAADeffectively detects various adversarial at-tacks and
achieves high ROC-AUC (over 97.5%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xueping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shasha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Min Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaonan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIMON: Towards High-quality Hash Codes. (arXiv:2010.07804v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07804</id>
        <link href="http://arxiv.org/abs/2010.07804"/>
        <updated>2021-08-24T01:40:28.269Z</updated>
        <summary type="html"><![CDATA[Recently, hashing is widely used in approximate nearest neighbor search for
its storage and computational efficiency. Most of the unsupervised hashing
methods learn to map images into semantic similarity-preserving hash codes by
constructing local semantic similarity structure from the pre-trained model as
the guiding information, i.e., treating each point pair similar if their
distance is small in feature space. However, due to the inefficient
representation ability of the pre-trained model, many false positives and
negatives in local semantic similarity will be introduced and lead to error
propagation during the hash code learning. Moreover, few of the methods
consider the robustness of models, which will cause instability of hash codes
to disturbance. In this paper, we propose a new method named
{\textbf{C}}omprehensive s{\textbf{I}}milarity {\textbf{M}}ining and
c{\textbf{O}}nsistency lear{\textbf{N}}ing (CIMON). First, we use global
refinement and similarity statistical distribution to obtain reliable and
smooth guidance. Second, both semantic and contrastive consistency learning are
introduced to derive both disturb-invariant and discriminative hash codes.
Extensive experiments on several benchmark datasets show that the proposed
method outperforms a wide range of state-of-the-art methods in both retrieval
performance and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Daqing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zeyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jinwen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhongming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Spatial Relationships by Transformers for Domain Generalization. (arXiv:2108.10046v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10046</id>
        <link href="http://arxiv.org/abs/2108.10046"/>
        <updated>2021-08-24T01:40:28.254Z</updated>
        <summary type="html"><![CDATA[Due to the rapid increase in the diversity of image data, the problem of
domain generalization has received increased attention recently. While domain
generalization is a challenging problem, it has achieved great development
thanks to the fast development of AI techniques in computer vision. Most of
these advanced algorithms are proposed with deep architectures based on
convolution neural nets (CNN). However, though CNNs have a strong ability to
find the discriminative features, they do a poor job of modeling the relations
between different locations in the image due to the response to CNN filters are
mostly local. Since these local and global spatial relationships are
characterized to distinguish an object under consideration, they play a
critical role in improving the generalization ability against the domain gap.
In order to get the object parts relationships to gain better domain
generalization, this work proposes to use the self attention model. However,
the attention models are proposed for sequence, which are not expert in
discriminate feature extraction for 2D images. Considering this, we proposed a
hybrid architecture to discover the spatial relationships between these local
features, and derive a composite representation that encodes both the
discriminative features and their relationships to improve the domain
generalization. Evaluation on three well-known benchmarks demonstrates the
benefits of modeling relationships between the features of an image using the
proposed method and achieves state-of-the-art domain generalization
performance. More specifically, the proposed algorithm outperforms the
state-of-the-art by $2.2\%$ and $3.4\%$ on PACS and Office-Home databases,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1"&gt;Cuicui Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1"&gt;Karthik Nandakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[External Knowledge Augmented Text Visual Question Answering. (arXiv:2108.09717v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09717</id>
        <link href="http://arxiv.org/abs/2108.09717"/>
        <updated>2021-08-24T01:40:28.248Z</updated>
        <summary type="html"><![CDATA[The open-ended question answering task of Text-VQA requires reading and
reasoning about local, often previously unseen, scene-text content of an image
to generate answers. In this work, we propose the generalized use of external
knowledge to augment our understanding of the said scene-text. We design a
framework to extract, filter, and encode knowledge atop a standard multimodal
transformer for vision language understanding tasks. Through empirical
evidence, we demonstrate how knowledge can highlight instance-only cues and
thus help deal with training data bias, improve answer entity type correctness,
and detect multiword named entities. We generate results comparable to the
state-of-the-art on two publicly available datasets, under the constraints of
similar upstream OCR systems and training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1"&gt;Arka Ujjal Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1"&gt;Ernest Valveny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1"&gt;Gaurav Harit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All-Optical Synthesis of an Arbitrary Linear Transformation Using Diffractive Surfaces. (arXiv:2108.09833v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2108.09833</id>
        <link href="http://arxiv.org/abs/2108.09833"/>
        <updated>2021-08-24T01:40:28.243Z</updated>
        <summary type="html"><![CDATA[We report the design of diffractive surfaces to all-optically perform
arbitrary complex-valued linear transformations between an input (N_i) and
output (N_o), where N_i and N_o represent the number of pixels at the input and
output fields-of-view (FOVs), respectively. First, we consider a single
diffractive surface and use a matrix pseudoinverse-based method to determine
the complex-valued transmission coefficients of the diffractive
features/neurons to all-optically perform a desired/target linear
transformation. In addition to this data-free design approach, we also consider
a deep learning-based design method to optimize the transmission coefficients
of diffractive surfaces by using examples of input/output fields corresponding
to the target transformation. We compared the all-optical transformation errors
and diffraction efficiencies achieved using data-free designs as well as
data-driven (deep learning-based) diffractive designs to all-optically perform
(i) arbitrarily-chosen complex-valued transformations including unitary,
nonunitary and noninvertible transforms, (ii) 2D discrete Fourier
transformation, (iii) arbitrary 2D permutation operations, and (iv) high-pass
filtered coherent imaging. Our analyses reveal that if the total number (N) of
spatially-engineered diffractive features/neurons is N_i x N_o or larger, both
design methods succeed in all-optical implementation of the target
transformation, achieving negligible error. However, compared to data-free
designs, deep learning-based diffractive designs are found to achieve
significantly larger diffraction efficiencies for a given N and their
all-optical transformations are more accurate for N < N_i x N_o. These
conclusions are generally applicable to various optical processors that employ
spatially-engineered diffractive surfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kulce_O/0/1/0/all/0/1"&gt;Onur Kulce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1"&gt;Deniz Mengu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rivenson_Y/0/1/0/all/0/1"&gt;Yair Rivenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1"&gt;Aydogan Ozcan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Real-world X-ray Security Inspection: A High-Quality Benchmark and Lateral Inhibition Module for Prohibited Items Detection. (arXiv:2108.09917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09917</id>
        <link href="http://arxiv.org/abs/2108.09917"/>
        <updated>2021-08-24T01:40:28.235Z</updated>
        <summary type="html"><![CDATA[Prohibited items detection in X-ray images often plays an important role in
protecting public safety, which often deals with color-monotonous and
luster-insufficient objects, resulting in unsatisfactory performance. Till now,
there have been rare studies touching this topic due to the lack of specialized
high-quality datasets. In this work, we first present a High-quality X-ray
(HiXray) security inspection image dataset, which contains 102,928 common
prohibited items of 8 categories. It is the largest dataset of high quality for
prohibited items detection, gathered from the real-world airport security
inspection and annotated by professional security inspectors. Besides, for
accurate prohibited item detection, we further propose the Lateral Inhibition
Module (LIM) inspired by the fact that humans recognize these items by ignoring
irrelevant information and focusing on identifiable characteristics, especially
when objects are overlapped with each other. Specifically, LIM, the elaborately
designed flexible additional module, suppresses the noisy information flowing
maximumly by the Bidirectional Propagation (BP) module and activates the most
identifiable charismatic, boundary, from four directions by Boundary Activation
(BA) module. We evaluate our method extensively on HiXray and OPIXray and the
results demonstrate that it outperforms SOTA detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Renshuai Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yanlu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiangjian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hainan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1"&gt;Haotong Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiakai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuqing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Libo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianglong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic Image Synthesis with Configurable 3D Scene Layouts. (arXiv:2108.10031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10031</id>
        <link href="http://arxiv.org/abs/2108.10031"/>
        <updated>2021-08-24T01:40:28.229Z</updated>
        <summary type="html"><![CDATA[Recent conditional image synthesis approaches provide high-quality
synthesized images. However, it is still challenging to accurately adjust image
contents such as the positions and orientations of objects, and synthesized
images often have geometrically invalid contents. To provide users with rich
controllability on synthesized images in the aspect of 3D geometry, we propose
a novel approach to realistic-looking image synthesis based on a configurable
3D scene layout. Our approach takes a 3D scene with semantic class labels as
input and trains a 3D scene painting network that synthesizes color values for
the input 3D scene. With the trained painting network, realistic-looking images
for the input 3D scene can be rendered and manipulated. To train the painting
network without 3D color supervision, we exploit an off-the-shelf 2D semantic
image synthesis method. In experiments, we show that our approach produces
images with geometrically correct structures and supports geometric
manipulation such as the change of the viewpoint and object poses as well as
manipulation of the painting style.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Jaebong Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Janghun Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Sunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jaesik Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Transferable Are Self-supervised Features in Medical Image Classification Tasks?. (arXiv:2108.10048v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10048</id>
        <link href="http://arxiv.org/abs/2108.10048"/>
        <updated>2021-08-24T01:40:28.213Z</updated>
        <summary type="html"><![CDATA[Transfer learning has become a standard practice to mitigate the lack of
labeled data in medical classification tasks. Whereas finetuning a downstream
task using supervised ImageNet pretrained features is straightforward and
extensively investigated in many works, there is little study on the usefulness
of self-supervised pretraining. In this paper, we assess the transferability of
ImageNet self-supervisedpretraining by evaluating the performance of models
initialized with pretrained features from three self-supervised techniques
(SimCLR, SwAV, and DINO) on selected medical classification tasks. The chosen
tasks cover tumor detection in sentinel axillary lymph node images, diabetic
retinopathy classification in fundus images, and multiple pathological
condition classification in chest X-ray images. We demonstrate that
self-supervised pretrained models yield richer embeddings than their supervised
counterpart, which benefits downstream tasks in view of both linear evaluation
and finetuning. For example, in view of linear evaluation at acritically small
subset of the data, we see an improvement up to 14.79% in Kappa score in the
diabetic retinopathy classification task, 5.4% in AUC in the tumor
classification task, 7.03% AUC in the pneumonia detection, and 9.4% in AUC in
the detection of pathological conditions in chest X-ray. In addition, we
introduce Dynamic Visual Meta-Embedding (DVME) as an end-to-end transfer
learning approach that fuses pretrained embeddings from multiple models. We
show that the collective representation obtained by DVME leads to a significant
improvement in the performance of selected tasks compared to using a single
pretrained model approach and can be generalized to any combination of
pretrained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuan Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1"&gt;Sadegh Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1"&gt;Matthias Lenga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imbalance-XGBoost: Leveraging Weighted and Focal Losses for Binary Label-Imbalanced Classification with XGBoost. (arXiv:1908.01672v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.01672</id>
        <link href="http://arxiv.org/abs/1908.01672"/>
        <updated>2021-08-24T01:40:28.208Z</updated>
        <summary type="html"><![CDATA[The paper presents Imbalance-XGBoost, a Python package that combines the
powerful XGBoost software with weighted and focal losses to tackle binary
label-imbalanced classification tasks. Though a small-scale program in terms of
size, the package is, to the best of the authors' knowledge, the first of its
kind which provides an integrated implementation for the two losses on XGBoost
and brings a general-purpose extension on XGBoost for label-imbalanced
scenarios. In this paper, the design and usage of the package are described
with exemplar code listings, and its convenience to be integrated into
Python-driven Machine Learning projects is illustrated. Furthermore, as the
first- and second-order derivatives of the loss functions are essential for the
implementations, the algebraic derivation is discussed and it can be deemed as
a separate algorithmic contribution. The performances of the algorithms
implemented in the package are empirically evaluated on Parkinson's disease
classification data set, and multiple state-of-the-art performances have been
observed. Given the scalable nature of XGBoost, the package has great
potentials to be applied to real-life binary classification tasks, which are
usually of large-scale and label-imbalanced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Chengyuan Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suzhen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning, Bit by Bit. (arXiv:2103.04047v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04047</id>
        <link href="http://arxiv.org/abs/2103.04047"/>
        <updated>2021-08-24T01:40:28.202Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning agents have demonstrated remarkable achievements in
simulated environments. Data efficiency poses an impediment to carrying this
success over to real environments. The design of data-efficient agents calls
for a deeper understanding of information acquisition and representation. We
develop concepts and establish a regret bound that together offer principled
guidance. The bound sheds light on questions of what information to seek, how
to seek that information, and it what information to retain. To illustrate
concepts, we design simple agents that build on them and present computational
results that demonstrate improvements in data efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiuyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwaracherla_V/0/1/0/all/0/1"&gt;Vikranth Dwaracherla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1"&gt;Morteza Ibrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zheng Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SemiFed: Semi-supervised Federated Learning with Consistency and Pseudo-Labeling. (arXiv:2108.09412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09412</id>
        <link href="http://arxiv.org/abs/2108.09412"/>
        <updated>2021-08-24T01:40:28.195Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple clients, such as mobile phones and
organizations, to collaboratively learn a shared model for prediction while
protecting local data privacy. However, most recent research and applications
of federated learning assume that all clients have fully labeled data, which is
impractical in real-world settings. In this work, we focus on a new scenario
for cross-silo federated learning, where data samples of each client are
partially labeled. We borrow ideas from semi-supervised learning methods where
a large amount of unlabeled data is utilized to improve the model's accuracy
despite limited access to labeled examples. We propose a new framework dubbed
SemiFed that unifies two dominant approaches for semi-supervised learning:
consistency regularization and pseudo-labeling. SemiFed first applies advanced
data augmentation techniques to enforce consistency regularization and then
generates pseudo-labels using the model's predictions during training. SemiFed
takes advantage of the federation so that for a given image, the pseudo-label
holds only if multiple models from different clients produce a high-confidence
prediction and agree on the same label. Extensive experiments on two image
benchmarks demonstrate the effectiveness of our approach under both homogeneous
and heterogeneous data distribution settings]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haowen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahabi_C/0/1/0/all/0/1"&gt;Cyrus Shahabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-08-24T01:40:28.190Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep survival analysis with longitudinal X-rays for COVID-19. (arXiv:2108.09641v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09641</id>
        <link href="http://arxiv.org/abs/2108.09641"/>
        <updated>2021-08-24T01:40:28.173Z</updated>
        <summary type="html"><![CDATA[Time-to-event analysis is an important statistical tool for allocating
clinical resources such as ICU beds. However, classical techniques like the Cox
model cannot directly incorporate images due to their high dimensionality. We
propose a deep learning approach that naturally incorporates multiple,
time-dependent imaging studies as well as non-imaging data into time-to-event
analysis. Our techniques are benchmarked on a clinical dataset of 1,894
COVID-19 patients, and show that image sequences significantly improve
predictions. For example, classical time-to-event methods produce a concordance
error of around 30-40% for predicting hospital admission, while our error is
25% without images and 20% with multiple X-rays included. Ablation studies
suggest that our models are not learning spurious features such as scanner
artifacts. While our focus and evaluation is on COVID-19, the methods we
develop are broadly applicable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shu_M/0/1/0/all/0/1"&gt;Michelle Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bowen_R/0/1/0/all/0/1"&gt;Richard Strong Bowen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herrmann_C/0/1/0/all/0/1"&gt;Charles Herrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_G/0/1/0/all/0/1"&gt;Gengmo Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Santacatterina_M/0/1/0/all/0/1"&gt;Michele Santacatterina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zabih_R/0/1/0/all/0/1"&gt;Ramin Zabih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function. (arXiv:2108.09598v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09598</id>
        <link href="http://arxiv.org/abs/2108.09598"/>
        <updated>2021-08-24T01:40:28.168Z</updated>
        <summary type="html"><![CDATA[Activation functions play a pivotal role in determining the training dynamics
and neural network performance. The widely adopted activation function ReLU
despite being simple and effective has few disadvantages including the Dying
ReLU problem. In order to tackle such problems, we propose a novel activation
function called Serf which is self-regularized and nonmonotonic in nature. Like
Mish, Serf also belongs to the Swish family of functions. Based on several
experiments on computer vision (image classification and object detection) and
natural language processing (machine translation, sentiment classification and
multimodal entailment) tasks with different state-of-the-art architectures, it
is observed that Serf vastly outperforms ReLU (baseline) and other activation
functions including both Swish and Mish, with a markedly bigger margin on
deeper architectures. Ablation studies further demonstrate that Serf based
architectures perform better than those of Swish and Mish in varying scenarios,
validating the effectiveness and compatibility of Serf with varying depth,
complexity, optimizers, learning rates, batch sizes, initializers and dropout
rates. Finally, we investigate the mathematical relation between Swish and
Serf, thereby showing the impact of preconditioner function ingrained in the
first derivative of Serf which provides a regularization effect making
gradients smoother and optimization faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sayan Nag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_M/0/1/0/all/0/1"&gt;Mayukh Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Reconstruction from public webcams. (arXiv:2108.09476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09476</id>
        <link href="http://arxiv.org/abs/2108.09476"/>
        <updated>2021-08-24T01:40:28.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the possibility of reconstructing the 3D
geometry of a scene captured by multiple webcams. The number of publicly
accessible webcams is already large and it is growing every day. A logical
question arises - can we use this free source of data for something beyond
leisure activities? The challenge is that no internal, external, or temporal
calibration of these cameras is available. We show that using recent advances
in computer vision, we successfully calibrate the cameras, perform 3D
reconstructions of the static scene and also recover the 3D trajectories of
moving objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albl_C/0/1/0/all/0/1"&gt;Cenek Albl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating a Latent Tree for Extremes. (arXiv:2102.06197v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06197</id>
        <link href="http://arxiv.org/abs/2102.06197"/>
        <updated>2021-08-24T01:40:28.156Z</updated>
        <summary type="html"><![CDATA[The Latent River Problem has emerged as a flagship problem for causal
discovery in extreme value statistics. This paper gives QTree, a simple and
efficient algorithm to solve the Latent River Problem that outperforms existing
methods. QTree returns a directed graph and achieves almost perfect recovery on
the Upper Danube, the existing benchmark dataset, as well as on new data from
the Lower Colorado River in Texas. It can handle missing data, has an automated
parameter tuning procedure, and runs in time $O(n |V|^2)$, where $n$ is the
number of observations and $|V|$ the number of nodes in the graph. In addition,
under a Bayesian network model for extreme values with propagating noise, we
show that the QTree estimator returns for $n\to\infty$ a.s. the correct tree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tran_N/0/1/0/all/0/1"&gt;Ngoc Mai Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buck_J/0/1/0/all/0/1"&gt;Johannes Buck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kluppelberg_C/0/1/0/all/0/1"&gt;Claudia Kl&amp;#xfc;ppelberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collection and Validation of Psychophysiological Data from Professional and Amateur Players: a Multimodal eSports Dataset. (arXiv:2011.00958v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00958</id>
        <link href="http://arxiv.org/abs/2011.00958"/>
        <updated>2021-08-24T01:40:28.151Z</updated>
        <summary type="html"><![CDATA[Proper training and analytics in eSports require accurately collected and
annotated data. Most eSports research focuses exclusively on in-game data
analysis, and there is a lack of prior work involving eSports athletes'
psychophysiological data. In this paper, we present a dataset collected from
professional and amateur teams in 22 matches in League of Legends video game
with more than 40 hours of recordings. Recorded data include the players'
physiological activity, e.g. movements, pulse, saccades, obtained from various
sensors, self-reported aftermatch survey, and in-game data. An important
feature of the dataset is simultaneous data collection from five players, which
facilitates the analysis of sensor data on a team level. Upon the collection of
dataset we carried out its validation. In particular, we demonstrate that
stress and concentration levels for professional players are less correlated,
meaning more independent playstyle. Also, we show that the absence of team
communication does not affect the professional players as much as amateur ones.
To investigate other possible use cases of the dataset, we have trained
classical machine learning algorithms for skill prediction and player
re-identification using 3-minute sessions of sensor data. Best models achieved
0.856 and 0.521 (0.10 for a chance level) accuracy scores on a validation set
for skill prediction and player re-id problems, respectively. The dataset is
available at https://github.com/smerdov/eSports Sensors Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smerdov_A/0/1/0/all/0/1"&gt;Anton Smerdov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1"&gt;Paul Lukowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somov_A/0/1/0/all/0/1"&gt;Andrey Somov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Cloud. (arXiv:2108.06230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06230</id>
        <link href="http://arxiv.org/abs/2108.06230"/>
        <updated>2021-08-24T01:40:28.145Z</updated>
        <summary type="html"><![CDATA[While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D
images, its application to 3D data is still recent and scarce, with just a few
methods limited to classification. We present the first generative approach for
both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both
classification and, for the first time, semantic segmentation. We show that it
reaches or outperforms the state of the art on ModelNet40 classification for
both inductive ZSL and inductive GZSL. For semantic segmentation, we created
three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and
SemanticKITTI. Our experiments show that our method outperforms strong
baselines, which we additionally propose for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michele_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Michele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1"&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1"&gt;Gilles Puy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucher_M/0/1/0/all/0/1"&gt;Maxime Bucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1"&gt;Renaud Marlet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing the Spatio-temporal Observability of Grid-Edge Resources in Distribution Grids. (arXiv:2102.07801v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07801</id>
        <link href="http://arxiv.org/abs/2102.07801"/>
        <updated>2021-08-24T01:40:28.130Z</updated>
        <summary type="html"><![CDATA[Enhancing the spatio-temporal observability of distributed energy resources
(DERs) is crucial for achieving secure and efficient operations in distribution
grids. This paper puts forth a joint recovery framework for residential loads
by leveraging the complimentary strengths of heterogeneous measurements in real
time. The proposed framework integrates low-resolution smart meter data
collected at every load node with fast-sampled feeder-level measurements from
limited number of distribution phasor measurement units. To address the lack of
data, we exploit two key characteristics for the loads and DERs, namely the
sparse changes due to infrequent activities of appliances and electric vehicles
(EVs) and the locational dependence of solar photovoltaic (PV) generation.
Accordingly, meaningful regularization terms are introduced to cast a convex
load recovery problem, which will be further simplified to reduce the
computational complexity. The load recovery solutions can be utilized to
identify the EV charging events at each load node and to infer the total
behind-the-meter PV output. Numerical tests using real-world data have
demonstrated the effectiveness of the proposed approaches in enhancing the
visibility of grid-edge DERs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shanny Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Segment Medical Images from Few-Shot Sparse Labels. (arXiv:2108.05476v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.05476</id>
        <link href="http://arxiv.org/abs/2108.05476"/>
        <updated>2021-08-24T01:40:28.124Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel approach for few-shot semantic segmentation
with sparse labeled images. We investigate the effectiveness of our method,
which is based on the Model-Agnostic Meta-Learning (MAML) algorithm, in the
medical scenario, where the use of sparse labeling and few-shot can alleviate
the cost of producing new annotated datasets. Our method uses sparse labels in
the meta-training and dense labels in the meta-test, thus making the model
learn to predict dense labels from sparse ones. We conducted experiments with
four Chest X-Ray datasets to evaluate two types of annotations (grid and
points). The results show that our method is the most suitable when the target
domain highly differs from source domains, achieving Jaccard scores comparable
to dense labels, using less than 2% of the pixels of an image with labels in
few-shot scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1"&gt;Pedro H. T. Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LightFuse: Lightweight CNN based Dual-exposure Fusion. (arXiv:2107.02299v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02299</id>
        <link href="http://arxiv.org/abs/2107.02299"/>
        <updated>2021-08-24T01:40:28.109Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (DCNN) aided high dynamic range (HDR)
imaging recently received a lot of attention. The quality of DCNN generated HDR
images have overperformed the traditional counterparts. However, DCNN is prone
to be computationally intensive and power-hungry. To address the challenge, we
propose LightFuse, a light-weight CNN-based algorithm for extreme dual-exposure
image fusion, which can be implemented on various embedded computing platforms
with limited power and hardware resources. Two sub-networks are utilized: a
GlobalNet (G) and a DetailNet (D). The goal of G is to learn the global
illumination information on the spatial dimension, whereas D aims to enhance
local details on the channel dimension. Both G and D are based solely on
depthwise convolution (D Conv) and pointwise convolution (P Conv) to reduce
required parameters and computations. Experimental results display that the
proposed technique could generate HDR images with plausible details in
extremely exposed regions. Our PSNR score exceeds the other state-of-the-art
approaches by 1.2 to 1.6 times and achieves 1.4 to 20 times FLOP and parameter
reduction compared with others.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yanushkevich_S/0/1/0/all/0/1"&gt;Svetlana Yanushkevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadid_Pecht_O/0/1/0/all/0/1"&gt;Orly Yadid-Pecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Filtering and Neural Networks with Non Commutative Algebras. (arXiv:2108.09923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09923</id>
        <link href="http://arxiv.org/abs/2108.09923"/>
        <updated>2021-08-24T01:40:28.103Z</updated>
        <summary type="html"><![CDATA[In this paper we provide stability results for algebraic neural networks
(AlgNNs) based on non commutative algebras. AlgNNs are stacked layered
structures with each layer associated to an algebraic signal model (ASM)
determined by an algebra, a vector space, and a homomorphism. Signals are
modeled as elements of the vector space, filters are elements in the algebra,
while the homomorphism provides a realization of the filters as concrete
operators. We study the stability of the algebraic filters in non commutative
algebras to perturbations on the homomorphisms, and we provide conditions under
which stability is guaranteed. We show that the commutativity between shift
operators and between shifts and perturbations does not affect the property of
an architecture of being stable. This provides an answer to the question of
whether shift invariance was a necessary attribute of convolutional
architectures to guarantee stability. Additionally, we show that although the
frequency responses of filters in non commutative algebras exhibit substantial
differences with respect to filters in commutative algebras, their derivatives
for stable filters have a similar behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parada_Mayorga_A/0/1/0/all/0/1"&gt;Alejandro Parada-Mayorga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focal Frequency Loss for Image Reconstruction and Synthesis. (arXiv:2012.12821v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12821</id>
        <link href="http://arxiv.org/abs/2012.12821"/>
        <updated>2021-08-24T01:40:28.084Z</updated>
        <summary type="html"><![CDATA[Image reconstruction and synthesis have witnessed remarkable progress thanks
to the development of generative models. Nonetheless, gaps could still exist
between the real and generated images, especially in the frequency domain. In
this study, we show that narrowing gaps in the frequency domain can ameliorate
image reconstruction and synthesis quality further. We propose a novel focal
frequency loss, which allows a model to adaptively focus on frequency
components that are hard to synthesize by down-weighting the easy ones. This
objective function is complementary to existing spatial losses, offering great
impedance against the loss of important frequency information due to the
inherent bias of neural networks. We demonstrate the versatility and
effectiveness of focal frequency loss to improve popular models, such as VAE,
pix2pix, and SPADE, in both perceptual quality and quantitative performance. We
further show its potential on StyleGAN2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wayne Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omnidirectional Transfer for Quasilinear Lifelong Learning. (arXiv:2004.12908v8 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12908</id>
        <link href="http://arxiv.org/abs/2004.12908"/>
        <updated>2021-08-24T01:40:28.066Z</updated>
        <summary type="html"><![CDATA[In biological learning, data are used to improve performance not only on the
current task, but also on previously encountered and as yet unencountered
tasks. In contrast, classical machine learning starts from a blank slate, or
tabula rasa, using data only for the single task at hand. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
catastrophic forgetting). Many recent approaches for continual or lifelong
learning have attempted to maintain performance given new tasks. But striving
to avoid forgetting sets the goal unnecessarily low: the goal of lifelong
learning, whether biological or artificial, should be to improve performance on
all tasks (including past and future) with any new data. We propose
omnidirectional transfer learning algorithms, which includes two special cases
of interest: decision forests and deep networks. Our key insight is the
development of the omni-voter layer, which ensembles representations learned
independently on all tasks to jointly decide how to proceed on any given new
data point, thereby improving performance on both past and future tasks. Our
algorithms demonstrate omnidirectional transfer in a variety of simulated and
real data scenarios, including tabular data, image data, spoken data, and
adversarial tasks. Moreover, they do so with quasilinear space and time
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1"&gt;Jayanta Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1"&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1"&gt;Will LeVine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Ronak D. Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geisa_A/0/1/0/all/0/1"&gt;Ali Geisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1"&gt;Gido M. van de Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1"&gt;Emily Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chenyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tower_B/0/1/0/all/0/1"&gt;Bryan Tower&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1"&gt;Jonathan Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Christopher M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified survey of treatment effect heterogeneity modeling and uplift modeling. (arXiv:2007.12769v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12769</id>
        <link href="http://arxiv.org/abs/2007.12769"/>
        <updated>2021-08-24T01:40:28.059Z</updated>
        <summary type="html"><![CDATA[A central question in many fields of scientific research is to determine how
an outcome would be affected by an action, or to measure the effect of an
action (a.k.a treatment effect). In recent years, a need for estimating the
heterogeneous treatment effects conditioning on the different characteristics
of individuals has emerged from research fields such as personalized
healthcare, social science, and online marketing. To meet the need, researchers
and practitioners from different communities have developed algorithms by
taking the treatment effect heterogeneity modeling approach and the uplift
modeling approach, respectively. In this paper, we provide a unified survey of
these two seemingly disconnected yet closely related approaches under the
potential outcome framework. We then provide a structured survey of existing
methods by emphasizing on their inherent connections with a set of unified
notations to make comparisons of the different methods easy. We then review the
main applications of the surveyed methods in personalized marketing,
personalized medicine, and social studies. Finally, we summarize the existing
software packages and present discussions based on the use of methods on
synthetic, semi-synthetic and real world data sets and provide some general
guidelines for choosing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiuyong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Optimization with Seasonal Patterns. (arXiv:2005.08088v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08088</id>
        <link href="http://arxiv.org/abs/2005.08088"/>
        <updated>2021-08-24T01:40:28.054Z</updated>
        <summary type="html"><![CDATA[A standard assumption adopted in the multi-armed bandit (MAB) framework is
that the mean rewards are constant over time. This assumption can be
restrictive in the business world as decision-makers often face an evolving
environment where the mean rewards are time-varying. In this paper, we consider
a non-stationary MAB model with $K$ arms whose mean rewards vary over time in a
periodic manner. The unknown periods can be different across arms and scale
with the length of the horizon $T$ polynomially. We propose a two-stage policy
that combines the Fourier analysis with a confidence-bound-based learning
procedure to learn the periods and minimize the regret. In stage one, the
policy correctly estimates the periods of all arms with high probability. In
stage two, the policy explores the periodic mean rewards of arms using the
periods estimated in stage one and exploits the optimal arm in the long run. We
show that our learning policy incurs a regret upper bound
$\tilde{O}(\sqrt{T\sum_{k=1}^K T_k})$ where $T_k$ is the period of arm $k$.
Moreover, we establish a general lower bound $\Omega(\sqrt{T\max_{k}\{ T_k\}})$
for any policy. Therefore, our policy is near-optimal up to a factor of
$\sqrt{K}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Ningyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longlin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Inspired Top-k Adversarial Perturbations. (arXiv:2006.15669v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15669</id>
        <link href="http://arxiv.org/abs/2006.15669"/>
        <updated>2021-08-24T01:40:28.044Z</updated>
        <summary type="html"><![CDATA[The brittleness of deep image classifiers to small adversarial input
perturbations have been extensively studied in the last several years. However,
the main objective of existing perturbations is primarily limited to change the
correctly predicted Top-$1$ class by an incorrect one, which does not intend
changing the Top-$k$ prediction. In many digital real-world scenarios Top-$k$
prediction is more relevant. In this work, we propose a fast and accurate
method of computing Top-$k$ adversarial examples as a simple multi-objective
optimization. We demonstrate its efficacy and performance by comparing it to
other adversarial example crafting techniques. Moreover, based on this method,
we propose Top-$k$ Universal Adversarial Perturbations, image-agnostic tiny
perturbations that cause the true class to be absent among the Top-$k$
prediction for the majority of natural images. We experimentally show that our
approach outperforms baseline methods and even improves existing techniques of
generating Universal Adversarial Perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tursynbek_N/0/1/0/all/0/1"&gt;Nurislam Tursynbek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1"&gt;Aleksandr Petiushko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1"&gt;Ivan Oseledets&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRUGAL: Unlocking SSL for Software Analytics. (arXiv:2108.09847v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.09847</id>
        <link href="http://arxiv.org/abs/2108.09847"/>
        <updated>2021-08-24T01:40:28.037Z</updated>
        <summary type="html"><![CDATA[Standard software analytics often involves having a large amount of data with
labels in order to commission models with acceptable performance. However,
prior work has shown that such requirements can be expensive, taking several
weeks to label thousands of commits, and not always available when traversing
new research problems and domains. Unsupervised Learning is a promising
direction to learn hidden patterns within unlabelled data, which has only been
extensively studied in defect prediction. Nevertheless, unsupervised learning
can be ineffective by itself and has not been explored in other domains (e.g.,
static analysis and issue close time).

Motivated by this literature gap and technical limitations, we present
FRUGAL, a tuned semi-supervised method that builds on a simple optimization
scheme that does not require sophisticated (e.g., deep learners) and expensive
(e.g., 100% manually labelled data) methods. FRUGAL optimizes the unsupervised
learner's configurations (via a simple grid search) while validating our design
decision of labelling just 2.5% of the data before prediction.

As shown by the experiments of this paper FRUGAL outperforms the
state-of-the-art adoptable static code warning recognizer and issue closed time
predictor, while reducing the cost of labelling by a factor of 40 (from 100% to
2.5%). Hence we assert that FRUGAL can save considerable effort in data
labelling especially in validating prior work or researching new problems.

Based on this work, we suggest that proponents of complex and expensive
methods should always baseline such methods against simpler and cheaper
alternatives. For instance, a semi-supervised learner like FRUGAL can serve as
a baseline to the state-of-the-art software analytics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1"&gt;Huy Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1"&gt;Tim Menzies&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrUMAn: Trope Understanding in Movies and Animations. (arXiv:2108.04542v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04542</id>
        <link href="http://arxiv.org/abs/2108.04542"/>
        <updated>2021-08-24T01:40:28.023Z</updated>
        <summary type="html"><![CDATA[Understanding and comprehending video content is crucial for many real-world
applications such as search and recommendation systems. While recent progress
of deep learning has boosted performance on various tasks using visual cues,
deep cognition to reason intentions, motivation, or causality remains
challenging. Existing datasets that aim to examine video reasoning capability
focus on visual signals such as actions, objects, relations, or could be
answered utilizing text bias. Observing this, we propose a novel task, along
with a new dataset: Trope Understanding in Movies and Animations (TrUMAn), with
2423 videos associated with 132 tropes, intending to evaluate and develop
learning systems beyond visual signals. Tropes are frequently used storytelling
devices for creative works. By coping with the trope understanding task and
enabling the deep cognition skills of machines, data mining applications and
algorithms could be taken to the next level. To tackle the challenging TrUMAn
dataset, we present a Trope Understanding and Storytelling (TrUSt) with a new
Conceptual Storyteller module, which guides the video encoder by performing
video storytelling on a latent space. Experimental results demonstrate that
state-of-the-art learning systems on existing tasks reach only 12.01% of
accuracy with raw input signals. Also, even in the oracle case with
human-annotated descriptions, BERT contextual embedding achieves at most 28% of
accuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%
performance. We also provide detailed analysis to pave the way for future
research. TrUMAn is publicly available
at:https://www.cmlab.csie.ntu.edu.tw/project/trope]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1"&gt;Po-Wei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1"&gt;Bing-Chen Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wen-Feng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Ke-Jyun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Self-Adaptive Hashing for Image Retrieval. (arXiv:2108.07094v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07094</id>
        <link href="http://arxiv.org/abs/2108.07094"/>
        <updated>2021-08-24T01:40:28.017Z</updated>
        <summary type="html"><![CDATA[Hashing technology has been widely used in image retrieval due to its
computational and storage efficiency. Recently, deep unsupervised hashing
methods have attracted increasing attention due to the high cost of human
annotations in the real world and the superiority of deep learning technology.
However, most deep unsupervised hashing methods usually pre-compute a
similarity matrix to model the pairwise relationship in the pre-trained feature
space. Then this similarity matrix would be used to guide hash learning, in
which most of the data pairs are treated equivalently. The above process is
confronted with the following defects: 1) The pre-computed similarity matrix is
inalterable and disconnected from the hash learning process, which cannot
explore the underlying semantic information. 2) The informative data pairs may
be buried by the large number of less-informative data pairs. To solve the
aforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model
to adaptively capture the semantic information with two special designs:
Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).
Firstly, we adopt the AND to initially construct a neighborhood-based
similarity matrix, and then refine this initial similarity matrix with a novel
update strategy to further investigate the semantic structure behind the
learned representation. Secondly, we measure the priorities of data pairs with
PIC and assign adaptive weights to them, which is relies on the assumption that
more dissimilar data pairs contain more discriminative information for hash
learning. Extensive experiments on several datasets demonstrate that the above
two technologies facilitate the deep hashing model to achieve superior
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qinghong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1"&gt;Shangxuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yudong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning: A Signal Processing Perspective. (arXiv:2103.17150v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17150</id>
        <link href="http://arxiv.org/abs/2103.17150"/>
        <updated>2021-08-24T01:40:28.012Z</updated>
        <summary type="html"><![CDATA[The dramatic success of deep learning is largely due to the availability of
data. Data samples are often acquired on edge devices, such as smart phones,
vehicles and sensors, and in some cases cannot be shared due to privacy
considerations. Federated learning is an emerging machine learning paradigm for
training models across multiple edge devices holding local datasets, without
explicitly exchanging the data. Learning in a federated manner differs from
conventional centralized machine learning, and poses several core unique
challenges and requirements, which are closely related to classical problems
studied in the areas of signal processing and communications. Consequently,
dedicated schemes derived from these areas are expected to play an important
role in the success of federated learning and the transition of deep learning
from the domain of centralized servers to mobile edge devices. In this article,
we provide a unified systematic framework for federated learning in a manner
that encapsulates and highlights the main challenges that are natural to treat
using signal processing tools. We present a formulation for the federated
learning paradigm from a signal processing perspective, and survey a set of
candidate approaches for tackling its unique challenges. We further provide
guidelines for the design and adaptation of signal processing and communication
methods to facilitate federated learning at large scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gafni_T/0/1/0/all/0/1"&gt;Tomer Gafni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1"&gt;Nir Shlezinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_K/0/1/0/all/0/1"&gt;Kobi Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Bounds for Sparse Random Feature Expansions. (arXiv:2103.03191v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03191</id>
        <link href="http://arxiv.org/abs/2103.03191"/>
        <updated>2021-08-24T01:40:28.006Z</updated>
        <summary type="html"><![CDATA[Random feature methods have been successful in various machine learning
tasks, are easy to compute, and come with theoretical accuracy bounds. They
serve as an alternative approach to standard neural networks since they can
represent similar function spaces without a costly training phase. However, for
accuracy, random feature methods require more measurements than trainable
parameters, limiting their use for data-scarce applications or problems in
scientific machine learning. This paper introduces the sparse random feature
expansion to obtain parsimonious random feature models. Specifically, we
leverage ideas from compressive sensing to generate random feature expansions
with theoretical guarantees even in the data-scarce setting. In particular, we
provide generalization bounds for functions in a certain class (that is dense
in a reproducing kernel Hilbert space) depending on the number of samples and
the distribution of features. The generalization bounds improve with additional
structural conditions, such as coordinate sparsity, compact clusters of the
spectrum, or rapid spectral decay. In particular, by introducing sparse
features, i.e. features with random sparse weights, we provide improved bounds
for low order functions. We show that the sparse random feature expansions
outperforms shallow networks in several scientific machine learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hashemi_A/0/1/0/all/0/1"&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schaeffer_H/0/1/0/all/0/1"&gt;Hayden Schaeffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shi_R/0/1/0/all/0/1"&gt;Robert Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tran_G/0/1/0/all/0/1"&gt;Giang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ward_R/0/1/0/all/0/1"&gt;Rachel Ward&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KoDF: A Large-scale Korean DeepFake Detection Dataset. (arXiv:2103.10094v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10094</id>
        <link href="http://arxiv.org/abs/2103.10094"/>
        <updated>2021-08-24T01:40:27.984Z</updated>
        <summary type="html"><![CDATA[A variety of effective face-swap and face-reenactment methods have been
publicized in recent years, democratizing the face synthesis technology to a
great extent. Videos generated as such have come to be called deepfakes with a
negative connotation, for various social problems they have caused. Facing the
emerging threat of deepfakes, we have built the Korean DeepFake Detection
Dataset (KoDF), a large-scale collection of synthesized and real videos focused
on Korean subjects. In this paper, we provide a detailed description of methods
used to construct the dataset, experimentally show the discrepancy between the
distributions of KoDF and existing deepfake detection datasets, and underline
the importance of using multiple datasets for real-world generalization. KoDF
is publicly available at https://moneybrain-research.github.io/kodf in its
entirety (i.e. real clips, synthesized clips, clips with adversarial attack,
and metadata).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_P/0/1/0/all/0/1"&gt;Patrick Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jaeseong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1"&gt;Gyuhyeon Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chae_G/0/1/0/all/0/1"&gt;Gyeongsu Chae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of Information Transfer from Heterogeneous Sources via Precise High-dimensional Asymptotics. (arXiv:2010.11750v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11750</id>
        <link href="http://arxiv.org/abs/2010.11750"/>
        <updated>2021-08-24T01:40:27.970Z</updated>
        <summary type="html"><![CDATA[We consider the problem of transfer learning -- gaining knowledge from one
source task and applying it to a different but related target task. A
fundamental question in transfer learning is whether combining the data of both
tasks works better than using only the target task's data (equivalently,
whether a "positive information transfer" happens). We study this question
formally in a linear regression setting where a two-layer linear neural network
estimator combines both tasks' data. The estimator uses a shared parameter
vector for both tasks and exhibits positive or negative information transfer by
varying dataset characteristics.

We characterize the precise asymptotic limit of the prediction risk of the
above estimator when the sample sizes increase with the feature dimension
proportionally at fixed ratios. We also show that the asymptotic limit is
sufficiently accurate for finite dimensions. Then, we provide the exact
condition to determine positive (and negative) information transfer in a
random-effect model, leading to several theoretical insights. For example, the
risk curve is non-monotone under model shift, thus motivating a transfer
learning procedure that progressively adds data from the source task. We
validate this procedure's efficiency on text classification tasks with a neural
network that applies a shared feature space for both tasks, similar to the
above estimator. The main ingredient of the analysis is finding the
high-dimensional asymptotic limits of various functions involving the sum of
two independent sample covariance matrices with different population covariance
matrices, which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyang R. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition. (arXiv:2102.01063v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01063</id>
        <link href="http://arxiv.org/abs/2102.01063"/>
        <updated>2021-08-24T01:40:27.936Z</updated>
        <summary type="html"><![CDATA[Accuracy predictor is a key component in Neural Architecture Search (NAS) for
ranking architectures. Building a high-quality accuracy predictor usually costs
enormous computation. To address this issue, instead of using an accuracy
predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the
architectures. The Zen-Score represents the network expressivity and positively
correlates with the model accuracy. The calculation of Zen-Score only takes a
few forward inferences through a randomly initialized network, without training
network parameters. Built upon the Zen-Score, we further propose a new NAS
algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network
under given inference budgets. Within less than half GPU day, Zen-NAS is able
to directly search high performance architectures in a data-free style.
Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times
faster on multiple server-side and mobile-side GPU platforms with
state-of-the-art accuracy on ImageNet. Our source code and pre-trained models
are released on https://github.com/idstcv/ZenNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Ming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pichao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenhong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hesen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1"&gt;Qi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Borrowing for Generalized Zero-Shot Learning. (arXiv:2102.04969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04969</id>
        <link href="http://arxiv.org/abs/2102.04969"/>
        <updated>2021-08-24T01:40:27.918Z</updated>
        <summary type="html"><![CDATA[Generalized zero-shot learning (GZSL) is one of the most realistic but
challenging problems due to the partiality of the classifier to supervised
classes, especially under the class-inductive instance-inductive (CIII)
training setting, where testing data are not available. Instance-borrowing
methods and synthesizing methods solve it to some extent with the help of
testing semantics, but therefore neither can be used under CIII. Besides, the
latter require the training process of a classifier after generating examples.
In contrast, a novel non-transductive regularization under CIII called Semantic
Borrowing (SB) for improving GZSL methods with compatibility metric learning is
proposed in this paper, which not only can be used for training linear models,
but also nonlinear ones such as artificial neural networks. This regularization
item in the loss function borrows similar semantics in the training set, so
that the classifier can model the relationship between the semantics of
zero-shot and supervised classes more accurately during training. In practice,
the information of semantics of unknown classes would not be available for
training while this approach does NOT need it. Extensive experiments on GZSL
benchmark datasets show that SB can reduce the partiality of the classifier to
supervised classes and improve the performance of generalized zero-shot
classification, surpassing inductive GZSL state of the arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaowei Chen&lt;/a&gt; (Sun Yat-sen University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STL Robustness Risk over Discrete-Time Stochastic Processes. (arXiv:2104.01503v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01503</id>
        <link href="http://arxiv.org/abs/2104.01503"/>
        <updated>2021-08-24T01:40:27.907Z</updated>
        <summary type="html"><![CDATA[We present a framework to interpret signal temporal logic (STL) formulas over
discrete-time stochastic processes in terms of the induced risk. Each
realization of a stochastic process either satisfies or violates an STL
formula. In fact, we can assign a robustness value to each realization that
indicates how robustly this realization satisfies an STL formula. We then
define the risk of a stochastic process not satisfying an STL formula robustly,
referred to as the STL robustness risk. In our definition, we permit general
classes of risk measures such as, but not limited to, the conditional
value-at-risk. While in general hard to compute, we propose an approximation of
the STL robustness risk. This approximation has the desirable property of being
an upper bound of the STL robustness risk when the chosen risk measure is
monotone, a property satisfied by most risk measures. Motivated by the interest
in data-driven approaches, we present a sampling-based method for estimating
the approximate STL robustness risk from data for the value-at-risk. While we
consider the value-at-risk, we highlight that such sampling-based methods are
viable for other risk measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lindemann_L/0/1/0/all/0/1"&gt;Lars Lindemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Matni_N/0/1/0/all/0/1"&gt;Nikolai Matni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rainfall-runoff prediction using a Gustafson-Kessel clustering based Takagi-Sugeno Fuzzy model. (arXiv:2108.09684v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09684</id>
        <link href="http://arxiv.org/abs/2108.09684"/>
        <updated>2021-08-24T01:40:27.828Z</updated>
        <summary type="html"><![CDATA[A rainfall-runoff model predicts surface runoff either using a
physically-based approach or using a systems-based approach. Takagi-Sugeno (TS)
Fuzzy models are systems-based approaches and a popular modeling choice for
hydrologists in recent decades due to several advantages and improved accuracy
in prediction over other existing models. In this paper, we propose a new
rainfall-runoff model developed using Gustafson-Kessel (GK) clustering-based TS
Fuzzy model. We present comparative performance measures of GK algorithms with
two other clustering algorithms: (i) Fuzzy C-Means (FCM), and (ii)Subtractive
Clustering (SC). Our proposed TS Fuzzy model predicts surface runoff using: (i)
observed rainfall in a drainage basin and (ii) previously observed
precipitation flow in the basin outlet. The proposed model is validated using
the rainfall-runoff data collected from the sensors installed on the campus of
the Indian Institute of Technology, Kharagpur. The optimal number of rules of
the proposed model is obtained by different validation indices. A comparative
study of four performance criteria: RootMean Square Error (RMSE), Coefficient
of Efficiency (CE), Volumetric Error (VE), and Correlation Coefficient of
Determination(R) have been quantitatively demonstrated for each clustering
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1"&gt;Subhrasankha Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1"&gt;Tanmoy Dam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Shared Representations for Personalized Federated Learning. (arXiv:2102.07078v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07078</id>
        <link href="http://arxiv.org/abs/2102.07078"/>
        <updated>2021-08-24T01:40:27.818Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have shown the ability to extract universal feature
representations from data such as images and text that have been useful for a
variety of learning tasks. However, the fruits of representation learning have
yet to be fully-realized in federated settings. Although data in federated
settings is often non-i.i.d. across clients, the success of centralized deep
learning suggests that data often shares a global feature representation, while
the statistical heterogeneity across clients or tasks is concentrated in the
labels. Based on this intuition, we propose a novel federated learning
framework and algorithm for learning a shared data representation across
clients and unique local heads for each client. Our algorithm harnesses the
distributed computational power across clients to perform many local-updates
with respect to the low-dimensional local parameters for every update of the
representation. We prove that this method obtains linear convergence to the
ground-truth representation with near-optimal sample complexity in a linear
setting, demonstrating that it can efficiently reduce the problem dimension for
each client. This result is of interest beyond federated learning to a broad
class of problems in which we aim to learn a shared low-dimensional
representation among data distributions, for example in meta-learning and
multi-task learning. Further, extensive experimental results show the empirical
improvement of our method over alternative personalized federated learning
approaches in federated environments with heterogeneous data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Collins_L/0/1/0/all/0/1"&gt;Liam Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1"&gt;Hamed Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtari_A/0/1/0/all/0/1"&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Directed Online Machine Learning for Topology Optimization. (arXiv:2002.01927v7 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01927</id>
        <link href="http://arxiv.org/abs/2002.01927"/>
        <updated>2021-08-24T01:40:27.813Z</updated>
        <summary type="html"><![CDATA[Topology optimization by optimally distributing materials in a given domain
requires gradient-free optimizers to solve highly complicated problems.
However, with hundreds of design variables or more involved, solving such
problems would require millions of Finite Element Method (FEM) calculations
whose computational cost is huge and impractical. Here we report Self-directed
Online Learning Optimization (SOLO) which integrates Deep Neural Network (DNN)
with FEM calculations. A DNN learns and substitutes the objective as a function
of design variables. A small number of training data is generated dynamically
based on the DNN's prediction of the global optimum. The DNN adapts to the new
training data and gives better prediction in the region of interest until
convergence. Our algorithm was tested by four types of problems including
compliance minimization, fluid-structure optimization, heat transfer
enhancement and truss optimization. It reduced the computational time by 2 ~ 5
orders of magnitude compared with directly using heuristic methods, and
outperformed all state-of-the-art algorithms tested in our experiments. This
approach enables solving large multi-dimensional optimization problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Changyu Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1"&gt;Can Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative and Contrastive Self-Supervised Learning for Graph Anomaly Detection. (arXiv:2108.09896v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09896</id>
        <link href="http://arxiv.org/abs/2108.09896"/>
        <updated>2021-08-24T01:40:27.806Z</updated>
        <summary type="html"><![CDATA[Anomaly detection from graph data has drawn much attention due to its
practical significance in many critical applications including cybersecurity,
finance, and social networks. Existing data mining and machine learning methods
are either shallow methods that could not effectively capture the complex
interdependency of graph data or graph autoencoder methods that could not fully
exploit the contextual information as supervision signals for effective anomaly
detection. To overcome these challenges, in this paper, we propose a novel
method, Self-Supervised Learning for Graph Anomaly Detection (SL-GAD). Our
method constructs different contextual subgraphs (views) based on a target node
and employs two modules, generative attribute regression and multi-view
contrastive learning for anomaly detection. While the generative attribute
regression module allows us to capture the anomalies in the attribute space,
the multi-view contrastive learning module can exploit richer structure
information from multiple subgraphs, thus abling to capture the anomalies in
the structure space, mixing of structure, and attribute information. We conduct
extensive experiments on six benchmark datasets and the results demonstrate
that our method outperforms state-of-the-art methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Ming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_L/0/1/0/all/0/1"&gt;Lianhua Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1"&gt;Khoa T. Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ping Phoebe Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Democratic Source Coding: An Optimal Fixed-Length Quantization Scheme for Distributed Optimization Under Communication Constraints. (arXiv:2103.07578v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07578</id>
        <link href="http://arxiv.org/abs/2103.07578"/>
        <updated>2021-08-24T01:40:27.800Z</updated>
        <summary type="html"><![CDATA[The communication cost of distributed optimization algorithms is a major
bottleneck in their scalability. This work considers a parameter-server setting
in which the worker is constrained to communicate information to the server
using only $R$ bits per dimension. We show that $\mathbf{democratic}$
$\mathbf{embeddings}$ from random matrix theory are significantly useful for
designing efficient and optimal vector quantizers that respect this bit budget.
The resulting polynomial complexity source coding schemes are used to design
distributed optimization algorithms with convergence rates matching the minimax
optimal lower bounds for (i) Smooth and Strongly-Convex objectives with access
to an Exact Gradient oracle, as well as (ii) General Convex and Non-Smooth
objectives with access to a Noisy Subgradient oracle. We further propose a
relaxation of this coding scheme which is nearly minimax optimal. Numerical
simulations validate our theoretical claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1"&gt;Rajarshi Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldsmith_A/0/1/0/all/0/1"&gt;Andrea J. Goldsmith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Image Coding for Machines: A Content-Adaptive Approach. (arXiv:2108.09992v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09992</id>
        <link href="http://arxiv.org/abs/2108.09992"/>
        <updated>2021-08-24T01:40:27.793Z</updated>
        <summary type="html"><![CDATA[Today, according to the Cisco Annual Internet Report (2018-2023), the
fastest-growing category of Internet traffic is machine-to-machine
communication. In particular, machine-to-machine communication of images and
videos represents a new challenge and opens up new perspectives in the context
of data compression. One possible solution approach consists of adapting
current human-targeted image and video coding standards to the use case of
machine consumption. Another approach consists of developing completely new
compression paradigms and architectures for machine-to-machine communications.
In this paper, we focus on image compression and present an inference-time
content-adaptive finetuning scheme that optimizes the latent representation of
an end-to-end learned image codec, aimed at improving the compression
efficiency for machine-consumption. The conducted experiments show that our
online finetuning brings an average bitrate saving (BD-rate) of -3.66% with
respect to our pretrained image codec. In particular, at low bitrate points,
our proposed method results in a significant bitrate saving of -9.85%. Overall,
our pretrained-and-then-finetuned system achieves -30.54% BD-rate over the
state-of-the-art image/video codec Versatile Video Coding (VVC).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1"&gt;Nam Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cricri_F/0/1/0/all/0/1"&gt;Francesco Cricri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1"&gt;Ramin Ghaznavi-Youvalari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tavakoli_H/0/1/0/all/0/1"&gt;Hamed Rezazadegan Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Primal and Dual Combinatorial Dimensions. (arXiv:2108.10037v1 [math.CO])]]></title>
        <id>http://arxiv.org/abs/2108.10037</id>
        <link href="http://arxiv.org/abs/2108.10037"/>
        <updated>2021-08-24T01:40:27.776Z</updated>
        <summary type="html"><![CDATA[We give tight bounds on the relation between the primal and dual of various
combinatorial dimensions, such as the pseudo-dimension and fat-shattering
dimension, for multi-valued function classes. These dimensional notions play an
important role in the area of learning theory. We first review some (folklore)
results that bound the dual dimension of a function class in terms of its
primal, and after that give (almost) matching lower bounds. In particular, we
give an appropriate generalization to multi-valued function classes of a
well-known bound due to Assouad (1983), that relates the primal and dual
VC-dimension of a binary function class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kleer_P/0/1/0/all/0/1"&gt;Pieter Kleer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simon_H/0/1/0/all/0/1"&gt;Hans Simon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slice Tuner: A Selective Data Acquisition Framework for Accurate and Fair Machine Learning Models. (arXiv:2003.04549v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04549</id>
        <link href="http://arxiv.org/abs/2003.04549"/>
        <updated>2021-08-24T01:40:27.770Z</updated>
        <summary type="html"><![CDATA[As machine learning becomes democratized in the era of Software 2.0, a
serious bottleneck is acquiring enough data to ensure accurate and fair models.
Recent techniques including crowdsourcing provide cost-effective ways to gather
such data. However, simply acquiring data as much as possible is not
necessarily an effective strategy for optimizing accuracy and fairness. For
example, if an online app store has enough training data for certain slices of
data (say American customers), but not for others, obtaining more American
customer data will only bias the model training. Instead, we contend that one
needs to selectively acquire data and propose Slice Tuner, which acquires
possibly-different amounts of data per slice such that the model accuracy and
fairness on all slices are optimized. This problem is different than labeling
existing data (as in active learning or weak supervision) because the goal is
obtaining the right amounts of new data. At its core, Slice Tuner maintains
learning curves of slices that estimate the model accuracies given more data
and uses convex optimization to find the best data acquisition strategy. The
key challenges of estimating learning curves are that they may be inaccurate if
there is not enough data, and there may be dependencies among slices where
acquiring data for one slice influences the learning curves of others. We solve
these issues by iteratively and efficiently updating the learning curves as
more data is acquired. We evaluate Slice Tuner on real datasets using
crowdsourcing for data acquisition and show that Slice Tuner significantly
outperforms baselines in terms of model accuracy and fairness, even when the
learning curves cannot be reliably estimated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tae_K/0/1/0/all/0/1"&gt;Ki Hyun Tae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whang_S/0/1/0/all/0/1"&gt;Steven Euijong Whang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Quantitative Approach for Optimizing Convolutional Neural Networks. (arXiv:2009.05236v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05236</id>
        <link href="http://arxiv.org/abs/2009.05236"/>
        <updated>2021-08-24T01:40:27.764Z</updated>
        <summary type="html"><![CDATA[With the increasing popularity of deep learning, Convolutional Neural
Networks (CNNs) have been widely applied in various domains, such as image
classification and object detection, and achieve stunning success in terms of
their high accuracy over the traditional statistical methods. To exploit the
potential of CNN models, a huge amount of research and industry efforts have
been devoted to optimizing CNNs. Among these endeavors, CNN architecture design
has attracted tremendous attention because of its great potential of improving
model accuracy or reducing model complexity. However, existing work either
introduces repeated training overhead in the search process or lacks an
interpretable metric to guide the design. To clear these hurdles, we propose
Information Field (IF), an explainable and easy-to-compute metric, to estimate
the quality of a CNN architecture and guide the search process of designs. To
validate the effectiveness of IF, we build a static optimizer to improve the
CNN architectures at both the stage level and the kernel level. Our optimizer
not only provides a clear and reproducible procedure but also mitigates
unnecessary training efforts in the architecture search process. Extensive
experiments and studies show that the models generated by our optimizer can
achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction,
compared with state-of-the-art CNN structures like MobileNet and ResNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Boyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xueqiao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yufei Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling COVID-19 uncertainties evolving over time and density-dependent social reinforcement and asymptomatic infections. (arXiv:2108.10029v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.10029</id>
        <link href="http://arxiv.org/abs/2108.10029"/>
        <updated>2021-08-24T01:40:27.744Z</updated>
        <summary type="html"><![CDATA[The novel coronavirus disease 2019 (COVID-19) presents unique and unknown
problem complexities and modeling challenges, where an imperative task is to
model both its process and data uncertainties, represented in implicit and
high-proportional undocumented infections, asymptomatic contagion, social
reinforcement of infections, and various quality issues in the reported data.
These uncertainties become even more phenomenal in the overwhelming
mutation-dominated resurgences with vaccinated but still susceptible
populations. Here we introduce a novel hybrid approach to (1) characterizing
and distinguishing Undocumented (U) and Documented (D) infections commonly seen
during COVID-19 incubation periods and asymptomatic infections by expanding the
foundational compartmental epidemic Susceptible-Infected-Recovered (SIR) model
with two compartments, resulting in a new Susceptible-Undocumented
infected-Documented infected-Recovered (SUDR) model; (2) characterizing the
probabilistic density of infections by empowering SUDR to capture exogenous
processes like clustering contagion interactions, superspreading and social
reinforcement; and (3) approximating the density likelihood of COVID-19
prevalence over time by incorporating Bayesian inference into SUDR. Different
from existing COVID-19 models, SUDR characterizes the undocumented infections
during unknown transmission processes. To capture the uncertainties of temporal
transmission and social reinforcement during the COVID-19 contagion, the
transmission rate is modeled by a time-varying density function of undocumented
infectious cases. We solve the modeling by sampling from the mean-field
posterior distribution with reasonable priors, making SUDR suitable to handle
the randomness, noise and sparsity of COVID-19 observations widely seen in the
public COVID-19 case data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Extensible and Modular Design and Implementation of Monte Carlo Tree Search for the JVM. (arXiv:2108.10061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.10061</id>
        <link href="http://arxiv.org/abs/2108.10061"/>
        <updated>2021-08-24T01:40:27.738Z</updated>
        <summary type="html"><![CDATA[Flexible implementations of Monte Carlo Tree Search (MCTS), combined with
domain specific knowledge and hybridization with other search algorithms, can
be powerful for finding the solutions to problems in complex planning. We
introduce mctreesearch4j, an MCTS implementation written as a standard JVM
library following key design principles of object oriented programming. We
define key class abstractions allowing the MCTS library to flexibly adapt to
any well defined Markov Decision Process or turn-based adversarial game.
Furthermore, our library is designed to be modular and extensible, utilizing
class inheritance and generic typing to standardize custom algorithm
definitions. We demonstrate that the design of the MCTS implementation provides
ease of adaptation for unique heuristics and customization across varying
Markov Decision Process (MDP) domains. In addition, the implementation is
reasonably performant and accurate for standard MDP's. In addition, via the
implementation of mctreesearch4j, the nuances of different types of MCTS
algorithms are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Larkin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jun Tao Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Transformer Architecture for Stress Detection from ECG. (arXiv:2108.09737v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.09737</id>
        <link href="http://arxiv.org/abs/2108.09737"/>
        <updated>2021-08-24T01:40:27.732Z</updated>
        <summary type="html"><![CDATA[Electrocardiogram (ECG) has been widely used for emotion recognition. This
paper presents a deep neural network based on convolutional layers and a
transformer mechanism to detect stress using ECG signals. We perform
leave-one-subject-out experiments on two publicly available datasets, WESAD and
SWELL-KW, to evaluate our method. Our experiments show that the proposed model
achieves strong results, comparable or better than the state-of-the-art models
for ECG-based stress detection on these two datasets. Moreover, our method is
end-to-end, does not require handcrafted features, and can learn robust
representations with only a few convolutional blocks and the transformer
component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Behinaein_B/0/1/0/all/0/1"&gt;Behnam Behinaein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhatti_A/0/1/0/all/0/1"&gt;Anubhav Bhatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodenburg_D/0/1/0/all/0/1"&gt;Dirk Rodenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hungler_P/0/1/0/all/0/1"&gt;Paul Hungler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Gaussian Neural Processes for Regression. (arXiv:2108.09676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09676</id>
        <link href="http://arxiv.org/abs/2108.09676"/>
        <updated>2021-08-24T01:40:27.727Z</updated>
        <summary type="html"><![CDATA[Conditional Neural Processes (CNP; Garnelo et al., 2018) are an attractive
family of meta-learning models which produce well-calibrated predictions,
enable fast inference at test time, and are trainable via a simple maximum
likelihood procedure. A limitation of CNPs is their inability to model
dependencies in the outputs. This significantly hurts predictive performance
and renders it impossible to draw coherent function samples, which limits the
applicability of CNPs in down-stream applications and decision making.
NeuralProcesses (NPs; Garnelo et al., 2018) attempt to alleviate this issue by
using latent variables, rely-ing on these to model output dependencies, but
introduces difficulties stemming from approximate inference. One recent
alternative (Bruinsma et al.,2021), which we refer to as the FullConvGNP,
models dependencies in the predictions while still being trainable via exact
maximum-likelihood.Unfortunately, the FullConvGNP relies on expensive
2D-dimensional convolutions, which limit its applicability to only
one-dimensional data.In this work, we present an alternative way to model
output dependencies which also lends it-self maximum likelihood training but,
unlike the FullConvGNP, can be scaled to two- and three-dimensional data. The
proposed models exhibit good performance in synthetic experiments]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markou_S/0/1/0/all/0/1"&gt;Stratis Markou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Requeima_J/0/1/0/all/0/1"&gt;James Requeima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruinsma_W/0/1/0/all/0/1"&gt;Wessel Bruinsma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Richard Turner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subject Envelope based Multitype Reconstruction Algorithm of Speech Samples of Parkinson's Disease. (arXiv:2108.09922v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.09922</id>
        <link href="http://arxiv.org/abs/2108.09922"/>
        <updated>2021-08-24T01:40:27.720Z</updated>
        <summary type="html"><![CDATA[The risk of Parkinson's disease (PD) is extremely serious, and PD speech
recognition is an effective method of diagnosis nowadays. However, due to the
influence of the disease stage, corpus, and other factors on data collection,
the ability of every samples within one subject to reflect the status of PD
vary. No samples are useless totally, and not samples are 100% perfect. This
characteristic means that it is not suitable just to remove some samples or
keep some samples. It is necessary to consider the sample transformation for
obtaining high quality new samples. Unfortunately, existing PD speech
recognition methods focus mainly on feature learning and classifier design
rather than sample learning, and few methods consider the sample
transformation. To solve the problem above, a PD speech sample transformation
algorithm based on multitype reconstruction operators is proposed in this
paper. The algorithm is divided into four major steps. Three types of
reconstruction operators are designed in the algorithm: types A, B and C.
Concerning the type A operator, the original dataset is directly reconstructed
by designing a linear transformation to obtain the first dataset. The type B
operator is designed for clustering and linear transformation of the dataset to
obtain the second new dataset. The third operator, namely, the type C operator,
reconstructs the dataset by clustering and convolution to obtain the third
dataset. Finally, the base classifier is trained based on the three new
datasets, and then the classification results are fused by decision weighting.
In the experimental section, two representative PD speech datasets are used for
verification. The results show that the proposed algorithm is effective.
Compared with other algorithms, the proposed algorithm achieves apparent
improvements in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chengyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hehua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_A/0/1/0/all/0/1"&gt;Anhai Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized and Smooth Double Core Tensor Factorization for Heterogeneous Data. (arXiv:1911.10454v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.10454</id>
        <link href="http://arxiv.org/abs/1911.10454"/>
        <updated>2021-08-24T01:40:27.714Z</updated>
        <summary type="html"><![CDATA[We introduce a general tensor model suitable for data analytic tasks for {\em
heterogeneous} datasets, wherein there are joint low-rank structures within
groups of observations, but also discriminative structures across different
groups. To capture such complex structures, a double core tensor (DCOT)
factorization model is introduced together with a family of smoothing loss
functions. By leveraging the proposed smoothing function, the model accurately
estimates the model factors, even in the presence of missing entries. A
linearized ADMM method is employed to solve regularized versions of DCOT
factorizations, that avoid large tensor operations and large memory storage
requirements. Further, we establish theoretically its global convergence,
together with consistency of the estimates of the model parameters. The
effectiveness of the DCOT model is illustrated on several real-world examples
including image completion, recommender systems, subspace clustering and
detecting modules in heterogeneous Omics multi-modal data, since it provides
more insightful decompositions than conventional tensor methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tarzanagh_D/0/1/0/all/0/1"&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Michailidis_G/0/1/0/all/0/1"&gt;George Michailidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safeguarded Dynamic Label Regression for Generalized Noisy Supervision. (arXiv:1903.02152v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.02152</id>
        <link href="http://arxiv.org/abs/1903.02152"/>
        <updated>2021-08-24T01:40:27.708Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels, which aims to reduce expensive labors on accurate
annotations, has become imperative in the Big Data era. Previous noise
transition based method has achieved promising results and presented a
theoretical guarantee on performance in the case of class-conditional noise.
However, this type of approaches critically depend on an accurate
pre-estimation of the noise transition, which is usually impractical.
Subsequent improvement adapts the pre-estimation along with the training
progress via a Softmax layer. However, the parameters in the Softmax layer are
highly tweaked for the fragile performance due to the ill-posed stochastic
approximation. To address these issues, we propose a Latent Class-Conditional
Noise model (LCCN) that naturally embeds the noise transition under a Bayesian
framework. By projecting the noise transition into a Dirichlet-distributed
space, the learning is constrained on a simplex based on the whole dataset,
instead of some ad-hoc parametric space. We then deduce a dynamic label
regression method for LCCN to iteratively infer the latent labels, to
stochastically train the classifier and to model the noise. Our approach
safeguards the bounded update of the noise transition, which avoids previous
arbitrarily tuning via a batch of samples. We further generalize LCCN for
open-set noisy labels and the semi-supervised setting. We perform extensive
experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and
the agnostic noise data sets, Clothing1M and WebVision17. The experimental
results have demonstrated that the proposed model outperforms several
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W. Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning Meets Fairness and Differential Privacy. (arXiv:2108.09932v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09932</id>
        <link href="http://arxiv.org/abs/2108.09932"/>
        <updated>2021-08-24T01:40:27.702Z</updated>
        <summary type="html"><![CDATA[Deep learning's unprecedented success raises several ethical concerns ranging
from biased predictions to data privacy. Researchers tackle these issues by
introducing fairness metrics, or federated learning, or differential privacy. A
first, this work presents an ethical federated learning model, incorporating
all three measures simultaneously. Experiments on the Adult, Bank and Dutch
datasets highlight the resulting ``empirical interplay" between accuracy,
fairness, and privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padala_M/0/1/0/all/0/1"&gt;Manisha Padala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damle_S/0/1/0/all/0/1"&gt;Sankarshan Damle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1"&gt;Sujit Gujar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anarchic Federated Learning. (arXiv:2108.09875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09875</id>
        <link href="http://arxiv.org/abs/2108.09875"/>
        <updated>2021-08-24T01:40:27.684Z</updated>
        <summary type="html"><![CDATA[Present-day federated learning (FL) systems deployed over edge networks have
to consistently deal with a large number of workers with high degrees of
heterogeneity in data and/or computing capabilities. This diverse set of
workers necessitates the development of FL algorithms that allow: (1) flexible
worker participation that grants the workers' capability to engage in training
at will, (2) varying number of local updates (based on computational resources)
at each worker along with asynchronous communication with the server, and (3)
heterogeneous data across workers. To address these challenges, in this work,
we propose a new paradigm in FL called ``Anarchic Federated Learning'' (AFL).
In stark contrast to conventional FL models, each worker in AFL has complete
freedom to choose i) when to participate in FL, and ii) the number of local
steps to perform in each round based on its current situation (e.g., battery
level, communication channels, privacy concerns). However, AFL also introduces
significant challenges in algorithmic design because the server needs to handle
the chaotic worker behaviors. Toward this end, we propose two Anarchic
FedAvg-like algorithms with two-sided learning rates for both cross-device and
cross-silo settings, which are named AFedAvg-TSLR-CD and AFedAvg-TSLR-CS,
respectively. For general worker information arrival processes, we show that
both algorithms retain the highly desirable linear speedup effect in the new
AFL paradigm. Moreover, we show that our AFedAvg-TSLR algorithmic framework can
be viewed as a {\em meta-algorithm} for AFL in the sense that they can utilize
advanced FL algorithms as worker- and/or server-side optimizers to achieve
enhanced performance under AFL. We validate the proposed algorithms with
extensive experiments on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1"&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective and Privacy preserving Tabular Data Synthesizing. (arXiv:2108.10064v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.10064</id>
        <link href="http://arxiv.org/abs/2108.10064"/>
        <updated>2021-08-24T01:40:27.671Z</updated>
        <summary type="html"><![CDATA[While data sharing is crucial for knowledge development, privacy concerns and
strict regulation (e.g., European General Data Protection Regulation (GDPR))
unfortunately limits its full effectiveness. Synthetic tabular data emerges as
an alternative to enable data sharing while fulfilling regulatory and privacy
constraints. The state-of-the-art tabular data synthesizers draw methodologies
from Generative Adversarial Networks (GAN). In this thesis, we develop
CTAB-GAN, a novel conditional table GAN architecture that can effectively model
diverse data types with complex distributions. CTAB-GAN is extensively
evaluated with the state of the art GANs that generate synthetic tables, in
terms of data similarity and analysis utility. The results on five datasets
show that the synthetic data of CTAB-GAN remarkably resembles the real data for
all three types of variables and results in higher accuracy for five machine
learning algorithms, by up to 17%.

Additionally, to ensure greater security for training tabular GANs against
malicious privacy attacks, differential privacy (DP) is studied and used to
train CTAB-GAN with strict privacy guarantees. DP-CTAB-GAN is rigorously
evaluated using state-of-the-art DP-tabular GANs in terms of data utility and
privacy robustness against membership and attribute inference attacks. Our
results on three datasets indicate that strict theoretical differential privacy
guarantees come only after severely affecting data utility. However, it is
shown empirically that these guarantees help provide a stronger defence against
privacy attacks. Overall, it is found that DP-CTABGAN is capable of being
robust to privacy attacks while maintaining the highest data utility as
compared to prior work, by up to 18% in terms of the average precision score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1"&gt;Aditya Kunar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating LSTMs and GNNs for COVID-19 Forecasting. (arXiv:2108.10052v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.10052</id>
        <link href="http://arxiv.org/abs/2108.10052"/>
        <updated>2021-08-24T01:40:27.652Z</updated>
        <summary type="html"><![CDATA[The spread of COVID-19 has coincided with the rise of Graph Neural Networks
(GNNs), leading to several studies proposing their use to better forecast the
evolution of the pandemic. Many such models also include Long Short Term Memory
(LSTM) networks, a common tool for time series forecasting. In this work, we
further investigate the integration of these two methods by implementing GNNs
within the gates of an LSTM and exploiting spatial information. In addition, we
introduce a skip connection which proves critical to jointly capture the
spatial and temporal patterns in the data. We validate our daily COVID-19 new
cases forecast model on data of 37 European nations for the last 472 days and
show superior performance compared to state-of-the-art graph time series models
based on mean absolute scaled error (MASE). This area of research has important
applications to policy-making and we analyze its potential for pandemic
resource control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sesti_N/0/1/0/all/0/1"&gt;Nathan Sesti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garau_Luis_J/0/1/0/all/0/1"&gt;Juan Jose Garau-Luis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crawley_E/0/1/0/all/0/1"&gt;Edward Crawley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cameron_B/0/1/0/all/0/1"&gt;Bruce Cameron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment. (arXiv:2108.09980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09980</id>
        <link href="http://arxiv.org/abs/2108.09980"/>
        <updated>2021-08-24T01:40:27.630Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been widely used to train transformer-based
vision-language models for video-text alignment and multi-modal representation
learning. This paper presents a new algorithm called Token-Aware Cascade
contrastive learning (TACo) that improves contrastive learning using two novel
techniques. The first is the token-aware contrastive loss which is computed by
taking into account the syntactic classes of words. This is motivated by the
observation that for a video-text pair, the content words in the text, such as
nouns and verbs, are more likely to be aligned with the visual contents in the
video than the function words. Second, a cascade sampling method is applied to
generate a small set of hard negative examples for efficient loss estimation
for multi-modal fusion layers. To validate the effectiveness of TACo, in our
experiments we finetune pretrained models for a set of downstream tasks
including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video
action step localization (CrossTask), video action segmentation (COIN). The
results show that our models attain consistent improvements across different
experimental settings over previous methods, setting new state-of-the-art on
three public text-video retrieval benchmarks of YouCook2, MSR-VTT and
ActivityNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Boosting Approach to Reinforcement Learning. (arXiv:2108.09767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09767</id>
        <link href="http://arxiv.org/abs/2108.09767"/>
        <updated>2021-08-24T01:40:27.623Z</updated>
        <summary type="html"><![CDATA[We study efficient algorithms for reinforcement learning in Markov decision
processes whose complexity is independent of the number of states. This
formulation succinctly captures large scale problems, but is also known to be
computationally hard in its general form. Previous approaches attempt to
circumvent the computational hardness by assuming structure in either
transition function or the value function, or by relaxing the solution
guarantee to a local optimality condition.

We consider the methodology of boosting, borrowed from supervised learning,
for converting weak learners into an accurate policy. The notion of weak
learning we study is that of sampled-based approximate optimization of linear
functions over policies. Under this assumption of weak learnability, we give an
efficient algorithm that is capable of improving the accuracy of such weak
learning methods, till global optimality is reached. We prove sample complexity
and running time bounds on our method, that are polynomial in the natural
parameters of the problem: approximation guarantee, discount factor,
distribution mismatch and number of actions. In particular, our bound does not
depend on the number of states.

A technical difficulty in applying previous boosting results, is that the
value function over policy space is not convex. We show how to use a non-convex
variant of the Frank-Wolfe method, coupled with recent advances in gradient
boosting that allow incorporating a weak learner with multiplicative
approximation guarantee, to overcome the non-convexity and attain global
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brukhim_N/0/1/0/all/0/1"&gt;Nataly Brukhim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1"&gt;Elad Hazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Karan Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APObind: A Dataset of Ligand Unbound Protein Conformations for Machine Learning Applications in De Novo Drug Design. (arXiv:2108.09926v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2108.09926</id>
        <link href="http://arxiv.org/abs/2108.09926"/>
        <updated>2021-08-24T01:40:27.617Z</updated>
        <summary type="html"><![CDATA[Protein-ligand complex structures have been utilised to design benchmark
machine learning methods that perform important tasks related to drug design
such as receptor binding site detection, small molecule docking and binding
affinity prediction. However, these methods are usually trained on only ligand
bound (or holo) conformations of the protein and therefore are not guaranteed
to perform well when the protein structure is in its native unbound
conformation (or apo), which is usually the conformation available for a newly
identified receptor. A primary reason for this is that the local structure of
the binding site usually changes upon ligand binding. To facilitate solutions
for this problem, we propose a dataset called APObind that aims to provide apo
conformations of proteins present in the PDBbind dataset, a popular dataset
used in drug design. Furthermore, we explore the performance of methods
specific to three use cases on this dataset, through which, the importance of
validating them on the APObind dataset is demonstrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Aggarwal_R/0/1/0/all/0/1"&gt;Rishal Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Akash Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Priyakumar_U/0/1/0/all/0/1"&gt;U Deva Priyakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation Using Many-To-Many RNNs for Session-Aware Recommender Systems. (arXiv:2108.09858v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09858</id>
        <link href="http://arxiv.org/abs/2108.09858"/>
        <updated>2021-08-24T01:40:27.612Z</updated>
        <summary type="html"><![CDATA[The ACM WSDM WebTour 2021 Challenge organized by Booking.com focuses on
applying Session-Aware recommender systems in the travel domain. Given a
sequence of travel bookings in a user trip, we look to recommend the user's
next destination. To handle the large dimensionality of the output's space, we
propose a many-to-many RNN model, predicting the next destination chosen by the
user at every sequence step as opposed to only the final one. We show how this
is a computationally efficient alternative to doing data augmentation in a
many-to-one RNN, where we consider every subsequence of a session starting from
the first element. Our solution achieved 4th place in the final leaderboard,
with an accuracy@4 of 0.5566.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_M/0/1/0/all/0/1"&gt;Mart&amp;#xed;n Baigorria Alonso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Clustered Federated Learning for Client-Level Data Distribution Shift. (arXiv:2108.09749v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09749</id>
        <link href="http://arxiv.org/abs/2108.09749"/>
        <updated>2021-08-24T01:40:27.606Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) enables the multiple participating devices to
collaboratively contribute to a global neural network model while keeping the
training data locally. Unlike the centralized training setting, the non-IID,
imbalanced (statistical heterogeneity) and distribution shifted training data
of FL is distributed in the federated network, which will increase the
divergences between the local models and the global model, further degrading
performance. In this paper, we propose a flexible clustered federated learning
(CFL) framework named FlexCFL, in which we 1) group the training of clients
based on the similarities between the clients' optimization directions for
lower training divergence; 2) implement an efficient newcomer device cold start
mechanism for framework scalability and practicality; 3) flexibly migrate
clients to meet the challenge of client-level data distribution shift. FlexCFL
can achieve improvements by dividing joint optimization into groups of
sub-optimization and can strike a balance between accuracy and communication
efficiency in the distribution shift environment. The convergence and
complexity are analyzed to demonstrate the efficiency of FlexCFL. We also
evaluate FlexCFL on several open datasets and made comparisons with related CFL
frameworks. The results show that FlexCFL can significantly improve absolute
test accuracy by +10.6% on FEMNIST compared to FedAvg, +3.5% on FashionMNIST
compared to FedProx, +8.4% on MNIST compared to FeSEM. The experiment results
show that FlexCFL is also communication efficient in the distribution shift
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1"&gt;Moming Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Duo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xinyuan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Liang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xianzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yujuan Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal-Textual Point Processes for Crime Linkage Detection. (arXiv:1902.00440v6 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.00440</id>
        <link href="http://arxiv.org/abs/1902.00440"/>
        <updated>2021-08-24T01:40:27.588Z</updated>
        <summary type="html"><![CDATA[Crimes emerge out of complex interactions of human behaviors and situations.
Linkages between crime incidents are highly complex. Detecting crime linkage
given a set of incidents is a highly challenging task since we only have
limited information, including text descriptions, incident times, and
locations. In practice, there are very few labels. We propose a new statistical
modeling framework for {\it spatio-temporal-textual} data and demonstrate its
usage on crime linkage detection. We capture linkages of crime incidents via
multivariate marked spatio-temporal Hawkes processes and treat embedding
vectors of the free-text as {\it marks} of the incident, inspired by the notion
of {\it modus operandi} (M.O.) in crime analysis. Numerical results using real
data demonstrate the good performance of our method as well as reveals
interesting patterns in the crime data: the joint modeling of space, time, and
text information enhances crime linkage detection compared with the
state-of-the-art, and the learned spatial dependence from data can be useful
for police operations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shixiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electroencephalogram Signal Processing with Independent Component Analysis and Cognitive Stress Classification using Convolutional Neural Networks. (arXiv:2108.09817v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.09817</id>
        <link href="http://arxiv.org/abs/2108.09817"/>
        <updated>2021-08-24T01:40:27.583Z</updated>
        <summary type="html"><![CDATA[Electroencephalogram (EEG) is the recording which is the result due to the
activity of bio-electrical signals that is acquired from electrodes placed on
the scalp. In Electroencephalogram signal(EEG) recordings, the signals obtained
are contaminated predominantly by the Electrooculogram(EOG) signal. Since this
artifact has higher magnitude compared to EEG signals, these noise signals have
to be removed in order to have a better understanding regarding the functioning
of a human brain for applications such as medical diagnosis. This paper
proposes an idea of using Independent Component Analysis(ICA) along with
cross-correlation to de-noise EEG signal. This is done by selecting the
component based on the cross-correlation coefficient with a threshold value and
reducing its effect instead of zeroing it out completely, thus reducing the
information loss. The results of the recorded data show that this algorithm can
eliminate the EOG signal artifact with little loss in EEG data. The denoising
is verified by an increase in SNR value and the decrease in cross-correlation
coefficient value. The denoised signals are used to train an Artificial Neural
Network(ANN) which would examine the features of the input EEG signal and
predict the stress levels of the individual.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sutharsan_V/0/1/0/all/0/1"&gt;Venkatakrishnan Sutharsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Swaminathan_A/0/1/0/all/0/1"&gt;Alagappan Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramachandran_S/0/1/0/all/0/1"&gt;Saisrinivasan Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lakshmanan_M/0/1/0/all/0/1"&gt;Madan Kumar Lakshmanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahadevan_B/0/1/0/all/0/1"&gt;Balaji Mahadevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Neural Network Architectural and Topological Adaptation and Related Methods -- A Survey. (arXiv:2108.10066v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.10066</id>
        <link href="http://arxiv.org/abs/2108.10066"/>
        <updated>2021-08-24T01:40:27.577Z</updated>
        <summary type="html"><![CDATA[Training and inference in deep neural networks (DNNs) has, due to a steady
increase in architectural complexity and data set size, lead to the development
of strategies for reducing time and space requirements of DNN training and
inference, which is of particular importance in scenarios where training takes
place in resource constrained computation environments or inference is part of
a time critical application. In this survey, we aim to provide a general
overview and categorization of state-of-the-art (SOTA) of techniques to reduced
DNN training and inference time and space complexities with a particular focus
on architectural adaptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1"&gt;Lorenz Kummer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image coding for machines: an end-to-end learned approach. (arXiv:2108.09993v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09993</id>
        <link href="http://arxiv.org/abs/2108.09993"/>
        <updated>2021-08-24T01:40:27.571Z</updated>
        <summary type="html"><![CDATA[Over recent years, deep learning-based computer vision systems have been
applied to images at an ever-increasing pace, oftentimes representing the only
type of consumption for those images. Given the dramatic explosion in the
number of images generated per day, a question arises: how much better would an
image codec targeting machine-consumption perform against state-of-the-art
codecs targeting human-consumption? In this paper, we propose an image codec
for machines which is neural network (NN) based and end-to-end learned. In
particular, we propose a set of training strategies that address the delicate
problem of balancing competing loss functions, such as computer vision task
losses, image distortion losses, and rate loss. Our experimental results show
that our NN-based codec outperforms the state-of-the-art Versa-tile Video
Coding (VVC) standard on the object detection and instance segmentation tasks,
achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast
thanks to its compact size. To the best of our knowledge, this is the first
end-to-end learned machine-targeted image codec.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Nam Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cricri_F/0/1/0/all/0/1"&gt;Francesco Cricri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1"&gt;Ramin Ghaznavi-Youvalari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A universally consistent learning rule with a universally monotone error. (arXiv:2108.09733v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09733</id>
        <link href="http://arxiv.org/abs/2108.09733"/>
        <updated>2021-08-24T01:40:27.564Z</updated>
        <summary type="html"><![CDATA[We present a universally consistent learning rule whose expected error is
monotone non-increasing with the sample size under every data distribution. The
question of existence of such rules was brought up in 1996 by Devroye, Gy\"orfi
and Lugosi (who called them "smart"). Our rule is fully deterministic, a
data-dependent partitioning rule constructed in an arbitrary domain (a standard
Borel space) using a cyclic order. The central idea is to only partition at
each step those cyclic intervals that exhibit a sufficient empirical diversity
of labels, thus avoiding a region where the error function is convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pestov_V/0/1/0/all/0/1"&gt;Vladimir Pestov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Photo-Sketch Recognition Using Bidirectional Collaborative Synthesis Network. (arXiv:2108.09898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09898</id>
        <link href="http://arxiv.org/abs/2108.09898"/>
        <updated>2021-08-24T01:40:27.554Z</updated>
        <summary type="html"><![CDATA[This research features a deep-learning based framework to address the problem
of matching a given face sketch image against a face photo database. The
problem of photo-sketch matching is challenging because 1) there is large
modality gap between photo and sketch, and 2) the number of paired training
samples is insufficient to train deep learning based networks. To circumvent
the problem of large modality gap, our approach is to use an intermediate
latent space between the two modalities. We effectively align the distributions
of the two modalities in this latent space by employing a bidirectional (photo
-> sketch and sketch -> photo) collaborative synthesis network. A StyleGAN-like
architecture is utilized to make the intermediate latent space be equipped with
rich representation power. To resolve the problem of insufficient training
samples, we introduce a three-step training scheme. Extensive evaluation on
public composite face sketch database confirms superior performance of our
method compared to existing state-of-the-art methods. The proposed methodology
can be employed in matching other modality pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Seho Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Din_N/0/1/0/all/0/1"&gt;Nizam Ud Din&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunkyu Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Juneho Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Reinforcement Learning of Optimal Threshold Policies for Markov Decision Processes. (arXiv:1912.10325v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.10325</id>
        <link href="http://arxiv.org/abs/1912.10325"/>
        <updated>2021-08-24T01:40:27.538Z</updated>
        <summary type="html"><![CDATA[To overcome the curses of dimensionality and modeling of Dynamic Programming
(DP) methods to solve Markov Decision Process (MDP) problems, Reinforcement
Learning (RL) methods are adopted in practice. Contrary to traditional RL
algorithms which do not consider the structural properties of the optimal
policy, we propose a structure-aware learning algorithm to exploit the ordered
multi-threshold structure of the optimal policy, if any. We prove the
asymptotic convergence of the proposed algorithm to the optimal policy. Due to
the reduction in the policy space, the proposed algorithm provides remarkable
improvements in storage and computational complexities over classical RL
algorithms. Simulation results establish that the proposed algorithm converges
faster than other RL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Arghyadip Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karandikar_A/0/1/0/all/0/1"&gt;Abhay Karandikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaporkar_P/0/1/0/all/0/1"&gt;Prasanna Chaporkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Relational Metric Learning. (arXiv:2108.10026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.10026</id>
        <link href="http://arxiv.org/abs/2108.10026"/>
        <updated>2021-08-24T01:40:27.533Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep relational metric learning (DRML) framework for
image clustering and retrieval. Most existing deep metric learning methods
learn an embedding space with a general objective of increasing interclass
distances and decreasing intraclass distances. However, the conventional losses
of metric learning usually suppress intraclass variations which might be
helpful to identify samples of unseen classes. To address this problem, we
propose to adaptively learn an ensemble of features that characterizes an image
from different aspects to model both interclass and intraclass distributions.
We further employ a relational module to capture the correlations among each
feature in the ensemble and construct a graph to represent an image. We then
perform relational inference on the graph to integrate the ensemble and obtain
a relation-aware embedding to measure the similarities. Extensive experiments
on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets
demonstrate that our framework improves existing deep metric learning methods
and achieves very competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Borui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revealing Distributional Vulnerability of Explicit Discriminators by Implicit Generators. (arXiv:2108.09976v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09976</id>
        <link href="http://arxiv.org/abs/2108.09976"/>
        <updated>2021-08-24T01:40:27.527Z</updated>
        <summary type="html"><![CDATA[An explicit discriminator trained on observable in-distribution (ID) samples
can make high-confidence prediction on out-of-distribution (OOD) samples due to
its distributional vulnerability. This is primarily caused by the limited ID
samples observable for training discriminators when OOD samples are
unavailable. To address this issue, the state-of-the-art methods train the
discriminator with OOD samples generated by general assumptions without
considering the data and network characteristics. However, different network
architectures and training ID datasets may cause diverse vulnerabilities, and
the generated OOD samples thus usually misaddress the specific distributional
vulnerability of the explicit discriminator. To reveal and patch the
distributional vulnerabilities, we propose a novel method of
\textit{fine-tuning explicit discriminators by implicit generators} (FIG).
According to the Shannon entropy, an explicit discriminator can construct its
corresponding implicit generator to generate specific OOD samples without extra
training costs. A Langevin Dynamic sampler then draws high-quality OOD samples
from the generator to reveal the vulnerability. Finally, a regularizer,
constructed according to the design principle of the implicit generator,
patches the distributional vulnerability by encouraging those generated OOD
samples with high entropy. Our experiments on four networks, four ID datasets
and seven OOD datasets demonstrate that FIG achieves state-of-the-art OOD
detection performance and maintains a competitive classification capability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kun-Yu Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger. (arXiv:2108.09779v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.09779</id>
        <link href="http://arxiv.org/abs/2108.09779"/>
        <updated>2021-08-24T01:40:27.521Z</updated>
        <summary type="html"><![CDATA[We present a system for learning a challenging dexterous manipulation task
involving moving a cube to an arbitrary 6-DoF pose with only 3-fingers trained
with NVIDIA's IsaacGym simulator. We show empirical benefits, both in
simulation and sim-to-real transfer, of using keypoints as opposed to
position+quaternion representations for the object pose in 6-DoF for policy
observations and in reward calculation to train a model-free reinforcement
learning agent. By utilizing domain randomization strategies along with the
keypoint representation of the pose of the manipulated object, we achieve a
high success rate of 83% on a remote TriFinger system maintained by the
organizers of the Real Robot Challenge. With the aim of assisting further
research in learning in-hand manipulation, we make the codebase of our system,
along with trained checkpoints that come with billions of steps of experience
available, at https://s2r2-ig.github.io]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allshire_A/0/1/0/all/0/1"&gt;Arthur Allshire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_M/0/1/0/all/0/1"&gt;Mayank Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodaya_V/0/1/0/all/0/1"&gt;Varun Lodaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makoviychuk_V/0/1/0/all/0/1"&gt;Viktor Makoviychuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makoviichuk_D/0/1/0/all/0/1"&gt;Denys Makoviichuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1"&gt;Felix Widmaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1"&gt;Manuel W&amp;#xfc;thrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Handa_A/0/1/0/all/0/1"&gt;Ankur Handa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Animesh Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Framework for Online Supervised Learning with Feature Selection. (arXiv:1803.11521v8 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1803.11521</id>
        <link href="http://arxiv.org/abs/1803.11521"/>
        <updated>2021-08-24T01:40:27.515Z</updated>
        <summary type="html"><![CDATA[Current online learning methods suffer issues such as lower convergence rates
and limited capability to recover the support of the true features compared to
their offline counterparts. In this paper, we present a novel framework for
online learning based on running averages and introduce a series of online
versions of some popular existing offline methods such as Elastic Net, Minimax
Concave Penalty and Feature Selection with Annealing. The framework can handle
an arbitrarily large number of observations with the restriction that the data
dimension is not too large, e.g. $p<50,000$. We prove the equivalence between
our online methods and their offline counterparts and give theoretical true
feature recovery and convergence guarantees for some of them. In contrast to
the existing online methods, the proposed methods can extract models with any
desired sparsity level at any time. Numerical experiments indicate that our new
methods enjoy high accuracy of true feature recovery and a fast convergence
rate, compared with standard online and offline algorithms. We also show how
the running averages framework can be used for model adaptation in the presence
of model drift. Finally, we present some applications to large datasets where
again the proposed framework shows competitive results compared to popular
online and offline algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lizhe Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yangzi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Barbu_A/0/1/0/all/0/1"&gt;Adrian Barbu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Adversarial Learning Based Approach for Unknown View Tomographic Reconstruction. (arXiv:2108.09873v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09873</id>
        <link href="http://arxiv.org/abs/2108.09873"/>
        <updated>2021-08-24T01:40:27.499Z</updated>
        <summary type="html"><![CDATA[The goal of 2D tomographic reconstruction is to recover an image given its
projection lines from various views. It is often presumed that projection
angles associated with the projection lines are known in advance. Under certain
situations, however, these angles are known only approximately or are
completely unknown. It becomes more challenging to reconstruct the image from a
collection of random projection lines. We propose an adversarial learning based
approach to recover the image and the projection angle distribution by matching
the empirical distribution of the measurements with the generated data. Fitting
the distributions is achieved through solving a min-max game between a
generator and a critic based on Wasserstein generative adversarial network
structure. To accommodate the update of the projection angle distribution
through gradient back propagation, we approximate the loss using the
Gumbel-Softmax reparameterization of samples from discrete distributions. Our
theoretical analysis verifies the unique recovery of the image and the
projection distribution up to a rotation and reflection upon convergence. Our
extensive numerical experiments showcase the potential of our method to
accurately recover the image and the projection angle distribution under noise
contamination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zehni_M/0/1/0/all/0/1"&gt;Mona Zehni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhizhen Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wind Power Projection using Weather Forecasts by Novel Deep Neural Networks. (arXiv:2108.09797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09797</id>
        <link href="http://arxiv.org/abs/2108.09797"/>
        <updated>2021-08-24T01:40:27.479Z</updated>
        <summary type="html"><![CDATA[The transition from conventional methods of energy production to renewable
energy production necessitates better prediction models of the upcoming supply
of renewable energy. In wind power production, error in forecasting production
is impossible to negate owing to the intermittence of wind. For successful
power grid integration, it is crucial to understand the uncertainties that
arise in predicting wind power production and use this information to build an
accurate and reliable forecast. This can be achieved by observing the
fluctuations in wind power production with changes in different parameters such
as wind speed, temperature, and wind direction, and deriving functional
dependencies for the same. Using optimized machine learning algorithms, it is
possible to find obscured patterns in the observations and obtain meaningful
data, which can then be used to accurately predict wind power requirements .
Utilizing the required data provided by the Gamesa's wind farm at Bableshwar,
the paper explores the use of both parametric and the non-parametric models for
calculating wind power prediction using power curves. The obtained results are
subject to comparison to better understand the accuracy of the utilized models
and to determine the most suitable model for predicting wind power production
based on the given data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1"&gt;Alagappan Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutharsan_V/0/1/0/all/0/1"&gt;Venkatakrishnan Sutharsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selvaraj_T/0/1/0/all/0/1"&gt;Tamilselvi Selvaraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fluent: An AI Augmented Writing Tool for People who Stutter. (arXiv:2108.09918v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.09918</id>
        <link href="http://arxiv.org/abs/2108.09918"/>
        <updated>2021-08-24T01:40:27.473Z</updated>
        <summary type="html"><![CDATA[Stuttering is a speech disorder which impacts the personal and professional
lives of millions of people worldwide. To save themselves from stigma and
discrimination, people who stutter (PWS) may adopt different strategies to
conceal their stuttering. One of the common strategies is word substitution
where an individual avoids saying a word they might stutter on and use an
alternative instead. This process itself can cause stress and add more burden.
In this work, we present Fluent, an AI augmented writing tool which assists PWS
in writing scripts which they can speak more fluently. Fluent embodies a novel
active learning based method of identifying words an individual might struggle
pronouncing. Such words are highlighted in the interface. On hovering over any
such word, Fluent presents a set of alternative words which have similar
meaning but are easier to speak. The user is free to accept or ignore these
suggestions. Based on such user interaction (feedback), Fluent continuously
evolves its classifier to better suit the personalized needs of each user. We
evaluated our tool by measuring its ability to identify difficult words for 10
simulated users. We found that our tool can identify difficult words with a
mean accuracy of over 80% in under 20 interactions and it keeps improving with
more feedback. Our tool can be beneficial for certain important life situations
like giving a talk, presentation, etc. The source code for this tool has been
made publicly accessible at github.com/bhavyaghai/Fluent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghai_B/0/1/0/all/0/1"&gt;Bhavya Ghai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1"&gt;Klaus Mueller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convex Latent Effect Logit Model via Sparse and Low-rank Decomposition. (arXiv:2108.09859v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09859</id>
        <link href="http://arxiv.org/abs/2108.09859"/>
        <updated>2021-08-24T01:40:27.467Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a convex formulation for learning logistic
regression model (logit) with latent heterogeneous effect on sub-population. In
transportation, logistic regression and its variants are often interpreted as
discrete choice models under utility theory (McFadden, 2001). Two prominent
applications of logit models in the transportation domain are traffic accident
analysis and choice modeling. In these applications, researchers often want to
understand and capture the individual variation under the same accident or
choice scenario. The mixed effect logistic regression (mixed logit) is a
popular model employed by transportation researchers. To estimate the
distribution of mixed logit parameters, a non-convex optimization problem with
nested high-dimensional integrals needs to be solved. Simulation-based
optimization is typically applied to solve the mixed logit parameter estimation
problem. Despite its popularity, the mixed logit approach for learning
individual heterogeneity has several downsides. First, the parametric form of
the distribution requires domain knowledge and assumptions imposed by users,
although this issue can be addressed to some extent by using a non-parametric
approach. Second, the optimization problems arise from parameter estimation for
mixed logit and the non-parametric extensions are non-convex, which leads to
unstable model interpretation. Third, the simulation size in
simulation-assisted estimation lacks finite-sample theoretical guarantees and
is chosen somewhat arbitrarily in practice. To address these issues, we are
motivated to develop a formulation that models the latent individual
heterogeneity while preserving convexity, and avoids the need for
simulation-based approximation. Our setup is based on decomposing the
parameters into a sparse homogeneous component in the population and low-rank
heterogeneous parts for each individual.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Hongyuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madduri_K/0/1/0/all/0/1"&gt;Kamesh Madduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Venkataraman Shankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative Entropy-Regularized Optimal Transport on a Graph: a new algorithm and an experimental comparison. (arXiv:2108.10004v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.10004</id>
        <link href="http://arxiv.org/abs/2108.10004"/>
        <updated>2021-08-24T01:40:27.461Z</updated>
        <summary type="html"><![CDATA[Following [21, 23], the present work investigates a new relative
entropy-regularized algorithm for solving the optimal transport on a graph
problem within the randomized shortest paths formalism. More precisely, a unit
flow is injected into a set of input nodes and collected from a set of output
nodes while minimizing the expected transportation cost together with a paths
relative entropy regularization term, providing a randomized routing policy.
The main advantage of this new formulation is the fact that it can easily
accommodate edge flow capacity constraints which commonly occur in real-world
problems. The resulting optimal routing policy, i.e., the probability
distribution of following an edge in each node, is Markovian and is computed by
constraining the input and output flows to the prescribed marginal
probabilities thanks to a variant of the algorithm developed in [8]. Besides,
experimental comparisons with other recently developed techniques show that the
distance measure between nodes derived from the introduced model provides
competitive results on semi-supervised classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Courtain_S/0/1/0/all/0/1"&gt;Sylvain Courtain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guex_G/0/1/0/all/0/1"&gt;Guillaume Guex&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kivimaki_I/0/1/0/all/0/1"&gt;Ilkka Kivimaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saerens_M/0/1/0/all/0/1"&gt;Marco Saerens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Sensing and Machine Learning for Food Crop Production Data in Africa Post-COVID-19. (arXiv:2108.10054v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.10054</id>
        <link href="http://arxiv.org/abs/2108.10054"/>
        <updated>2021-08-24T01:40:27.391Z</updated>
        <summary type="html"><![CDATA[In the agricultural sector, the COVID-19 threatens to lead to a severe food
security crisis in the region, with disruptions in the food supply chain and
agricultural production expected to contract between 2.6% and 7%. From the food
crop production side, the travel bans and border closures, the late reception
and the use of agricultural inputs such as imported seeds, fertilizers, and
pesticides could lead to poor food crop production performances. Another layer
of disruption introduced by the mobility restriction measures is the scarcity
of agricultural workers, mainly seasonal workers. The lockdown measures and
border closures limit seasonal workers' availability to get to the farm on time
for planting and harvesting activities. Moreover, most of the imported
agricultural inputs travel by air, which the pandemic has heavily impacted.
Such transportation disruptions can also negatively affect the food crop
production system.

This chapter assesses food crop production levels in 2020 -- before the
harvesting period -- in all African regions and four staples such as maize,
cassava, rice, and wheat. The production levels are predicted using the
combination of biogeophysical remote sensing data retrieved from satellite
images and machine learning artificial neural networks (ANNs) technique. The
remote sensing products are used as input variables and the ANNs as the
predictive modeling framework. The input remote sensing products are the
Normalized Difference Vegetation Index (NDVI), the daytime Land Surface
Temperature (LST), rainfall data, and agricultural lands' Evapotranspiration
(ET). The output maps and data are made publicly available on a web-based
platform, AAgWa (Africa Agriculture Watch, www.aagwa.org), to facilitate access
to such information to policymakers, deciders, and other stakeholders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ly_R/0/1/0/all/0/1"&gt;Racine Ly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dia_K/0/1/0/all/0/1"&gt;Khadim Dia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_M/0/1/0/all/0/1"&gt;Mariam Diallo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Algorithms for Learning from Coarse Labels. (arXiv:2108.09805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09805</id>
        <link href="http://arxiv.org/abs/2108.09805"/>
        <updated>2021-08-24T01:40:27.380Z</updated>
        <summary type="html"><![CDATA[For many learning problems one may not have access to fine grained label
information; e.g., an image can be labeled as husky, dog, or even animal
depending on the expertise of the annotator. In this work, we formalize these
settings and study the problem of learning from such coarse data. Instead of
observing the actual labels from a set $\mathcal{Z}$, we observe coarse labels
corresponding to a partition of $\mathcal{Z}$ (or a mixture of partitions).

Our main algorithmic result is that essentially any problem learnable from
fine grained labels can also be learned efficiently when the coarse data are
sufficiently informative. We obtain our result through a generic reduction for
answering Statistical Queries (SQ) over fine grained labels given only coarse
labels. The number of coarse labels required depends polynomially on the
information distortion due to coarsening and the number of fine labels
$|\mathcal{Z}|$.

We also investigate the case of (infinitely many) real valued labels focusing
on a central problem in censored and truncated statistics: Gaussian mean
estimation from coarse data. We provide an efficient algorithm when the sets in
the partition are convex and establish that the problem is NP-hard even for
very simple non-convex sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1"&gt;Dimitris Fotakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1"&gt;Alkis Kalavasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1"&gt;Vasilis Kontonis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolutionary Ensemble Learning for Multivariate Time Series Prediction. (arXiv:2108.09659v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2108.09659</id>
        <link href="http://arxiv.org/abs/2108.09659"/>
        <updated>2021-08-24T01:40:27.363Z</updated>
        <summary type="html"><![CDATA[Multivariate time series (MTS) prediction plays a key role in many fields
such as finance, energy and transport, where each individual time series
corresponds to the data collected from a certain data source, so-called
channel. A typical pipeline of building an MTS prediction model (PM) consists
of selecting a subset of channels among all available ones, extracting features
from the selected channels, and building a PM based on the extracted features,
where each component involves certain optimization tasks, i.e., selection of
channels, feature extraction (FE) methods, and PMs as well as configuration of
the selected FE method and PM. Accordingly, pursuing the best prediction
performance corresponds to optimizing the pipeline by solving all of its
involved optimization problems. This is a non-trivial task due to the vastness
of the solution space. Different from most of the existing works which target
at optimizing certain components of the pipeline, we propose a novel
evolutionary ensemble learning framework to optimize the entire pipeline in a
holistic manner. In this framework, a specific pipeline is encoded as a
candidate solution and a multi-objective evolutionary algorithm is applied
under different population sizes to produce multiple Pareto optimal sets
(POSs). Finally, selective ensemble learning is designed to choose the optimal
subset of solutions from the POSs and combine them to yield final prediction by
using greedy sequential selection and least square methods. We implement the
proposed framework and evaluate our implementation on two real-world
applications, i.e., electricity consumption prediction and air quality
prediction. The performance comparison with state-of-the-art techniques
demonstrates the superiority of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1"&gt;A. K. Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Trends in Quantum Machine Learning. (arXiv:2108.09664v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.09664</id>
        <link href="http://arxiv.org/abs/2108.09664"/>
        <updated>2021-08-24T01:40:27.331Z</updated>
        <summary type="html"><![CDATA[Here we will give a perspective on new possible interplays between Machine
Learning and Quantum Physics, including also practical cases and applications.
We will explore the ways in which machine learning could benefit from new
quantum technologies and algorithms to find new ways to speed up their
computations by breakthroughs in physical hardware, as well as to improve
existing models or devise new learning schemes in the quantum domain. Moreover,
there are lots of experiments in quantum physics that do generate incredible
amounts of data and machine learning would be a great tool to analyze those and
make predictions, or even control the experiment itself. On top of that, data
visualization techniques and other schemes borrowed from machine learning can
be of great use to theoreticians to have better intuition on the structure of
complex manifolds or to make predictions on theoretical models. This new
research field, named as Quantum Machine Learning, is very rapidly growing
since it is expected to provide huge advantages over its classical counterpart
and deeper investigations are timely needed since they can be already tested on
the already commercially available quantum machines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Buffoni_L/0/1/0/all/0/1"&gt;Lorenzo Buffoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Caruso_F/0/1/0/all/0/1"&gt;Filippo Caruso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using growth transform dynamical systems for spatio-temporal data sonification. (arXiv:2108.09537v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.09537</id>
        <link href="http://arxiv.org/abs/2108.09537"/>
        <updated>2021-08-24T01:40:27.325Z</updated>
        <summary type="html"><![CDATA[Sonification, or encoding information in meaningful audio signatures, has
several advantages in augmenting or replacing traditional visualization methods
for human-in-the-loop decision-making. Standard sonification methods reported
in the literature involve either (i) using only a subset of the variables, or
(ii) first solving a learning task on the data and then mapping the output to
an audio waveform, which is utilized by the end-user to make a decision. This
paper presents a novel framework for sonifying high-dimensional data using a
complex growth transform dynamical system model where both the learning (or,
more generally, optimization) and the sonification processes are integrated
together. Our algorithm takes as input the data and optimization parameters
underlying the learning or prediction task and combines it with the
psychoacoustic parameters defined by the user. As a result, the proposed
framework outputs binaural audio signatures that not only encode some
statistical properties of the high-dimensional data but also reveal the
underlying complexity of the optimization/learning process. Along with
extensive experiments using synthetic datasets, we demonstrate the framework on
sonifying Electro-encephalogram (EEG) data with the potential for detecting
epileptic seizures in pediatric patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_O/0/1/0/all/0/1"&gt;Oindrila Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabartty_S/0/1/0/all/0/1"&gt;Shantanu Chakrabartty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sparse Structure Learning Algorithm for Bayesian Network Identification from Discrete High-Dimensional Data. (arXiv:2108.09501v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09501</id>
        <link href="http://arxiv.org/abs/2108.09501"/>
        <updated>2021-08-24T01:40:27.311Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of learning a sparse structure Bayesian
network from high-dimensional discrete data. Compared to continuous Bayesian
networks, learning a discrete Bayesian network is a challenging problem due to
the large parameter space. Although many approaches have been developed for
learning continuous Bayesian networks, few approaches have been proposed for
the discrete ones. In this paper, we address learning Bayesian networks as an
optimization problem and propose a score function that satisfies the sparsity
and the DAG property simultaneously. Besides, we implement a block-wised
stochastic coordinate descent algorithm to optimize the score function.
Specifically, we use a variance reducing method in our optimization algorithm
to make the algorithm work efficiently in high-dimensional data. The proposed
approach is applied to synthetic data from well-known benchmark networks. The
quality, scalability, and robustness of the constructed network are measured.
Compared to some competitive approaches, the results reveal that our algorithm
outperforms the others in evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shajoonnezhad_N/0/1/0/all/0/1"&gt;Nazanin Shajoonnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform. (arXiv:2108.09551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09551</id>
        <link href="http://arxiv.org/abs/2108.09551"/>
        <updated>2021-08-24T01:40:27.300Z</updated>
        <summary type="html"><![CDATA[We propose a versatile deep image compression network based on Spatial
Feature Transform (SFT arXiv:1804.02815), which takes a source image and a
corresponding quality map as inputs and produce a compressed image with
variable rates. Our model covers a wide range of compression rates using a
single model, which is controlled by arbitrary pixel-wise quality maps. In
addition, the proposed framework allows us to perform task-aware image
compressions for various tasks, e.g., classification, by efficiently estimating
optimized quality maps specific to target tasks for our encoding network. This
is even possible with a pretrained network without learning separate models for
individual tasks. Our algorithm achieves outstanding rate-distortion trade-off
compared to the approaches based on multiple models that are optimized
separately for several different target rates. At the same level of
compression, the proposed approach successfully improves performance on image
classification and text region quality preservation via task-aware quality map
estimation without additional model training. The code is available at the
project website: https://github.com/micmic123/QmapCompression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Song_M/0/1/0/all/0/1"&gt;Myungseo Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalised Federated Learning: A Combinational Approach. (arXiv:2108.09618v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09618</id>
        <link href="http://arxiv.org/abs/2108.09618"/>
        <updated>2021-08-24T01:40:27.291Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a distributed machine learning approach involving
multiple clients collaboratively training a shared model. Such a system has the
advantage of more training data from multiple clients, but data can be
non-identically and independently distributed (non-i.i.d.). Privacy and
integrity preserving features such as differential privacy (DP) and robust
aggregation (RA) are commonly used in FL. In this work, we show that on common
deep learning tasks, the performance of FL models differs amongst clients and
situations, and FL models can sometimes perform worse than local models due to
non-i.i.d. data. Secondly, we show that incorporating DP and RA degrades
performance further. Then, we conduct an ablation study on the performance
impact of different combinations of common personalization approaches for FL,
such as finetuning, mixture-of-experts ensemble, multi-task learning, and
knowledge distillation. It is observed that certain combinations of
personalization approaches are more impactful in certain scenarios while others
always improve performance, and combination approaches are better than
individual ones. Most clients obtained better performance with combined
personalized FL and recover from performance degradation caused by non-i.i.d.
data, DP, and RA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pye_S/0/1/0/all/0/1"&gt;Sone Kyaw Pye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEDI: Few-shot learning based on Earth Mover's Distance algorithm combined with deep residual network to identify diabetic retinopathy. (arXiv:2108.09711v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09711</id>
        <link href="http://arxiv.org/abs/2108.09711"/>
        <updated>2021-08-24T01:40:27.276Z</updated>
        <summary type="html"><![CDATA[Diabetic retinopathy(DR) is the main cause of blindness in diabetic patients.
However, DR can easily delay the occurrence of blindness through the diagnosis
of the fundus. In view of the reality, it is difficult to collect a large
amount of diabetic retina data in clinical practice. This paper proposes a
few-shot learning model of a deep residual network based on Earth Mover's
Distance algorithm to assist in diagnosing DR. We build training and validation
classification tasks for few-shot learning based on 39 categories of 1000
sample data, train deep residual networks, and obtain experience maximization
pre-training models. Based on the weights of the pre-trained model, the Earth
Mover's Distance algorithm calculates the distance between the images, obtains
the similarity between the images, and changes the model's parameters to
improve the accuracy of the training model. Finally, the experimental
construction of the small sample classification task of the test set to
optimize the model further, and finally, an accuracy of 93.5667% on the
3way10shot task of the diabetic retina test set. For the experimental code and
results, please refer to:
https://github.com/panliangrui/few-shot-learning-funds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1"&gt;Liangrui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_B/0/1/0/all/0/1"&gt;Boya Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1"&gt;Peng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chongcheawchamnan_M/0/1/0/all/0/1"&gt;Mitchai Chongcheawchamnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaoliang Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09484</id>
        <link href="http://arxiv.org/abs/2108.09484"/>
        <updated>2021-08-24T01:40:27.255Z</updated>
        <summary type="html"><![CDATA[Human evaluation has always been expensive while researchers struggle to
trust the automatic metrics. To address this, we propose to customise
traditional metrics by taking advantages of the pre-trained language models
(PLMs) and the limited available human labelled scores. We first re-introduce
the hLEPOR metric factors, followed by the Python portable version we developed
which achieved the automatic tuning of the weighting parameters in hLEPOR
metric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE
distilled knowledge model to improve the metric agreement with human judgements
by automatically optimised factor weights regarding the exact MT language pairs
that cushLEPOR is deployed to. We also optimise cushLEPOR towards human
evaluation data based on MQM and pSQM framework on English-German and
Chinese-English language pairs. The experimental investigations show cushLEPOR
boosts hLEPOR performances towards better agreements to PLMs like LABSE with
much lower cost, and better agreements to human evaluations including MQM and
pSQM scores, and yields much better performances than BLEU (data available at
\url{https://github.com/poethan/cushLEPOR}).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Lifeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1"&gt;Irina Sorokina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1"&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1"&gt;Serge Gladkoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Induced Self-Play for Stochastic Bayesian Games. (arXiv:2108.09444v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2108.09444</id>
        <link href="http://arxiv.org/abs/2108.09444"/>
        <updated>2021-08-24T01:40:27.224Z</updated>
        <summary type="html"><![CDATA[One practical requirement in solving dynamic games is to ensure that the
players play well from any decision point onward. To satisfy this requirement,
existing efforts focus on equilibrium refinement, but the scalability and
applicability of existing techniques are limited. In this paper, we propose
Temporal-Induced Self-Play (TISP), a novel reinforcement learning-based
framework to find strategies with decent performances from any decision point
onward. TISP uses belief-space representation, backward induction, policy
learning, and non-parametric approximation. Building upon TISP, we design a
policy-gradient-based algorithm TISP-PG. We prove that TISP-based algorithms
can find approximate Perfect Bayesian Equilibrium in zero-sum one-sided
stochastic Bayesian games with finite horizon. We test TISP-based algorithms in
various games, including finitely repeated security games and a grid-world
game. The results show that TISP-PG is more scalable than existing mathematical
programming-based methods and significantly outperforms other learning-based
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zihan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1"&gt;Fei Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning. (arXiv:2108.09335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09335</id>
        <link href="http://arxiv.org/abs/2108.09335"/>
        <updated>2021-08-24T01:40:27.217Z</updated>
        <summary type="html"><![CDATA[Deep metric learning has been effectively used to learn distance metrics for
different visual tasks like image retrieval, clustering, etc. In order to aid
the training process, existing methods either use a hard mining strategy to
extract the most informative samples or seek to generate hard synthetics using
an additional network. Such approaches face different challenges and can lead
to biased embeddings in the former case, and (i) harder optimization (ii)
slower training speed (iii) higher model complexity in the latter case. In
order to overcome these challenges, we propose a novel approach that looks for
optimal hard negatives (LoOp) in the embedding space, taking full advantage of
each tuple by calculating the minimum distance between a pair of positives and
a pair of negatives. Unlike mining-based methods, our approach considers the
entire space between pairs of embeddings to calculate the optimal hard
negatives. Extensive experiments combining our approach and representative
metric learning losses reveal a significant boost in performance on three
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasudeva_B/0/1/0/all/0/1"&gt;Bhavya Vasudeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deora_P/0/1/0/all/0/1"&gt;Puneesh Deora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1"&gt;Sukalpa Chanda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-Convolutional Deep Learning to Identify Optimized Molecular Configurations. (arXiv:2108.09637v1 [physics.app-ph])]]></title>
        <id>http://arxiv.org/abs/2108.09637</id>
        <link href="http://arxiv.org/abs/2108.09637"/>
        <updated>2021-08-24T01:40:27.212Z</updated>
        <summary type="html"><![CDATA[Tackling molecular optimization problems using conventional computational
methods is challenging, because the determination of the optimized
configuration is known to be an NP-hard problem. Recently, there has been
increasing interest in applying different deep-learning techniques to benchmark
molecular optimization tasks. In this work, we implement a graph-convolutional
method to classify molecular structures using the equilibrium and
non-equilibrium configurations provided in the QM7-X data set. Atomic forces
are encoded in graph vertices and the substantial suppression in the total
force magnitude on the atoms in the optimized structure is learned for the
graph classification task. We demonstrate the results using two different graph
pooling layers and compare their respective performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Joshi_E/0/1/0/all/0/1"&gt;Eshan Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Somuyiwa_S/0/1/0/all/0/1"&gt;Samuel Somuyiwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jooya_H/0/1/0/all/0/1"&gt;Hossein Z. Jooya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotationally Equivariant Neural Operators for Learning Transformations on Tensor Fields (eg 3D Images and Vector Fields). (arXiv:2108.09541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09541</id>
        <link href="http://arxiv.org/abs/2108.09541"/>
        <updated>2021-08-24T01:40:27.205Z</updated>
        <summary type="html"><![CDATA[We introduce equivariant neural operators for learning resolution invariant
as well as translation and rotation equivariant transformations between sets of
tensor fields. Input and output may contain arbitrary mixes of scalar fields,
vector fields, second order tensor fields and higher order fields. Our tensor
field convolution layers emulate any linear operator by learning its impulse
response or Green's function as the convolution kernel. Our tensor field
attention layers emulate pairwise field coupling via local tensor products.
Convolutions and associated adjoints can be in real or Fourier space allowing
for linear scaling. By unifying concepts from E3NN, TBNN and FNO, we achieve
good predictive performance on a wide range of PDEs and dynamical systems in
engineering and quantum chemistry. Code is in Julia and available upon request
from authors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1"&gt;Paul Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbst_M/0/1/0/all/0/1"&gt;Michael Herbst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1"&gt;Venkat Viswanathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift. (arXiv:2108.09671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09671</id>
        <link href="http://arxiv.org/abs/2108.09671"/>
        <updated>2021-08-24T01:40:27.199Z</updated>
        <summary type="html"><![CDATA[Recently proposed neural architecture search (NAS) methods co-train billions
of architectures in a supernet and estimate their potential accuracy using the
network weights detached from the supernet. However, the ranking correlation
between the architectures' predicted accuracy and their actual capability is
incorrect, which causes the existing NAS methods' dilemma. We attribute this
ranking correlation problem to the supernet training consistency shift,
including feature shift and parameter shift. Feature shift is identified as
dynamic input distributions of a hidden layer due to random path sampling. The
input distribution dynamic affects the loss descent and finally affects
architecture ranking. Parameter shift is identified as contradictory parameter
updates for a shared layer lay in different paths in different training steps.
The rapidly-changing parameter could not preserve architecture ranking. We
address these two shifts simultaneously using a nontrivial supernet-Pi model,
called Pi-NAS. Specifically, we employ a supernet-Pi model that contains
cross-path learning to reduce the feature consistency shift between different
paths. Meanwhile, we adopt a novel nontrivial mean teacher containing negative
samples to overcome parameter shift and model collision. Furthermore, our
Pi-NAS runs in an unsupervised manner, which can search for more transferable
architectures. Extensive experiments on ImageNet and a wide range of downstream
tasks (e.g., COCO 2017, ADE20K, and Cityscapes) demonstrate the effectiveness
and universality of our Pi-NAS compared to supervised NAS. See Codes:
https://github.com/Ernie1/Pi-NAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jiefeng Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-DARTS: Distributed Differentiable Architecture Search. (arXiv:2108.09306v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09306</id>
        <link href="http://arxiv.org/abs/2108.09306"/>
        <updated>2021-08-24T01:40:27.181Z</updated>
        <summary type="html"><![CDATA[Differentiable ARchiTecture Search (DARTS) is one of the most trending Neural
Architecture Search (NAS) methods, drastically reducing search cost by
resorting to Stochastic Gradient Descent (SGD) and weight-sharing. However, it
also greatly reduces the search space, thus excluding potential promising
architectures from being discovered. In this paper, we propose D-DARTS, a novel
solution that addresses this problem by nesting several neural networks at
cell-level instead of using weight-sharing to produce more diversified and
specialized architectures. Moreover, we introduce a novel algorithm which can
derive deeper architectures from a few trained cells, increasing performance
and saving computation time. Our solution is able to provide state-of-the-art
results on CIFAR-10, CIFAR-100 and ImageNet while using significantly less
parameters than previous baselines, resulting in more hardware-efficient neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heuillet_A/0/1/0/all/0/1"&gt;Alexandre Heuillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1"&gt;Hedi Tabia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arioui_H/0/1/0/all/0/1"&gt;Hichem Arioui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Youcef_Toumi_K/0/1/0/all/0/1"&gt;Kamal Youcef-Toumi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principal Gradient Direction and Confidence Reservoir Sampling for Continual Learning. (arXiv:2108.09592v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09592</id>
        <link href="http://arxiv.org/abs/2108.09592"/>
        <updated>2021-08-24T01:40:27.175Z</updated>
        <summary type="html"><![CDATA[Task-free online continual learning aims to alleviate catastrophic forgetting
of the learner on a non-iid data stream. Experience Replay (ER) is a SOTA
continual learning method, which is broadly used as the backbone algorithm for
other replay-based methods. However, the training strategy of ER is too simple
to take full advantage of replayed examples and its reservoir sampling strategy
is also suboptimal. In this work, we propose a general proximal gradient
framework so that ER can be viewed as a special case. We further propose two
improvements accordingly: Principal Gradient Direction (PGD) and Confidence
Reservoir Sampling (CRS). In Principal Gradient Direction, we optimize a target
gradient that not only represents the major contribution of past gradients, but
also retains the new knowledge of the current gradient. We then present
Confidence Reservoir Sampling for maintaining a more informative memory buffer
based on a margin-based metric that measures the value of stored examples.
Experiments substantiate the effectiveness of both our improvements and our new
algorithm consistently boosts the performance of MIR-replay, a SOTA ER-based
method: our algorithm increases the average accuracy up to 7.9% and reduces
forgetting up to 15.4% on four datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tong Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Adversarial Examples" for Proof-of-Learning. (arXiv:2108.09454v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09454</id>
        <link href="http://arxiv.org/abs/2108.09454"/>
        <updated>2021-08-24T01:40:27.169Z</updated>
        <summary type="html"><![CDATA[In S&P '21, Jia et al. proposed a new concept/mechanism named
proof-of-learning (PoL), which allows a prover to demonstrate ownership of a
machine learning model by proving integrity of the training procedure. It
guarantees that an adversary cannot construct a valid proof with less cost (in
both computation and storage) than that made by the prover in generating the
proof. A PoL proof includes a set of intermediate models recorded during
training, together with the corresponding data points used to obtain each
recorded model. Jia et al. claimed that an adversary merely knowing the final
model and training dataset cannot efficiently find a set of intermediate models
with correct data points. In this paper, however, we show that PoL is
vulnerable to "adversarial examples"! Specifically, in a similar way as
optimizing an adversarial example, we could make an arbitrarily-chosen data
point "generate" a given model, hence efficiently generating intermediate
models with correct data points. We demonstrate, both theoretically and
empirically, that we are able to generate a valid proof with significantly less
cost than generating a proof by the prover, thereby we successfully break PoL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yuan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingbiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SemiFed: Semi-supervised Federated Learning with Consistency and Pseudo-Labeling. (arXiv:2108.09412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09412</id>
        <link href="http://arxiv.org/abs/2108.09412"/>
        <updated>2021-08-24T01:40:27.163Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple clients, such as mobile phones and
organizations, to collaboratively learn a shared model for prediction while
protecting local data privacy. However, most recent research and applications
of federated learning assume that all clients have fully labeled data, which is
impractical in real-world settings. In this work, we focus on a new scenario
for cross-silo federated learning, where data samples of each client are
partially labeled. We borrow ideas from semi-supervised learning methods where
a large amount of unlabeled data is utilized to improve the model's accuracy
despite limited access to labeled examples. We propose a new framework dubbed
SemiFed that unifies two dominant approaches for semi-supervised learning:
consistency regularization and pseudo-labeling. SemiFed first applies advanced
data augmentation techniques to enforce consistency regularization and then
generates pseudo-labels using the model's predictions during training. SemiFed
takes advantage of the federation so that for a given image, the pseudo-label
holds only if multiple models from different clients produce a high-confidence
prediction and agree on the same label. Extensive experiments on two image
benchmarks demonstrate the effectiveness of our approach under both homogeneous
and heterogeneous data distribution settings]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haowen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahabi_C/0/1/0/all/0/1"&gt;Cyrus Shahabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function. (arXiv:2108.09598v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09598</id>
        <link href="http://arxiv.org/abs/2108.09598"/>
        <updated>2021-08-24T01:40:27.155Z</updated>
        <summary type="html"><![CDATA[Activation functions play a pivotal role in determining the training dynamics
and neural network performance. The widely adopted activation function ReLU
despite being simple and effective has few disadvantages including the Dying
ReLU problem. In order to tackle such problems, we propose a novel activation
function called Serf which is self-regularized and nonmonotonic in nature. Like
Mish, Serf also belongs to the Swish family of functions. Based on several
experiments on computer vision (image classification and object detection) and
natural language processing (machine translation, sentiment classification and
multimodal entailment) tasks with different state-of-the-art architectures, it
is observed that Serf vastly outperforms ReLU (baseline) and other activation
functions including both Swish and Mish, with a markedly bigger margin on
deeper architectures. Ablation studies further demonstrate that Serf based
architectures perform better than those of Swish and Mish in varying scenarios,
validating the effectiveness and compatibility of Serf with varying depth,
complexity, optimizers, learning rates, batch sizes, initializers and dropout
rates. Finally, we investigate the mathematical relation between Swish and
Serf, thereby showing the impact of preconditioner function ingrained in the
first derivative of Serf which provides a regularization effect making
gradients smoother and optimization faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sayan Nag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_M/0/1/0/all/0/1"&gt;Mayukh Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Smart Ponzi Scheme Detection. (arXiv:2108.09305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09305</id>
        <link href="http://arxiv.org/abs/2108.09305"/>
        <updated>2021-08-24T01:40:27.138Z</updated>
        <summary type="html"><![CDATA[A smart Ponzi scheme is a new form of economic crime that uses Ethereum smart
contract account and cryptocurrency to implement Ponzi scheme. The smart Ponzi
scheme has harmed the interests of many investors, but researches on smart
Ponzi scheme detection is still very limited. The existing smart Ponzi scheme
detection methods have the problems of requiring many human resources in
feature engineering and poor model portability. To solve these problems, we
propose a data-driven smart Ponzi scheme detection system in this paper. The
system uses dynamic graph embedding technology to automatically learn the
representation of an account based on multi-source and multi-modal data related
to account transactions. Compared with traditional methods, the proposed system
requires very limited human-computer interaction. To the best of our knowledge,
this is the first work to implement smart Ponzi scheme detection through
dynamic graph embedding. Experimental results show that this method is
significantly better than the existing smart Ponzi scheme detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yuzhi Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Weijing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_K/0/1/0/all/0/1"&gt;Kai Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feiyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Quantifying Literals in Boolean Logic and Its Applications to Explainable AI. (arXiv:2108.09876v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.09876</id>
        <link href="http://arxiv.org/abs/2108.09876"/>
        <updated>2021-08-24T01:40:27.132Z</updated>
        <summary type="html"><![CDATA[Quantified Boolean logic results from adding operators to Boolean logic for
existentially and universally quantifying variables. This extends the reach of
Boolean logic by enabling a variety of applications that have been explored
over the decades. The existential quantification of literals (variable states)
and its applications have also been studied in the literature. In this paper,
we complement this by studying universal literal quantification and its
applications, particularly to explainable AI. We also provide a novel semantics
for quantification, discuss the interplay between variable/literal and
existential/universal quantification. We further identify some classes of
Boolean formulas and circuits on which quantification can be done efficiently.
Literal quantification is more fine-grained than variable quantification as the
latter can be defined in terms of the former. This leads to a refinement of
quantified Boolean logic with literal quantification as its primitive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Darwiche_A/0/1/0/all/0/1"&gt;Adnan Darwiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marquis_P/0/1/0/all/0/1"&gt;Pierre Marquis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservoir Computing with Diverse Timescales for Prediction of Multiscale Dynamics. (arXiv:2108.09446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09446</id>
        <link href="http://arxiv.org/abs/2108.09446"/>
        <updated>2021-08-24T01:40:27.126Z</updated>
        <summary type="html"><![CDATA[Machine learning approaches have recently been leveraged as a substitute or
an aid for physical/mathematical modeling approaches to dynamical systems. To
develop an efficient machine learning method dedicated to modeling and
prediction of multiscale dynamics, we propose a reservoir computing model with
diverse timescales by using a recurrent network of heterogeneous leaky
integrator neurons. In prediction tasks with fast-slow chaotic dynamical
systems including a large gap in timescales of their subsystems dynamics, we
demonstrate that the proposed model has a higher potential than the existing
standard model and yields a performance comparable to the best one of the
standard model even without an optimization of the leak rate parameter. Our
analysis reveals that the timescales required for producing each component of
target dynamics are appropriately and flexibly selected from the reservoir
dynamics by model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_G/0/1/0/all/0/1"&gt;Gouhei Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumori_T/0/1/0/all/0/1"&gt;Tadayoshi Matsumori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshida_H/0/1/0/all/0/1"&gt;Hiroaki Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aihara_K/0/1/0/all/0/1"&gt;Kazuyuki Aihara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Mini-batch Method via Partial Transportation. (arXiv:2108.09645v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.09645</id>
        <link href="http://arxiv.org/abs/2108.09645"/>
        <updated>2021-08-24T01:40:27.120Z</updated>
        <summary type="html"><![CDATA[Mini-batch optimal transport (m-OT) has been widely used recently to deal
with the memory issue of OT in large-scale applications. Despite their
practicality, m-OT suffers from misspecified mappings, namely, mappings that
are optimal on the mini-batch level but do not exist in the optimal
transportation plan between the original measures. To address the misspecified
mappings issue, we propose a novel mini-batch method by using partial optimal
transport (POT) between mini-batch empirical measures, which we refer to as
mini-batch partial optimal transport (m-POT). Leveraging the insight from the
partial transportation, we explain the source of misspecified mappings from the
m-OT and motivate why limiting the amount of transported masses among
mini-batches via POT can alleviate the incorrect mappings. Finally, we carry
out extensive experiments on various applications to compare m-POT with m-OT
and recently proposed mini-batch method, mini-batch unbalanced optimal
transport (m-UOT). We observe that m-POT is better than m-OT deep domain
adaptation applications while having comparable performance with m-UOT. On
other applications, such as deep generative model, gradient flow, and color
transfer, m-POT yields more favorable performance than both m-OT and m-UOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pham_T/0/1/0/all/0/1"&gt;Tung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive unsupervised learning with enhanced feature representation for intra-tumor partitioning and survival prediction for glioblastoma. (arXiv:2108.09423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09423</id>
        <link href="http://arxiv.org/abs/2108.09423"/>
        <updated>2021-08-24T01:40:27.115Z</updated>
        <summary type="html"><![CDATA[Glioblastoma is profoundly heterogeneous in regional microstructure and
vasculature. Characterizing the spatial heterogeneity of glioblastoma could
lead to more precise treatment. With unsupervised learning techniques,
glioblastoma MRI-derived radiomic features have been widely utilized for tumor
sub-region segmentation and survival prediction. However, the reliability of
algorithm outcomes is often challenged by both ambiguous intermediate process
and instability introduced by the randomness of clustering algorithms,
especially for data from heterogeneous patients.

In this paper, we propose an adaptive unsupervised learning approach for
efficient MRI intra-tumor partitioning and glioblastoma survival prediction. A
novel and problem-specific Feature-enhanced Auto-Encoder (FAE) is developed to
enhance the representation of pairwise clinical modalities and therefore
improve clustering stability of unsupervised learning algorithms such as
K-means. Moreover, the entire process is modelled by the Bayesian optimization
(BO) technique with a custom loss function that the hyper-parameters can be
adaptively optimized in a reasonably few steps. The results demonstrate that
the proposed approach can produce robust and clinically relevant MRI
sub-regions and statistically significant survival predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yiran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_S/0/1/0/all/0/1"&gt;Stephen Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Co-designing the Data Ingestion Pipeline for Industry-Scale RecSys Training. (arXiv:2108.09373v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.09373</id>
        <link href="http://arxiv.org/abs/2108.09373"/>
        <updated>2021-08-24T01:40:27.099Z</updated>
        <summary type="html"><![CDATA[The data ingestion pipeline, responsible for storing and preprocessing
training data, is an important component of any machine learning training job.
At Facebook, we use recommendation models extensively across our services. The
data ingestion requirements to train these models are substantial. In this
paper, we present an extensive characterization of the data ingestion
challenges for industry-scale recommendation model training. First, dataset
storage requirements are massive and variable; exceeding local storage
capacities. Secondly, reading and preprocessing data is computationally
expensive, requiring substantially more compute, memory, and network resources
than are available on trainers themselves. These demands result in drastically
reduced training throughput, and thus wasted GPU resources, when current
on-trainer preprocessing solutions are used. To address these challenges, we
present a disaggregated data ingestion pipeline. It includes a central data
warehouse built on distributed storage nodes. We introduce Data PreProcessing
Service (DPP), a fully disaggregated preprocessing service that scales to
hundreds of nodes, eliminating data stalls that can reduce training throughput
by 56%. We implement important optimizations across storage and DPP, increasing
storage and preprocessing throughput by 1.9x and 2.3x, respectively, addressing
the substantial power requirements of data ingestion. We close with lessons
learned and cover the important remaining challenges and opportunities
surrounding data ingestion at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mark Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1"&gt;Niket Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basant_A/0/1/0/all/0/1"&gt;Aarti Basant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedik_B/0/1/0/all/0/1"&gt;Bugra Gedik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Satadru Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdal_M/0/1/0/all/0/1"&gt;Mustafa Ozdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komuravelli_R/0/1/0/all/0/1"&gt;Rakesh Komuravelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jerry Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_T/0/1/0/all/0/1"&gt;Tianshu Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haowei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Sundaram Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langman_J/0/1/0/all/0/1"&gt;Jack Langman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilfong_K/0/1/0/all/0/1"&gt;Kevin Wilfong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_H/0/1/0/all/0/1"&gt;Harsha Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Carole-Jean Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozyrakis_C/0/1/0/all/0/1"&gt;Christos Kozyrakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pol_P/0/1/0/all/0/1"&gt;Parik Pol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automating Crystal-Structure Phase Mapping: Combining Deep Learning with Constraint Reasoning. (arXiv:2108.09523v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09523</id>
        <link href="http://arxiv.org/abs/2108.09523"/>
        <updated>2021-08-24T01:40:27.093Z</updated>
        <summary type="html"><![CDATA[Crystal-structure phase mapping is a core, long-standing challenge in
materials science that requires identifying crystal structures, or mixtures
thereof, in synthesized materials. Materials science experts excel at solving
simple systems but cannot solve complex systems, creating a major bottleneck in
high-throughput materials discovery. Herein we show how to automate
crystal-structure phase mapping. We formulate phase mapping as an unsupervised
pattern demixing problem and describe how to solve it using Deep Reasoning
Networks (DRNets). DRNets combine deep learning with constraint reasoning for
incorporating scientific prior knowledge and consequently require only a modest
amount of (unlabeled) data. DRNets compensate for the limited data by
exploiting and magnifying the rich prior knowledge about the thermodynamic
rules governing the mixtures of crystals with constraint reasoning seamlessly
integrated into neural network optimization. DRNets are designed with an
interpretable latent space for encoding prior-knowledge domain constraints and
seamlessly integrate constraint reasoning into neural network optimization.
DRNets surpass previous approaches on crystal-structure phase mapping,
unraveling the Bi-Cu-V oxide phase diagram, and aiding the discovery of
solar-fuels materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Di Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yiwei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ament_S/0/1/0/all/0/1"&gt;Sebastian Ament&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenting Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1"&gt;Dan Guevarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Lan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selman_B/0/1/0/all/0/1"&gt;Bart Selman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dover_R/0/1/0/all/0/1"&gt;R. Bruce van Dover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1"&gt;John M. Gregoire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation Methodologies for Code Learning Tasks. (arXiv:2108.09619v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.09619</id>
        <link href="http://arxiv.org/abs/2108.09619"/>
        <updated>2021-08-24T01:40:27.087Z</updated>
        <summary type="html"><![CDATA[There has been a growing interest in developing machine learning (ML) models
for code learning tasks, e.g., comment generation and method naming. Despite
substantial increase in the effectiveness of ML models, the evaluation
methodologies, i.e., the way people split datasets into training, validation,
and testing sets, were not well designed. Specifically, no prior work on the
aforementioned topics considered the timestamps of code and comments during
evaluation (e.g., examples in the testing set might be from 2010 and examples
from the training set might be from 2020). This may lead to evaluations that
are inconsistent with the intended use cases of the ML models. In this paper,
we formalize a novel time-segmented evaluation methodology, as well as the two
methodologies commonly used in the literature: mixed-project and cross-project.
We argue that time-segmented methodology is the most realistic. We also
describe various use cases of ML models and provide a guideline for using
methodologies to evaluate each use case. To assess the impact of methodologies,
we collect a dataset of code-comment pairs with timestamps to train and
evaluate several recent code learning ML models for the comment generation and
method naming tasks. Our results show that different methodologies can lead to
conflicting and inconsistent results. We invite the community to adopt the
time-segmented evaluation methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_P/0/1/0/all/0/1"&gt;Pengyu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Jessy Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gligoric_M/0/1/0/all/0/1"&gt;Milos Gligoric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hard Label Black-box Adversarial Attack Against Graph Neural Networks. (arXiv:2108.09513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09513</id>
        <link href="http://arxiv.org/abs/2108.09513"/>
        <updated>2021-08-24T01:40:27.081Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
various graph structure related tasks such as node classification and graph
classification. However, GNNs are vulnerable to adversarial attacks. Existing
works mainly focus on attacking GNNs for node classification; nevertheless, the
attacks against GNNs for graph classification have not been well explored.

In this work, we conduct a systematic study on adversarial attacks against
GNNs for graph classification via perturbing the graph structure. In
particular, we focus on the most challenging attack, i.e., hard label black-box
attack, where an attacker has no knowledge about the target GNN model and can
only obtain predicted labels through querying the target model.To achieve this
goal, we formulate our attack as an optimization problem, whose objective is to
minimize the number of edges to be perturbed in a graph while maintaining the
high attack success rate. The original optimization problem is intractable to
solve, and we relax the optimization problem to be a tractable one, which is
solved with theoretical convergence guarantee. We also design a coarse-grained
searching algorithm and a query-efficient gradient computation algorithm to
decrease the number of queries to the target GNN model. Our experimental
results on three real-world datasets demonstrate that our attack can
effectively attack representative GNNs for graph classification with less
queries and perturbations. We also evaluate the effectiveness of our attack
under two defenses: one is well-designed adversarial graph detector and the
other is that the target GNN model itself is equipped with a defense to prevent
adversarial graph generation. Our experimental results show that such defenses
are not effective enough, which highlights more advanced defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1"&gt;Jiaming Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Binghui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Kun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhuotao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence Selection for Active Learning. (arXiv:2108.09331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09331</id>
        <link href="http://arxiv.org/abs/2108.09331"/>
        <updated>2021-08-24T01:40:27.059Z</updated>
        <summary type="html"><![CDATA[The existing active learning methods select the samples by evaluating the
sample's uncertainty or its effect on the diversity of labeled datasets based
on different task-specific or model-specific criteria. In this paper, we
propose the Influence Selection for Active Learning(ISAL) which selects the
unlabeled samples that can provide the most positive Influence on model
performance. To obtain the Influence of the unlabeled sample in the active
learning scenario, we design the Untrained Unlabeled sample Influence
Calculation(UUIC) to estimate the unlabeled sample's expected gradient with
which we calculate its Influence. To prove the effectiveness of UUIC, we
provide both theoretical and experimental analyses. Since the UUIC just depends
on the model gradients, which can be obtained easily from any neural network,
our active learning algorithm is task-agnostic and model-agnostic. ISAL
achieves state-of-the-art performance in different active learning settings for
different tasks with different datasets. Compared with previous methods, our
method decreases the annotation cost at least by 12%, 13% and 16% on CIFAR10,
VOC2012 and COCO, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhuoming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Hao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Huaping Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weijia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Conghui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascade Watchdog: A Multi-tiered Adversarial Guard for Outlier Detection. (arXiv:2108.09375v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09375</id>
        <link href="http://arxiv.org/abs/2108.09375"/>
        <updated>2021-08-24T01:40:27.050Z</updated>
        <summary type="html"><![CDATA[The identification of out-of-distribution content is critical to the
successful implementation of neural networks. Watchdog techniques have been
developed to support the detection of these inputs, but the performance can be
limited by the amount of available data. Generative adversarial networks have
displayed numerous capabilities, including the ability to generate facsimiles
with excellent accuracy. This paper presents and empirically evaluates a
multi-tiered watchdog, which is developed using GAN generated data, for
improved out-of-distribution detection. The cascade watchdog uses adversarial
training to increase the amount of available data similar to the
out-of-distribution elements that are more difficult to detect. Then, a
specialized second guard is added in sequential order. The results show a solid
and significant improvement on the detection of the most challenging
out-of-distribution inputs while preserving an extremely low false positive
rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galan_G/0/1/0/all/0/1"&gt;Glauco A. Amigo Gal&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_J/0/1/0/all/0/1"&gt;Justin Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marks_R/0/1/0/all/0/1"&gt;Robert J. Marks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04657</id>
        <link href="http://arxiv.org/abs/2108.04657"/>
        <updated>2021-08-24T01:40:27.032Z</updated>
        <summary type="html"><![CDATA[Multi-head attention, a collection of several attention mechanisms that
independently attend to different parts of the input, is the key ingredient in
the Transformer. Recent work has shown, however, that a large proportion of the
heads in a Transformer's multi-head attention mechanism can be safely pruned
away without significantly harming the performance of the model; such pruning
leads to models that are noticeably smaller and faster in practice. Our work
introduces a new head pruning technique that we term differentiable subset
pruning. Intuitively, our method learns per-head importance variables and then
enforces a user-specified hard constraint on the number of unpruned heads. The
importance variables are learned via stochastic gradient descent. We conduct
experiments on natural language inference and machine translation; we show that
differentiable subset pruning performs comparably or better than previous works
while offering precise control of the sparsity level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaoda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness-Aware Online Meta-learning. (arXiv:2108.09435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09435</id>
        <link href="http://arxiv.org/abs/2108.09435"/>
        <updated>2021-08-24T01:40:27.026Z</updated>
        <summary type="html"><![CDATA[In contrast to offline working fashions, two research paradigms are devised
for online learning: (1) Online Meta Learning (OML) learns good priors over
model parameters (or learning to learn) in a sequential setting where tasks are
revealed one after another. Although it provides a sub-linear regret bound,
such techniques completely ignore the importance of learning with fairness
which is a significant hallmark of human intelligence. (2) Online
Fairness-Aware Learning. This setting captures many classification problems for
which fairness is a concern. But it aims to attain zero-shot generalization
without any task-specific adaptation. This therefore limits the capability of a
model to adapt onto newly arrived data. To overcome such issues and bridge the
gap, in this paper for the first time we proposed a novel online meta-learning
algorithm, namely FFML, which is under the setting of unfairness prevention.
The key part of FFML is to learn good priors of an online fair classification
model's primal and dual parameters that are associated with the model's
accuracy and fairness, respectively. The problem is formulated in the form of a
bi-level convex-concave optimization. Theoretic analysis provides sub-linear
upper bounds for loss regret and for violation of cumulative fairness
constraints. Our experiments demonstrate the versatility of FFML by applying it
to classification on three real-world datasets and show substantial
improvements over the best prior work on the tradeoff between fairness and
classification accuracy]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thuraisingham_B/0/1/0/all/0/1"&gt;Bhavani Thuraisingham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Exploitation of Distance Distributions for Clustering. (arXiv:2108.09649v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09649</id>
        <link href="http://arxiv.org/abs/2108.09649"/>
        <updated>2021-08-24T01:40:27.020Z</updated>
        <summary type="html"><![CDATA[Although distance measures are used in many machine learning algorithms, the
literature on the context-independent selection and evaluation of distance
measures is limited in the sense that prior knowledge is used. In cluster
analysis, current studies evaluate the choice of distance measure after
applying unsupervised methods based on error probabilities, implicitly setting
the goal of reproducing predefined partitions in data. Such studies use
clusters of data that are often based on the context of the data as well as the
custom goal of the specific study. Depending on the data context, different
properties for distance distributions are judged to be relevant for appropriate
distance selection. However, if cluster analysis is based on the task of
finding similar partitions of data, then the intrapartition distances should be
smaller than the interpartition distances. By systematically investigating this
specification using distribution analysis through a mirrored-density plot, it
is shown that multimodal distance distributions are preferable in cluster
analysis. As a consequence, it is advantageous to model distance distributions
with Gaussian mixtures prior to the evaluation phase of unsupervised methods.
Experiments are performed on several artificial datasets and natural datasets
for the task of clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thrun_M/0/1/0/all/0/1"&gt;Michael C. Thrun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Stochastic Optimization in Separable Learning Environments. (arXiv:2108.09585v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.09585</id>
        <link href="http://arxiv.org/abs/2108.09585"/>
        <updated>2021-08-24T01:40:27.014Z</updated>
        <summary type="html"><![CDATA[We consider a class of sequential decision-making problems under uncertainty
that can encompass various types of supervised learning concepts. These
problems have a completely observed state process and a partially observed
modulation process, where the state process is affected by the modulation
process only through an observation process, the observation process only
observes the modulation process, and the modulation process is exogenous to
control. We model this broad class of problems as a partially observed Markov
decision process (POMDP). The belief function for the modulation process is
control invariant, thus separating the estimation of the modulation process
from the control of the state process. We call this specially structured POMDP
the separable POMDP, or SEP-POMDP, and show it (i) can serve as a model for a
broad class of application areas, e.g., inventory control, finance, healthcare
systems, (ii) inherits value function and optimal policy structure from a set
of completely observed MDPs, (iii) can serve as a bridge between classical
models of sequential decision making under uncertainty having fully specified
model artifacts and such models that are not fully specified and require the
use of predictive methods from statistics and machine learning, and (iv) allows
for specialized approximate solution procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bishop_R/0/1/0/all/0/1"&gt;R. Reid Bishop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+White_C/0/1/0/all/0/1"&gt;Chelsea C. White III&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Delineation of Geological Structures using Orthogonal Latent Space Projection. (arXiv:2108.09605v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2108.09605</id>
        <link href="http://arxiv.org/abs/2108.09605"/>
        <updated>2021-08-24T01:40:26.999Z</updated>
        <summary type="html"><![CDATA[We developed two machine learning frameworks that could assist in automated
litho-stratigraphic interpretation of seismic volumes without any manual hand
labeling from an experienced seismic interpreter. The first framework is an
unsupervised hierarchical clustering model to divide seismic images from a
volume into certain number of clusters determined by the algorithm. The
clustering framework uses a combination of density and hierarchical techniques
to determine the size and homogeneity of the clusters. The second framework
consists of a self-supervised deep learning framework to label regions of
geological interest in seismic images. It projects the latent-space of an
encoder-decoder architecture unto two orthogonal subspaces, from which it
learns to delineate regions of interest in the seismic images. To demonstrate
an application of both frameworks, a seismic volume was clustered into various
contiguous clusters, from which four clusters were selected based on distinct
seismic patterns: horizons, faults, salt domes and chaotic structures. Images
from the selected clusters are used to train the encoder-decoder network. The
output of the encoder-decoder network is a probability map of the possibility
an amplitude reflection event belongs to an interesting geological structure.
The structures are delineated using the probability map. The delineated images
are further used to post-train a segmentation model to extend our results to
full-vertical sections. The results on vertical sections show that we can
factorize a seismic volume into its corresponding structural components.
Lastly, we showed that our deep learning framework could be modeled as an
attribute extractor and we compared our attribute result with various existing
attributes in literature and demonstrate competitive performance with them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Aribido_O/0/1/0/all/0/1"&gt;Oluwaseun Joseph Aribido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+AlRegib_G/0/1/0/all/0/1"&gt;Ghassan AlRegib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Alaudah_Y/0/1/0/all/0/1"&gt;Yazeed Alaudah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Term Interrelations and Trends in Software Engineering. (arXiv:2108.09529v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.09529</id>
        <link href="http://arxiv.org/abs/2108.09529"/>
        <updated>2021-08-24T01:40:26.993Z</updated>
        <summary type="html"><![CDATA[The Software Engineering (SE) community is prolific, making it challenging
for experts to keep up with the flood of new papers and for neophytes to enter
the field. Therefore, we posit that the community may benefit from a tool
extracting terms and their interrelations from the SE community's text corpus
and showing terms' trends. In this paper, we build a prototyping tool using the
word embedding technique. We train the embeddings on the SE Body of Knowledge
handbook and 15,233 research papers' titles and abstracts. We also create test
cases necessary for validation of the training of the embeddings. We provide
representative examples showing that the embeddings may aid in summarizing
terms and uncovering trends in the knowledge base.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baskararajah_J/0/1/0/all/0/1"&gt;Janusan Baskararajah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miranskyy_A/0/1/0/all/0/1"&gt;Andriy Miranskyy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Task Learning Framework for COVID-19 Monitoring and Prediction of PPE Demand in Community Health Centres. (arXiv:2108.09402v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09402</id>
        <link href="http://arxiv.org/abs/2108.09402"/>
        <updated>2021-08-24T01:40:26.956Z</updated>
        <summary type="html"><![CDATA[Currently, the world seeks to find appropriate mitigation techniques to
control and prevent the spread of the new SARS-CoV-2. In our paper herein, we
present a peculiar Multi-Task Learning framework that jointly predicts the
effect of SARS-CoV-2 as well as Personal-Protective-Equipment consumption in
Community Health Centres for a given populace. Predicting the effect of the
virus (SARS-CoV-2), via studies and analyses, enables us to understand the
nature of SARS-CoV- 2 with reference to factors that promote its growth and
spread. Therefore, these foster widespread awareness; and the populace can
become more proactive and cautious so as to mitigate the spread of Corona Virus
Disease 2019 (COVID- 19). Furthermore, understanding and predicting the demand
for Personal Protective Equipment promotes the efficiency and safety of
healthcare workers in Community Health Centres. Owing to the novel nature and
strains of SARS-CoV-2, relatively few literature and research exist in this
regard. These existing literature have attempted to solve the problem
statement(s) using either Agent-based Models, Machine Learning Models, or
Mathematical Models. In view of this, our work herein adds to existing
literature via modeling our problem statements as Multi- Task Learning
problems. Results from our research indicate that government actions and human
factors are the most significant determinants that influence the spread of
SARS-CoV-2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Molokwu_B/0/1/0/all/0/1"&gt;Bonaventure Chidube Molokwu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuvo_S/0/1/0/all/0/1"&gt;Shaon Bhatta Shuvo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobti_Z/0/1/0/all/0/1"&gt;Ziad Kobti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snowdon_A/0/1/0/all/0/1"&gt;Anne Snowdon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ExamGAN and Twin-ExamGAN for Exam Script Generation. (arXiv:2108.09656v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09656</id>
        <link href="http://arxiv.org/abs/2108.09656"/>
        <updated>2021-08-24T01:40:26.937Z</updated>
        <summary type="html"><![CDATA[Nowadays, the learning management system (LMS) has been widely used in
different educational stages from primary to tertiary education for student
administration, documentation, tracking, reporting, and delivery of educational
courses, training programs, or learning and development programs. Towards
effective learning outcome assessment, the exam script generation problem has
attracted many attentions and been investigated recently. But the research in
this field is still in its early stage. There are opportunities to further
improve the quality of generated exam scripts in various aspects. In
particular, two essential issues have been ignored largely by existing
solutions. First, given a course, it is unknown yet how to generate an exam
script which can result in a desirable distribution of student scores in a
class (or across different classes). Second, while it is frequently encountered
in practice, it is unknown so far how to generate a pair of high quality exam
scripts which are equivalent in assessment (i.e., the student scores are
comparable by taking either of them) but have significantly different sets of
questions. To fill the gap, this paper proposes ExamGAN (Exam Script Generative
Adversarial Network) to generate high quality exam scripts, and then extends
ExamGAN to T-ExamGAN (Twin-ExamGAN) to generate a pair of high quality exam
scripts. Based on extensive experiments on three benchmark datasets, it has
verified the superiority of proposed solutions in various aspects against the
state-of-the-art. Moreover, we have conducted a case study which demonstrated
the effectiveness of proposed solution in a real teaching scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhengyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1"&gt;Ke Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Judy Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yong Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02180</id>
        <link href="http://arxiv.org/abs/2108.02180"/>
        <updated>2021-08-24T01:40:26.918Z</updated>
        <summary type="html"><![CDATA[We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1"&gt;Tom Braude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early-exit deep neural networks for distorted images: providing an efficient edge offloading. (arXiv:2108.09343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09343</id>
        <link href="http://arxiv.org/abs/2108.09343"/>
        <updated>2021-08-24T01:40:26.896Z</updated>
        <summary type="html"><![CDATA[Edge offloading for deep neural networks (DNNs) can be adaptive to the
input's complexity by using early-exit DNNs. These DNNs have side branches
throughout their architecture, allowing the inference to end earlier in the
edge. The branches estimate the accuracy for a given input. If this estimated
accuracy reaches a threshold, the inference ends on the edge. Otherwise, the
edge offloads the inference to the cloud to process the remaining DNN layers.
However, DNNs for image classification deals with distorted images, which
negatively impact the branches' estimated accuracy. Consequently, the edge
offloads more inferences to the cloud. This work introduces expert side
branches trained on a particular distortion type to improve robustness against
image distortion. The edge detects the distortion type and selects appropriate
expert branches to perform the inference. This approach increases the estimated
accuracy on the edge, improving the offloading decisions. We validate our
proposal in a realistic scenario, in which the edge offloads DNN inference to
Amazon EC2 instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1"&gt;Roberto G. Pacheco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Fernanda D.V.R. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couto_R/0/1/0/all/0/1"&gt;Rodrigo S. Couto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integer-arithmetic-only Certified Robustness for Quantized Neural Networks. (arXiv:2108.09413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09413</id>
        <link href="http://arxiv.org/abs/2108.09413"/>
        <updated>2021-08-24T01:40:26.882Z</updated>
        <summary type="html"><![CDATA[Adversarial data examples have drawn significant attention from the machine
learning and security communities. A line of work on tackling adversarial
examples is certified robustness via randomized smoothing that can provide a
theoretical robustness guarantee. However, such a mechanism usually uses
floating-point arithmetic for calculations in inference and requires large
memory footprints and daunting computational costs. These defensive models
cannot run efficiently on edge devices nor be deployed on integer-only logical
units such as Turing Tensor Cores or integer-only ARM processors. To overcome
these challenges, we propose an integer randomized smoothing approach with
quantization to convert any classifier into a new smoothed classifier, which
uses integer-only arithmetic for certified robustness against adversarial
perturbations. We prove a tight robustness guarantee under L2-norm for the
proposed approach. We show our approach can obtain a comparable accuracy and
4x~5x speedup over floating-point arithmetic certified robust methods on
general-purpose CPUs and mobile devices on two distinct datasets (CIFAR-10 and
Caltech-101).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haowen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahabi_C/0/1/0/all/0/1"&gt;Cyrus Shahabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offensive Language and Hate Speech Detection with Deep Learning and Transfer Learning. (arXiv:2108.03305v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03305</id>
        <link href="http://arxiv.org/abs/2108.03305"/>
        <updated>2021-08-24T01:40:26.873Z</updated>
        <summary type="html"><![CDATA[Toxic online speech has become a crucial problem nowadays due to an
exponential increase in the use of internet by people from different cultures
and educational backgrounds. Differentiating if a text message belongs to hate
speech and offensive language is a key challenge in automatic detection of
toxic text content. In this paper, we propose an approach to automatically
classify tweets into three classes: Hate, offensive and Neither. Using public
tweet data set, we first perform experiments to build BI-LSTM models from empty
embedding and then we also try the same neural network architecture with
pre-trained Glove embedding. Next, we introduce a transfer learning approach
for hate speech detection using an existing pre-trained language model BERT
(Bidirectional Encoder Representations from Transformers), DistilBert
(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform
hyper parameters tuning analysis of our best model (BI-LSTM) considering
different neural network architectures, learn-ratings and normalization methods
etc. After tuning the model and with the best combination of parameters, we
achieve over 92 percent accuracy upon evaluating it on test data. We also
create a class module which contains main functionality including text
classification, sentiment checking and text data augmentation. This model could
serve as an intermediate module between user and Twitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1"&gt;Bencheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jason Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ajay Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umair_H/0/1/0/all/0/1"&gt;Hafiza Umair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vovor_A/0/1/0/all/0/1"&gt;Atsu Vovor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durzynski_N/0/1/0/all/0/1"&gt;Natalie Durzynski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model. (arXiv:2108.04556v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04556</id>
        <link href="http://arxiv.org/abs/2108.04556"/>
        <updated>2021-08-24T01:40:26.866Z</updated>
        <summary type="html"><![CDATA[Code pre-trained models have shown great success in various code-related
tasks, such as code search, code clone detection, and code translation. Most
existing code pre-trained models often treat a code snippet as a plain sequence
of tokens. However, the inherent syntax and hierarchy that provide important
structure and semantic information are ignored. The native derived sequence
representations of them are insufficient. To this end, we propose CLSEBERT, a
Contrastive Learning Framework for Syntax Enhanced Code Pre-Trained Model, to
deal with various code intelligence tasks. In the pre-training stage, we
consider the code syntax and hierarchy contained in the Abstract Syntax Tree
(AST) and leverage the Contrastive Learning (CL) to learn noise-invariant code
representations. Besides the original masked language model (MLM) objective, we
also introduce two novel pre-training objectives: (1) ``AST Node Edge
Prediction (NEP)'' to predict edges between nodes in the abstract syntax tree;
(2) ``Code Token Type Prediction (TTP)'' to predict the types of code tokens.
Extensive experiments on four code intelligence tasks demonstrate the superior
performance of CLSEBERT compared to state-of-the-art at the same pre-training
corpus and parameter scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pingyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1"&gt;Fei Mi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1"&gt;Meng Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yadao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEMix Layers: Disentangling Domains for Modular Language Modeling. (arXiv:2108.05036v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.05036</id>
        <link href="http://arxiv.org/abs/2108.05036"/>
        <updated>2021-08-24T01:40:26.859Z</updated>
        <summary type="html"><![CDATA[We introduce a new domain expert mixture (DEMix) layer that enables
conditioning a language model (LM) on the domain of the input text. A DEMix
layer is a collection of expert feedforward networks, each specialized to a
domain, that makes the LM modular: experts can be mixed, added or removed after
initial training. Extensive experiments with autoregressive transformer LMs (up
to 1.3B parameters) show that DEMix layers reduce test-time perplexity,
increase training efficiency, and enable rapid adaptation with little overhead.
We show that mixing experts during inference, using a parameter-free weighted
ensemble, allows the model to better generalize to heterogeneous or unseen
domains. We also show that experts can be added to iteratively incorporate new
domains without forgetting older ones, and that experts can be removed to
restrict access to unwanted domains, without additional training. Overall,
these results demonstrate benefits of explicitly conditioning on textual
domains during language modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1"&gt;Suchin Gururangan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1"&gt;Ari Holtzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Characterization of Spatiotemporal Data Manifolds. (arXiv:2108.09545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09545</id>
        <link href="http://arxiv.org/abs/2108.09545"/>
        <updated>2021-08-24T01:40:26.845Z</updated>
        <summary type="html"><![CDATA[Spatiotemporal (ST) image data are increasingly common and often
high-dimensional (high-D). Modeling ST data can be a challenge due to the
plethora of independent and interacting processes which may or may not
contribute to the measurements. Characterization can be considered the
complement to modeling by helping guide assumptions about generative processes
and their representation in the data. Dimensionality reduction (DR) is a
frequently implemented type of characterization designed to mitigate the "curse
of dimensionality" on high-D signals. For decades, Principal Component (PC) and
Empirical Orthogonal Function (EOF) analysis has been used as a linear,
invertible approach to DR and ST analysis. Recent years have seen the
additional development of a suite of nonlinear DR algorithms, frequently
categorized as "manifold learning". Here, we explore the idea of joint
characterization of ST data manifolds using PCs/EOFs alongside two nonlinear DR
approaches: Laplacian Eigenmaps (LE) and t-distributed stochastic neighbor
embedding (t-SNE). Starting with a synthetic example and progressing to global,
regional, and field scale ST datasets spanning roughly 5 orders of magnitude in
space and 2 in time, we show these three DR approaches can yield complementary
information about ST manifold topology. Compared to the relatively diffuse TFS
produced by PCs/EOFs, the nonlinear approaches yield more compact manifolds
with decreased ambiguity in temporal endmembers (LE) and/or in spatiotemporal
clustering (t-SNE). These properties are compensated by the greater
interpretability, significantly lower computational demand and diminished
sensitivity to spatial aliasing for PCs/EOFs than LE or t-SNE. Taken together,
we find joint characterization using the three complementary DR approaches
capable of greater insight into generative ST processes than possible using any
single approach alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sousa_D/0/1/0/all/0/1"&gt;Daniel Sousa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Small_C/0/1/0/all/0/1"&gt;Christopher Small&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Survey on Schema-based Event Extraction with Deep Learning. (arXiv:2107.02126v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02126</id>
        <link href="http://arxiv.org/abs/2107.02126"/>
        <updated>2021-08-24T01:40:26.827Z</updated>
        <summary type="html"><![CDATA[Schema-based event extraction is a critical technique to apprehend the
essential content of events promptly. With the rapid development of deep
learning technology, event extraction technology based on deep learning has
become a research hotspot. Numerous methods, datasets, and evaluation metrics
have been proposed in the literature, raising the need for a comprehensive and
updated survey. This paper fills the gap by reviewing the state-of-the-art
approaches, focusing on deep learning-based models. We summarize the task
definition, paradigm, and models of schema-based event extraction and then
discuss each of these in detail. We introduce benchmark datasets that support
tests of predictions and evaluation metrics. A comprehensive comparison between
different techniques is also provided in this survey. Finally, we conclude by
summarizing future research directions facing the research area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1"&gt;Yiming Hei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Rui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1"&gt;Jiawei Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1"&gt;Amin Beheshti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sangrahaka: A Tool for Annotating and Querying Knowledge Graphs. (arXiv:2107.02782v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02782</id>
        <link href="http://arxiv.org/abs/2107.02782"/>
        <updated>2021-08-24T01:40:26.748Z</updated>
        <summary type="html"><![CDATA[In this work, we present a web-based annotation and querying tool Sangrahaka.
It annotates entities and relationships from text corpora and constructs a
knowledge graph (KG). The KG is queried using templatized natural language
queries. The application is language and corpus agnostic, but can be tuned for
special needs of a specific language or a corpus. A customized version of the
framework has been used in two annotation tasks. The application is available
for download and installation. Besides having a user-friendly interface, it is
fast, supports customization, and is fault tolerant on both client and server
side. The code is available at https://github.com/hrishikeshrt/sangrahaka and
the presentation with a demo is available at https://youtu.be/nw9GFLVZMMo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terdalkar_H/0/1/0/all/0/1"&gt;Hrishikesh Terdalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Document-level Relation Extraction as Semantic Segmentation. (arXiv:2106.03618v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03618</id>
        <link href="http://arxiv.org/abs/2106.03618"/>
        <updated>2021-08-24T01:40:26.692Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction aims to extract relations among multiple
entity pairs from a document. Previously proposed graph-based or
transformer-based models utilize the entities independently, regardless of
global information among relational triples. This paper approaches the problem
by predicting an entity-level relation matrix to capture local and global
information, parallel to the semantic segmentation task in computer vision.
Herein, we propose a Document U-shaped Network for document-level relation
extraction. Specifically, we leverage an encoder module to capture the context
information of entities and a U-shaped segmentation module over the image-style
feature map to capture global interdependency among triples. Experimental
results show that our approach can obtain state-of-the-art performance on three
benchmark datasets DocRED, CDR, and GDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mosha Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Contrastive Learning for Learning Robust Textual Representations. (arXiv:2104.04907v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04907</id>
        <link href="http://arxiv.org/abs/2104.04907"/>
        <updated>2021-08-24T01:40:26.685Z</updated>
        <summary type="html"><![CDATA[Although the self-supervised pre-training of transformer models has resulted
in the revolutionizing of natural language processing (NLP) applications and
the achievement of state-of-the-art results with regard to various benchmarks,
this process is still vulnerable to small and imperceptible permutations
originating from legitimate inputs. Intuitively, the representations should be
similar in the feature space with subtle input permutations, while large
variations occur with different meanings. This motivates us to investigate the
learning of robust textual representation in a contrastive manner. However, it
is non-trivial to obtain opposing semantic instances for textual samples. In
this study, we propose a disentangled contrastive learning method that
separately optimizes the uniformity and alignment of representations without
negative sampling. Specifically, we introduce the concept of momentum
representation consistency to align features and leverage power normalization
while conforming the uniformity. Our experimental results for the NLP
benchmarks demonstrate that our approach can obtain better results compared
with the baselines, as well as achieve promising improvements with invariance
tests and adversarial attacks. The code is available in
https://github.com/zxlzr/DCL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1"&gt;Zhen Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hongbin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfographicVQA. (arXiv:2104.12756v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12756</id>
        <link href="http://arxiv.org/abs/2104.12756"/>
        <updated>2021-08-24T01:40:26.666Z</updated>
        <summary type="html"><![CDATA[Infographics are documents designed to effectively communicate information
using a combination of textual, graphical and visual elements. In this work, we
explore the automatic understanding of infographic images by using Visual
Question Answering technique.To this end, we present InfographicVQA, a new
dataset that comprises a diverse collection of infographics along with natural
language questions and answers annotations. The collected questions require
methods to jointly reason over the document layout, textual content, graphical
elements, and data visualizations. We curate the dataset with emphasis on
questions that require elementary reasoning and basic arithmetic skills.
Finally, we evaluate two strong baselines based on state of the art multi-modal
VQA models, and establish baseline performance for the new task. The dataset,
code and leaderboard will be made available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1"&gt;Minesh Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagal_V/0/1/0/all/0/1"&gt;Viraj Bagal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1"&gt;Rub&amp;#xe8;n P&amp;#xe9;rez Tito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1"&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1"&gt;Ernest Valveny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C.V Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-08-24T01:40:26.651Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples for Relation Extraction. (arXiv:2104.00312v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00312</id>
        <link href="http://arxiv.org/abs/2104.00312"/>
        <updated>2021-08-24T01:40:26.614Z</updated>
        <summary type="html"><![CDATA[Recent neural-based relation extraction approaches, though achieving
promising improvement on benchmark datasets, have reported their vulnerability
towards adversarial attacks. Thus far, efforts mostly focused on generating
adversarial samples or defending adversarial attacks, but little is known about
the difference between normal and adversarial samples. In this work, we take
the first step to leverage the salience-based method to analyze those
adversarial samples. We observe that salience tokens have a direct correlation
with adversarial perturbations. We further find the adversarial perturbations
are either those tokens not existing in the training set or superficial cues
associated with relation labels. To some extent, our approach unveils the
characters against adversarial samples. We release an open-source testbed,
"DiagnoseAdv".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luoqiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mosha Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASQ: Automatically Generating Question-Answer Pairs using AMRs. (arXiv:2105.10023v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10023</id>
        <link href="http://arxiv.org/abs/2105.10023"/>
        <updated>2021-08-24T01:40:26.603Z</updated>
        <summary type="html"><![CDATA[We introduce ASQ, a tool to automatically mine questions and answers from a
sentence using the Abstract Meaning Representation (AMR). Previous work has
used question-answer pairs to specify the predicate-argument structure of a
sentence using natural language, which does not require linguistic expertise or
training, and created datasets such as QA-SRL and QAMR, for which the
question-answer pair annotations were crowdsourced. Our goal is to build a tool
(ASQ) that maps from the traditional meaning representation AMR to a
question-answer meaning representation (QMR). This enables construction of QMR
datasets automatically in various domains using existing high-quality AMR
parsers, and provides an automatic mapping AMR to QMR for ease of understanding
by non-experts. A qualitative evaluation of the output generated by ASQ from
the AMR 2.0 data shows that the question-answer pairs are natural and valid,
and demonstrate good coverage of the content. We run ASQ on the sentences from
the QAMR dataset, to observe that the semantic roles in QAMR are also captured
by ASQ. We intend to make this tool and the results publicly available for
others to use and build upon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rakshit_G/0/1/0/all/0/1"&gt;Geetanjali Rakshit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1"&gt;Jeffrey Flanigan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02284</id>
        <link href="http://arxiv.org/abs/2104.02284"/>
        <updated>2021-08-24T01:40:26.589Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed the prosperity of legal artificial intelligence
with the development of technologies. In this paper, we propose a novel legal
application of legal provision prediction (LPP), which aims to predict the
related legal provisions of affairs. We formulate this task as a challenging
knowledge graph completion problem, which requires not only text understanding
but also graph reasoning. To this end, we propose a novel text-guided graph
reasoning approach. We collect amounts of real-world legal provision data from
the Guangdong government service website and construct a legal dataset called
LegalLPP. Extensive experimental results on the dataset show that our approach
achieves better performance compared with baselines. The code and dataset are
available in \url{https://github.com/zxlzr/LegalPP} for reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luoqiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1"&gt;Zhen Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hongbin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1"&gt;Huaixiao Tou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Extraction by Associating Event Types and Argument Roles. (arXiv:2108.10038v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.10038</id>
        <link href="http://arxiv.org/abs/2108.10038"/>
        <updated>2021-08-24T01:40:26.582Z</updated>
        <summary type="html"><![CDATA[Event extraction (EE), which acquires structural event knowledge from texts,
can be divided into two sub-tasks: event type classification and element
extraction (namely identifying triggers and arguments under different role
patterns). As different event types always own distinct extraction schemas
(i.e., role patterns), previous work on EE usually follows an isolated learning
paradigm, performing element extraction independently for different event
types. It ignores meaningful associations among event types and argument roles,
leading to relatively poor performance for less frequent types/roles. This
paper proposes a novel neural association framework for the EE task. Given a
document, it first performs type classification via constructing a
document-level graph to associate sentence nodes of different types, and
adopting a graph attention network to learn sentence embeddings. Then, element
extraction is achieved by building a universal schema of argument roles, with a
parameter inheritance mechanism to enhance role preference for extracted
elements. As such, our model takes into account type and role associations
during EE, enabling implicit information sharing among them. Experimental
results show that our approach consistently outperforms most state-of-the-art
EE methods in both sub-tasks. Particularly, for types/roles with less training
data, the performance is superior to the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1"&gt;Jiawei Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaohan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models across Languages. (arXiv:2102.02585v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02585</id>
        <link href="http://arxiv.org/abs/2102.02585"/>
        <updated>2021-08-24T01:40:26.575Z</updated>
        <summary type="html"><![CDATA[Unsupervised representation learning of words from large multilingual corpora
is useful for downstream tasks such as word sense disambiguation, semantic text
similarity, and information retrieval. The representation precision of
log-bilinear fastText models is mostly due to their use of subword information.
In previous work, the optimization of fastText's subword sizes has not been
fully explored, and non-English fastText models were trained using subword
sizes optimized for English and German word analogy tasks. In our work, we find
the optimal subword sizes on the English, German, Czech, Italian, Spanish,
French, Hindi, Turkish, and Russian word analogy tasks. We then propose a
simple n-gram coverage model and we show that it predicts better-than-default
subword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy
tasks. We show that the optimization of fastText's subword sizes matters and
results in a 14% improvement on the Czech word analogy task. We also show that
expensive parameter optimization can be replaced by a simple n-gram coverage
model that consistently improves the accuracy of fastText models on the word
analogy tasks by up to 3% compared to the default subword sizes, and that it is
within 1% accuracy of the optimal subword sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1"&gt;V&amp;#xed;t Novotn&amp;#xfd;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1"&gt;Eniafe Festus Ayetiran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Bacovsky_D/0/1/0/all/0/1"&gt;Dalibor Ba&amp;#x10d;ovsk&amp;#xfd;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Lupt&amp;#xe1;k&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1"&gt;Michal &amp;#x160;tef&amp;#xe1;nik&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1"&gt;Petr Sojka&lt;/a&gt; (1) ((1) Faculty of Informatics Masaryk University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Answer from Multi-spans for Reading Comprehension. (arXiv:2009.06141v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06141</id>
        <link href="http://arxiv.org/abs/2009.06141"/>
        <updated>2021-08-24T01:40:26.569Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel method to generate answers for non-extraction
machine reading comprehension (MRC) tasks whose answers cannot be simply
extracted as one span from the given passages. Using a pointer network-style
extractive decoder for such type of MRC may result in unsatisfactory
performance when the ground-truth answers are given by human annotators or
highly re-paraphrased from parts of the passages. On the other hand, using
generative decoder cannot well guarantee the resulted answers with well-formed
syntax and semantics when encountering long sentences. Therefore, to alleviate
the obvious drawbacks of both sides, we propose an answer making-up method from
extracted multi-spans that are learned by our model as highly confident
$n$-gram candidates in the given passage. That is, the returned answers are
composed of discontinuous multi-spans but not just one consecutive span in the
given passages anymore. The proposed method is simple but effective: empirical
experiments on MS MARCO show that the proposed method has a better performance
on accurately generating long answers, and substantially outperforms two
competitive typical one-span and Seq2Seq baseline decoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. (arXiv:1906.04225v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.04225</id>
        <link href="http://arxiv.org/abs/1906.04225"/>
        <updated>2021-08-24T01:40:26.563Z</updated>
        <summary type="html"><![CDATA[Retrieve-and-edit based approaches to structured prediction, where structures
associated with retrieved neighbors are edited to form new structures, have
recently attracted increased interest. However, much recent work merely
conditions on retrieved structures (e.g., in a sequence-to-sequence framework),
rather than explicitly manipulating them. We show we can perform accurate
sequence labeling by explicitly (and only) copying labels from retrieved
neighbors. Moreover, because this copying is label-agnostic, we can achieve
impressive performance when transferring to new sequence-labeling tasks without
retraining. We additionally consider a dynamic programming approach to sequence
labeling in the presence of retrieved neighbors, which allows for controlling
the number of distinct (copied) segments used to form a prediction, and leads
to both more interpretable and accurate predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1"&gt;Sam Wiseman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1"&gt;Karl Stratos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Granularity and Cost of Annotation in Clinical Sequence Labeling. (arXiv:2108.09913v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09913</id>
        <link href="http://arxiv.org/abs/2108.09913"/>
        <updated>2021-08-24T01:40:26.556Z</updated>
        <summary type="html"><![CDATA[Well-annotated datasets, as shown in recent top studies, are becoming more
important for researchers than ever before in supervised machine learning (ML).
However, the dataset annotation process and its related human labor costs
remain overlooked. In this work, we analyze the relationship between the
annotation granularity and ML performance in sequence labeling, using clinical
records from nursing shift-change handover. We first study a model derived from
textual language features alone, without additional information based on
nursing knowledge. We find that this sequence tagger performs well in most
categories under this granularity. Then, we further include the additional
manual annotations by a nurse, and find the sequence tagging performance
remaining nearly the same. Finally, we give a guideline and reference to the
community arguing it is not necessary and even not recommended to annotate in
detailed granularity because of a low Return on Investment. Therefore we
recommend emphasizing other features, like textual knowledge, for researchers
and practitioners as a cost-effective source for increasing the sequence
labeling performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haozhan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenchen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suominen_H/0/1/0/all/0/1"&gt;Hanna Suominen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training. (arXiv:2010.15366v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15366</id>
        <link href="http://arxiv.org/abs/2010.15366"/>
        <updated>2021-08-24T01:40:26.549Z</updated>
        <summary type="html"><![CDATA[Speech separation has been well developed, with the very successful
permutation invariant training (PIT) approach, although the frequent label
assignment switching happening during PIT training remains to be a problem when
better convergence speed and achievable performance are desired. In this paper,
we propose to perform self-supervised pre-training to stabilize the label
assignment in training the speech separation model. Experiments over several
types of self-supervised approaches, several typical speech separation models
and two different datasets showed that very good improvements are achievable if
a proper self-supervised approach is chosen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sung-Feng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1"&gt;Shun-Po Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Da-Rong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Gene-Ping Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Transformer-based Framework for Duplex Text Normalization. (arXiv:2108.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09889</id>
        <link href="http://arxiv.org/abs/2108.09889"/>
        <updated>2021-08-24T01:40:26.541Z</updated>
        <summary type="html"><![CDATA[Text normalization (TN) and inverse text normalization (ITN) are essential
preprocessing and postprocessing steps for text-to-speech synthesis and
automatic speech recognition, respectively. Many methods have been proposed for
either TN or ITN, ranging from weighted finite-state transducers to neural
networks. Despite their impressive performance, these methods aim to tackle
only one of the two tasks but not both. As a result, in a complete spoken
dialog system, two separate models for TN and ITN need to be built. This
heterogeneity increases the technical complexity of the system, which in turn
increases the cost of maintenance in a production setting. Motivated by this
observation, we propose a unified framework for building a single neural duplex
system that can simultaneously handle TN and ITN. Combined with a simple but
effective data augmentation method, our systems achieve state-of-the-art
results on the Google TN dataset for English and Russian. They can also reach
over 95% sentence-level accuracy on an internal English TN dataset without any
additional fine-tuning. In addition, we also create a cleaned dataset from the
Spoken Wikipedia Corpora for German and report the performance of our systems
on the dataset. Overall, experimental results demonstrate the proposed duplex
text normalization framework is highly effective and applicable to a range of
domains and languages]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1"&gt;Tuan Manh Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1"&gt;Evelina Bakhturina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1"&gt;Boris Ginsburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sarcasm Detection in Twitter -- Performance Impact when using Data Augmentation: Word Embeddings. (arXiv:2108.09924v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09924</id>
        <link href="http://arxiv.org/abs/2108.09924"/>
        <updated>2021-08-24T01:40:26.495Z</updated>
        <summary type="html"><![CDATA[Sarcasm is the use of words usually used to either mock or annoy someone, or
for humorous purposes. Sarcasm is largely used in social networks and
microblogging websites, where people mock or censure in a way that makes it
difficult even for humans to tell if what is said is what is meant. Failure to
identify sarcastic utterances in Natural Language Processing applications such
as sentiment analysis and opinion mining will confuse classification algorithms
and generate false results. Several studies on sarcasm detection have utilized
different learning algorithms. However, most of these learning models have
always focused on the contents of expression only, leaving the contextual
information in isolation. As a result, they failed to capture the contextual
information in the sarcastic expression. Moreover, some datasets used in
several studies have an unbalanced dataset which impacting the model result. In
this paper, we propose a contextual model for sarcasm identification in twitter
using RoBERTa, and augmenting the dataset by applying Global Vector
representation (GloVe) for the construction of word embedding and context
learning to generate more data and balancing the dataset. The effectiveness of
this technique is tested with various datasets and data augmentation settings.
In particular, we achieve performance gain by 3.2% in the iSarcasm dataset when
using data augmentation to increase 20% of data labeled as sarcastic, resulting
F-score of 40.4% compared to 37.2% without data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Handoyo_A/0/1/0/all/0/1"&gt;Alif Tri Handoyo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hidayaturrahman/0/1/0/all/0/1"&gt;Hidayaturrahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhartono_D/0/1/0/all/0/1"&gt;Derwin Suhartono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hierarchical Entity Graph Convolutional Network for Relation Extraction across Documents. (arXiv:2108.09505v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09505</id>
        <link href="http://arxiv.org/abs/2108.09505"/>
        <updated>2021-08-24T01:40:26.483Z</updated>
        <summary type="html"><![CDATA[Distantly supervised datasets for relation extraction mostly focus on
sentence-level extraction, and they cover very few relations. In this work, we
propose cross-document relation extraction, where the two entities of a
relation tuple appear in two different documents that are connected via a chain
of common entities. Following this idea, we create a dataset for two-hop
relation extraction, where each chain contains exactly two documents. Our
proposed dataset covers a higher number of relations than the publicly
available sentence-level datasets. We also propose a hierarchical entity graph
convolutional network (HEGCN) model for this task that improves performance by
1.1\% F1 score on our two-hop relation extraction dataset, compared to some
strong neural baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1"&gt;Tapas Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1"&gt;Hwee Tou Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Cute is Pikachu? Gathering and Ranking Pok\'emon Properties from Data with Pok\'emon Word Embeddings. (arXiv:2108.09546v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09546</id>
        <link href="http://arxiv.org/abs/2108.09546"/>
        <updated>2021-08-24T01:40:26.476Z</updated>
        <summary type="html"><![CDATA[We present different methods for obtaining descriptive properties
automatically for the 151 original Pok\'emon. We train several different word
embeddings models on a crawled Pok\'emon corpus, and use them to rank
automatically English adjectives based on how characteristic they are to a
given Pok\'emon. Based on our experiments, it is better to train a model with
domain specific data than to use a pretrained model. Word2Vec produces less
noise in the results than fastText model. Furthermore, we expand the list of
properties for each Pok\'emon automatically. However, none of the methods is
spot on and there is a considerable amount of noise in the different semantic
models. Our models have been released on Zenodo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1"&gt;Mika H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1"&gt;Khalid Alnajjar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1"&gt;Niko Partanen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles. (arXiv:2108.09355v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09355</id>
        <link href="http://arxiv.org/abs/2108.09355"/>
        <updated>2021-08-24T01:40:26.463Z</updated>
        <summary type="html"><![CDATA[Personalized chatbots focus on endowing chatbots with a consistent
personality to behave like real users, give more informative responses, and
further act as personal assistants. Existing personalized approaches tried to
incorporate several text descriptions as explicit user profiles. However, the
acquisition of such explicit profiles is expensive and time-consuming, thus
being impractical for large-scale real-world applications. Moreover, the
restricted predefined profile neglects the language behavior of a real user and
cannot be automatically updated together with the change of user interests. In
this paper, we propose to learn implicit user profiles automatically from
large-scale user dialogue history for building personalized chatbots.
Specifically, leveraging the benefits of Transformer on language understanding,
we train a personalized language model to construct a general user profile from
the user's historical responses. To highlight the relevant historical responses
to the input post, we further establish a key-value memory network of
historical post-response pairs, and build a dynamic post-aware user profile.
The dynamic profile mainly describes what and how the user has responded to
similar posts in history. To explicitly utilize users' frequently used words,
we design a personalized decoder to fuse two decoding strategies, including
generating a word from the generic vocabulary and copying one word from the
user's personalized vocabulary. Experiments on two real-world datasets show the
significant improvement of our model compared with existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengyi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yutao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Hanxun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic-Preserving Adversarial Text Attacks. (arXiv:2108.10015v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.10015</id>
        <link href="http://arxiv.org/abs/2108.10015"/>
        <updated>2021-08-24T01:40:26.446Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are known to be vulnerable to adversarial images,
while their robustness in text classification is rarely studied. Several lines
of text attack methods have been proposed in the literature, including
character-level, word-level, and sentence-level attacks. However, it is still a
challenge to minimize the number of word changes necessary to induce
misclassification, while simultaneously ensuring lexical correctness, syntactic
soundness, and semantic similarity. In this paper, we propose a Bigram and
Unigram based adaptive Semantic Preservation Optimization (BU-SPO) method to
examine the vulnerability of deep models. Our method has four major merits.
Firstly, we propose to attack text documents not only at the unigram word level
but also at the bigram level which better keeps semantics and avoids producing
meaningless outputs. Secondly, we propose a hybrid method to replace the input
words with options among both their synonyms candidates and sememe candidates,
which greatly enriches the potential substitutions compared to only using
synonyms. Thirdly, we design an optimization algorithm, i.e., Semantic
Preservation Optimization (SPO), to determine the priority of word
replacements, aiming to reduce the modification cost. Finally, we further
improve the SPO with a semantic Filter (named SPOF) to find the adversarial
example with the highest semantic similarity. We evaluate the effectiveness of
our BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by
attacking four popular DNNs models. Results show that our methods achieve the
highest attack success rates and semantics rates by changing the smallest
number of words compared with existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1"&gt;James Bailey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tianqing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training. (arXiv:2108.09479v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.09479</id>
        <link href="http://arxiv.org/abs/2108.09479"/>
        <updated>2021-08-24T01:40:26.430Z</updated>
        <summary type="html"><![CDATA[Existing approaches to vision-language pre-training (VLP) heavily rely on an
object detector based on bounding boxes (regions), where salient objects are
first detected from images and then a Transformer-based model is used for
cross-modal fusion. Despite their superior performance, these approaches are
bounded by the capability of the object detector in terms of both effectiveness
and efficiency. Besides, the presence of object detection imposes unnecessary
constraints on model designs and makes it difficult to support end-to-end
training. In this paper, we revisit grid-based convolutional features for
vision-language pre-training, skipping the expensive region-related steps. We
propose a simple yet effective grid-based VLP method that works surprisingly
well with the grid features. By pre-training only with in-domain datasets, the
proposed Grid-VLP method can outperform most competitive region-based VLP
methods on three examined vision-language understanding tasks. We hope that our
findings help to further advance the state of the art of vision-language
pre-training, and provide a new direction towards effective and efficient VLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Junfeng Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1"&gt;Min Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Palmira: A Deep Deformable Network for Instance Segmentation of Dense and Uneven Layouts in Handwritten Manuscripts. (arXiv:2108.09436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09436</id>
        <link href="http://arxiv.org/abs/2108.09436"/>
        <updated>2021-08-24T01:40:26.408Z</updated>
        <summary type="html"><![CDATA[Handwritten documents are often characterized by dense and uneven layout.
Despite advances, standard deep network based approaches for semantic layout
segmentation are not robust to complex deformations seen across semantic
regions. This phenomenon is especially pronounced for the low-resource Indic
palm-leaf manuscript domain. To address the issue, we first introduce
Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with
semantic layout annotations. Indiscapes2 contains documents from four different
historical collections and is 150% larger than its predecessor, Indiscapes. We
also propose a novel deep network Palmira for robust, deformation-aware
instance segmentation of regions in handwritten manuscripts. We also report
Hausdorff distance and its variants as a boundary-aware performance measure.
Our experiments demonstrate that Palmira provides robust layouts, outperforms
strong baseline approaches and ablative variants. We also include qualitative
results on Arabic, South-East Asian and Hebrew historical manuscripts to
showcase the generalization capability of Palmira.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharan_P/0/1/0/all/0/1"&gt;Prema Satish Sharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitha_S/0/1/0/all/0/1"&gt;Sowmya Aitha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Amandeep Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Abhishek Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augustine_A/0/1/0/all/0/1"&gt;Aaron Augustine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance. (arXiv:2108.00644v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00644</id>
        <link href="http://arxiv.org/abs/2108.00644"/>
        <updated>2021-08-24T01:40:26.368Z</updated>
        <summary type="html"><![CDATA[Recently, Information Retrieval community has witnessed fast-paced advances
in Dense Retrieval (DR), which performs first-stage retrieval with
embedding-based search. Despite the impressive ranking performance, previous
studies usually adopt brute-force search to acquire candidates, which is
prohibitive in practical Web search scenarios due to its tremendous memory
usage and time cost. To overcome these problems, vector compression methods
have been adopted in many practical embedding-based retrieval applications. One
of the most popular methods is Product Quantization (PQ). However, although
existing vector compression methods including PQ can help improve the
efficiency of DR, they incur severely decayed retrieval performance due to the
separation between encoding and compression. To tackle this problem, we present
JPQ, which stands for Joint optimization of query encoding and Product
Quantization. It trains the query encoder and PQ index jointly in an end-to-end
manner based on three optimization strategies, namely ranking-oriented loss, PQ
centroid optimization, and end-to-end negative sampling. We evaluate JPQ on two
publicly available retrieval benchmarks. Experimental results show that JPQ
significantly outperforms popular vector compression methods. Compared with
previous DR models that use brute-force search, JPQ almost matches the best
retrieval performance with 30x compression on index size. The compressed index
further brings 10x speedup on CPU and 2x speedup on GPU in query latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1"&gt;Jingtao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiaxin Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiqun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaoping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alignment Knowledge Distillation for Online Streaming Attention-based Speech Recognition. (arXiv:2103.00422v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00422</id>
        <link href="http://arxiv.org/abs/2103.00422"/>
        <updated>2021-08-24T01:40:26.360Z</updated>
        <summary type="html"><![CDATA[This article describes an efficient training method for online streaming
attention-based encoder-decoder (AED) automatic speech recognition (ASR)
systems. AED models have achieved competitive performance in offline scenarios
by jointly optimizing all components. They have recently been extended to an
online streaming framework via models such as monotonic chunkwise attention
(MoChA). However, the elaborate attention calculation process is not robust for
long-form speech utterances. Moreover, the sequence-level training objective
and time-restricted streaming encoder cause a nonnegligible delay in token
emission during inference. To address these problems, we propose CTC
synchronous training (CTC-ST), in which CTC alignments are leveraged as a
reference for token boundaries to enable a MoChA model to learn optimal
monotonic input-output alignments. We formulate a purely end-to-end training
objective to synchronize the boundaries of MoChA to those of CTC. The CTC model
shares an encoder with the MoChA model to enhance the encoder representation.
Moreover, the proposed method provides alignment information learned in the CTC
branch to the attention-based decoder. Therefore, CTC-ST can be regarded as
self-distillation of alignment knowledge from CTC to MoChA. Experimental
evaluations on a variety of benchmark datasets show that the proposed method
significantly reduces recognition errors and emission latency simultaneously.
The robustness to long-form and noisy speech is also demonstrated. We compare
CTC-ST with several methods that distill alignment knowledge from a hybrid ASR
system and show that the CTC-ST can achieve a comparable tradeoff of accuracy
and latency without relying on external alignment information. The best MoChA
system shows recognition accuracy comparable to that of RNN-transducer (RNN-T)
while achieving lower emission latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1"&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Broad-Coverage Medical Entity Linking with Semantic Type Prediction and Large-Scale Datasets. (arXiv:2005.00460v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00460</id>
        <link href="http://arxiv.org/abs/2005.00460"/>
        <updated>2021-08-24T01:40:26.352Z</updated>
        <summary type="html"><![CDATA[Medical entity linking is the task of identifying and standardizing medical
concepts referred to in an unstructured text. Most of the existing methods
adopt a three-step approach of (1) detecting mentions, (2) generating a list of
candidate concepts, and finally (3) picking the best concept among them. In
this paper, we probe into alleviating the problem of overgeneration of
candidate concepts in the candidate generation module, the most under-studied
component of medical entity linking. For this, we present MedType, a fully
modular system that prunes out irrelevant candidate concepts based on the
predicted semantic type of an entity mention. We incorporate MedType into five
off-the-shelf toolkits for medical entity linking and demonstrate that it
consistently improves entity linking performance across several benchmark
datasets. To address the dearth of annotated training data for medical entity
linking, we present WikiMed and PubMedDS, two large-scale medical entity
linking datasets, and demonstrate that pre-training MedType on these datasets
further improves entity linking performance. We make our source code and
datasets publicly available for medical entity linking research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1"&gt;Shikhar Vashishth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_Griffis_D/0/1/0/all/0/1"&gt;Denis Newman-Griffis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Rishabh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1"&gt;Ritam Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1"&gt;Carolyn Rose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Video Game Player Burnout with the Use of Sensor Data and Machine Learning. (arXiv:2012.02299v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02299</id>
        <link href="http://arxiv.org/abs/2012.02299"/>
        <updated>2021-08-24T01:40:26.329Z</updated>
        <summary type="html"><![CDATA[Current research in eSports lacks the tools for proper game practising and
performance analytics. The majority of prior work relied only on in-game data
for advising the players on how to perform better. However, in-game mechanics
and trends are frequently changed by new patches limiting the lifespan of the
models trained exclusively on the in-game logs. In this article, we propose the
methods based on the sensor data analysis for predicting whether a player will
win the future encounter. The sensor data were collected from 10 participants
in 22 matches in League of Legends video game. We have trained machine learning
models including Transformer and Gated Recurrent Unit to predict whether the
player wins the encounter taking place after some fixed time in the future. For
10 seconds forecasting horizon Transformer neural network architecture achieves
ROC AUC score 0.706. This model is further developed into the detector capable
of predicting that a player will lose the encounter occurring in 10 seconds in
88.3% of cases with 73.5% accuracy. This might be used as a players' burnout or
fatigue detector, advising players to retreat. We have also investigated which
physiological features affect the chance to win or lose the next in-game
encounter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smerdov_A/0/1/0/all/0/1"&gt;Anton Smerdov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somov_A/0/1/0/all/0/1"&gt;Andrey Somov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1"&gt;Paul Lukowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer. (arXiv:2108.06625v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06625</id>
        <link href="http://arxiv.org/abs/2108.06625"/>
        <updated>2021-08-24T01:40:26.323Z</updated>
        <summary type="html"><![CDATA[In order to model the evolution of user preference, we should learn user/item
embeddings based on time-ordered item purchasing sequences, which is defined as
Sequential Recommendation (SR) problem. Existing methods leverage sequential
patterns to model item transitions. However, most of them ignore crucial
temporal collaborative signals, which are latent in evolving user-item
interactions and coexist with sequential patterns. Therefore, we propose to
unify sequential patterns and temporal collaborative signals to improve the
quality of recommendation, which is rather challenging. Firstly, it is hard to
simultaneously encode sequential patterns and collaborative signals. Secondly,
it is non-trivial to express the temporal effects of collaborative signals.

Hence, we design a new framework Temporal Graph Sequential Recommender
(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel
Temporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the
self-attention mechanism by adopting a novel collaborative attention. TCT layer
can simultaneously capture collaborative signals from both users and items, as
well as considering temporal dynamics inside sequential patterns. We propagate
the information learned fromTCTlayerover the temporal graph to unify sequential
patterns and temporal collaborative signals. Empirical results on five datasets
show that TGSRec significantly outperforms other baselines, in average up to
22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Ziwei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Many Dimensions of Truthfulness: Crowdsourcing Misinformation Assessments on a Multidimensional Scale. (arXiv:2108.01222v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01222</id>
        <link href="http://arxiv.org/abs/2108.01222"/>
        <updated>2021-08-24T01:40:26.279Z</updated>
        <summary type="html"><![CDATA[Recent work has demonstrated the viability of using crowdsourcing as a tool
for evaluating the truthfulness of public statements. Under certain conditions
such as: (1) having a balanced set of workers with different backgrounds and
cognitive abilities; (2) using an adequate set of mechanisms to control the
quality of the collected data; and (3) using a coarse grained assessment scale,
the crowd can provide reliable identification of fake news. However, fake news
are a subtle matter: statements can be just biased ("cherrypicked"), imprecise,
wrong, etc. and the unidimensional truth scale used in existing work cannot
account for such differences. In this paper we propose a multidimensional
notion of truthfulness and we ask the crowd workers to assess seven different
dimensions of truthfulness selected based on existing literature: Correctness,
Neutrality, Comprehensibility, Precision, Completeness, Speaker's
Trustworthiness, and Informativeness. We deploy a set of quality control
mechanisms to ensure that the thousands of assessments collected on 180
publicly available fact-checked statements distributed over two datasets are of
adequate quality, including a custom search engine used by the crowd workers to
find web pages supporting their truthfulness assessments. A comprehensive
analysis of crowdsourced judgments shows that: (1) the crowdsourced assessments
are reliable when compared to an expert-provided gold standard; (2) the
proposed dimensions of truthfulness capture independent pieces of information;
(3) the crowdsourcing task can be easily learned by the workers; and (4) the
resulting assessments provide a useful basis for a more complete estimation of
statement truthfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soprano_M/0/1/0/all/0/1"&gt;Michael Soprano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roitero_K/0/1/0/all/0/1"&gt;Kevin Roitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbera_D/0/1/0/all/0/1"&gt;David La Barbera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceolin_D/0/1/0/all/0/1"&gt;Davide Ceolin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1"&gt;Damiano Spina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mizzaro_S/0/1/0/all/0/1"&gt;Stefano Mizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1"&gt;Gianluca Demartini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yseop at FinSim-3 Shared Task 2021: Specializing Financial Domain Learning with Phrase Representations. (arXiv:2108.09485v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09485</id>
        <link href="http://arxiv.org/abs/2108.09485"/>
        <updated>2021-08-24T01:40:26.270Z</updated>
        <summary type="html"><![CDATA[In this paper, we present our approaches for the FinSim-3 Shared Task 2021:
Learning Semantic Similarities for the Financial Domain. The aim of this shared
task is to correctly classify a list of given terms from the financial domain
into the most relevant hypernym (or top-level) concept in an external ontology.
For our system submission, we evaluate two methods: a Sentence-RoBERTa
(SRoBERTa) embeddings model pre-trained on a custom corpus, and a dual
word-sentence embeddings model that builds on the first method by improving the
proposed baseline word embeddings construction using the FastText model to
boost the classification performance. Our system ranks 2nd overall on both
metrics, scoring 0.917 on Average Accuracy and 1.141 on Mean Rank.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akl_H/0/1/0/all/0/1"&gt;Hanna Abi Akl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mariko_D/0/1/0/all/0/1"&gt;Dominique Mariko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazancourt_H/0/1/0/all/0/1"&gt;Hugues de Mazancourt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Summarization for Longform Spoken Dialog. (arXiv:2108.09597v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09597</id>
        <link href="http://arxiv.org/abs/2108.09597"/>
        <updated>2021-08-24T01:40:26.264Z</updated>
        <summary type="html"><![CDATA[Every day we are surrounded by spoken dialog. This medium delivers rich
diverse streams of information auditorily; however, systematically
understanding dialog can often be non-trivial. Despite the pervasiveness of
spoken dialog, automated speech understanding and quality information
extraction remains markedly poor, especially when compared to written prose.
Furthermore, compared to understanding text, auditory communication poses many
additional challenges such as speaker disfluencies, informal prose styles, and
lack of structure. These concerns all demonstrate the need for a distinctly
speech tailored interactive system to help users understand and navigate the
spoken language domain. While individual automatic speech recognition (ASR) and
text summarization methods already exist, they are imperfect technologies;
neither consider user purpose and intent nor address spoken language induced
complications. Consequently, we design a two stage ASR and text summarization
pipeline and propose a set of semantic segmentation and merging algorithms to
resolve these speech modeling challenges. Our system enables users to easily
browse and navigate content as well as recover from errors in these underlying
technologies. Finally, we present an evaluation of the system which highlights
user preference for hierarchical summarization as a tool to quickly skim audio
and identify content of interest to the user.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Daniel Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Thomas Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_A/0/1/0/all/0/1"&gt;Albert Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1"&gt;Lydia Chilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorize, Factorize, or be Na\"ive: Learning Optimal Feature Interaction Methods for CTR Prediction. (arXiv:2108.01265v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01265</id>
        <link href="http://arxiv.org/abs/2108.01265"/>
        <updated>2021-08-24T01:40:26.246Z</updated>
        <summary type="html"><![CDATA[Click-through rate prediction is one of the core tasks in commercial
recommender systems. It aims to predict the probability of a user clicking a
particular item given user and item features. As feature interactions bring in
non-linearity, they are widely adopted to improve the performance of CTR
prediction models. Therefore, effectively modelling feature interactions has
attracted much attention in both the research and industry field. The current
approaches can generally be categorized into three classes: (1) na\"ive
methods, which do not model feature interactions and only use original
features; (2) memorized methods, which memorize feature interactions by
explicitly viewing them as new features and assigning trainable embeddings; (3)
factorized methods, which learn latent vectors for original features and
implicitly model feature interactions through factorization functions. Studies
have shown that modelling feature interactions by one of these methods alone
are suboptimal due to the unique characteristics of different feature
interactions. To address this issue, we first propose a general framework
called OptInter which finds the most suitable modelling method for each feature
interaction. Different state-of-the-art deep CTR models can be viewed as
instances of OptInter. To realize the functionality of OptInter, we also
introduce a learning algorithm that automatically searches for the optimal
modelling method. We conduct extensive experiments on four large datasets. Our
experiments show that OptInter improves the best performed state-of-the-art
baseline deep CTR models by up to 2.21%. Compared to the memorized method,
which also outperforms baselines, we reduce up to 91% parameters. In addition,
we conduct several ablation studies to investigate the influence of different
components of OptInter. Finally, we provide interpretable discussions on the
results of OptInter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1"&gt;Fuyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xue Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fluent: An AI Augmented Writing Tool for People who Stutter. (arXiv:2108.09918v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.09918</id>
        <link href="http://arxiv.org/abs/2108.09918"/>
        <updated>2021-08-24T01:40:26.239Z</updated>
        <summary type="html"><![CDATA[Stuttering is a speech disorder which impacts the personal and professional
lives of millions of people worldwide. To save themselves from stigma and
discrimination, people who stutter (PWS) may adopt different strategies to
conceal their stuttering. One of the common strategies is word substitution
where an individual avoids saying a word they might stutter on and use an
alternative instead. This process itself can cause stress and add more burden.
In this work, we present Fluent, an AI augmented writing tool which assists PWS
in writing scripts which they can speak more fluently. Fluent embodies a novel
active learning based method of identifying words an individual might struggle
pronouncing. Such words are highlighted in the interface. On hovering over any
such word, Fluent presents a set of alternative words which have similar
meaning but are easier to speak. The user is free to accept or ignore these
suggestions. Based on such user interaction (feedback), Fluent continuously
evolves its classifier to better suit the personalized needs of each user. We
evaluated our tool by measuring its ability to identify difficult words for 10
simulated users. We found that our tool can identify difficult words with a
mean accuracy of over 80% in under 20 interactions and it keeps improving with
more feedback. Our tool can be beneficial for certain important life situations
like giving a talk, presentation, etc. The source code for this tool has been
made publicly accessible at github.com/bhavyaghai/Fluent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghai_B/0/1/0/all/0/1"&gt;Bhavya Ghai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1"&gt;Klaus Mueller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UzBERT: pretraining a BERT model for Uzbek. (arXiv:2108.09814v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09814</id>
        <link href="http://arxiv.org/abs/2108.09814"/>
        <updated>2021-08-24T01:40:26.228Z</updated>
        <summary type="html"><![CDATA[Pretrained language models based on the Transformer architecture have
achieved state-of-the-art results in various natural language processing tasks
such as part-of-speech tagging, named entity recognition, and question
answering. However, no such monolingual model for the Uzbek language is
publicly available. In this paper, we introduce UzBERT, a pretrained Uzbek
language model based on the BERT architecture. Our model greatly outperforms
multilingual BERT on masked language model accuracy. We make the model publicly
available under the MIT open-source license.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansurov_B/0/1/0/all/0/1"&gt;B. Mansurov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansurov_A/0/1/0/all/0/1"&gt;A. Mansurov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering. (arXiv:2108.09689v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09689</id>
        <link href="http://arxiv.org/abs/2108.09689"/>
        <updated>2021-08-24T01:40:26.154Z</updated>
        <summary type="html"><![CDATA[Distantly supervised models are very popular for relation extraction since we
can obtain a large amount of training data using the distant supervision method
without human annotation. In distant supervision, a sentence is considered as a
source of a tuple if the sentence contains both entities of the tuple. However,
this condition is too permissive and does not guarantee the presence of
relevant relation-specific information in the sentence. As such, distantly
supervised training data contains much noise which adversely affects the
performance of the models. In this paper, we propose a self-ensemble filtering
mechanism to filter out the noisy samples during the training process. We
evaluate our proposed framework on the New York Times dataset which is obtained
via distant supervision. Our experiments with multiple state-of-the-art neural
relation extraction models show that our proposed filtering mechanism improves
the robustness of the models and increases their F1 scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1"&gt;Tapas Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09484</id>
        <link href="http://arxiv.org/abs/2108.09484"/>
        <updated>2021-08-24T01:40:26.134Z</updated>
        <summary type="html"><![CDATA[Human evaluation has always been expensive while researchers struggle to
trust the automatic metrics. To address this, we propose to customise
traditional metrics by taking advantages of the pre-trained language models
(PLMs) and the limited available human labelled scores. We first re-introduce
the hLEPOR metric factors, followed by the Python portable version we developed
which achieved the automatic tuning of the weighting parameters in hLEPOR
metric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE
distilled knowledge model to improve the metric agreement with human judgements
by automatically optimised factor weights regarding the exact MT language pairs
that cushLEPOR is deployed to. We also optimise cushLEPOR towards human
evaluation data based on MQM and pSQM framework on English-German and
Chinese-English language pairs. The experimental investigations show cushLEPOR
boosts hLEPOR performances towards better agreements to PLMs like LABSE with
much lower cost, and better agreements to human evaluations including MQM and
pSQM scores, and yields much better performances than BLEU (data available at
\url{https://github.com/poethan/cushLEPOR}).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Lifeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1"&gt;Irina Sorokina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1"&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1"&gt;Serge Gladkoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoundaryNet: An Attentive Deep Network with Fast Marching Distance Maps for Semi-automatic Layout Annotation. (arXiv:2108.09433v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09433</id>
        <link href="http://arxiv.org/abs/2108.09433"/>
        <updated>2021-08-24T01:40:26.078Z</updated>
        <summary type="html"><![CDATA[Precise boundary annotations of image regions can be crucial for downstream
applications which rely on region-class semantics. Some document collections
contain densely laid out, highly irregular and overlapping multi-class region
instances with large range in aspect ratio. Fully automatic boundary estimation
approaches tend to be data intensive, cannot handle variable-sized images and
produce sub-optimal results for aforementioned images. To address these issues,
we propose BoundaryNet, a novel resizing-free approach for high-precision
semi-automatic layout annotation. The variable-sized user selected region of
interest is first processed by an attention-guided skip network. The network
optimization is guided via Fast Marching distance maps to obtain a good quality
initial boundary estimate and an associated feature representation. These
outputs are processed by a Residual Graph Convolution Network optimized using
Hausdorff loss to obtain the final region boundary. Results on a challenging
image manuscript dataset demonstrate that BoundaryNet outperforms strong
baselines and produces high-quality semantic region boundaries. Qualitatively,
our approach generalizes across multiple document image datasets containing
different script systems and layouts, all without additional fine-tuning. We
integrate BoundaryNet into a document annotation system and show that it
provides high annotation throughput compared to manual and fully automatic
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Abhishek Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment. (arXiv:2108.09495v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09495</id>
        <link href="http://arxiv.org/abs/2108.09495"/>
        <updated>2021-08-24T01:40:26.062Z</updated>
        <summary type="html"><![CDATA[Document alignment techniques based on multilingual sentence representations
have recently shown state of the art results. However, these techniques rely on
unsupervised distance measurement techniques, which cannot be fined-tuned to
the task at hand. In this paper, instead of these unsupervised distance
measurement techniques, we employ Metric Learning to derive task-specific
distance measurements. These measurements are supervised, meaning that the
distance measurement metric is trained using a parallel dataset. Using a
dataset belonging to English, Sinhala, and Tamil, which belong to three
different language families, we show that these task-specific supervised
distance learning metrics outperform their unsupervised counterparts, for
document alignment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajitha_C/0/1/0/all/0/1"&gt;Charith Rajitha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piyarathne_L/0/1/0/all/0/1"&gt;Lakmali Piyarathne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachintha_D/0/1/0/all/0/1"&gt;Dilan Sachintha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1"&gt;Surangika Ranathunga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Self-Adaptive Hashing for Image Retrieval. (arXiv:2108.07094v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07094</id>
        <link href="http://arxiv.org/abs/2108.07094"/>
        <updated>2021-08-24T01:40:26.050Z</updated>
        <summary type="html"><![CDATA[Hashing technology has been widely used in image retrieval due to its
computational and storage efficiency. Recently, deep unsupervised hashing
methods have attracted increasing attention due to the high cost of human
annotations in the real world and the superiority of deep learning technology.
However, most deep unsupervised hashing methods usually pre-compute a
similarity matrix to model the pairwise relationship in the pre-trained feature
space. Then this similarity matrix would be used to guide hash learning, in
which most of the data pairs are treated equivalently. The above process is
confronted with the following defects: 1) The pre-computed similarity matrix is
inalterable and disconnected from the hash learning process, which cannot
explore the underlying semantic information. 2) The informative data pairs may
be buried by the large number of less-informative data pairs. To solve the
aforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model
to adaptively capture the semantic information with two special designs:
Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).
Firstly, we adopt the AND to initially construct a neighborhood-based
similarity matrix, and then refine this initial similarity matrix with a novel
update strategy to further investigate the semantic structure behind the
learned representation. Secondly, we measure the priorities of data pairs with
PIC and assign adaptive weights to them, which is relies on the assumption that
more dissimilar data pairs contain more discriminative information for hash
learning. Extensive experiments on several datasets demonstrate that the above
two technologies facilitate the deep hashing model to achieve superior
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qinghong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1"&gt;Shangxuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yudong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-Aware Query Auto-completion using Extreme Multi-label Ranking. (arXiv:2012.07654v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07654</id>
        <link href="http://arxiv.org/abs/2012.07654"/>
        <updated>2021-08-24T01:40:26.043Z</updated>
        <summary type="html"><![CDATA[Query auto-completion (QAC) is a fundamental feature in search engines where
the task is to suggest plausible completions of a prefix typed in the search
bar. Previous queries in the user session can provide useful context for the
user's intent and can be leveraged to suggest auto-completions that are more
relevant while adhering to the user's prefix. Such session-aware QACs can be
generated by recent sequence-to-sequence deep learning models; however, these
generative approaches often do not meet the stringent latency requirements of
responding to each user keystroke. Moreover, these generative approaches pose
the risk of showing nonsensical queries.

In this paper, we provide a solution to this problem: we take the novel
approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)
problem where the input is the previous query in the session and the user's
current prefix, while the output space is the set of tens of millions of
queries entered by users in the recent past. We adapt a popular XMR algorithm
for this purpose by proposing several modifications to the key steps in the
algorithm. The proposed modifications yield a 3.9x improvement in terms of Mean
Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs
dataset. We are able to maintain an inference latency of less than 10 ms while
still using session context. When compared against baseline models of
acceptable latency, we observed a 33% improvement in MRR for short prefixes of
up to 3 characters. Moreover, our model yielded a statistically significant
improvement of 2.81% over a production QAC system in terms of suggestion
acceptance rate, when deployed on the search bar of an online shopping store as
part of an A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nishant Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Daniel N. Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08649</id>
        <link href="http://arxiv.org/abs/2105.08649"/>
        <updated>2021-08-24T01:40:25.975Z</updated>
        <summary type="html"><![CDATA[User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models. Public codes are
available at https://github.com/zachstarkk/DCAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangtian Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1"&gt;Robert Pless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. (arXiv:2108.04690v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04690</id>
        <link href="http://arxiv.org/abs/2108.04690"/>
        <updated>2021-08-24T01:40:25.955Z</updated>
        <summary type="html"><![CDATA[Recommendation for new users, also called user cold start, has been a
well-recognized challenge for online recommender systems. Most existing methods
view the crux as the lack of initial data. However, in this paper, we argue
that there are neglected problems: 1) New users' behaviour follows much
different distributions from regular users. 2) Although personalized features
are involved, heavily imbalanced samples prevent the model from balancing
new/regular user distributions, as if the personalized features are
overwhelmed. We name the problem as the "submergence" of personalization. To
tackle this problem, we propose a novel module: Personalized COld Start MOdules
(POSO). Considering from a model architecture perspective, POSO personalizes
existing modules by introducing multiple user-group-specialized sub-modules.
Then, it fuses their outputs by personalized gates, resulting in comprehensive
representations. In such way, POSO projects imbalanced features to even
modules. POSO can be flexibly integrated into many existing modules and
effectively improves their performance with negligible computational overheads.
The proposed method shows remarkable advantage in industrial scenario. It has
been deployed on the large-scale recommender system of Kwai, and improves new
user Watch Time by a large margin (+7.75%). Moreover, POSO can be further
generalized to regular users, inactive users and returning users (+2%-3% on
Watch Time), as well as item cold start (+3.8% on Watch Time). Its
effectiveness has also been verified on public dataset (MovieLens 20M). We
believe such practical experience can be well generalized to other scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Shangfeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haobin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianying Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Honghuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v5 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09665</id>
        <link href="http://arxiv.org/abs/2106.09665"/>
        <updated>2021-08-24T01:40:25.946Z</updated>
        <summary type="html"><![CDATA[Modern E-commerce websites contain heterogeneous sources of information, such
as numerical ratings, textual reviews and images. These information can be
utilized to assist recommendation. Through textual reviews, a user explicitly
express her affinity towards the item. Previous researchers found that by using
the information extracted from these reviews, we can better profile the users'
explicit preferences as well as the item features, leading to the improvement
of recommendation performance. However, most of the previous algorithms were
only utilizing the review information for explicit-feedback problem i.e. rating
prediction, and when it comes to implicit-feedback ranking problem such as
top-N recommendation, the usage of review information has not been fully
explored. Seeing this gap, in this work, we investigate the effectiveness of
textual review information for top-N recommendation under E-commerce settings.
We adapt several SOTA review-based rating prediction models for top-N
recommendation tasks and compare them to existing top-N recommendation models
from both performance and efficiency. We find that models utilizing only review
information can not achieve better performances than vanilla implicit-feedback
matrix factorization method. When utilizing review information as a regularizer
or auxiliary information, the performance of implicit-feedback matrix
factorization method can be further improved. However, the optimal model
structure to utilize textual reviews for E-commerce top-N recommendation is yet
to be determined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Hansi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2020 U.S. Presidential Election: Analysis of Female and Male Users on Twitter. (arXiv:2108.09416v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.09416</id>
        <link href="http://arxiv.org/abs/2108.09416"/>
        <updated>2021-08-24T01:40:25.929Z</updated>
        <summary type="html"><![CDATA[Social media is commonly used by the public during election campaigns to
express their opinions regarding different issues. Among various social media
channels, Twitter provides an efficient platform for researchers and
politicians to explore public opinion regarding a wide range of topics such as
economy and foreign policy. Current literature mainly focuses on analyzing the
content of tweets without considering the gender of users. This research
collects and analyzes a large number of tweets and uses computational, human
coding, and statistical analyses to identify topics in more than 300,000 tweets
posted during the 2020 U.S. presidential election and to compare female and
male users regarding the average weight of the topics. Our findings are based
upon a wide range of topics, such as tax, climate change, and the COVID-19
pandemic. Out of the topics, there exists a significant difference between
female and male users for more than 70% of topics. Our research approach can
inform studies in the areas of informatics, politics, and communication, and it
can be used by political campaigns to obtain a gender-based understanding of
public opinion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1"&gt;Amir Karami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1"&gt;Spring B. Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackenzie_A/0/1/0/all/0/1"&gt;Anderson Mackenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dorathea Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Michael Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1"&gt;Hannah R. Boyajieff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1"&gt;Bailey Goldschmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation Using Many-To-Many RNNs for Session-Aware Recommender Systems. (arXiv:2108.09858v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09858</id>
        <link href="http://arxiv.org/abs/2108.09858"/>
        <updated>2021-08-24T01:40:25.910Z</updated>
        <summary type="html"><![CDATA[The ACM WSDM WebTour 2021 Challenge organized by Booking.com focuses on
applying Session-Aware recommender systems in the travel domain. Given a
sequence of travel bookings in a user trip, we look to recommend the user's
next destination. To handle the large dimensionality of the output's space, we
propose a many-to-many RNN model, predicting the next destination chosen by the
user at every sequence step as opposed to only the final one. We show how this
is a computationally efficient alternative to doing data augmentation in a
many-to-one RNN, where we consider every subsequence of a session starting from
the first element. Our solution achieved 4th place in the final leaderboard,
with an accuracy@4 of 0.5566.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_M/0/1/0/all/0/1"&gt;Mart&amp;#xed;n Baigorria Alonso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Server Private Linear Transformation with Joint Privacy. (arXiv:2108.09843v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.09843</id>
        <link href="http://arxiv.org/abs/2108.09843"/>
        <updated>2021-08-24T01:40:25.898Z</updated>
        <summary type="html"><![CDATA[This paper focuses on the Private Linear Transformation (PLT) problem in the
multi-server scenario. In this problem, there are $N$ servers, each of which
stores an identical copy of a database consisting of $K$ independent messages,
and there is a user who wishes to compute $L$ independent linear combinations
of a subset of $D$ messages in the database while leaking no information to the
servers about the identity of the entire set of these $D$ messages required for
the computation. We focus on the setting in which the coefficient matrix of the
desired $L$ linear combinations generates a Maximum Distance Separable (MDS)
code. We characterize the capacity of the PLT problem, defined as the supremum
of all achievable download rates, for all parameters $N, K, D \geq 1$ and
$L=1$, i.e., when the user wishes to compute one linear combination of $D$
messages. Moreover, we establish an upper bound on the capacity of PLT problem
for all parameters $N, K, D, L \geq 1$, and leveraging some known capacity
results, we show the tightness of this bound in the following regimes: (i) the
case when there is a single server (i.e., $N=1$), (ii) the case when $L=1$, and
(iii) the case when $L=D$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazemi_F/0/1/0/all/0/1"&gt;Fatemeh Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1"&gt;Alex Sprintson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Answer from Multi-spans for Reading Comprehension. (arXiv:2009.06141v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06141</id>
        <link href="http://arxiv.org/abs/2009.06141"/>
        <updated>2021-08-24T01:40:25.877Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel method to generate answers for non-extraction
machine reading comprehension (MRC) tasks whose answers cannot be simply
extracted as one span from the given passages. Using a pointer network-style
extractive decoder for such type of MRC may result in unsatisfactory
performance when the ground-truth answers are given by human annotators or
highly re-paraphrased from parts of the passages. On the other hand, using
generative decoder cannot well guarantee the resulted answers with well-formed
syntax and semantics when encountering long sentences. Therefore, to alleviate
the obvious drawbacks of both sides, we propose an answer making-up method from
extracted multi-spans that are learned by our model as highly confident
$n$-gram candidates in the given passage. That is, the returned answers are
composed of discontinuous multi-spans but not just one consecutive span in the
given passages anymore. The proposed method is simple but effective: empirical
experiments on MS MARCO show that the proposed method has a better performance
on accurately generating long answers, and substantially outperforms two
competitive typical one-span and Seq2Seq baseline decoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Term Interrelations and Trends in Software Engineering. (arXiv:2108.09529v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.09529</id>
        <link href="http://arxiv.org/abs/2108.09529"/>
        <updated>2021-08-24T01:40:25.867Z</updated>
        <summary type="html"><![CDATA[The Software Engineering (SE) community is prolific, making it challenging
for experts to keep up with the flood of new papers and for neophytes to enter
the field. Therefore, we posit that the community may benefit from a tool
extracting terms and their interrelations from the SE community's text corpus
and showing terms' trends. In this paper, we build a prototyping tool using the
word embedding technique. We train the embeddings on the SE Body of Knowledge
handbook and 15,233 research papers' titles and abstracts. We also create test
cases necessary for validation of the training of the embeddings. We provide
representative examples showing that the embeddings may aid in summarizing
terms and uncovering trends in the knowledge base.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baskararajah_J/0/1/0/all/0/1"&gt;Janusan Baskararajah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miranskyy_A/0/1/0/all/0/1"&gt;Andriy Miranskyy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-to-Sequence Learning on Keywords for Efficient FAQ Retrieval. (arXiv:2108.10019v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.10019</id>
        <link href="http://arxiv.org/abs/2108.10019"/>
        <updated>2021-08-24T01:40:25.859Z</updated>
        <summary type="html"><![CDATA[Frequently-Asked-Question (FAQ) retrieval provides an effective procedure for
responding to user's natural language based queries. Such platforms are
becoming common in enterprise chatbots, product question answering, and
preliminary technical support for customers. However, the challenge in such
scenarios lies in bridging the lexical and semantic gap between varied query
formulations and the corresponding answers, both of which typically have a very
short span. This paper proposes TI-S2S, a novel learning framework combining
TF-IDF based keyword extraction and Word2Vec embeddings for training a
Sequence-to-Sequence (Seq2Seq) architecture. It achieves high precision for FAQ
retrieval by better understanding the underlying intent of a user question
captured via the representative keywords. We further propose a variant with an
additional neural network module for guiding retrieval via relevant candidate
identification based on similarity features. Experiments on publicly available
dataset depict our approaches to provide around 92% precision-at-rank-5,
exhibiting nearly 13% improvement over existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sourav Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assem_H/0/1/0/all/0/1"&gt;Haytham Assem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgin_E/0/1/0/all/0/1"&gt;Edward Burgin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Attention for Grounded Image Captioning. (arXiv:2108.01056v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01056</id>
        <link href="http://arxiv.org/abs/2108.01056"/>
        <updated>2021-08-24T01:40:25.836Z</updated>
        <summary type="html"><![CDATA[We study the problem of weakly supervised grounded image captioning. That is,
given an image, the goal is to automatically generate a sentence describing the
context of the image with each noun word grounded to the corresponding region
in the image. This task is challenging due to the lack of explicit fine-grained
region word alignments as supervision. Previous weakly supervised methods
mainly explore various kinds of regularization schemes to improve attention
accuracy. However, their performances are still far from the fully supervised
ones. One main issue that has been ignored is that the attention for generating
visually groundable words may only focus on the most discriminate parts and can
not cover the whole object. To this end, we propose a simple yet effective
method to alleviate the issue, termed as partial grounding problem in our
paper. Specifically, we design a distributed attention mechanism to enforce the
network to aggregate information from multiple spatially different regions with
consistent semantics while generating the words. Therefore, the union of the
focused region proposals should form a visual region that encloses the object
of interest completely. Extensive experiments have demonstrated the superiority
of our proposed method compared with the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nenglun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runnan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhiwen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yuqiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Haolei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Literature Review of Automated Query Reformulations in Source Code Search. (arXiv:2108.09646v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.09646</id>
        <link href="http://arxiv.org/abs/2108.09646"/>
        <updated>2021-08-24T01:40:25.818Z</updated>
        <summary type="html"><![CDATA[Software developers often fix critical bugs to ensure the reliability of
their software. They might also need to add new features to their software at a
regular interval to stay competitive in the market. These bugs and features are
reported as change requests (i.e., technical documents written by software
users). Developers consult these documents to implement the required changes in
the software code. As a part of change implementation, they often choose a few
important keywords from a change request as an ad hoc query. Then they execute
the query with a code search engine (e.g., Lucene) and attempt to find out the
exact locations within the software code that need to be changed.
Unfortunately, even experienced developers often fail to choose the right
queries. As a consequence, the developers often experience difficulties in
detecting the appropriate locations within the code and spend the majority of
their time in numerous trials and errors. There have been many studies that
attempt to support developers in constructing queries by automatically
reformulating their ad hoc queries. In this systematic literature review, we
carefully select 70 primary studies on query reformulations from 2,970
candidate studies, perform an in-depth qualitative analysis using the Grounded
Theory approach, and then answer six important research questions. Our
investigation has reported several major findings. First, to date, eight major
methodologies (e.g., term weighting, query-term co-occurrence analysis,
thesaurus lookup) have been adopted in query reformulation. Second, the
existing studies suffer from several major limitations (e.g., lack of
generalizability, vocabulary mismatch problem, weak evaluation, the extra
burden on the developers) that might prevent their wide adoption. Finally, we
discuss several open issues in search query reformulations and suggest multiple
future research opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mohammad Masudur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_C/0/1/0/all/0/1"&gt;Chanchal K. Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training for Ad-hoc Retrieval: Hyperlink is Also You Need. (arXiv:2108.09346v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09346</id>
        <link href="http://arxiv.org/abs/2108.09346"/>
        <updated>2021-08-24T01:40:25.672Z</updated>
        <summary type="html"><![CDATA[Designing pre-training objectives that more closely resemble the downstream
tasks for pre-trained language models can lead to better performance at the
fine-tuning stage, especially in the ad-hoc retrieval area. Existing
pre-training approaches tailored for IR tried to incorporate weak supervised
signals, such as query-likelihood based sampling, to construct pseudo
query-document pairs from the raw textual corpus. However, these signals rely
heavily on the sampling method. For example, the query likelihood model may
lead to much noise in the constructed pre-training data. \blfootnote{$\dagger$
This work was done during an internship at Huawei.} In this paper, we propose
to leverage the large-scale hyperlinks and anchor texts to pre-train the
language model for ad-hoc retrieval. Since the anchor texts are created by
webmasters and can usually summarize the target document, it can help to build
more accurate and reliable pre-training samples than a specific algorithm.
Considering different views of the downstream ad-hoc retrieval, we devise four
pre-training tasks based on the hyperlinks. We then pre-train the Transformer
model to predict the pair-wise preference, jointly with the Masked Language
Model objective. Experimental results on two large-scale ad-hoc retrieval
datasets show the significant improvement of our model compared with the
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengyi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhao Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08649</id>
        <link href="http://arxiv.org/abs/2105.08649"/>
        <updated>2021-08-24T01:40:25.640Z</updated>
        <summary type="html"><![CDATA[User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models. Public codes are
available at https://github.com/zachstarkk/DCAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangtian Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1"&gt;Robert Pless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoundaryNet: An Attentive Deep Network with Fast Marching Distance Maps for Semi-automatic Layout Annotation. (arXiv:2108.09433v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09433</id>
        <link href="http://arxiv.org/abs/2108.09433"/>
        <updated>2021-08-24T01:40:25.610Z</updated>
        <summary type="html"><![CDATA[Precise boundary annotations of image regions can be crucial for downstream
applications which rely on region-class semantics. Some document collections
contain densely laid out, highly irregular and overlapping multi-class region
instances with large range in aspect ratio. Fully automatic boundary estimation
approaches tend to be data intensive, cannot handle variable-sized images and
produce sub-optimal results for aforementioned images. To address these issues,
we propose BoundaryNet, a novel resizing-free approach for high-precision
semi-automatic layout annotation. The variable-sized user selected region of
interest is first processed by an attention-guided skip network. The network
optimization is guided via Fast Marching distance maps to obtain a good quality
initial boundary estimate and an associated feature representation. These
outputs are processed by a Residual Graph Convolution Network optimized using
Hausdorff loss to obtain the final region boundary. Results on a challenging
image manuscript dataset demonstrate that BoundaryNet outperforms strong
baselines and produces high-quality semantic region boundaries. Qualitatively,
our approach generalizes across multiple document image datasets containing
different script systems and layouts, all without additional fine-tuning. We
integrate BoundaryNet into a document annotation system and show that it
provides high annotation throughput compared to manual and fully automatic
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Abhishek Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphIQA:Learning Distortion Graph Representations for Blind Image Quality Assessment. (arXiv:2103.07666v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07666</id>
        <link href="http://arxiv.org/abs/2103.07666"/>
        <updated>2021-08-24T01:40:25.597Z</updated>
        <summary type="html"><![CDATA[A good distortion representation is crucial for the success of deep blind
image quality assessment (BIQA). However, most previous methods do not
effectively model the relationship between distortions or the distribution of
samples with the same distortion type but different distortion levels. In this
work, we start from the analysis of the relationship between perceptual image
quality and distortion-related factors, such as distortion types and levels.
Then, we propose a Distortion Graph Representation (DGR) learning framework for
IQA, named GraphIQA, in which each distortion is represented as a graph, \ieno,
DGR. One can distinguish distortion types by learning the contrast relationship
between these different DGRs, and infer the ranking distribution of samples
from different levels in a DGR. Specifically, we develop two sub-networks to
learn the DGRs: a) Type Discrimination Network (TDN) that aims to embed DGR
into a compact code for better discriminating distortion types and learning the
relationship between types; b) Fuzzy Prediction Network (FPN) that aims to
extract the distributional characteristics of the samples in a DGR and predicts
fuzzy degrees based on a Gaussian prior. Experiments show that our GraphIQA
achieves the state-of-the-art performance on many benchmark datasets of both
synthetic and authentic distortions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Simeng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiahua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00139</id>
        <link href="http://arxiv.org/abs/2108.00139"/>
        <updated>2021-08-24T01:40:25.582Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (ReID) aims to match person images with
occlusion. It is fundamentally challenging because of the serious occlusion
which aggravates the misalignment problem between images. At the cost of
incorporating a pose estimator, many works introduce pose information to
alleviate the misalignment in both training and testing. To achieve high
accuracy while preserving low inference complexity, we propose a network named
Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the
pose information is exploited to regularize the learning of semantics aligned
features but is discarded in testing. PGFL-KD consists of a main branch (MB),
and two pose-guided branches, \ieno, a foreground-enhanced branch (FEB), and a
body part semantics aligned branch (SAB). The FEB intends to emphasise the
features of visible body parts while excluding the interference of obstructions
and background (\ieno, foreground feature alignment). The SAB encourages
different channel groups to focus on different body parts to have body part
semantics aligned representation. To get rid of the dependency on pose
information when testing, we regularize the MB to learn the merits of the FEB
and SAB through knowledge distillation and interaction-based training.
Extensive experiments on occluded, partial, and holistic ReID tasks show the
effectiveness of our proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training. (arXiv:2108.09479v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.09479</id>
        <link href="http://arxiv.org/abs/2108.09479"/>
        <updated>2021-08-24T01:40:25.548Z</updated>
        <summary type="html"><![CDATA[Existing approaches to vision-language pre-training (VLP) heavily rely on an
object detector based on bounding boxes (regions), where salient objects are
first detected from images and then a Transformer-based model is used for
cross-modal fusion. Despite their superior performance, these approaches are
bounded by the capability of the object detector in terms of both effectiveness
and efficiency. Besides, the presence of object detection imposes unnecessary
constraints on model designs and makes it difficult to support end-to-end
training. In this paper, we revisit grid-based convolutional features for
vision-language pre-training, skipping the expensive region-related steps. We
propose a simple yet effective grid-based VLP method that works surprisingly
well with the grid features. By pre-training only with in-domain datasets, the
proposed Grid-VLP method can outperform most competitive region-based VLP
methods on three examined vision-language understanding tasks. We hope that our
findings help to further advance the state of the art of vision-language
pre-training, and provide a new direction towards effective and efficient VLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Junfeng Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1"&gt;Min Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting and Segmenting Adversarial Graphics Patterns from Images. (arXiv:2108.09383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09383</id>
        <link href="http://arxiv.org/abs/2108.09383"/>
        <updated>2021-08-24T01:40:25.529Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks pose a substantial threat to computer vision system
security, but the social media industry constantly faces another form of
"adversarial attack" in which the hackers attempt to upload inappropriate
images and fool the automated screening systems by adding artificial graphics
patterns. In this paper, we formulate the defense against such attacks as an
artificial graphics pattern segmentation problem. We evaluate the efficacy of
several segmentation algorithms and, based on observation of their performance,
propose a new method tailored to this specific problem. Extensive experiments
show that the proposed method outperforms the baselines and has a promising
generalization capability, which is the most crucial aspect in segmenting
artificial graphics patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiangyu Qu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt; (1) ((1) Purdue University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Palmira: A Deep Deformable Network for Instance Segmentation of Dense and Uneven Layouts in Handwritten Manuscripts. (arXiv:2108.09436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09436</id>
        <link href="http://arxiv.org/abs/2108.09436"/>
        <updated>2021-08-24T01:40:25.511Z</updated>
        <summary type="html"><![CDATA[Handwritten documents are often characterized by dense and uneven layout.
Despite advances, standard deep network based approaches for semantic layout
segmentation are not robust to complex deformations seen across semantic
regions. This phenomenon is especially pronounced for the low-resource Indic
palm-leaf manuscript domain. To address the issue, we first introduce
Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with
semantic layout annotations. Indiscapes2 contains documents from four different
historical collections and is 150% larger than its predecessor, Indiscapes. We
also propose a novel deep network Palmira for robust, deformation-aware
instance segmentation of regions in handwritten manuscripts. We also report
Hausdorff distance and its variants as a boundary-aware performance measure.
Our experiments demonstrate that Palmira provides robust layouts, outperforms
strong baseline approaches and ablative variants. We also include qualitative
results on Arabic, South-East Asian and Hebrew historical manuscripts to
showcase the generalization capability of Palmira.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharan_P/0/1/0/all/0/1"&gt;Prema Satish Sharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitha_S/0/1/0/all/0/1"&gt;Sowmya Aitha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Amandeep Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Abhishek Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augustine_A/0/1/0/all/0/1"&gt;Aaron Augustine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[External Knowledge Augmented Text Visual Question Answering. (arXiv:2108.09717v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09717</id>
        <link href="http://arxiv.org/abs/2108.09717"/>
        <updated>2021-08-24T01:40:25.478Z</updated>
        <summary type="html"><![CDATA[The open-ended question answering task of Text-VQA requires reading and
reasoning about local, often previously unseen, scene-text content of an image
to generate answers. In this work, we propose the generalized use of external
knowledge to augment our understanding of the said scene-text. We design a
framework to extract, filter, and encode knowledge atop a standard multimodal
transformer for vision language understanding tasks. Through empirical
evidence, we demonstrate how knowledge can highlight instance-only cues and
thus help deal with training data bias, improve answer entity type correctness,
and detect multiword named entities. We generate results comparable to the
state-of-the-art on two publicly available datasets, under the constraints of
similar upstream OCR systems and training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1"&gt;Arka Ujjal Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1"&gt;Ernest Valveny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1"&gt;Gaurav Harit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01440</id>
        <link href="http://arxiv.org/abs/2108.01440"/>
        <updated>2021-08-23T01:36:37.527Z</updated>
        <summary type="html"><![CDATA[E-commerce sites strive to provide users the most timely relevant information
in order to reduce shopping frictions and increase customer satisfaction. Multi
armed bandit models (MAB) as a type of adaptive optimization algorithms provide
possible approaches for such purposes. In this paper, we analyze using three
classic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper
confidence bound 1 (UCB1) for dynamic content recommendations, and walk through
the process of developing these algorithms internally to solve a real world
e-commerce use case. First, we analyze the three MAB algorithms using simulated
purchasing datasets with non-stationary reward distributions to simulate the
possible time-varying customer preferences, where the traffic allocation
dynamics and the accumulative rewards of different algorithms are studied.
Second, we compare the accumulative rewards of the three MAB algorithms with
more than 1,000 trials using actual historical A/B test datasets. We find that
the larger difference between the success rates of competing recommendations
the more accumulative rewards the MAB algorithms can achieve. In addition, we
find that TS shows the highest average accumulative rewards under different
testing scenarios. Third, we develop a batch-updated MAB algorithm to overcome
the delayed reward issue in e-commerce and enable an online content
optimization on our App homepage. For a state-of-the-art comparison, a real A/B
test among our batch-updated MAB algorithm, a third-party MAB solution, and the
default business logic are conducted. The result shows that our batch-updated
MAB algorithm outperforms the counterparts and achieves 6.13% relative
click-through rate (CTR) increase and 16.1% relative conversion rate (CVR)
increase compared to the default experience, and 2.9% relative CTR increase and
1.4% relative CVR increase compared to the external MAB service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1"&gt;Ding Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1"&gt;Becky West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiquan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jinzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting. (arXiv:2103.14023v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14023</id>
        <link href="http://arxiv.org/abs/2103.14023"/>
        <updated>2021-08-23T01:36:37.514Z</updated>
        <summary type="html"><![CDATA[Predicting accurate future trajectories of multiple agents is essential for
autonomous systems, but is challenging due to the complex agent interaction and
the uncertainty in each agent's future behavior. Forecasting multi-agent
trajectories requires modeling two key dimensions: (1) time dimension, where we
model the influence of past agent states over future states; (2) social
dimension, where we model how the state of each agent affects others. Most
prior methods model these two dimensions separately, e.g., first using a
temporal model to summarize features over time for each agent independently and
then modeling the interaction of the summarized features with a social model.
This approach is suboptimal since independent feature encoding over either the
time or social dimension can result in a loss of information. Instead, we would
prefer a method that allows an agent's state at one time to directly affect
another agent's state at a future time. To this end, we propose a new
Transformer, AgentFormer, that jointly models the time and social dimensions.
The model leverages a sequence representation of multi-agent trajectories by
flattening trajectory features across time and agents. Since standard attention
operations disregard the agent identity of each element in the sequence,
AgentFormer uses a novel agent-aware attention mechanism that preserves agent
identities by attending to elements of the same agent differently than elements
of other agents. Based on AgentFormer, we propose a stochastic multi-agent
trajectory prediction model that can attend to features of any agent at any
previous timestep when inferring an agent's future position. The latent intent
of all agents is also jointly modeled, allowing the stochasticity in one
agent's behavior to affect other agents. Our method significantly improves the
state of the art on well-established pedestrian and autonomous driving
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Ye Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1"&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1"&gt;Yanglan Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris Kitani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods. (arXiv:2103.03078v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03078</id>
        <link href="http://arxiv.org/abs/2103.03078"/>
        <updated>2021-08-23T01:36:37.495Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models used in medical imaging diagnostics can be
vulnerable to a variety of privacy attacks, including membership inference
attacks, that lead to violations of regulations governing the use of medical
data and threaten to compromise their effective deployment in the clinic. In
contrast to most recent work in privacy-aware ML that has been focused on model
alteration and post-processing steps, we propose here a novel and complementary
scheme that enhances the security of medical data by controlling the data
sharing process. We develop and evaluate a privacy defense protocol based on
using a generative adversarial network (GAN) that allows a medical data sourcer
(e.g. a hospital) to provide an external agent (a modeler) a proxy dataset
synthesized from the original images, so that the resulting diagnostic systems
made available to model consumers is rendered resilient to privacy attackers.
We validate the proposed method on retinal diagnostics AI used for diabetic
retinopathy that bears the risk of possibly leaking private information. To
incorporate concerns of both privacy advocates and modelers, we introduce a
metric to evaluate privacy and utility performance in combination, and
demonstrate, using these novel and classical metrics, that our approach, by
itself or in conjunction with other defenses, provides state of the art (SOTA)
performance for defending against privacy attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yinzhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miaomiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08801</id>
        <link href="http://arxiv.org/abs/2106.08801"/>
        <updated>2021-08-23T01:36:37.489Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment aims at finding equivalent entities and
relations (i.e., mappings) between two KGs. The existing approaches utilize
either reasoning-based or semantic embedding-based techniques, but few studies
explore their combination. In this demonstration, we present PRASEMap, an
unsupervised KG alignment system that iteratively computes the Mappings with
both Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.
PRASEMap can support various embedding-based KG alignment approaches as the SE
module, and enables easy human computer interaction that additionally provides
an option for users to feed the mapping annotations back to the system for
better results. The demonstration showcases these features via a stand-alone
Web application with user friendly interfaces. The demo is available at
https://prasemap.qizhy.com.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[List-Decodable Coded Computing: Breaking the Adversarial Toleration Barrier. (arXiv:2101.11653v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11653</id>
        <link href="http://arxiv.org/abs/2101.11653"/>
        <updated>2021-08-23T01:36:37.473Z</updated>
        <summary type="html"><![CDATA[We consider the problem of coded computing, where a computational task is
performed in a distributed fashion in the presence of adversarial workers. We
propose techniques to break the adversarial toleration threshold barrier
previously known in coded computing. More specifically, we leverage
list-decoding techniques for folded Reed-Solomon codes and propose novel
algorithms to recover the correct codeword using side information. In the coded
computing setting, we show how the master node can perform certain carefully
designed extra computations to obtain the side information. The workload of
computing this side information is negligible compared to the computations done
by each worker. This side information is then utilized to prune the output of
the list decoder and uniquely recover the true outcome. We further propose
folded Lagrange coded computing (FLCC) to incorporate the developed techniques
into a specific coded computing setting. Our results show that FLCC outperforms
LCC by breaking the barrier on the number of adversaries that can be tolerated.
In particular, the corresponding threshold in FLCC is improved by a factor of
two asymptotically compared to that of LCC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1"&gt;Mahdi Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1"&gt;Ramy E. Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavifar_H/0/1/0/all/0/1"&gt;Hessam Mahdavifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_A/0/1/0/all/0/1"&gt;A. Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Deep Video Denoising. (arXiv:2011.15045v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15045</id>
        <link href="http://arxiv.org/abs/2011.15045"/>
        <updated>2021-08-23T01:36:37.466Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) for video denoising are typically
trained with supervision, assuming the availability of clean videos. However,
in many applications, such as microscopy, noiseless videos are not available.
To address this, we propose an Unsupervised Deep Video Denoiser (UDVD), a CNN
architecture designed to be trained exclusively with noisy data. The
performance of UDVD is comparable to the supervised state-of-the-art, even when
trained only on a single short noisy video. We demonstrate the promise of our
approach in real-world imaging applications by denoising raw video,
fluorescence-microscopy and electron-microscopy data. In contrast to many
current approaches to video denoising, UDVD does not require explicit motion
compensation. This is advantageous because motion compensation is
computationally expensive, and can be unreliable when the input data are noisy.
A gradient-based analysis reveals that UDVD automatically adapts to local
motion in the input noisy videos. Thus, the network learns to perform implicit
motion compensation, even though it is only trained for denoising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sheth_D/0/1/0/all/0/1"&gt;Dev Yashpal Sheth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohan_S/0/1/0/all/0/1"&gt;Sreyas Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vincent_J/0/1/0/all/0/1"&gt;Joshua L. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manzorro_R/0/1/0/all/0/1"&gt;Ramon Manzorro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crozier_P/0/1/0/all/0/1"&gt;Peter A. Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khapra_M/0/1/0/all/0/1"&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simoncelli_E/0/1/0/all/0/1"&gt;Eero P. Simoncelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Am I fit for this physical activity? Neural embedding of physical conditioning from inertial sensors. (arXiv:2103.12095v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12095</id>
        <link href="http://arxiv.org/abs/2103.12095"/>
        <updated>2021-08-23T01:36:37.459Z</updated>
        <summary type="html"><![CDATA[Inertial Measurement Unit (IMU) sensors are present in everyday devices such
as smartphones and fitness watches. As a result, the array of health-related
research and applications that tap onto this data has been growing, but little
attention has been devoted to the prediction of an individual's heart rate (HR)
from IMU data, when undergoing a physical activity. Would that be even
possible? If so, this could be used to design personalized sets of aerobic
exercises, for instance. In this work, we show that it is viable to obtain
accurate HR predictions from IMU data using Recurrent Neural Networks, provided
only access to HR and IMU data from a short-lived, previously executed
activity. We propose a novel method for initializing an RNN's hidden state
vectors, using a specialized network that attempts to extract an embedding of
the physical conditioning (PCE) of a subject. We show that using a
discriminator in the training phase to help the model learn whether two PCEs
belong to the same individual further reduces the prediction error. We evaluate
the proposed model when predicting the HR of 23 subjects performing a variety
of physical activities from IMU data available in public datasets (PAMAP2,
PPG-DaLiA). For comparison, we use as baselines the only model specifically
proposed for this task and an adapted state-of-the-art model for Human Activity
Recognition (HAR), a closely related task. Our method, PCE-LSTM, yields over
10% lower mean absolute error. We demonstrate empirically that this error
reduction is in part due to the use of the PCE. Last, we use the two datasets
(PPG-DaLiA, WESAD) to show that PCE-LSTM can also be successfully applied when
photoplethysmography (PPG) sensors are available, outperforming the
state-of-the-art deep learning baselines by more than 30%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aguiar_D/0/1/0/all/0/1"&gt;Davi Pedrosa de Aguiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1"&gt;Fabricio Murai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jointly-Learned State-Action Embedding for Efficient Reinforcement Learning. (arXiv:2010.04444v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04444</id>
        <link href="http://arxiv.org/abs/2010.04444"/>
        <updated>2021-08-23T01:36:37.383Z</updated>
        <summary type="html"><![CDATA[While reinforcement learning has achieved considerable successes in recent
years, state-of-the-art models are often still limited by the size of state and
action spaces. Model-free reinforcement learning approaches use some form of
state representations and the latest work has explored embedding techniques for
actions, both with the aim of achieving better generalization and
applicability. However, these approaches consider only states or actions,
ignoring the interaction between them when generating embedded representations.
In this work, we establish the theoretical foundations for the validity of
training a reinforcement learning agent using embedded states and actions. We
then propose a new approach for jointly learning embeddings for states and
actions that combines aspects of model-free and model-based reinforcement
learning, which can be applied in both discrete and continuous domains.
Specifically, we use a model of the environment to obtain embeddings for states
and actions and present a generic architecture that leverages these to learn a
policy. In this way, the embedded representations obtained via our approach
enable better generalization over both states and actions by capturing
similarities in the embedding spaces. Evaluations of our approach on several
gaming, robotic control, and recommender systems show it significantly
outperforms state-of-the-art models in both discrete/continuous domains with
large state/action spaces, thus confirming its efficacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pritz_P/0/1/0/all/0/1"&gt;Paul J. Pritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Liang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1"&gt;Kin K. Leung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors. (arXiv:1806.05438v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.05438</id>
        <link href="http://arxiv.org/abs/1806.05438"/>
        <updated>2021-08-23T01:36:37.376Z</updated>
        <summary type="html"><![CDATA[We consider stochastic gradient descent and its averaging variant for binary
classification problems in a reproducing kernel Hilbert space. In the
traditional analysis using a consistency property of loss functions, it is
known that the expected classification error converges more slowly than the
expected risk even when assuming a low-noise condition on the conditional label
probabilities. Consequently, the resulting rate is sublinear. Therefore, it is
important to consider whether much faster convergence of the expected
classification error can be achieved. In recent research, an exponential
convergence rate for stochastic gradient descent was shown under a strong
low-noise condition but provided theoretical analysis was limited to the
squared loss function, which is somewhat inadequate for binary classification
tasks. In this paper, we show an exponential convergence of the expected
classification error in the final phase of the stochastic gradient descent for
a wide class of differentiable convex loss functions under similar assumptions.
As for the averaged stochastic gradient descent, we show that the same
convergence rate holds from the early phase of training. In experiments, we
verify our analyses on the $L_2$-regularized logistic regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-08-23T01:36:37.343Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Variational Approximation for the Log-Partition Function. (arXiv:2102.10196v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10196</id>
        <link href="http://arxiv.org/abs/2102.10196"/>
        <updated>2021-08-23T01:36:37.311Z</updated>
        <summary type="html"><![CDATA[Variational approximation, such as mean-field (MF) and tree-reweighted (TRW),
provide a computationally efficient approximation of the log-partition function
for a generic graphical model. TRW provably provides an upper bound, but the
approximation ratio is generally not quantified.

As the primary contribution of this work, we provide an approach to quantify
the approximation ratio through the property of the underlying graph structure.
Specifically, we argue that (a variant of) TRW produces an estimate that is
within factor $\frac{1}{\sqrt{\kappa(G)}}$ of the true log-partition function
for any discrete pairwise graphical model over graph $G$, where $\kappa(G) \in
(0,1]$ captures how far $G$ is from tree structure with $\kappa(G) = 1$ for
trees and $2/N$ for the complete graph over $N$ vertices. As a consequence, the
approximation ratio is $1$ for trees, $\sqrt{(d+1)/2}$ for any graph with
maximum average degree $d$, and $\stackrel{\beta\to\infty}{\approx}
1+1/(2\beta)$ for graphs with girth (shortest cycle) at least $\beta \log N$.
In general, $\kappa(G)$ is the solution of a max-min problem associated with
$G$ that can be evaluated in polynomial time for any graph.

Using samples from the uniform distribution over the spanning trees of G, we
provide a near linear-time variant that achieves an approximation ratio equal
to the inverse of square-root of minimal (across edges) effective resistance of
the graph. We connect our results to the graph partition-based approximation
method and thus provide a unified perspective.

Keywords: variational inference, log-partition function, spanning tree
polytope, minimum effective resistance, min-max spanning tree, local inference]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cosson_R/0/1/0/all/0/1"&gt;Romain Cosson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1"&gt;Devavrat Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03315</id>
        <link href="http://arxiv.org/abs/2107.03315"/>
        <updated>2021-08-23T01:36:37.303Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that the performance of machine learning models can
vary substantially when models are evaluated on data drawn from a distribution
that is close to but different from the training distribution. As a result,
predicting model performance on unseen distributions is an important challenge.
Our work connects techniques from domain adaptation and predictive uncertainty
literature, and allows us to predict model accuracy on challenging unseen
distributions without access to labeled data. In the context of distribution
shift, distributional distances are often used to adapt models and improve
their performance on new domains, however accuracy estimation, or other forms
of predictive uncertainty, are often neglected in these investigations. Through
investigating a wide range of established distributional distances, such as
Frechet distance or Maximum Mean Discrepancy, we determine that they fail to
induce reliable estimates of performance under distribution shift. On the other
hand, we find that the difference of confidences (DoC) of a classifier's
predictions successfully estimates the classifier's performance change over a
variety of shifts. We specifically investigate the distinction between
synthetic and natural distribution shifts and observe that despite its
simplicity DoC consistently outperforms other quantifications of distributional
difference. $DoC$ reduces predictive error by almost half ($46\%$) on several
realistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust
and ImageNet-Rendition datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1"&gt;Devin Guillory&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1"&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Verification and Reranking for Open Fact Checking Over Tables. (arXiv:2012.15115v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15115</id>
        <link href="http://arxiv.org/abs/2012.15115"/>
        <updated>2021-08-23T01:36:37.296Z</updated>
        <summary type="html"><![CDATA[Structured information is an important knowledge source for automatic
verification of factual claims. Nevertheless, the majority of existing research
into this task has focused on textual data, and the few recent inquiries into
structured data have been for the closed-domain setting where appropriate
evidence for each claim is assumed to have already been retrieved. In this
paper, we investigate verification over structured data in the open-domain
setting, introducing a joint reranking-and-verification model which fuses
evidence documents in the verification component. Our open-domain model
achieves performance comparable to the closed-domain state-of-the-art on the
TabFact dataset, and demonstrates performance gains from the inclusion of
multiple tables as well as a significant improvement over a heuristic retrieval
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1"&gt;Michael Schlichtkrull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1"&gt;Vladimir Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1"&gt;Barlas O&amp;#x11f;uz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social NCE: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2012.11717v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11717</id>
        <link href="http://arxiv.org/abs/2012.11717"/>
        <updated>2021-08-23T01:36:37.274Z</updated>
        <summary type="html"><![CDATA[Learning socially-aware motion representations is at the core of recent
advances in multi-agent problems, such as human motion forecasting and robot
navigation in crowds. Despite promising progress, existing representations
learned with neural networks still struggle to generalize in closed-loop
predictions (e.g., output colliding trajectories). This issue largely arises
from the non-i.i.d. nature of sequential prediction in conjunction with
ill-distributed training data. Intuitively, if the training data only comes
from human behaviors in safe spaces, i.e., from "positive" examples, it is
difficult for learning algorithms to capture the notion of "negative" examples
like collisions. In this work, we aim to address this issue by explicitly
modeling negative examples through self-supervision: (i) we introduce a social
contrastive loss that regularizes the extracted motion representation by
discerning the ground-truth positive events from synthetic negative ones; (ii)
we construct informative negative samples based on our prior knowledge of rare
but dangerous circumstances. Our method substantially reduces the collision
rates of recent trajectory forecasting, behavioral cloning and reinforcement
learning algorithms, outperforming state-of-the-art methods on several
benchmarks. Our code is available at https://github.com/vita-epfl/social-nce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuejiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1"&gt;Qi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1"&gt;Alexandre Alahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01077</id>
        <link href="http://arxiv.org/abs/2108.01077"/>
        <updated>2021-08-23T01:36:37.267Z</updated>
        <summary type="html"><![CDATA[A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any
user-information. We optimize these faces, by using an evolutionary algorithm
in the latent embedding space of the StyleGAN face generator. Multiple
evolutionary strategies are compared, and we propose a novel approach that
employs a neural network in order to direct the search in the direction of
promising samples, without adding fitness evaluations. The results we present
demonstrate that it is possible to obtain a high coverage of the LFW identities
(over 40%) with less than 10 master faces, for three leading deep face
recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STRATA: Simple, Gradient-Free Attacks for Models of Code. (arXiv:2009.13562v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13562</id>
        <link href="http://arxiv.org/abs/2009.13562"/>
        <updated>2021-08-23T01:36:37.259Z</updated>
        <summary type="html"><![CDATA[Neural networks are well-known to be vulnerable to imperceptible
perturbations in the input, called adversarial examples, that result in
misclassification. Generating adversarial examples for source code poses an
additional challenge compared to the domains of images and natural language,
because source code perturbations must retain the functional meaning of the
code. We identify a striking relationship between token frequency statistics
and learned token embeddings: the L2 norm of learned token embeddings increases
with the frequency of the token except for the highest-frequnecy tokens. We
leverage this relationship to construct a simple and efficient gradient-free
method for generating state-of-the-art adversarial examples on models of code.
Our method empirically outperforms competing gradient-based methods with less
information and less computational effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1"&gt;Jacob M. Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reinstadler_B/0/1/0/all/0/1"&gt;Bryn Marie Reinstadler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1"&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphReach: Position-Aware Graph Neural Network using Reachability Estimations. (arXiv:2008.09657v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09657</id>
        <link href="http://arxiv.org/abs/2008.09657"/>
        <updated>2021-08-23T01:36:37.252Z</updated>
        <summary type="html"><![CDATA[Majority of the existing graph neural networks (GNN) learn node embeddings
that encode their local neighborhoods but not their positions. Consequently,
two nodes that are vastly distant but located in similar local neighborhoods
map to similar embeddings in those networks. This limitation prevents accurate
performance in predictive tasks that rely on position information. In this
paper, we develop GraphReach, a position-aware inductive GNN that captures the
global positions of nodes through reachability estimations with respect to a
set of anchor nodes. The anchors are strategically selected so that
reachability estimations across all the nodes are maximized. We show that this
combinatorial anchor selection problem is NP-hard and, consequently, develop a
greedy (1-1/e) approximation heuristic. Empirical evaluation against
state-of-the-art GNN architectures reveal that GraphReach provides up to 40%
relative improvement in accuracy. In addition, it is more robust to adversarial
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishad_S/0/1/0/all/0/1"&gt;Sunil Nishad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Shubhangi Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1"&gt;Sayan Ranu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Graph Kernels for Classifying Dissemination Processes. (arXiv:1911.05496v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.05496</id>
        <link href="http://arxiv.org/abs/1911.05496"/>
        <updated>2021-08-23T01:36:37.245Z</updated>
        <summary type="html"><![CDATA[Many real-world graphs or networks are temporal, e.g., in a social network
persons only interact at specific points in time. This information directs
dissemination processes on the network, such as the spread of rumors, fake
news, or diseases. However, the current state-of-the-art methods for supervised
graph classification are designed mainly for static graphs and may not be able
to capture temporal information. Hence, they are not powerful enough to
distinguish between graphs modeling different dissemination processes. To
address this, we introduce a framework to lift standard graph kernels to the
temporal domain. Specifically, we explore three different approaches and
investigate the trade-offs between loss of temporal information and efficiency.
Moreover, to handle large-scale graphs, we propose stochastic variants of our
kernels with provable approximation guarantees. We evaluate our methods on a
wide range of real-world social networks. Our methods beat static kernels by a
large margin in terms of accuracy while still being scalable to large graphs
and data sets. Hence, we confirm that taking temporal information into account
is crucial for the successful classification of dissemination processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oettershagen_L/0/1/0/all/0/1"&gt;Lutz Oettershagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kriege_N/0/1/0/all/0/1"&gt;Nils M. Kriege&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1"&gt;Christopher Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mutzel_P/0/1/0/all/0/1"&gt;Petra Mutzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Calibration for Domain Generalization under Covariate Shift. (arXiv:2104.00742v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00742</id>
        <link href="http://arxiv.org/abs/2104.00742"/>
        <updated>2021-08-23T01:36:37.227Z</updated>
        <summary type="html"><![CDATA[Existing calibration algorithms address the problem of covariate shift via
unsupervised domain adaptation. However, these methods suffer from the
following limitations: 1) they require unlabeled data from the target domain,
which may not be available at the stage of calibration in real-world
applications and 2) their performance depends heavily on the disparity between
the distributions of the source and target domains. To address these two
limitations, we present novel calibration solutions via domain generalization.
Our core idea is to leverage multiple calibration domains to reduce the
effective distribution disparity between the target and calibration domains for
improved calibration transfer without needing any data from the target domain.
We provide theoretical justification and empirical experimental results to
demonstrate the effectiveness of our proposed algorithms. Compared against
state-of-the-art calibration methods designed for domain adaptation, we observe
a decrease of 8.86 percentage points in expected calibration error or,
equivalently, an increase of 35 percentage points in improvement ratio for
multi-class classification on the Office-Home dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yunye Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xiao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1"&gt;Thomas G. Dietterich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1"&gt;Ajay Divakaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gervasio_M/0/1/0/all/0/1"&gt;Melinda Gervasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources. (arXiv:2008.10327v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10327</id>
        <link href="http://arxiv.org/abs/2008.10327"/>
        <updated>2021-08-23T01:36:37.220Z</updated>
        <summary type="html"><![CDATA[Machine Reading Comprehension (MRC) aims to extract answers to questions
given a passage. It has been widely studied recently, especially in open
domains. However, few efforts have been made on closed-domain MRC, mainly due
to the lack of large-scale training data. In this paper, we introduce a
multi-target MRC task for the medical domain, whose goal is to predict answers
to medical questions and the corresponding support sentences from medical
information sources simultaneously, in order to ensure the high reliability of
medical knowledge serving. A high-quality dataset is manually constructed for
the purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with
detailed analysis conducted. We further propose the Chinese medical BERT model
for the task (CMedBERT), which fuses medical knowledge into pre-trained
language models by the dynamic fusion mechanism of heterogeneous features and
the multi-task learning strategy. Experiments show that CMedBERT consistently
outperforms strong baselines by fusing context-aware and knowledge-aware token
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Taolin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bite Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Truly Unsupervised Image-to-Image Translation. (arXiv:2006.06500v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06500</id>
        <link href="http://arxiv.org/abs/2006.06500"/>
        <updated>2021-08-23T01:36:37.214Z</updated>
        <summary type="html"><![CDATA[Every recent image-to-image translation model inherently requires either
image-level (i.e. input-output pairs) or set-level (i.e. domain labels)
supervision. However, even set-level supervision can be a severe bottleneck for
data collection in practice. In this paper, we tackle image-to-image
translation in a fully unsupervised setting, i.e., neither paired images nor
domain labels. To this end, we propose a truly unsupervised image-to-image
translation model (TUNIT) that simultaneously learns to separate image domains
and translates input images into the estimated domains. Experimental results
show that our model achieves comparable or even better performance than the
set-level supervised model trained with full labels, generalizes well on
various datasets, and is robust against the choice of hyperparameters (e.g. the
preset number of pseudo domains). Furthermore, TUNIT can be easily extended to
semi-supervised learning with a few labeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_K/0/1/0/all/0/1"&gt;Kyungjune Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yunjey Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1"&gt;Youngjung Uh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Jaejun Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1"&gt;Hyunjung Shim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13322</id>
        <link href="http://arxiv.org/abs/2011.13322"/>
        <updated>2021-08-23T01:36:37.207Z</updated>
        <summary type="html"><![CDATA[Skeleton-based human action recognition has attracted much attention with the
prevalence of accessible depth sensors. Recently, graph convolutional networks
(GCNs) have been widely used for this task due to their powerful capability to
model graph data. The topology of the adjacency graph is a key factor for
modeling the correlations of the input skeletons. Thus, previous methods mainly
focus on the design/learning of the graph topology. But once the topology is
learned, only a single-scale feature and one transformation exist in each layer
of the networks. Many insights, such as multi-scale information and multiple
sets of transformations, that have been proven to be very effective in
convolutional neural networks (CNNs), have not been investigated in GCNs. The
reason is that, due to the gap between graph-structured skeleton data and
conventional image/video data, it is very challenging to embed these insights
into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy
in GCNs for skeleton sequence processing. Specifically, we design a simple and
highly modularized graph convolutional network architecture for skeleton-based
action recognition. Our network is constructed by repeating a building block
that aggregates multi-granularity information from both the spatial and
temporal paths. Extensive experiments demonstrate that our network outperforms
state-of-the-art methods by a significant margin with only 1/5 of the
parameters and 1/10 of the FLOPs. Code is available at
https://github.com/yellowtownhz/STIGCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xinmei Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph to Graphs Framework for Retrosynthesis Prediction. (arXiv:2003.12725v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12725</id>
        <link href="http://arxiv.org/abs/2003.12725"/>
        <updated>2021-08-23T01:36:37.200Z</updated>
        <summary type="html"><![CDATA[A fundamental problem in computational chemistry is to find a set of
reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction.
Existing state-of-the-art methods rely on matching the target molecule with a
large set of reaction templates, which are very computationally expensive and
also suffer from the problem of coverage. In this paper, we propose a novel
template-free approach called G2Gs by transforming a target molecular graph
into a set of reactant molecular graphs. G2Gs first splits the target molecular
graph into a set of synthons by identifying the reaction centers, and then
translates the synthons to the final reactant graphs via a variational graph
translation framework. Experimental results show that G2Gs significantly
outperforms existing template-free approaches by up to 63% in terms of the
top-1 accuracy and achieves a performance close to that of state-of-the-art
template based approaches, but does not require domain knowledge and is much
more scalable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonconvex and Nonsmooth Sparse Optimization via Adaptively Iterative Reweighted Methods. (arXiv:1810.10167v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.10167</id>
        <link href="http://arxiv.org/abs/1810.10167"/>
        <updated>2021-08-23T01:36:37.183Z</updated>
        <summary type="html"><![CDATA[We propose a general formulation of nonconvex and nonsmooth sparse
optimization problems with convex set constraint, which can take into account
most existing types of nonconvex sparsity-inducing terms, bringing strong
applicability to a wide range of applications. We design a general algorithmic
framework of iteratively reweighted algorithms for solving the proposed
nonconvex and nonsmooth sparse optimization problems, which solves a sequence
of weighted convex regularization problems with adaptively updated weights.
First-order optimality condition is derived and global convergence results are
provided under loose assumptions, making our theoretical results a practical
tool for analyzing a family of various reweighted algorithms. The effectiveness
and efficiency of our proposed formulation and the algorithms are demonstrated
in numerical experiments on various sparse optimization problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuanming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yaohua Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning 1-Dimensional Submanifolds for Subsequent Inference on Random Dot Product Graphs. (arXiv:2004.07348v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07348</id>
        <link href="http://arxiv.org/abs/2004.07348"/>
        <updated>2021-08-23T01:36:37.175Z</updated>
        <summary type="html"><![CDATA[A random dot product graph (RDPG) is a generative model for networks in which
vertices correspond to positions in a latent Euclidean space and edge
probabilities are determined by the dot products of the latent positions. We
consider RDPGs for which the latent positions are randomly sampled from an
unknown $1$-dimensional submanifold of the latent space. In principle,
restricted inference, i.e., procedures that exploit the structure of the
submanifold, should be more effective than unrestricted inference; however, it
is not clear how to conduct restricted inference when the submanifold is
unknown. We submit that techniques for manifold learning can be used to learn
the unknown submanifold well enough to realize benefit from restricted
inference. To illustrate, we test $1$- and $2$-sample hypotheses about the
Fr\'{e}chet means of small communities of vertices, using the complete set of
vertices to infer latent structure. We propose test statistics that deploy the
Isomap procedure for manifold learning, using shortest path distances on
neighborhood graphs constructed from estimated latent positions to estimate arc
lengths on the unknown $1$-dimensional submanifold. Unlike conventional
applications of Isomap, the estimated latent positions do not lie on the
submanifold of interest. We extend existing convergence results for Isomap to
this setting and use them to demonstrate that, as the number of auxiliary
vertices increases, the power of our test converges to the power of the
corresponding test when the submanifold is known. Finally, we apply our methods
to an inference problem that arises in studying the connectome of the
Drosophila larval mushroom body. The univariate learnt manifold test rejects
($p<0.05$), while the multivariate ambient space test does not ($p\gg0.05$),
illustrating the value of identifying and exploiting low-dimensional structure
for subsequent inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Trosset_M/0/1/0/all/0/1"&gt;Michael W. Trosset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingyue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1"&gt;Minh Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Recommender System for Scientific Datasets and Analysis Pipelines. (arXiv:2108.09275v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09275</id>
        <link href="http://arxiv.org/abs/2108.09275"/>
        <updated>2021-08-23T01:36:37.168Z</updated>
        <summary type="html"><![CDATA[Scientific datasets and analysis pipelines are increasingly being shared
publicly in the interest of open science. However, mechanisms are lacking to
reliably identify which pipelines and datasets can appropriately be used
together. Given the increasing number of high-quality public datasets and
pipelines, this lack of clear compatibility threatens the findability and
reusability of these resources. We investigate the feasibility of a
collaborative filtering system to recommend pipelines and datasets based on
provenance records from previous executions. We evaluate our system using
datasets and pipelines extracted from the Canadian Open Neuroscience Platform,
a national initiative for open neuroscience. The recommendations provided by
our system (AUC$=0.83$) are significantly better than chance and outperform
recommendations made by domain experts using their previous knowledge as well
as pipeline and dataset descriptions (AUC$=0.63$). In particular, domain
experts often neglect low-level technical aspects of a pipeline-dataset
interaction, such as the level of pre-processing, which are captured by a
provenance-based system. We conclude that provenance-based pipeline and dataset
recommenders are feasible and beneficial to the sharing and usage of
open-science resources. Future work will focus on the collection of more
comprehensive provenance traces, and on deploying the system in production.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_M/0/1/0/all/0/1"&gt;Mandana Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1"&gt;Gregory Kiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1"&gt;Tristan Glatard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation for Singing Voice Detection. (arXiv:2011.04297v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04297</id>
        <link href="http://arxiv.org/abs/2011.04297"/>
        <updated>2021-08-23T01:36:37.162Z</updated>
        <summary type="html"><![CDATA[Singing Voice Detection (SVD) has been an active area of research in music
information retrieval (MIR). Currently, two deep neural network-based methods,
one based on CNN and the other on RNN, exist in literature that learn optimized
features for the voice detection (VD) task and achieve state-of-the-art
performance on common datasets. Both these models have a huge number of
parameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for
deployment on devices like smartphones or embedded sensors with limited
capacity in terms of memory and computation power. The most popular method to
address this issue is known as knowledge distillation in deep learning
literature (in addition to model compression) where a large pre-trained network
known as the teacher is used to train a smaller student network. Given the wide
applications of SVD in music information retrieval, to the best of our
knowledge, model compression for practical deployment has not yet been
explored. In this paper, efforts have been made to investigate this issue using
both conventional as well as ensemble knowledge distillation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Soumava Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+M_G/0/1/0/all/0/1"&gt;Gurunath Reddy M&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1"&gt;K Sreenivasa Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Partha Pratim Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Online Estimation of Causal Effects by Deciding What to Observe. (arXiv:2108.09265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09265</id>
        <link href="http://arxiv.org/abs/2108.09265"/>
        <updated>2021-08-23T01:36:37.153Z</updated>
        <summary type="html"><![CDATA[Researchers often face data fusion problems, where multiple data sources are
available, each capturing a distinct subset of variables. While problem
formulations typically take the data as given, in practice, data acquisition
can be an ongoing process. In this paper, we aim to estimate any functional of
a probabilistic model (e.g., a causal effect) as efficiently as possible, by
deciding, at each time, which data source to query. We propose online moment
selection (OMS), a framework in which structural assumptions are encoded as
moment conditions. The optimal action at each step depends, in part, on the
very moments that identify the functional of interest. Our algorithms balance
exploration with choosing the best action as suggested by current estimates of
the moments. We propose two selection strategies: (1) explore-then-commit
(OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero
asymptotic regret as assessed by MSE. We instantiate our setup for average
treatment effect estimation, where structural assumptions are given by a causal
graph and data sources may include subsets of mediators, confounders, and
instrumental variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shantanu Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Childers_D/0/1/0/all/0/1"&gt;David Childers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization. (arXiv:2009.13586v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13586</id>
        <link href="http://arxiv.org/abs/2009.13586"/>
        <updated>2021-08-23T01:36:37.135Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce Apollo, a quasi-Newton method for nonconvex
stochastic optimization, which dynamically incorporates the curvature of the
loss function by approximating the Hessian via a diagonal matrix. Importantly,
the update and storage of the diagonal approximation of Hessian is as efficient
as adaptive first-order optimization methods with linear complexity for both
time and memory. To handle nonconvexity, we replace the Hessian with its
rectified absolute value, which is guaranteed to be positive-definite.
Experiments on three tasks of vision and language show that Apollo achieves
significant improvements over other stochastic optimization methods, including
SGD and variants of Adam, in term of both convergence speed and generalization
performance. The implementation of the algorithm is available at
https://github.com/XuezheMax/apollo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuezhe Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning in two-player games between transparent opponents. (arXiv:2012.02671v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02671</id>
        <link href="http://arxiv.org/abs/2012.02671"/>
        <updated>2021-08-23T01:36:37.127Z</updated>
        <summary type="html"><![CDATA[We consider a scenario in which two reinforcement learning agents repeatedly
play a matrix game against each other and update their parameters after each
round. The agents' decision-making is transparent to each other, which allows
each agent to predict how their opponent will play against them. To prevent an
infinite regress of both agents recursively predicting each other indefinitely,
each agent is required to give an opponent-independent response with some
probability at least epsilon. Transparency also allows each agent to anticipate
and shape the other agent's gradient step, i.e. to move to regions of parameter
space in which the opponent's gradient points in a direction favourable to
them. We study the resulting dynamics experimentally, using two algorithms from
previous literature (LOLA and SOS) for opponent-aware learning. We find that
the combination of mutually transparent decision-making and opponent-aware
learning robustly leads to mutual cooperation in a single-shot prisoner's
dilemma. In a game of chicken, in which both agents try to manoeuvre their
opponent towards their preferred equilibrium, converging to a mutually
beneficial outcome turns out to be much harder, and opponent-aware learning can
even lead to worst-case outcomes for both agents. This highlights the need to
develop opponent-aware learning algorithms that achieve acceptable outcomes in
social dilemmas involving an equilibrium selection problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_A/0/1/0/all/0/1"&gt;Adrian Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development and evaluation of an Explainable Prediction Model for Chronic Kidney Disease Patients based on Ensemble Trees. (arXiv:2105.10368v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10368</id>
        <link href="http://arxiv.org/abs/2105.10368"/>
        <updated>2021-08-23T01:36:37.121Z</updated>
        <summary type="html"><![CDATA[Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing
incidence and high cost to health systems. A delayed recognition implies
premature mortality due to progressive loss of kidney function. The employment
of data mining to discover subtle patterns in CKD indicators would contribute
achieving early diagnosis. This work presents the development and evaluation of
an explainable prediction model that would support clinicians in the early
diagnosis of CKD patients. The model development is based on a data management
pipeline that detects the best combination of ensemble trees algorithms and
features selected concerning classification performance. The results obtained
through the pipeline equals the performance of the best CKD prediction models
identified in the literature. Furthermore, the main contribution of the paper
involves an explainability-driven approach that allows selecting the best
prediction model maintaining a balance between accuracy and explainability.
Therefore, the most balanced explainable prediction model of CKD implements an
XGBoost classifier over a group of 4 features (packed cell value, specific
gravity, albumin, and hypertension), achieving an accuracy of 98.9% and 97.5%
with cross-validation technique and with new unseen data respectively. In
addition, by analysing the model's explainability by means of different
post-hoc techniques, the packed cell value and the specific gravity are
determined as the most relevant features that influence the prediction results
of the model. This small number of feature selected results in a reduced cost
of the early diagnosis of CKD implying a promising solution for developing
countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Sanchez_P/0/1/0/all/0/1"&gt;Pedro A. Moreno-Sanchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network-wide link travel time and station waiting time estimation using automatic fare collection data: A computational graph approach. (arXiv:2108.09292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09292</id>
        <link href="http://arxiv.org/abs/2108.09292"/>
        <updated>2021-08-23T01:36:37.114Z</updated>
        <summary type="html"><![CDATA[Urban rail transit (URT) system plays a dominating role in many megacities
like Beijing and Hong Kong. Due to its important role and complex nature, it is
always in great need for public agencies to better understand the performance
of the URT system. This paper focuses on an essential and hard problem to
estimate the network-wide link travel time and station waiting time using the
automatic fare collection (AFC) data in the URT system, which is beneficial to
better understand the system-wide real-time operation state. The emerging
data-driven techniques, such as computational graph (CG) models in the machine
learning field, provide a new solution for solving this problem. In this study,
we first formulate a data-driven estimation optimization framework to estimate
the link travel time and station waiting time. Then, we cast the estimation
optimization model into a CG framework to solve the optimization problem and
obtain the estimation results. The methodology is verified on a synthetic URT
network and applied to a real-world URT network using the synthetic and
real-world AFC data, respectively. Results show the robustness and
effectiveness of the CG-based framework. To the best of our knowledge, this is
the first time that the CG is applied to the URT. This study can provide
critical insights to better understand the operational state in URT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinlei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lixing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1"&gt;Guangyin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Ziyou Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoPilot: Automating SoC Design Space Exploration for SWaP Constrained Autonomous UAVs. (arXiv:2102.02988v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02988</id>
        <link href="http://arxiv.org/abs/2102.02988"/>
        <updated>2021-08-23T01:36:37.108Z</updated>
        <summary type="html"><![CDATA[Building domain-specific accelerators for autonomous unmanned aerial vehicles
(UAVs) is challenging due to a lack of systematic methodology for designing
onboard compute. Balancing a computing system for a UAV requires considering
both the cyber (e.g., sensor rate, compute performance) and physical (e.g.,
payload weight) characteristics that affect overall performance. Iterating over
the many component choices results in a combinatorial explosion of the number
of possible combinations: from 10s of thousands to billions, depending on
implementation details. Manually selecting combinations of these components is
tedious and expensive. To navigate the {cyber-physical design space}
efficiently, we introduce \emph{AutoPilot}, a framework that automates
full-system UAV co-design. AutoPilot uses Bayesian optimization to navigate a
large design space and automatically select a combination of autonomy algorithm
and hardware accelerator while considering the cross-product effect of other
cyber and physical UAV components. We show that the AutoPilot methodology
consistently outperforms general-purpose hardware selections like Xavier NX and
Jetson TX2, as well as dedicated hardware accelerators built for autonomous
UAVs, across a range of representative scenarios (three different UAV types and
three deployment environments). Designs generated by AutoPilot increase the
number of missions on average by up to 2.25x, 1.62x, and 1.43x for nano, micro,
and mini-UAVs respectively over baselines. Our work demonstrates the need for
holistic full-UAV co-design to achieve maximum overall UAV performance and the
need for automated flows to simplify the design process for autonomous
cyber-physical systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1"&gt;Srivatsan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1"&gt;Zishen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_K/0/1/0/all/0/1"&gt;Kshitij Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1"&gt;Paul Whatmough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neuman_S/0/1/0/all/0/1"&gt;Sabrina Neuman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1"&gt;Gu-Yeon Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1"&gt;David Brooks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Vijay Janapa Reddi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v3 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02297</id>
        <link href="http://arxiv.org/abs/2108.02297"/>
        <updated>2021-08-23T01:36:37.101Z</updated>
        <summary type="html"><![CDATA[Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time-sequential data such as speech recognition. However, it is
difficult to deploy these networks on hardware to achieve high throughput and
low latency because the fully connected structure makes LSTM networks a
memory-bounded algorithm. Previous LSTM accelerators either exploited weight
spatial sparsity or temporal activation sparsity. This paper proposes a new
accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve
ultra-low latency inference. The spatial sparsity is induced using our proposed
pruning method called Column-Balanced Targeted Dropout (CBTD), which structures
sparse weight matrices for balanced workload. It achieved up to 96% weight
sparsity with negligible accuracy difference for an LSTM network trained on a
TIMIT phone recognition task. To induce temporal sparsity in LSTM, we create
the DeltaLSTM by extending the previous DeltaGRU method to the LSTM network.
This combined sparsity simultaneously saves on the weight memory access and
associated arithmetic operations. Spartus was implemented on a Xilinx Zynq-7100
FPGA. The Spartus per-sample latency for a single DeltaLSTM layer of 1024
neurons averages 1 us. Spartus achieved 9.4 TOp/s effective batch-1 throughput
and 1.1 TOp/J energy efficiency, which, respectively, are 4X and 7X higher than
the previous state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1"&gt;Tobi Delbruck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shih-Chii Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the experimental feasibility of quantum state reconstruction via machine learning. (arXiv:2012.09432v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09432</id>
        <link href="http://arxiv.org/abs/2012.09432"/>
        <updated>2021-08-23T01:36:37.083Z</updated>
        <summary type="html"><![CDATA[We determine the resource scaling of machine learning-based quantum state
reconstruction methods, in terms of inference and training, for systems of up
to four qubits when constrained to pure states. Further, we examine system
performance in the low-count regime, likely to be encountered in the tomography
of high-dimensional systems. Finally, we implement our quantum state
reconstruction method on an IBM Q quantum computer, and compare against both
unconstrained and constrained MLE state reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1"&gt;Sanjaya Lohani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1"&gt;Thomas A. Searles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1"&gt;Brian T. Kirby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1"&gt;Ryan T. Glasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minority Oversampling for Imbalanced Time Series Classification. (arXiv:2004.06373v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.06373</id>
        <link href="http://arxiv.org/abs/2004.06373"/>
        <updated>2021-08-23T01:36:37.077Z</updated>
        <summary type="html"><![CDATA[Many important real-world applications involve time-series data with skewed
distribution. Compared to conventional imbalance learning problems, the
classification of imbalanced time-series data is more challenging due to high
dimensionality and high inter-variable correlation. This paper proposes a
structure preserving Oversampling method to combat the High-dimensional
Imbalanced Time-series classification (OHIT). OHIT first leverages a
density-ratio based shared nearest neighbor clustering algorithm to capture the
modes of minority class in high-dimensional space. It then for each mode
applies the shrinkage technique of large-dimensional covariance matrix to
obtain accurate and reliable covariance structure. Finally, OHIT generates the
structure-preserving synthetic samples based on multivariate Gaussian
distribution by using the estimated covariance matrices. Experimental results
on several publicly available time-series datasets (including unimodal and
multimodal) demonstrate the superiority of OHIT against the state-of-the-art
oversampling algorithms in terms of F1, G-mean, and AUC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tuanfei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Cheng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Siqi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance Space Analysis for the Car Sequencing Problem. (arXiv:2012.10053v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10053</id>
        <link href="http://arxiv.org/abs/2012.10053"/>
        <updated>2021-08-23T01:36:37.070Z</updated>
        <summary type="html"><![CDATA[We investigate an important research question for solving the car sequencing
problem, that is, which characteristics make an instance hard to solve? To do
so, we carry out an instance space analysis for the car sequencing problem, by
extracting a vector of problem features to characterize an instance. In order
to visualize the instance space, the feature vectors are projected onto a
two-dimensional space using dimensionality reduction techniques. The resulting
two-dimensional visualizations provide new insights into the characteristics of
the instances used for testing and how these characteristics influence the
behaviours of an optimization algorithm. This analysis guides us in
constructing a new set of benchmark instances with a range of instance
properties. We demonstrate that these new instances are more diverse than the
previous benchmarks, including some instances that are significantly more
difficult to solve. We introduce two new algorithms for solving the car
sequencing problem and compare them with four existing methods from the
literature. Our new algorithms are shown to perform competitively for this
problem but no single algorithm can outperform all others over all instances.
This observation motivates us to build an algorithm selection model based on
machine learning, to identify the niche in the instance space that an algorithm
is expected to perform well on. Our analysis helps to understand problem
hardness and select an appropriate algorithm for solving a given car sequencing
problem instance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Esler_S/0/1/0/all/0/1"&gt;Samuel Esler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Thiruvady_D/0/1/0/all/0/1"&gt;Dhananjay Thiruvady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ernst_A/0/1/0/all/0/1"&gt;Andreas T. Ernst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaodong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Morgan_K/0/1/0/all/0/1"&gt;Kerri Morgan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation. (arXiv:2105.06421v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06421</id>
        <link href="http://arxiv.org/abs/2105.06421"/>
        <updated>2021-08-23T01:36:37.064Z</updated>
        <summary type="html"><![CDATA[Over the past few years, best SSL methods, gradually moved from the pre-text
task learning to the Contrastive learning. But contrastive methods have some
drawbacks which could not be solved completely, such as performing poor on
fine-grained visual tasks compare to supervised learning methods. In this
study, at first, the impact of ImageNet pre-training on fine-grained Facial
Expression Recognition (FER) was tested. It could be seen from the results that
training from scratch is better than ImageNet fine-tuning at stronger
augmentation levels. After that, a framework was proposed for standard
Supervised Learning (SL), called Hybrid Multi-Task Learning (HMTL) which merged
Self-Supervised as auxiliary task to the SL training setting. Leveraging
Self-Supervised Learning (SSL) can gain additional information from input data
than labels which can help the main fine-grained SL task. It is been
investigated how this method could be used for FER by designing two customized
version of common pre-text techniques, Jigsaw puzzling and in-painting. The
state-of-the-art was reached on AffectNet via two types of HMTL, without
utilizing pre-training on additional datasets. Moreover, we showed the
difference between SS pre-training and HMTL to demonstrate superiority of
proposed method. Furthermore, the impact of proposed method was shown on two
other fine-grained facial tasks, Head Poses estimation and Gender Recognition,
which concluded to reduce in error rate by 11% and 1% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pourmirzaei_M/0/1/0/all/0/1"&gt;Mahdi Pourmirzaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montazer_G/0/1/0/all/0/1"&gt;Gholam Ali Montazer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaili_F/0/1/0/all/0/1"&gt;Farzaneh Esmaili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Network Embedding with Differentiable Deep Quantisation. (arXiv:2108.09128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09128</id>
        <link href="http://arxiv.org/abs/2108.09128"/>
        <updated>2021-08-23T01:36:36.903Z</updated>
        <summary type="html"><![CDATA[Learning accurate low-dimensional embeddings for a network is a crucial task
as it facilitates many downstream network analytics tasks. For large networks,
the trained embeddings often require a significant amount of space to store,
making storage and processing a challenge. Building on our previous work on
semi-supervised network embedding, we develop d-SNEQ, a differentiable
DNN-based quantisation method for network embedding. d-SNEQ incorporates a rank
loss to equip the learned quantisation codes with rich high-order information
and is able to substantially compress the size of trained embeddings, thus
reducing storage footprint and accelerating retrieval speed. We also propose a
new evaluation metric, path prediction, to fairly and more directly evaluate
model performance on the preservation of high-order information. Our evaluation
on four real-world networks of diverse characteristics shows that d-SNEQ
outperforms a number of state-of-the-art embedding methods in link prediction,
path prediction, node classification, and node recommendation while being far
more space- and time-efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Order Simple Regret for Gaussian Process Bandits. (arXiv:2108.09262v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.09262</id>
        <link href="http://arxiv.org/abs/2108.09262"/>
        <updated>2021-08-23T01:36:36.887Z</updated>
        <summary type="html"><![CDATA[Consider the sequential optimization of a continuous, possibly non-convex,
and expensive to evaluate objective function $f$. The problem can be cast as a
Gaussian Process (GP) bandit where $f$ lives in a reproducing kernel Hilbert
space (RKHS). The state of the art analysis of several learning algorithms
shows a significant gap between the lower and upper bounds on the simple regret
performance. When $N$ is the number of exploration trials and $\gamma_N$ is the
maximal information gain, we prove an $\tilde{\mathcal{O}}(\sqrt{\gamma_N/N})$
bound on the simple regret performance of a pure exploration algorithm that is
significantly tighter than the existing bounds. We show that this bound is
order optimal up to logarithmic factors for the cases where a lower bound on
regret is known. To establish these results, we prove novel and sharp
confidence intervals for GP models applicable to RKHS elements which may be of
broader interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vakili_S/0/1/0/all/0/1"&gt;Sattar Vakili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bouziani_N/0/1/0/all/0/1"&gt;Nacime Bouziani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jalali_S/0/1/0/all/0/1"&gt;Sepehr Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Adaptable Deep Learning-Based Intrusion Detection System to Zero-Day Attacks. (arXiv:2108.09199v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09199</id>
        <link href="http://arxiv.org/abs/2108.09199"/>
        <updated>2021-08-23T01:36:36.881Z</updated>
        <summary type="html"><![CDATA[The intrusion detection system (IDS) is an essential element of security
monitoring in computer networks. An IDS distinguishes the malicious traffic
from the benign one and determines the attack types targeting the assets of the
organization. The main challenge of an IDS is facing new (i.e., zero-day)
attacks and separating them from benign traffic and existing types of attacks.
Along with the power of the deep learning-based IDSes in auto-extracting
high-level features and its independence from the time-consuming and costly
signature extraction process, the mentioned challenge still exists in this new
generation of IDSes.

In this paper, we propose a framework for deep learning-based IDSes
addressing new attacks. This framework is the first approach using both deep
novelty-based classifiers besides the traditional clustering based on the
specialized layer of deep structures, in the security scope. Additionally, we
introduce DOC++ as a newer version of DOC as a deep novelty-based classifier.
We also employ the Deep Intrusion Detection (DID) framework for the
preprocessing phase, which improves the ability of deep learning algorithms to
detect content-based attacks. We compare four different algorithms (including
DOC, DOC++, OpenMax, and AutoSVM) as the novelty classifier of the framework
and use both the CIC-IDS2017 and CSE-CIC-IDS2018 datasets for the evaluation.
Our results show that DOC++ is the best implementation of the open set
recognition module. Besides, the completeness and homogeneity of the clustering
and post-training phase prove that this model is good enough for the supervised
labeling and updating phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1"&gt;Mahdi Soltani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ousat_B/0/1/0/all/0/1"&gt;Behzad Ousat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siavoshani_M/0/1/0/all/0/1"&gt;Mahdi Jafari Siavoshani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahangir_A/0/1/0/all/0/1"&gt;Amir Hossein Jahangir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction. (arXiv:2108.09091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09091</id>
        <link href="http://arxiv.org/abs/2108.09091"/>
        <updated>2021-08-23T01:36:36.868Z</updated>
        <summary type="html"><![CDATA[Nowadays, with the rapid development of IoT (Internet of Things) and CPS
(Cyber-Physical Systems) technologies, big spatiotemporal data are being
generated from mobile phones, car navigation systems, and traffic sensors. By
leveraging state-of-the-art deep learning technologies on such data, urban
traffic prediction has drawn a lot of attention in AI and Intelligent
Transportation System community. The problem can be uniformly modeled with a 3D
tensor (T, N, C), where T denotes the total time steps, N denotes the size of
the spatial domain (i.e., mesh-grids or graph-nodes), and C denotes the
channels of information. According to the specific modeling strategy, the
state-of-the-art deep learning models can be divided into three categories:
grid-based, graph-based, and multivariate time-series models. In this study, we
first synthetically review the deep traffic models as well as the widely used
datasets, then build a standard benchmark to comprehensively evaluate their
performances with the same settings and metrics. Our study named DL-Traff is
implemented with two most popular deep learning frameworks, i.e., TensorFlow
and PyTorch, which is already publicly available as two GitHub repositories
https://github.com/deepkashiwa20/DL-Traff-Grid and
https://github.com/deepkashiwa20/DL-Traff-Graph. With DL-Traff, we hope to
deliver a useful resource to researchers who are interested in spatiotemporal
data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Renhe Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Du Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaonan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiewen Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hangchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zekun Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jinliang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shibasaki_R/0/1/0/all/0/1"&gt;Ryosuke Shibasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding the Generative Capability of Adversarially Robust Classifiers. (arXiv:2108.09093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09093</id>
        <link href="http://arxiv.org/abs/2108.09093"/>
        <updated>2021-08-23T01:36:36.861Z</updated>
        <summary type="html"><![CDATA[Recently, some works found an interesting phenomenon that adversarially
robust classifiers can generate good images comparable to generative models. We
investigate this phenomenon from an energy perspective and provide a novel
explanation. We reformulate adversarial example generation, adversarial
training, and image generation in terms of an energy function. We find that
adversarial training contributes to obtaining an energy function that is flat
and has low energy around the real data, which is the key for generative
capability. Based on our new understanding, we further propose a better
adversarial training method, Joint Energy Adversarial Training (JEAT), which
can generate high-quality images and achieve new state-of-the-art robustness
under a wide range of attacks. The Inception Score of the images (CIFAR-10)
generated by JEAT is 8.80, much better than original robust classifiers (7.50).
In particular, we achieve new state-of-the-art robustness on CIFAR-10 (from
57.20% to 62.04%) and CIFAR-100 (from 30.03% to 30.18%) without extra training
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiacheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Rongxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvDrop: Adversarial Attack to DNNs by Dropping Information. (arXiv:2108.09034v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09034</id>
        <link href="http://arxiv.org/abs/2108.09034"/>
        <updated>2021-08-23T01:36:36.853Z</updated>
        <summary type="html"><![CDATA[Human can easily recognize visual objects with lost information: even losing
most details with only contour reserved, e.g. cartoon. However, in terms of
visual perception of Deep Neural Networks (DNNs), the ability for recognizing
abstract objects (visual objects with lost information) is still a challenge.
In this work, we investigate this issue from an adversarial viewpoint: will the
performance of DNNs decrease even for the images only losing a little
information? Towards this end, we propose a novel adversarial attack, named
\textit{AdvDrop}, which crafts adversarial examples by dropping existing
information of images. Previously, most adversarial attacks add extra
disturbing information on clean images explicitly. Opposite to previous works,
our proposed work explores the adversarial robustness of DNN models in a novel
perspective by dropping imperceptible details to craft adversarial examples. We
demonstrate the effectiveness of \textit{AdvDrop} by extensive experiments, and
show that this new type of adversarial examples is more difficult to be
defended by current defense systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1"&gt;Ranjie Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Dantong Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1"&gt;A. K. Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PASTO: Strategic Parameter Optimization in Recommendation Systems -- Probabilistic is Better than Deterministic. (arXiv:2108.09076v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09076</id>
        <link href="http://arxiv.org/abs/2108.09076"/>
        <updated>2021-08-23T01:36:36.836Z</updated>
        <summary type="html"><![CDATA[Real-world recommendation systems often consist of two phases. In the first
phase, multiple predictive models produce the probability of different
immediate user actions. In the second phase, these predictions are aggregated
according to a set of 'strategic parameters' to meet a diverse set of business
goals, such as longer user engagement, higher revenue potential, or more
community/network interactions. In addition to building accurate predictive
models, it is also crucial to optimize this set of 'strategic parameters' so
that primary goals are optimized while secondary guardrails are not hurt. In
this setting with multiple and constrained goals, this paper discovers that a
probabilistic strategic parameter regime can achieve better value compared to
the standard regime of finding a single deterministic parameter. The new
probabilistic regime is to learn the best distribution over strategic parameter
choices and sample one strategic parameter from the distribution when each user
visits the platform. To pursue the optimal probabilistic solution, we formulate
the problem into a stochastic compositional optimization problem, in which the
unbiased stochastic gradient is unavailable. Our approach is applied in a
popular social network platform with hundreds of millions of daily users and
achieves +0.22% lift of user engagement in a recommendation task and +1.7% lift
in revenue in an advertising optimization scenario comparing to using the best
deterministic parameter strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weicong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hanlin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jingshuo Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guangxu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1"&gt;Qiang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Dong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xuezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1"&gt;Dongying Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1"&gt;Qiao Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobility-Aware Cluster Federated Learning in Hierarchical Wireless Networks. (arXiv:2108.09103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09103</id>
        <link href="http://arxiv.org/abs/2108.09103"/>
        <updated>2021-08-23T01:36:36.828Z</updated>
        <summary type="html"><![CDATA[Implementing federated learning (FL) algorithms in wireless networks has
garnered a wide range of attention. However, few works have considered the
impact of user mobility on the learning performance. To fill this research gap,
firstly, we develop a theoretical model to characterize the hierarchical
federated learning (HFL) algorithm in wireless networks where the mobile users
may roam across multiple edge access points, leading to incompletion of
inconsistent FL training. Secondly, we provide the convergence analysis of HFL
with user mobility. Our analysis proves that the learning performance of HFL
deteriorates drastically with highly-mobile users. And this decline in the
learning performance will be exacerbated with small number of participants and
large data distribution divergences among local data of users. To circumvent
these issues, we propose a mobility-aware cluster federated learning (MACFL)
algorithm by redesigning the access mechanism, local update rule and model
aggregation scheme. Finally, we provide experiments to evaluate the learning
performance of HFL and our MACFL. The results show that our MACFL can enhance
the learning performance, especially for three different cases, namely, the
case of users with non-independent and identical distribution data, the case of
users with high mobility, and the cases with a small number of users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chenyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Howard H. Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Deshun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhiwei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1"&gt;Tony Q. S. Quek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1"&gt;Geyong Min&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Sequence Modeling: Development and Applications in Asset Pricing. (arXiv:2108.08999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08999</id>
        <link href="http://arxiv.org/abs/2108.08999"/>
        <updated>2021-08-23T01:36:36.821Z</updated>
        <summary type="html"><![CDATA[We predict asset returns and measure risk premia using a prominent technique
from artificial intelligence -- deep sequence modeling. Because asset returns
often exhibit sequential dependence that may not be effectively captured by
conventional time series models, sequence modeling offers a promising path with
its data-driven approach and superior performance. In this paper, we first
overview the development of deep sequence models, introduce their applications
in asset pricing, and discuss their advantages and limitations. We then perform
a comparative analysis of these methods using data on U.S. equities. We
demonstrate how sequence modeling benefits investors in general through
incorporating complex historical path dependence, and that Long- and Short-term
Memory (LSTM) based models tend to have the best out-of-sample performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cong_L/0/1/0/all/0/1"&gt;Lin William Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical and Fast Momentum-Based Power Methods. (arXiv:2108.09264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09264</id>
        <link href="http://arxiv.org/abs/2108.09264"/>
        <updated>2021-08-23T01:36:36.814Z</updated>
        <summary type="html"><![CDATA[The power method is a classical algorithm with broad applications in machine
learning tasks, including streaming PCA, spectral clustering, and low-rank
matrix approximation. The distilled purpose of the vanilla power method is to
determine the largest eigenvalue (in absolute modulus) and its eigenvector of a
matrix. A momentum-based scheme can be used to accelerate the power method, but
achieving an optimal convergence rate with existing algorithms critically
relies on additional spectral information that is unavailable at run-time, and
sub-optimal initializations can result in divergence. In this paper, we provide
a pair of novel momentum-based power methods, which we call the delayed
momentum power method (DMPower) and a streaming variant, the delayed momentum
streaming method (DMStream). Our methods leverage inexact deflation and are
capable of achieving near-optimal convergence with far less restrictive
hyperparameter requirements. We provide convergence analyses for both
algorithms through the lens of perturbation theory. Further, we experimentally
demonstrate that DMPower routinely outperforms the vanilla power method and
that both algorithms match the convergence speed of an oracle running existing
accelerated methods with perfect spectral knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabbani_T/0/1/0/all/0/1"&gt;Tahseen Rabbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Apollo Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajkumar_A/0/1/0/all/0/1"&gt;Arjun Rajkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TabGNN: Multiplex Graph Neural Network for Tabular Data Prediction. (arXiv:2108.09127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09127</id>
        <link href="http://arxiv.org/abs/2108.09127"/>
        <updated>2021-08-23T01:36:36.798Z</updated>
        <summary type="html"><![CDATA[Tabular data prediction (TDP) is one of the most popular industrial
applications, and various methods have been designed to improve the prediction
performance. However, existing works mainly focus on feature interactions and
ignore sample relations, e.g., users with the same education level might have a
similar ability to repay the debt. In this work, by explicitly and
systematically modeling sample relations, we propose a novel framework TabGNN
based on recently popular graph neural networks (GNN). Specifically, we firstly
construct a multiplex graph to model the multifaceted sample relations, and
then design a multiplex graph neural network to learn enhanced representation
for each sample. To integrate TabGNN with the tabular solution in our company,
we concatenate the learned embeddings and the original ones, which are then fed
to prediction models inside the solution. Experiments on eleven TDP datasets
from various domains, including classification and regression ones, show that
TabGNN can consistently improve the performance compared to the tabular
solution AutoFE in 4Paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiawei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1"&gt;Yuhan Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Huan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Quanming Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1"&gt;Weiwei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks Against Split Learning. (arXiv:2108.09033v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09033</id>
        <link href="http://arxiv.org/abs/2108.09033"/>
        <updated>2021-08-23T01:36:36.792Z</updated>
        <summary type="html"><![CDATA[Training deep neural networks requires large scale data, which often forces
users to work in a distributed or outsourced setting, accompanied with privacy
concerns. Split learning framework aims to address this concern by splitting up
the model among the client and the server. The idea is that since the server
does not have access to client's part of the model, the scheme supposedly
provides privacy. We show that this is not true via two novel attacks. (1) We
show that an honest-but-curious split learning server, equipped only with the
knowledge of the client neural network architecture, can recover the input
samples and also obtain a functionally similar model to the client model,
without the client being able to detect the attack. (2) Furthermore, we show
that if split learning is used naively to protect the training labels, the
honest-but-curious server can infer the labels with perfect accuracy. We test
our attacks using three benchmark datasets and investigate various properties
of the overall system that affect the attacks' effectiveness. Our results show
that plaintext split learning paradigm can pose serious security risks and
provide no more than a false sense of security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1"&gt;Ege Erdogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kupcu_A/0/1/0/all/0/1"&gt;Alptekin Kupcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_A/0/1/0/all/0/1"&gt;A. Ercument Cicek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Distributionally Robust Optimization for Phase Configuration of RISs. (arXiv:2108.09026v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09026</id>
        <link href="http://arxiv.org/abs/2108.09026"/>
        <updated>2021-08-23T01:36:36.786Z</updated>
        <summary type="html"><![CDATA[In this article, we study the problem of robust reconfigurable intelligent
surface (RIS)-aided downlink communication over heterogeneous RIS types in the
supervised learning setting. By modeling downlink communication over
heterogeneous RIS designs as different workers that learn how to optimize phase
configurations in a distributed manner, we solve this distributed learning
problem using a distributionally robust formulation in a
communication-efficient manner, while establishing its rate of convergence. By
doing so, we ensure that the global model performance of the worst-case worker
is close to the performance of other workers. Simulation results show that our
proposed algorithm requires fewer communication rounds (about 50% lesser) to
achieve the same worst-case distribution test accuracy compared to competitive
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Issaid_C/0/1/0/all/0/1"&gt;Chaouki Ben Issaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samarakoon_S/0/1/0/all/0/1"&gt;Sumudu Samarakoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Time to Replace CNNs with Transformers for Medical Images?. (arXiv:2108.09038v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09038</id>
        <link href="http://arxiv.org/abs/2108.09038"/>
        <updated>2021-08-23T01:36:36.780Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have reigned for a decade as the de
facto approach to automated medical image diagnosis. Recently, vision
transformers (ViTs) have appeared as a competitive alternative to CNNs,
yielding similar levels of performance while possessing several interesting
properties that could prove beneficial for medical imaging tasks. In this work,
we explore whether it is time to move to transformer-based models or if we
should keep working with CNNs - can we trivially switch to transformers? If so,
what are the advantages and drawbacks of switching to ViTs for medical image
diagnosis? We consider these questions in a series of experiments on three
mainstream medical image datasets. Our findings show that, while CNNs perform
better when trained from scratch, off-the-shelf vision transformers using
default hyperparameters are on par with CNNs when pretrained on ImageNet, and
outperform their CNN counterparts when pretrained using self-supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1"&gt;Christos Matsoukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1"&gt;Johan Fredin Haslum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1"&gt;Magnus S&amp;#xf6;derberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimation of Convex Polytopes for Automatic Discovery of Charge State Transitions in Quantum Dot Arrays. (arXiv:2108.09133v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09133</id>
        <link href="http://arxiv.org/abs/2108.09133"/>
        <updated>2021-08-23T01:36:36.773Z</updated>
        <summary type="html"><![CDATA[In spin based quantum dot arrays, a leading technology for quantum
computation applications, material or fabrication imprecisions affect the
behaviour of the device, which is compensated via tuning parameters. Automatic
tuning of these device parameters constitutes a formidable challenge for
machine-learning. Here, we present the first practical algorithm for
controlling the transition of electrons in a spin qubit array. We exploit a
connection to computational geometry and phrase the task as estimating a convex
polytope from measurements.

Our proposed algorithm uses active learning, to find the count, shapes and
sizes of all facets of a given polytope. We test our algorithm on artifical
polytopes as well as a real 2x2 spin qubit array. Our results show that we can
reliably find the facets of the polytope, including small facets with sizes on
the order of the measurement precision. We discuss the implications of the
NP-hardness of the underlying estimation problem and outline design
considerations, limitations and tuning strategies for controlling future
large-scale spin qubit devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasmussen_T/0/1/0/all/0/1"&gt;Torbj&amp;#xf8;rn Rasmussen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brovang_B/0/1/0/all/0/1"&gt;Bertram Brovang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1"&gt;Anasua Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuemmeth_F/0/1/0/all/0/1"&gt;Ferdinand Kuemmeth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Federated Learning with a Global Biased Optimiser. (arXiv:2108.09134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09134</id>
        <link href="http://arxiv.org/abs/2108.09134"/>
        <updated>2021-08-23T01:36:36.767Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a recent development in the field of machine
learning that collaboratively trains models without the training data leaving
client devices, in order to preserve data-privacy. In realistic settings, the
total training set is distributed over clients in a highly non-Independent and
Identically Distributed (non-IID) fashion, which has been shown extensively to
harm FL convergence speed and final model performance. We propose a novel,
generalised approach for applying adaptive optimisation techniques to FL with
the Federated Global Biased Optimiser (FedGBO) algorithm. FedGBO accelerates FL
by applying a set of global biased optimiser values during the local training
phase of FL, which helps to reduce `client-drift' from non-IID data, whilst
also benefiting from adaptive momentum/learning-rate methods. We show that the
FedGBO update with a generic optimiser can be viewed as a centralised update
with biased gradients and optimiser update, and use this theoretical framework
to prove the convergence of FedGBO using momentum-Stochastic Gradient Descent.
We also perform extensive experiments using 4 realistic benchmark FL datasets
and 3 popular adaptive optimisers to compare the performance of different
adaptive-FL approaches, demonstrating that FedGBO has highly competitive
performance considering its low communication and computation costs, and
providing highly practical insights for the use of adaptive optimisation in FL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1"&gt;Jed Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jia Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1"&gt;Geyong Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rui Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Siwei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Localization Based on Call Detail Records. (arXiv:2108.09157v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09157</id>
        <link href="http://arxiv.org/abs/2108.09157"/>
        <updated>2021-08-23T01:36:36.761Z</updated>
        <summary type="html"><![CDATA[Understanding human mobility is essential for many fields, including
transportation planning. Currently, surveys are the primary source for such
analysis. However, in the recent past, many researchers have focused on Call
Detail Records (CDR) for identifying travel patterns. CDRs have shown
correlation to human mobility behavior. However, one of the main issues in
using CDR data is that it is difficult to identify the precise location of the
user due to the low spacial resolution of the data and other artifacts such as
the load sharing effect. Existing approaches have certain limitations. Previous
studies using CDRs do not consider the transmit power of cell towers when
localizing the users and use an oversimplified approach to identify load
sharing effects. Furthermore, they consider the entire population of users as
one group neglecting the differences in mobility patterns of different segments
of users. This research introduces a novel methodology to user position
localization from CDRs through improved detection of load sharing effects, by
taking the transmit power into account, and segmenting the users into distinct
groups for the purpose of learning any parameters of the model. Moreover, this
research uses several methods to address the existing limitations and validate
the generated results using nearly 4 billion CDR data points with travel survey
data and voluntarily collected mobile data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayesha_B/0/1/0/all/0/1"&gt;Buddhi Ayesha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeewanthi_B/0/1/0/all/0/1"&gt;Bhagya Jeewanthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitraranjan_C/0/1/0/all/0/1"&gt;Charith Chitraranjan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perera_A/0/1/0/all/0/1"&gt;Amal Shehan Perera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumarage_A/0/1/0/all/0/1"&gt;Amal S. Kumarage&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09151</id>
        <link href="http://arxiv.org/abs/2108.09151"/>
        <updated>2021-08-23T01:36:36.754Z</updated>
        <summary type="html"><![CDATA[Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiuniu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Antoni B. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MHealth: An Artificial Intelligence Oriented Mobile Application for Personal Healthcare Support. (arXiv:2108.09277v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.09277</id>
        <link href="http://arxiv.org/abs/2108.09277"/>
        <updated>2021-08-23T01:36:36.713Z</updated>
        <summary type="html"><![CDATA[Main objective of this study is to introduce an expert system-based mHealth
application that takes Artificial Intelligence support by considering
previously introduced solutions from the literature and employing possible
requirements for a better solution. Thanks to that research study, a mobile
software system having Artificial Intelligence support and providing dynamic
support against the common health problems in daily life was designed-developed
and it was evaluated via survey and diagnosis-based evaluation tasks.
Evaluation tasks indicated positive outcomes for the mHealth system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afrah_I/0/1/0/all/0/1"&gt;Ismail Ali Afrah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kose_U/0/1/0/all/0/1"&gt;Utku Kose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAE-CE: Visual Contrastive Explanation using Disentangled VAEs. (arXiv:2108.09159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09159</id>
        <link href="http://arxiv.org/abs/2108.09159"/>
        <updated>2021-08-23T01:36:36.703Z</updated>
        <summary type="html"><![CDATA[The goal of a classification model is to assign the correct labels to data.
In most cases, this data is not fully described by the given set of labels.
Often a rich set of meaningful concepts exist in the domain that can much more
precisely describe each datapoint. Such concepts can also be highly useful for
interpreting the model's classifications. In this paper we propose a model,
denoted as Variational Autoencoder-based Contrastive Explanation (VAE-CE), that
represents data with high-level concepts and uses this representation for both
classification and generating explanations. The explanations are produced in a
contrastive manner, conveying why a datapoint is assigned to one class rather
than an alternative class. An explanation is specified as a set of
transformations of the input datapoint, with each step depicting a concept
changing towards the contrastive class. We build the model using a disentangled
VAE, extended with a new supervised method for disentangling individual
dimensions. An analysis on synthetic data and MNIST shows that the approaches
to both disentanglement and explanation provide benefits over other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poels_Y/0/1/0/all/0/1"&gt;Yoeri Poels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1"&gt;Vlado Menkovski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain-adaptive Hash for Networks. (arXiv:2108.09136v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09136</id>
        <link href="http://arxiv.org/abs/2108.09136"/>
        <updated>2021-08-23T01:36:36.695Z</updated>
        <summary type="html"><![CDATA[Abundant real-world data can be naturally represented by large-scale
networks, which demands efficient and effective learning algorithms. At the
same time, labels may only be available for some networks, which demands these
algorithms to be able to adapt to unlabeled networks. Domain-adaptive hash
learning has enjoyed considerable success in the computer vision community in
many practical tasks due to its lower cost in both retrieval time and storage
footprint. However, it has not been applied to multiple-domain networks. In
this work, we bridge this gap by developing an unsupervised domain-adaptive
hash learning method for networks, dubbed UDAH. Specifically, we develop four
{task-specific yet correlated} components: (1) network structure preservation
via a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,
(3) cross-domain intersected discriminators, and (4) semantic center alignment.
We conduct a wide range of experiments to evaluate the effectiveness and
efficiency of our method on a range of tasks including link prediction, node
classification, and neighbor recommendation. Our evaluation results demonstrate
that our model achieves better performance than the state-of-the-art
conventional discrete embedding methods over all the tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A fuzzy-rough uncertainty measure to discover bias encoded explicitly or implicitly in features of structured pattern classification datasets. (arXiv:2108.09098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09098</id>
        <link href="http://arxiv.org/abs/2108.09098"/>
        <updated>2021-08-23T01:36:36.684Z</updated>
        <summary type="html"><![CDATA[The need to measure bias encoded in tabular data that are used to solve
pattern recognition problems is widely recognized by academia, legislators and
enterprises alike. In previous work, we proposed a bias quantification measure,
called fuzzy-rough uncer-tainty, which relies on the fuzzy-rough set theory.
The intuition dictates that protected features should not change the
fuzzy-rough boundary regions of a decision class significantly. The extent to
which this happens is a proxy for bias expressed as uncertainty in
adecision-making context. Our measure's main advantage is that it does not
depend on any machine learning prediction model but adistance function. In this
paper, we extend our study by exploring the existence of bias encoded
implicitly in non-protected featuresas defined by the correlation between
protected and unprotected attributes. This analysis leads to four scenarios
that domain experts should evaluate before deciding how to tackle bias. In
addition, we conduct a sensitivity analysis to determine the fuzzy operatorsand
distance function that best capture change in the boundary regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1"&gt;Gonzalo N&amp;#xe1;poles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koumeri_L/0/1/0/all/0/1"&gt;Lisa Koutsoviti Koumeri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representations for Label Noise Require Fine-Tuning. (arXiv:2108.09154v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09154</id>
        <link href="http://arxiv.org/abs/2108.09154"/>
        <updated>2021-08-23T01:36:36.673Z</updated>
        <summary type="html"><![CDATA[In this paper we show that the combination of a Contrastive representation
with a label noise-robust classification head requires fine-tuning the
representation in order to achieve state-of-the-art performances. Since
fine-tuned representations are shown to outperform frozen ones, one can
conclude that noise-robust classification heads are indeed able to promote
meaningful representations if provided with a suitable starting point.
Experiments are conducted to draw a comprehensive picture of performances by
featuring six methods and nine noise instances of three different kinds (none,
symmetric, and asymmetric). In presence of noise the experiments show that fine
tuning of Contrastive representation allows the six methods to achieve better
results than end-to-end learning and represent a new reference compare to the
recent state of art. Results are also remarkable stable versus the noise level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nodet_P/0/1/0/all/0/1"&gt;Pierre Nodet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1"&gt;Vincent Lemaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondu_A/0/1/0/all/0/1"&gt;Alexis Bondu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornuejols_A/0/1/0/all/0/1"&gt;Antoine Cornu&amp;#xe9;jols&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combination of Transfer Learning, Recursive Learning and Ensemble Learning for Multi-Day Ahead COVID-19 Cases Prediction in India using Gated Recurrent Unit Networks. (arXiv:2108.09131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09131</id>
        <link href="http://arxiv.org/abs/2108.09131"/>
        <updated>2021-08-23T01:36:36.658Z</updated>
        <summary type="html"><![CDATA[The current COVID-19 pandemic has put a huge challenge on the Indian health
infrastructure. With more and more people getting affected during the second
wave, the hospitals were over-burdened, running out of supplies and oxygen. In
this scenario, prediction of the number of COVID-19 cases beforehand might have
helped in the better utilization of limited resources and supplies. This
manuscript deals with the prediction of new COVID-19 cases, new deaths and
total active cases for multiple days in advance. The proposed method uses gated
recurrent unit networks as the main predicting model. A study is conducted by
building four models that are pre-trained on the data from four different
countries (United States of America, Brazil, Spain and Bangladesh) and are
fine-tuned or retrained on India's data. Since the four countries chosen have
experienced different types of infection curves, the pre-training provides a
transfer learning to the models incorporating diverse situations into account.
Each of the four models then give a multiple days ahead predictions using
recursive learning method for the Indian test data. The final prediction comes
from an ensemble of the predictions of the combination of different models.
This method with two countries, Spain and Brazil, is seen to achieve the best
performance amongst all the combinations as well as compared to other
traditional regression models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1"&gt;Debasrita Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1"&gt;Debayan Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Susmita Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Ashish Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1"&gt;Jonathan H. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[State-Of-The-Art Algorithms For Low-Rank Dynamic Mode Decomposition. (arXiv:2108.09160v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.09160</id>
        <link href="http://arxiv.org/abs/2108.09160"/>
        <updated>2021-08-23T01:36:36.632Z</updated>
        <summary type="html"><![CDATA[This technical note reviews sate-of-the-art algorithms for linear
approximation of high-dimensional dynamical systems using low-rank dynamic mode
decomposition (DMD). While repeating several parts of our article "low-rank
dynamic mode decomposition: an exact and tractable solution", this work
provides additional details useful for building a comprehensive picture of
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Heas_P/0/1/0/all/0/1"&gt;Patrick Heas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Herzet_C/0/1/0/all/0/1"&gt;Cedric Herzet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lessons from the Clustering Analysis of a Search Space: A Centroid-based Approach to Initializing NAS. (arXiv:2108.09126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09126</id>
        <link href="http://arxiv.org/abs/2108.09126"/>
        <updated>2021-08-23T01:36:36.618Z</updated>
        <summary type="html"><![CDATA[Lots of effort in neural architecture search (NAS) research has been
dedicated to algorithmic development, aiming at designing more efficient and
less costly methods. Nonetheless, the investigation of the initialization of
these techniques remain scare, and currently most NAS methodologies rely on
stochastic initialization procedures, because acquiring information prior to
search is costly. However, the recent availability of NAS benchmarks have
enabled low computational resources prototyping. In this study, we propose to
accelerate a NAS algorithm using a data-driven initialization technique,
leveraging the availability of NAS benchmarks. Particularly, we proposed a
two-step methodology. First, a calibrated clustering analysis of the search
space is performed. Second, the centroids are extracted and used to initialize
a NAS algorithm. We tested our proposal using Aging Evolution, an evolutionary
algorithm, on NAS-bench-101. The results show that, compared to a random
initialization, a faster convergence and a better performance of the final
solution is achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Traore_K/0/1/0/all/0/1"&gt;Kalifou Rene Traore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Camero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SplitGuard: Detecting and Mitigating Training-Hijacking Attacks in Split Learning. (arXiv:2108.09052v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09052</id>
        <link href="http://arxiv.org/abs/2108.09052"/>
        <updated>2021-08-23T01:36:36.593Z</updated>
        <summary type="html"><![CDATA[Distributed deep learning frameworks, such as split learning, have recently
been proposed to enable a group of participants to collaboratively train a deep
neural network without sharing their raw data. Split learning in particular
achieves this goal by dividing a neural network between a client and a server
so that the client computes the initial set of layers, and the server computes
the rest. However, this method introduces a unique attack vector for a
malicious server attempting to steal the client's private data: the server can
direct the client model towards learning a task of its choice. With a concrete
example already proposed, such training-hijacking attacks present a significant
risk for the data privacy of split learning clients.

In this paper, we propose SplitGuard, a method by which a split learning
client can detect whether it is being targeted by a training-hijacking attack
or not. We experimentally evaluate its effectiveness, and discuss in detail
various points related to its use. We conclude that SplitGuard can effectively
detect training-hijacking attacks while minimizing the amount of information
recovered by the adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1"&gt;Ege Erdogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kupcu_A/0/1/0/all/0/1"&gt;Alptekin Kupcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_A/0/1/0/all/0/1"&gt;A. Ercument Cicek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedSkel: Efficient Federated Learning on Heterogeneous Systems with Skeleton Gradients Update. (arXiv:2108.09081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09081</id>
        <link href="http://arxiv.org/abs/2108.09081"/>
        <updated>2021-08-23T01:36:36.544Z</updated>
        <summary type="html"><![CDATA[Federated learning aims to protect users' privacy while performing data
analysis from different participants. However, it is challenging to guarantee
the training efficiency on heterogeneous systems due to the various
computational capabilities and communication bottlenecks. In this work, we
propose FedSkel to enable computation-efficient and communication-efficient
federated learning on edge devices by only updating the model's essential
parts, named skeleton networks. FedSkel is evaluated on real edge devices with
imbalanced datasets. Experimental results show that it could achieve up to
5.52$\times$ speedups for CONV layers' back-propagation, 1.82$\times$ speedups
for the whole training process, and reduce 64.8% communication cost, with
negligible accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Junyu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianlei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xucheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weisheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08910</id>
        <link href="http://arxiv.org/abs/2108.08910"/>
        <updated>2021-08-23T01:36:36.470Z</updated>
        <summary type="html"><![CDATA[Though recent years have witnessed remarkable progress in single image
super-resolution (SISR) tasks with the prosperous development of deep neural
networks (DNNs), the deep learning methods are confronted with the computation
and memory consumption issues in practice, especially for resource-limited
platforms such as mobile devices. To overcome the challenge and facilitate the
real-time deployment of SISR tasks on mobile, we combine neural architecture
search with pruning search and propose an automatic search framework that
derives sparse super-resolution (SR) models with high image quality while
satisfying the real-time inference requirement. To decrease the search cost, we
leverage the weight sharing strategy by introducing a supernet and decouple the
search problem into three stages, including supernet construction,
compiler-aware architecture and pruning search, and compiler-aware pruning
ratio search. With the proposed framework, we are the first to achieve
real-time SR inference (with only tens of milliseconds per frame) for
implementing 720p resolution with competitive image quality (in terms of PSNR
and SSIM) on mobile platforms (Samsung Galaxy S20).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhan_Z/0/1/0/all/0/1"&gt;Zheng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yushu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jayaweera_M/0/1/0/all/0/1"&gt;Malith Jayaweera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaeli_D/0/1/0/all/0/1"&gt;David Kaeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09105</id>
        <link href="http://arxiv.org/abs/2108.09105"/>
        <updated>2021-08-23T01:36:36.450Z</updated>
        <summary type="html"><![CDATA[Vision-and-language navigation (VLN) aims to enable embodied agents to
navigate in realistic environments using natural language instructions. Given
the scarcity of domain-specific training data and the high diversity of image
and language inputs, the generalization of VLN agents to unseen environments
remains challenging. Recent methods explore pretraining to improve
generalization, however, the use of generic image-caption datasets or existing
small-scale VLN environments is suboptimal and results in limited improvements.
In this work, we introduce BnB, a large-scale and diverse in-domain VLN
dataset. We first collect image-caption (IC) pairs from hundreds of thousands
of listings from online rental marketplaces. Using IC pairs we next propose
automatic strategies to generate millions of VLN path-instruction (PI) pairs.
We further propose a shuffling loss that improves the learning of temporal
order inside PI pairs. We use BnB pretrain our Airbert model that can be
adapted to discriminative and generative settings and show that it outperforms
state of the art for Room-to-Room (R2R) navigation and Remote Referring
Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining
significantly increases performance on a challenging few-shot VLN evaluation,
where we train the model only on VLN instructions from a few houses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1"&gt;Pierre-Louis Guhur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1"&gt;Makarand Tapaswi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1"&gt;Ivan Laptev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Learning. (arXiv:2108.08993v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08993</id>
        <link href="http://arxiv.org/abs/2108.08993"/>
        <updated>2021-08-23T01:36:36.444Z</updated>
        <summary type="html"><![CDATA[This monograph develops a comprehensive statistical learning framework that
is robust to (distributional) perturbations in the data using Distributionally
Robust Optimization (DRO) under the Wasserstein metric. Beginning with
fundamental properties of the Wasserstein metric and the DRO formulation, we
explore duality to arrive at tractable formulations and develop finite-sample,
as well as asymptotic, performance guarantees. We consider a series of learning
problems, including (i) distributionally robust linear regression; (ii)
distributionally robust regression with group structure in the predictors;
(iii) distributionally robust multi-output regression and multiclass
classification, (iv) optimal decision making that combines distributionally
robust regression with nearest-neighbor estimation; (v) distributionally robust
semi-supervised learning, and (vi) distributionally robust reinforcement
learning. A tractable DRO relaxation for each problem is being derived,
establishing a connection between robustness and regularization, and obtaining
bounds on the prediction and estimation errors of the solution. Beyond theory,
we include numerical experiments and case studies using synthetic and real
data. The real data experiments are all associated with various health
informatics problems, an application area which provided the initial impetus
for this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ruidi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paschalidis_I/0/1/0/all/0/1"&gt;Ioannis Ch. Paschalidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation. (arXiv:2105.06421v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06421</id>
        <link href="http://arxiv.org/abs/2105.06421"/>
        <updated>2021-08-23T01:36:36.437Z</updated>
        <summary type="html"><![CDATA[Over the past few years, best SSL methods, gradually moved from the pre-text
task learning to the Contrastive learning. But contrastive methods have some
drawbacks which could not be solved completely, such as performing poor on
fine-grained visual tasks compare to supervised learning methods. In this
study, at first, the impact of ImageNet pre-training on fine-grained Facial
Expression Recognition (FER) was tested. It could be seen from the results that
training from scratch is better than ImageNet fine-tuning at stronger
augmentation levels. After that, a framework was proposed for standard
Supervised Learning (SL), called Hybrid Multi-Task Learning (HMTL) which merged
Self-Supervised as auxiliary task to the SL training setting. Leveraging
Self-Supervised Learning (SSL) can gain additional information from input data
than labels which can help the main fine-grained SL task. It is been
investigated how this method could be used for FER by designing two customized
version of common pre-text techniques, Jigsaw puzzling and in-painting. The
state-of-the-art was reached on AffectNet via two types of HMTL, without
utilizing pre-training on additional datasets. Moreover, we showed the
difference between SS pre-training and HMTL to demonstrate superiority of
proposed method. Furthermore, the impact of proposed method was shown on two
other fine-grained facial tasks, Head Poses estimation and Gender Recognition,
which concluded to reduce in error rate by 11% and 1% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pourmirzaei_M/0/1/0/all/0/1"&gt;Mahdi Pourmirzaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montazer_G/0/1/0/all/0/1"&gt;Gholam Ali Montazer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaili_F/0/1/0/all/0/1"&gt;Farzaneh Esmaili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation. (arXiv:2106.16126v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16126</id>
        <link href="http://arxiv.org/abs/2106.16126"/>
        <updated>2021-08-23T01:36:36.430Z</updated>
        <summary type="html"><![CDATA[Facial expressions are the most common universal forms of body language. In
the past few years, automatic facial expression recognition (FER) has been an
active field of research. However, it is still a challenging task due to
different uncertainties and complications. Nevertheless, efficiency and
performance are yet essential aspects for building robust systems. We proposed
two models, EmoXNet which is an ensemble learning technique for learning
convoluted facial representations, and EmoXNetLite which is a distillation
technique that is useful for transferring the knowledge from our ensemble model
to an efficient deep neural network using label-smoothen soft labels for able
to effectively detect expressions in real-time. Both of the techniques
performed quite well, where the ensemble model (EmoXNet) helped to achieve
85.07% test accuracy on FER2013 with FER+ annotations and 86.25% test accuracy
on RAF-DB. Moreover, the distilled model (EmoXNetLite) showed 82.07% test
accuracy on FER2013 with FER+ annotations and 81.78% test accuracy on RAF-DB.
Results show that our models seem to generalize well on new data and are
learned to focus on relevant facial representations for expressions
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Momin_R/0/1/0/all/0/1"&gt;Rauf Momin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momin_A/0/1/0/all/0/1"&gt;Ali Shan Momin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1"&gt;Khalid Rasheed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1"&gt;Muhammad Saqib&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and Horizontal Data Partitioning. (arXiv:2108.08930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08930</id>
        <link href="http://arxiv.org/abs/2108.08930"/>
        <updated>2021-08-23T01:36:36.423Z</updated>
        <summary type="html"><![CDATA[We consider federated learning in tiered communication networks. Our network
model consists of a set of silos, each holding a vertical partition of the
data. Each silo contains a hub and a set of clients, with the silo's vertical
data shard partitioned horizontally across its clients. We propose Tiered
Decentralized Coordinate Descent (TDCD), a communication-efficient
decentralized training algorithm for such two-tiered networks. To reduce
communication overhead, the clients in each silo perform multiple local
gradient steps before sharing updates with their hub. Each hub adjusts its
coordinates by averaging its workers' updates, and then hubs exchange
intermediate updates with one another. We present a theoretical analysis of our
algorithm and show the dependence of the convergence rate on the number of
vertical partitions, the number of local updates, and the number of clients in
each hub. We further validate our approach empirically via simulation-based
experiments using a variety of datasets and objectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Anirban Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patterson_S/0/1/0/all/0/1"&gt;Stacy Patterson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized next-best action recommendation with multi-party interaction learning for automated decision-making. (arXiv:2108.08846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08846</id>
        <link href="http://arxiv.org/abs/2108.08846"/>
        <updated>2021-08-23T01:36:36.407Z</updated>
        <summary type="html"><![CDATA[Automated next-best action recommendation for each customer in a sequential,
dynamic and interactive context has been widely needed in natural, social and
business decision-making. Personalized next-best action recommendation must
involve past, current and future customer demographics and circumstances
(states) and behaviors, long-range sequential interactions between customers
and decision-makers, multi-sequence interactions between states, behaviors and
actions, and their reactions to their counterpart's actions. No existing
modeling theories and tools, including Markovian decision processes, user and
behavior modeling, deep sequential modeling, and personalized sequential
recommendation, can quantify such complex decision-making on a personal level.
We take a data-driven approach to learn the next-best actions for personalized
decision-making by a reinforced coupled recurrent neural network (CRN). CRN
represents multiple coupled dynamic sequences of a customer's historical and
current states, responses to decision-makers' actions, decision rewards to
actions, and learns long-term multi-sequence interactions between parties
(customer and decision-maker). Next-best actions are then recommended on each
customer at a time point to change their state for an optimal decision-making
objective. Our study demonstrates the potential of personalized deep learning
of multi-sequence interactions and automated dynamic intervention for
personalized decision-making in complex systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chengzhang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08988</id>
        <link href="http://arxiv.org/abs/2108.08988"/>
        <updated>2021-08-23T01:36:36.400Z</updated>
        <summary type="html"><![CDATA[Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tunazzina Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1"&gt;Dan Goldwasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data. (arXiv:2108.09020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09020</id>
        <link href="http://arxiv.org/abs/2108.09020"/>
        <updated>2021-08-23T01:36:36.393Z</updated>
        <summary type="html"><![CDATA[Continual learning is the problem of learning and retaining knowledge through
time over multiple tasks and environments. Research has primarily focused on
the incremental classification setting, where new tasks/classes are added at
discrete time intervals. Such an "offline" setting does not evaluate the
ability of agents to learn effectively and efficiently, since an agent can
perform multiple learning epochs without any time limitation when a task is
added. We argue that "online" continual learning, where data is a single
continuous stream without task boundaries, enables evaluating both information
retention and online learning efficacy. In online continual learning, each
incoming small batch of data is first used for testing and then added to the
training set, making the problem truly online. Trained models are later
evaluated on historical data to assess information retention. We introduce a
new benchmark for online continual visual learning that exhibits large scale
and natural distribution shifts. Through a large-scale analysis, we identify
critical and previously unobserved phenomena of gradient-based optimization in
continual learning, and propose effective strategies for improving
gradient-based online continual learning with real data. The source code and
dataset are available in: https://github.com/IntelLabs/continuallearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhipeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sener_O/0/1/0/all/0/1"&gt;Ozan Sener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1"&gt;Vladlen Koltun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Domain Adaptation for Single Stage Detectors. (arXiv:2106.07283v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07283</id>
        <link href="http://arxiv.org/abs/2106.07283"/>
        <updated>2021-08-23T01:36:36.387Z</updated>
        <summary type="html"><![CDATA[While domain adaptation has been used to improve the performance of object
detectors when the training and test data follow different distributions,
previous work has mostly focused on two-stage detectors. This is because their
use of region proposals makes it possible to perform local adaptation, which
has been shown to significantly improve the adaptation effectiveness. Here, by
contrast, we target single-stage architectures, which are better suited to
resource-constrained detection than two-stage ones but do not provide region
proposals. To nonetheless benefit from the strength of local adaptation, we
introduce an attention mechanism that lets us identify the important regions on
which adaptation should focus. Our method gradually adapts the features from
global, image-level to local, instance-level. Our approach is generic and can
be integrated into any single-stage detector. We demonstrate this on standard
benchmark datasets by applying it to both SSD and YOLOv5. Furthermore, for
equivalent single-stage architectures, our method outperforms the
state-of-the-art domain adaptation techniques even though they were designed
for specific detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vidit_V/0/1/0/all/0/1"&gt;Vidit Vidit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Relationship Detection Using Part-and-Sum Transformers with Composite Queries. (arXiv:2105.02170v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02170</id>
        <link href="http://arxiv.org/abs/2105.02170"/>
        <updated>2021-08-23T01:36:36.380Z</updated>
        <summary type="html"><![CDATA[Computer vision applications such as visual relationship detection and human
object interaction can be formulated as a composite (structured) set detection
problem in which both the parts (subject, object, and predicate) and the sum
(triplet as a whole) are to be detected in a hierarchical fashion. In this
paper, we present a new approach, denoted Part-and-Sum detection Transformer
(PST), to perform end-to-end visual composite set detection. Different from
existing Transformers in which queries are at a single level, we simultaneously
model the joint part and sum hypotheses/interactions with composite queries and
attention modules. We explicitly incorporate sum queries to enable better
modeling of the part-and-sum relations that are absent in the standard
Transformers. Our approach also uses novel tensor-based part queries and
vector-based sum queries, and models their joint interaction. We report
experiments on two vision tasks, visual relationship detection and human object
interaction and demonstrate that PST achieves state of the art results among
single-stage models, while nearly matching the results of custom designed
two-stage models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1"&gt;Qi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Haofu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1"&gt;Vijay Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CloudShield: Real-time Anomaly Detection in the Cloud. (arXiv:2108.08977v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08977</id>
        <link href="http://arxiv.org/abs/2108.08977"/>
        <updated>2021-08-23T01:36:36.361Z</updated>
        <summary type="html"><![CDATA[In cloud computing, it is desirable if suspicious activities can be detected
by automatic anomaly detection systems. Although anomaly detection has been
investigated in the past, it remains unsolved in cloud computing. Challenges
are: characterizing the normal behavior of a cloud server, distinguishing
between benign and malicious anomalies (attacks), and preventing alert fatigue
due to false alarms.

We propose CloudShield, a practical and generalizable real-time anomaly and
attack detection system for cloud computing. Cloudshield uses a general,
pretrained deep learning model with different cloud workloads, to predict the
normal behavior and provide real-time and continuous detection by examining the
model reconstruction error distributions. Once an anomaly is detected, to
reduce alert fatigue, CloudShield automatically distinguishes between benign
programs, known attacks, and zero-day attacks, by examining the prediction
error distributions. We evaluate the proposed CloudShield on representative
cloud benchmarks. Our evaluation shows that CloudShield, using model
pretraining, can apply to a wide scope of cloud workloads. Especially, we
observe that CloudShield can detect the recently proposed speculative execution
attacks, e.g., Spectre and Meltdown attacks, in milliseconds. Furthermore, we
show that CloudShield accurately differentiates and prioritizes known attacks,
and potential zero-day attacks, from benign programs. Thus, it significantly
reduces false alarms by up to 99.0%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zecheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1"&gt;Ruby B. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Bounds and Calibration for a Smart Predict-then-Optimize Method. (arXiv:2108.08887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08887</id>
        <link href="http://arxiv.org/abs/2108.08887"/>
        <updated>2021-08-23T01:36:36.355Z</updated>
        <summary type="html"><![CDATA[The predict-then-optimize framework is fundamental in practical stochastic
decision-making problems: first predict unknown parameters of an optimization
model, then solve the problem using the predicted values. A natural loss
function in this setting is defined by measuring the decision error induced by
the predicted parameters, which was named the Smart Predict-then-Optimize (SPO)
loss by Elmachtoub and Grigas [arXiv:1710.08005]. Since the SPO loss is
typically nonconvex and possibly discontinuous, Elmachtoub and Grigas
[arXiv:1710.08005] introduced a convex surrogate, called the SPO+ loss, that
importantly accounts for the underlying structure of the optimization model. In
this paper, we greatly expand upon the consistency results for the SPO+ loss
provided by Elmachtoub and Grigas [arXiv:1710.08005]. We develop risk bounds
and uniform calibration results for the SPO+ loss relative to the SPO loss,
which provide a quantitative way to transfer the excess surrogate risk to
excess true risk. By combining our risk bounds with generalization bounds, we
show that the empirical minimizer of the SPO+ loss achieves low excess true
risk with high probability. We first demonstrate these results in the case when
the feasible region of the underlying optimization problem is a polyhedron, and
then we show that the results can be strengthened substantially when the
feasible region is a level set of a strongly convex function. We perform
experiments to empirically demonstrate the strength of the SPO+ surrogate, as
compared to standard $\ell_1$ and squared $\ell_2$ prediction error losses, on
portfolio allocation and cost-sensitive multi-class classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Heyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigas_P/0/1/0/all/0/1"&gt;Paul Grigas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators. (arXiv:2108.08891v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08891</id>
        <link href="http://arxiv.org/abs/2108.08891"/>
        <updated>2021-08-23T01:36:36.349Z</updated>
        <summary type="html"><![CDATA[We study how stochastic differential equation (SDE) based ideas can inspire
new modifications to existing algorithms for a set of problems in computer
vision. Loosely speaking, our formulation is related to both explicit and
implicit strategies for data augmentation and group equivariance, but is
derived from new results in the SDE literature on estimating infinitesimal
generators of a class of stochastic processes. If and when there is nominal
agreement between the needs of an application/task and the inherent properties
and behavior of the types of processes that we can efficiently handle, we
obtain a very simple and efficient plug-in layer that can be incorporated
within any existing network architecture, with minimal modification and only a
few additional parameters. We show promising experiments on a number of vision
tasks including few shot learning, point cloud transformers and deep
variational segmentation obtaining efficiency or performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zihang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vikas Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Sathya N. Ravi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised learning for medical image classification using imbalanced training data. (arXiv:2108.08956v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08956</id>
        <link href="http://arxiv.org/abs/2108.08956"/>
        <updated>2021-08-23T01:36:36.343Z</updated>
        <summary type="html"><![CDATA[Medical image classification is often challenging for two reasons: a lack of
labelled examples due to expensive and time-consuming annotation protocols, and
imbalanced class labels due to the relative scarcity of disease-positive
individuals in the wider population. Semi-supervised learning (SSL) methods
exist for dealing with a lack of labels, but they generally do not address the
problem of class imbalance. In this study we propose Adaptive Blended
Consistency Loss (ABCL), a drop-in replacement for consistency loss in
perturbation-based SSL methods. ABCL counteracts data skew by adaptively mixing
the target class distribution of the consistency loss in accordance with class
frequency. Our experiments with ABCL reveal improvements to unweighted average
recall on two different imbalanced medical image classification datasets when
compared with existing consistency losses that are not designed to counteract
class imbalance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Tri Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1"&gt;Aiden Nibali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIAM: Chiplet-based Scalable In-Memory Acceleration with Mesh for Deep Neural Networks. (arXiv:2108.08903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08903</id>
        <link href="http://arxiv.org/abs/2108.08903"/>
        <updated>2021-08-23T01:36:36.337Z</updated>
        <summary type="html"><![CDATA[In-memory computing (IMC) on a monolithic chip for deep learning faces
dramatic challenges on area, yield, and on-chip interconnection cost due to the
ever-increasing model sizes. 2.5D integration or chiplet-based architectures
interconnect multiple small chips (i.e., chiplets) to form a large computing
system, presenting a feasible solution beyond a monolithic IMC architecture to
accelerate large deep learning models. This paper presents a new benchmarking
simulator, SIAM, to evaluate the performance of chiplet-based IMC architectures
and explore the potential of such a paradigm shift in IMC architecture design.
SIAM integrates device, circuit, architecture, network-on-chip (NoC),
network-on-package (NoP), and DRAM access models to realize an end-to-end
system. SIAM is scalable in its support of a wide range of deep neural networks
(DNNs), customizable to various network structures and configurations, and
capable of efficient design space exploration. We demonstrate the flexibility,
scalability, and simulation speed of SIAM by benchmarking different
state-of-the-art DNNs with CIFAR-10, CIFAR-100, and ImageNet datasets. We
further calibrate the simulation results with a published silicon result,
SIMBA. The chiplet-based IMC architecture obtained through SIAM shows
130$\times$ and 72$\times$ improvement in energy-efficiency for ResNet-50 on
the ImageNet dataset compared to Nvidia V100 and T4 GPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1"&gt;Gokul Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1"&gt;Sumit K. Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pannala_M/0/1/0/all/0/1"&gt;Manvitha Pannala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_C/0/1/0/all/0/1"&gt;Chaitali Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Jae-sun Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1"&gt;Umit Y. Ogras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yu Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure Learning for Directed Trees. (arXiv:2108.08871v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08871</id>
        <link href="http://arxiv.org/abs/2108.08871"/>
        <updated>2021-08-23T01:36:36.330Z</updated>
        <summary type="html"><![CDATA[Knowing the causal structure of a system is of fundamental interest in many
areas of science and can aid the design of prediction algorithms that work well
under manipulations to the system. The causal structure becomes identifiable
from the observational distribution under certain restrictions. To learn the
structure from data, score-based methods evaluate different graphs according to
the quality of their fits. However, for large nonlinear models, these rely on
heuristic optimization approaches with no general guarantees of recovering the
true causal structure. In this paper, we consider structure learning of
directed trees. We propose a fast and scalable method based on Chu-Liu-Edmonds'
algorithm we call causal additive trees (CAT). For the case of Gaussian errors,
we prove consistency in an asymptotic regime with a vanishing identifiability
gap. We also introduce a method for testing substructure hypotheses with
asymptotic family-wise error rate control that is valid post-selection and in
unidentified settings. Furthermore, we study the identifiability gap, which
quantifies how much better the true causal model fits the observational
distribution, and prove that it is lower bounded by local properties of the
causal model. Simulation studies demonstrate the favorable performance of CAT
compared to competing structure learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jakobsen_M/0/1/0/all/0/1"&gt;Martin Emil Jakobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rajen D. Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1"&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jonas Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection. (arXiv:2106.01178v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01178</id>
        <link href="http://arxiv.org/abs/2106.01178"/>
        <updated>2021-08-23T01:36:36.312Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the task of multi-view RGB-based 3D object
detection as an end-to-end optimization problem. To address this problem, we
propose ImVoxelNet, a novel fully convolutional method of 3D object detection
based on monocular or multi-view RGB images. The number of monocular images in
each multi-view input can variate during training and inference; actually, this
number might be unique for each multi-view input. ImVoxelNet successfully
handles both indoor and outdoor scenes, which makes it general-purpose.
Specifically, it achieves state-of-the-art results in car detection on KITTI
(monocular) and nuScenes (multi-view) benchmarks among all methods that accept
RGB images. Moreover, it surpasses existing RGB-based 3D object detection
methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark
for multi-view 3D object detection. The source code and the trained models are
available at https://github.com/saic-vul/imvoxelnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rukhovich_D/0/1/0/all/0/1"&gt;Danila Rukhovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1"&gt;Anna Vorontsova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1"&gt;Anton Konushin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Deep Reinforcement Learning Using Introspection in a Non-episodic Task. (arXiv:2108.08911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08911</id>
        <link href="http://arxiv.org/abs/2108.08911"/>
        <updated>2021-08-23T01:36:36.306Z</updated>
        <summary type="html"><![CDATA[Explainable reinforcement learning allows artificial agents to explain their
behavior in a human-like manner aiming at non-expert end-users. An efficient
alternative of creating explanations is to use an introspection-based method
that transforms Q-values into probabilities of success used as the base to
explain the agent's decision-making process. This approach has been effectively
used in episodic and discrete scenarios, however, to compute the probability of
success in non-episodic and more complex environments has not been addressed
yet. In this work, we adapt the introspection method to be used in a
non-episodic task and try it in a continuous Atari game scenario solved with
the Rainbow algorithm. Our initial results show that the probability of success
can be computed directly from the Q-values for all possible actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayala_A/0/1/0/all/0/1"&gt;Angel Ayala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_F/0/1/0/all/0/1"&gt;Francisco Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_B/0/1/0/all/0/1"&gt;Bruno Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dazeley_R/0/1/0/all/0/1"&gt;Richard Dazeley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Neural Topic Modeling of Text Corpora. (arXiv:2108.08946v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08946</id>
        <link href="http://arxiv.org/abs/2108.08946"/>
        <updated>2021-08-23T01:36:36.300Z</updated>
        <summary type="html"><![CDATA[Topic Modeling refers to the problem of discovering the main topics that have
occurred in corpora of textual data, with solutions finding crucial
applications in numerous fields. In this work, inspired by the recent
advancements in the Natural Language Processing domain, we introduce FAME, an
open-source framework enabling an efficient mechanism of extracting and
incorporating textual features and utilizing them in discovering topics and
clustering text documents that are semantically similar in a corpus. These
features range from traditional approaches (e.g., frequency-based) to the most
recent auto-encoding embeddings from transformer-based language models such as
BERT model family. To demonstrate the effectiveness of this library, we
conducted experiments on the well-known News-Group dataset. The library is
available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1"&gt;Shayan Fazeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1"&gt;Majid Sarrafzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v3 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02297</id>
        <link href="http://arxiv.org/abs/2108.02297"/>
        <updated>2021-08-23T01:36:36.294Z</updated>
        <summary type="html"><![CDATA[Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time-sequential data such as speech recognition. However, it is
difficult to deploy these networks on hardware to achieve high throughput and
low latency because the fully connected structure makes LSTM networks a
memory-bounded algorithm. Previous LSTM accelerators either exploited weight
spatial sparsity or temporal activation sparsity. This paper proposes a new
accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve
ultra-low latency inference. The spatial sparsity is induced using our proposed
pruning method called Column-Balanced Targeted Dropout (CBTD), which structures
sparse weight matrices for balanced workload. It achieved up to 96% weight
sparsity with negligible accuracy difference for an LSTM network trained on a
TIMIT phone recognition task. To induce temporal sparsity in LSTM, we create
the DeltaLSTM by extending the previous DeltaGRU method to the LSTM network.
This combined sparsity simultaneously saves on the weight memory access and
associated arithmetic operations. Spartus was implemented on a Xilinx Zynq-7100
FPGA. The Spartus per-sample latency for a single DeltaLSTM layer of 1024
neurons averages 1 us. Spartus achieved 9.4 TOp/s effective batch-1 throughput
and 1.1 TOp/J energy efficiency, which, respectively, are 4X and 7X higher than
the previous state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1"&gt;Tobi Delbruck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shih-Chii Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plug and Play, Model-Based Reinforcement Learning. (arXiv:2108.08960v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08960</id>
        <link href="http://arxiv.org/abs/2108.08960"/>
        <updated>2021-08-23T01:36:36.287Z</updated>
        <summary type="html"><![CDATA[Sample-efficient generalisation of reinforcement learning approaches have
always been a challenge, especially, for complex scenes with many components.
In this work, we introduce Plug and Play Markov Decision Processes, an
object-based representation that allows zero-shot integration of new objects
from known object classes. This is achieved by representing the global
transition dynamics as a union of local transition functions, each with respect
to one active object in the scene. Transition dynamics from an object class can
be pre-learnt and thus would be ready to use in a new environment. Each active
object is also endowed with its reward function. Since there is no central
reward function, addition or removal of objects can be handled efficiently by
only updating the reward functions of objects involved. A new transfer learning
mechanism is also proposed to adapt reward function in such cases. Experiments
show that our representation can achieve sample-efficiency in a variety of
set-ups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdolshah_M/0/1/0/all/0/1"&gt;Majid Abdolshah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1"&gt;Thommen Karimpanal George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1"&gt;Santu Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASAT: Adaptively Scaled Adversarial Training in Time Series. (arXiv:2108.08976v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08976</id>
        <link href="http://arxiv.org/abs/2108.08976"/>
        <updated>2021-08-23T01:36:36.269Z</updated>
        <summary type="html"><![CDATA[Adversarial training is a method for enhancing neural networks to improve the
robustness against adversarial examples. Besides the security concerns of
potential adversarial examples, adversarial training can also improve the
performance of the neural networks, train robust neural networks, and provide
interpretability for neural networks. In this work, we take the first step to
introduce adversarial training in time series analysis by taking the finance
field as an example. Rethinking existing researches of adversarial training, we
propose the adaptively scaled adversarial training (ASAT) in time series
analysis, by treating data at different time slots with time-dependent
importance weights. Experimental results show that the proposed ASAT can
improve both the accuracy and the adversarial robustness of neural networks.
Besides enhancing neural networks, we also propose the dimension-wise
adversarial sensitivity indicator to probe the sensitivities and importance of
input dimensions. With the proposed indicator, we can explain the decision
bases of black box neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1"&gt;Ruihan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harimoto_K/0/1/0/all/0/1"&gt;Keiko Harimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Latin Hypercube Refinement for Multi-objective Design Uncertainty Optimization. (arXiv:2108.08890v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08890</id>
        <link href="http://arxiv.org/abs/2108.08890"/>
        <updated>2021-08-23T01:36:36.258Z</updated>
        <summary type="html"><![CDATA[Optimizing the reliability and the robustness of a design is important but
often unaffordable due to high sample requirements. Surrogate models based on
statistical and machine learning methods are used to increase the sample
efficiency. However, for higher dimensional or multi-modal systems, surrogate
models may also require a large amount of samples to achieve good results. We
propose a sequential sampling strategy for the surrogate based solution of
multi-objective reliability based robust design optimization problems. Proposed
local Latin hypercube refinement (LoLHR) strategy is model-agnostic and can be
combined with any surrogate model because there is no free lunch but possibly a
budget one. The proposed method is compared to stationary sampling as well as
other proposed strategies from the literature. Gaussian process and support
vector regression are both used as surrogate models. Empirical evidence is
presented, showing that LoLHR achieves on average better results compared to
other surrogate based strategies on the tested examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bogoclu_C/0/1/0/all/0/1"&gt;Can Bogoclu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roos_D/0/1/0/all/0/1"&gt;Dirk Roos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nestorovic_T/0/1/0/all/0/1"&gt;Tamara Nestorovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few Shot Activity Recognition Using Variational Inference. (arXiv:2108.08990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08990</id>
        <link href="http://arxiv.org/abs/2108.08990"/>
        <updated>2021-08-23T01:36:36.251Z</updated>
        <summary type="html"><![CDATA[There has been a remarkable progress in learning a model which could
recognise novel classes with only a few labeled examples in the last few years.
Few-shot learning (FSL) for action recognition is a challenging task of
recognising novel action categories which are represented by few instances in
the training data. We propose a novel variational inference based architectural
framework (HF-AR) for few shot activity recognition. Our framework leverages
volume-preserving Householder Flow to learn a flexible posterior distribution
of the novel classes. This results in better performance as compared to
state-of-the-art few shot approaches for human activity recognition. approach
consists of base model and an adapter model. Our architecture consists of a
base model and an adapter model. The base model is trained on seen classes and
it computes an embedding that represent the spatial and temporal insights
extracted from the input video, e.g. combination of Resnet-152 and LSTM based
encoder-decoder model. The adapter model applies a series of Householder
transformations to compute a flexible posterior distribution that lends higher
accuracy in the few shot approach. Extensive experiments on three well-known
datasets: UCF101, HMDB51 and Something-Something-V2, demonstrate similar or
better performance on 1-shot and 5-shot classification as compared to
state-of-the-art few shot approaches that use only RGB frame sequence as input.
To the best of our knowledge, we are the first to explore variational inference
along with householder transformations to capture the full rank covariance
matrix of posterior distribution, for few shot learning in activity
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1"&gt;Siddhansh Narang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of Adversarial Examples to Physical ECG Signals. (arXiv:2108.08972v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08972</id>
        <link href="http://arxiv.org/abs/2108.08972"/>
        <updated>2021-08-23T01:36:36.245Z</updated>
        <summary type="html"><![CDATA[This work aims to assess the reality and feasibility of the adversarial
attack against cardiac diagnosis system powered by machine learning algorithms.
To this end, we introduce adversarial beats, which are adversarial
perturbations tailored specifically against electrocardiograms (ECGs)
beat-by-beat classification system. We first formulate an algorithm to generate
adversarial examples for the ECG classification neural network model, and study
its attack success rate. Next, to evaluate its feasibility in a physical
environment, we mount a hardware attack by designing a malicious signal
generator which injects adversarial beats into ECG sensor readings. To the best
of our knowledge, our work is the first in evaluating the proficiency of
adversarial examples for ECGs in a physical setup. Our real-world experiments
demonstrate that adversarial beats successfully manipulated the diagnosis
results 3-5 times out of 40 attempts throughout the course of 2 minutes.
Finally, we discuss the overall feasibility and impact of the attack, by
clearly defining motives and constraints of expected attackers along with our
experimental results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ono_T/0/1/0/all/0/1"&gt;Taiga Ono&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_T/0/1/0/all/0/1"&gt;Takeshi Sugawara&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Sakuma_J/0/1/0/all/0/1"&gt;Jun Sakuma&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1"&gt;Tatsuya Mori&lt;/a&gt; (1 and 4) ((1) Waseda University, (2) The University of Electro-Communications, (3) University of Tsukuba, (4) RIKEN AIP)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing concepts of quantum and classical neural network models for image classification task. (arXiv:2108.08875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08875</id>
        <link href="http://arxiv.org/abs/2108.08875"/>
        <updated>2021-08-23T01:36:36.221Z</updated>
        <summary type="html"><![CDATA[While quantum architectures are still under development, when available, they
will only be able to process quantum data when machine learning algorithms can
only process numerical data. Therefore, in the issues of classification or
regression, it is necessary to simulate and study quantum systems that will
transfer the numerical input data to a quantum form and enable quantum
computers to use the available methods of machine learning. This material
includes the results of experiments on training and performance of a hybrid
quantum-classical neural network developed for the problem of classification of
handwritten digits from the MNIST data set. The comparative results of two
models: classical and quantum neural networks of a similar number of training
parameters, indicate that the quantum network, although its simulation is
time-consuming, overcomes the classical network (it has better convergence and
achieves higher training and testing accuracy).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Porebski_S/0/1/0/all/0/1"&gt;Sebastian Porebski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potempa_R/0/1/0/all/0/1"&gt;Rafal Potempa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some thoughts on catastrophic forgetting and how to learn an algorithm. (arXiv:2108.03940v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03940</id>
        <link href="http://arxiv.org/abs/2108.03940"/>
        <updated>2021-08-23T01:36:36.122Z</updated>
        <summary type="html"><![CDATA[The work of McCloskey and Cohen popularized the concept of catastrophic
interference. They used a neural network that tried to learn addition using two
groups of examples as two different tasks. In their case, learning the second
task rapidly deteriorated the acquired knowledge about the previous one. This
could be a symptom of a fundamental problem: addition is an algorithmic task
that should not be learned through pattern recognition. We propose to use a
neural network with a different architecture that can be trained to recover the
correct algorithm for the addition of binary numbers. We test it in the setting
proposed by McCloskey and Cohen and training on random examples one by one. The
neural network not only does not suffer from catastrophic forgetting but it
improves its predictive power on unseen pairs of numbers as training
progresses. This work emphasizes the importance that neural network
architecture has for the emergence of catastrophic forgetting and introduces a
neural network that is able to learn an algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1"&gt;Miguel Ruiz-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v4 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04679</id>
        <link href="http://arxiv.org/abs/2106.04679"/>
        <updated>2021-08-23T01:36:36.104Z</updated>
        <summary type="html"><![CDATA[Distributed artificial intelligence (DAI) studies artificial intelligence
entities working together to reason, plan, solve problems, organize behaviors
and strategies, make collective decisions and learn. This Ph.D. research
proposes a principled Multi-Agent Systems (MAS) cooperation framework --
Self-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap
between perception, communication, planning, execution, decision-making, and
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs. (arXiv:2106.13199v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13199</id>
        <link href="http://arxiv.org/abs/2106.13199"/>
        <updated>2021-08-23T01:36:36.048Z</updated>
        <summary type="html"><![CDATA[Sharing data from clinical studies can facilitate innovative data-driven
research and ultimately lead to better public health. However, sharing
biomedical data can put sensitive personal information at risk. This is usually
solved by anonymization, which is a slow and expensive process. An alternative
to anonymization is sharing a synthetic dataset that bears a behaviour similar
to the real data but preserves privacy. As part of the collaboration between
Novartis and the Oxford Big Data Institute, we generate a synthetic dataset
based on COSENTYX (secukinumab) Ankylosing Spondylitis clinical study. We apply
an Auxiliary Classifier GAN to generate synthetic MRIs of vertebral units. The
images are conditioned on the VU location (cervical, thoracic and lumbar). In
this paper, we present a method for generating a synthetic dataset and conduct
an in-depth analysis on its properties along three key metrics: image fidelity,
sample diversity and dataset privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hanxi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plawinski_J/0/1/0/all/0/1"&gt;Jason Plawinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1"&gt;Sajanth Subramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Readie_A/0/1/0/all/0/1"&gt;Aimee Readie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ligozio_G/0/1/0/all/0/1"&gt;Gregory Ligozio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohlssen_D/0/1/0/all/0/1"&gt;David Ohlssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baillie_M/0/1/0/all/0/1"&gt;Mark Baillie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coroller_T/0/1/0/all/0/1"&gt;Thibaud Coroller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08369</id>
        <link href="http://arxiv.org/abs/2107.08369"/>
        <updated>2021-08-23T01:36:36.041Z</updated>
        <summary type="html"><![CDATA[Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. Accurate and
robust flood detection including delineating open water flood areas and
identifying flood levels can aid in disaster response and mitigation. However,
estimating flood levels remotely is of essence as physical access to flooded
areas is limited and the ability to deploy instruments in potential flood zones
can be dangerous. Aligning flood extent mapping with local topography can
provide a plan-of-action that the disaster response team can consider. Thus,
remote flood level estimation via satellites like Sentinel-1 can prove to be
remedial. The Emerging Techniques in Computational Intelligence (ETCI)
competition on Flood Detection tasked participants with predicting flooded
pixels after training with synthetic aperture radar (SAR) images in a
supervised setting. We use a cyclical approach involving two stages (1)
training an ensemble model of multiple UNet architectures with available high
and low confidence labeled data and, generating pseudo labels or low confidence
labels on the entire unlabeled test dataset, and then, (2) filter out quality
generated labels and, (3) combining the generated labels with the previously
available high confidence labeled dataset. This assimilated dataset is used for
the next round of training ensemble models. This cyclical process is repeated
until the performance improvement plateaus. Additionally, we post process our
results with Conditional Random Fields. Our approach sets the second highest
score on the public hold-out test leaderboard for the ETCI competition with
0.7654 IoU. To the best of our knowledge we believe this is one of the first
works to try out semi-supervised learning to improve flood segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sayak Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantization Backdoors to Deep Learning Models. (arXiv:2108.09187v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.09187</id>
        <link href="http://arxiv.org/abs/2108.09187"/>
        <updated>2021-08-23T01:36:36.034Z</updated>
        <summary type="html"><![CDATA[There is currently a burgeoning demand for deploying deep learning (DL)
models on ubiquitous edge Internet of Things devices attributing to their low
latency and high privacy preservation. However, DL models are often large in
size and require large-scale computation, which prevents them from being placed
directly onto IoT devices where resources are constrained and 32-bit
floating-point operations are unavailable. Model quantization is a pragmatic
solution, which enables DL deployment on mobile devices and embedded systems by
effortlessly post-quantizing a large high-precision model into a small
low-precision model while retaining the model inference accuracy.

This work reveals that the standard quantization operation can be abused to
activate a backdoor. We demonstrate that a full-precision backdoored model that
does not have any backdoor effect in the presence of a trigger -- as the
backdoor is dormant -- can be activated by the default TensorFlow-Lite
quantization, the only product-ready quantization framework to date. We
ascertain that all trained float-32 backdoored models exhibit no backdoor
effect even in the presence of trigger inputs. State-of-the-art frontend
detection approaches, such as Neural Cleanse and STRIP, fail to identify the
backdoor in the float-32 models. When each of the float-32 models is converted
into an int-8 format model through the standard TFLite post-training
quantization, the backdoor is activated in the quantized model, which shows a
stable attack success rate close to 100% upon inputs with the trigger, while
behaves normally upon non-trigger inputs. This work highlights that a stealthy
security threat occurs when end users utilize the on-device post-training model
quantization toolkits, informing security researchers of cross-platform
overhaul of DL models post quantization even if they pass frontend inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hua Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Huming Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yansong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1"&gt;Alsharif Abuadbba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1"&gt;Anmin Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Sarawi_S/0/1/0/all/0/1"&gt;Said Al-Sarawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbott_D/0/1/0/all/0/1"&gt;Derek Abbott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parsing Birdsong with Deep Audio Embeddings. (arXiv:2108.09203v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09203</id>
        <link href="http://arxiv.org/abs/2108.09203"/>
        <updated>2021-08-23T01:36:36.016Z</updated>
        <summary type="html"><![CDATA[Monitoring of bird populations has played a vital role in conservation
efforts and in understanding biodiversity loss. The automation of this process
has been facilitated by both sensing technologies, such as passive acoustic
monitoring, and accompanying analytical tools, such as deep learning. However,
machine learning models frequently have difficulty generalizing to examples not
encountered in the training data. In our work, we present a semi-supervised
approach to identify characteristic calls and environmental noise. We utilize
several methods to learn a latent representation of audio samples, including a
convolutional autoencoder and two pre-trained networks, and group the resulting
embeddings for a domain expert to identify cluster labels. We show that our
approach can improve classification precision and provide insight into the
latent structure of environmental acoustic datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolkova_I/0/1/0/all/0/1"&gt;Irina Tolkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_B/0/1/0/all/0/1"&gt;Brian Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedman_M/0/1/0/all/0/1"&gt;Marcel Hedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahl_S/0/1/0/all/0/1"&gt;Stefan Kahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinck_H/0/1/0/all/0/1"&gt;Holger Klinck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Illicit Drug Trafficking Events on Instagram: A Deep Multimodal Multilabel Learning Approach. (arXiv:2108.08920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08920</id>
        <link href="http://arxiv.org/abs/2108.08920"/>
        <updated>2021-08-23T01:36:36.010Z</updated>
        <summary type="html"><![CDATA[Social media such as Instagram and Twitter have become important platforms
for marketing and selling illicit drugs. Detection of online illicit drug
trafficking has become critical to combat the online trade of illicit drugs.
However, the legal status often varies spatially and temporally; even for the
same drug, federal and state legislation can have different regulations about
its legality. Meanwhile, more drug trafficking events are disguised as a novel
form of advertising commenting leading to information heterogeneity.
Accordingly, accurate detection of illicit drug trafficking events (IDTEs) from
social media has become even more challenging. In this work, we conduct the
first systematic study on fine-grained detection of IDTEs on Instagram. We
propose to take a deep multimodal multilabel learning (DMML) approach to detect
IDTEs and demonstrate its effectiveness on a newly constructed dataset called
multimodal IDTE(MM-IDTE). Specifically, our model takes text and image data as
the input and combines multimodal information to predict multiple labels of
illicit drugs. Inspired by the success of BERT, we have developed a
self-supervised multimodal bidirectional transformer by jointly fine-tuning
pretrained text and image encoders. We have constructed a large-scale dataset
MM-IDTE with manually annotated multiple drug labels to support fine-grained
detection of illicit drugs. Extensive experimental results on the MM-IDTE
dataset show that the proposed DMML methodology can accurately detect IDTEs
even in the presence of special characters and style changes attempting to
evade detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuanbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Minglei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yanfang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World. (arXiv:2107.03172v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03172</id>
        <link href="http://arxiv.org/abs/2107.03172"/>
        <updated>2021-08-23T01:36:36.003Z</updated>
        <summary type="html"><![CDATA[Common fully glazed facades and transparent objects present architectural
barriers and impede the mobility of people with low vision or blindness, for
instance, a path detected behind a glass door is inaccessible unless it is
correctly perceived and reacted. However, segmenting these safety-critical
objects is rarely covered by conventional assistive technologies. To tackle
this issue, we construct a wearable system with a novel dual-head Transformer
for Transparency (Trans4Trans) model, which is capable of segmenting general
and transparent objects and performing real-time wayfinding to assist people
walking alone more safely. Especially, both decoders created by our proposed
Transformer Parsing Module (TPM) enable effective joint learning from different
datasets. Besides, the efficient Trans4Trans model composed of symmetric
transformer-based encoder and decoder, requires little computational expenses
and is readily deployed on portable GPUs. Our Trans4Trans model outperforms
state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2
datasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various
pre-tests and a user study conducted in indoor and outdoor scenarios, the
usability and reliability of our assistive system have been extensively
verified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1"&gt;Angela Constantinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1"&gt;Kunyu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Karin M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1"&gt;Rainer Stiefelhagen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composable Augmentation Encoding for Video Representation Learning. (arXiv:2104.00616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00616</id>
        <link href="http://arxiv.org/abs/2104.00616"/>
        <updated>2021-08-23T01:36:35.997Z</updated>
        <summary type="html"><![CDATA[We focus on contrastive methods for self-supervised video representation
learning. A common paradigm in contrastive learning is to construct positive
pairs by sampling different data views for the same instance, with different
data instances as negatives. These methods implicitly assume a set of
representational invariances to the view selection mechanism (eg, sampling
frames with temporal shifts), which may lead to poor performance on downstream
tasks which violate these invariances (fine-grained video action recognition
that would benefit from temporal information). To overcome this limitation, we
propose an 'augmentation aware' contrastive learning framework, where we
explicitly provide a sequence of augmentation parameterisations (such as the
values of the time shifts used to create data views) as composable augmentation
encodings (CATE) to our model when projecting the video representations for
contrastive learning. We show that representations learned by our method encode
valuable information about specified spatial or temporal augmentation, and in
doing so also achieve state-of-the-art performance on a number of video
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1"&gt;Arsha Nagrani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topo2vec: Topography Embedding Using the Fractal Effect. (arXiv:2108.08870v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08870</id>
        <link href="http://arxiv.org/abs/2108.08870"/>
        <updated>2021-08-23T01:36:35.990Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have transformed many fields by introducing
generic embedding spaces, capable of achieving great predictive performance
with minimal labeling effort. The geology field has not yet met such success.
In this work, we introduce an extension for self-supervised learning techniques
tailored for exploiting the fractal-effect in remote-sensing images. The
fractal-effect assumes that the same structures (for example rivers, peaks and
saddles) will appear in all scales. We demonstrate our method's effectiveness
on elevation data, we also use the effect in inference. We perform an extensive
analysis on several classification tasks and emphasize its effectiveness in
detecting the same class on different scales. To the best of our knowledge, it
is the first attempt to build a generic representation for topographic images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kavitzky_J/0/1/0/all/0/1"&gt;Jonathan Kavitzky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarecki_J/0/1/0/all/0/1"&gt;Jonathan Zarecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brusilovsky_I/0/1/0/all/0/1"&gt;Idan Brusilovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1"&gt;Uriel Singer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MGSampler: An Explainable Sampling Strategy for Video Action Recognition. (arXiv:2104.09952v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09952</id>
        <link href="http://arxiv.org/abs/2104.09952"/>
        <updated>2021-08-23T01:36:35.972Z</updated>
        <summary type="html"><![CDATA[Frame sampling is a fundamental problem in video action recognition due to
the essential redundancy in time and limited computation resources. The
existing sampling strategy often employs a fixed frame selection and lacks the
flexibility to deal with complex variations in videos. In this paper, we
present a simple, sparse, and explainable frame sampler, termed as
Motion-Guided Sampler (MGSampler). Our basic motivation is that motion is an
important and universal signal that can drive us to adaptively select frames
from videos. Accordingly, we propose two important properties in our MGSampler
design: motion sensitive and motion uniform. First, we present two different
motion representations to enable us to efficiently distinguish the
motion-salient frames from the background. Then, we devise a motion-uniform
sampling strategy based on the cumulative motion distribution to ensure the
sampled frames evenly cover all the important segments with high motion
salience. Our MGSampler yields a new principled and holistic sampling scheme,
that could be incorporated into any existing video architecture. Experiments on
five benchmarks demonstrate the effectiveness of our MGSampler over the
previous fixed sampling strategies, and its generalization power across
different backbones, video models, and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1"&gt;Yuan Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1"&gt;Zhan Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gangshan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Learning to Operationalize a Domain Agnostic Data Quality Scoring. (arXiv:2108.08905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08905</id>
        <link href="http://arxiv.org/abs/2108.08905"/>
        <updated>2021-08-23T01:36:35.965Z</updated>
        <summary type="html"><![CDATA[Data is expanding at an unimaginable rate, and with this development comes
the responsibility of the quality of data. Data Quality refers to the relevance
of the information present and helps in various operations like decision making
and planning in a particular organization. Mostly data quality is measured on
an ad-hoc basis, and hence none of the developed concepts provide any practical
application. The current empirical study was undertaken to formulate a concrete
automated data quality platform to assess the quality of incoming dataset and
generate a quality label, score and comprehensive report. We utilize various
datasets from healthdata.gov, opendata.nhs and Demographics and Health Surveys
(DHS) Program to observe the variations in the quality score and formulate a
label using Principal Component Analysis(PCA). The results of the current
empirical study revealed a metric that encompasses nine quality ingredients,
namely provenance, dataset characteristics, uniformity, metadata coupling,
percentage of missing cells and duplicate rows, skewness of data, the ratio of
inconsistencies of categorical columns, and correlation between these
attributes. The study also provides an illustrative case study and validation
of the metric following Mutation Testing approaches. This research study
provides an automated platform which takes an incoming dataset and metadata to
provide the DQ score, report and label. The results of this study would be
useful to data scientists as the value of this quality label would instill
confidence before deploying the data for his/her respective practical
application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chug_S/0/1/0/all/0/1"&gt;Sezal Chug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushal_P/0/1/0/all/0/1"&gt;Priya Kaushal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1"&gt;Tavpritesh Sethi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Correctness in Unsupervised Many-to-Many Image Translation. (arXiv:2103.15727v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15727</id>
        <link href="http://arxiv.org/abs/2103.15727"/>
        <updated>2021-08-23T01:36:35.959Z</updated>
        <summary type="html"><![CDATA[Given an input image from a source domain and a guidance image from a target
domain, unsupervised many-to-many image-to-image (UMMI2I) translation methods
seek to generate a plausible example from the target domain that preserves
domain-invariant information of the input source image and inherits the
domain-specific information from the guidance image. For example, when
translating female faces to male faces, the generated male face should have the
same expression, pose and hair color as the input female image, and the same
facial hairstyle and other male-specific attributes as the guidance male image.
Current state-of-the art UMMI2I methods generate visually pleasing images, but,
since for most pairs of real datasets we do not know which attributes are
domain-specific and which are domain-invariant, the semantic correctness of
existing approaches has not been quantitatively evaluated yet. In this paper,
we propose a set of benchmarks and metrics for the evaluation of semantic
correctness of these methods. We provide an extensive study of existing
state-of-the-art UMMI2I translation methods, showing that all methods, to
different degrees, fail to infer which attributes are domain-specific and which
are domain-invariant from data, and mostly rely on inductive biases hard-coded
into their architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1"&gt;Ben Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effectively Leveraging Attributes for Visual Similarity. (arXiv:2105.01695v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01695</id>
        <link href="http://arxiv.org/abs/2105.01695"/>
        <updated>2021-08-23T01:36:35.953Z</updated>
        <summary type="html"><![CDATA[Measuring similarity between two images often requires performing complex
reasoning along different axes (e.g., color, texture, or shape). Insights into
what might be important for measuring similarity can can be provided by
annotated attributes, but prior work tends to view these annotations as
complete, resulting in them using a simplistic approach of predicting
attributes on single images, which are, in turn, used to measure similarity.
However, it is impractical for a dataset to fully annotate every attribute that
may be important. Thus, only representing images based on these incomplete
annotations may miss out on key information. To address this issue, we propose
the Pairwise Attribute-informed similarity Network (PAN), which breaks
similarity learning into capturing similarity conditions and relevance scores
from a joint representation of two images. This enables our model to identify
that two images contain the same attribute, but can have it deemed irrelevant
(e.g., due to fine-grained differences between them) and ignored for measuring
similarity between the two images. Notably, while prior methods of using
attribute annotations are often unable to outperform prior art, PAN obtains a
4-9% improvement on compatibility prediction between clothing items on Polyvore
Outfits, a 5% gain on few shot classification of images using Caltech-UCSD
Birds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval.
Implementation available at https://github.com/samarth4149/PAN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Samarth Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhongping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ranjitha Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1"&gt;Venkatesh Saligrama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1"&gt;Bryan Plummer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Spacecraft Relative Navigation Methods: A Survey. (arXiv:2108.08876v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.08876</id>
        <link href="http://arxiv.org/abs/2108.08876"/>
        <updated>2021-08-23T01:36:35.947Z</updated>
        <summary type="html"><![CDATA[Autonomous spacecraft relative navigation technology has been planned for and
applied to many famous space missions. The development of on-board electronics
systems has enabled the use of vision-based and LiDAR-based methods to achieve
better performances. Meanwhile, deep learning has reached great success in
different areas, especially in computer vision, which has also attracted the
attention of space researchers. However, spacecraft navigation differs from
ground tasks due to high reliability requirements but lack of large datasets.
This survey aims to systematically investigate the current deep learning-based
autonomous spacecraft relative navigation methods, focusing on concrete orbital
applications such as spacecraft rendezvous and landing on small bodies or the
Moon. The fundamental characteristics, primary motivations, and contributions
of deep learning-based relative navigation algorithms are first summarised from
three perspectives of spacecraft rendezvous, asteroid exploration, and terrain
navigation. Furthermore, popular visual tracking benchmarks and their
respective properties are compared and summarised. Finally, potential
applications are discussed, along with expected impediments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jianing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1"&gt;Duarte Rondao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1"&gt;Nabil Aouf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13322</id>
        <link href="http://arxiv.org/abs/2011.13322"/>
        <updated>2021-08-23T01:36:35.929Z</updated>
        <summary type="html"><![CDATA[Skeleton-based human action recognition has attracted much attention with the
prevalence of accessible depth sensors. Recently, graph convolutional networks
(GCNs) have been widely used for this task due to their powerful capability to
model graph data. The topology of the adjacency graph is a key factor for
modeling the correlations of the input skeletons. Thus, previous methods mainly
focus on the design/learning of the graph topology. But once the topology is
learned, only a single-scale feature and one transformation exist in each layer
of the networks. Many insights, such as multi-scale information and multiple
sets of transformations, that have been proven to be very effective in
convolutional neural networks (CNNs), have not been investigated in GCNs. The
reason is that, due to the gap between graph-structured skeleton data and
conventional image/video data, it is very challenging to embed these insights
into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy
in GCNs for skeleton sequence processing. Specifically, we design a simple and
highly modularized graph convolutional network architecture for skeleton-based
action recognition. Our network is constructed by repeating a building block
that aggregates multi-granularity information from both the spatial and
temporal paths. Extensive experiments demonstrate that our network outperforms
state-of-the-art methods by a significant margin with only 1/5 of the
parameters and 1/10 of the FLOPs. Code is available at
https://github.com/yellowtownhz/STIGCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xinmei Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOFit: A Framework to reduce Obesity using Machine learning and IoT. (arXiv:2108.08868v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08868</id>
        <link href="http://arxiv.org/abs/2108.08868"/>
        <updated>2021-08-23T01:36:35.922Z</updated>
        <summary type="html"><![CDATA[From the past few years, due to advancements in technologies, the sedentary
living style in urban areas is at its peak. This results in individuals getting
a victim of obesity at an early age. There are various health impacts of
obesity like Diabetes, Heart disease, Blood pressure problems, and many more.
Machine learning from the past few years is showing its implications in all
expertise like forecasting, healthcare, medical imaging, sentiment analysis,
etc. In this work, we aim to provide a framework that uses machine learning
algorithms namely, Random Forest, Decision Tree, XGBoost, Extra Trees, and KNN
to train models that would help predict obesity levels (Classification),
Bodyweight, and fat percentage levels (Regression) using various parameters. We
also applied and compared various hyperparameter optimization (HPO) algorithms
such as Genetic algorithm, Random Search, Grid Search, Optuna to further
improve the accuracy of the models. The website framework contains various
other features like making customizable Diet plans, workout plans, and a
dashboard to track the progress. The framework is built using the Python Flask.
Furthermore, a weighing scale using the Internet of Things (IoT) is also
integrated into the framework to track calories and macronutrients from food
intake.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Satvik Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pundir_P/0/1/0/all/0/1"&gt;Pradyumn Pundir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Domain-Invariant Adversarial Network for Deep Domain Generalization. (arXiv:2108.08995v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08995</id>
        <link href="http://arxiv.org/abs/2108.08995"/>
        <updated>2021-08-23T01:36:35.916Z</updated>
        <summary type="html"><![CDATA[Domain generalization approaches aim to learn a domain invariant prediction
model for unknown target domains from multiple training source domains with
different distributions. Significant efforts have recently been committed to
broad domain generalization, which is a challenging and topical problem in
machine learning and computer vision communities. Most previous domain
generalization approaches assume that the conditional distribution across the
domains remain the same across the source domains and learn a domain invariant
model by minimizing the marginal distributions. However, the assumption of a
stable conditional distribution of the training source domains does not really
hold in practice. The hyperplane learned from the source domains will easily
misclassify samples scattered at the boundary of clusters or far from their
corresponding class centres. To address the above two drawbacks, we propose a
discriminative domain-invariant adversarial network (DDIAN) for domain
generalization. The discriminativeness of the features are guaranteed through a
discriminative feature module and domain-invariant features are guaranteed
through the global domain and local sub-domain alignment modules. Extensive
experiments on several benchmarks show that DDIAN achieves better prediction on
unseen target data during training compared to state-of-the-art domain
generalization approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mohammad Mahfujur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1"&gt;Clinton Fookes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1"&gt;Sridha Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation. (arXiv:2108.09141v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09141</id>
        <link href="http://arxiv.org/abs/2108.09141"/>
        <updated>2021-08-23T01:36:35.910Z</updated>
        <summary type="html"><![CDATA[Recommender system plays a crucial role in modern E-commerce platform. Due to
the lack of historical interactions between users and items, cold-start
recommendation is a challenging problem. In order to alleviate the cold-start
issue, most existing methods introduce content and contextual information as
the auxiliary information. Nevertheless, these methods assume the recommended
items behave steadily over time, while in a typical E-commerce scenario, items
generally have very different performances throughout their life period. In
such a situation, it would be beneficial to consider the long-term return from
the item perspective, which is usually ignored in conventional methods.
Reinforcement learning (RL) naturally fits such a long-term optimization
problem, in which the recommender could identify high potential items,
proactively allocate more user impressions to boost their growth, therefore
improve the multi-period cumulative gains. Inspired by this idea, we model the
process as a Partially Observable and Controllable Markov Decision Process
(POC-MDP), and propose an actor-critic RL framework (RL-LTV) to incorporate the
item lifetime values (LTV) into the recommendation. In RL-LTV, the critic
studies historical trajectories of items and predict the future LTV of fresh
item, while the actor suggests a score-based policy which maximizes the future
LTV expectation. Scores suggested by the actor are then combined with classical
ranking scores in a dual-rank framework, therefore the recommendation is
balanced with the LTV consideration. Our method outperforms the strong live
baseline with a relative improvement of 8.67% and 18.03% on IPV and GMV of
cold-start items, on one of the largest E-commerce platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Luo Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1"&gt;Qin Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bingqing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Calibration for Domain Generalization under Covariate Shift. (arXiv:2104.00742v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00742</id>
        <link href="http://arxiv.org/abs/2104.00742"/>
        <updated>2021-08-23T01:36:35.903Z</updated>
        <summary type="html"><![CDATA[Existing calibration algorithms address the problem of covariate shift via
unsupervised domain adaptation. However, these methods suffer from the
following limitations: 1) they require unlabeled data from the target domain,
which may not be available at the stage of calibration in real-world
applications and 2) their performance depends heavily on the disparity between
the distributions of the source and target domains. To address these two
limitations, we present novel calibration solutions via domain generalization.
Our core idea is to leverage multiple calibration domains to reduce the
effective distribution disparity between the target and calibration domains for
improved calibration transfer without needing any data from the target domain.
We provide theoretical justification and empirical experimental results to
demonstrate the effectiveness of our proposed algorithms. Compared against
state-of-the-art calibration methods designed for domain adaptation, we observe
a decrease of 8.86 percentage points in expected calibration error or,
equivalently, an increase of 35 percentage points in improvement ratio for
multi-class classification on the Office-Home dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yunye Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xiao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1"&gt;Thomas G. Dietterich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1"&gt;Ajay Divakaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gervasio_M/0/1/0/all/0/1"&gt;Melinda Gervasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields. (arXiv:2108.08931v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08931</id>
        <link href="http://arxiv.org/abs/2108.08931"/>
        <updated>2021-08-23T01:36:35.896Z</updated>
        <summary type="html"><![CDATA[Implicit neural representation is a recent approach to learn shape
collections as zero level-sets of neural networks, where each shape is
represented by a latent code. So far, the focus has been shape reconstruction,
while shape generalization was mostly left to generic encoder-decoder or
auto-decoder regularization.

In this paper we advocate deformation-aware regularization for implicit
neural representations, aiming at producing plausible deformations as latent
code changes. The challenge is that implicit representations do not capture
correspondences between different shapes, which makes it difficult to represent
and regularize their deformations. Thus, we propose to pair the implicit
representation of the shapes with an explicit, piecewise linear deformation
field, learned as an auxiliary function. We demonstrate that, by regularizing
these deformation fields, we can encourage the implicit neural representation
to induce natural deformations in the learned shape space, such as
as-rigid-as-possible deformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atzmon_M/0/1/0/all/0/1"&gt;Matan Atzmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1"&gt;David Novotny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08369</id>
        <link href="http://arxiv.org/abs/2107.08369"/>
        <updated>2021-08-23T01:36:35.878Z</updated>
        <summary type="html"><![CDATA[Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. Accurate and
robust flood detection including delineating open water flood areas and
identifying flood levels can aid in disaster response and mitigation. However,
estimating flood levels remotely is of essence as physical access to flooded
areas is limited and the ability to deploy instruments in potential flood zones
can be dangerous. Aligning flood extent mapping with local topography can
provide a plan-of-action that the disaster response team can consider. Thus,
remote flood level estimation via satellites like Sentinel-1 can prove to be
remedial. The Emerging Techniques in Computational Intelligence (ETCI)
competition on Flood Detection tasked participants with predicting flooded
pixels after training with synthetic aperture radar (SAR) images in a
supervised setting. We use a cyclical approach involving two stages (1)
training an ensemble model of multiple UNet architectures with available high
and low confidence labeled data and, generating pseudo labels or low confidence
labels on the entire unlabeled test dataset, and then, (2) filter out quality
generated labels and, (3) combining the generated labels with the previously
available high confidence labeled dataset. This assimilated dataset is used for
the next round of training ensemble models. This cyclical process is repeated
until the performance improvement plateaus. Additionally, we post process our
results with Conditional Random Fields. Our approach sets the second highest
score on the public hold-out test leaderboard for the ETCI competition with
0.7654 IoU. To the best of our knowledge we believe this is one of the first
works to try out semi-supervised learning to improve flood segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sayak Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting. (arXiv:2103.14023v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14023</id>
        <link href="http://arxiv.org/abs/2103.14023"/>
        <updated>2021-08-23T01:36:35.871Z</updated>
        <summary type="html"><![CDATA[Predicting accurate future trajectories of multiple agents is essential for
autonomous systems, but is challenging due to the complex agent interaction and
the uncertainty in each agent's future behavior. Forecasting multi-agent
trajectories requires modeling two key dimensions: (1) time dimension, where we
model the influence of past agent states over future states; (2) social
dimension, where we model how the state of each agent affects others. Most
prior methods model these two dimensions separately, e.g., first using a
temporal model to summarize features over time for each agent independently and
then modeling the interaction of the summarized features with a social model.
This approach is suboptimal since independent feature encoding over either the
time or social dimension can result in a loss of information. Instead, we would
prefer a method that allows an agent's state at one time to directly affect
another agent's state at a future time. To this end, we propose a new
Transformer, AgentFormer, that jointly models the time and social dimensions.
The model leverages a sequence representation of multi-agent trajectories by
flattening trajectory features across time and agents. Since standard attention
operations disregard the agent identity of each element in the sequence,
AgentFormer uses a novel agent-aware attention mechanism that preserves agent
identities by attending to elements of the same agent differently than elements
of other agents. Based on AgentFormer, we propose a stochastic multi-agent
trajectory prediction model that can attend to features of any agent at any
previous timestep when inferring an agent's future position. The latent intent
of all agents is also jointly modeled, allowing the stochasticity in one
agent's behavior to affect other agents. Our method significantly improves the
state of the art on well-established pedestrian and autonomous driving
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Ye Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1"&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1"&gt;Yanglan Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris Kitani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Greenhouse Gas Emissions Through Generative Adversarial Networks Based Wildfire Prediction. (arXiv:2108.08952v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08952</id>
        <link href="http://arxiv.org/abs/2108.08952"/>
        <updated>2021-08-23T01:36:35.864Z</updated>
        <summary type="html"><![CDATA[Over the past decade, the number of wildfire has increased significantly
around the world, especially in the State of California. The high-level
concentration of greenhouse gas (GHG) emitted by wildfires aggravates global
warming that further increases the risk of more fires. Therefore, an accurate
prediction of wildfire occurrence greatly helps in preventing large-scale and
long-lasting wildfires and reducing the consequent GHG emissions. Various
methods have been explored for wildfire risk prediction. However, the complex
correlations among a lot of natural and human factors and wildfire ignition
make the prediction task very challenging. In this paper, we develop a deep
learning based data augmentation approach for wildfire risk prediction. We
build a dataset consisting of diverse features responsible for fire ignition
and utilize a conditional tabular generative adversarial network to explore the
underlying patterns between the target value of risk levels and all involved
features. For fair and comprehensive comparisons, we compare our proposed
scheme with five other baseline methods where the former outperformed most of
them. To corroborate the robustness, we have also tested the performance of our
method with another dataset that also resulted in better efficiency. By
adopting the proposed method, we can take preventive strategies of wildfire
mitigation to reduce global GHG emissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Sifat Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1"&gt;Kai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Danish Fungi 2020 -- Not Just Another Image Recognition Dataset. (arXiv:2103.10107v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10107</id>
        <link href="http://arxiv.org/abs/2103.10107"/>
        <updated>2021-08-23T01:36:35.858Z</updated>
        <summary type="html"><![CDATA[We introduce a novel fine-grained dataset and benchmark, the Danish Fungi
2020 (DF20). The dataset, constructed from observations submitted to the Atlas
of Danish Fungi, is unique in its taxonomy-accurate class labels, small number
of errors, highly unbalanced long-tailed class distribution, rich observation
metadata, and well-defined class hierarchy. DF20 has zero overlap with
ImageNet, allowing unbiased comparison of models fine-tuned from publicly
available ImageNet checkpoints. The proposed evaluation protocol enables
testing the ability to improve classification using metadata -- e.g. precise
geographic location, habitat, and substrate, facilitates classifier calibration
testing, and finally allows to study the impact of the device settings on the
classification performance. Experiments using Convolutional Neural Networks
(CNN) and the recent Vision Transformers (ViT) show that DF20 presents a
challenging task. Interestingly, ViT achieves results superior to CNN baselines
with 80.45% accuracy and 0.743 macro F1 score, reducing the CNN error by 9% and
12% respectively. A simple procedure for including metadata into the decision
process improves the classification accuracy by more than 2.95 percentage
points, reducing the error rate by 15%. The source code for all methods and
experiments is available at https://sites.google.com/view/danish-fungi-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Picek_L/0/1/0/all/0/1"&gt;Luk&amp;#xe1;&amp;#x161; Picek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1"&gt;Milan &amp;#x160;ulc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Ji&amp;#x159;&amp;#xed; Matas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilmann_Clausen_J/0/1/0/all/0/1"&gt;Jacob Heilmann-Clausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeppesen_T/0/1/0/all/0/1"&gt;Thomas S. Jeppesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laessoe_T/0/1/0/all/0/1"&gt;Thomas L&amp;#xe6;ss&amp;#xf8;e&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Froslev_T/0/1/0/all/0/1"&gt;Tobias Fr&amp;#xf8;slev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation of Lungs COVID Infected Regions by Attention Mechanism and Synthetic Data. (arXiv:2108.08895v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08895</id>
        <link href="http://arxiv.org/abs/2108.08895"/>
        <updated>2021-08-23T01:36:35.851Z</updated>
        <summary type="html"><![CDATA[Coronavirus has caused hundreds of thousands of deaths. Fatalities could
decrease if every patient could get suitable treatment by the healthcare
system. Machine learning, especially computer vision methods based on deep
learning, can help healthcare professionals diagnose and treat COVID-19
infected cases more efficiently. Hence, infected patients can get better
service from the healthcare system and decrease the number of deaths caused by
the coronavirus. This research proposes a method for segmenting infected lung
regions in a CT image. For this purpose, a convolutional neural network with an
attention mechanism is used to detect infected areas with complex patterns.
Attention blocks improve the segmentation accuracy by focusing on informative
parts of the image. Furthermore, a generative adversarial network generates
synthetic images for data augmentation and expansion of small available
datasets. Experimental results show the superiority of the proposed method
compared to some existing procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yazdekhasty_P/0/1/0/all/0/1"&gt;Parham Yazdekhasty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zindari_A/0/1/0/all/0/1"&gt;Ali Zindari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1"&gt;Zahra Nabizadeh-ShahreBabak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1"&gt;Nader Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01077</id>
        <link href="http://arxiv.org/abs/2108.01077"/>
        <updated>2021-08-23T01:36:35.835Z</updated>
        <summary type="html"><![CDATA[A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any
user-information. We optimize these faces, by using an evolutionary algorithm
in the latent embedding space of the StyleGAN face generator. Multiple
evolutionary strategies are compared, and we propose a novel approach that
employs a neural network in order to direct the search in the direction of
promising samples, without adding fitness evaluations. The results we present
demonstrate that it is possible to obtain a high coverage of the LFW identities
(over 40%) with less than 10 master faces, for three leading deep face
recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stack of discriminative autoencoders for multiclass anomaly detection in endoscopy images. (arXiv:2103.08508v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08508</id>
        <link href="http://arxiv.org/abs/2103.08508"/>
        <updated>2021-08-23T01:36:35.828Z</updated>
        <summary type="html"><![CDATA[Wireless Capsule Endoscopy (WCE) helps physicians examine the
gastrointestinal (GI) tract noninvasively. There are few studies that address
pathological assessment of endoscopy images in multiclass classification and
most of them are based on binary anomaly detection or aim to detect a specific
type of anomaly. Multiclass anomaly detection is challenging, especially when
the dataset is poorly sampled or imbalanced. Many available datasets in
endoscopy field, such as KID2, suffer from an imbalance issue, which makes it
difficult to train a high-performance model. Additionally, increasing the
number of classes makes classification more difficult. We proposed a multiclass
classification algorithm that is extensible to any number of classes and can
handle an imbalance issue. The proposed method uses multiple autoencoders where
each one is trained on one class to extract features with the most
discrimination from other classes. The loss function of autoencoders is set
based on reconstruction, compactness, distance from other classes, and
Kullback-Leibler (KL) divergence. The extracted features are clustered and then
classified using an ensemble of support vector data descriptors. A total of
1,778 normal, 227 inflammation, 303 vascular, and 44 polyp images from the KID2
dataset are used for evaluation. The entire algorithm ran 5 times and achieved
F1-score of 96.3 +- 0.2% and 85.0 +- 0.4% on the test set for binary and
multiclass anomaly detection, respectively. The impact of each step of the
algorithm was investigated by various ablation studies and the results were
compared with published works. The suggested approach is a competitive option
for detecting multiclass anomalies in the GI field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohebbian_M/0/1/0/all/0/1"&gt;Mohammad Reza Mohebbian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wahid_K/0/1/0/all/0/1"&gt;Khan A. Wahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babyn_P/0/1/0/all/0/1"&gt;Paul Babyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-08-23T01:36:35.821Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Distant Supervision for Scene Graph Generation. (arXiv:2103.15365v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15365</id>
        <link href="http://arxiv.org/abs/2103.15365"/>
        <updated>2021-08-23T01:36:35.812Z</updated>
        <summary type="html"><![CDATA[Scene graph generation aims to identify objects and their relations in
images, providing structured image representations that can facilitate numerous
applications in computer vision. However, scene graph models usually require
supervised learning on large quantities of labeled data with intensive human
annotation. In this work, we propose visual distant supervision, a novel
paradigm of visual relation learning, which can train scene graph models
without any human-labeled data. The intuition is that by aligning commonsense
knowledge bases and images, we can automatically create large-scale labeled
data to provide distant supervision for visual relation learning. To alleviate
the noise in distantly labeled data, we further propose a framework that
iteratively estimates the probabilistic relation labels and eliminates the
noisy ones. Comprehensive experimental results show that our distantly
supervised model outperforms strong weakly supervised and semi-supervised
baselines. By further incorporating human-labeled data in a semi-supervised
fashion, our model outperforms state-of-the-art fully supervised models by a
large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for
predicate classification in Visual Genome evaluation). We make the data and
code for this paper publicly available at https://github.com/thunlp/VisualDS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Ao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengdi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Cornelius Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1"&gt;Stefan Wermter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03315</id>
        <link href="http://arxiv.org/abs/2107.03315"/>
        <updated>2021-08-23T01:36:35.803Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that the performance of machine learning models can
vary substantially when models are evaluated on data drawn from a distribution
that is close to but different from the training distribution. As a result,
predicting model performance on unseen distributions is an important challenge.
Our work connects techniques from domain adaptation and predictive uncertainty
literature, and allows us to predict model accuracy on challenging unseen
distributions without access to labeled data. In the context of distribution
shift, distributional distances are often used to adapt models and improve
their performance on new domains, however accuracy estimation, or other forms
of predictive uncertainty, are often neglected in these investigations. Through
investigating a wide range of established distributional distances, such as
Frechet distance or Maximum Mean Discrepancy, we determine that they fail to
induce reliable estimates of performance under distribution shift. On the other
hand, we find that the difference of confidences (DoC) of a classifier's
predictions successfully estimates the classifier's performance change over a
variety of shifts. We specifically investigate the distinction between
synthetic and natural distribution shifts and observe that despite its
simplicity DoC consistently outperforms other quantifications of distributional
difference. $DoC$ reduces predictive error by almost half ($46\%$) on several
realistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust
and ImageNet-Rendition datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1"&gt;Devin Guillory&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1"&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planar Surface Reconstruction from Sparse Views. (arXiv:2103.14644v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14644</id>
        <link href="http://arxiv.org/abs/2103.14644"/>
        <updated>2021-08-23T01:36:35.785Z</updated>
        <summary type="html"><![CDATA[The paper studies planar surface reconstruction of indoor scenes from two
views with unknown camera poses. While prior approaches have successfully
created object-centric reconstructions of many scenes, they fail to exploit
other structures, such as planes, which are typically the dominant components
of indoor scenes. In this paper, we reconstruct planar surfaces from multiple
views, while jointly estimating camera pose. Our experiments demonstrate that
our method is able to advance the state of the art of reconstruction from
sparse views, on challenging scenes from Matterport3D. Project site:
https://jinlinyi.github.io/SparsePlanes/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linyi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shengyi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1"&gt;Andrew Owens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1"&gt;David F. Fouhey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[4D Atlas: Statistical Analysis of the Spatiotemporal Variability in Longitudinal 3D Shape Data. (arXiv:2101.09403v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09403</id>
        <link href="http://arxiv.org/abs/2101.09403"/>
        <updated>2021-08-23T01:36:35.741Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework to learn the spatiotemporal variability in
longitudinal 3D shape data sets, which contain observations of objects that
evolve and deform over time. This problem is challenging since surfaces come
with arbitrary parameterizations and thus, they need to be spatially
registered. Also, different deforming objects, also called 4D surfaces, evolve
at different speeds and thus they need to be temporally aligned. We solve this
spatiotemporal registration problem using a Riemannian approach. We treat a 3D
surface as a point in a shape space equipped with an elastic Riemannian metric
that measures the amount of bending and stretching that the surfaces undergo. A
4D surface can then be seen as a trajectory in this space. With this
formulation, the statistical analysis of 4D surfaces can be cast as the problem
of analyzing trajectories embedded in a nonlinear Riemannian manifold. However,
performing the spatiotemporal registration, and subsequently computing
statistics, on such nonlinear spaces is not straightforward as they rely on
complex nonlinear optimizations. Our core contribution is the mapping of the
surfaces to the space of Square-Root Normal Fields where the L2 metric is
equivalent to the partial elastic metric in the space of surfaces. Thus, by
solving the spatial registration in the SRNF space, the problem of analyzing 4D
surfaces becomes the problem of analyzing trajectories embedded in the SRNF
space, which has a Euclidean structure. In this paper, we develop the building
blocks that enable such analysis. These include: (1) the spatiotemporal
registration of arbitrarily parameterized 4D surfaces in the presence of large
elastic deformations and large variations in their execution rates; (2) the
computation of geodesics between 4D surfaces; (3) the computation of
statistical summaries; and (4) the synthesis of random 4D surfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laga_H/0/1/0/all/0/1"&gt;Hamid Laga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padilla_M/0/1/0/all/0/1"&gt;Marcel Padilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jermyn_I/0/1/0/all/0/1"&gt;Ian H. Jermyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurtek_S/0/1/0/all/0/1"&gt;Sebastian Kurtek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Anuj Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing Patchy Reionization with Deep Learning. (arXiv:2101.01214v2 [astro-ph.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01214</id>
        <link href="http://arxiv.org/abs/2101.01214"/>
        <updated>2021-08-23T01:36:35.700Z</updated>
        <summary type="html"><![CDATA[The precision anticipated from next-generation cosmic microwave background
(CMB) surveys will create opportunities for characteristically new insights
into cosmology. Secondary anisotropies of the CMB will have an increased
importance in forthcoming surveys, due both to the cosmological information
they encode and the role they play in obscuring our view of the primary
fluctuations. Quadratic estimators have become the standard tools for
reconstructing the fields that distort the primary CMB and produce secondary
anisotropies. While successful for lensing reconstruction with current data,
quadratic estimators will be sub-optimal for the reconstruction of lensing and
other effects at the expected sensitivity of the upcoming CMB surveys. In this
paper we describe a convolutional neural network, ResUNet-CMB, that is capable
of the simultaneous reconstruction of two sources of secondary CMB
anisotropies, gravitational lensing and patchy reionization. We show that the
ResUNet-CMB network significantly outperforms the quadratic estimator at low
noise levels and is not subject to the lensing-induced bias on the patchy
reionization reconstruction that would be present with a straightforward
application of the quadratic estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Guzman_E/0/1/0/all/0/1"&gt;Eric Guzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Meyers_J/0/1/0/all/0/1"&gt;Joel Meyers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Automatic Annotation For Visual Object Tracking. (arXiv:2101.06977v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06977</id>
        <link href="http://arxiv.org/abs/2101.06977"/>
        <updated>2021-08-23T01:36:35.685Z</updated>
        <summary type="html"><![CDATA[We propose a semi-automatic bounding box annotation method for visual object
tracking by utilizing temporal information with a tracking-by-detection
approach. For detection, we use an off-the-shelf object detector which is
trained iteratively with the annotations generated by the proposed method, and
we perform object detection on each frame independently. We employ Multiple
Hypothesis Tracking (MHT) to exploit temporal information and to reduce the
number of false-positives which makes it possible to use lower objectness
thresholds for detection to increase recall. The tracklets formed by MHT are
evaluated by human operators to enlarge the training set. This novel
incremental learning approach helps to perform annotation iteratively. The
experiments performed on AUTH Multidrone Dataset reveal that the annotation
workload can be reduced up to 96% by the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1"&gt;Kutalmis Gokalp Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1"&gt;Aybora Koksal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazla_A/0/1/0/all/0/1"&gt;Arda Fazla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus-based Optimization for 3D Human Pose Estimation in Camera Coordinates. (arXiv:1911.09245v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.09245</id>
        <link href="http://arxiv.org/abs/1911.09245"/>
        <updated>2021-08-23T01:36:35.669Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is frequently seen as the task of estimating 3D
poses relative to the root body joint. Alternatively, we propose a 3D human
pose estimation method in camera coordinates, which allows effective
combination of 2D annotated data and 3D poses and a straightforward multi-view
generalization. To that end, we cast the problem as a view frustum space pose
estimation, where absolute depth prediction and joint relative depth
estimations are disentangled. Final 3D predictions are obtained in camera
coordinates by the inverse camera projection. Based on this, we also present a
consensus-based optimization algorithm for multi-view predictions from
uncalibrated images, which requires a single monocular training procedure.
Although our method is indirectly tied to the training camera intrinsics, it
still converges for cameras with different intrinsic parameters, resulting in
coherent estimations up to a scale factor. Our method improves the state of the
art on well known 3D human pose datasets, reducing the prediction error by 32%
in the most common benchmark. We also reported our results in absolute pose
position error, achieving 80~mm for monocular estimations and 51~mm for
multi-view, on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1"&gt;Diogo C Luvizon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1"&gt;Hedi Tabia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1"&gt;David Picard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distance Metric-Based Learning with Interpolated Latent Features for Location Classification in Endoscopy Image and Video. (arXiv:2103.08504v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08504</id>
        <link href="http://arxiv.org/abs/2103.08504"/>
        <updated>2021-08-23T01:36:35.661Z</updated>
        <summary type="html"><![CDATA[Conventional Endoscopy (CE) and Wireless Capsule Endoscopy (WCE) are known
tools for diagnosing gastrointestinal (GI) tract disorders. Detecting the
anatomical location of GI tract can help clinicians to determine a more
appropriate treatment plan, can reduce repetitive endoscopy and is important in
drug-delivery. There are few research that address detecting anatomical
location of WCE and CE images using classification, mainly because of
difficulty in collecting data and anotating them. In this study, we present a
few-shot learning method based on distance metric learning which combines
transfer-learning and manifold mixup scheme for localizing endoscopy frames and
can be trained on few samples. The manifold mixup process improves few-shot
learning by increasing the number of training epochs while reducing
overfitting, as well as providing more accurate decision boundaries. A dataset
is collected from 10 different anatomical positions of human GI tract. Two
models were trained using only 78 CE and 27 WCE annotated frames to predict the
location of 25700 and 1825 video frames from CE and WCE, respectively. In
addition, we performed subjective evaluation using nine gastroenterologists to
show the necessaity of having an AI system for localization. Various ablation
studies and interpretations are performed to show the importance of each step,
such effect of transfer-learning approach, and impact of manifold mixup on
performance. The proposed method is also compared with various methods trained
on categorical cross-entropy loss and produced better results which show that
proposed method has potential to be used for endoscopy image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohebbian_M/0/1/0/all/0/1"&gt;Mohammad Reza Mohebbian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wahid_K/0/1/0/all/0/1"&gt;Khan A. Wahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_A/0/1/0/all/0/1"&gt;Anh Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babyn_P/0/1/0/all/0/1"&gt;Paul Babyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods. (arXiv:2103.03078v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03078</id>
        <link href="http://arxiv.org/abs/2103.03078"/>
        <updated>2021-08-23T01:36:35.630Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models used in medical imaging diagnostics can be
vulnerable to a variety of privacy attacks, including membership inference
attacks, that lead to violations of regulations governing the use of medical
data and threaten to compromise their effective deployment in the clinic. In
contrast to most recent work in privacy-aware ML that has been focused on model
alteration and post-processing steps, we propose here a novel and complementary
scheme that enhances the security of medical data by controlling the data
sharing process. We develop and evaluate a privacy defense protocol based on
using a generative adversarial network (GAN) that allows a medical data sourcer
(e.g. a hospital) to provide an external agent (a modeler) a proxy dataset
synthesized from the original images, so that the resulting diagnostic systems
made available to model consumers is rendered resilient to privacy attackers.
We validate the proposed method on retinal diagnostics AI used for diabetic
retinopathy that bears the risk of possibly leaking private information. To
incorporate concerns of both privacy advocates and modelers, we introduce a
metric to evaluate privacy and utility performance in combination, and
demonstrate, using these novel and classical metrics, that our approach, by
itself or in conjunction with other defenses, provides state of the art (SOTA)
performance for defending against privacy attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yinzhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miaomiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Truly Unsupervised Image-to-Image Translation. (arXiv:2006.06500v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06500</id>
        <link href="http://arxiv.org/abs/2006.06500"/>
        <updated>2021-08-23T01:36:35.617Z</updated>
        <summary type="html"><![CDATA[Every recent image-to-image translation model inherently requires either
image-level (i.e. input-output pairs) or set-level (i.e. domain labels)
supervision. However, even set-level supervision can be a severe bottleneck for
data collection in practice. In this paper, we tackle image-to-image
translation in a fully unsupervised setting, i.e., neither paired images nor
domain labels. To this end, we propose a truly unsupervised image-to-image
translation model (TUNIT) that simultaneously learns to separate image domains
and translates input images into the estimated domains. Experimental results
show that our model achieves comparable or even better performance than the
set-level supervised model trained with full labels, generalizes well on
various datasets, and is robust against the choice of hyperparameters (e.g. the
preset number of pseudo domains). Furthermore, TUNIT can be easily extended to
semi-supervised learning with a few labeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_K/0/1/0/all/0/1"&gt;Kyungjune Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yunjey Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1"&gt;Youngjung Uh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Jaejun Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1"&gt;Hyunjung Shim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Region-based Multi-Label Zero-Shot Learning. (arXiv:2108.09301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09301</id>
        <link href="http://arxiv.org/abs/2108.09301"/>
        <updated>2021-08-23T01:36:35.610Z</updated>
        <summary type="html"><![CDATA[Multi-label zero-shot learning (ZSL) is a more realistic counter-part of
standard single-label ZSL since several objects can co-exist in a natural
image. However, the occurrence of multiple objects complicates the reasoning
and requires region-specific processing of visual features to preserve their
contextual cues. We note that the best existing multi-label ZSL method takes a
shared approach towards attending to region features with a common set of
attention maps for all the classes. Such shared maps lead to diffused
attention, which does not discriminatively focus on relevant locations when the
number of classes are large. Moreover, mapping spatially-pooled visual features
to the class semantics leads to inter-class feature entanglement, thus
hampering the classification. Here, we propose an alternate approach towards
region-based discriminability-preserving multi-label zero-shot classification.
Our approach maintains the spatial resolution to preserve region-level
characteristics and utilizes a bi-level attention module (BiAM) to enrich the
features by incorporating both region and scene context information. The
enriched region-level features are then mapped to the class semantics and only
their class predictions are spatially pooled to obtain image-level predictions,
thereby keeping the multi-class features disentangled. Our approach sets a new
state of the art on two large-scale multi-label zero-shot benchmarks: NUS-WIDE
and Open Images. On NUS-WIDE, our approach achieves an absolute gain of 6.9%
mAP for ZSL, compared to the best published results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1"&gt;Sanath Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Akshita Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervision on Unlabelled OR Data for Multi-person 2D/3D Human Pose Estimation. (arXiv:2007.08354v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08354</id>
        <link href="http://arxiv.org/abs/2007.08354"/>
        <updated>2021-08-23T01:36:35.603Z</updated>
        <summary type="html"><![CDATA[2D/3D human pose estimation is needed to develop novel intelligent tools for
the operating room that can analyze and support the clinical activities. The
lack of annotated data and the complexity of state-of-the-art pose estimation
approaches limit, however, the deployment of such techniques inside the OR. In
this work, we propose to use knowledge distillation in a teacher/student
framework to harness the knowledge present in a large-scale non-annotated
dataset and in an accurate but complex multi-stage teacher network to train a
lightweight network for joint 2D/3D pose estimation. The teacher network also
exploits the unlabeled data to generate both hard and soft labels useful in
improving the student predictions. The easily deployable network trained using
this effective self-supervision strategy performs on par with the teacher
network on \emph{MVOR+}, an extension of the public MVOR dataset where all
persons have been fully annotated, thus providing a viable solution for
real-time 2D/3D human pose estimation in the OR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1"&gt;Vinkle Srivastav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1"&gt;Afshin Gangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Neighborhood Deep Fusion Network for Point Cloud Analysis. (arXiv:2108.09228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09228</id>
        <link href="http://arxiv.org/abs/2108.09228"/>
        <updated>2021-08-23T01:36:35.596Z</updated>
        <summary type="html"><![CDATA[Convolutional neural network has made remarkable achievements in
classification of idealized point cloud, however, non-idealized point cloud
classification is still a challenging task. In this paper, DNDFN, namely,
Dual-Neighborhood Deep Fusion Network, is proposed to deal with this problem.
DNDFN has two key points. One is combination of local neighborhood and global
neigh-borhood. nearest neighbor (kNN) or ball query can capture the local
neighborhood but ignores long-distance dependencies. A trainable neighborhood
learning meth-od called TN-Learning is proposed, which can capture the global
neighborhood. TN-Learning is combined with them to obtain richer neighborhood
information. The other is information transfer convolution (IT-Conv) which can
learn the structural information between two points and transfer features
through it. Extensive exper-iments on idealized and non-idealized benchmarks
across four tasks verify DNDFN achieves the state of the arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guoquan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Hezhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1"&gt;Jianwei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yanxin Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives. (arXiv:2003.10739v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10739</id>
        <link href="http://arxiv.org/abs/2003.10739"/>
        <updated>2021-08-23T01:36:35.589Z</updated>
        <summary type="html"><![CDATA[While the depth of modern Convolutional Neural Networks (CNNs) surpasses that
of the pioneering networks with a significant margin, the traditional way of
appending supervision only over the final classifier and progressively
propagating gradient flow upstream remains the training mainstay. Seminal
Deeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of
optimization arising from gradient flow through a long chain. However, it is
still vulnerable to issues including interference to the hierarchical
representation generation process and inconsistent optimization objectives, as
illustrated theoretically and empirically in this paper. Complementary to
previous training strategies, we propose Dynamic Hierarchical Mimicking, a
generic feature learning mechanism, to advance CNN training with enhanced
generalization ability. Partially inspired by DSN, we fork delicately designed
side branches from the intermediate layers of a given neural network. Each
branch can emerge from certain locations of the main branch dynamically, which
not only retains representation rooted in the backbone network but also
generates more diverse representations along its own pathway. We go one step
further to promote multi-level interactions among different branches through an
optimization formula with probabilistic prediction matching losses, thus
guaranteeing a more robust optimization process and better representation
ability. Experiments on both category and instance recognition tasks
demonstrate the substantial improvements of our proposed method over its
corresponding counterparts using diverse state-of-the-art CNN architectures.
Code and models are publicly available at https://github.com/d-li14/DHM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Duo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Deep Video Denoising. (arXiv:2011.15045v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15045</id>
        <link href="http://arxiv.org/abs/2011.15045"/>
        <updated>2021-08-23T01:36:35.564Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) for video denoising are typically
trained with supervision, assuming the availability of clean videos. However,
in many applications, such as microscopy, noiseless videos are not available.
To address this, we propose an Unsupervised Deep Video Denoiser (UDVD), a CNN
architecture designed to be trained exclusively with noisy data. The
performance of UDVD is comparable to the supervised state-of-the-art, even when
trained only on a single short noisy video. We demonstrate the promise of our
approach in real-world imaging applications by denoising raw video,
fluorescence-microscopy and electron-microscopy data. In contrast to many
current approaches to video denoising, UDVD does not require explicit motion
compensation. This is advantageous because motion compensation is
computationally expensive, and can be unreliable when the input data are noisy.
A gradient-based analysis reveals that UDVD automatically adapts to local
motion in the input noisy videos. Thus, the network learns to perform implicit
motion compensation, even though it is only trained for denoising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sheth_D/0/1/0/all/0/1"&gt;Dev Yashpal Sheth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohan_S/0/1/0/all/0/1"&gt;Sreyas Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vincent_J/0/1/0/all/0/1"&gt;Joshua L. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manzorro_R/0/1/0/all/0/1"&gt;Ramon Manzorro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crozier_P/0/1/0/all/0/1"&gt;Peter A. Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khapra_M/0/1/0/all/0/1"&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simoncelli_E/0/1/0/all/0/1"&gt;Eero P. Simoncelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zoom, Enhance! Measuring Surveillance GAN Up-sampling. (arXiv:2108.09285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09285</id>
        <link href="http://arxiv.org/abs/2108.09285"/>
        <updated>2021-08-23T01:36:35.553Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks have been very successfully used for many computer
vision and pattern recognition applications. While Convolutional Neural
Networks(CNNs) have shown the path to state of art image classifications,
Generative Adversarial Networks or GANs have provided state of art capabilities
in image generation. In this paper we extend the applications of CNNs and GANs
to experiment with up-sampling techniques in the domains of security and
surveillance. Through this work we evaluate, compare and contrast the state of
art techniques in both CNN and GAN based image and video up-sampling in the
surveillance domain. As a result of this study we also provide experimental
evidence to establish DISTS as a stronger Image Quality Assessment(IQA) metric
for comparing GAN Based Image Up-sampling in the surveillance domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sparkman_J/0/1/0/all/0/1"&gt;Jake Sparkman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Ayrot_A/0/1/0/all/0/1"&gt;Abdalla Al-Ayrot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Contractor_U/0/1/0/all/0/1"&gt;Utkarsh Contractor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusion Moves for Graph Matching. (arXiv:2101.12085v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12085</id>
        <link href="http://arxiv.org/abs/2101.12085"/>
        <updated>2021-08-23T01:36:35.545Z</updated>
        <summary type="html"><![CDATA[We contribute to approximate algorithms for the quadratic assignment problem
also known as graph matching. Inspired by the success of the fusion moves
technique developed for multilabel discrete Markov random fields, we
investigate its applicability to graph matching. In particular, we show how
fusion moves can be efficiently combined with the dedicated state-of-the-art
dual methods that have recently shown superior results in computer vision and
bio-imaging applications. As our empirical evaluation on a wide variety of
graph matching datasets suggests, fusion moves significantly improve
performance of these methods in terms of speed and quality of the obtained
solutions. Our method sets a new state-of-the-art with a notable margin with
respect to its competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hutschenreiter_L/0/1/0/all/0/1"&gt;Lisa Hutschenreiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haller_S/0/1/0/all/0/1"&gt;Stefan Haller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feineis_L/0/1/0/all/0/1"&gt;Lorenz Feineis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1"&gt;Carsten Rother&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainmuller_D/0/1/0/all/0/1"&gt;Dagmar Kainm&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1"&gt;Bogdan Savchynskyy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PowerLinear Activation Functions with application to the first layer of CNNs. (arXiv:2108.09256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09256</id>
        <link href="http://arxiv.org/abs/2108.09256"/>
        <updated>2021-08-23T01:36:35.520Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have become the state-of-the-art tool
for dealing with unsolved problems in computer vision and image processing.
Since the convolution operator is a linear operator, several generalizations
have been proposed to improve the performance of CNNs. One way to increase the
capability of the convolution operator is by applying activation functions on
the inner product operator. In this paper, we will introduce PowerLinear
activation functions, which are based on the polynomial kernel generalization
of the convolution operator. EvenPowLin functions are the main branch of the
PowerLinear activation functions. This class of activation functions is
saturated neither in the positive input region nor in the negative one. Also,
the negative inputs are activated with the same magnitude as the positive
inputs. These features made the EvenPowLin activation functions able to be
utilized in the first layer of CNN architectures and learn complex features of
input images. Additionally, EvenPowLin activation functions are used in CNN
models to classify the inversion of grayscale images as accurately as the
original grayscale images, which is significantly better than commonly used
activation functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasiri_K/0/1/0/all/0/1"&gt;Kamyar Nasiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_Shirazi_K/0/1/0/all/0/1"&gt;Kamaledin Ghiasi-Shirazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Bayesian Neural Doppler Imaging. (arXiv:2108.09266v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2108.09266</id>
        <link href="http://arxiv.org/abs/2108.09266"/>
        <updated>2021-08-23T01:36:35.506Z</updated>
        <summary type="html"><![CDATA[The non-uniform surface temperature distribution of rotating active stars is
routinely mapped with the Doppler Imaging technique. Inhomogeneities in the
surface produce features in high-resolution spectroscopic observations that
shift in wavelength depending on their position on the visible hemisphere. The
inversion problem has been systematically solved using maximum a-posteriori
regularized methods assuming smoothness or maximum entropy. Our aim in this
work is to solve the full Bayesian inference problem, by providing access to
the posterior distribution of the surface temperature in the star. We use
amortized neural posterior estimation to produce a model that approximates the
high-dimensional posterior distribution for spectroscopic observations of
selected spectral ranges sampled at arbitrary rotation phases. The posterior
distribution is approximated with conditional normalizing flows, which are
flexible, tractable and easy to sample approximations to arbitrary
distributions. When conditioned on the spectroscopic observations, they provide
a very efficient way of obtaining samples from the posterior distribution. The
conditioning on observations is obtained through the use of Transformer
encoders, which can deal with arbitrary wavelength sampling and rotation
phases. Our model can produce thousands of posterior samples per second. Our
validation of the model for very high signal-to-noise observations shows that
it correctly approximates the posterior, although with some overestimation of
the broadening. We apply the model to the moderately fast rotator II Peg,
producing the first Bayesian map of its temperature inhomogenities. We conclude
that conditional normalizing flows are a very promising tool to carry out
approximate Bayesian inference in more complex problems in stellar physics,
like constraining the magnetic properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ramos_A/0/1/0/all/0/1"&gt;A. Asensio Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Baso_C/0/1/0/all/0/1"&gt;C. Diaz Baso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kochukhov_O/0/1/0/all/0/1"&gt;O. Kochukhov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition. (arXiv:2009.06138v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06138</id>
        <link href="http://arxiv.org/abs/2009.06138"/>
        <updated>2021-08-23T01:36:35.489Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has been gaining attention in the past
few years. However, most existing methods are based on gradients or
intermediate features, which are not directly involved in the decision-making
process of the classifier. In this paper, we propose a slot attention-based
classifier called SCOUTER for transparent yet accurate classification. Two
major differences from other attention-based methods include: (a) SCOUTER's
explanation is involved in the final confidence for each category, offering
more intuitive interpretation, and (b) all the categories have their
corresponding positive or negative explanation, which tells "why the image is
of a certain category" or "why the image is not of a certain category." We
design a new loss tailored for SCOUTER that controls the model's behavior to
switch between positive and negative explanations, as well as the size of
explanatory regions. Experimental results show that SCOUTER can give better
visual explanations in terms of various metrics while keeping good accuracy on
small and medium-sized datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bowen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1"&gt;Manisha Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawasaki_R/0/1/0/all/0/1"&gt;Ryo Kawasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1"&gt;Hajime Nagahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAE-CE: Visual Contrastive Explanation using Disentangled VAEs. (arXiv:2108.09159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09159</id>
        <link href="http://arxiv.org/abs/2108.09159"/>
        <updated>2021-08-23T01:36:35.476Z</updated>
        <summary type="html"><![CDATA[The goal of a classification model is to assign the correct labels to data.
In most cases, this data is not fully described by the given set of labels.
Often a rich set of meaningful concepts exist in the domain that can much more
precisely describe each datapoint. Such concepts can also be highly useful for
interpreting the model's classifications. In this paper we propose a model,
denoted as Variational Autoencoder-based Contrastive Explanation (VAE-CE), that
represents data with high-level concepts and uses this representation for both
classification and generating explanations. The explanations are produced in a
contrastive manner, conveying why a datapoint is assigned to one class rather
than an alternative class. An explanation is specified as a set of
transformations of the input datapoint, with each step depicting a concept
changing towards the contrastive class. We build the model using a disentangled
VAE, extended with a new supervised method for disentangling individual
dimensions. An analysis on synthetic data and MNIST shows that the approaches
to both disentanglement and explanation provide benefits over other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poels_Y/0/1/0/all/0/1"&gt;Yoeri Poels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1"&gt;Vlado Menkovski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation. (arXiv:1808.08180v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.08180</id>
        <link href="http://arxiv.org/abs/1808.08180"/>
        <updated>2021-08-23T01:36:35.469Z</updated>
        <summary type="html"><![CDATA[Person detection and pose estimation is a key requirement to develop
intelligent context-aware assistance systems. To foster the development of
human pose estimation methods and their applications in the Operating Room
(OR), we release the Multi-View Operating Room (MVOR) dataset, the first public
dataset recorded during real clinical interventions. It consists of 732
synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR.
It also includes the visual challenges present in such environments, such as
occlusions and clutter. We provide camera calibration parameters, color and
depth frames, human bounding boxes, and 2D/3D pose annotations. In this paper,
we present the dataset, its annotations, as well as baseline results from
several recent person detection and 2D/3D pose estimation methods. Since we
need to blur some parts of the images to hide identity and nudity in the
released dataset, we also present a comparative study of how the baselines have
been impacted by the blurring. Results show a large margin for improvement and
suggest that the MVOR dataset can be useful to compare the performance of the
different methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1"&gt;Vinkle Srivastav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1"&gt;Thibaut Issenhuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadkhodamohammadi_A/0/1/0/all/0/1"&gt;Abdolrahim Kadkhodamohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_M/0/1/0/all/0/1"&gt;Michel de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1"&gt;Afshin Gangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Estimation on Privacy-Preserving Low-Resolution Depth Images. (arXiv:2007.08340v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08340</id>
        <link href="http://arxiv.org/abs/2007.08340"/>
        <updated>2021-08-23T01:36:35.451Z</updated>
        <summary type="html"><![CDATA[Human pose estimation (HPE) is a key building block for developing AI-based
context-aware systems inside the operating room (OR). The 24/7 use of images
coming from cameras mounted on the OR ceiling can however raise concerns for
privacy, even in the case of depth images captured by RGB-D sensors. Being able
to solely use low-resolution privacy-preserving images would address these
concerns and help scale up the computer-assisted approaches that rely on such
data to a larger number of ORs. In this paper, we introduce the problem of HPE
on low-resolution depth images and propose an end-to-end solution that
integrates a multi-scale super-resolution network with a 2D human pose
estimation network. By exploiting intermediate feature-maps generated at
different super-resolution, our approach achieves body pose results on
low-resolution images (of size 64x48) that are on par with those of an approach
trained and tested on full resolution images (of size 640x480).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1"&gt;Vinkle Srivastav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1"&gt;Afshin Gangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Photorealistic Colorization by Imagination. (arXiv:2108.09195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09195</id>
        <link href="http://arxiv.org/abs/2108.09195"/>
        <updated>2021-08-23T01:36:35.444Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to automatic image colorization by imitating the
imagination process of human experts. Our imagination module is designed to
generate color images that are context-correlated with black-and-white photos.
Given a black-and-white image, our imagination module firstly extracts the
context information, which is then used to synthesize colorful and diverse
images using a conditional image synthesis network (e.g., semantic image
synthesis model). We then design a colorization module to colorize the
black-and-white images with the guidance of imagination for photorealistic
colorization. Experimental results show that our work produces more colorful
and diverse results than state-of-the-art image colorization methods. Our
source codes will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1"&gt;Chenyang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Aware Self-Training for Unsupervised Domain Adaptationon Object Point Clouds. (arXiv:2108.09169v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09169</id>
        <link href="http://arxiv.org/abs/2108.09169"/>
        <updated>2021-08-23T01:36:35.437Z</updated>
        <summary type="html"><![CDATA[The point cloud representation of an object can have a large geometric
variation in view of inconsistent data acquisition procedure, which thus leads
to domain discrepancy due to diverse and uncontrollable shape representation
cross datasets. To improve discrimination on unseen distribution of point-based
geometries in a practical and feasible perspective, this paper proposes a new
method of geometry-aware self-training (GAST) for unsupervised domain
adaptation of object point cloud classification. Specifically, this paper aims
to learn a domain-shared representation of semantic categories, via two novel
self-supervised geometric learning tasks as feature regularization. On one
hand, the representation learning is empowered by a linear mixup of point cloud
samples with their self-generated rotation labels, to capture a global
topological configuration of local geometries. On the other hand, a diverse
point distribution across datasets can be normalized with a novel
curvature-aware distortion localization. Experiments on the PointDA-10 dataset
show that our GAST method can significantly outperform the state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1"&gt;Longkun Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Ads Content Structuring by Combining Scene Confidence Prediction and Tagging. (arXiv:2108.09215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09215</id>
        <link href="http://arxiv.org/abs/2108.09215"/>
        <updated>2021-08-23T01:36:35.430Z</updated>
        <summary type="html"><![CDATA[Video ads segmentation and tagging is a challenging task due to two main
reasons: (1) the video scene structure is complex and (2) it includes multiple
modalities (e.g., visual, audio, text.). While previous work focuses mostly on
activity videos (e.g. "cooking", "sports"), it is not clear how they can be
leveraged to tackle the task of video ads content structuring. In this paper,
we propose a two-stage method that first provides the boundaries of the scenes,
and then combines a confidence score for each segmented scene and the tag
classes predicted for that scene. We provide extensive experimental results on
the network architectures and modalities used for the proposed method. Our
combined method improves the previous baselines on the challenging "Tencent
Advertisement Video" dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Tomoyuki Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1"&gt;Antonio Tejero-de-Pablos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-level Active Learning for Cluttered Scenes. (arXiv:2108.09186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09186</id>
        <link href="http://arxiv.org/abs/2108.09186"/>
        <updated>2021-08-23T01:36:35.423Z</updated>
        <summary type="html"><![CDATA[Active learning for object detection is conventionally achieved by applying
techniques developed for classification in a way that aggregates individual
detections into image-level selection criteria. This is typically coupled with
the costly assumption that every image selected for labelling must be
exhaustively annotated. This yields incremental improvements on well-curated
vision datasets and struggles in the presence of data imbalance and visual
clutter that occurs in real-world imagery. Alternatives to the image-level
approach are surprisingly under-explored in the literature. In this work, we
introduce a new strategy that subsumes previous Image-level and Object-level
approaches into a generalized, Region-level approach that promotes
spatial-diversity by avoiding nearby redundant queries from the same image and
minimizes context-switching for the labeler. We show that this approach
significantly decreases labeling effort and improves rare object search on
realistic data with inherent class-imbalance and cluttered scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laielli_M/0/1/0/all/0/1"&gt;Michael Laielli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biamby_G/0/1/0/all/0/1"&gt;Giscard Biamby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loeffler_A/0/1/0/all/0/1"&gt;Adam Loeffler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phat Dat Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Ross Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1"&gt;Sayna Ebrahimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social NCE: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2012.11717v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11717</id>
        <link href="http://arxiv.org/abs/2012.11717"/>
        <updated>2021-08-23T01:36:35.416Z</updated>
        <summary type="html"><![CDATA[Learning socially-aware motion representations is at the core of recent
advances in multi-agent problems, such as human motion forecasting and robot
navigation in crowds. Despite promising progress, existing representations
learned with neural networks still struggle to generalize in closed-loop
predictions (e.g., output colliding trajectories). This issue largely arises
from the non-i.i.d. nature of sequential prediction in conjunction with
ill-distributed training data. Intuitively, if the training data only comes
from human behaviors in safe spaces, i.e., from "positive" examples, it is
difficult for learning algorithms to capture the notion of "negative" examples
like collisions. In this work, we aim to address this issue by explicitly
modeling negative examples through self-supervision: (i) we introduce a social
contrastive loss that regularizes the extracted motion representation by
discerning the ground-truth positive events from synthetic negative ones; (ii)
we construct informative negative samples based on our prior knowledge of rare
but dangerous circumstances. Our method substantially reduces the collision
rates of recent trajectory forecasting, behavioral cloning and reinforcement
learning algorithms, outperforming state-of-the-art methods on several
benchmarks. Our code is available at https://github.com/vita-epfl/social-nce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuejiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1"&gt;Qi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1"&gt;Alexandre Alahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AlphaPilot: Autonomous Drone Racing. (arXiv:2005.12813v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12813</id>
        <link href="http://arxiv.org/abs/2005.12813"/>
        <updated>2021-08-23T01:36:35.399Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel system for autonomous, vision-based drone racing
combining learned data abstraction, nonlinear filtering, and time-optimal
trajectory planning. The system has successfully been deployed at the first
autonomous drone racing world championship: the 2019 AlphaPilot Challenge.
Contrary to traditional drone racing systems, which only detect the next gate,
our approach makes use of any visible gate and takes advantage of multiple,
simultaneous gate detections to compensate for drift in the state estimate and
build a global map of the gates. The global map and drift-compensated state
estimate allow the drone to navigate through the race course even when the
gates are not immediately visible and further enable to plan a near
time-optimal path through the race course in real time based on approximate
drone dynamics. The proposed system has been demonstrated to successfully guide
the drone through tight race courses reaching speeds up to 8m/s and ranked
second at the 2019 AlphaPilot Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foehn_P/0/1/0/all/0/1"&gt;Philipp Foehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brescianini_D/0/1/0/all/0/1"&gt;Dario Brescianini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Elia Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cieslewski_T/0/1/0/all/0/1"&gt;Titus Cieslewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1"&gt;Mathias Gehrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1"&gt;Manasi Muglikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1"&gt;Davide Scaramuzza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaneAF: Robust Multi-Lane Detection with Affinity Fields. (arXiv:2103.12040v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12040</id>
        <link href="http://arxiv.org/abs/2103.12040"/>
        <updated>2021-08-23T01:36:35.391Z</updated>
        <summary type="html"><![CDATA[This study presents an approach to lane detection involving the prediction of
binary segmentation masks and per-pixel affinity fields. These affinity fields,
along with the binary masks, can then be used to cluster lane pixels
horizontally and vertically into corresponding lane instances in a
post-processing step. This clustering is achieved through a simple row-by-row
decoding process with little overhead; such an approach allows LaneAF to detect
a variable number of lanes without assuming a fixed or maximum number of lanes.
Moreover, this form of clustering is more interpretable in comparison to
previous visual clustering approaches, and can be analyzed to identify and
correct sources of error. Qualitative and quantitative results obtained on
popular lane detection datasets demonstrate the model's ability to detect and
cluster lanes effectively and robustly. Our proposed approach sets a new
state-of-the-art on the challenging CULane dataset and the recently introduced
Unsupervised LLAMAS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abualsaud_H/0/1/0/all/0/1"&gt;Hala Abualsaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sean Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;David Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Situ_K/0/1/0/all/0/1"&gt;Kenny Situ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction. (arXiv:2108.09274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09274</id>
        <link href="http://arxiv.org/abs/2108.09274"/>
        <updated>2021-08-23T01:36:35.383Z</updated>
        <summary type="html"><![CDATA[Pedestrian trajectory prediction is challenging due to its uncertain and
multimodal nature. While generative adversarial networks can learn a
distribution over future trajectories, they tend to predict out-of-distribution
samples when the distribution of future trajectories is a mixture of multiple,
possibly disconnected modes. To address this issue, we propose a
multi-generator model for pedestrian trajectory prediction. Each generator
specializes in learning a distribution over trajectories routing towards one of
the primary modes in the scene, while a second network learns a categorical
distribution over these generators, conditioned on the dynamics and scene
input. This architecture allows us to effectively sample from specialized
generators and to significantly reduce the out-of-distribution samples compared
to single generator methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dendorfer_P/0/1/0/all/0/1"&gt;Patrick Dendorfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elflein_S/0/1/0/all/0/1"&gt;Sven Elflein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation from Ensemble of Offsets for Head Pose Estimation. (arXiv:2108.09183v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09183</id>
        <link href="http://arxiv.org/abs/2108.09183"/>
        <updated>2021-08-23T01:36:35.372Z</updated>
        <summary type="html"><![CDATA[This paper proposes a method for estimating the head pose from a single
image. This estimation uses a neural network (NN) obtained in two stages. In
the first stage, we trained the base NN, which has one regression head and four
regression via classification (RvC) heads. We build the ensemble of offsets
using small offsets of face bounding boxes. In the second stage, we perform
knowledge distillation (KD) from the ensemble of offsets of the base NN into
the final NN with one RvC head. On the main test protocol, the use of the
offset ensemble improves the results of the base NN, and the KD improves the
results from the offset ensemble. The KD improves the results by an average of
7.7\% compared to the non-ensemble version. The proposed NN on the main test
protocol improves the state-of-the-art result on AFLW2000 and approaches, with
only a minimal gap, the state-of-the-art result on BIWI. Our NN uses only head
pose data, but the previous state-of-the-art model also uses facial landmarks
during training. We have made publicly available trained NNs and face bounding
boxes for the 300W-LP, AFLW, AFLW2000, and BIWI datasets. KD-ResNet152 has the
best results, and KD-ResNet18 has a better result on the AFLW2000 dataset than
any previous method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheka_A/0/1/0/all/0/1"&gt;Andrey Sheka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samun_V/0/1/0/all/0/1"&gt;Victor Samun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Data Aggregation and Transformations to Generalize across Visual Domains. (arXiv:2108.09208v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09208</id>
        <link href="http://arxiv.org/abs/2108.09208"/>
        <updated>2021-08-23T01:36:35.350Z</updated>
        <summary type="html"><![CDATA[Computer vision has flourished in recent years thanks to Deep Learning
advancements, fast and scalable hardware solutions and large availability of
structured image data. Convolutional Neural Networks trained on supervised
tasks with backpropagation learn to extract meaningful representations from raw
pixels automatically, and surpass shallow methods in image understanding.
Though convenient, data-driven feature learning is prone to dataset bias: a
network learns its parameters from training signals alone, and will usually
perform poorly if train and test distribution differ. To alleviate this
problem, research on Domain Generalization (DG), Domain Adaptation (DA) and
their variations is increasing. This thesis contributes to these research
topics by presenting novel and effective ways to solve the dataset bias problem
in its various settings. We propose new frameworks for Domain Generalization
and Domain Adaptation which make use of feature aggregation strategies and
visual transformations via data-augmentation and multi-task integration of
self-supervision. We also design an algorithm that adapts an object detection
model to any out of distribution sample at test time. With through
experimentation, we show how our proposed solutions outperform competitive
state-of-the-art approaches in established DG and DA benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DInnocente_A/0/1/0/all/0/1"&gt;Antono D&amp;#x27;Innocente&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Rule to Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection. (arXiv:2108.09178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09178</id>
        <link href="http://arxiv.org/abs/2108.09178"/>
        <updated>2021-08-23T01:36:35.331Z</updated>
        <summary type="html"><![CDATA[Supervised learning is constrained by the availability of labeled data, which
are especially expensive to acquire in the field of digital pathology. Making
use of open-source data for pre-training or using domain adaptation can be a
way to overcome this issue. However, pre-trained networks often fail to
generalize to new test domains that are not distributed identically due to
variations in tissue stainings, types, and textures. Additionally, current
domain adaptation methods mainly rely on fully-labeled source datasets. In this
work, we propose SRA, which takes advantage of self-supervised learning to
perform domain adaptation and removes the necessity of a fully-labeled source
dataset. SRA can effectively transfer the discriminative knowledge obtained
from a few labeled source domain's data to a new target domain without
requiring additional tissue annotations. Our method harnesses both domains'
structures by capturing visual similarity with intra-domain and cross-domain
self-supervision. Moreover, we present a generalized formulation of our
approach that allows the architecture to learn from multi-source domains. We
show that our proposed method outperforms baselines for domain adaptation of
colorectal tissue type classification and further validate our approach on our
in-house clinical cohort. The code and models are available open-source:
https://github.com/christianabbet/SRA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbet_C/0/1/0/all/0/1"&gt;Christian Abbet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Studer_L/0/1/0/all/0/1"&gt;Linda Studer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Andreas Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_H/0/1/0/all/0/1"&gt;Heather Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zlobec_I/0/1/0/all/0/1"&gt;Inti Zlobec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1"&gt;Behzad Bozorgtabar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1"&gt;Jean-Philippe Thiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance. (arXiv:2108.09174v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09174</id>
        <link href="http://arxiv.org/abs/2108.09174"/>
        <updated>2021-08-23T01:36:35.322Z</updated>
        <summary type="html"><![CDATA[Transparent objects, such as glass walls and doors, constitute architectural
obstacles hindering the mobility of people with low vision or blindness. For
instance, the open space behind glass doors is inaccessible, unless it is
correctly perceived and interacted with. However, traditional assistive
technologies rarely cover the segmentation of these safety-critical transparent
objects. In this paper, we build a wearable system with a novel dual-head
Transformer for Transparency (Trans4Trans) perception model, which can segment
general- and transparent objects. The two dense segmentation results are
further combined with depth information in the system to help users navigate
safely and assist them to negotiate transparent obstacles. We propose a
lightweight Transformer Parsing Module (TPM) to perform multi-scale feature
interpretation in the transformer-based decoder. Benefiting from TPM, the
double decoders can perform joint learning from corresponding datasets to
pursue robustness, meanwhile maintain efficiency on a portable GPU, with
negligible calculation increase. The entire Trans4Trans model is constructed in
a symmetrical encoder-decoder architecture, which outperforms state-of-the-art
methods on the test sets of Stanford2D3D and Trans10K-v2 datasets, obtaining
mIoU of 45.13% and 75.14%, respectively. Through a user study and various
pre-tests conducted in indoor and outdoor scenes, the usability and reliability
of our assistive system have been extensively verified. Meanwhile, the
Tran4Trans model has outstanding performances on driving scene datasets. On
Cityscapes, ACDC, and DADA-seg datasets corresponding to common environments,
adverse weather, and traffic accident scenarios, mIoU scores of 81.5%, 76.3%,
and 39.2% are obtained, demonstrating its high efficiency and robustness for
real-world transportation applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1"&gt;Angela Constantinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1"&gt;Kunyu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Karin M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1"&gt;Rainer Stiefelhagen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representations for Label Noise Require Fine-Tuning. (arXiv:2108.09154v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09154</id>
        <link href="http://arxiv.org/abs/2108.09154"/>
        <updated>2021-08-23T01:36:35.237Z</updated>
        <summary type="html"><![CDATA[In this paper we show that the combination of a Contrastive representation
with a label noise-robust classification head requires fine-tuning the
representation in order to achieve state-of-the-art performances. Since
fine-tuned representations are shown to outperform frozen ones, one can
conclude that noise-robust classification heads are indeed able to promote
meaningful representations if provided with a suitable starting point.
Experiments are conducted to draw a comprehensive picture of performances by
featuring six methods and nine noise instances of three different kinds (none,
symmetric, and asymmetric). In presence of noise the experiments show that fine
tuning of Contrastive representation allows the six methods to achieve better
results than end-to-end learning and represent a new reference compare to the
recent state of art. Results are also remarkable stable versus the noise level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nodet_P/0/1/0/all/0/1"&gt;Pierre Nodet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1"&gt;Vincent Lemaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondu_A/0/1/0/all/0/1"&gt;Alexis Bondu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornuejols_A/0/1/0/all/0/1"&gt;Antoine Cornu&amp;#xe9;jols&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier. (arXiv:2108.09135v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09135</id>
        <link href="http://arxiv.org/abs/2108.09135"/>
        <updated>2021-08-23T01:36:35.203Z</updated>
        <summary type="html"><![CDATA[The adversarial patch attack against image classification models aims to
inject adversarially crafted pixels within a localized restricted image region
(i.e., a patch) for inducing model misclassification. This attack can be
realized in the physical world by printing and attaching the patch to the
victim object and thus imposes a real-world threat to computer vision systems.
To counter this threat, we propose PatchCleanser as a certifiably robust
defense against adversarial patches that is compatible with any image
classifier. In PatchCleanser, we perform two rounds of pixel masking on the
input image to neutralize the effect of the adversarial patch. In the first
round of masking, we apply a set of carefully generated masks to the input
image and evaluate the model prediction on every masked image. If model
predictions on all one-masked images reach a unanimous agreement, we output the
agreed prediction label. Otherwise, we perform a second round of masking to
settle the disagreement, in which we evaluate model predictions on two-masked
images to robustly recover the correct prediction label. Notably, we can prove
that our defense will always make correct predictions on certain images against
any adaptive white-box attacker within our threat model, achieving certified
robustness. We extensively evaluate our defense on the ImageNet, ImageNette,
CIFAR-10, CIFAR-100, SVHN, and Flowers-102 datasets and demonstrate that our
defense achieves similar clean accuracy as state-of-the-art classification
models and also significantly improves certified robustness from prior works.
Notably, our defense can achieve 83.8% top-1 clean accuracy and 60.4% top-1
certified robust accuracy against a 2%-pixel square patch anywhere on the
1000-class ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1"&gt;Chong Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain-adaptive Hash for Networks. (arXiv:2108.09136v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09136</id>
        <link href="http://arxiv.org/abs/2108.09136"/>
        <updated>2021-08-23T01:36:35.196Z</updated>
        <summary type="html"><![CDATA[Abundant real-world data can be naturally represented by large-scale
networks, which demands efficient and effective learning algorithms. At the
same time, labels may only be available for some networks, which demands these
algorithms to be able to adapt to unlabeled networks. Domain-adaptive hash
learning has enjoyed considerable success in the computer vision community in
many practical tasks due to its lower cost in both retrieval time and storage
footprint. However, it has not been applied to multiple-domain networks. In
this work, we bridge this gap by developing an unsupervised domain-adaptive
hash learning method for networks, dubbed UDAH. Specifically, we develop four
{task-specific yet correlated} components: (1) network structure preservation
via a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,
(3) cross-domain intersected discriminators, and (4) semantic center alignment.
We conduct a wide range of experiments to evaluate the effectiveness and
efficiency of our method on a range of tasks including link prediction, node
classification, and neighbor recommendation. Our evaluation results demonstrate
that our model achieves better performance than the state-of-the-art
conventional discrete embedding methods over all the tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09151</id>
        <link href="http://arxiv.org/abs/2108.09151"/>
        <updated>2021-08-23T01:36:35.175Z</updated>
        <summary type="html"><![CDATA[Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiuniu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Antoni B. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Network (CNN) vs Visual Transformer (ViT) for Digital Holography. (arXiv:2108.09147v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09147</id>
        <link href="http://arxiv.org/abs/2108.09147"/>
        <updated>2021-08-23T01:36:35.167Z</updated>
        <summary type="html"><![CDATA[In Digital Holography (DH), it is crucial to extract the object distance from
a hologram in order to reconstruct its amplitude and phase. This step is called
auto-focusing and it is conventionally solved by first reconstructing a stack
of images and then by sharpening each reconstructed image using a focus metric
such as entropy or variance. The distance corresponding to the sharpest image
is considered the focal position. This approach, while effective, is
computationally demanding and time-consuming. In this paper, the determination
of the distance is performed by Deep Learning (DL). Two deep learning (DL)
architectures are compared: Convolutional Neural Network (CNN)and Visual
transformer (ViT). ViT and CNN are used to cope with the problem of
auto-focusing as a classification problem. Compared to a first attempt [11] in
which the distance between two consecutive classes was 100{\mu}m, our proposal
allows us to drastically reduce this distance to 1{\mu}m. Moreover, ViT reaches
similar accuracy and is more robust than CNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cuenat_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Cuenat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Couturier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields. (arXiv:2108.08931v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08931</id>
        <link href="http://arxiv.org/abs/2108.08931"/>
        <updated>2021-08-23T01:36:35.161Z</updated>
        <summary type="html"><![CDATA[Implicit neural representation is a recent approach to learn shape
collections as zero level-sets of neural networks, where each shape is
represented by a latent code. So far, the focus has been shape reconstruction,
while shape generalization was mostly left to generic encoder-decoder or
auto-decoder regularization.

In this paper we advocate deformation-aware regularization for implicit
neural representations, aiming at producing plausible deformations as latent
code changes. The challenge is that implicit representations do not capture
correspondences between different shapes, which makes it difficult to represent
and regularize their deformations. Thus, we propose to pair the implicit
representation of the shapes with an explicit, piecewise linear deformation
field, learned as an auxiliary function. We demonstrate that, by regularizing
these deformation fields, we can encourage the implicit neural representation
to induce natural deformations in the learned shape space, such as
as-rigid-as-possible deformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atzmon_M/0/1/0/all/0/1"&gt;Matan Atzmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1"&gt;David Novotny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Image-Based Camera Localization. (arXiv:2108.09112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09112</id>
        <link href="http://arxiv.org/abs/2108.09112"/>
        <updated>2021-08-23T01:36:35.154Z</updated>
        <summary type="html"><![CDATA[For several emerging technologies such as augmented reality, autonomous
driving and robotics, visual localization is a critical component. Directly
regressing camera pose/3D scene coordinates from the input image using deep
neural networks has shown great potential. However, such methods assume a
stationary data distribution with all scenes simultaneously available during
training. In this paper, we approach the problem of visual localization in a
continual learning setup -- whereby the model is trained on scenes in an
incremental manner. Our results show that similar to the classification domain,
non-stationary data induces catastrophic forgetting in deep networks for visual
localization. To address this issue, a strong baseline based on storing and
replaying images from a fixed buffer is proposed. Furthermore, we propose a new
sampling method based on coverage score (Buff-CS) that adapts the existing
sampling strategies in the buffering process to the problem of visual
localization. Results demonstrate consistent improvements over standard
buffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also
19Scenes by combining the former scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuzhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskar_Z/0/1/0/all/0/1"&gt;Zakaria Laskar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melekhov_I/0/1/0/all/0/1"&gt;Iaroslav Melekhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaotian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection. (arXiv:2108.09017v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09017</id>
        <link href="http://arxiv.org/abs/2108.09017"/>
        <updated>2021-08-23T01:36:35.146Z</updated>
        <summary type="html"><![CDATA[Few-shot object detection, which aims at detecting novel objects rapidly from
extremely few annotated examples of previously unseen classes, has attracted
significant research interest in the community. Most existing approaches employ
the Faster R-CNN as basic detection framework, yet, due to the lack of tailored
considerations for data-scarce scenario, their performance is often not
satisfactory. In this paper, we look closely into the conventional Faster R-CNN
and analyze its contradictions from two orthogonal perspectives, namely
multi-stage (RPN vs. RCNN) and multi-task (classification vs. localization). To
resolve these issues, we propose a simple yet effective architecture, named
Decoupled Faster R-CNN (DeFRCN). To be concrete, we extend Faster R-CNN by
introducing Gradient Decoupled Layer for multi-stage decoupling and
Prototypical Calibration Block for multi-task decoupling. The former is a novel
deep layer with redefining the feature-forward operation and gradient-backward
operation for decoupling its subsequent layer and preceding layer, and the
latter is an offline prototype-based classification model with taking the
proposals from detector as input and boosting the original classification
scores with additional pairwise scores for calibration. Extensive experiments
on multiple benchmarks show our framework is remarkably superior to other
existing approaches and establishes a new state-of-the-art in few-shot
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1"&gt;Limeng Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xi Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data. (arXiv:2108.09020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09020</id>
        <link href="http://arxiv.org/abs/2108.09020"/>
        <updated>2021-08-23T01:36:35.139Z</updated>
        <summary type="html"><![CDATA[Continual learning is the problem of learning and retaining knowledge through
time over multiple tasks and environments. Research has primarily focused on
the incremental classification setting, where new tasks/classes are added at
discrete time intervals. Such an "offline" setting does not evaluate the
ability of agents to learn effectively and efficiently, since an agent can
perform multiple learning epochs without any time limitation when a task is
added. We argue that "online" continual learning, where data is a single
continuous stream without task boundaries, enables evaluating both information
retention and online learning efficacy. In online continual learning, each
incoming small batch of data is first used for testing and then added to the
training set, making the problem truly online. Trained models are later
evaluated on historical data to assess information retention. We introduce a
new benchmark for online continual visual learning that exhibits large scale
and natural distribution shifts. Through a large-scale analysis, we identify
critical and previously unobserved phenomena of gradient-based optimization in
continual learning, and propose effective strategies for improving
gradient-based online continual learning with real data. The source code and
dataset are available in: https://github.com/IntelLabs/continuallearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhipeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sener_O/0/1/0/all/0/1"&gt;Ozan Sener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1"&gt;Vladlen Koltun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kompetenzerwerbsf\"orderung durch E-Assessment: Individuelle Kompetenzerfassung am Beispiel des Fachs Mathematik. (arXiv:2108.09072v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.09072</id>
        <link href="http://arxiv.org/abs/2108.09072"/>
        <updated>2021-08-23T01:36:35.116Z</updated>
        <summary type="html"><![CDATA[In this article, we present a concept of how micro- and e-assessments can be
used for the mathematical domain to automatically determine acquired and
missing individual skills and, based on these information, guide individuals to
acquire missing or additional skills in a software-supported process. The
models required for this concept are a digitally prepared and annotated
e-assessment item pool, a digital modeling of the domain that includes topics,
necessary competencies, as well as introductory and continuative material, as
well as a digital individual model, which can reliably record competencies and
integrates aspects about the loss of such.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meissner_R/0/1/0/all/0/1"&gt;Roy Meissner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruhland_C/0/1/0/all/0/1"&gt;Claudia Ruhland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihsberner_K/0/1/0/all/0/1"&gt;Katja Ihsberner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by Attack Re-generation. (arXiv:2108.09130v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09130</id>
        <link href="http://arxiv.org/abs/2108.09130"/>
        <updated>2021-08-23T01:36:35.110Z</updated>
        <summary type="html"><![CDATA[Face morphing attacks aim at creating face images that are verifiable to be
the face of multiple identities, which can lead to building faulty identity
links in operations like border checks. While creating a morphed face detector
(MFD), training on all possible attack types is essential to achieve good
detection performance. Therefore, investigating new methods of creating
morphing attacks drives the generalizability of MADs. Creating morphing attacks
was performed on the image level, by landmark interpolation, or on the
latent-space level, by manipulating latent vectors in a generative adversarial
network. The earlier results in varying blending artifacts and the latter
results in synthetic-like striping artifacts. This work presents the novel
morphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using
a GAN-based generation, as well as, eliminate the manipulation in the latent
space, resulting in visibly realistic morphed images compared to previous
works. The generated ReGenMorph appearance is compared to recent morphing
approaches and evaluated for face recognition vulnerability and attack
detectability, whether as known or unknown attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1"&gt;Naser Damer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1"&gt;Kiran Raja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sussmilch_M/0/1/0/all/0/1"&gt;Marius S&amp;#xfc;&amp;#xdf;milch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Sushma Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1"&gt;Fadi Boutros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Meiling Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1"&gt;Florian Kirchbuchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1"&gt;Raghavendra Ramachandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1"&gt;Arjan Kuijper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Joint Anomaly Detection and Classification. (arXiv:2108.08996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08996</id>
        <link href="http://arxiv.org/abs/2108.08996"/>
        <updated>2021-08-23T01:36:35.102Z</updated>
        <summary type="html"><![CDATA[Anomaly activities such as robbery, explosion, accidents, etc. need immediate
actions for preventing loss of human life and property in real world
surveillance systems. Although the recent automation in surveillance systems
are capable of detecting the anomalies, but they still need human efforts for
categorizing the anomalies and taking necessary preventive actions. This is due
to the lack of methodology performing both anomaly detection and classification
for real world scenarios. Thinking of a fully automatized surveillance system,
which is capable of both detecting and classifying the anomalies that need
immediate actions, a joint anomaly detection and classification method is a
pressing need. The task of joint detection and classification of anomalies
becomes challenging due to the unavailability of dense annotated videos
pertaining to anomalous classes, which is a crucial factor for training modern
deep architecture. Furthermore, doing it through manual human effort seems
impossible. Thus, we propose a method that jointly handles the anomaly
detection and classification in a single framework by adopting a
weakly-supervised learning paradigm. In weakly-supervised learning instead of
dense temporal annotations, only video-level labels are sufficient for
learning. The proposed model is validated on a large-scale publicly available
UCF-Crime dataset, achieving state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majhi_S/0/1/0/all/0/1"&gt;Snehashis Majhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Srijan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1"&gt;Francois Bremond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_R/0/1/0/all/0/1"&gt;Ratnakar Dash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_P/0/1/0/all/0/1"&gt;Pankaj Kumar Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding the Generative Capability of Adversarially Robust Classifiers. (arXiv:2108.09093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09093</id>
        <link href="http://arxiv.org/abs/2108.09093"/>
        <updated>2021-08-23T01:36:35.096Z</updated>
        <summary type="html"><![CDATA[Recently, some works found an interesting phenomenon that adversarially
robust classifiers can generate good images comparable to generative models. We
investigate this phenomenon from an energy perspective and provide a novel
explanation. We reformulate adversarial example generation, adversarial
training, and image generation in terms of an energy function. We find that
adversarial training contributes to obtaining an energy function that is flat
and has low energy around the real data, which is the key for generative
capability. Based on our new understanding, we further propose a better
adversarial training method, Joint Energy Adversarial Training (JEAT), which
can generate high-quality images and achieve new state-of-the-art robustness
under a wide range of attacks. The Inception Score of the images (CIFAR-10)
generated by JEAT is 8.80, much better than original robust classifiers (7.50).
In particular, we achieve new state-of-the-art robustness on CIFAR-10 (from
57.20% to 62.04%) and CIFAR-100 (from 30.03% to 30.18%) without extra training
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiacheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Rongxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised learning for medical image classification using imbalanced training data. (arXiv:2108.08956v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08956</id>
        <link href="http://arxiv.org/abs/2108.08956"/>
        <updated>2021-08-23T01:36:35.089Z</updated>
        <summary type="html"><![CDATA[Medical image classification is often challenging for two reasons: a lack of
labelled examples due to expensive and time-consuming annotation protocols, and
imbalanced class labels due to the relative scarcity of disease-positive
individuals in the wider population. Semi-supervised learning (SSL) methods
exist for dealing with a lack of labels, but they generally do not address the
problem of class imbalance. In this study we propose Adaptive Blended
Consistency Loss (ABCL), a drop-in replacement for consistency loss in
perturbation-based SSL methods. ABCL counteracts data skew by adaptively mixing
the target class distribution of the consistency loss in accordance with class
frequency. Our experiments with ABCL reveal improvements to unweighted average
recall on two different imbalanced medical image classification datasets when
compared with existing consistency losses that are not designed to counteract
class imbalance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Tri Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1"&gt;Aiden Nibali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Contactless Fingerprint Recognition System. (arXiv:2108.09048v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09048</id>
        <link href="http://arxiv.org/abs/2108.09048"/>
        <updated>2021-08-23T01:36:35.070Z</updated>
        <summary type="html"><![CDATA[Fingerprints are one of the most widely explored biometric traits.
Specifically, contact-based fingerprint recognition systems reign supreme due
to their robustness, portability and the extensive research work done in the
field. However, these systems suffer from issues such as hygiene, sensor
degradation due to constant physical contact, and latent fingerprint threats.
In this paper, we propose an approach for developing a contactless fingerprint
recognition system that captures finger photo from a distance using an image
sensor in a suitable environment. The captured finger photos are then processed
further to obtain global and local (minutiae-based) features. Specifically, a
Siamese convolutional neural network (CNN) is designed to extract global
features from a given finger photo. The proposed system computes matching
scores from CNN-based features and minutiae-based features. Finally, the two
scores are fused to obtain the final matching score between the probe and
reference fingerprint templates. Most importantly, the proposed system is
developed using the Nvidia Jetson Nano development kit, which allows us to
perform contactless fingerprint recognition in real-time with minimum latency
and acceptable matching accuracy. The performance of the proposed system is
evaluated on an in-house IITI contactless fingerprint dataset (IITI-CFD)
containing 105train and 100 test subjects. The proposed system achieves an
equal-error-rate of 2.19% on IITI-CFD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Attrish_A/0/1/0/all/0/1"&gt;Aman Attrish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharat_N/0/1/0/all/0/1"&gt;Nagasai Bharat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_V/0/1/0/all/0/1"&gt;Vijay Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanhangad_V/0/1/0/all/0/1"&gt;Vivek Kanhangad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLay: Benchmarking amodal layout estimation for autonomous driving. (arXiv:2108.09047v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.09047</id>
        <link href="http://arxiv.org/abs/2108.09047"/>
        <updated>2021-08-23T01:36:35.063Z</updated>
        <summary type="html"><![CDATA[Given an image or a video captured from a monocular camera, amodal layout
estimation is the task of predicting semantics and occupancy in bird's eye
view. The term amodal implies we also reason about entities in the scene that
are occluded or truncated in image space. While several recent efforts have
tackled this problem, there is a lack of standardization in task specification,
datasets, and evaluation protocols. We address these gaps with AutoLay, a
dataset and benchmark for amodal layout estimation from monocular images.
AutoLay encompasses driving imagery from two popular datasets: KITTI and
Argoverse. In addition to fine-grained attributes such as lanes, sidewalks, and
vehicles, we also provide semantically annotated 3D point clouds. We implement
several baselines and bleeding edge approaches, and release our data and code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mani_K/0/1/0/all/0/1"&gt;Kaustubh Mani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1"&gt;N. Sai Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1"&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1"&gt;K. Madhava Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09105</id>
        <link href="http://arxiv.org/abs/2108.09105"/>
        <updated>2021-08-23T01:36:35.056Z</updated>
        <summary type="html"><![CDATA[Vision-and-language navigation (VLN) aims to enable embodied agents to
navigate in realistic environments using natural language instructions. Given
the scarcity of domain-specific training data and the high diversity of image
and language inputs, the generalization of VLN agents to unseen environments
remains challenging. Recent methods explore pretraining to improve
generalization, however, the use of generic image-caption datasets or existing
small-scale VLN environments is suboptimal and results in limited improvements.
In this work, we introduce BnB, a large-scale and diverse in-domain VLN
dataset. We first collect image-caption (IC) pairs from hundreds of thousands
of listings from online rental marketplaces. Using IC pairs we next propose
automatic strategies to generate millions of VLN path-instruction (PI) pairs.
We further propose a shuffling loss that improves the learning of temporal
order inside PI pairs. We use BnB pretrain our Airbert model that can be
adapted to discriminative and generative settings and show that it outperforms
state of the art for Room-to-Room (R2R) navigation and Remote Referring
Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining
significantly increases performance on a challenging few-shot VLN evaluation,
where we train the model only on VLN instructions from a few houses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1"&gt;Pierre-Louis Guhur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1"&gt;Makarand Tapaswi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1"&gt;Ivan Laptev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions. (arXiv:2108.09108v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09108</id>
        <link href="http://arxiv.org/abs/2108.09108"/>
        <updated>2021-08-23T01:36:35.050Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel deep learning approach for single image defocus
deblurring based on inverse kernels. In a defocused image, the blur shapes are
similar among pixels although the blur sizes can spatially vary. To utilize the
property with inverse kernels, we exploit the observation that when only the
size of a defocus blur changes while keeping the shape, the shape of the
corresponding inverse kernel remains the same and only the scale changes. Based
on the observation, we propose a kernel-sharing parallel atrous convolutional
(KPAC) block specifically designed by incorporating the property of inverse
kernels for single image defocus deblurring. To effectively simulate the
invariant shapes of inverse kernels with different scales, KPAC shares the same
convolutional weights among multiple atrous convolution layers. To efficiently
simulate the varying scales of inverse kernels, KPAC consists of only a few
atrous convolution layers with different dilations and learns per-pixel scale
attentions to aggregate the outputs of the layers. KPAC also utilizes the shape
attention to combine the outputs of multiple convolution filters in each atrous
convolution layer, to deal with defocus blur with a slightly varying shape. We
demonstrate that our approach achieves state-of-the-art performance with a much
smaller number of parameters than previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1"&gt;Hyeongseok Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junyong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Sunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-boundary View Synthesis Towards Full-Frame Video Stabilization. (arXiv:2108.09041v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09041</id>
        <link href="http://arxiv.org/abs/2108.09041"/>
        <updated>2021-08-23T01:36:35.042Z</updated>
        <summary type="html"><![CDATA[Warping-based video stabilizers smooth camera trajectory by constraining each
pixel's displacement and warp stabilized frames from unstable ones accordingly.
However, since the view outside the boundary is not available during warping,
the resulting holes around the boundary of the stabilized frame must be
discarded (i.e., cropping) to maintain visual consistency, and thus does leads
to a tradeoff between stability and cropping ratio. In this paper, we make a
first attempt to address this issue by proposing a new Out-of-boundary View
Synthesis (OVS) method. By the nature of spatial coherence between adjacent
frames and within each frame, OVS extrapolates the out-of-boundary view by
aligning adjacent frames to each reference one. Technically, it first
calculates the optical flow and propagates it to the outer boundary region
according to the affinity, and then warps pixels accordingly. OVS can be
integrated into existing warping-based stabilizers as a plug-and-play module to
significantly improve the cropping ratio of the stabilized results. In
addition, stability is improved because the jitter amplification effect caused
by cropping and resizing is reduced. Experimental results on the NUS benchmark
show that OVS can improve the performance of five representative
state-of-the-art methods in terms of objective metrics and subjective visual
quality. The code is publicly available at
https://github.com/Annbless/OVS_Stabilization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yufei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video-based Person Re-identification with Spatial and Temporal Memory Networks. (arXiv:2108.09039v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09039</id>
        <link href="http://arxiv.org/abs/2108.09039"/>
        <updated>2021-08-23T01:36:35.036Z</updated>
        <summary type="html"><![CDATA[Video-based person re-identification (reID) aims to retrieve person videos
with the same identity as a query person across multiple cameras. Spatial and
temporal distractors in person videos, such as background clutter and partial
occlusions over frames, respectively, make this task much more challenging than
image-based person reID. We observe that spatial distractors appear
consistently in a particular location, and temporal distractors show several
patterns, e.g., partial occlusions occur in the first few frames, where such
patterns provide informative cues for predicting which frames to focus on
(i.e., temporal attentions). Based on this, we introduce a novel Spatial and
Temporal Memory Networks (STMN). The spatial memory stores features for spatial
distractors that frequently emerge across video frames, while the temporal
memory saves attentions which are optimized for typical temporal patterns in
person videos. We leverage the spatial and temporal memories to refine
frame-level person representations and to aggregate the refined frame-level
features into a sequence-level person representation, respectively, effectively
handling spatial and temporal distractors in person videos. We also introduce a
memory spread loss preventing our model from addressing particular items only
in the memories. Experimental results on standard benchmarks, including MARS,
DukeMTMC-VideoReID, and LS-VID, demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eom_C/0/1/0/all/0/1"&gt;Chanho Eom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Geon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junghyup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1"&gt;Bumsub Ham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvDrop: Adversarial Attack to DNNs by Dropping Information. (arXiv:2108.09034v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09034</id>
        <link href="http://arxiv.org/abs/2108.09034"/>
        <updated>2021-08-23T01:36:35.016Z</updated>
        <summary type="html"><![CDATA[Human can easily recognize visual objects with lost information: even losing
most details with only contour reserved, e.g. cartoon. However, in terms of
visual perception of Deep Neural Networks (DNNs), the ability for recognizing
abstract objects (visual objects with lost information) is still a challenge.
In this work, we investigate this issue from an adversarial viewpoint: will the
performance of DNNs decrease even for the images only losing a little
information? Towards this end, we propose a novel adversarial attack, named
\textit{AdvDrop}, which crafts adversarial examples by dropping existing
information of images. Previously, most adversarial attacks add extra
disturbing information on clean images explicitly. Opposite to previous works,
our proposed work explores the adversarial robustness of DNN models in a novel
perspective by dropping imperceptible details to craft adversarial examples. We
demonstrate the effectiveness of \textit{AdvDrop} by extensive experiments, and
show that this new type of adversarial examples is more difficult to be
defended by current defense systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1"&gt;Ranjie Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Dantong Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1"&gt;A. K. Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pixel Contrastive-Consistent Semi-Supervised Semantic Segmentation. (arXiv:2108.09025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09025</id>
        <link href="http://arxiv.org/abs/2108.09025"/>
        <updated>2021-08-23T01:36:35.009Z</updated>
        <summary type="html"><![CDATA[We present a novel semi-supervised semantic segmentation method which jointly
achieves two desiderata of segmentation model regularities: the label-space
consistency property between image augmentations and the feature-space
contrastive property among different pixels. We leverage the pixel-level L2
loss and the pixel contrastive loss for the two purposes respectively. To
address the computational efficiency issue and the false negative noise issue
involved in the pixel contrastive loss, we further introduce and investigate
several negative sampling techniques. Extensive experiments demonstrate the
state-of-the-art performance of our method (PC2Seg) with the DeepLab-v3+
architecture, in several challenging semi-supervised settings derived from the
VOC, Cityscapes, and COCO datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yuanyi Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bodi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zhiqiang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Time to Replace CNNs with Transformers for Medical Images?. (arXiv:2108.09038v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09038</id>
        <link href="http://arxiv.org/abs/2108.09038"/>
        <updated>2021-08-23T01:36:34.994Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have reigned for a decade as the de
facto approach to automated medical image diagnosis. Recently, vision
transformers (ViTs) have appeared as a competitive alternative to CNNs,
yielding similar levels of performance while possessing several interesting
properties that could prove beneficial for medical imaging tasks. In this work,
we explore whether it is time to move to transformer-based models or if we
should keep working with CNNs - can we trivially switch to transformers? If so,
what are the advantages and drawbacks of switching to ViTs for medical image
diagnosis? We consider these questions in a series of experiments on three
mainstream medical image datasets. Our findings show that, while CNNs perform
better when trained from scratch, off-the-shelf vision transformers using
default hyperparameters are on par with CNNs when pretrained on ImageNet, and
outperform their CNN counterparts when pretrained using self-supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1"&gt;Christos Matsoukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1"&gt;Johan Fredin Haslum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1"&gt;Magnus S&amp;#xf6;derberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised learning for joint SAR and multispectral land cover classification. (arXiv:2108.09075v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.09075</id>
        <link href="http://arxiv.org/abs/2108.09075"/>
        <updated>2021-08-23T01:36:34.987Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning techniques are gaining popularity due to their
capability of building models that are effective, even when scarce amounts of
labeled data are available. In this paper, we present a framework and specific
tasks for self-supervised training of multichannel models, such as the fusion
of multispectral and synthetic aperture radar images. We show that the proposed
self-supervised approach is highly effective at learning features that
correlate with the labels for land cover classification. This is enabled by an
explicit design of pretraining tasks which promotes bridging the gaps between
sensing modalities and exploiting the spectral characteristics of the input.
When limited labels are available, using the proposed self-supervised
pretraining and supervised finetuning for land cover classification with SAR
and multispectral data outperforms conventional approaches such as purely
supervised learning, initialization from training on Imagenet and recent
self-supervised approaches for computer vision tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Montanaro_A/0/1/0/all/0/1"&gt;Antonio Montanaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1"&gt;Diego Valsesia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fracastoro_G/0/1/0/all/0/1"&gt;Giulia Fracastoro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1"&gt;Enrico Magli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indoor Scene Generation from a Collection of Semantic-Segmented Depth Images. (arXiv:2108.09022v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09022</id>
        <link href="http://arxiv.org/abs/2108.09022"/>
        <updated>2021-08-23T01:36:34.959Z</updated>
        <summary type="html"><![CDATA[We present a method for creating 3D indoor scenes with a generative model
learned from a collection of semantic-segmented depth images captured from
different unknown scenes. Given a room with a specified size, our method
automatically generates 3D objects in a room from a randomly sampled latent
code. Different from existing methods that represent an indoor scene with the
type, location, and other properties of objects in the room and learn the scene
layout from a collection of complete 3D indoor scenes, our method models each
indoor scene as a 3D semantic scene volume and learns a volumetric generative
adversarial network (GAN) from a collection of 2.5D partial observations of 3D
scenes. To this end, we apply a differentiable projection layer to project the
generated 3D semantic scene volumes into semantic-segmented depth images and
design a new multiple-view discriminator for learning the complete 3D scene
volume from 2.5D semantic-segmented depth images. Compared to existing methods,
our method not only efficiently reduces the workload of modeling and acquiring
3D scenes for training, but also produces better object shapes and their
detailed layouts in the scene. We evaluate our method with different indoor
scene datasets and demonstrate the advantages of our method. We also extend our
method for generating 3D indoor scenes from semantic-segmented depth images
inferred from RGB images of real scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Jia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yu-Xiao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure-Preserving Deraining with Residue Channel Prior Guidance. (arXiv:2108.09079v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09079</id>
        <link href="http://arxiv.org/abs/2108.09079"/>
        <updated>2021-08-23T01:36:34.942Z</updated>
        <summary type="html"><![CDATA[Single image deraining is important for many high-level computer vision tasks
since the rain streaks can severely degrade the visibility of images, thereby
affecting the recognition and analysis of the image. Recently, many CNN-based
methods have been proposed for rain removal. Although these methods can remove
part of the rain streaks, it is difficult for them to adapt to real-world
scenarios and restore high-quality rain-free images with clear and accurate
structures. To solve this problem, we propose a Structure-Preserving Deraining
Network (SPDNet) with RCP guidance. SPDNet directly generates high-quality
rain-free images with clear and accurate structures under the guidance of RCP
but does not rely on any rain-generating assumptions. Specifically, we found
that the RCP of images contains more accurate structural information than rainy
images. Therefore, we introduced it to our deraining network to protect
structure information of the rain-free image. Meanwhile, a Wavelet-based
Multi-Level Module (WMLM) is proposed as the backbone for learning the
background information of rainy images and an Interactive Fusion Module (IFM)
is designed to make full use of RCP information. In addition, an iterative
guidance strategy is proposed to gradually improve the accuracy of RCP,
refining the result in a progressive path. Extensive experimental results on
both synthetic and real-world datasets demonstrate that the proposed model
achieves new state-of-the-art results. Code: https://github.com/Joyies/SPDNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1"&gt;Qiaosi Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juncheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qinyan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1"&gt;Faming Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guixu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1"&gt;Tieyong Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Spacecraft Relative Navigation Methods: A Survey. (arXiv:2108.08876v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.08876</id>
        <link href="http://arxiv.org/abs/2108.08876"/>
        <updated>2021-08-23T01:36:34.921Z</updated>
        <summary type="html"><![CDATA[Autonomous spacecraft relative navigation technology has been planned for and
applied to many famous space missions. The development of on-board electronics
systems has enabled the use of vision-based and LiDAR-based methods to achieve
better performances. Meanwhile, deep learning has reached great success in
different areas, especially in computer vision, which has also attracted the
attention of space researchers. However, spacecraft navigation differs from
ground tasks due to high reliability requirements but lack of large datasets.
This survey aims to systematically investigate the current deep learning-based
autonomous spacecraft relative navigation methods, focusing on concrete orbital
applications such as spacecraft rendezvous and landing on small bodies or the
Moon. The fundamental characteristics, primary motivations, and contributions
of deep learning-based relative navigation algorithms are first summarised from
three perspectives of spacecraft rendezvous, asteroid exploration, and terrain
navigation. Furthermore, popular visual tracking benchmarks and their
respective properties are compared and summarised. Finally, potential
applications are discussed, along with expected impediments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jianing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1"&gt;Duarte Rondao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1"&gt;Nabil Aouf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Underwater Image Enhancement Using an Analysis-Synthesis Network. (arXiv:2108.09023v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09023</id>
        <link href="http://arxiv.org/abs/2108.09023"/>
        <updated>2021-08-23T01:36:34.890Z</updated>
        <summary type="html"><![CDATA[Most deep models for underwater image enhancement resort to training on
synthetic datasets based on underwater image formation models. Although
promising performances have been achieved, they are still limited by two
problems: (1) existing underwater image synthesis models have an intrinsic
limitation, in which the homogeneous ambient light is usually randomly
generated and many important dependencies are ignored, and thus the synthesized
training data cannot adequately express characteristics of real underwater
environments; (2) most of deep models disregard lots of favorable underwater
priors and heavily rely on training data, which extensively limits their
application ranges. To address these limitations, a new underwater synthetic
dataset is first established, in which a revised ambient light synthesis
equation is embedded. The revised equation explicitly defines the complex
mathematical relationship among intensity values of the ambient light in RGB
channels and many dependencies such as surface-object depth, water types, etc,
which helps to better simulate real underwater scene appearances. Secondly, a
unified framework is proposed, named ANA-SYN, which can effectively enhance
underwater images under collaborations of priors (underwater domain knowledge)
and data information (underwater distortion distribution). The proposed
framework includes an analysis network and a synthesis network, one for priors
exploration and another for priors integration. To exploit more accurate
priors, the significance of each prior for the input image is explored in the
analysis network and an adaptive weighting module is designed to dynamically
recalibrate them. Meanwhile, a novel prior guidance module is introduced in the
synthesis network, which effectively aggregates the prior and data features and
thus provides better hybrid information to perform the more reasonable image
enhancement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengyong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Liquan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yufei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qiuyu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Domain-Invariant Adversarial Network for Deep Domain Generalization. (arXiv:2108.08995v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08995</id>
        <link href="http://arxiv.org/abs/2108.08995"/>
        <updated>2021-08-23T01:36:34.882Z</updated>
        <summary type="html"><![CDATA[Domain generalization approaches aim to learn a domain invariant prediction
model for unknown target domains from multiple training source domains with
different distributions. Significant efforts have recently been committed to
broad domain generalization, which is a challenging and topical problem in
machine learning and computer vision communities. Most previous domain
generalization approaches assume that the conditional distribution across the
domains remain the same across the source domains and learn a domain invariant
model by minimizing the marginal distributions. However, the assumption of a
stable conditional distribution of the training source domains does not really
hold in practice. The hyperplane learned from the source domains will easily
misclassify samples scattered at the boundary of clusters or far from their
corresponding class centres. To address the above two drawbacks, we propose a
discriminative domain-invariant adversarial network (DDIAN) for domain
generalization. The discriminativeness of the features are guaranteed through a
discriminative feature module and domain-invariant features are guaranteed
through the global domain and local sub-domain alignment modules. Extensive
experiments on several benchmarks show that DDIAN achieves better prediction on
unseen target data during training compared to state-of-the-art domain
generalization approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mohammad Mahfujur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1"&gt;Clinton Fookes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1"&gt;Sridha Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN Inversion for Out-of-Range Images with Geometric Transformations. (arXiv:2108.08998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08998</id>
        <link href="http://arxiv.org/abs/2108.08998"/>
        <updated>2021-08-23T01:36:34.874Z</updated>
        <summary type="html"><![CDATA[For successful semantic editing of real images, it is critical for a GAN
inversion method to find an in-domain latent code that aligns with the domain
of a pre-trained GAN model. Unfortunately, such in-domain latent codes can be
found only for in-range images that align with the training images of a GAN
model. In this paper, we propose BDInvert, a novel GAN inversion approach to
semantic editing of out-of-range images that are geometrically unaligned with
the training images of a GAN model. To find a latent code that is semantically
editable, BDInvert inverts an input out-of-range image into an alternative
latent space than the original latent space. We also propose a regularized
inversion method to find a solution that supports semantic editing in the
alternative space. Our experiments show that BDInvert effectively supports
semantic editing of out-of-range images with geometric transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1"&gt;Kyoungkook Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seongtae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Sunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Illicit Drug Trafficking Events on Instagram: A Deep Multimodal Multilabel Learning Approach. (arXiv:2108.08920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08920</id>
        <link href="http://arxiv.org/abs/2108.08920"/>
        <updated>2021-08-23T01:36:34.781Z</updated>
        <summary type="html"><![CDATA[Social media such as Instagram and Twitter have become important platforms
for marketing and selling illicit drugs. Detection of online illicit drug
trafficking has become critical to combat the online trade of illicit drugs.
However, the legal status often varies spatially and temporally; even for the
same drug, federal and state legislation can have different regulations about
its legality. Meanwhile, more drug trafficking events are disguised as a novel
form of advertising commenting leading to information heterogeneity.
Accordingly, accurate detection of illicit drug trafficking events (IDTEs) from
social media has become even more challenging. In this work, we conduct the
first systematic study on fine-grained detection of IDTEs on Instagram. We
propose to take a deep multimodal multilabel learning (DMML) approach to detect
IDTEs and demonstrate its effectiveness on a newly constructed dataset called
multimodal IDTE(MM-IDTE). Specifically, our model takes text and image data as
the input and combines multimodal information to predict multiple labels of
illicit drugs. Inspired by the success of BERT, we have developed a
self-supervised multimodal bidirectional transformer by jointly fine-tuning
pretrained text and image encoders. We have constructed a large-scale dataset
MM-IDTE with manually annotated multiple drug labels to support fine-grained
detection of illicit drugs. Extensive experimental results on the MM-IDTE
dataset show that the proposed DMML methodology can accurately detect IDTEs
even in the presence of special characters and style changes attempting to
evade detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuanbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Minglei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yanfang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Solutions for Utilizing Earth Observations in the "Big Data" era. (arXiv:2108.08886v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.08886</id>
        <link href="http://arxiv.org/abs/2108.08886"/>
        <updated>2021-08-23T01:36:34.774Z</updated>
        <summary type="html"><![CDATA[The ever-growing need of data preservation and their systematic analysis
contributing to sustainable development of the society spurred in the past
decade,numerous Big Data projects and initiatives are focusing on the Earth
Observation (EO). The number of Big Data EO applications has grown extremely
worldwide almost simultaneously with other scientific and technological areas
of the human knowledge due to the revolutionary technological progress in the
space and information technology sciences. The substantial contribution to this
development are the space programs of the renowned space agencies, such as
NASA, ESA,Roskosmos, JAXA, DLR, INPE, ISRO, CNES etc. A snap-shot of the
current Big Data sets from available satellite missions covering the Bulgarian
territory is also presented. This short overview of the geoscience Big Data
collection with a focus on EO will emphasize to the multiple Vs of EO in order
to provide a snapshot on the current state-of-the-art in EO data preservation
and manipulation. Main modern approaches for compressing, clustering and
modelling EO in the geoinformation science for Big Data analysis,
interpretation and visualization for a variety of applications are outlined.
Special attention is paid to the contemporary EO data modelling and
visualization systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Filchev_L/0/1/0/all/0/1"&gt;Lachezar Filchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pashova_L/0/1/0/all/0/1"&gt;Lyubka Pashova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1"&gt;Vasil Kolev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frye_S/0/1/0/all/0/1"&gt;Stuart Frye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08910</id>
        <link href="http://arxiv.org/abs/2108.08910"/>
        <updated>2021-08-23T01:36:34.767Z</updated>
        <summary type="html"><![CDATA[Though recent years have witnessed remarkable progress in single image
super-resolution (SISR) tasks with the prosperous development of deep neural
networks (DNNs), the deep learning methods are confronted with the computation
and memory consumption issues in practice, especially for resource-limited
platforms such as mobile devices. To overcome the challenge and facilitate the
real-time deployment of SISR tasks on mobile, we combine neural architecture
search with pruning search and propose an automatic search framework that
derives sparse super-resolution (SR) models with high image quality while
satisfying the real-time inference requirement. To decrease the search cost, we
leverage the weight sharing strategy by introducing a supernet and decouple the
search problem into three stages, including supernet construction,
compiler-aware architecture and pruning search, and compiler-aware pruning
ratio search. With the proposed framework, we are the first to achieve
real-time SR inference (with only tens of milliseconds per frame) for
implementing 720p resolution with competitive image quality (in terms of PSNR
and SSIM) on mobile platforms (Samsung Galaxy S20).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhan_Z/0/1/0/all/0/1"&gt;Zheng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yushu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jayaweera_M/0/1/0/all/0/1"&gt;Malith Jayaweera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaeli_D/0/1/0/all/0/1"&gt;David Kaeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling. (arXiv:2108.08965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08965</id>
        <link href="http://arxiv.org/abs/2108.08965"/>
        <updated>2021-08-23T01:36:34.758Z</updated>
        <summary type="html"><![CDATA[As an important task in multimodal context understanding, Text-VQA (Visual
Question Answering) aims at question answering through reading text information
in images. It differentiates from the original VQA task as Text-VQA requires
large amounts of scene-text relationship understanding, in addition to the
cross-modal grounding capability. In this paper, we propose Localize, Group,
and Select (LOGOS), a novel model which attempts to tackle this problem from
multiple aspects. LOGOS leverages two grounding tasks to better localize the
key information of the image, utilizes scene text clustering to group
individual OCR tokens, and learns to select the best answer from different
sources of OCR (Optical Character Recognition) texts. Experiments show that
LOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks
without using additional OCR annotation data. Ablation studies and analysis
demonstrate the capability of LOGOS to bridge different modalities and better
understand scene text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhen Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1"&gt;Carolyn P. Rose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Understanding with Privacy-Preserving BERT. (arXiv:2104.07504v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07504</id>
        <link href="http://arxiv.org/abs/2104.07504"/>
        <updated>2021-08-23T01:36:34.751Z</updated>
        <summary type="html"><![CDATA[Privacy preservation remains a key challenge in data mining and Natural
Language Understanding (NLU). Previous research shows that the input text or
even text embeddings can leak private information. This concern motivates our
research on effective privacy preservation approaches for pretrained Language
Models (LMs). We investigate the privacy and utility implications of applying
dx-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU
applications. More importantly, we further propose privacy-adaptive LM
pretraining methods and show that our approach can boost the utility of BERT
dramatically while retaining the same level of privacy protection. We also
quantify the level of privacy preservation and provide guidance on privacy
configuration. Our experiments and findings lay the groundwork for future
explorations of privacy-preserving NLU with pretrained LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1"&gt;Chen Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1"&gt;Weize Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1"&gt;Michael Bendersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1"&gt;Marc Najork&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Czert -- Czech BERT-like Model for Language Representation. (arXiv:2103.13031v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13031</id>
        <link href="http://arxiv.org/abs/2103.13031"/>
        <updated>2021-08-23T01:36:34.734Z</updated>
        <summary type="html"><![CDATA[This paper describes the training process of the first Czech monolingual
language representation models based on BERT and ALBERT architectures. We
pre-train our models on more than 340K of sentences, which is 50 times more
than multilingual models that include Czech data. We outperform the
multilingual models on 9 out of 11 datasets. In addition, we establish the new
state-of-the-art results on nine datasets. At the end, we discuss properties of
monolingual and multilingual models based upon our results. We publish all the
pre-trained and fine-tuned models freely for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1"&gt;Jakub Sido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Pra&amp;#x17e;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1"&gt;Pavel P&amp;#x159;ib&amp;#xe1;&amp;#x148;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasek_J/0/1/0/all/0/1"&gt;Jan Pa&amp;#x161;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1"&gt;Michal Sej&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1"&gt;Miloslav Konop&amp;#xed;k&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform for NLP Applications. (arXiv:2011.09463v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09463</id>
        <link href="http://arxiv.org/abs/2011.09463"/>
        <updated>2021-08-23T01:36:34.726Z</updated>
        <summary type="html"><![CDATA[The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hanjie Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators. (arXiv:2108.08891v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08891</id>
        <link href="http://arxiv.org/abs/2108.08891"/>
        <updated>2021-08-23T01:36:34.718Z</updated>
        <summary type="html"><![CDATA[We study how stochastic differential equation (SDE) based ideas can inspire
new modifications to existing algorithms for a set of problems in computer
vision. Loosely speaking, our formulation is related to both explicit and
implicit strategies for data augmentation and group equivariance, but is
derived from new results in the SDE literature on estimating infinitesimal
generators of a class of stochastic processes. If and when there is nominal
agreement between the needs of an application/task and the inherent properties
and behavior of the types of processes that we can efficiently handle, we
obtain a very simple and efficient plug-in layer that can be incorporated
within any existing network architecture, with minimal modification and only a
few additional parameters. We show promising experiments on a number of vision
tasks including few shot learning, point cloud transformers and deep
variational segmentation obtaining efficiency or performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zihang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vikas Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Sathya N. Ravi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatiotemporal Texture Reconstruction for Dynamic Objects Using a Single RGB-D Camera. (arXiv:2108.09007v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09007</id>
        <link href="http://arxiv.org/abs/2108.09007"/>
        <updated>2021-08-23T01:36:34.712Z</updated>
        <summary type="html"><![CDATA[This paper presents an effective method for generating a spatiotemporal
(time-varying) texture map for a dynamic object using a single RGB-D camera.
The input of our framework is a 3D template model and an RGB-D image sequence.
Since there are invisible areas of the object at a frame in a single-camera
setup, textures of such areas need to be borrowed from other frames. We
formulate the problem as an MRF optimization and define cost functions to
reconstruct a plausible spatiotemporal texture for a dynamic object.
Experimental results demonstrate that our spatiotemporal textures can reproduce
the active appearances of captured objects better than approaches using a
single texture map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyomin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jungeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1"&gt;Hyeonseo Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jaesik Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset -- Addressing the Noise-Latent Trade-Off. (arXiv:2108.08922v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08922</id>
        <link href="http://arxiv.org/abs/2108.08922"/>
        <updated>2021-08-23T01:36:34.706Z</updated>
        <summary type="html"><![CDATA[The state-of-the-art StyleGAN2 network supports powerful methods to create
and edit art, including generating random images, finding images "like" some
query, and modifying content or style. Further, recent advancements enable
training with small datasets. We apply these methods to synthesize card art, by
training on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are
essential for good synthesis, we find that, for small datasets, coarse-scale
noise interferes with latent variables because both control long-scale image
effects. We observe over-aggressive variation in art with changes in noise and
weak content control via latent variable edits. Here, we demonstrate that
training a modified StyleGAN2, where coarse-scale noise is suppressed, removes
these unwanted effects. We obtain a superior FID; changes in noise result in
local exploration of style; and identity control is markedly improved. These
results and analysis lead towards a GAN-assisted art synthesis tool for digital
artists of all skill levels, which can be used in film, games, or any creative
industry for artistic ideation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1"&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1"&gt;David Forsyth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs. (arXiv:2105.04850v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04850</id>
        <link href="http://arxiv.org/abs/2105.04850"/>
        <updated>2021-08-23T01:36:34.699Z</updated>
        <summary type="html"><![CDATA[The rise of personal assistants has made conversational question answering
(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art
methods for ConvQA over knowledge graphs (KGs) can only learn from crisp
question-answer pairs found in popular benchmarks. In reality, however, such
training data is hard to come by: users would rarely mark answers explicitly as
correct or wrong. In this work, we take a step towards a more natural learning
paradigm - from noisy and implicit feedback via question reformulations. A
reformulation is likely to be triggered by an incorrect system response,
whereas a new follow-up question could be a positive signal on the previous
turn's answer. We present a reinforcement learning model, termed CONQUER, that
can learn from a conversational stream of questions and reformulations. CONQUER
models the answering process as multiple agents walking in parallel on the KG,
where the walks are determined by actions sampled using a policy network. This
policy network takes the question along with the conversational context as
inputs and is trained via noisy rewards obtained from the reformulation
likelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark
with about 11k natural conversations containing around 205k reformulations.
Experiments show that CONQUER successfully learns to answer conversational
questions from noisy reward signals, significantly improving over a
state-of-the-art baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1"&gt;Magdalena Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1"&gt;Gerhard Weikum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few Shot Activity Recognition Using Variational Inference. (arXiv:2108.08990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08990</id>
        <link href="http://arxiv.org/abs/2108.08990"/>
        <updated>2021-08-23T01:36:34.681Z</updated>
        <summary type="html"><![CDATA[There has been a remarkable progress in learning a model which could
recognise novel classes with only a few labeled examples in the last few years.
Few-shot learning (FSL) for action recognition is a challenging task of
recognising novel action categories which are represented by few instances in
the training data. We propose a novel variational inference based architectural
framework (HF-AR) for few shot activity recognition. Our framework leverages
volume-preserving Householder Flow to learn a flexible posterior distribution
of the novel classes. This results in better performance as compared to
state-of-the-art few shot approaches for human activity recognition. approach
consists of base model and an adapter model. Our architecture consists of a
base model and an adapter model. The base model is trained on seen classes and
it computes an embedding that represent the spatial and temporal insights
extracted from the input video, e.g. combination of Resnet-152 and LSTM based
encoder-decoder model. The adapter model applies a series of Householder
transformations to compute a flexible posterior distribution that lends higher
accuracy in the few shot approach. Extensive experiments on three well-known
datasets: UCF101, HMDB51 and Something-Something-V2, demonstrate similar or
better performance on 1-shot and 5-shot classification as compared to
state-of-the-art few shot approaches that use only RGB frame sequence as input.
To the best of our knowledge, we are the first to explore variational inference
along with householder transformations to capture the full rank covariance
matrix of posterior distribution, for few shot learning in activity
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1"&gt;Siddhansh Narang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Virtual Markers for Articulated 3D Shapes. (arXiv:2108.09000v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09000</id>
        <link href="http://arxiv.org/abs/2108.09000"/>
        <updated>2021-08-23T01:36:34.674Z</updated>
        <summary type="html"><![CDATA[We propose deep virtual markers, a framework for estimating dense and
accurate positional information for various types of 3D data. We design a
concept and construct a framework that maps 3D points of 3D articulated models,
like humans, into virtual marker labels. To realize the framework, we adopt a
sparse convolutional neural network and classify 3D points of an articulated
model into virtual marker labels. We propose to use soft labels for the
classifier to learn rich and dense interclass relationships based on geodesic
distance. To measure the localization accuracy of the virtual markers, we test
FAUST challenge, and our result outperforms the state-of-the-art. We also
observe outstanding performance on the generalizability test, unseen data
evaluation, and different 3D data types (meshes and depth maps). We show
additional applications using the estimated virtual markers, such as non-rigid
registration, texture transfer, and realtime dense marker prediction from depth
maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyomin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jungeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kam_J/0/1/0/all/0/1"&gt;Jaewon Kam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jaesik Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Projection Generative Adversarial Networks for Conditional Image Generation. (arXiv:2108.09016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09016</id>
        <link href="http://arxiv.org/abs/2108.09016"/>
        <updated>2021-08-23T01:36:34.667Z</updated>
        <summary type="html"><![CDATA[Conditional Generative Adversarial Networks (cGANs) extend the standard
unconditional GAN framework to learning joint data-label distributions from
samples, and have been established as powerful generative models capable of
generating high-fidelity imagery. A challenge of training such a model lies in
properly infusing class information into its generator and discriminator. For
the discriminator, class conditioning can be achieved by either (1) directly
incorporating labels as input or (2) involving labels in an auxiliary
classification loss. In this paper, we show that the former directly aligns the
class-conditioned fake-and-real data distributions
$P(\text{image}|\text{class})$ ({\em data matching}), while the latter aligns
data-conditioned class distributions $P(\text{class}|\text{image})$ ({\em label
matching}). Although class separability does not directly translate to sample
quality and becomes a burden if classification itself is intrinsically
difficult, the discriminator cannot provide useful guidance for the generator
if features of distinct classes are mapped to the same point and thus become
inseparable. Motivated by this intuition, we propose a Dual Projection GAN
(P2GAN) model that learns to balance between {\em data matching} and {\em label
matching}. We then propose an improved cGAN model with Auxiliary Classification
that directly aligns the fake and real conditionals
$P(\text{class}|\text{image})$ by minimizing their $f$-divergence. Experiments
on a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world
datasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of
our proposed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Ligong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1"&gt;Martin Renqiang Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stathopoulos_A/0/1/0/all/0/1"&gt;Anastasis Stathopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruijiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1"&gt;Asim Kadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1"&gt;Dimitris Metaxas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility. (arXiv:2108.08943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08943</id>
        <link href="http://arxiv.org/abs/2108.08943"/>
        <updated>2021-08-23T01:36:34.658Z</updated>
        <summary type="html"><![CDATA[Recent learning-based multi-view stereo (MVS) methods show excellent
performance with dense cameras and small depth ranges. However, non-learning
based approaches still outperform for scenes with large depth ranges and
sparser wide-baseline views, in part due to their PatchMatch optimization over
pixelwise estimates of depth, normals, and visibility. In this paper, we
propose an end-to-end trainable PatchMatch-based MVS approach that combines
advantages of trainable costs and regularizations with pixelwise estimates. To
overcome the challenge of the non-differentiable PatchMatch optimization that
involves iterative sampling and hard decisions, we use reinforcement learning
to minimize expected photometric cost and maximize likelihood of ground truth
depth and normals. We incorporate normal estimation by using dilated patch
kernels, and propose a recurrent cost regularization that applies beyond
frontal plane-sweep algorithms to our pixelwise depth/normal estimates. We
evaluate our method on widely used MVS benchmarks, ETH3D and Tanks and Temples
(TnT), and compare to other state of the art learning based MVS models. On
ETH3D, our method outperforms other recent learning-based approaches and
performs comparably on advanced TnT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae Yong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeGol_J/0/1/0/all/0/1"&gt;Joseph DeGol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1"&gt;Chuhang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1"&gt;Derek Hoiem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIGLI: Conditional Image Generation from Language & Image. (arXiv:2108.08955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08955</id>
        <link href="http://arxiv.org/abs/2108.08955"/>
        <updated>2021-08-23T01:36:34.635Z</updated>
        <summary type="html"><![CDATA[Multi-modal generation has been widely explored in recent years. Current
research directions involve generating text based on an image or vice versa. In
this paper, we propose a new task called CIGLI: Conditional Image Generation
from Language and Image. Instead of generating an image based on text as in
text-image generation, this task requires the generation of an image from a
textual description and an image prompt. We designed a new dataset to ensure
that the text description describes information from both images, and that
solely analyzing the description is insufficient to generate an image. We then
propose a novel language-image fusion model which improves the performance over
two established baseline methods, as evaluated by quantitative (automatic) and
qualitative (human) evaluations. The code and dataset is available at
https://github.com/vincentlux/CIGLI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1"&gt;Lynnette Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1"&gt;Jared Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation of Lungs COVID Infected Regions by Attention Mechanism and Synthetic Data. (arXiv:2108.08895v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08895</id>
        <link href="http://arxiv.org/abs/2108.08895"/>
        <updated>2021-08-23T01:36:34.628Z</updated>
        <summary type="html"><![CDATA[Coronavirus has caused hundreds of thousands of deaths. Fatalities could
decrease if every patient could get suitable treatment by the healthcare
system. Machine learning, especially computer vision methods based on deep
learning, can help healthcare professionals diagnose and treat COVID-19
infected cases more efficiently. Hence, infected patients can get better
service from the healthcare system and decrease the number of deaths caused by
the coronavirus. This research proposes a method for segmenting infected lung
regions in a CT image. For this purpose, a convolutional neural network with an
attention mechanism is used to detect infected areas with complex patterns.
Attention blocks improve the segmentation accuracy by focusing on informative
parts of the image. Furthermore, a generative adversarial network generates
synthetic images for data augmentation and expansion of small available
datasets. Experimental results show the superiority of the proposed method
compared to some existing procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yazdekhasty_P/0/1/0/all/0/1"&gt;Parham Yazdekhasty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zindari_A/0/1/0/all/0/1"&gt;Ali Zindari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1"&gt;Zahra Nabizadeh-ShahreBabak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1"&gt;Nader Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Based Automatic Defect Analysis Framework for In-situ TEM Ion Irradiations. (arXiv:2108.08882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08882</id>
        <link href="http://arxiv.org/abs/2108.08882"/>
        <updated>2021-08-23T01:36:34.621Z</updated>
        <summary type="html"><![CDATA[Videos captured using Transmission Electron Microscopy (TEM) can encode
details regarding the morphological and temporal evolution of a material by
taking snapshots of the microstructure sequentially. However, manual analysis
of such video is tedious, error-prone, unreliable, and prohibitively
time-consuming if one wishes to analyze a significant fraction of frames for
even videos of modest length. In this work, we developed an automated TEM video
analysis system for microstructural features based on the advanced object
detection model called YOLO and tested the system on an in-situ ion irradiation
TEM video of dislocation loops formed in a FeCrAl alloy. The system provides
analysis of features observed in TEM including both static and dynamic
properties using the YOLO-based defect detection module coupled to a geometry
analysis module and a dynamic tracking module. Results show that the system can
achieve human comparable performance with an F1 score of 0.89 for fast,
consistent, and scalable frame-level defect analysis. This result is obtained
on a real but exceptionally clean and stable data set and more challenging data
sets may not achieve this performance. The dynamic tracking also enabled
evaluation of individual defect evolution like per defect growth rate at a
fidelity never before achieved using common human analysis methods. Our work
shows that automatically detecting and tracking interesting microstructures and
properties contained in TEM videos is viable and opens new doors for evaluating
materials dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mingren Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanzhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaguchi_Y/0/1/0/all/0/1"&gt;Yudai Yaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haley_J/0/1/0/all/0/1"&gt;Jack C. Haley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Field_K/0/1/0/all/0/1"&gt;Kevin G. Field&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1"&gt;Dane Morgan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Injection Attacks against CCD Image Sensors. (arXiv:2108.08881v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08881</id>
        <link href="http://arxiv.org/abs/2108.08881"/>
        <updated>2021-08-23T01:36:34.615Z</updated>
        <summary type="html"><![CDATA[Since cameras have become a crucial part in many safety-critical systems and
applications, such as autonomous vehicles and surveillance, a large body of
academic and non-academic work has shown attacks against their main component -
the image sensor. However, these attacks are limited to coarse-grained and
often suspicious injections because light is used as an attack vector.
Furthermore, due to the nature of optical attacks, they require the
line-of-sight between the adversary and the target camera.

In this paper, we present a novel post-transducer signal injection attack
against CCD image sensors, as they are used in professional, scientific, and
even military settings. We show how electromagnetic emanation can be used to
manipulate the image information captured by a CCD image sensor with the
granularity down to the brightness of individual pixels. We study the
feasibility of our attack and then demonstrate its effects in the scenario of
automatic barcode scanning. Our results indicate that the injected distortion
can disrupt automated vision-based intelligent systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_S/0/1/0/all/0/1"&gt;Sebastian K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1"&gt;Richard Baker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinovic_I/0/1/0/all/0/1"&gt;Ivan Martinovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CenterPoly: real-time instance segmentation using bounding polygons. (arXiv:2108.08923v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08923</id>
        <link href="http://arxiv.org/abs/2108.08923"/>
        <updated>2021-08-23T01:36:34.607Z</updated>
        <summary type="html"><![CDATA[We present a novel method, called CenterPoly, for real-time instance
segmentation using bounding polygons. We apply it to detect road users in dense
urban environments, making it suitable for applications in intelligent
transportation systems like automated vehicles. CenterPoly detects objects by
their center keypoint while predicting a fixed number of polygon vertices for
each object, thus performing detection and segmentation in parallel. Most of
the network parameters are shared by the network heads, making it fast and
lightweight enough to run at real-time speed. To properly convert mask
ground-truth to polygon ground-truth, we designed a vertex selection strategy
to facilitate the learning of the polygons. Additionally, to better segment
overlapping objects in dense urban scenes, we also train a relative depth
branch to determine which instances are closer and which are further, using
available weak annotations. We propose several models with different backbones
to show the possible speed / accuracy trade-offs. The models were trained and
evaluated on Cityscapes, KITTI and IDD and the results are reported on their
public benchmark, which are state-of-the-art at real-time speeds. Code is
available at https://github.com/hu64/CenterPoly]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perreault_H/0/1/0/all/0/1"&gt;Hughes Perreault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1"&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1"&gt;Nicolas Saunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heritier_M/0/1/0/all/0/1"&gt;Maguelonne H&amp;#xe9;ritier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards A Fairer Landmark Recognition Dataset. (arXiv:2108.08874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08874</id>
        <link href="http://arxiv.org/abs/2108.08874"/>
        <updated>2021-08-23T01:36:34.601Z</updated>
        <summary type="html"><![CDATA[We introduce a new landmark recognition dataset, which is created with a
focus on fair worldwide representation. While previous work proposes to collect
as many images as possible from web repositories, we instead argue that such
approaches can lead to biased data. To create a more comprehensive and
equitable dataset, we start by defining the fair relevance of a landmark to the
world population. These relevances are estimated by combining anonymized Google
Maps user contribution statistics with the contributors' demographic
information. We present a stratification approach and analysis which leads to a
much fairer coverage of the world, compared to existing datasets. The resulting
datasets are used to evaluate computer vision models as part of the the Google
Landmark Recognition and RetrievalChallenges 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1"&gt;Zu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1"&gt;Bingyi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Askew_C/0/1/0/all/0/1"&gt;Cam Askew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1"&gt;Jack Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1"&gt;Mike Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yilla_N/0/1/0/all/0/1"&gt;N&amp;#x27;Mah Fodiatu Yilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1"&gt;Tobias Weyand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Representation of Geometric Primitives for Graph-SLAM Optimization Using Decomposed Quadrics. (arXiv:2108.08957v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.08957</id>
        <link href="http://arxiv.org/abs/2108.08957"/>
        <updated>2021-08-23T01:36:34.594Z</updated>
        <summary type="html"><![CDATA[In Simultaneous Localization And Mapping (SLAM) problems, high-level
landmarks have the potential to build compact and informative maps compared to
traditional point-based landmarks. This work is focused on the parameterization
problem of high-level geometric primitives that are most frequently used,
including points, lines, planes, ellipsoids, cylinders, and cones. We first
present a unified representation of those geometric primitives using
\emph{quadrics} which yields a consistent and concise formulation. Then we
further study a decomposed model of quadrics that discloses the symmetric and
degenerated nature of quadrics. Based on the decomposition, we develop
physically meaningful quadrics factors in the settings of the graph-SLAM
problem. Finally, in simulation experiments, it is shown that the decomposed
formulation has better efficiency and robustness to observation noises than
baseline parameterizations. And in real-world experiments, the proposed
back-end framework is demonstrated to be capable of building compact and
regularized maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_W/0/1/0/all/0/1"&gt;Weikun Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Huai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yaoyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi defect detection and analysis of electron microscopy images with deep learning. (arXiv:2108.08883v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08883</id>
        <link href="http://arxiv.org/abs/2108.08883"/>
        <updated>2021-08-23T01:36:34.493Z</updated>
        <summary type="html"><![CDATA[Electron microscopy is widely used to explore defects in crystal structures,
but human detecting of defects is often time-consuming, error-prone, and
unreliable, and is not scalable to large numbers of images or real-time
analysis. In this work, we discuss the application of machine learning
approaches to find the location and geometry of different defect clusters in
irradiated steels. We show that a deep learning based Faster R-CNN analysis
system has a performance comparable to human analysis with relatively small
training data sets. This study proves the promising ability to apply deep
learning to assist the development of automated microscopy data analysis even
when multiple features are present and paves the way for fast, scalable, and
reliable analysis systems for massive amounts of modern electron microscopy
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mingren Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanzhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuhan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greaves_J/0/1/0/all/0/1"&gt;Jacob Greaves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Wei Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krakauer_N/0/1/0/all/0/1"&gt;Nathaniel J. Krakauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krudy_L/0/1/0/all/0/1"&gt;Leah Krudy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1"&gt;Jacob Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasan_V/0/1/0/all/0/1"&gt;Varun Sreenivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_B/0/1/0/all/0/1"&gt;Bryan Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torres_O/0/1/0/all/0/1"&gt;Oigimer Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Field_K/0/1/0/all/0/1"&gt;Kevin Field&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1"&gt;Dane Morgan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Conversation Generation Model via Equivalent Shared Memory Investigation. (arXiv:2108.09164v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09164</id>
        <link href="http://arxiv.org/abs/2108.09164"/>
        <updated>2021-08-23T01:36:34.452Z</updated>
        <summary type="html"><![CDATA[Conversation generation as a challenging task in Natural Language Generation
(NLG) has been increasingly attracting attention over the last years. A number
of recent works adopted sequence-to-sequence structures along with external
knowledge, which successfully enhanced the quality of generated conversations.
Nevertheless, few works utilized the knowledge extracted from similar
conversations for utterance generation. Taking conversations in customer
service and court debate domains as examples, it is evident that essential
entities/phrases, as well as their associated logic and inter-relationships can
be extracted and borrowed from similar conversation instances. Such information
could provide useful signals for improving conversation generation. In this
paper, we propose a novel reading and memory framework called Deep Reading
Memory Network (DRMN) which is capable of remembering useful information of
similar conversations for improving utterance generation. We apply our model to
two large-scale conversation datasets of justice and e-commerce fields.
Experiments prove that the proposed model outperforms the state-of-the-art
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1"&gt;Changzhen Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yating Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1"&gt;Adam Jatowt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Conghui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiejun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-identifying Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models. (arXiv:2101.00146v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00146</id>
        <link href="http://arxiv.org/abs/2101.00146"/>
        <updated>2021-08-23T01:36:34.438Z</updated>
        <summary type="html"><![CDATA[Electronic Medical Records contain clinical narrative text that is of great
potential value to medical researchers. However, this information is mixed with
Personally Identifiable Information that presents risks to patient and
clinician confidentiality. This paper presents an end-to-end de-identification
framework to automatically remove PII from hospital discharge summaries. Our
corpus included 600 hospital discharge summaries which were extracted from the
EMRs of two principal referral hospitals in Sydney, Australia. Our end-to-end
de-identification framework consists of three components: 1) Annotation:
labelling of PII in the hospital discharge summaries using five pre-defined
categories: person, address, date of birth, individual identification number,
phone/fax number; 2) Modelling: training six named entity recognition deep
learning base-models on balanced and imbalanced datasets; and evaluating
ensembles that combine all six base-models, the three base-models with the best
F1 scores and the three base-models with the best recall scores respectively,
using token-level majority voting and stacking methods; and 3)
De-identification: removing PII from the hospital discharge summaries. Our
results showed that the ensemble model combined using the stacking Support
Vector Machine method on the three base-models with the best F1 scores achieved
excellent results with a F1 score of 99.16% on the test set of our corpus. We
also evaluated the robustness of our modelling component on the 2014 i2b2
de-identification dataset. Our ensemble model, which uses the token-level
majority voting method on all six base-models, achieved the highest F1 score of
96.24% at strict entity matching and the highest F1 score of 98.64% at binary
token-level matching compared to two state-of-the-art methods. The end-to-end
framework provides a robust solution to de-identifying clinical narrative
corpuses safely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Leibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1"&gt;Oscar Perez-Concha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anthony Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1"&gt;Vicki Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1"&gt;Louisa Jorm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fastformer: Additive Attention is All You Need. (arXiv:2108.09084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09084</id>
        <link href="http://arxiv.org/abs/2108.09084"/>
        <updated>2021-08-23T01:36:34.415Z</updated>
        <summary type="html"><![CDATA[Transformer is a powerful model for text understanding. However, it is
inefficient due to its quadratic complexity to input sequence length. Although
there are many methods on Transformer acceleration, they are still either
inefficient on long sequences or not effective enough. In this paper, we
propose Fastformer, which is an efficient Transformer model based on additive
attention. In Fastformer, instead of modeling the pair-wise interactions
between tokens, we first use additive attention mechanism to model global
contexts, and then further transform each token representation based on its
interaction with global context representations. In this way, Fastformer can
achieve effective context modeling with linear complexity. Extensive
experiments on five datasets show that Fastformer is much more efficient than
many existing Transformer models and can meanwhile achieve comparable or even
better long text modeling performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08801</id>
        <link href="http://arxiv.org/abs/2106.08801"/>
        <updated>2021-08-23T01:36:34.405Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment aims at finding equivalent entities and
relations (i.e., mappings) between two KGs. The existing approaches utilize
either reasoning-based or semantic embedding-based techniques, but few studies
explore their combination. In this demonstration, we present PRASEMap, an
unsupervised KG alignment system that iteratively computes the Mappings with
both Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.
PRASEMap can support various embedding-based KG alignment approaches as the SE
module, and enables easy human computer interaction that additionally provides
an option for users to feed the mapping annotations back to the system for
better results. The demonstration showcases these features via a stand-alone
Web application with user friendly interfaces. The demo is available at
https://prasemap.qizhy.com.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs. (arXiv:2105.04850v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04850</id>
        <link href="http://arxiv.org/abs/2105.04850"/>
        <updated>2021-08-23T01:36:34.398Z</updated>
        <summary type="html"><![CDATA[The rise of personal assistants has made conversational question answering
(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art
methods for ConvQA over knowledge graphs (KGs) can only learn from crisp
question-answer pairs found in popular benchmarks. In reality, however, such
training data is hard to come by: users would rarely mark answers explicitly as
correct or wrong. In this work, we take a step towards a more natural learning
paradigm - from noisy and implicit feedback via question reformulations. A
reformulation is likely to be triggered by an incorrect system response,
whereas a new follow-up question could be a positive signal on the previous
turn's answer. We present a reinforcement learning model, termed CONQUER, that
can learn from a conversational stream of questions and reformulations. CONQUER
models the answering process as multiple agents walking in parallel on the KG,
where the walks are determined by actions sampled using a policy network. This
policy network takes the question along with the conversational context as
inputs and is trained via noisy rewards obtained from the reformulation
likelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark
with about 11k natural conversations containing around 205k reformulations.
Experiments show that CONQUER successfully learns to answer conversational
questions from noisy reward signals, significantly improving over a
state-of-the-art baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1"&gt;Magdalena Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1"&gt;Gerhard Weikum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Verification and Reranking for Open Fact Checking Over Tables. (arXiv:2012.15115v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15115</id>
        <link href="http://arxiv.org/abs/2012.15115"/>
        <updated>2021-08-23T01:36:34.390Z</updated>
        <summary type="html"><![CDATA[Structured information is an important knowledge source for automatic
verification of factual claims. Nevertheless, the majority of existing research
into this task has focused on textual data, and the few recent inquiries into
structured data have been for the closed-domain setting where appropriate
evidence for each claim is assumed to have already been retrieved. In this
paper, we investigate verification over structured data in the open-domain
setting, introducing a joint reranking-and-verification model which fuses
evidence documents in the verification component. Our open-domain model
achieves performance comparable to the closed-domain state-of-the-art on the
TabFact dataset, and demonstrates performance gains from the inclusion of
multiple tables as well as a significant improvement over a heuristic retrieval
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1"&gt;Michael Schlichtkrull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1"&gt;Vladimir Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1"&gt;Barlas O&amp;#x11f;uz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09119</id>
        <link href="http://arxiv.org/abs/2108.09119"/>
        <updated>2021-08-23T01:36:34.382Z</updated>
        <summary type="html"><![CDATA[With the development of deep learning (DL), natural language processing (NLP)
makes it possible for us to analyze and understand a large amount of language
texts. Accordingly, we can achieve a semantic communication in terms of joint
semantic source and channel coding over a noisy channel with the help of NLP.
However, the existing method to realize this goal is to use a fixed transformer
of NLP while ignoring the difference of semantic information contained in each
sentence. To solve this problem, we propose a new semantic communication system
based on Universal Transformer. Compared with the traditional transformer, an
adaptive circulation mechanism is introduced in the Universal Transformer.
Through the introduction of the circulation mechanism, the new semantic
communication system can be more flexible to transmit sentences with different
semantic information, and achieve better end-to-end performance under various
channel conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rongpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhifeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chenghui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honggang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Contrastive Learning for Interpretable Long Document Comparison. (arXiv:2108.09190v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09190</id>
        <link href="http://arxiv.org/abs/2108.09190"/>
        <updated>2021-08-23T01:36:34.363Z</updated>
        <summary type="html"><![CDATA[Recent advancements in deep learning techniques have transformed the area of
semantic text matching. However, most of the state-of-the-art models are
designed to operate with short documents such as tweets, user reviews,
comments, etc., and have fundamental limitations when applied to long-form
documents such as scientific papers, legal documents, and patents. When
handling such long documents, there are three primary challenges: (i) The
presence of different contexts for the same word throughout the document, (ii)
Small sections of contextually similar text between two documents, but
dissimilar text in the remaining parts -- this defies the basic understanding
of "similarity", and (iii) The coarse nature of a single global similarity
measure which fails to capture the heterogeneity of the document content. In
this paper, we describe CoLDE: Contrastive Long Document Encoder -- a
transformer-based framework that addresses these challenges and allows for
interpretable comparisons of long documents. CoLDE uses unique positional
embeddings and a multi-headed chunkwise attention layer in conjunction with a
contrastive learning framework to capture similarity at three different levels:
(i) high-level similarity scores between a pair of documents, (ii) similarity
scores between different sections within and across documents, and (iii)
similarity scores between different chunks in the same document and also other
documents. These fine-grained similarity scores aid in better interpretability.
We evaluate CoLDE on three long document datasets namely, ACL Anthology
publications, Wikipedia articles, and USPTO patents. Besides outperforming the
state-of-the-art methods on the document comparison task, CoLDE also proves
interpretable and robust to changes in document length and text perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Akshita Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakesh_V/0/1/0/all/0/1"&gt;Vineeth Rakesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrashekhar_J/0/1/0/all/0/1"&gt;Jaideep Chandrashekhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavedhi_A/0/1/0/all/0/1"&gt;Adithya Samavedhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v9 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.02358</id>
        <link href="http://arxiv.org/abs/1906.02358"/>
        <updated>2021-08-23T01:36:34.351Z</updated>
        <summary type="html"><![CDATA[Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1"&gt;Nisansa de Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08988</id>
        <link href="http://arxiv.org/abs/2108.08988"/>
        <updated>2021-08-23T01:36:34.343Z</updated>
        <summary type="html"><![CDATA[Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tunazzina Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1"&gt;Dan Goldwasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software Mentions in Scientific Articles. (arXiv:2108.09070v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09070</id>
        <link href="http://arxiv.org/abs/2108.09070"/>
        <updated>2021-08-23T01:36:34.336Z</updated>
        <summary type="html"><![CDATA[Knowledge about software used in scientific investigations is important for
several reasons, for instance, to enable an understanding of provenance and
methods involved in data handling. However, software is usually not formally
cited, but rather mentioned informally within the scholarly description of the
investigation, raising the need for automatic information extraction and
disambiguation. Given the lack of reliable ground truth data, we present
SoMeSci (Software Mentions in Science) a gold standard knowledge graph of
software mentions in scientific articles. It contains high quality annotations
(IRR: $\kappa{=}.82$) of 3756 software mentions in 1367 PubMed Central
articles. Besides the plain mention of the software, we also provide relation
labels for additional information, such as the version, the developer, a URL or
citations. Moreover, we distinguish between different types, such as
application, plugin or programming environment, as well as different types of
mentions, such as usage or creation. To the best of our knowledge, SoMeSci is
the most comprehensive corpus about software mentions in scientific articles,
providing training samples for Named Entity Recognition, Relation Extraction,
Entity Disambiguation, and Entity Linking. Finally, we sketch potential use
cases and provide baseline results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1"&gt;David Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensmann_F/0/1/0/all/0/1"&gt;Felix Bensmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1"&gt;Stefan Dietze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1"&gt;Frank Kr&amp;#xfc;ger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEDIT: Geographic-Enhanced and Dependency-Guided Tagging for Joint POI and Accessibility Extraction at Baidu Maps. (arXiv:2108.09104v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09104</id>
        <link href="http://arxiv.org/abs/2108.09104"/>
        <updated>2021-08-23T01:36:34.328Z</updated>
        <summary type="html"><![CDATA[Providing timely accessibility reminders of a point-of-interest (POI) plays a
vital role in improving user satisfaction of finding places and making visiting
decisions. However, it is difficult to keep the POI database in sync with the
real-world counterparts due to the dynamic nature of business changes. To
alleviate this problem, we formulate and present a practical solution that
jointly extracts POI mentions and identifies their coupled accessibility labels
from unstructured text. We approach this task as a sequence tagging problem,
where the goal is to produce <POI name, accessibility label> pairs from
unstructured text. This task is challenging because of two main issues: (1) POI
names are often newly-coined words so as to successfully register new entities
or brands and (2) there may exist multiple pairs in the text, which
necessitates dealing with one-to-many or many-to-one mapping to make each POI
coupled with its accessibility label. To this end, we propose a
Geographic-Enhanced and Dependency-guIded sequence Tagging (GEDIT) model to
concurrently address the two challenges. First, to alleviate challenge #1, we
develop a geographic-enhanced pre-trained model to learn the text
representations. Second, to mitigate challenge #2, we apply a relational graph
convolutional network to learn the tree node representations from the parsed
dependency tree. Finally, we construct a neural sequence tagging model by
integrating and feeding the previously pre-learned representations into a CRF
layer. Extensive experiments conducted on a real-world dataset demonstrate the
superiority and effectiveness of GEDIT. In addition, it has already been
deployed in production at Baidu Maps. Statistics show that the proposed
solution can save significant human effort and labor costs to deal with the
same amount of documents, which confirms that it is a practical way for POI
accessibility maintenance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yibo Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jizhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chunyuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Miao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bing Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Relation Modeling: Learning to Define Relations between Entities. (arXiv:2108.09241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09241</id>
        <link href="http://arxiv.org/abs/2108.09241"/>
        <updated>2021-08-23T01:36:34.317Z</updated>
        <summary type="html"><![CDATA[Relations between entities can be represented by different instances, e.g., a
sentence containing both entities or a fact in a Knowledge Graph (KG). However,
these instances may not well capture the general relations between entities,
may be difficult to understand by humans, even may not be found due to the
incompleteness of the knowledge source.

In this paper, we introduce the Open Relation Modeling task - given two
entities, generate a coherent sentence describing the relation between them. To
solve this task, we propose to teach machines to generate definition-like
relation descriptions by letting them learn from definitions of entities.
Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce
definitions conditioned on extracted entity pairs. To help PLMs reason between
entities and provide additional relational knowledge to PLMs for open relation
modeling, we incorporate reasoning paths in KGs and include a reasoning path
selection mechanism. We show that PLMs can select interpretable and informative
reasoning paths by confidence estimation, and the selected path can guide PLMs
to generate better relation descriptions. Experimental results show that our
model can generate concise but informative relation descriptions that capture
the representative characteristics of entities and relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kevin Chen-Chuan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1"&gt;Wen-mei Hwu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation for Singing Voice Detection. (arXiv:2011.04297v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04297</id>
        <link href="http://arxiv.org/abs/2011.04297"/>
        <updated>2021-08-23T01:36:34.293Z</updated>
        <summary type="html"><![CDATA[Singing Voice Detection (SVD) has been an active area of research in music
information retrieval (MIR). Currently, two deep neural network-based methods,
one based on CNN and the other on RNN, exist in literature that learn optimized
features for the voice detection (VD) task and achieve state-of-the-art
performance on common datasets. Both these models have a huge number of
parameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for
deployment on devices like smartphones or embedded sensors with limited
capacity in terms of memory and computation power. The most popular method to
address this issue is known as knowledge distillation in deep learning
literature (in addition to model compression) where a large pre-trained network
known as the teacher is used to train a smaller student network. Given the wide
applications of SVD in music information retrieval, to the best of our
knowledge, model compression for practical deployment has not yet been
explored. In this paper, efforts have been made to investigate this issue using
both conventional as well as ensemble knowledge distillation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Soumava Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+M_G/0/1/0/all/0/1"&gt;Gurunath Reddy M&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1"&gt;K Sreenivasa Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Partha Pratim Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Radiological Findings With Normalized Anatomical Information Using a Span-Based BERT Relation Extraction Model. (arXiv:2108.09211v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09211</id>
        <link href="http://arxiv.org/abs/2108.09211"/>
        <updated>2021-08-23T01:36:34.284Z</updated>
        <summary type="html"><![CDATA[Medical imaging is critical to the diagnosis and treatment of numerous
medical problems, including many forms of cancer. Medical imaging reports
distill the findings and observations of radiologists, creating an unstructured
textual representation of unstructured medical images. Large-scale use of this
text-encoded information requires converting the unstructured text to a
structured, semantic representation. We explore the extraction and
normalization of anatomical information in radiology reports that is associated
with radiological findings. We investigate this extraction and normalization
task using a span-based relation extraction model that jointly extracts
entities and relations using BERT. This work examines the factors that
influence extraction and normalization performance, including the body
part/organ system, frequency of occurrence, span length, and span diversity. It
discusses approaches for improving performance and creating high-quality
semantic representations of radiological phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1"&gt;Kevin Lybarger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damani_A/0/1/0/all/0/1"&gt;Aashka Damani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunn_M/0/1/0/all/0/1"&gt;Martin Gunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1"&gt;Ozlem Uzuner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1"&gt;Meliha Yetisgen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.09193</id>
        <link href="http://arxiv.org/abs/2108.09193"/>
        <updated>2021-08-23T01:36:34.271Z</updated>
        <summary type="html"><![CDATA[Transformer has achieved great success in NLP. However, the quadratic
complexity of the self-attention mechanism in Transformer makes it inefficient
in handling long sequences. Many existing works explore to accelerate
Transformers by computing sparse self-attention instead of a dense one, which
usually attends to tokens at certain positions or randomly selected tokens.
However, manually selected or random tokens may be uninformative for context
modeling. In this paper, we propose Smart Bird, which is an efficient and
effective Transformer with learnable sparse attention. In Smart Bird, we first
compute a sketched attention matrix with a single-head low-dimensional
Transformer, which aims to find potential important interactions between
tokens. We then sample token pairs based on their probability scores derived
from the sketched attention matrix to generate different sparse attention index
matrices for different attention heads. Finally, we select token embeddings
according to the index matrices to form the input of sparse attention networks.
Extensive experiments on six benchmark datasets for different tasks validate
the efficiency and effectiveness of Smart Bird in text modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09151</id>
        <link href="http://arxiv.org/abs/2108.09151"/>
        <updated>2021-08-23T01:36:34.206Z</updated>
        <summary type="html"><![CDATA[Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiuniu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Antoni B. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PASTO: Strategic Parameter Optimization in Recommendation Systems -- Probabilistic is Better than Deterministic. (arXiv:2108.09076v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.09076</id>
        <link href="http://arxiv.org/abs/2108.09076"/>
        <updated>2021-08-23T01:36:34.111Z</updated>
        <summary type="html"><![CDATA[Real-world recommendation systems often consist of two phases. In the first
phase, multiple predictive models produce the probability of different
immediate user actions. In the second phase, these predictions are aggregated
according to a set of 'strategic parameters' to meet a diverse set of business
goals, such as longer user engagement, higher revenue potential, or more
community/network interactions. In addition to building accurate predictive
models, it is also crucial to optimize this set of 'strategic parameters' so
that primary goals are optimized while secondary guardrails are not hurt. In
this setting with multiple and constrained goals, this paper discovers that a
probabilistic strategic parameter regime can achieve better value compared to
the standard regime of finding a single deterministic parameter. The new
probabilistic regime is to learn the best distribution over strategic parameter
choices and sample one strategic parameter from the distribution when each user
visits the platform. To pursue the optimal probabilistic solution, we formulate
the problem into a stochastic compositional optimization problem, in which the
unbiased stochastic gradient is unavailable. Our approach is applied in a
popular social network platform with hundreds of millions of daily users and
achieves +0.22% lift of user engagement in a recommendation task and +1.7% lift
in revenue in an advertising optimization scenario comparing to using the best
deterministic parameter strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weicong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hanlin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jingshuo Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guangxu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1"&gt;Qiang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Dong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xuezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1"&gt;Dongying Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1"&gt;Qiao Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Server Private Linear Computation with Joint and Individual Privacy Guarantees. (arXiv:2108.09271v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.09271</id>
        <link href="http://arxiv.org/abs/2108.09271"/>
        <updated>2021-08-23T01:36:34.087Z</updated>
        <summary type="html"><![CDATA[This paper considers the problem of multi-server Private Linear Computation,
under the joint and individual privacy guarantees. In this problem, identical
copies of a dataset comprised of $K$ messages are stored on $N$ non-colluding
servers, and a user wishes to obtain one linear combination of a $D$-subset of
messages belonging to the dataset. The goal is to design a scheme for
performing the computation such that the total amount of information downloaded
from the servers is minimized, while the privacy of the $D$ messages required
for the computation is protected. When joint privacy is required, the
identities of all of these $D$ messages must be kept private jointly, and when
individual privacy is required, the identity of every one of these $D$ messages
must be kept private individually. In this work, we characterize the capacity,
which is defined as the maximum achievable download rate, under both joint and
individual privacy requirements. In particular, we show that the capacity is
given respectively by ${(1+1/N+\dots+1/N^{K-D})^{-1}}$ or
${(1+1/N+\dots+1/N^{\lceil K/D\rceil-1})^{-1}}$ when joint or individual
privacy is required. Our converse proofs are based on reduction from two
variants of the multi-server Private Information Retrieval problem in the
presence of side information. Our achievability schemes build up on our
recently proposed schemes for single-server Private Linear Transformation and
the multi-server private computation scheme proposed by Sun and Jafar. Using
similar proof techniques, we also establish upper and lower bounds on the
capacity for the cases in which the user wants to compute $L$ (potentially more
than one) linear combinations. Specifically, we show that the capacity is upper
and lower bounded respectively by ${(1+1/N+\dots+1/N^{(K-D)/L})^{-1}}$ and
${(1+1/N+\dots+1/N^{K-D+L-1})^{-1}}$, when joint privacy is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1"&gt;Nahid Esmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1"&gt;Anoosheh Heidarzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PARIMA: Viewport Adaptive 360-Degree Video Streaming. (arXiv:2103.00981v3 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00981</id>
        <link href="http://arxiv.org/abs/2103.00981"/>
        <updated>2021-08-23T01:36:34.065Z</updated>
        <summary type="html"><![CDATA[With increasing advancements in technologies for capturing 360{\deg} videos,
advances in streaming such videos have become a popular research topic.
However, streaming 360{\deg} videos require high bandwidth, thus escalating the
need for developing optimized streaming algorithms. Researchers have proposed
various methods to tackle the problem, considering the network bandwidth or
attempt to predict future viewports in advance. However, most of the existing
works either (1) do not consider video contents to predict user viewport, or
(2) do not adapt to user preferences dynamically, or (3) require a lot of
training data for new videos, thus making them potentially unfit for video
streaming purposes. We develop PARIMA, a fast and efficient online viewport
prediction model that uses past viewports of users along with the trajectories
of prime objects as a representative of video content to predict future
viewports. We claim that the head movement of a user majorly depends upon the
trajectories of the prime objects in the video. We employ a pyramid-based
bitrate allocation scheme and perform a comprehensive evaluation of the
performance of PARIMA. In our evaluation, we show that PARIMA outperforms
state-of-the-art approaches, improving the Quality of Experience by over 30\%
while maintaining a short response time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_L/0/1/0/all/0/1"&gt;Lovish Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Sarthak Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1"&gt;Abhijit Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Sandip Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. (arXiv:2108.08877v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08877</id>
        <link href="http://arxiv.org/abs/2108.08877"/>
        <updated>2021-08-23T01:36:34.055Z</updated>
        <summary type="html"><![CDATA[We provide the first exploration of text-to-text transformers (T5) sentence
embeddings. Sentence embeddings are broadly useful for language processing
tasks. While T5 achieves impressive performance on language tasks cast as
sequence-to-sequence mapping problems, it is unclear how to produce sentence
embeddings from encoder-decoder models. We investigate three methods for
extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses
the full T5 encoder-decoder model. Our encoder-only models outperforms
BERT-based sentence embeddings on both transfer tasks and semantic textual
similarity (STS). Our encoder-decoder method achieves further improvement on
STS. Scaling up T5 from millions to billions of parameters is found to produce
consistent improvements on downstream tasks. Finally, we introduce a two-stage
contrastive learning approach that achieves a new state-of-art on STS using
sentence embeddings, outperforming both Sentence BERT and SimCSE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jianmo Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+%7BA%7Dbrego_G/0/1/0/all/0/1"&gt;Gustavo Hern&amp;#xe1;ndez {&amp;#xc1;}brego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Ji Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1"&gt;Keith B. Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1"&gt;Daniel Cer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining. (arXiv:2108.08983v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08983</id>
        <link href="http://arxiv.org/abs/2108.08983"/>
        <updated>2021-08-23T01:36:34.045Z</updated>
        <summary type="html"><![CDATA[Recently, the performance of Pre-trained Language Models (PLMs) has been
significantly improved by injecting knowledge facts to enhance their abilities
of language understanding. For medical domains, the background knowledge
sources are especially useful, due to the massive medical terms and their
complicated relations are difficult to understand in text. In this work, we
introduce SMedBERT, a medical PLM trained on large-scale medical corpora,
incorporating deep structured semantic knowledge from neighbors of
linked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to
learn heterogeneous-entity information, which infuses the semantic
representations of entity types into the homogeneous neighboring entity
structure. Apart from knowledge integration as external features, we propose to
employ the neighbors of linked-entities in the knowledge graph as additional
global contexts of text mentions, allowing them to communicate via shared
neighbors, thus enrich their semantic representations. Experiments demonstrate
that SMedBERT significantly outperforms strong baselines in various
knowledge-intensive Chinese medical tasks. It also improves the performance of
other tasks such as question answering, question matching and natural language
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Taolin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zerui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bite Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling. (arXiv:2108.08965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08965</id>
        <link href="http://arxiv.org/abs/2108.08965"/>
        <updated>2021-08-23T01:36:34.035Z</updated>
        <summary type="html"><![CDATA[As an important task in multimodal context understanding, Text-VQA (Visual
Question Answering) aims at question answering through reading text information
in images. It differentiates from the original VQA task as Text-VQA requires
large amounts of scene-text relationship understanding, in addition to the
cross-modal grounding capability. In this paper, we propose Localize, Group,
and Select (LOGOS), a novel model which attempts to tackle this problem from
multiple aspects. LOGOS leverages two grounding tasks to better localize the
key information of the image, utilizes scene text clustering to group
individual OCR tokens, and learns to select the best answer from different
sources of OCR (Optical Character Recognition) texts. Experiments show that
LOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks
without using additional OCR annotation data. Ablation studies and analysis
demonstrate the capability of LOGOS to bridge different modalities and better
understand scene text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhen Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1"&gt;Carolyn P. Rose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Neural Topic Modeling of Text Corpora. (arXiv:2108.08946v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08946</id>
        <link href="http://arxiv.org/abs/2108.08946"/>
        <updated>2021-08-23T01:36:34.015Z</updated>
        <summary type="html"><![CDATA[Topic Modeling refers to the problem of discovering the main topics that have
occurred in corpora of textual data, with solutions finding crucial
applications in numerous fields. In this work, inspired by the recent
advancements in the Natural Language Processing domain, we introduce FAME, an
open-source framework enabling an efficient mechanism of extracting and
incorporating textual features and utilizing them in discovering topics and
clustering text documents that are semantically similar in a corpus. These
features range from traditional approaches (e.g., frequency-based) to the most
recent auto-encoding embeddings from transformer-based language models such as
BERT model family. To demonstrate the effectiveness of this library, we
conducted experiments on the well-known News-Group dataset. The library is
available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1"&gt;Shayan Fazeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1"&gt;Majid Sarrafzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIGLI: Conditional Image Generation from Language & Image. (arXiv:2108.08955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08955</id>
        <link href="http://arxiv.org/abs/2108.08955"/>
        <updated>2021-08-23T01:36:34.004Z</updated>
        <summary type="html"><![CDATA[Multi-modal generation has been widely explored in recent years. Current
research directions involve generating text based on an image or vice versa. In
this paper, we propose a new task called CIGLI: Conditional Image Generation
from Language and Image. Instead of generating an image based on text as in
text-image generation, this task requires the generation of an image from a
textual description and an image prompt. We designed a new dataset to ensure
that the text description describes information from both images, and that
solely analyzing the description is insufficient to generate an image. We then
propose a novel language-image fusion model which improves the performance over
two established baseline methods, as evaluated by quantitative (automatic) and
qualitative (human) evaluations. The code and dataset is available at
https://github.com/vincentlux/CIGLI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1"&gt;Lynnette Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1"&gt;Jared Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software Mentions in Scientific Articles. (arXiv:2108.09070v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09070</id>
        <link href="http://arxiv.org/abs/2108.09070"/>
        <updated>2021-08-23T01:36:33.994Z</updated>
        <summary type="html"><![CDATA[Knowledge about software used in scientific investigations is important for
several reasons, for instance, to enable an understanding of provenance and
methods involved in data handling. However, software is usually not formally
cited, but rather mentioned informally within the scholarly description of the
investigation, raising the need for automatic information extraction and
disambiguation. Given the lack of reliable ground truth data, we present
SoMeSci (Software Mentions in Science) a gold standard knowledge graph of
software mentions in scientific articles. It contains high quality annotations
(IRR: $\kappa{=}.82$) of 3756 software mentions in 1367 PubMed Central
articles. Besides the plain mention of the software, we also provide relation
labels for additional information, such as the version, the developer, a URL or
citations. Moreover, we distinguish between different types, such as
application, plugin or programming environment, as well as different types of
mentions, such as usage or creation. To the best of our knowledge, SoMeSci is
the most comprehensive corpus about software mentions in scientific articles,
providing training samples for Named Entity Recognition, Relation Extraction,
Entity Disambiguation, and Entity Linking. Finally, we sketch potential use
cases and provide baseline results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1"&gt;David Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensmann_F/0/1/0/all/0/1"&gt;Felix Bensmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1"&gt;Stefan Dietze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1"&gt;Frank Kr&amp;#xfc;ger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01440</id>
        <link href="http://arxiv.org/abs/2108.01440"/>
        <updated>2021-08-23T01:36:33.980Z</updated>
        <summary type="html"><![CDATA[E-commerce sites strive to provide users the most timely relevant information
in order to reduce shopping frictions and increase customer satisfaction. Multi
armed bandit models (MAB) as a type of adaptive optimization algorithms provide
possible approaches for such purposes. In this paper, we analyze using three
classic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper
confidence bound 1 (UCB1) for dynamic content recommendations, and walk through
the process of developing these algorithms internally to solve a real world
e-commerce use case. First, we analyze the three MAB algorithms using simulated
purchasing datasets with non-stationary reward distributions to simulate the
possible time-varying customer preferences, where the traffic allocation
dynamics and the accumulative rewards of different algorithms are studied.
Second, we compare the accumulative rewards of the three MAB algorithms with
more than 1,000 trials using actual historical A/B test datasets. We find that
the larger difference between the success rates of competing recommendations
the more accumulative rewards the MAB algorithms can achieve. In addition, we
find that TS shows the highest average accumulative rewards under different
testing scenarios. Third, we develop a batch-updated MAB algorithm to overcome
the delayed reward issue in e-commerce and enable an online content
optimization on our App homepage. For a state-of-the-art comparison, a real A/B
test among our batch-updated MAB algorithm, a third-party MAB solution, and the
default business logic are conducted. The result shows that our batch-updated
MAB algorithm outperforms the counterparts and achieves 6.13% relative
click-through rate (CTR) increase and 16.1% relative conversion rate (CVR)
increase compared to the default experience, and 2.9% relative CTR increase and
1.4% relative CVR increase compared to the external MAB service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1"&gt;Ding Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1"&gt;Becky West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiquan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jinzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.09105</id>
        <link href="http://arxiv.org/abs/2108.09105"/>
        <updated>2021-08-23T01:36:33.966Z</updated>
        <summary type="html"><![CDATA[Vision-and-language navigation (VLN) aims to enable embodied agents to
navigate in realistic environments using natural language instructions. Given
the scarcity of domain-specific training data and the high diversity of image
and language inputs, the generalization of VLN agents to unseen environments
remains challenging. Recent methods explore pretraining to improve
generalization, however, the use of generic image-caption datasets or existing
small-scale VLN environments is suboptimal and results in limited improvements.
In this work, we introduce BnB, a large-scale and diverse in-domain VLN
dataset. We first collect image-caption (IC) pairs from hundreds of thousands
of listings from online rental marketplaces. Using IC pairs we next propose
automatic strategies to generate millions of VLN path-instruction (PI) pairs.
We further propose a shuffling loss that improves the learning of temporal
order inside PI pairs. We use BnB pretrain our Airbert model that can be
adapted to discriminative and generative settings and show that it outperforms
state of the art for Room-to-Room (R2R) navigation and Remote Referring
Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining
significantly increases performance on a challenging few-shot VLN evaluation,
where we train the model only on VLN instructions from a few houses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1"&gt;Pierre-Louis Guhur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1"&gt;Makarand Tapaswi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1"&gt;Ivan Laptev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources. (arXiv:2008.10327v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10327</id>
        <link href="http://arxiv.org/abs/2008.10327"/>
        <updated>2021-08-23T01:36:33.941Z</updated>
        <summary type="html"><![CDATA[Machine Reading Comprehension (MRC) aims to extract answers to questions
given a passage. It has been widely studied recently, especially in open
domains. However, few efforts have been made on closed-domain MRC, mainly due
to the lack of large-scale training data. In this paper, we introduce a
multi-target MRC task for the medical domain, whose goal is to predict answers
to medical questions and the corresponding support sentences from medical
information sources simultaneously, in order to ensure the high reliability of
medical knowledge serving. A high-quality dataset is manually constructed for
the purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with
detailed analysis conducted. We further propose the Chinese medical BERT model
for the task (CMedBERT), which fuses medical knowledge into pre-trained
language models by the dynamic fusion mechanism of heterogeneous features and
the multi-task learning strategy. Experiments show that CMedBERT consistently
outperforms strong baselines by fusing context-aware and knowledge-aware token
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Taolin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bite Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can We Model News Recommendation as Sequential Recommendation?. (arXiv:2108.08984v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08984</id>
        <link href="http://arxiv.org/abs/2108.08984"/>
        <updated>2021-08-23T01:36:33.929Z</updated>
        <summary type="html"><![CDATA[News recommendation is often modeled as a sequential recommendation task,
which assumes that there are rich short-term dependencies over historical
clicked news. However, in news recommendation scenarios users usually have
strong preferences on the temporal diversity of news information and may not
tend to click similar news successively, which is very different from many
sequential recommendation scenarios such as e-commerce recommendation. In this
paper, we study whether news recommendation can be regarded as a standard
sequential recommendation problem. Through extensive experiments on two
real-world datasets, we find that modeling news recommendation as a sequential
recommendation problem is suboptimal. To handle this challenge, we further
propose a temporal diversity-aware news recommendation method that can promote
candidate news that are diverse from recently clicked news, which can help
predict future clicks more accurately. Experiments show that our approach can
consistently improve various news recommendation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Recommender System for Scientific Datasets and Analysis Pipelines. (arXiv:2108.09275v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09275</id>
        <link href="http://arxiv.org/abs/2108.09275"/>
        <updated>2021-08-23T01:36:33.898Z</updated>
        <summary type="html"><![CDATA[Scientific datasets and analysis pipelines are increasingly being shared
publicly in the interest of open science. However, mechanisms are lacking to
reliably identify which pipelines and datasets can appropriately be used
together. Given the increasing number of high-quality public datasets and
pipelines, this lack of clear compatibility threatens the findability and
reusability of these resources. We investigate the feasibility of a
collaborative filtering system to recommend pipelines and datasets based on
provenance records from previous executions. We evaluate our system using
datasets and pipelines extracted from the Canadian Open Neuroscience Platform,
a national initiative for open neuroscience. The recommendations provided by
our system (AUC$=0.83$) are significantly better than chance and outperform
recommendations made by domain experts using their previous knowledge as well
as pipeline and dataset descriptions (AUC$=0.63$). In particular, domain
experts often neglect low-level technical aspects of a pipeline-dataset
interaction, such as the level of pre-processing, which are captured by a
provenance-based system. We conclude that provenance-based pipeline and dataset
recommenders are feasible and beneficial to the sharing and usage of
open-science resources. Future work will focus on the collection of more
comprehensive provenance traces, and on deploying the system in production.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_M/0/1/0/all/0/1"&gt;Mandana Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1"&gt;Gregory Kiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1"&gt;Tristan Glatard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-identifying Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models. (arXiv:2101.00146v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00146</id>
        <link href="http://arxiv.org/abs/2101.00146"/>
        <updated>2021-08-23T01:36:33.877Z</updated>
        <summary type="html"><![CDATA[Electronic Medical Records contain clinical narrative text that is of great
potential value to medical researchers. However, this information is mixed with
Personally Identifiable Information that presents risks to patient and
clinician confidentiality. This paper presents an end-to-end de-identification
framework to automatically remove PII from hospital discharge summaries. Our
corpus included 600 hospital discharge summaries which were extracted from the
EMRs of two principal referral hospitals in Sydney, Australia. Our end-to-end
de-identification framework consists of three components: 1) Annotation:
labelling of PII in the hospital discharge summaries using five pre-defined
categories: person, address, date of birth, individual identification number,
phone/fax number; 2) Modelling: training six named entity recognition deep
learning base-models on balanced and imbalanced datasets; and evaluating
ensembles that combine all six base-models, the three base-models with the best
F1 scores and the three base-models with the best recall scores respectively,
using token-level majority voting and stacking methods; and 3)
De-identification: removing PII from the hospital discharge summaries. Our
results showed that the ensemble model combined using the stacking Support
Vector Machine method on the three base-models with the best F1 scores achieved
excellent results with a F1 score of 99.16% on the test set of our corpus. We
also evaluated the robustness of our modelling component on the 2014 i2b2
de-identification dataset. Our ensemble model, which uses the token-level
majority voting method on all six base-models, achieved the highest F1 score of
96.24% at strict entity matching and the highest F1 score of 98.64% at binary
token-level matching compared to two state-of-the-art methods. The end-to-end
framework provides a robust solution to de-identifying clinical narrative
corpuses safely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Leibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1"&gt;Oscar Perez-Concha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anthony Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1"&gt;Vicki Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1"&gt;Louisa Jorm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation. (arXiv:2108.09141v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.09141</id>
        <link href="http://arxiv.org/abs/2108.09141"/>
        <updated>2021-08-23T01:36:33.843Z</updated>
        <summary type="html"><![CDATA[Recommender system plays a crucial role in modern E-commerce platform. Due to
the lack of historical interactions between users and items, cold-start
recommendation is a challenging problem. In order to alleviate the cold-start
issue, most existing methods introduce content and contextual information as
the auxiliary information. Nevertheless, these methods assume the recommended
items behave steadily over time, while in a typical E-commerce scenario, items
generally have very different performances throughout their life period. In
such a situation, it would be beneficial to consider the long-term return from
the item perspective, which is usually ignored in conventional methods.
Reinforcement learning (RL) naturally fits such a long-term optimization
problem, in which the recommender could identify high potential items,
proactively allocate more user impressions to boost their growth, therefore
improve the multi-period cumulative gains. Inspired by this idea, we model the
process as a Partially Observable and Controllable Markov Decision Process
(POC-MDP), and propose an actor-critic RL framework (RL-LTV) to incorporate the
item lifetime values (LTV) into the recommendation. In RL-LTV, the critic
studies historical trajectories of items and predict the future LTV of fresh
item, while the actor suggests a score-based policy which maximizes the future
LTV expectation. Scores suggested by the actor are then combined with classical
ranking scores in a dual-rank framework, therefore the recommendation is
balanced with the LTV consideration. Our method outperforms the strong live
baseline with a relative improvement of 8.67% and 18.03% on IPV and GMV of
cold-start items, on one of the largest E-commerce platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Luo Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1"&gt;Qin Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bingqing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13322</id>
        <link href="http://arxiv.org/abs/2011.13322"/>
        <updated>2021-08-23T01:36:33.538Z</updated>
        <summary type="html"><![CDATA[Skeleton-based human action recognition has attracted much attention with the
prevalence of accessible depth sensors. Recently, graph convolutional networks
(GCNs) have been widely used for this task due to their powerful capability to
model graph data. The topology of the adjacency graph is a key factor for
modeling the correlations of the input skeletons. Thus, previous methods mainly
focus on the design/learning of the graph topology. But once the topology is
learned, only a single-scale feature and one transformation exist in each layer
of the networks. Many insights, such as multi-scale information and multiple
sets of transformations, that have been proven to be very effective in
convolutional neural networks (CNNs), have not been investigated in GCNs. The
reason is that, due to the gap between graph-structured skeleton data and
conventional image/video data, it is very challenging to embed these insights
into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy
in GCNs for skeleton sequence processing. Specifically, we design a simple and
highly modularized graph convolutional network architecture for skeleton-based
action recognition. Our network is constructed by repeating a building block
that aggregates multi-granularity information from both the spatial and
temporal paths. Extensive experiments demonstrate that our network outperforms
state-of-the-art methods by a significant margin with only 1/5 of the
parameters and 1/10 of the FLOPs. Code is available at
https://github.com/yellowtownhz/STIGCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xinmei Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metaverse for Social Good: A University Campus Prototype. (arXiv:2108.08985v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.08985</id>
        <link href="http://arxiv.org/abs/2108.08985"/>
        <updated>2021-08-23T01:36:33.460Z</updated>
        <summary type="html"><![CDATA[In recent years, the metaverse has attracted enormous attention from around
the world with the development of related technologies. The expected metaverse
should be a realistic society with more direct and physical interactions, while
the concepts of race, gender, and even physical disability would be weakened,
which would be highly beneficial for society. However, the development of
metaverse is still in its infancy, with great potential for improvement.
Regarding metaverse's huge potential, industry has already come forward with
advance preparation, accompanied by feverish investment, but there are few
discussions about metaverse in academia to scientifically guide its
development. In this paper, we highlight the representative applications for
social good. Then we propose a three-layer metaverse architecture from a macro
perspective, containing infrastructure, interaction, and ecosystem. Moreover,
we journey toward both a historical and novel metaverse with a detailed
timeline and table of specific attributes. Lastly, we illustrate our
implemented blockchain-driven metaverse prototype of a university campus and
discuss the prototype design and insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1"&gt;Haihan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaye Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1"&gt;Sizheng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhonghao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Wei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Bike Spreading Problem. (arXiv:2107.00761v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00761</id>
        <link href="http://arxiv.org/abs/2107.00761"/>
        <updated>2021-08-20T01:53:53.880Z</updated>
        <summary type="html"><![CDATA[A free-floating bike-sharing system (FFBSS) is a dockless rental system where
an individual can borrow a bike and returns it anywhere, within the service
area. To improve the rental service, available bikes should be distributed over
the entire service area: a customer leaving from any position is then more
likely to find a near bike and then to use the service. Moreover, spreading
bikes among the entire service area increases urban spatial equity since the
benefits of FFBSS are not a prerogative of just a few zones. For guaranteeing
such distribution, the FFBSS operator can use vans to manually relocate bikes,
but it incurs high economic and environmental costs. We propose a novel
approach that exploits the existing bike flows generated by customers to
distribute bikes. More specifically, by envisioning the problem as an Influence
Maximization problem, we show that it is possible to position batches of bikes
on a small number of zones, and then the daily use of FFBSS will efficiently
spread these bikes on a large area. We show that detecting these zones is
NP-complete, but there exists a simple and efficient $1-1/e$ approximation
algorithm; our approach is then evaluated on a dataset of rides from the
free-floating bike-sharing system of the city of Padova.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1"&gt;Elia Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1"&gt;Francesco Silvestri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Federated Learning for UAV Networks: Architecture, Challenges, and Opportunities. (arXiv:2104.07557v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07557</id>
        <link href="http://arxiv.org/abs/2104.07557"/>
        <updated>2021-08-20T01:53:53.870Z</updated>
        <summary type="html"><![CDATA[Unmanned aerial vehicles (UAVs), or say drones, are envisioned to support
extensive applications in next-generation wireless networks in both civil and
military fields. Empowering UAVs networks intelligence by artificial
intelligence (AI) especially machine learning (ML) techniques is inevitable and
appealing to enable the aforementioned applications. To solve the problems of
traditional cloud-centric ML for UAV networks such as privacy concern,
unacceptable latency, and resource burden, a distributed ML technique,
\textit(i.e.), federated learning (FL), has been recently proposed to enable
multiple UAVs to collaboratively train ML model without letting out raw data.
However, almost all existing FL paradigms are still centralized, \textit{i.e.},
a central entity is in charge of ML model aggregation and fusion over the whole
network, which could result in the issue of a single point of failure and are
inappropriate to UAV networks with both unreliable nodes and links. Thus
motivated, in this article, we propose a novel architecture called DFL-UN
(\underline{D}ecentralized \underline{F}ederated \underline{L}earning for
\underline{U}AV \underline{N}etworks), which enables FL within UAV networks
without a central entity. We also conduct a preliminary simulation study to
validate the feasibility and effectiveness of the DFL-UN architecture. Finally,
we discuss the main challenges and potential research directions in the DFL-UN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yuben Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Haipeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiafa Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Song Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09701</id>
        <link href="http://arxiv.org/abs/2106.09701"/>
        <updated>2021-08-20T01:53:53.864Z</updated>
        <summary type="html"><![CDATA[Modern computer vision applications suffer from catastrophic forgetting when
incrementally learning new concepts over time. The most successful approaches
to alleviate this forgetting require extensive replay of previously seen data,
which is problematic when memory constraints or data legality concerns exist.
In this work, we consider the high-impact problem of Data-Free
Class-Incremental Learning (DFCIL), where an incremental learning agent must
learn new concepts over time without storing generators or training data from
past tasks. One approach for DFCIL is to replay synthetic images produced by
inverting a frozen copy of the learner's classification model, but we show this
approach fails for common class-incremental benchmarks when using standard
distillation strategies. We diagnose the cause of this failure and propose a
novel incremental distillation strategy for DFCIL, contributing a modified
cross-entropy training and importance-weighted feature distillation, and show
that our method results in up to a 25.1% increase in final task accuracy
(absolute difference) compared to SOTA DFCIL methods for common
class-incremental benchmarks. Our method even outperforms several standard
replay based methods which store a coreset of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;James Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1"&gt;Jonathan Balloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yilin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hongxia Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search. (arXiv:2103.12424v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12424</id>
        <link href="http://arxiv.org/abs/2103.12424"/>
        <updated>2021-08-20T01:53:53.857Z</updated>
        <summary type="html"><![CDATA[A myriad of recent breakthroughs in hand-crafted neural architectures for
visual recognition have highlighted the urgent need to explore hybrid
architectures consisting of diversified building blocks. Meanwhile, neural
architecture search methods are surging with an expectation to reduce human
efforts. However, whether NAS methods can efficiently and effectively handle
diversified search spaces with disparate candidates (e.g. CNNs and
transformers) is still an open question. In this work, we present Block-wisely
Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS
method that addresses the problem of inaccurate architecture rating caused by
large weight-sharing space and biased supervision in previous methods. More
specifically, we factorize the search space into blocks and utilize a novel
self-supervised training scheme, named ensemble bootstrapping, to train each
block separately before searching them as a whole towards the population
center. Additionally, we present HyTra search space, a fabric-like hybrid
CNN-transformer search space with searchable down-sampling positions. On this
challenging search space, our searched model, BossNet-T, achieves up to 82.5%
accuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute
time. Moreover, our method achieves superior architecture rating accuracy with
0.78 and 0.76 Spearman correlation on the canonical MBConv search space with
ImageNet and on NATS-Bench size search space with CIFAR-100, respectively,
surpassing state-of-the-art NAS methods. Code:
https://github.com/changlin31/BossNAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jiefeng Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06985</id>
        <link href="http://arxiv.org/abs/2012.06985"/>
        <updated>2021-08-20T01:53:53.839Z</updated>
        <summary type="html"><![CDATA[Collecting labeled data for the task of semantic segmentation is expensive
and time-consuming, as it requires dense pixel-level annotations. While recent
Convolutional Neural Network (CNN) based semantic segmentation approaches have
achieved impressive results by using large amounts of labeled training data,
their performance drops significantly as the amount of labeled data decreases.
This happens because deep CNNs trained with the de facto cross-entropy loss can
easily overfit to small amounts of labeled data. To address this issue, we
propose a simple and effective contrastive learning-based training strategy in
which we first pretrain the network using a pixel-wise, label-based contrastive
loss, and then fine-tune it using the cross-entropy loss. This approach
increases intra-class compactness and inter-class separability, thereby
resulting in a better pixel classifier. We demonstrate the effectiveness of the
proposed training strategy using the Cityscapes and PASCAL VOC 2012
segmentation datasets. Our results show that pretraining with the proposed
contrastive loss results in large performance gains (more than 20% absolute
improvement in some settings) when the amount of labeled data is limited. In
many settings, the proposed contrastive pretraining strategy, which does not
use any additional data, is able to match or outperform the widely-used
ImageNet pretraining strategy that uses more than a million additional labeled
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangyun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1"&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1"&gt;Philip Mansfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1"&gt;Bradley Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1"&gt;Lior Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ultra-Reliable Indoor Millimeter Wave Communications using Multiple Artificial Intelligence-Powered Intelligent Surfaces. (arXiv:2104.00075v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00075</id>
        <link href="http://arxiv.org/abs/2104.00075"/>
        <updated>2021-08-20T01:53:53.831Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel framework for guaranteeing ultra-reliable millimeter
wave (mmW) communications using multiple artificial intelligence (AI)-enabled
reconfigurable intelligent surfaces (RISs) is proposed. The use of multiple
AI-powered RISs allows changing the propagation direction of the signals
transmitted from a mmW access point (AP) thereby improving coverage
particularly for non-line-of-sight (NLoS) areas. However, due to the
possibility of highly stochastic blockage over mmW links, designing an
intelligent controller to jointly optimize the mmW AP beam and RIS phase shifts
is a daunting task. In this regard, first, a parametric risk-sensitive episodic
return is proposed to maximize the expected bit rate and mitigate the risk of
mmW link blockage. Then, a closed-form approximation of the policy gradient of
the risk-sensitive episodic return is analytically derived. Next, the problem
of joint beamforming for mmW AP and phase shift control for mmW RISs is modeled
as an identical payoff stochastic game within a cooperative multi-agent
environment, in which the agents are the mmW AP and the RISs. Two centralized
and distributed controllers are proposed to control the policies of the mmW AP
and RISs. To directly find an optimal solution, the parametric functional-form
policies for these controllers are modeled using deep recurrent neural networks
(RNNs). Simulation results show that the error between policies of the optimal
and the RNN-based controllers is less than 1.5%. Moreover, the variance of the
achievable rates resulting from the deep RNN-based controllers is 60% less than
the variance of the risk-averse baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soorki_M/0/1/0/all/0/1"&gt;Mehdi Naderi Soorki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1"&gt;Walid Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1"&gt;Choong Seon Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12815</id>
        <link href="http://arxiv.org/abs/2011.12815"/>
        <updated>2021-08-20T01:53:53.824Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been tremendously successful in
solving imaging inverse problems. To understand their success, an effective
strategy is to construct simpler and mathematically more tractable
convolutional sparse coding (CSC) models that share essential ingredients with
CNNs. Existing CSC methods, however, underperform leading CNNs in challenging
inverse problems. We hypothesize that the performance gap may be attributed in
part to how they process images at different spatial scales: While many CNNs
use multiscale feature representations, existing CSC models mostly rely on
single-scale dictionaries. To close the performance gap, we thus propose a
multiscale convolutional dictionary structure. The proposed dictionary
structure is derived from the U-Net, arguably the most versatile and widely
used CNN for image-to-image learning problems. We show that incorporating the
proposed multiscale dictionary in an otherwise standard CSC framework yields
performance competitive with state-of-the-art CNNs across a range of
challenging inverse problems including CT and MRI reconstruction. Our work thus
demonstrates the effectiveness and scalability of the multiscale CSC approach
in solving challenging inverse problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1"&gt;Anadi Chaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1"&gt;David Belius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1"&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Security in Vehicular Networks: A Comprehensive Survey. (arXiv:2105.15035v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15035</id>
        <link href="http://arxiv.org/abs/2105.15035"/>
        <updated>2021-08-20T01:53:53.815Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) has emerged as an attractive and viable technique to
provide effective solutions for a wide range of application domains. An
important application domain is vehicular networks wherein ML-based approaches
are found to be very useful to address various problems. The use of wireless
communication between vehicular nodes and/or infrastructure makes it vulnerable
to different types of attacks. In this regard, ML and its variants are gaining
popularity to detect attacks and deal with different kinds of security issues
in vehicular communication. In this paper, we present a comprehensive survey of
ML-based techniques for different security issues in vehicular networks. We
first briefly introduce the basics of vehicular networks and different types of
communications. Apart from the traditional vehicular networks, we also consider
modern vehicular network architectures. We propose a taxonomy of security
attacks in vehicular networks and discuss various security challenges and
requirements. We classify the ML techniques developed in the literature
according to their use in vehicular network applications. We explain the
solution approaches and working principles of these ML techniques in addressing
various security challenges and provide insightful discussion. The limitations
and challenges in using ML-based methods in vehicular networks are discussed.
Finally, we present observations and lessons learned before we conclude our
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1"&gt;Anum Talpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1"&gt;Mohan Gurusamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11320</id>
        <link href="http://arxiv.org/abs/2107.11320"/>
        <updated>2021-08-20T01:53:53.809Z</updated>
        <summary type="html"><![CDATA[Forest carbon offsets are increasingly popular and can play a significant
role in financing climate mitigation, forest conservation, and reforestation.
Measuring how much carbon is stored in forests is, however, still largely done
via expensive, time-consuming, and sometimes unaccountable field measurements.
To overcome these limitations, many verification bodies are leveraging machine
learning (ML) algorithms to estimate forest carbon from satellite or aerial
imagery. Aerial imagery allows for tree species or family classification, which
improves the satellite imagery-based forest type classification. However,
aerial imagery is significantly more expensive to collect and it is unclear by
how much the higher resolution improves the forest carbon estimation. This
proposal paper describes the first systematic comparison of forest carbon
estimation from aerial imagery, satellite imagery, and ground-truth field
measurements via deep learning-based algorithms for a tropical reforestation
project. Our initial results show that forest carbon estimates from satellite
imagery can overestimate above-ground biomass by up to 10-times for tropical
reforestation projects. The significant difference between aerial and
satellite-derived forest carbon measurements shows the potential for aerial
imagery-based ML algorithms and raises the importance to extend this study to a
global benchmark between options for carbon measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1"&gt;Gyri Reiersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1"&gt;David Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1"&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier. (arXiv:2101.00562v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00562</id>
        <link href="http://arxiv.org/abs/2101.00562"/>
        <updated>2021-08-20T01:53:53.788Z</updated>
        <summary type="html"><![CDATA[Recent papers have suggested that transfer learning can outperform
sophisticated meta-learning methods for few-shot image classification. We take
this hypothesis to its logical conclusion, and suggest the use of an ensemble
of high-quality, pre-trained feature extractors for few-shot image
classification. We show experimentally that a library of pre-trained feature
extractors combined with a simple feed-forward network learned with an
L2-regularizer can be an excellent option for solving cross-domain few-shot
image classification. Our experimental results suggest that this simpler
sample-efficient approach far outperforms several well-established
meta-learning algorithms on a variety of few-shot tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Arkabandhu Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Mingchao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1"&gt;Chris Jermaine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video. (arXiv:1910.00618v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.00618</id>
        <link href="http://arxiv.org/abs/1910.00618"/>
        <updated>2021-08-20T01:53:53.782Z</updated>
        <summary type="html"><![CDATA[Pushing is a fundamental robotic skill. Existing work has shown how to
exploit models of pushing to achieve a variety of tasks, including grasping
under uncertainty, in-hand manipulation and clearing clutter. Such models,
however, are approximate, which limits their applicability. Learning-based
methods can reason directly from raw sensory data with accuracy, and have the
potential to generalize to a wider diversity of scenarios. However, developing
and testing such methods requires rich-enough datasets. In this paper we
introduce Omnipush, a dataset with high variety of planar pushing behavior. In
particular, we provide 250 pushes for each of 250 objects, all recorded with
RGB-D and a high precision tracking system. The objects are constructed so as
to systematically explore key factors that affect pushing -- the shape of the
object and its mass distribution -- which have not been broadly explored in
previous datasets, and allow to study generalization in model learning.
Omnipush includes a benchmark for meta-learning dynamic models, which requires
algorithms that make good predictions and estimate their own uncertainty. We
also provide an RGB video prediction benchmark and propose other relevant tasks
that can be suited with this dataset.

Data and code are available at
\url{https://web.mit.edu/mcube/omnipush-dataset/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1"&gt;Maria Bauza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1"&gt;Ferran Alet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1"&gt;Tomas Lozano-Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1"&gt;Leslie P. Kaelbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alberto Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Curvature Activations Reduce Overfitting in Adversarial Training. (arXiv:2102.07861v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07861</id>
        <link href="http://arxiv.org/abs/2102.07861"/>
        <updated>2021-08-20T01:53:53.775Z</updated>
        <summary type="html"><![CDATA[Adversarial training is one of the most effective defenses against
adversarial attacks. Previous works suggest that overfitting is a dominant
phenomenon in adversarial training leading to a large generalization gap
between test and train accuracy in neural networks. In this work, we show that
the observed generalization gap is closely related to the choice of the
activation function. In particular, we show that using activation functions
with low (exact or approximate) curvature values has a regularization effect
that significantly reduces both the standard and robust generalization gaps in
adversarial training. We observe this effect for both differentiable/smooth
activations such as SiLU as well as non-differentiable/non-smooth activations
such as LeakyReLU. In the latter case, the "approximate" curvature of the
activation is low. Finally, we show that for activation functions with low
curvature, the double descent phenomenon for adversarially trained models does
not occur.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_V/0/1/0/all/0/1"&gt;Vasu Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sahil Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1"&gt;David Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Astraea: Grammar-based Fairness Testing. (arXiv:2010.02542v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02542</id>
        <link href="http://arxiv.org/abs/2010.02542"/>
        <updated>2021-08-20T01:53:53.768Z</updated>
        <summary type="html"><![CDATA[Software often produces biased outputs. In particular, machine learning (ML)
based software are known to produce erroneous predictions when processing
discriminatory inputs. Such unfair program behavior can be caused by societal
bias. In the last few years, Amazon, Microsoft and Google have provided
software services that produce unfair outputs, mostly due to societal bias
(e.g. gender or race). In such events, developers are saddled with the task of
conducting fairness testing. Fairness testing is challenging; developers are
tasked with generating discriminatory inputs that reveal and explain biases.

We propose a grammar-based fairness testing approach (called ASTRAEA) which
leverages context-free grammars to generate discriminatory inputs that reveal
fairness violations in software systems. Using probabilistic grammars, ASTRAEA
also provides fault diagnosis by isolating the cause of observed software bias.
ASTRAEA's diagnoses facilitate the improvement of ML fairness.

ASTRAEA was evaluated on 18 software systems that provide three major natural
language processing (NLP) services. In our evaluation, ASTRAEA generated
fairness violations with a rate of ~18%. ASTRAEA generated over 573K
discriminatory test cases and found over 102K fairness violations. Furthermore,
ASTRAEA improves software fairness by ~76%, via model-retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1"&gt;Ezekiel Soremekun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1"&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simulated Annealing Algorithm for Joint Stratification and Sample Allocation Designs. (arXiv:2011.13006v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13006</id>
        <link href="http://arxiv.org/abs/2011.13006"/>
        <updated>2021-08-20T01:53:53.762Z</updated>
        <summary type="html"><![CDATA[This study combines simulated annealing with delta evaluation to solve the
joint stratification and sample allocation problem. In this problem, atomic
strata are partitioned into mutually exclusive and collectively exhaustive
strata. Each stratification is a solution, the quality of which is measured by
its cost. The Bell number of possible solutions is enormous for even a moderate
number of atomic strata and an additional layer of complexity is added with the
evaluation time of each solution. Many larger scale combinatorial optimisation
problems cannot be solved to optimality because the search for an optimum
solution requires a prohibitive amount of computation time; a number of local
search heuristic algorithms have been designed for this problem but these can
become trapped in local minima preventing any further improvements. We add to
the existing suite of local search algorithms a simulated annealing algorithm
that allows for an escape from local minima and uses delta evaluation to
exploit the similarity between consecutive solutions and thereby reduce the
evaluation time. We compare the simulated annealing algorithm with two recent
algorithms. In both cases the SAA attains a solution of comparable quality in
considerably less computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OLuing_M/0/1/0/all/0/1"&gt;Mervyn O&amp;#x27;Luing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prestwich_S/0/1/0/all/0/1"&gt;Steven Prestwich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarim_S/0/1/0/all/0/1"&gt;S. Armagan Tarim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Adaptive Multi-Factor Model and the Financial Market. (arXiv:2107.14410v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14410</id>
        <link href="http://arxiv.org/abs/2107.14410"/>
        <updated>2021-08-20T01:53:53.744Z</updated>
        <summary type="html"><![CDATA[Modern evolvements of the technologies have been leading to a profound
influence on the financial market. The introduction of constituents like
Exchange-Traded Funds, and the wide-use of advanced technologies such as
algorithmic trading, results in a boom of the data which provides more
opportunities to reveal deeper insights. However, traditional statistical
methods always suffer from the high-dimensional, high-correlation, and
time-varying instinct of the financial data. In this dissertation, we focus on
developing techniques to stress these difficulties. With the proposed
methodologies, we can have more interpretable models, clearer explanations, and
better predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Theoretical Properties of the Exchange Algorithm. (arXiv:2005.09235v4 [stat.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09235</id>
        <link href="http://arxiv.org/abs/2005.09235"/>
        <updated>2021-08-20T01:53:53.737Z</updated>
        <summary type="html"><![CDATA[The exchange algorithm is one of the most popular extensions of the
Metropolis--Hastings algorithm to sample from doubly-intractable distributions.
However, the theoretical exploration of the exchange algorithm is very limited.
For example, natural questions like `Does exchange algorithm converge at a
geometric rate?' or `Does the exchange algorithm admit a Central Limit
Theorem?' have not been answered yet. In this paper, we study the theoretical
properties of the exchange algorithm, in terms of asymptotic variance and
convergence speed. We compare the exchange algorithm with the original
Metropolis--Hastings algorithm and provide both necessary and sufficient
conditions for the geometric ergodicity of the exchange algorithm. Moreover, we
prove that our results can be applied to various practical applications such as
location models, Gaussian models, Poisson models, and a large class of
exponential families, which includes most of the practical applications of the
exchange algorithm. A central limit theorem for the exchange algorithm is also
established. Our results justify the theoretical usefulness of the exchange
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BARF: Bundle-Adjusting Neural Radiance Fields. (arXiv:2104.06405v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06405</id>
        <link href="http://arxiv.org/abs/2104.06405"/>
        <updated>2021-08-20T01:53:53.731Z</updated>
        <summary type="html"><![CDATA[Neural Radiance Fields (NeRF) have recently gained a surge of interest within
the computer vision community for its power to synthesize photorealistic novel
views of real-world scenes. One limitation of NeRF, however, is its requirement
of accurate camera poses to learn the scene representations. In this paper, we
propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from
imperfect (or even unknown) camera poses -- the joint problem of learning
neural 3D representations and registering camera frames. We establish a
theoretical connection to classical image alignment and show that
coarse-to-fine registration is also applicable to NeRF. Furthermore, we show
that na\"ively applying positional encoding in NeRF has a negative impact on
registration with a synthesis-based objective. Experiments on synthetic and
real-world data show that BARF can effectively optimize the neural scene
representations and resolve large camera pose misalignment at the same time.
This enables view synthesis and localization of video sequences from unknown
camera poses, opening up new avenues for visual localization systems (e.g.
SLAM) and potential applications for dense 3D mapping and reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen-Hsuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wei-Chiu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1"&gt;Simon Lucey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10623</id>
        <link href="http://arxiv.org/abs/2009.10623"/>
        <updated>2021-08-20T01:53:53.721Z</updated>
        <summary type="html"><![CDATA[From CNNs to attention mechanisms, encoding inductive biases into neural
networks has been a fruitful source of improvement in machine learning. Adding
auxiliary losses to the main objective function is a general way of encoding
biases that can help networks learn better representations. However, since
auxiliary losses are minimized only on training data, they suffer from the same
generalization gap as regular task losses. Moreover, by adding a term to the
loss function, the model optimizes a different objective than the one we care
about. In this work we address both problems: first, we take inspiration from
\textit{transductive learning} and note that after receiving an input but
before making a prediction, we can fine-tune our networks on any unsupervised
loss. We call this process {\em tailoring}, because we customize the model to
each input to ensure our prediction satisfies the inductive bias. Second, we
formulate {\em meta-tailoring}, a nested optimization similar to that in
meta-learning, and train our models to perform well on the task objective after
adapting them using an unsupervised loss. The advantages of tailoring and
meta-tailoring are discussed theoretically and demonstrated empirically on a
diverse set of examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1"&gt;Ferran Alet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1"&gt;Maria Bauza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1"&gt;Nurullah Giray Kuru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1"&gt;Tomas Lozano-Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1"&gt;Leslie Pack Kaelbling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04351</id>
        <link href="http://arxiv.org/abs/2108.04351"/>
        <updated>2021-08-20T01:53:53.681Z</updated>
        <summary type="html"><![CDATA[This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning TSP Requires Rethinking Generalization. (arXiv:2006.07054v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07054</id>
        <link href="http://arxiv.org/abs/2006.07054"/>
        <updated>2021-08-20T01:53:53.633Z</updated>
        <summary type="html"><![CDATA[End-to-end training of neural network solvers for combinatorial optimization
problems such as the Travelling Salesman Problem is intractable and inefficient
beyond a few hundreds of nodes. While state-of-the-art Machine Learning
approaches perform closely to classical solvers when trained on trivially small
sizes, they are unable to generalize the learnt policy to larger instances of
practical scales. Towards leveraging transfer learning to solve large-scale
TSPs, this paper identifies inductive biases, model architectures and learning
algorithms that promote generalization to instances larger than those seen in
training. Our controlled experiments provide the first principled investigation
into such zero-shot generalization, revealing that extrapolating beyond
training data requires rethinking the neural combinatorial optimization
pipeline, from network layers and learning paradigms to evaluation protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_C/0/1/0/all/0/1"&gt;Chaitanya K. Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1"&gt;Quentin Cappart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1"&gt;Louis-Martin Rousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laurent_T/0/1/0/all/0/1"&gt;Thomas Laurent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Optimizing $f$-divergence is Robust with Label Noise. (arXiv:2011.03687v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03687</id>
        <link href="http://arxiv.org/abs/2011.03687"/>
        <updated>2021-08-20T01:53:53.627Z</updated>
        <summary type="html"><![CDATA[We show when maximizing a properly defined $f$-divergence measure with
respect to a classifier's predictions and the supervised labels is robust with
label noise. Leveraging its variational form, we derive a nice decoupling
property for a family of $f$-divergence measures when label noise presents,
where the divergence is shown to be a linear combination of the variational
difference defined on the clean distribution and a bias term introduced due to
the noise. The above derivation helps us analyze the robustness of different
$f$-divergence functions. With established robustness, this family of
$f$-divergence functions arises as useful metrics for the problem of learning
with noisy labels, which do not require the specification of the labels' noise
rate. When they are possibly not robust, we propose fixes to make them so. In
addition to the analytical results, we present thorough experimental evidence.
Our code is available at
https://github.com/UCSC-REAL/Robust-f-divergence-measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually Guided End-to-End Text-to-Speech With MOS Prediction. (arXiv:2011.01174v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01174</id>
        <link href="http://arxiv.org/abs/2011.01174"/>
        <updated>2021-08-20T01:53:53.619Z</updated>
        <summary type="html"><![CDATA[Although recent end-to-end text-to-speech (TTS) systems have achieved
high-quality speech synthesis, there are still several factors that degrade the
quality of synthesized speech, including lack of training data or information
loss during knowledge distillation. To address the problem, we propose a novel
way to train a TTS model under the supervision of perceptual loss, which
measures the distance between the maximum speech quality score and the
predicted one. We first pre-train a mean opinion score (MOS) prediction model
and then train a TTS model in the direction of maximizing the MOS of
synthesized speech predicted by the pre-trained MOS prediction model. Through
this method, we can improve the quality of synthesized speech universally
(i.e., regardless of the network architecture or the cause of the speech
quality degradation) and efficiently (i.e., without increasing the inference
time or the model complexity). The evaluation results for MOS and phone error
rate demonstrate that our proposed approach improves previous models in terms
of both naturalness and intelligibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yeunju Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1"&gt;Youngmoon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1"&gt;Youngjoo Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hoirin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A k-mer Based Approach for SARS-CoV-2 Variant Identification. (arXiv:2108.03465v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03465</id>
        <link href="http://arxiv.org/abs/2108.03465"/>
        <updated>2021-08-20T01:53:53.613Z</updated>
        <summary type="html"><![CDATA[With the rapid spread of the novel coronavirus (COVID-19) across the globe
and its continuous mutation, it is of pivotal importance to design a system to
identify different known (and unknown) variants of SARS-CoV-2. Identifying
particular variants helps to understand and model their spread patterns, design
effective mitigation strategies, and prevent future outbreaks. It also plays a
crucial role in studying the efficacy of known vaccines against each variant
and modeling the likelihood of breakthrough infections. It is well known that
the spike protein contains most of the information/variation pertaining to
coronavirus variants.

In this paper, we use spike sequences to classify different variants of the
coronavirus in humans. We show that preserving the order of the amino acids
helps the underlying classifiers to achieve better performance. We also show
that we can train our model to outperform the baseline algorithms using only a
small number of training samples ($1\%$ of the data). Finally, we show the
importance of the different amino acids which play a key role in identifying
variants and how they coincide with those reported by the USA's Centers for
Disease Control and Prevention (CDC).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sarwan Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sahoo_B/0/1/0/all/0/1"&gt;Bikram Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ullah_N/0/1/0/all/0/1"&gt;Naimat Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zelikovskiy_A/0/1/0/all/0/1"&gt;Alexander Zelikovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1"&gt;Murray Patterson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1"&gt;Imdadullah Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08810</id>
        <link href="http://arxiv.org/abs/2108.08810"/>
        <updated>2021-08-20T01:53:53.605Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have so far been the de-facto model for
visual data. Recent work has shown that (Vision) Transformer models (ViT) can
achieve comparable or even superior performance on image classification tasks.
This raises a central question: how are Vision Transformers solving these
tasks? Are they acting like convolutional networks, or learning entirely
different visual representations? Analyzing the internal representation
structure of ViTs and CNNs on image classification benchmarks, we find striking
differences between the two architectures, such as ViT having more uniform
representations across all layers. We explore how these differences arise,
finding crucial roles played by self-attention, which enables early aggregation
of global information, and ViT residual connections, which strongly propagate
features from lower to higher layers. We study the ramifications for spatial
localization, demonstrating ViTs successfully preserve input spatial
information, with noticeable effects from different classification methods.
Finally, we study the effect of (pretraining) dataset scale on intermediate
features and transfer learning, and conclude with a discussion on connections
to new architectures such as the MLP-Mixer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1"&gt;Maithra Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1"&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1"&gt;Simon Kornblith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1"&gt;Alexey Dosovitskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Computing. (arXiv:2108.08802v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.08802</id>
        <link href="http://arxiv.org/abs/2108.08802"/>
        <updated>2021-08-20T01:53:53.590Z</updated>
        <summary type="html"><![CDATA[Computing systems form the backbone of many aspects of our life, hence they
are becoming as vital as water, electricity, and road infrastructures for our
society. Yet, engineering long running computing systems that achieve their
goals in ever-changing environments pose significant challenges. Currently, we
can build computing systems that adjust or learn over time to match changes
that were anticipated. However, dealing with unanticipated changes, such as
anomalies, novelties, new goals or constraints, requires system evolution,
which remains in essence a human-driven activity. Given the growing complexity
of computing systems and the vast amount of highly complex data to process,
this approach will eventually become unmanageable. To break through the status
quo, we put forward a new paradigm for the design and operation of computing
systems that we coin "lifelong computing." The paradigm starts from
computing-learning systems that integrate computing/service modules and
learning modules. Computing warehouses offer such computing elements together
with data sheets and usage guides. When detecting anomalies, novelties, new
goals or constraints, a lifelong computing system activates an evolutionary
self-learning engine that runs online experiments to determine how the
computing-learning system needs to evolve to deal with the changes, thereby
changing its architecture and integrating new computing elements from computing
warehouses as needed. Depending on the domain at hand, some activities of
lifelong computing systems can be supported by humans. We motivate the need for
lifelong computing with a future fish farming scenario, outline a blueprint
architecture for lifelong computing systems, and highlight key research
challenges to realise the vision of lifelong computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1"&gt;Danny Weyns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1"&gt;Thomas B&amp;#xe4;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Ren&amp;#xe8; Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xin Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belbachir_A/0/1/0/all/0/1"&gt;Ahmed Nabil Belbachir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12245</id>
        <link href="http://arxiv.org/abs/2006.12245"/>
        <updated>2021-08-20T01:53:53.584Z</updated>
        <summary type="html"><![CDATA[We develop a transductive meta-learning method that uses unlabelled instances
to improve few-shot image classification performance. Our approach combines a
regularized Mahalanobis-distance-based soft k-means clustering procedure with a
modified state of the art neural adaptive feature extractor to achieve improved
test-time classification accuracy using unlabelled data. We evaluate our method
on transductive few-shot learning tasks, in which the goal is to jointly
predict labels for query (test) examples given a set of support (training)
examples. We achieve state-of-the-art performance on the Meta-Dataset,
mini-ImageNet and tiered-ImageNet benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1"&gt;Peyman Bateni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1"&gt;Jarred Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1"&gt;Jan-Willem van de Meent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1"&gt;Frank Wood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Network Based Coarse-Grained Mapping Prediction. (arXiv:2007.04921v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04921</id>
        <link href="http://arxiv.org/abs/2007.04921"/>
        <updated>2021-08-20T01:53:53.577Z</updated>
        <summary type="html"><![CDATA[The selection of coarse-grained (CG) mapping operators is a critical step for
CG molecular dynamics (MD) simulation. It is still an open question about what
is optimal for this choice and there is a need for theory. The current
state-of-the art method is mapping operators manually selected by experts. In
this work, we demonstrate an automated approach by viewing this problem as
supervised learning where we seek to reproduce the mapping operators produced
by experts. We present a graph neural network based CG mapping predictor called
DEEP SUPERVISED GRAPH PARTITIONING MODEL(DSGPM) that treats mapping operators
as a graph segmentation problem. DSGPM is trained on a novel dataset,
Human-annotated Mappings (HAM), consisting of 1,206 molecules with expert
annotated mapping operators. HAM can be used to facilitate further research in
this area. Our model uses a novel metric learning objective to produce
high-quality atomic features that are used in spectral clustering. The results
show that the DSGPM outperforms state-of-the-art methods in the field of graph
segmentation. Finally, we find that predicted CG mapping operators indeed
result in good CG MD models when used in simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wellawatte_G/0/1/0/all/0/1"&gt;Geemi P. Wellawatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chakraborty_M/0/1/0/all/0/1"&gt;Maghesree Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gandhi_H/0/1/0/all/0/1"&gt;Heta A. Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+White_A/0/1/0/all/0/1"&gt;Andrew D. White&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum algorithms for escaping from saddle points. (arXiv:2007.10253v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10253</id>
        <link href="http://arxiv.org/abs/2007.10253"/>
        <updated>2021-08-20T01:53:53.569Z</updated>
        <summary type="html"><![CDATA[We initiate the study of quantum algorithms for escaping from saddle points
with provable guarantee. Given a function $f\colon\mathbb{R}^{n}\to\mathbb{R}$,
our quantum algorithm outputs an $\epsilon$-approximate second-order stationary
point using $\tilde{O}(\log^{2} (n)/\epsilon^{1.75})$ queries to the quantum
evaluation oracle (i.e., the zeroth-order oracle). Compared to the classical
state-of-the-art algorithm by Jin et al. with $\tilde{O}(\log^{6}
(n)/\epsilon^{1.75})$ queries to the gradient oracle (i.e., the first-order
oracle), our quantum algorithm is polynomially better in terms of $\log n$ and
matches its complexity in terms of $1/\epsilon$. Technically, our main
contribution is the idea of replacing the classical perturbations in gradient
descent methods by simulating quantum wave equations, which constitutes the
improvement in the quantum query complexity with $\log n$ factors for escaping
from saddle points. We also show how to use a quantum gradient computation
algorithm due to Jordan to replace the classical gradient queries by quantum
evaluation queries with the same complexity. Finally, we also perform numerical
experiments that support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chenyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Leng_J/0/1/0/all/0/1"&gt;Jiaqi Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_T/0/1/0/all/0/1"&gt;Tongyang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for an Assessment of the Kernel-target Alignment in Tree Ensemble Kernel Learning. (arXiv:2108.08752v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08752</id>
        <link href="http://arxiv.org/abs/2108.08752"/>
        <updated>2021-08-20T01:53:53.562Z</updated>
        <summary type="html"><![CDATA[Kernels ensuing from tree ensembles such as random forest (RF) or gradient
boosted trees (GBT), when used for kernel learning, have been shown to be
competitive to their respective tree ensembles (particularly in higher
dimensional scenarios). On the other hand, it has been also shown that
performance of the kernel algorithms depends on the degree of the kernel-target
alignment. However, the kernel-target alignment for kernel learning based on
the tree ensembles has not been investigated and filling this gap is the main
goal of our work.

Using the eigenanalysis of the kernel matrix, we demonstrate that for
continuous targets good performance of the tree-based kernel learning is
associated with strong kernel-target alignment. Moreover, we show that well
performing tree ensemble based kernels are characterized by strong target
aligned components that are expressed through scalar products between the
eigenvectors of the kernel matrix and the target. This suggests that when tree
ensemble based kernel learning is successful, relevant information for the
supervised problem is concentrated near lower dimensional manifold spanned by
the target aligned components. Persistence of the strong target aligned
components in tree ensemble based kernels is further supported by sensitivity
analysis via landmark learning. In addition to a comprehensive simulation
study, we also provide experimental results from several real life data sets
that are in line with the simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_D/0/1/0/all/0/1"&gt;Dai Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Baumgartner_R/0/1/0/all/0/1"&gt;Richard Baumgartner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impulse data models for the inverse problem of electrocardiography. (arXiv:2102.00570v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00570</id>
        <link href="http://arxiv.org/abs/2102.00570"/>
        <updated>2021-08-20T01:53:53.545Z</updated>
        <summary type="html"><![CDATA[The proposed method re-frames traditional inverse problems of
electrocardiography into regression problems, constraining the solution space
by decomposing signals with multidimensional Gaussian impulse basis functions.
Impulse HSPs were generated with single Gaussian basis functions at discrete
heart surface locations and projected to corresponding BSPs using a volume
conductor torso model. Both BSP (inputs) and HSP (outputs) were mapped to
regular 2D surface meshes and used to train a neural network. Predictive
capabilities of the network were tested with unseen synthetic and experimental
data. A dense full connected single hidden layer neural network was trained to
map body surface impulses to heart surface Gaussian basis functions for
reconstructing HSP. Synthetic pulses moving across the heart surface were
predicted from the neural network with root mean squared error of $9.1\pm1.4$%.
Predicted signals were robust to noise up to 20 dB and errors due to
displacement and rotation of the heart within the torso were bounded and
predictable. A shift of the heart 40 mm toward the spine resulted in a 4\%
increase in signal feature localization error. The set of training impulse
function data could be reduced and prediction error remained bounded. Recorded
HSPs from in-vitro pig hearts were reliably decomposed using space-time
Gaussian basis functions. Predicted HSPs for left-ventricular pacing had a mean
absolute error of $10.4\pm11.4$ ms. Other pacing scenarios were analyzed with
similar success. Conclusion: Impulses from Gaussian basis functions are
potentially an effective and robust way to train simple neural network data
models for reconstructing HSPs from decomposed BSPs. The HSPs predicted by the
neural network can be used to generate activation maps that non-invasively
identify features of cardiac electrical dysfunction and can guide subsequent
treatment options.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Peng_T/0/1/0/all/0/1"&gt;Tommy Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Malik_A/0/1/0/all/0/1"&gt;Avinash Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bear_L/0/1/0/all/0/1"&gt;Laura R. Bear&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Trew_M/0/1/0/all/0/1"&gt;Mark L. Trew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrasting Contrastive Self-Supervised Representation Learning Pipelines. (arXiv:2103.14005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14005</id>
        <link href="http://arxiv.org/abs/2103.14005"/>
        <updated>2021-08-20T01:53:53.538Z</updated>
        <summary type="html"><![CDATA[In the past few years, we have witnessed remarkable breakthroughs in
self-supervised representation learning. Despite the success and adoption of
representations learned through this paradigm, much is yet to be understood
about how different training methods and datasets influence performance on
downstream tasks. In this paper, we analyze contrastive approaches as one of
the most successful and popular variants of self-supervised representation
learning. We perform this analysis from the perspective of the training
algorithms, pre-training datasets and end tasks. We examine over 700 training
experiments including 30 encoders, 4 pre-training datasets and 20 diverse
downstream tasks. Our experiments address various questions regarding the
performance of self-supervised models compared to their supervised
counterparts, current benchmarks used for evaluation, and the effect of the
pre-training data on end task performance. Our Visual Representation Benchmark
(ViRB) is available at: https://github.com/allenai/virb.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1"&gt;Klemen Kotar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1"&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1"&gt;Kiana Ehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09493</id>
        <link href="http://arxiv.org/abs/2104.09493"/>
        <updated>2021-08-20T01:53:53.531Z</updated>
        <summary type="html"><![CDATA[Active learning algorithms select a subset of data for annotation to maximize
the model performance on a budget. One such algorithm is Expected Gradient
Length, which as the name suggests uses the approximate gradient induced per
example in the sampling process. While Expected Gradient Length has been
successfully used for classification and regression, the formulation for
regression remains intuitively driven. Hence, our theoretical contribution
involves deriving this formulation, thereby supporting the experimental
evidence. Subsequently, we show that expected gradient length in regression is
equivalent to Bayesian uncertainty. If certain assumptions are infeasible, our
algorithmic contribution (EGL++) approximates the effect of ensembles with a
single deterministic network. Instead of computing multiple possible inferences
per input, we leverage previously annotated samples to quantify the probability
of previous labels being the true label. Such an approach allows us to extend
expected gradient length to a new task: human pose estimation. We perform
experimental validation on two human pose datasets (MPII and LSP/LSPET),
highlighting the interpretability and competitiveness of EGL++ with different
active learning algorithms for human pose estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1"&gt;Megh Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Federated Learning via Robust Distributed Mean Estimation. (arXiv:2108.08842v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08842</id>
        <link href="http://arxiv.org/abs/2108.08842"/>
        <updated>2021-08-20T01:53:53.524Z</updated>
        <summary type="html"><![CDATA[Federated learning commonly relies on algorithms such as distributed
(mini-batch) SGD, where multiple clients compute their gradients and send them
to a central coordinator for averaging and updating the model. To optimize the
transmission time and the scalability of the training process, clients often
use lossy compression to reduce the message sizes. DRIVE is a recent state of
the art algorithm that compresses gradients using one bit per coordinate (with
some lower-order overhead). In this technical report, we generalize DRIVE to
support any bandwidth constraint as well as extend it to support heterogeneous
client resources and make it robust to packet loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1"&gt;Shay Vargaftik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basat_R/0/1/0/all/0/1"&gt;Ran Ben Basat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portnoy_A/0/1/0/all/0/1"&gt;Amit Portnoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendelson_G/0/1/0/all/0/1"&gt;Gal Mendelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1"&gt;Yaniv Ben-Itzhak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1"&gt;Michael Mitzenmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images. (arXiv:2108.08775v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08775</id>
        <link href="http://arxiv.org/abs/2108.08775"/>
        <updated>2021-08-20T01:53:53.518Z</updated>
        <summary type="html"><![CDATA[The world is going through a challenging phase due to the disastrous effect
caused by the COVID-19 pandemic on the healthcare system and the economy. The
rate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of
COVID-19 have put the healthcare systems in disruption across the globe. Due to
this, the task of accurately screening COVID-19 cases has become of utmost
priority. Since the virus infects the respiratory system, Chest X-Ray is an
imaging modality that is adopted extensively for the initial screening. We have
performed a comprehensive study that uses CXR images to identify COVID-19 cases
and realized the necessity of having a more generalizable model. We utilize
MobileNetV2 architecture as the feature extractor and integrate it into Capsule
Networks to construct a fully automated and lightweight model termed as
MobileCaps. MobileCaps is trained and evaluated on the publicly available
dataset with the model ensembling and Bayesian optimization strategies to
efficiently classify CXR images of patients with COVID-19 from non-COVID-19
pneumonia and healthy cases. The proposed model is further evaluated on two
additional RT-PCR confirmed datasets to demonstrate the generalizability. We
also introduce MobileCaps-S and leverage it for performing severity assessment
of CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema
(RALE) scoring technique. Our classification model achieved an overall recall
of 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,
non-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity
assessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that
the proposed models have fewer trainable parameters than the state-of-the-art
models reported in the literature, we believe our models will go a long way in
aiding healthcare systems in the battle against the pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1"&gt;S J Pawan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1"&gt;Rahul Sankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prabhudev_A/0/1/0/all/0/1"&gt;Amithash M Prabhudev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahesh_P/0/1/0/all/0/1"&gt;P A Mahesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prakashini_K/0/1/0/all/0/1"&gt;K Prakashini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1"&gt;Sudha Kiran Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1"&gt;Jeny Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[eGAN: Unsupervised approach to class imbalance using transfer learning. (arXiv:2104.04162v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04162</id>
        <link href="http://arxiv.org/abs/2104.04162"/>
        <updated>2021-08-20T01:53:53.501Z</updated>
        <summary type="html"><![CDATA[Class imbalance is an inherent problem in many machine learning
classification tasks. This often leads to trained models that are unusable for
any practical purpose. In this study we explore an unsupervised approach to
address these imbalances by leveraging transfer learning from pre-trained image
classification models to encoder-based Generative Adversarial Network (eGAN).
To the best of our knowledge, this is the first work to tackle this problem
using GAN without needing to augment with synthesized fake images.

In the proposed approach we use the discriminator network to output a
negative or positive score. We classify as minority, test samples with negative
scores and as majority those with positive scores. Our approach eliminates
epistemic uncertainty in model predictions, as the P(minority) + P(majority)
need not sum up to 1. The impact of transfer learning and combinations of
different pre-trained image classification models at the generator and
discriminator is also explored. Best result of 0.69 F1-score was obtained on
CIFAR-10 classification task with imbalance ratio of 1:2500.

Our approach also provides a mechanism of thresholding the specificity or
sensitivity of our machine learning system. Keywords: Class imbalance, Transfer
Learning, GAN, nash equilibrium]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1"&gt;Ademola Okerinde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1"&gt;Lior Shamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;William Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theis_T/0/1/0/all/0/1"&gt;Tom Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nafi_N/0/1/0/all/0/1"&gt;Nasik Nafi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature-weighted Stacking for Nonseasonal Time Series Forecasts: A Case Study of the COVID-19 Epidemic Curves. (arXiv:2108.08723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08723</id>
        <link href="http://arxiv.org/abs/2108.08723"/>
        <updated>2021-08-20T01:53:53.495Z</updated>
        <summary type="html"><![CDATA[We investigate ensembling techniques in forecasting and examine their
potential for use in nonseasonal time-series similar to those in the early days
of the COVID-19 pandemic. Developing improved forecast methods is essential as
they provide data-driven decisions to organisations and decision-makers during
critical phases. We propose using late data fusion, using a stacked ensemble of
two forecasting models and two meta-features that prove their predictive power
during a preliminary forecasting stage. The final ensembles include a Prophet
and long short term memory (LSTM) neural network as base models. The base
models are combined by a multilayer perceptron (MLP), taking into account
meta-features that indicate the highest correlation with each base model's
forecast accuracy. We further show that the inclusion of meta-features
generally improves the ensemble's forecast accuracy across two forecast
horizons of seven and fourteen days. This research reinforces previous work and
demonstrates the value of combining traditional statistical models with deep
learning models to produce more accurate forecast models for time-series across
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cawood_P/0/1/0/all/0/1"&gt;Pieter Cawood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1"&gt;Terence L. van Zyl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Equilibria in Matching Markets from Bandit Feedback. (arXiv:2108.08843v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08843</id>
        <link href="http://arxiv.org/abs/2108.08843"/>
        <updated>2021-08-20T01:53:53.488Z</updated>
        <summary type="html"><![CDATA[Large-scale, two-sided matching platforms must find market outcomes that
align with user preferences while simultaneously learning these preferences
from data. However, since preferences are inherently uncertain during learning,
the classical notion of stability (Gale and Shapley, 1962; Shapley and Shubik,
1971) is unattainable in these settings. To bridge this gap, we develop a
framework and algorithms for learning stable market outcomes under uncertainty.
Our primary setting is matching with transferable utilities, where the platform
both matches agents and sets monetary transfers between them. We design an
incentive-aware learning objective that captures the distance of a market
outcome from equilibrium. Using this objective, we analyze the complexity of
learning as a function of preference structure, casting learning as a
stochastic multi-armed bandit problem. Algorithmically, we show that "optimism
in the face of uncertainty," the principle underlying many bandit algorithms,
applies to a primal-dual formulation of matching with transfers and leads to
near-optimal regret bounds. Our work takes a first step toward elucidating when
and how stable matchings arise in large, data-driven marketplaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1"&gt;Meena Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_A/0/1/0/all/0/1"&gt;Alexander Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning: The Basics. (arXiv:1805.05052v14 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.05052</id>
        <link href="http://arxiv.org/abs/1805.05052"/>
        <updated>2021-08-20T01:53:53.480Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) has become a commodity in our every-day lives. We
routinely ask ML empowered smartphones to suggest lovely food places or to
guide us through a strange place. ML methods have also become standard tools in
many fields of science and engineering. A plethora of ML applications transform
human lives at unprecedented pace and scale. This book portrays ML as the
combination of three basic components: data, model and loss. ML methods combine
these three components within computationally efficient implementations of the
basic scientific principle "trial and error". This principle consists of the
continuous adaptation of a hypothesis about a phenomenon that generates data.
ML methods use a hypothesis to compute predictions for future events. We
believe that thinking about ML as combinations of three components given by
data, model, and loss helps to navigate the steadily growing offer for
ready-to-use ML methods. Our three-component picture of ML allows a unified
treatment of a wide range of concepts and techniques which seem quite unrelated
at first sight. The regularization effect of early stopping in iterative
methods is due to the shrinking of the effective hypothesis space.
Privacy-preserving ML is obtained by particular choices for the features of
data points. Explainable ML methods are characterized by particular choices for
the hypothesis space. To make good use of ML tools it is instrumental to
understand its underlying principles at different levels of detail. On a lower
level, this tutorial helps ML engineers to choose suitable methods for the
application at hand. The book also offers a higher-level view on the
implementation of ML methods which is typically required to manage a team of ML
engineers and data scientists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1"&gt;Alexander Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Public Transportation Demand Analysis: A Case Study of Metropolitan Lagos. (arXiv:2105.11816v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11816</id>
        <link href="http://arxiv.org/abs/2105.11816"/>
        <updated>2021-08-20T01:53:53.472Z</updated>
        <summary type="html"><![CDATA[Modelling, simulation, and forecasting offer a means of facilitating better
planning and decision-making. These quantitative approaches can add value
beyond traditional methods that do not rely on data and are particularly
relevant for public transportation. Lagos is experiencing rapid urbanization
and currently has a population of just under 15 million. Both long waiting
times and uncertain travel times has driven many people to acquire their own
vehicle or use alternative modes of transport. This has significantly increased
the number of vehicles on the roads leading to even more traffic and greater
traffic congestion. This paper investigates urban travel demand in Lagos and
explores passenger dynamics in time and space. Using individual commuter trip
data from tickets purchased from the Lagos State Bus Rapid Transit (BRT), the
demand patterns through the hours of the day, days of the week and bus stations
are analysed. This study aims to quantify demand from actual passenger trips
and estimate the impact that dynamic scheduling could have on passenger waiting
times. Station segmentation is provided to cluster stations by their demand
characteristics in order to tailor specific bus schedules. Intra-day public
transportation demand in Lagos BRT is analysed and predictions are compared.
Simulations using fixed and dynamic bus scheduling demonstrate that the average
waiting time could be reduced by as much as 80%. The load curves, insights and
the approach developed will be useful for informing policymaking in Lagos and
similar African cities facing the challenges of rapid urbanization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1"&gt;Ozioma Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McSharry_P/0/1/0/all/0/1"&gt;Patrick McSharry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A marine radioisotope gamma-ray spectrum analysis method based on Monte Carlo simulation and MLP neural network. (arXiv:2010.15245v2 [physics.ins-det] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15245</id>
        <link href="http://arxiv.org/abs/2010.15245"/>
        <updated>2021-08-20T01:53:53.466Z</updated>
        <summary type="html"><![CDATA[The monitoring of Cs-137 in seawater using scintillation detector relies on
the spectrum analysis method to extract the Cs-137 concentration. And when in
poor statistic situation, the calculation result of the traditional net peak
area (NPA) method has a large uncertainty. We present a machine learning based
method to better analyze the gamma-ray spectrum with low Cs-137 concentration.
We apply multilayer perceptron (MLP) to analyze the 662 keV full energy peak of
Cs-137 in the seawater spectrum. And the MLP can be trained with a few measured
background spectrums by combining the simulated Cs-137 signal with measured
background spectrums. Thus, it can save the time of preparing and measuring the
standard samples for generating the training dataset. To validate the MLP-based
method, we use Geant4 and background gamma-ray spectrums measured by a seaborne
monitoring device to generate an independent test dataset to test the result by
our method and the traditional NPA method. We find that the MLP-based method
achieves a root mean squared error of 0.159, 2.3 times lower than that of the
traditional net peak area method, indicating the MLP-based method improves the
precision of Cs-137 concentration calculation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenhan Dai&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Zeng_Z/0/1/0/all/0/1"&gt;Zhi Zeng&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Dou_D/0/1/0/all/0/1"&gt;Daowei Dou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hao Ma&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianping Chen&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1"&gt;Junli Li&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hui Zhang&lt;/a&gt; (1) ((1) Department of Engineering Physics, Tsinghua University, Beijing, China, (2) College of Nuclear Science and Technology, Beijing Normal University, Beijing, China)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial domain adaptation to reduce sample bias of a high energy physics classifier. (arXiv:2005.00568v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00568</id>
        <link href="http://arxiv.org/abs/2005.00568"/>
        <updated>2021-08-20T01:53:53.449Z</updated>
        <summary type="html"><![CDATA[We apply adversarial domain adaptation in unsupervised setting to reduce
sample bias in a supervised high energy physics events classifier training. We
make use of a neural network containing event and domain classifier with a
gradient reversal layer to simultaneously enable signal versus background
events classification on the one hand, while on the other hand minimising the
difference in response of the network to background samples originating from
different MC models via adversarial domain classification loss. We show the
successful bias removal on the example of simulated events at the LHC with
$t\bar{t}H$ signal versus $t\bar{t}b\bar{b}$ background classification and
discuss implications and limitations of the method]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Clavijo_J/0/1/0/all/0/1"&gt;Jose M. Clavijo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Glaysher_P/0/1/0/all/0/1"&gt;Paul Glaysher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katzy_J/0/1/0/all/0/1"&gt;Judith M. Katzy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jitsev_J/0/1/0/all/0/1"&gt;Jenia Jitsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-08-20T01:53:53.442Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.14072</id>
        <link href="http://arxiv.org/abs/2107.14072"/>
        <updated>2021-08-20T01:53:52.906Z</updated>
        <summary type="html"><![CDATA[A core objective of the TERRA-REF project was to generate an open-access
reference dataset for the evaluation of sensing technologies to study plants
under field conditions. The TERRA-REF program deployed a suite of
high-resolution, cutting edge technology sensors on a gantry system with the
aim of scanning 1 hectare (10$^4$) at around 1 mm$^2$ spatial resolution
multiple times per week. The system contains co-located sensors including a
stereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D
structure, and two hyperspectral cameras covering wavelengths of 300-2500nm.
This sensor data is provided alongside over sixty types of traditional plant
phenotype measurements that can be used to train new machine learning models.
Associated weather and environmental measurements, information about agronomic
management and experimental design, and the genomic sequences of hundreds of
plant varieties have been collected and are available alongside the sensor and
plant phenotype data.

Over the course of four years and ten growing seasons, the TERRA-REF system
generated over 1 PB of sensor data and almost 45 million files. The subset that
has been released to the public domain accounts for two seasons and about half
of the total data volume. This provides an unprecedented opportunity for
investigations far beyond the core biological scope of the project.

The focus of this paper is to provide the Computer Vision and Machine
Learning communities an overview of the available data and some potential
applications of this one of a kind data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1"&gt;David LeBauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1"&gt;Max Burnette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1"&gt;Noah Fahlgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1"&gt;Rob Kooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1"&gt;Kenton McHenry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning. (arXiv:2108.08812v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08812</id>
        <link href="http://arxiv.org/abs/2108.08812"/>
        <updated>2021-08-20T01:53:52.892Z</updated>
        <summary type="html"><![CDATA[Actor-critic methods are widely used in offline reinforcement learning
practice, but are not so well-understood theoretically. We propose a new
offline actor-critic algorithm that naturally incorporates the pessimism
principle, leading to several key advantages compared to the state of the art.
The algorithm can operate when the Bellman evaluation operator is closed with
respect to the action value function of the actor's policies; this is a more
general setting than the low-rank MDP model. Despite the added generality, the
procedure is computationally tractable as it involves the solution of a
sequence of second-order programs. We prove an upper bound on the suboptimality
gap of the policy returned by the procedure that depends on the data coverage
of any arbitrary, possibly data dependent comparator policy. The achievable
guarantee is complemented with a minimax lower bound that is matching up to
logarithmic factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1"&gt;Andrea Zanette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1"&gt;Martin J. Wainwright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1"&gt;Emma Brunskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11857</id>
        <link href="http://arxiv.org/abs/2011.11857"/>
        <updated>2021-08-20T01:53:52.886Z</updated>
        <summary type="html"><![CDATA[Adversarial attack algorithms are dominated by penalty methods, which are
slow in practice, or more efficient distance-customized methods, which are
heavily tailored to the properties of the distance considered. We propose a
white-box attack algorithm to generate minimally perturbed adversarial examples
based on Augmented Lagrangian principles. We bring several algorithmic
modifications, which have a crucial effect on performance. Our attack enjoys
the generality of penalty methods and the computational efficiency of
distance-customized algorithms, and can be readily used for a wide set of
distances. We compare our attack to state-of-the-art methods on three datasets
and several models, and consistently obtain competitive performances with
similar or lower computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Rony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1"&gt;Eric Granger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering Structure of Microstructure Measures. (arXiv:2107.02283v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02283</id>
        <link href="http://arxiv.org/abs/2107.02283"/>
        <updated>2021-08-20T01:53:52.880Z</updated>
        <summary type="html"><![CDATA[This paper builds the clustering model of measures of market microstructure
features which are popular in predicting the stock returns. In a 10-second time
frequency, we study the clustering structure of different measures to find out
the best ones for predicting. In this way, we can predict more accurately with
a limited number of predictors, which removes the noise and makes the model
more interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Sun_N/0/1/0/all/0/1"&gt;Ningning Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wells_M/0/1/0/all/0/1"&gt;Martin T. Wells&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient remedies for outlier detection with variational autoencoders. (arXiv:2108.08760v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08760</id>
        <link href="http://arxiv.org/abs/2108.08760"/>
        <updated>2021-08-20T01:53:52.869Z</updated>
        <summary type="html"><![CDATA[Deep networks often make confident, yet incorrect, predictions when tested
with outlier data that is far removed from their training distributions.
Likelihoods computed by deep generative models are a candidate metric for
outlier detection with unlabeled data. Yet, previous studies have shown that
such likelihoods are unreliable and can be easily biased by simple
transformations to input data. Here, we examine outlier detection with
variational autoencoders (VAEs), among the simplest class of deep generative
models. First, we show that a theoretically-grounded correction readily
ameliorates a key bias with VAE likelihood estimates. The bias correction is
model-free, sample-specific, and accurately computed with the Bernoulli and
continuous Bernoulli visible distributions. Second, we show that a well-known
preprocessing technique, contrast normalization, extends the effectiveness of
bias correction to natural image datasets. Third, we show that the variance of
the likelihoods computed over an ensemble of VAEs also enables robust outlier
detection. We perform a comprehensive evaluation of our remedies with nine
(grayscale and natural) image datasets, and demonstrate significant advantages,
in terms of both speed and accuracy, over four other state-of-the-art methods.
Our lightweight remedies are biologically inspired and may serve to achieve
efficient outlier detection with many types of deep generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1"&gt;Kushal Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1"&gt;Pradeep Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1"&gt;Devarajan Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Assisted Strategies (The Parameterisation of an Infectious Disease Agent-Based Model). (arXiv:2108.08809v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08809</id>
        <link href="http://arxiv.org/abs/2108.08809"/>
        <updated>2021-08-20T01:53:52.863Z</updated>
        <summary type="html"><![CDATA[Parameter calibration is a significant challenge in agent-based modelling and
simulation (ABMS). An agent-based model's (ABM) complexity grows as the number
of parameters required to be calibrated increases. This parameter expansion
leads to the ABMS equivalent of the \say{curse of dimensionality}. In
particular, infeasible computational requirements searching an infinite
parameter space. We propose a more comprehensive and adaptive ABMS Framework
that can effectively swap out parameterisation strategies and surrogate models
to parameterise an infectious disease ABM. This framework allows us to evaluate
different strategy-surrogate combinations' performance in accuracy and
efficiency (speedup). We show that we achieve better than parity in accuracy
across the surrogate assisted sampling strategies and the baselines. Also, we
identify that the Metric Stochastic Response Surface strategy combined with the
Support Vector Machine surrogate is the best overall in getting closest to the
true synthetic parameters. Also, we show that DYnamic COOrdindate Search Using
Response Surface Models with XGBoost as a surrogate attains in combination the
highest probability of approximating a cumulative synthetic daily infection
data distribution and achieves the most significant speedup with regards to our
analysis. Lastly, we show in a real-world setting that DYCORS XGBoost and MSRS
SVM can approximate the real world cumulative daily infection distribution with
$97.12$\% and $96.75$\% similarity respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perumal_R/0/1/0/all/0/1"&gt;Rylan Perumal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1"&gt;Terence L van Zyl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary-task learning for geographic data with autoregressive embeddings. (arXiv:2006.10461v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10461</id>
        <link href="http://arxiv.org/abs/2006.10461"/>
        <updated>2021-08-20T01:53:52.856Z</updated>
        <summary type="html"><![CDATA[Machine learning is gaining popularity in a broad range of areas working with
geographic data, such as ecology or atmospheric sciences. Here, data often
exhibit spatial effects, which can be difficult to learn for neural networks.
In this study, we propose SXL, a method for embedding information on the
autoregressive nature of spatial data directly into the learning process using
auxiliary tasks. We utilize the local Moran's I, a popular measure of local
spatial autocorrelation, to "nudge" the model to learn the direction and
magnitude of local spatial effects, complementing the learning of the primary
task. We further introduce a novel expansion of Moran's I to multiple
resolutions, thus capturing spatial interactions over longer and shorter
distances simultaneously. The novel multi-resolution Moran's I can be
constructed easily and as a multi-dimensional tensor offers seamless
integration into existing machine learning frameworks. Throughout a range of
experiments using real-world data, we highlight how our method consistently
improves the training of neural networks in unsupervised and supervised
learning tasks. In generative spatial modeling experiments, we propose a novel
loss for auxiliary task GANs utilizing task uncertainty weights. Our proposed
method outperforms domain-specific spatial interpolation benchmarks,
highlighting its potential for downstream applications. This study bridges
expertise from geographic information science and machine learning, showing how
this integration of disciplines can help to address domain-specific challenges.
The code for our experiments is available on Github:
https://github.com/konstantinklemmer/sxl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1"&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1"&gt;Daniel B. Neill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08728</id>
        <link href="http://arxiv.org/abs/2108.08728"/>
        <updated>2021-08-20T01:53:52.849Z</updated>
        <summary type="html"><![CDATA[Attention mechanism has demonstrated great potential in fine-grained visual
recognition tasks. In this paper, we present a counterfactual attention
learning method to learn more effective attention based on causal inference.
Unlike most existing methods that learn visual attention based on conventional
likelihood, we propose to learn the attention with counterfactual causality,
which provides a tool to measure the attention quality and a powerful
supervisory signal to guide the learning process. Specifically, we analyze the
effect of the learned visual attention on network prediction through
counterfactual intervention and maximize the effect to encourage the network to
learn more useful attention for fine-grained image recognition. Empirically, we
evaluate our method on a wide range of fine-grained recognition tasks where
attention plays a crucial role, including fine-grained image categorization,
person re-identification, and vehicle re-identification. The consistent
improvement on all benchmarks demonstrates the effectiveness of our method.
Code is available at https://github.com/raoyongming/CAL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning. (arXiv:2004.12571v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12571</id>
        <link href="http://arxiv.org/abs/2004.12571"/>
        <updated>2021-08-20T01:53:52.842Z</updated>
        <summary type="html"><![CDATA[As a decentralized model training method, federated learning is designed to
integrate the isolated data islands and protect data privacy. Recent studies,
however, have demonstrated that the Generative Adversarial Network (GAN) based
attacks can be used in federated learning to learn the distribution of the
victim's private dataset and accordingly reconstruct human-distinguishable
images. In this paper, we exploit defenses against GAN-based attacks in
federated learning, and propose a framework, Anti-GAN, to prevent attackers
from learning the real distribution of the victim's data. The core idea of
Anti-GAN is to corrupt the visual features of the victim's private training
images, such that the images restored by the attacker are indistinguishable to
human eyes. Specifically, in Anti-GAN, the victim first projects the personal
dataset onto a GAN's generator, then mixes the fake images generated by the
generator with the real images to obtain the training dataset, which will be
fed into the federated model for training. We redesign the structure of the
victim's GAN to encourage it to learn the classification features (instead of
the visual features) of the real images. We further introduce an unsupervised
task to the GAN model for obfuscating the visual features of the generated
images. The experiments demonstrate that Anti-GAN can effectively prevent the
attacker from learning the distribution of the private images, meanwhile
causing little harm to the accuracy of the federated model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xinjian Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Graph Network Embedding with Causal Anonymous Walks Representations. (arXiv:2108.08754v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08754</id>
        <link href="http://arxiv.org/abs/2108.08754"/>
        <updated>2021-08-20T01:53:52.797Z</updated>
        <summary type="html"><![CDATA[Many tasks in graph machine learning, such as link prediction and node
classification, are typically solved by using representation learning, in which
each node or edge in the network is encoded via an embedding. Though there
exists a lot of network embeddings for static graphs, the task becomes much
more complicated when the dynamic (i.e. temporal) network is analyzed. In this
paper, we propose a novel approach for dynamic network representation learning
based on Temporal Graph Network by using a highly custom message generating
function by extracting Causal Anonymous Walks. For evaluation, we provide a
benchmark pipeline for the evaluation of temporal network embeddings. This work
provides the first comprehensive comparison framework for temporal network
representation learning in every available setting for graph machine learning
problems involving node classification and link prediction. The proposed model
outperforms state-of-the-art baseline models. The work also justifies the
difference between them based on evaluation in various transductive/inductive
edge/node classification tasks. In addition, we show the applicability and
superior performance of our model in the real-world downstream graph machine
learning task provided by one of the top European banks, involving credit
scoring based on transaction data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1"&gt;Andrey Savchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korovko_A/0/1/0/all/0/1"&gt;Arseny Korovko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherstyuk_L/0/1/0/all/0/1"&gt;Leonid Sherstyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Severin_N/0/1/0/all/0/1"&gt;Nikita Severin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikheev_A/0/1/0/all/0/1"&gt;Aleksandr Mikheev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babaev_D/0/1/0/all/0/1"&gt;Dmitrii Babaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimally Efficient Sequential Calibration of Binary Classifiers to Minimize Classification Error. (arXiv:2108.08780v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08780</id>
        <link href="http://arxiv.org/abs/2108.08780"/>
        <updated>2021-08-20T01:53:52.787Z</updated>
        <summary type="html"><![CDATA[In this work, we aim to calibrate the score outputs of an estimator for the
binary classification problem by finding an 'optimal' mapping to class
probabilities, where the 'optimal' mapping is in the sense that minimizes the
classification error (or equivalently, maximizes the accuracy). We show that
for the given target variables and the score outputs of an estimator, an
'optimal' soft mapping, which monotonically maps the score values to
probabilities, is a hard mapping that maps the score values to $0$ and $1$. We
show that for class weighted (where the accuracy for one class is more
important) and sample weighted (where the samples' accurate classifications are
not equally important) errors, or even general linear losses; this hard mapping
characteristic is preserved. We propose a sequential recursive merger approach,
which produces an 'optimal' hard mapping (for the observed samples so far)
sequentially with each incoming new sample. Our approach has a logarithmic in
sample size time complexity, which is optimally efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1"&gt;Kaan Gokcesu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1"&gt;Hakan Gokcesu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering dynamics on graphs: from spectral clustering to mean shift through Fokker-Planck interpolation. (arXiv:2108.08687v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08687</id>
        <link href="http://arxiv.org/abs/2108.08687"/>
        <updated>2021-08-20T01:53:52.779Z</updated>
        <summary type="html"><![CDATA[In this work we build a unifying framework to interpolate between
density-driven and geometry-based algorithms for data clustering, and
specifically, to connect the mean shift algorithm with spectral clustering at
discrete and continuum levels. We seek this connection through the introduction
of Fokker-Planck equations on data graphs. Besides introducing new forms of
mean shift algorithms on graphs, we provide new theoretical insights on the
behavior of the family of diffusion maps in the large sample limit as well as
provide new connections between diffusion maps and mean shift dynamics on a
fixed graph. Several numerical examples illustrate our theoretical findings and
highlight the benefits of interpolating density-driven and geometry-based
clustering algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Craig_K/0/1/0/all/0/1"&gt;Katy Craig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trillos_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Garc&amp;#xed;a Trillos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Slepcev_D/0/1/0/all/0/1"&gt;Dejan Slep&amp;#x10d;ev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Generative Adversarial Imitation Learning for Online and Offline Setting with Linear Function Approximation. (arXiv:2108.08765v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08765</id>
        <link href="http://arxiv.org/abs/2108.08765"/>
        <updated>2021-08-20T01:53:52.766Z</updated>
        <summary type="html"><![CDATA[In generative adversarial imitation learning (GAIL), the agent aims to learn
a policy from an expert demonstration so that its performance cannot be
discriminated from the expert policy on a certain predefined reward set. In
this paper, we study GAIL in both online and offline settings with linear
function approximation, where both the transition and reward function are
linear in the feature maps. Besides the expert demonstration, in the online
setting the agent can interact with the environment, while in the offline
setting the agent only accesses an additional dataset collected by a prior. For
online GAIL, we propose an optimistic generative adversarial policy
optimization algorithm (OGAP) and prove that OGAP achieves
$\widetilde{\mathcal{O}}(H^2 d^{3/2}K^{1/2}+KH^{3/2}dN_1^{-1/2})$ regret. Here
$N_1$ represents the number of trajectories of the expert demonstration, $d$ is
the feature dimension, and $K$ is the number of episodes.

For offline GAIL, we propose a pessimistic generative adversarial policy
optimization algorithm (PGAP). For an arbitrary additional dataset, we obtain
the optimality gap of PGAP, achieving the minimax lower bound in the
utilization of the additional dataset. Assuming sufficient coverage on the
additional dataset, we show that PGAP achieves
$\widetilde{\mathcal{O}}(H^{2}dK^{-1/2}
+H^2d^{3/2}N_2^{-1/2}+H^{3/2}dN_1^{-1/2} \ )$ optimality gap. Here $N_2$
represents the number of trajectories of the additional dataset with sufficient
coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhihan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yufeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zuyue Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminating modelling approaches for Point in Time Economic Scenario Generation. (arXiv:2108.08818v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2108.08818</id>
        <link href="http://arxiv.org/abs/2108.08818"/>
        <updated>2021-08-20T01:53:52.748Z</updated>
        <summary type="html"><![CDATA[We introduce the notion of Point in Time Economic Scenario Generation (PiT
ESG) with a clear mathematical problem formulation to unify and compare
economic scenario generation approaches conditional on forward looking market
data. Such PiT ESGs should provide quicker and more flexible reactions to
sudden economic changes than traditional ESGs calibrated solely to long periods
of historical data. We specifically take as economic variable the S&P500 Index
with the VIX Index as forward looking market data to compare the nonparametric
filtered historical simulation, GARCH model with joint likelihood estimation
(parametric), Restricted Boltzmann Machine and the conditional Variational
Autoencoder (Generative Networks) for their suitability as PiT ESG. Our
evaluation consists of statistical tests for model fit and benchmarking the out
of sample forecasting quality with a strategy backtest using model output as
stop loss criterion. We find that both Generative Networks outperform the
nonparametric and classic parametric model in our tests, but that the CVAE
seems to be particularly well suited for our purposes: yielding more robust
performance and being computationally lighter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EqGNN: Equalized Node Opportunity in Graphs. (arXiv:2108.08800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08800</id>
        <link href="http://arxiv.org/abs/2108.08800"/>
        <updated>2021-08-20T01:53:52.734Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs), has been widely used for supervised learning
tasks in graphs reaching state-of-the-art results. However, little work was
dedicated to creating unbiased GNNs, i.e., where the classification is
uncorrelated with sensitive attributes, such as race or gender. Some ignore the
sensitive attributes or optimize for the criteria of statistical parity for
fairness. However, it has been shown that neither approaches ensure fairness,
but rather cripple the utility of the prediction task. In this work, we present
a GNN framework that allows optimizing representations for the notion of
Equalized Odds fairness criteria. The architecture is composed of three
components: (1) a GNN classifier predicting the utility class, (2) a sampler
learning the distribution of the sensitive attributes of the nodes given their
labels. It generates samples fed into a (3) discriminator that discriminates
between true and sampled sensitive attributes using a novel "permutation loss"
function. Using these components, we train a model to neglect information
regarding the sensitive attribute only with respect to its label. To the best
of our knowledge, we are the first to optimize GNNs for the equalized odds
criteria. We evaluate our classifier over several graph datasets and sensitive
attributes and show our algorithm reaches state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1"&gt;Uriel Singer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1"&gt;Kira Radinsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation. (arXiv:2108.08762v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.08762</id>
        <link href="http://arxiv.org/abs/2108.08762"/>
        <updated>2021-08-20T01:53:52.715Z</updated>
        <summary type="html"><![CDATA[Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1"&gt;Silvan Mertes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangelova_S/0/1/0/all/0/1"&gt;Stanislava Rangelova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flutura_S/0/1/0/all/0/1"&gt;Simon Flutura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08536</id>
        <link href="http://arxiv.org/abs/2108.08536"/>
        <updated>2021-08-20T01:53:52.709Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims
at inferring novel object categories in an unlabeled set by leveraging from
prior knowledge of a labeled set containing different, but related classes.
Existing approaches tackle this problem by considering multiple objective
functions, usually involving specialized loss terms for the labeled and the
unlabeled samples respectively, and often requiring auxiliary regularization
terms. In this paper, we depart from this traditional scheme and introduce a
UNified Objective function (UNO) for discovering novel classes, with the
explicit purpose of favoring synergy between supervised and unsupervised
learning. Using a multi-view self-labeling strategy, we generate pseudo-labels
that can be treated homogeneously with ground truth labels. This leads to a
single classification objective operating on both known and unknown classes.
Despite its simplicity, UNO outperforms the state of the art by a significant
margin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The
project page is available at: \url{https://ncd-uno.github.io}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1"&gt;Enrico Fini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1"&gt;Moin Nabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple is better: Making Decision Trees faster using random sampling. (arXiv:2108.08790v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08790</id>
        <link href="http://arxiv.org/abs/2108.08790"/>
        <updated>2021-08-20T01:53:52.702Z</updated>
        <summary type="html"><![CDATA[In recent years, gradient boosted decision trees have become popular in
building robust machine learning models on big data. The primary technique that
has enabled these algorithms success has been distributing the computation
while building the decision trees. A distributed decision tree building, in
turn, has been enabled by building quantiles of the big datasets and choosing
the candidate split points from these quantile sets. In XGBoost, for instance,
a sophisticated quantile building algorithm is employed to identify the
candidate split points for the decision trees. This method is often projected
to yield better results when the computation is distributed. In this paper, we
dispel the notion that these methods provide more accurate and scalable methods
for building decision trees in a distributed manner. In a significant
contribution, we show theoretically and empirically that choosing the split
points uniformly at random provides the same or even better performance in
terms of accuracy and computational efficiency. Hence, a simple random
selection of points suffices for decision tree building compared to more
sophisticated methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vignesh Nanda Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edakunni_N/0/1/0/all/0/1"&gt;Narayanan U Edakunni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiReN: Sign-Aware Recommendation Using Graph Neural Networks. (arXiv:2108.08735v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08735</id>
        <link href="http://arxiv.org/abs/2108.08735"/>
        <updated>2021-08-20T01:53:52.696Z</updated>
        <summary type="html"><![CDATA[In recent years, many recommender systems using network embedding (NE) such
as graph neural networks (GNNs) have been extensively studied in the sense of
improving recommendation accuracy. However, such attempts have focused mostly
on utilizing only the information of positive user-item interactions with high
ratings. Thus, there is a challenge on how to make use of low rating scores for
representing users' preferences since low ratings can be still informative in
designing NE-based recommender systems. In this study, we present SiReN, a new
sign-aware recommender system based on GNN models. Specifically, SiReN has
three key components: 1) constructing a signed bipartite graph for more
precisely representing users' preferences, which is split into two
edge-disjoint graphs with positive and negative edges each, 2) generating two
embeddings for the partitioned graphs with positive and negative edges via a
GNN model and a multi-layer perceptron (MLP), respectively, and then using an
attention model to obtain the final embeddings, and 3) establishing a
sign-aware Bayesian personalized ranking (BPR) loss function in the process of
optimization. Through comprehensive experiments, we empirically demonstrate
that SiReN consistently outperforms state-of-the-art NE-aided recommendation
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1"&gt;Changwon Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1"&gt;Kyeong-Joong Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Sungsu Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won-Yong Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IT2CFNN: An Interval Type-2 Correlation-Aware Fuzzy Neural Network to Construct Non-Separable Fuzzy Rules with Uncertain and Adaptive Shapes for Nonlinear Function Approximation. (arXiv:2108.08704v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08704</id>
        <link href="http://arxiv.org/abs/2108.08704"/>
        <updated>2021-08-20T01:53:52.689Z</updated>
        <summary type="html"><![CDATA[In this paper, a new interval type-2 fuzzy neural network able to construct
non-separable fuzzy rules with adaptive shapes is introduced. To reflect the
uncertainty, the shape of fuzzy sets considered to be uncertain. Therefore, a
new form of interval type-2 fuzzy sets based on a general Gaussian model able
to construct different shapes (including triangular, bell-shaped, trapezoidal)
is proposed. To consider the interactions among input variables, input vectors
are transformed to new feature spaces with uncorrelated variables proper for
defining each fuzzy rule. Next, the new features are fed to a fuzzification
layer using proposed interval type-2 fuzzy sets with adaptive shape.
Consequently, interval type-2 non-separable fuzzy rules with proper shapes,
considering the local interactions of variables and the uncertainty are formed.
For type reduction the contribution of the upper and lower firing strengths of
each fuzzy rule are adaptively selected separately. To train different
parameters of the network, the Levenberg-Marquadt optimization method is
utilized. The performance of the proposed method is investigated on clean and
noisy datasets to show the ability to consider the uncertainty. Moreover, the
proposed paradigm, is successfully applied to real-world time-series
predictions, regression problems, and nonlinear system identification.
According to the experimental results, the performance of our proposed model
outperforms other methods with a more parsimonious structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salimi_Badr_A/0/1/0/all/0/1"&gt;Armin Salimi-Badr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2108.08635v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.08635</id>
        <link href="http://arxiv.org/abs/2108.08635"/>
        <updated>2021-08-20T01:53:52.671Z</updated>
        <summary type="html"><![CDATA[This paper presents a sensor fusion based Global Navigation Satellite System
(GNSS) spoofing attack detection framework for autonomous vehicles (AV) that
consists of two concurrent strategies: (i) detection of vehicle state using
predicted location shift -- i.e., distance traveled between two consecutive
timestamps -- and monitoring of vehicle motion state -- i.e., standstill/ in
motion; and (ii) detection and classification of turns (i.e., left or right).
Data from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering
angle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural
network model, which is a long short-term memory (LSTM) network for predicting
the location shift, i.e., the distance that an AV travels between two
consecutive timestamps. This location shift is then compared with the
GNSS-based location shift to detect an attack. We have then combined k-Nearest
Neighbors (k-NN) and Dynamic Time Warping (DTW) algorithms to detect and
classify left and right turns using data from the steering angle sensor. To
prove the efficacy of the sensor fusion-based attack detection framework,
attack datasets are created for four unique and sophisticated spoofing
attacks-turn-by-turn, overshoot, wrong turn, and stop, using the publicly
available real-world Honda Research Institute Driving Dataset (HDD). Our
analysis reveals that the sensor fusion-based detection framework successfully
detects all four types of spoofing attacks within the required computational
latency threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mhafuzul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Client Selection Approach in Support of Clustered Federated Learning over Wireless Edge Networks. (arXiv:2108.08768v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.08768</id>
        <link href="http://arxiv.org/abs/2108.08768"/>
        <updated>2021-08-20T01:53:52.665Z</updated>
        <summary type="html"><![CDATA[Clustered Federated Multitask Learning (CFL) was introduced as an efficient
scheme to obtain reliable specialized models when data is imbalanced and
distributed in a non-i.i.d. (non-independent and identically distributed)
fashion amongst clients. While a similarity measure metric, like the cosine
similarity, can be used to endow groups of the client with a specialized model,
this process can be arduous as the server should involve all clients in each of
the federated learning rounds. Therefore, it is imperative that a subset of
clients is selected periodically due to the limited bandwidth and latency
constraints at the network edge. To this end, this paper proposes a new client
selection algorithm that aims to accelerate the convergence rate for obtaining
specialized machine learning models that achieve high test accuracies for all
client groups. Specifically, we introduce a client selection approach that
leverages the devices' heterogeneity to schedule the clients based on their
round latency and exploits the bandwidth reuse for clients that consume more
time to update the model. Then, the server performs model averaging and
clusters the clients based on predefined thresholds. When a specific cluster
reaches a stationary point, the proposed algorithm uses a greedy scheduling
algorithm for that group by selecting the clients with less latency to update
the model. Extensive experiments show that the proposed approach lowers the
training time and accelerates the convergence rate by up to 50% while imbuing
each client with a specialized model that is fit for its local data
distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1"&gt;Abdullatif Albaseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1"&gt;Mohamed Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1"&gt;Ala Al-Fuqaha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-to-learn non-convex piecewise-Lipschitz functions. (arXiv:2108.08770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08770</id>
        <link href="http://arxiv.org/abs/2108.08770"/>
        <updated>2021-08-20T01:53:52.658Z</updated>
        <summary type="html"><![CDATA[We analyze the meta-learning of the initialization and step-size of learning
algorithms for piecewise-Lipschitz functions, a non-convex setting with
applications to both machine learning and algorithms. Starting from recent
regret bounds for the exponential forecaster on losses with dispersed
discontinuities, we generalize them to be initialization-dependent and then use
this result to propose a practical meta-learning procedure that learns both the
initialization and the step-size of the algorithm from multiple online learning
tasks. Asymptotically, we guarantee that the average regret across tasks scales
with a natural notion of task-similarity that measures the amount of overlap
between near-optimal regions of different tasks. Finally, we instantiate the
method and its guarantee in two important settings: robust meta-learning and
multi-task data-driven algorithm design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1"&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1"&gt;Mikhail Khodak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Dravyansh Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Tensor Train: a Flexible and Efficient Approach for Learning Multiple Multilinear Correlations. (arXiv:2108.08659v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08659</id>
        <link href="http://arxiv.org/abs/2108.08659"/>
        <updated>2021-08-20T01:53:52.651Z</updated>
        <summary type="html"><![CDATA[Tensor Train (TT) approach has been successfully applied in the modelling of
the multilinear interaction of features. Nevertheless, the existing models lack
flexibility and generalizability, as they only model a single type of
high-order correlation. In practice, multiple multilinear correlations may
exist within the features. In this paper, we present a novel Residual Tensor
Train (ResTT) which integrates the merits of TT and residual structure to
capture the multilinear feature correlations, from low to higher orders, within
the same model. In particular, we prove that the fully-connected layer in
neural networks and the Volterra series can be taken as special cases of ResTT.
Furthermore, we derive the rule for weight initialization that stabilizes the
training of ResTT based on a mean-field analysis. We prove that such a rule is
much more relaxed than that of TT, which means ResTT can easily address the
vanishing and exploding gradient problem that exists in the current TT models.
Numerical experiments demonstrate that ResTT outperforms the state-of-the-art
tensor network approaches, and is competitive with the benchmark deep learning
models on MNIST and Fashion-MNIST datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yu Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Daoyi Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. (arXiv:2108.08839v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08839</id>
        <link href="http://arxiv.org/abs/2108.08839"/>
        <updated>2021-08-20T01:53:52.644Z</updated>
        <summary type="html"><![CDATA[Point clouds captured in real-world applications are often incomplete due to
the limited sensor resolution, single viewpoint, and occlusion. Therefore,
recovering the complete point clouds from partial ones becomes an indispensable
task in many practical applications. In this paper, we present a new method
that reformulates point cloud completion as a set-to-set translation problem
and design a new model, called PoinTr that adopts a transformer encoder-decoder
architecture for point cloud completion. By representing the point cloud as a
set of unordered groups of points with position embeddings, we convert the
point cloud to a sequence of point proxies and employ the transformers for
point cloud generation. To facilitate transformers to better leverage the
inductive bias about 3D geometric structures of point clouds, we further devise
a geometry-aware block that models the local geometric relationships
explicitly. The migration of transformers enables our model to better learn
structural knowledge and preserve detailed information for point cloud
completion. Furthermore, we propose two more challenging benchmarks with more
diverse incomplete point clouds that can better reflect the real-world
scenarios to promote future research. Experimental results show that our method
outperforms state-of-the-art methods by a large margin on both the new
benchmarks and the existing ones. Code is available at
https://github.com/yuxumin/PoinTr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xumin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zuyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image2Lego: Customized LEGO Set Generation from Images. (arXiv:2108.08477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08477</id>
        <link href="http://arxiv.org/abs/2108.08477"/>
        <updated>2021-08-20T01:53:52.636Z</updated>
        <summary type="html"><![CDATA[Although LEGO sets have entertained generations of children and adults, the
challenge of designing customized builds matching the complexity of real-world
or imagined scenes remains too great for the average enthusiast. In order to
make this feat possible, we implement a system that generates a LEGO brick
model from 2D images. We design a novel solution to this problem that uses an
octree-structured autoencoder trained on 3D voxelized models to obtain a
feasible latent representation for model reconstruction, and a separate network
trained to predict this latent representation from 2D images. LEGO models are
obtained by algorithmic conversion of the 3D voxelized model to bricks. We
demonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An
octree architecture enables the flexibility to produce multiple resolutions to
best fit a user's creative vision or design needs. In order to demonstrate the
broad applicability of our system, we generate step-by-step building
instructions and animations for LEGO models of objects and human faces.
Finally, we test these automatically generated LEGO sets by constructing
physical builds using real LEGO bricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lennon_K/0/1/0/all/0/1"&gt;Kyle Lennon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fransen_K/0/1/0/all/0/1"&gt;Katharina Fransen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1"&gt;Alexander O&amp;#x27;Brien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yumeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1"&gt;Matthew Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arefeen_Y/0/1/0/all/0/1"&gt;Yamin Arefeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Nikhil Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Curation for Unsupervised Contrastive Representation Learning. (arXiv:2108.08643v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08643</id>
        <link href="http://arxiv.org/abs/2108.08643"/>
        <updated>2021-08-20T01:53:52.618Z</updated>
        <summary type="html"><![CDATA[The state-of-the-art unsupervised contrastive visual representation learning
methods that have emerged recently (SimCLR, MoCo, SwAV) all make use of data
augmentations in order to construct a pretext task of instant discrimination
consisting of similar and dissimilar pairs of images. Similar pairs are
constructed by randomly extracting patches from the same image and applying
several other transformations such as color jittering or blurring, while
transformed patches from different image instances in a given batch are
regarded as dissimilar pairs. We argue that this approach can result similar
pairs that are \textit{semantically} dissimilar. In this work, we address this
problem by introducing a \textit{batch curation} scheme that selects batches
during the training process that are more inline with the underlying
contrastive objective. We provide insights into what constitutes beneficial
similar and dissimilar pairs as well as validate \textit{batch curation} on
CIFAR10 by integrating it in the SimCLR model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Welle_M/0/1/0/all/0/1"&gt;Michael C. Welle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poklukar_P/0/1/0/all/0/1"&gt;Petra Poklukar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1"&gt;Danica Kragic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. (arXiv:2003.04390v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04390</id>
        <link href="http://arxiv.org/abs/2003.04390"/>
        <updated>2021-08-20T01:53:52.611Z</updated>
        <summary type="html"><![CDATA[Meta-learning has been the most common framework for few-shot learning in
recent years. It learns the model from collections of few-shot classification
tasks, which is believed to have a key advantage of making the training
objective consistent with the testing objective. However, some recent works
report that by training for whole-classification, i.e. classification on the
whole label-set, it can get comparable or even better embedding than many
meta-learning algorithms. The edge between these two lines of works has yet
been underexplored, and the effectiveness of meta-learning in few-shot learning
remains unclear. In this paper, we explore a simple process: meta-learning over
a whole-classification pre-trained model on its evaluation metric. We observe
this simple method achieves competitive performance to state-of-the-art methods
on standard benchmarks. Our further analysis shed some light on understanding
the trade-offs between the meta-learning objective and the whole-classification
objective in few-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinbo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huijuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural density estimation and uncertainty quantification for laser induced breakdown spectroscopy spectra. (arXiv:2108.08709v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08709</id>
        <link href="http://arxiv.org/abs/2108.08709"/>
        <updated>2021-08-20T01:53:52.604Z</updated>
        <summary type="html"><![CDATA[Constructing probability densities for inference in high-dimensional spectral
data is often intractable. In this work, we use normalizing flows on structured
spectral latent spaces to estimate such densities, enabling downstream
inference tasks. In addition, we evaluate a method for uncertainty
quantification when predicting unobserved state vectors associated with each
spectrum. We demonstrate the capability of this approach on laser-induced
breakdown spectroscopy data collected by the ChemCam instrument on the Mars
rover Curiosity. Using our approach, we are able to generate realistic spectral
samples and to accurately predict state vectors with associated well-calibrated
uncertainties. We anticipate that this methodology will enable efficient
probabilistic modeling of spectral data, leading to potential advances in
several areas, including out-of-distribution detection and sensitivity
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kontolati_K/0/1/0/all/0/1"&gt;Katiana Kontolati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_N/0/1/0/all/0/1"&gt;Natalie Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_N/0/1/0/all/0/1"&gt;Nishant Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1"&gt;Diane Oyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determinant-free fermionic wave function using feed-forward neural network. (arXiv:2108.08631v1 [cond-mat.str-el])]]></title>
        <id>http://arxiv.org/abs/2108.08631</id>
        <link href="http://arxiv.org/abs/2108.08631"/>
        <updated>2021-08-20T01:53:52.597Z</updated>
        <summary type="html"><![CDATA[We propose a general framework for finding the ground state of many-body
fermionic systems by using feed-forward neural networks. The anticommutation
relation for fermions is usually implemented to a variational wave function by
the Slater determinant (or Pfaffian), which is a computational bottleneck
because of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this
bottleneck by explicitly calculating the sign changes associated with particle
exchanges in real space and using fully connected neural networks for
optimizing the rest parts of the wave function. This reduces the computational
cost to $O(N^2)$ or less. We show that the accuracy of the approximation can be
improved by optimizing the "variance" of the energy simultaneously with the
energy itself. We also find that a reweighting method in Monte Carlo sampling
can stabilize the calculation. These improvements can be applied to other
approaches based on variational Monte Carlo methods. Moreover, we show that the
accuracy can be further improved by using the symmetry of the system, the
representative states, and an additional neural network implementing a
generalized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the
method by applying it to a two-dimensional Hubbard model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Inui_K/0/1/0/all/0/1"&gt;Koji Inui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kato_Y/0/1/0/all/0/1"&gt;Yasuyuki Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Motome_Y/0/1/0/all/0/1"&gt;Yukitoshi Motome&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50. (arXiv:2108.08473v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08473</id>
        <link href="http://arxiv.org/abs/2108.08473"/>
        <updated>2021-08-20T01:53:52.591Z</updated>
        <summary type="html"><![CDATA[In this work, deep learning algorithms are used to classify fundus images in
terms of diabetic retinopathy severity. Six different combinations of two model
architectures, the Dense Convolutional Network-121 and the Residual Neural
Network-50 and three image types, RGB, Green, and High Contrast, were tested to
find the highest performing combination. We achieved an average validation loss
of 0.17 and a max validation accuracy of 85 percent. By testing out multiple
combinations, certain combinations of parameters performed better than others,
though minimal variance was found overall. Green filtration was shown to
perform the poorest, while amplified contrast appeared to have a negligible
effect in comparison to RGB analysis. ResNet50 proved to be less of a robust
model as opposed to DenseNet121.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jonathan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1"&gt;Bowen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ram_R/0/1/0/all/0/1"&gt;Rahul Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;David Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Is All You Need to Improve the Robustness and Safety for the First Time Deployment of Meta RL. (arXiv:2108.08448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08448</id>
        <link href="http://arxiv.org/abs/2108.08448"/>
        <updated>2021-08-20T01:53:52.571Z</updated>
        <summary type="html"><![CDATA[The field of Meta Reinforcement Learning (Meta-RL) has seen substantial
advancements recently. In particular, off-policy methods were developed to
improve the data efficiency of Meta-RL techniques. \textit{Probabilistic
embeddings for actor-critic RL} (PEARL) is currently one of the leading
approaches for multi-MDP adaptation problems. A major drawback of many existing
Meta-RL methods, including PEARL, is that they do not explicitly consider the
safety of the prior policy when it is exposed to a new task for the very first
time. This is very important for some real-world applications, including field
robots and Autonomous Vehicles (AVs). In this paper, we develop the PEARL PLUS
(PEARL$^+$) algorithm, which optimizes the policy for both prior safety and
posterior adaptation. Building on top of PEARL, our proposed PEARL$^+$
algorithm introduces a prior regularization term in the reward function and a
new Q-network for recovering the state-action value with prior context
assumption, to improve the robustness and safety of the trained network
exposing to a new task for the first time. The performance of the PEARL$^+$
method is demonstrated by solving three safety-critical decision-making
problems related to robots and AVs, including two MuJoCo benchmark problems.
From the simulation experiments, we show that the safety of the prior policy is
significantly improved compared to that of the original PEARL method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Lu Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Songan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1"&gt;H. Eric Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1"&gt;Baljeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filev_D/0/1/0/all/0/1"&gt;Dimitar Filev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Huei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Human Decision-Making with Machine Learning. (arXiv:2108.08454v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08454</id>
        <link href="http://arxiv.org/abs/2108.08454"/>
        <updated>2021-08-20T01:53:52.565Z</updated>
        <summary type="html"><![CDATA[A key aspect of human intelligence is their ability to convey their knowledge
to others in succinct forms. However, despite their predictive power, current
machine learning models are largely blackboxes, making it difficult for humans
to extract useful insights. Focusing on sequential decision-making, we design a
novel machine learning algorithm that conveys its insights to humans in the
form of interpretable "tips". Our algorithm selects the tip that best bridges
the gap in performance between human users and the optimal policy. We evaluate
our approach through a series of randomized controlled user studies where
participants manage a virtual kitchen. Our experiments show that the tips
generated by our algorithm can significantly improve human performance relative
to intuitive baselines. In addition, we discuss a number of empirical insights
that can help inform the design of algorithms intended for human-AI
collaboration. For instance, we find evidence that participants do not simply
blindly follow our tips; instead, they combine them with their own experience
to discover additional strategies for improving performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_H/0/1/0/all/0/1"&gt;Hamsa Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1"&gt;Osbert Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinchaisri_W/0/1/0/all/0/1"&gt;Wichinpong Park Sinchaisri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Learning of Stochastic Policies withContinuous Actions: from Models to Offline Evaluation. (arXiv:2004.11722v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11722</id>
        <link href="http://arxiv.org/abs/2004.11722"/>
        <updated>2021-08-20T01:53:52.559Z</updated>
        <summary type="html"><![CDATA[Counterfactual reasoning from logged data has become increasingly important
for many applications such as web advertising or healthcare. In this paper, we
address the problem of learning stochastic policies with continuous actions
from the viewpoint of counterfactual risk minimization (CRM). While the CRM
framework is appealing and well studied for discrete actions, the continuous
action case raises new challenges about modelization, optimization, and~offline
model selection with real data which turns out to be particularly challenging.
Our paper contributes to these three aspects of the CRM estimation pipeline.
First, we introduce a modelling strategy based on a joint kernel embedding of
contexts and actions, which overcomes the shortcomings of previous
discretization approaches. Second, we empirically show that the optimization
aspect of counterfactual learning is important, and we demonstrate the benefits
of proximal point algorithms and differentiable estimators. Finally, we propose
an evaluation protocol for offline policies in real-world logged systems, which
is challenging since policies cannot be replayed on test data, and we release a
new large-scale dataset along with multiple synthetic, yet realistic,
evaluation setups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zenati_H/0/1/0/all/0/1"&gt;Houssam Zenati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martin_M/0/1/0/all/0/1"&gt;Matthieu Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Diemert_E/0/1/0/all/0/1"&gt;Eustache Diemert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attribute-based Explanations of Non-Linear Embeddings of High-Dimensional Data. (arXiv:2108.08706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08706</id>
        <link href="http://arxiv.org/abs/2108.08706"/>
        <updated>2021-08-20T01:53:52.551Z</updated>
        <summary type="html"><![CDATA[Embeddings of high-dimensional data are widely used to explore data, to
verify analysis results, and to communicate information. Their explanation, in
particular with respect to the input attributes, is often difficult. With
linear projects like PCA the axes can still be annotated meaningfully. With
non-linear projections this is no longer possible and alternative strategies
such as attribute-based color coding are required. In this paper, we review
existing augmentation techniques and discuss their limitations. We present the
Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation
strategy for projected data (rangesets) with interactive analysis in a small
multiples setting. Rangesets use a set-based visualization approach for binned
attribute values that enable the user to quickly observe structure and detect
outliers. We detail the link between algebraic topology and rangesets and
demonstrate the utility of NoLiES in case studies with various challenges
(complex attribute value distribution, many attributes, many data points) and a
real-world application to understand latent features of matrix completion in
thermodynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sohns_J/0/1/0/all/0/1"&gt;Jan-Tobias Sohns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1"&gt;Michaela Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jirasek_F/0/1/0/all/0/1"&gt;Fabian Jirasek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasse_H/0/1/0/all/0/1"&gt;Hans Hasse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leitte_H/0/1/0/all/0/1"&gt;Heike Leitte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Innovative Attack Modelling and Attack Detection Approach for a Waiting Time-based Adaptive Traffic Signal Controller. (arXiv:2108.08627v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08627</id>
        <link href="http://arxiv.org/abs/2108.08627"/>
        <updated>2021-08-20T01:53:52.545Z</updated>
        <summary type="html"><![CDATA[An adaptive traffic signal controller (ATSC) combined with a connected
vehicle (CV) concept uses real-time vehicle trajectory data to regulate green
time and has the ability to reduce intersection waiting time significantly and
thereby improve travel time in a signalized corridor. However, the CV-based
ATSC increases the size of the surface vulnerable to potential cyber-attack,
allowing an attacker to generate disastrous traffic congestion in a roadway
network. An attacker can congest a route by generating fake vehicles by
maintaining traffic and car-following rules at a slow rate so that the signal
timing and phase change without having any abrupt changes in number of
vehicles. Because of the adaptive nature of ATSC, it is a challenge to model
this kind of attack and also to develop a strategy for detection. This paper
introduces an innovative "slow poisoning" cyberattack for a waiting time based
ATSC algorithm and a corresponding detection strategy. Thus, the objectives of
this paper are to: (i) develop a "slow poisoning" attack generation strategy
for an ATSC, and (ii) develop a prediction-based "slow poisoning" attack
detection strategy using a recurrent neural network -- i.e., long short-term
memory model. We have generated a "slow poisoning" attack modeling strategy
using a microscopic traffic simulator -- Simulation of Urban Mobility (SUMO) --
and used generated data from the simulation to develop both the attack model
and detection model. Our analyses revealed that the attack strategy is
effective in creating a congestion in an approach and detection strategy is
able to flag the attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hollis_C/0/1/0/all/0/1"&gt;Courtland Hollis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkison_T/0/1/0/all/0/1"&gt;Travis Atkison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-input Multi-output Transformer-based Hybrid Neural Network for Multi-class Privacy Disclosure Detection. (arXiv:2108.08483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08483</id>
        <link href="http://arxiv.org/abs/2108.08483"/>
        <updated>2021-08-20T01:53:52.539Z</updated>
        <summary type="html"><![CDATA[The concern regarding users' data privacy has risen to its highest level due
to the massive increase in communication platforms, social networking sites,
and greater users' participation in online public discourse. An increasing
number of people exchange private information via emails, text messages, and
social media without being aware of the risks and implications. Researchers in
the field of Natural Language Processing (NLP) have concentrated on creating
tools and strategies to identify, categorize, and sanitize private information
in text data since a substantial amount of data is exchanged in textual form.
However, most of the detection methods solely rely on the existence of
pre-identified keywords in the text and disregard the inference of the
underlying meaning of the utterance in a specific context. Hence, in some
situations, these tools and algorithms fail to detect disclosure, or the
produced results are miss-classified. In this paper, we propose a multi-input,
multi-output hybrid neural network which utilizes transfer-learning,
linguistics, and metadata to learn the hidden patterns. Our goal is to better
classify disclosure/non-disclosure content in terms of the context of
situation. We trained and evaluated our model on a human-annotated ground truth
dataset, containing a total of 5,400 tweets. The results show that the proposed
model was able to identify privacy disclosure through tweets with an accuracy
of 77.4% while classifying the information type of those tweets with an
impressive accuracy of 99%, by jointly learning for two separate tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehdy_A/0/1/0/all/0/1"&gt;A K M Nuhil Mehdy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrpouyan_H/0/1/0/all/0/1"&gt;Hoda Mehrpouyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policies for elementary links in a quantum network. (arXiv:2007.03193v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03193</id>
        <link href="http://arxiv.org/abs/2007.03193"/>
        <updated>2021-08-20T01:53:52.519Z</updated>
        <summary type="html"><![CDATA[Distributing entanglement over long distances is one of the central tasks in
quantum networks. An important problem, especially for near-term quantum
networks, is to develop optimal entanglement distribution protocols that take
into account the limitations of current and near-term hardware, such as quantum
memories with limited coherence time. We address this problem by initiating the
study of quantum network protocols for entanglement distribution using the
theory of decision processes, such that optimal protocols (referred to as
policies in the context of decision processes) can be found using dynamic
programming or reinforcement learning algorithms. As a first step, in this work
we focus exclusively on the elementary link level. We start by defining a
quantum decision process for elementary links, along with figures of merit for
evaluating policies. We then provide two algorithms for determining policies,
one of which we prove to be optimal (with respect to fidelity and success
probability) among all policies. Then we show that the previously-studied
memory-cutoff protocol can be phrased as a policy within our decision process
framework, allowing us to obtain several new fundamental results about it. The
conceptual developments and results of this work pave the way for the
systematic study of the fundamental limitations of near-term quantum networks,
and the requirements for physically realizing them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Khatri_S/0/1/0/all/0/1"&gt;Sumeet Khatri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Approach for GNSS Spoofing Attack Detection of Autonomous Vehicles. (arXiv:2108.08628v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.08628</id>
        <link href="http://arxiv.org/abs/2108.08628"/>
        <updated>2021-08-20T01:53:52.513Z</updated>
        <summary type="html"><![CDATA[A resilient and robust positioning, navigation, and timing (PNT) system is a
necessity for the navigation of autonomous vehicles (AVs). Global Navigation
Satelite System (GNSS) provides satellite-based PNT services. However, a
spoofer can temper an authentic GNSS signal and could transmit wrong position
information to an AV. Therefore, a GNSS must have the capability of real-time
detection and feedback-correction of spoofing attacks related to PNT receivers,
whereby it will help the end-user (autonomous vehicle in this case) to navigate
safely if it falls into any compromises. This paper aims to develop a deep
reinforcement learning (RL)-based turn-by-turn spoofing attack detection using
low-cost in-vehicle sensor data. We have utilized Honda Driving Dataset to
create attack and non-attack datasets, develop a deep RL model, and evaluate
the performance of the RL-based attack detection model. We find that the
accuracy of the RL model ranges from 99.99% to 100%, and the recall value is
100%. However, the precision ranges from 93.44% to 100%, and the f1 score
ranges from 96.61% to 100%. Overall, the analyses reveal that the RL model is
effective in turn-by-turn spoofing attack detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghosh_T/0/1/0/all/0/1"&gt;Tonmoy Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mizanur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning in the Face of Adversaries. (arXiv:2108.08560v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08560</id>
        <link href="http://arxiv.org/abs/2108.08560"/>
        <updated>2021-08-20T01:53:52.507Z</updated>
        <summary type="html"><![CDATA[The vulnerability of deep neural networks against adversarial examples -
inputs with small imperceptible perturbations - has gained a lot of attention
in the research community recently. Simultaneously, the number of parameters of
state-of-the-art deep learning models has been growing massively, with
implications on the memory and computational resources required to train and
deploy such models. One approach to control the size of neural networks is
retrospectively reducing the number of parameters, so-called neural network
pruning. Available research on the impact of neural network pruning on the
adversarial robustness is fragmentary and often does not adhere to established
principles of robustness evaluation. We close this gap by evaluating the
robustness of pruned models against L-0, L-2 and L-infinity attacks for a wide
range of attack strengths, several architectures, data sets, pruning methods,
and compression rates. Our results confirm that neural network pruning and
adversarial robustness are not mutually exclusive. Instead, sweet spots can be
found that are favorable in terms of model size and adversarial robustness.
Furthermore, we extend our analysis to situations that incorporate additional
assumptions on the adversarial scenario and show that depending on the
situation, different strategies are optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merkle_F/0/1/0/all/0/1"&gt;Florian Merkle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samsinger_M/0/1/0/all/0/1"&gt;Maximilian Samsinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schottle_P/0/1/0/all/0/1"&gt;Pascal Sch&amp;#xf6;ttle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Settling the Variance of Multi-Agent Policy Gradients. (arXiv:2108.08612v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08612</id>
        <link href="http://arxiv.org/abs/2108.08612"/>
        <updated>2021-08-20T01:53:52.500Z</updated>
        <summary type="html"><![CDATA[Policy gradient (PG) methods are popular reinforcement learning (RL) methods
where a baseline is often applied to reduce the variance of gradient estimates.
In multi-agent RL (MARL), although the PG theorem can be naturally extended,
the effectiveness of multi-agent PG (MAPG) methods degrades as the variance of
gradient estimates increases rapidly with the number of agents. In this paper,
we offer a rigorous analysis of MAPG methods by, firstly, quantifying the
contributions of the number of agents and agents' explorations to the variance
of MAPG estimators. Based on this analysis, we derive the optimal baseline (OB)
that achieves the minimal variance. In comparison to the OB, we measure the
excess variance of existing MARL algorithms such as vanilla MAPG and COMA.
Considering using deep neural networks, we also propose a surrogate version of
OB, which can be seamlessly plugged into any existing PG methods in MARL. On
benchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique
effectively stabilises training and improves the performance of multi-agent PPO
and COMA algorithms by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1"&gt;Jakub Grudzien Kuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1"&gt;Muning Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Linghui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shangding Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haifeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1"&gt;David Henry Mguni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrent Discrimination and Alignment for Self-Supervised Feature Learning. (arXiv:2108.08562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08562</id>
        <link href="http://arxiv.org/abs/2108.08562"/>
        <updated>2021-08-20T01:53:52.493Z</updated>
        <summary type="html"><![CDATA[Existing self-supervised learning methods learn representation by means of
pretext tasks which are either (1) discriminating that explicitly specify which
features should be separated or (2) aligning that precisely indicate which
features should be closed together, but ignore the fact how to jointly and
principally define which features to be repelled and which ones to be
attracted. In this work, we combine the positive aspects of the discriminating
and aligning methods, and design a hybrid method that addresses the above
issue. Our method explicitly specifies the repulsion and attraction mechanism
respectively by discriminative predictive task and concurrently maximizing
mutual information between paired views sharing redundant information. We
qualitatively and quantitatively show that our proposed model learns better
features that are more effective for the diverse downstream tasks ranging from
classification to semantic segmentation. Our experiments on nine established
benchmarks show that the proposed model consistently outperforms the existing
state-of-the-art results of self-supervised and transfer learning protocol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning System Parameters from Turing Patterns. (arXiv:2108.08542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08542</id>
        <link href="http://arxiv.org/abs/2108.08542"/>
        <updated>2021-08-20T01:53:52.468Z</updated>
        <summary type="html"><![CDATA[The Turing mechanism describes the emergence of spatial patterns due to
spontaneous symmetry breaking in reaction-diffusion processes and underlies
many developmental processes. Identifying Turing mechanisms in biological
systems defines a challenging problem. This paper introduces an approach to the
prediction of Turing parameter values from observed Turing patterns. The
parameter values correspond to a parametrized system of reaction-diffusion
equations that generate Turing patterns as steady state. The Gierer-Meinhardt
model with four parameters is chosen as a case study. A novel invariant pattern
representation based on resistance distance histograms is employed, along with
Wasserstein kernels, in order to cope with the highly variable arrangement of
local pattern structure that depends on the initial conditions which are
assumed to be unknown. This enables to compute physically plausible distances
between patterns, to compute clusters of patterns and, above all, model
parameter prediction: for small training sets, classical state-of-the-art
methods including operator-valued kernels outperform neural networks that are
applied to raw pattern data, whereas for large training sets the latter are
more accurate. Excellent predictions are obtained for single parameter values
and reasonably accurate results for jointly predicting all parameter values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schnorr_D/0/1/0/all/0/1"&gt;David Schn&amp;#xf6;rr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1"&gt;Christoph Schn&amp;#xf6;rr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially-Adaptive Image Restoration using Distortion-Guided Networks. (arXiv:2108.08617v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08617</id>
        <link href="http://arxiv.org/abs/2108.08617"/>
        <updated>2021-08-20T01:53:52.461Z</updated>
        <summary type="html"><![CDATA[We present a general learning-based solution for restoring images suffering
from spatially-varying degradations. Prior approaches are typically
degradation-specific and employ the same processing across different images and
different pixels within. However, we hypothesize that such spatially rigid
processing is suboptimal for simultaneously restoring the degraded pixels as
well as reconstructing the clean regions of the image. To overcome this
limitation, we propose SPAIR, a network design that harnesses
distortion-localization information and dynamically adjusts computation to
difficult regions in the image. SPAIR comprises of two components, (1) a
localization network that identifies degraded pixels, and (2) a restoration
network that exploits knowledge from the localization network in filter and
feature domain to selectively and adaptively restore degraded pixels. Our key
idea is to exploit the non-uniformity of heavy degradations in spatial-domain
and suitably embed this knowledge within distortion-guided modules performing
sparse normalization, feature extraction and attention. Our architecture is
agnostic to physical formation model and generalizes across several types of
spatially-varying degradations. We demonstrate the efficacy of SPAIR
individually on four restoration tasks-removal of rain-streaks, raindrops,
shadows and motion blur. Extensive qualitative and quantitative comparisons
with prior art on 11 benchmark datasets demonstrate that our
degradation-agnostic network design offers significant performance gains over
state-of-the-art degradation-specific architectures. Code available at
https://github.com/human-analysis/spatially-adaptive-image-restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1"&gt;Kuldeep Purohit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1"&gt;Maitreya Suin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1"&gt;A. N. Rajagopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1"&gt;Vishnu Naresh Boddeti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Semi-Supervised Learning for Remaining Useful Lifetime Estimation Through Self-Supervision. (arXiv:2108.08721v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08721</id>
        <link href="http://arxiv.org/abs/2108.08721"/>
        <updated>2021-08-20T01:53:52.454Z</updated>
        <summary type="html"><![CDATA[RUL estimation suffers from a server data imbalance where data from machines
near their end of life is rare. Additionally, the data produced by a machine
can only be labeled after the machine failed. Semi-Supervised Learning (SSL)
can incorporate the unlabeled data produced by machines that did not yet fail.
Previous work on SSL evaluated their approaches under unrealistic conditions
where the data near failure was still available. Even so, only moderate
improvements were made. This paper proposes a novel SSL approach based on
self-supervised pre-training. The method can outperform two competing
approaches from the literature and a supervised baseline under realistic
conditions on the NASA C-MAPSS dataset. Nevertheless, we observe degraded
performance in some circumstances and discuss possible causes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krokotsch_T/0/1/0/all/0/1"&gt;Tilman Krokotsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knaak_M/0/1/0/all/0/1"&gt;Mirko Knaak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guhmann_C/0/1/0/all/0/1"&gt;Clemens G&amp;#xfc;hmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (arXiv:2108.08421v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08421</id>
        <link href="http://arxiv.org/abs/2108.08421"/>
        <updated>2021-08-20T01:53:52.435Z</updated>
        <summary type="html"><![CDATA[Vision systems that deploy Deep Neural Networks (DNNs) are known to be
vulnerable to adversarial examples. Recent research has shown that checking the
intrinsic consistencies in the input data is a promising way to detect
adversarial attacks (e.g., by checking the object co-occurrence relationships
in complex scenes). However, existing approaches are tied to specific models
and do not offer generalizability. Motivated by the observation that language
descriptions of natural scene images have already captured the object
co-occurrence relationships that can be learned by a language model, we develop
a novel approach to perform context consistency checks using such language
models. The distinguishing aspect of our approach is that it is independent of
the deployed object detector and yet offers very high accuracy in terms of
detecting adversarial examples in practical scenes with multiple objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Mingjun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shasha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zikui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chengyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;M. Salman Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Srikanth V. Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08504</id>
        <link href="http://arxiv.org/abs/2108.08504"/>
        <updated>2021-08-20T01:53:52.429Z</updated>
        <summary type="html"><![CDATA[The performance of a computer vision model depends on the size and quality of
its training data. Recent studies have unveiled previously-unknown composition
biases in common image datasets which then lead to skewed model outputs, and
have proposed methods to mitigate these biases. However, most existing works
assume that human-generated annotations can be considered gold-standard and
unbiased. In this paper, we reveal that this assumption can be problematic, and
that special care should be taken to prevent models from learning such
annotation biases. We focus on facial expression recognition and compare the
label biases between lab-controlled and in-the-wild datasets. We demonstrate
that many expression datasets contain significant annotation biases between
genders, especially when it comes to the happy and angry expressions, and that
traditional methods cannot fully mitigate such biases in trained models. To
remove expression annotation bias, we propose an AU-Calibrated Facial
Expression Recognition (AUC-FER) framework that utilizes facial action units
(AUs) and incorporates the triplet loss into the objective function.
Experimental results suggest that the proposed method is more effective in
removing expression annotation bias than existing techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TFRD: A Benchmark Dataset for Research on Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2108.08298v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08298</id>
        <link href="http://arxiv.org/abs/2108.08298"/>
        <updated>2021-08-20T01:53:52.409Z</updated>
        <summary type="html"><![CDATA[Heat management plays an important role in engineering. Temperature field
reconstruction of heat source systems (TFR-HSS) with limited monitoring
tensors, performs an essential role in heat management. However, prior methods
with common interpolations usually cannot provide accurate reconstruction. In
addition, there exists no public dataset for widely research of reconstruction
methods to further boost the field reconstruction in engineering. To overcome
this problem, this work construct a specific dataset, namely TFRD, for TFR-HSS
task with commonly used methods, including the interpolation methods and the
surrogate model based methods, as baselines to advance the research over
temperature field reconstruction. First, the TFR-HSS task is mathematically
modelled from real-world engineering problem and three types of numerically
modellings have been constructed to transform the problem into discrete mapping
forms. Besides, this work selects four typical reconstruction problem with
different heat source information and boundary conditions and generate the
standard samples as training and testing samples for further research. Finally,
a comprehensive review of the prior methods for TFR-HSS task as well as recent
widely used deep learning methods is given and we provide a performance
analysis of typical methods on TFRD, which can be served as the baseline
results on this benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiqiang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiaoyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSVMA: A Genetic-Support Vector Machine-Anova method for CAD diagnosis based on Z-Alizadeh Sani dataset. (arXiv:2108.08292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08292</id>
        <link href="http://arxiv.org/abs/2108.08292"/>
        <updated>2021-08-20T01:53:52.311Z</updated>
        <summary type="html"><![CDATA[Coronary heart disease (CAD) is one of the crucial reasons for cardiovascular
mortality in middle-aged people worldwide. The most typical tool is angiography
for diagnosing CAD. The challenges of CAD diagnosis using angiography are
costly and have side effects. One of the alternative solutions is the use of
machine learning-based patterns for CAD diagnosis. Hence, this paper provides a
new hybrid machine learning model called Genetic Support Vector Machine and
Analysis of Variance (GSVMA). The ANOVA is known as the kernel function for
SVM. The proposed model is performed based on the Z-Alizadeh Sani dataset. A
genetic optimization algorithm is used to select crucial features. In addition,
SVM with Anova, Linear SVM, and LibSVM with radial basis function methods were
applied to classify the dataset. As a result, the GSVMA hybrid method performs
better than other methods. This proposed method has the highest accuracy of
89.45% through a 10-fold cross-validation technique with 35 selected features
on the Z-Alizadeh Sani dataset. Therefore, the genetic optimization algorithm
is very effective for improving accuracy. The computer-aided GSVMA method can
be helped clinicians with CAD diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1"&gt;Javad Hassannataj Joloudari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_F/0/1/0/all/0/1"&gt;Faezeh Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nematollahi_M/0/1/0/all/0/1"&gt;Mohammad Ali Nematollahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassannataj_E/0/1/0/all/0/1"&gt;Edris Hassannataj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1"&gt;Amir Mosavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching Uncertainty Quantification in Machine Learning through Use Cases. (arXiv:2108.08712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08712</id>
        <link href="http://arxiv.org/abs/2108.08712"/>
        <updated>2021-08-20T01:53:52.302Z</updated>
        <summary type="html"><![CDATA[Uncertainty in machine learning is not generally taught as general knowledge
in Machine Learning course curricula. In this paper we propose a short
curriculum for a course about uncertainty in machine learning, and complement
the course with a selection of use cases, aimed to trigger discussion and let
students play with the concepts of uncertainty in a programming setting. Our
use cases cover the concept of output uncertainty, Bayesian neural networks and
weight distributions, sources of uncertainty, and out of distribution
detection. We expect that this curriculum and set of use cases motivates the
community to adopt these important concepts into courses for safety in AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1"&gt;Matias Valdenegro-Toro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders. (arXiv:2108.08557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08557</id>
        <link href="http://arxiv.org/abs/2108.08557"/>
        <updated>2021-08-20T01:53:52.296Z</updated>
        <summary type="html"><![CDATA[Human Pose Estimation (HPE) aims at retrieving the 3D position of human
joints from images or videos. We show that current 3D HPE methods suffer a lack
of viewpoint equivariance, namely they tend to fail or perform poorly when
dealing with viewpoints unseen at training time. Deep learning methods often
rely on either scale-invariant, translation-invariant, or rotation-invariant
operations, such as max-pooling. However, the adoption of such procedures does
not necessarily improve viewpoint generalization, rather leading to more
data-dependent methods. To tackle this issue, we propose a novel capsule
autoencoder network with fast Variational Bayes capsule routing, named DECA. By
modeling each joint as a capsule entity, combined with the routing algorithm,
our approach can preserve the joints' hierarchical and geometrical structure in
the feature space, independently from the viewpoint. By achieving viewpoint
equivariance, we drastically reduce the network data dependency at training
time, resulting in an improved ability to generalize for unseen viewpoints. In
the experimental validation, we outperform other methods on depth images from
both seen and unseen viewpoints, both top-view, and front-view. In the RGB
domain, the same network gives state-of-the-art results on the challenging
viewpoint transfer task, also establishing a new framework for top-view HPE.
The code can be found at https://github.com/mmlab-cv/DECA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1"&gt;Nicola Garau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1"&gt;Niccol&amp;#xf2; Bisagno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brodka_P/0/1/0/all/0/1"&gt;Piotr Br&amp;#xf3;dka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1"&gt;Nicola Conci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02456</id>
        <link href="http://arxiv.org/abs/2108.02456"/>
        <updated>2021-08-20T01:53:52.288Z</updated>
        <summary type="html"><![CDATA[Multi-label image recognition is a challenging computer vision task of
practical use. Progresses in this area, however, are often characterized by
complicated methods, heavy computations, and lack of intuitive explanations. To
effectively capture different spatial regions occupied by objects from
different categories, we propose an embarrassingly simple module, named
class-specific residual attention (CSRA). CSRA generates class-specific
features for every category by proposing a simple spatial attention score, and
then combines it with the class-agnostic average pooling feature. CSRA achieves
state-of-the-art results on multilabel recognition, and at the same time is
much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads
to consistent improvement across many diverse pretrained models and datasets
without any extra training. CSRA is both easy to implement and light in
computations, which also enjoys intuitive explanations and visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1"&gt;Ke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyze and Design Network Architectures by Recursion Formulas. (arXiv:2108.08689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08689</id>
        <link href="http://arxiv.org/abs/2108.08689"/>
        <updated>2021-08-20T01:53:52.269Z</updated>
        <summary type="html"><![CDATA[The effectiveness of shortcut/skip-connection has been widely verified, which
inspires massive explorations on neural architecture design. This work attempts
to find an effective way to design new network architectures. It is discovered
that the main difference between network architectures can be reflected in
their recursion formulas. Based on this, a methodology is proposed to design
novel network architectures from the perspective of mathematical formulas.
Afterwards, a case study is provided to generate an improved architecture based
on ResNet. Furthermore, the new architecture is compared with ResNet and then
tested on ResNet-based networks. Massive experiments are conducted on CIFAR and
ImageNet, which witnesses the significant performance improvements provided by
the architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yilin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhaoran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haozhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinggao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Accelerating Distributed Convex Optimizations. (arXiv:2108.08670v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.08670</id>
        <link href="http://arxiv.org/abs/2108.08670"/>
        <updated>2021-08-20T01:53:52.263Z</updated>
        <summary type="html"><![CDATA[This paper studies a distributed multi-agent convex optimization problem. The
system comprises multiple agents in this problem, each with a set of local data
points and an associated local cost function. The agents are connected to a
server, and there is no inter-agent communication. The agents' goal is to learn
a parameter vector that optimizes the aggregate of their local costs without
revealing their local data points. In principle, the agents can solve this
problem by collaborating with the server using the traditional distributed
gradient-descent method. However, when the aggregate cost is ill-conditioned,
the gradient-descent method (i) requires a large number of iterations to
converge, and (ii) is highly unstable against process noise. We propose an
iterative pre-conditioning technique to mitigate the deleterious effects of the
cost function's conditioning on the convergence rate of distributed
gradient-descent. Unlike the conventional pre-conditioning techniques, the
pre-conditioner matrix in our proposed technique updates iteratively to
facilitate implementation on the distributed network. In the distributed
setting, we provably show that the proposed algorithm converges linearly with
an improved rate of convergence than the traditional and adaptive
gradient-descent methods. Additionally, for the special case when the minimizer
of the aggregate cost is unique, our algorithm converges superlinearly. We
demonstrate our algorithm's superior performance compared to prominent
distributed algorithms for solving real logistic regression problems and
emulating neural network training via a noisy quadratic model, thereby
signifying the proposed algorithm's efficiency for distributively solving
non-convex optimization. Moreover, we empirically show that the proposed
algorithm results in faster training without compromising the generalization
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chakrabarti_K/0/1/0/all/0/1"&gt;Kushal Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nirupam Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chopra_N/0/1/0/all/0/1"&gt;Nikhil Chopra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04632</id>
        <link href="http://arxiv.org/abs/2106.04632"/>
        <updated>2021-08-20T01:53:52.257Z</updated>
        <summary type="html"><![CDATA[Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-benchmark.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Licheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1"&gt;Rohit Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara Lee Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate and Propagation Graph Attention Network for Spatial-Temporal Prediction with Outdoor Cellular Traffic. (arXiv:2108.08307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08307</id>
        <link href="http://arxiv.org/abs/2108.08307"/>
        <updated>2021-08-20T01:53:52.250Z</updated>
        <summary type="html"><![CDATA[Spatial-temporal prediction is a critical problem for intelligent
transportation, which is helpful for tasks such as traffic control and accident
prevention. Previous studies rely on large-scale traffic data collected from
sensors. However, it is unlikely to deploy sensors in all regions due to the
device and maintenance costs. This paper addresses the problem via outdoor
cellular traffic distilled from over two billion records per day in a telecom
company, because outdoor cellular traffic induced by user mobility is highly
related to transportation traffic. We study road intersections in urban and aim
to predict future outdoor cellular traffic of all intersections given historic
outdoor cellular traffic. Furthermore, We propose a new model for multivariate
spatial-temporal prediction, mainly consisting of two extending graph attention
networks (GAT). First GAT is used to explore correlations among multivariate
cellular traffic. Another GAT leverages the attention mechanism into graph
propagation to increase the efficiency of capturing spatial dependency.
Experiments show that the proposed model significantly outperforms the
state-of-the-art methods on our dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_S/0/1/0/all/0/1"&gt;Shen-Lung Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Contrastive Learning for Multi-View Network Embedding. (arXiv:2108.08296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08296</id>
        <link href="http://arxiv.org/abs/2108.08296"/>
        <updated>2021-08-20T01:53:52.244Z</updated>
        <summary type="html"><![CDATA[Multi-view network embedding aims at projecting nodes in the network to
low-dimensional vectors, while preserving their multiple relations and
attribute information. Contrastive learning-based methods have preliminarily
shown promising performance in this task. However, most contrastive
learning-based methods mostly rely on high-quality graph embedding and explore
less on the relationships between different graph views. To deal with these
deficiencies, we design a novel node-to-node Contrastive learning framework for
Multi-view network Embedding (CREME), which mainly contains two contrastive
objectives: Multi-view fusion InfoMax and Inter-view InfoMin. The former
objective distills information from embeddings generated from different graph
views, while the latter distinguishes different graph views better to capture
the complementary information between them. Specifically, we first apply a view
encoder to generate each graph view representation and utilize a multi-view
aggregator to fuse these representations. Then, we unify the two contrastive
objectives into one learning objective for training. Extensive experiments on
three real-world datasets show that CREME outperforms existing methods
consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards More Efficient Federated Learning with Better Optimization Objects. (arXiv:2108.08577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08577</id>
        <link href="http://arxiv.org/abs/2108.08577"/>
        <updated>2021-08-20T01:53:52.237Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a privacy-protected machine learning paradigm that
allows model to be trained directly at the edge without uploading data. One of
the biggest challenges faced by FL in practical applications is the
heterogeneity of edge node data, which will slow down the convergence speed and
degrade the performance of the model. For the above problems, a representative
solution is to add additional constraints in the local training, such as
FedProx, FedCurv and FedCL. However, the above algorithms still have room for
improvement. We propose to use the aggregation of all models obtained in the
past as new constraint target to further improve the performance of such
algorithms. Experiments in various settings demonstrate that our method
significantly improves the convergence speed and performance of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zirui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Ziyi Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08708</id>
        <link href="http://arxiv.org/abs/2108.08708"/>
        <updated>2021-08-20T01:53:52.222Z</updated>
        <summary type="html"><![CDATA[This paper describes a novel dataset consisting of sentences with semantic
similarity annotations. The data originate from the journalistic domain in the
Czech language. We describe the process of collecting and annotating the data
in detail. The dataset contains 138,556 human annotations divided into train
and test sets. In total, 485 journalism students participated in the creation
process. To increase the reliability of the test set, we compute the annotation
as an average of 9 individual annotations. We evaluate the quality of the
dataset by measuring inter and intra annotation annotators' agreements. Beside
agreement numbers, we provide detailed statistics of the collected dataset. We
conclude our paper with a baseline experiment of building a system for
predicting the semantic similarity of sentences. Due to the massive number of
training annotations (116 956), the model can perform significantly better than
an average annotator (0,92 versus 0,86 of Person's correlation coefficients).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1"&gt;Jakub Sido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1"&gt;Michal Sej&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Pra&amp;#x17e;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1"&gt;Miloslav Konop&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1"&gt;V&amp;#xe1;clav Moravec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse design optimization framework via a two-step deep learning approach: application to a wind turbine airfoil. (arXiv:2108.08500v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08500</id>
        <link href="http://arxiv.org/abs/2108.08500"/>
        <updated>2021-08-20T01:53:52.215Z</updated>
        <summary type="html"><![CDATA[Though inverse approach is computationally efficient in aerodynamic design as
the desired target performance distribution is specified, it has some
significant limitations that prevent full efficiency from being achieved.
First, the iterative procedure should be repeated whenever the specified target
distribution changes. Target distribution optimization can be performed to
clarify the ambiguity in specifying this distribution, but several additional
problems arise in this process such as loss of the representation capacity due
to parameterization of the distribution, excessive constraints for a realistic
distribution, inaccuracy of quantities of interest due to theoretical/empirical
predictions, and the impossibility of explicitly imposing geometric
constraints. To deal with these issues, a novel inverse design optimization
framework with a two-step deep learning approach is proposed. A variational
autoencoder and multi-layer perceptron are used to generate a realistic target
distribution and predict the quantities of interest and shape parameters from
the generated distribution, respectively. Then, target distribution
optimization is performed as the inverse design optimization. The proposed
framework applies active learning and transfer learning techniques to improve
accuracy and efficiency. Finally, the framework is validated through
aerodynamic shape optimizations of the airfoil of a wind turbine blade, where
inverse design is actively being applied. The results of the optimizations show
that this framework is sufficiently accurate, efficient, and flexible to be
applied to other inverse design engineering applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sunwoong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sanga Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1"&gt;Kwanjung Yee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepScale: An Online Frame Size Adaptation Approach to Accelerate Visual Multi-object Tracking. (arXiv:2107.10404v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10404</id>
        <link href="http://arxiv.org/abs/2107.10404"/>
        <updated>2021-08-20T01:53:52.208Z</updated>
        <summary type="html"><![CDATA[In surveillance and search and rescue applications, it is important to
perform multi-target tracking (MOT) in real-time on low-end devices. Today's
MOT solutions employ deep neural networks, which tend to have high computation
complexity. Recognizing the effects of frame sizes on tracking performance, we
propose DeepScale, a model agnostic frame size selection approach that operates
on top of existing fully convolutional network-based trackers to accelerate
tracking throughput. In the training stage, we incorporate detectability scores
into a one-shot tracker architecture so that DeepScale can learn representation
estimations for different frame sizes in a self-supervised manner. {During
inference, it can adapt frame sizes according to the complexity of visual
contents based on user-controlled parameters.} Extensive experiments and
benchmark tests on MOT datasets demonstrate the effectiveness and flexibility
of DeepScale. Compared to a state-of-the-art tracker, DeepScale++, a variant of
DeepScale achieves 1.57X accelerated with only moderate degradation (~ 2.3%) in
tracking accuracy on the MOT15 dataset in one configuration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nalaie_K/0/1/0/all/0/1"&gt;Keivan Nalaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Rong Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Illicit Drug Dealers on Instagram with Large-scale Multimodal Data Fusion. (arXiv:2108.08301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08301</id>
        <link href="http://arxiv.org/abs/2108.08301"/>
        <updated>2021-08-20T01:53:52.200Z</updated>
        <summary type="html"><![CDATA[Illicit drug trafficking via social media sites such as Instagram has become
a severe problem, thus drawing a great deal of attention from law enforcement
and public health agencies. How to identify illicit drug dealers from social
media data has remained a technical challenge due to the following reasons. On
the one hand, the available data are limited because of privacy concerns with
crawling social media sites; on the other hand, the diversity of drug dealing
patterns makes it difficult to reliably distinguish drug dealers from common
drug users. Unlike existing methods that focus on posting-based detection, we
propose to tackle the problem of illicit drug dealer identification by
constructing a large-scale multimodal dataset named Identifying Drug Dealers on
Instagram (IDDIG). Totally nearly 4,000 user accounts, of which over 1,400 are
drug dealers, have been collected from Instagram with multiple data sources
including post comments, post images, homepage bio, and homepage images. We
then design a quadruple-based multimodal fusion method to combine the multiple
data sources associated with each user account for drug dealer identification.
Experimental results on the constructed IDDIG dataset demonstrate the
effectiveness of the proposed method in identifying drug dealers (almost 95%
accuracy). Moreover, we have developed a hashtag-based community detection
technique for discovering evolving patterns, especially those related to
geography and drug types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuanbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Minglei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yanfang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11264</id>
        <link href="http://arxiv.org/abs/2107.11264"/>
        <updated>2021-08-20T01:53:52.193Z</updated>
        <summary type="html"><![CDATA[Identifying unexpected objects on roads in semantic segmentation (e.g.,
identifying dogs on roads) is crucial in safety-critical applications. Existing
approaches use images of unexpected objects from external datasets or require
additional training (e.g., retraining segmentation networks or training an
extra network), which necessitate a non-trivial amount of labor intensity or
lengthy inference time. One possible alternative is to use prediction scores of
a pre-trained network such as the max logits (i.e., maximum values among
classes before the final softmax layer) for detecting such objects. However,
the distribution of max logits of each predicted class is significantly
different from each other, which degrades the performance of identifying
unexpected objects in urban-scene segmentation. To address this issue, we
propose a simple yet effective approach that standardizes the max logits in
order to align the different distributions and reflect the relative meanings of
max logits within each predicted class. Moreover, we consider the local regions
from two different perspectives based on the intuition that neighboring pixels
share similar semantic information. In contrast to previous approaches, our
method does not utilize any external datasets or require additional training,
which makes our method widely applicable to existing pre-trained segmentation
models. Such a straightforward approach achieves a new state-of-the-art
performance on the publicly available Fishyscapes Lost & Found leaderboard with
a large margin. Our code is publicly available at this
$\href{https://github.com/shjung13/Standardized-max-logits}{link}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Sanghun Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungsoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1"&gt;Daehoon Gwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Sungha Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08411</id>
        <link href="http://arxiv.org/abs/2108.08411"/>
        <updated>2021-08-20T01:53:52.168Z</updated>
        <summary type="html"><![CDATA[Twitch chats pose a unique problem in natural language understanding due to a
large presence of neologisms, specifically emotes. There are a total of 8.06
million emotes, over 400k of which were used in the week studied. There is
virtually no information on the meaning or sentiment of emotes, and with a
constant influx of new emotes and drift in their frequencies, it becomes
impossible to maintain an updated manually-labeled dataset. Our paper makes a
two fold contribution. First we establish a new baseline for sentiment analysis
on Twitch data, outperforming the previous supervised benchmark by 7.9% points.
Secondly, we introduce a simple but powerful unsupervised framework based on
word embeddings and k-NN to enrich existing models with out-of-vocabulary
knowledge. This framework allows us to auto-generate a pseudo-dictionary of
emotes and we show that we can nearly match the supervised benchmark above even
when injecting such emote knowledge into sentiment classifiers trained on
extraneous datasets such as movie reviews or Twitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1"&gt;Pavel Dolin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+dHauthuille_L/0/1/0/all/0/1"&gt;Luc d&amp;#x27;Hauthuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vattani_A/0/1/0/all/0/1"&gt;Andrea Vattani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computing Steiner Trees using Graph Neural Networks. (arXiv:2108.08368v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08368</id>
        <link href="http://arxiv.org/abs/2108.08368"/>
        <updated>2021-08-20T01:53:52.160Z</updated>
        <summary type="html"><![CDATA[Graph neural networks have been successful in many learning problems and
real-world applications. A recent line of research explores the power of graph
neural networks to solve combinatorial and graph algorithmic problems such as
subgraph isomorphism, detecting cliques, and the traveling salesman problem.
However, many NP-complete problems are as of yet unexplored using this method.
In this paper, we tackle the Steiner Tree Problem. We employ four learning
frameworks to compute low cost Steiner trees: feed-forward neural networks,
graph neural networks, graph convolutional networks, and a graph attention
model. We use these frameworks in two fundamentally different ways: 1) to train
the models to learn the actual Steiner tree nodes, 2) to train the model to
learn good Steiner point candidates to be connected to the constructed tree
using a shortest path in a greedy fashion. We illustrate the robustness of our
heuristics on several random graph generation models as well as the SteinLib
data library. Our finding suggests that the out-of-the-box application of GNN
methods does worse than the classic 2-approximation method. However, when
combined with a greedy shortest path construction, it even does slightly better
than the 2-approximation algorithm. This result sheds light on the fundamental
capabilities and limitations of graph learning techniques on classical
NP-complete problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1"&gt;Reyan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turja_M/0/1/0/all/0/1"&gt;Md Asadullah Turja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahneh_F/0/1/0/all/0/1"&gt;Faryad Darabi Sahneh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1"&gt;Mithun Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1"&gt;Stephen Kobourov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Match: Iterative Shape Matching via Quantum Annealing. (arXiv:2105.02878v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02878</id>
        <link href="http://arxiv.org/abs/2105.02878"/>
        <updated>2021-08-20T01:53:52.153Z</updated>
        <summary type="html"><![CDATA[Finding shape correspondences can be formulated as an NP-hard quadratic
assignment problem (QAP) that becomes infeasible for shapes with high sampling
density. A promising research direction is to tackle such quadratic
optimization problems over binary variables with quantum annealing, which
allows for some problems a more efficient search in the solution space.
Unfortunately, enforcing the linear equality constraints in QAPs via a penalty
significantly limits the success probability of such methods on currently
available quantum hardware. To address this limitation, this paper proposes
Q-Match, i.e., a new iterative quantum method for QAPs inspired by the
alpha-expansion algorithm, which allows solving problems of an order of
magnitude larger than current quantum methods. It implicitly enforces the QAP
constraints by updating the current estimates in a cyclic fashion. Further,
Q-Match can be applied iteratively, on a subset of well-chosen correspondences,
allowing us to scale to real-world problems. Using the latest quantum annealer,
the D-Wave Advantage, we evaluate the proposed method on a subset of QAPLIB as
well as on isometric shape matching problems from the FAUST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1"&gt;Marcel Seelbach Benkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1"&gt;Zorah L&amp;#xe4;hner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wunderlich_C/0/1/0/all/0/1"&gt;Christof Wunderlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Variational Learning for Anomaly Detection in Multivariate Time Series. (arXiv:2108.08404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08404</id>
        <link href="http://arxiv.org/abs/2108.08404"/>
        <updated>2021-08-20T01:53:52.146Z</updated>
        <summary type="html"><![CDATA[Anomaly detection has been a challenging task given high-dimensional
multivariate time series data generated by networked sensors and actuators in
Cyber-Physical Systems (CPS). Besides the highly nonlinear, complex, and
dynamic natures of such time series, the lack of labeled data impedes data
exploitation in a supervised manner and thus prevents an accurate detection of
abnormal phenomenons. On the other hand, the collected data at the edge of the
network is often privacy sensitive and large in quantity, which may hinder the
centralized training at the main server. To tackle these issues, we propose an
unsupervised time series anomaly detection framework in a federated fashion to
continuously monitor the behaviors of interconnected devices within a network
and alerts for abnormal incidents so that countermeasures can be taken before
undesired consequences occur. To be specific, we leave the training data
distributed at the edge to learn a shared Variational Autoencoder (VAE) based
on Convolutional Gated Recurrent Unit (ConvGRU) model, which jointly captures
feature and temporal dependencies in the multivariate time series data for
representation learning and downstream anomaly detection tasks. Experiments on
three real-world networked sensor datasets illustrate the advantage of our
approach over other state-of-the-art models. We also conduct extensive
experiments to demonstrate the effectiveness of our detection framework under
non-federated and federated settings in terms of overall performance and
detection latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yushan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seversky_L/0/1/0/all/0/1"&gt;Lee Seversky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengtao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dahai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Houbing Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Kernel Consistency for Blind Video Super-Resolution. (arXiv:2108.08305v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08305</id>
        <link href="http://arxiv.org/abs/2108.08305"/>
        <updated>2021-08-20T01:53:52.138Z</updated>
        <summary type="html"><![CDATA[Deep learning-based blind super-resolution (SR) methods have recently
achieved unprecedented performance in upscaling frames with unknown
degradation. These models are able to accurately estimate the unknown
downscaling kernel from a given low-resolution (LR) image in order to leverage
the kernel during restoration. Although these approaches have largely been
successful, they are predominantly image-based and therefore do not exploit the
temporal properties of the kernels across multiple video frames. In this paper,
we investigated the temporal properties of the kernels and highlighted its
importance in the task of blind video super-resolution. Specifically, we
measured the kernel temporal consistency of real-world videos and illustrated
how the estimated kernels might change per frame in videos of varying
dynamicity of the scene and its objects. With this new insight, we revisited
previous popular video SR approaches, and showed that previous assumptions of
using a fixed kernel throughout the restoration process can lead to visual
artifacts when upscaling real-world videos. In order to counteract this, we
tailored existing single-image and video SR techniques to leverage kernel
consistency during both kernel estimation and video upscaling processes.
Extensive experiments on synthetic and real-world videos show substantial
restoration gains quantitatively and qualitatively, achieving the new
state-of-the-art in blind video SR and underlining the potential of exploiting
kernel temporal consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Lichuan Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1"&gt;Royson Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdelfattah_M/0/1/0/all/0/1"&gt;Mohamed S. Abdelfattah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_H/0/1/0/all/0/1"&gt;Hongkai Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12303</id>
        <link href="http://arxiv.org/abs/2106.12303"/>
        <updated>2021-08-20T01:53:52.118Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the development of deep image classification networks
has mostly been driven by the search for the best performance in terms of
classification accuracy on standardized benchmarks like ImageNet. More
recently, this focus has been expanded by the notion of model robustness, \ie
the generalization abilities of models towards previously unseen changes in the
data distribution. While new benchmarks, like ImageNet-C, have been introduced
to measure robustness properties, we argue that fixed testsets are only able to
capture a small portion of possible data variations and are thus limited and
prone to generate new overfitted solutions. To overcome these drawbacks, we
suggest to estimate the robustness of a model directly from the structure of
its learned feature-space. We introduce robustness indicators which are
obtained via unsupervised clustering of latent representations from a trained
classifier and show very high correlations to the model performance on
corrupted test data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1"&gt;Kalun Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1"&gt;Franz-Josef Pfreundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1"&gt;Margret Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08467</id>
        <link href="http://arxiv.org/abs/2108.08467"/>
        <updated>2021-08-20T01:53:52.112Z</updated>
        <summary type="html"><![CDATA[Computer-aided medical image analysis plays a significant role in assisting
medical practitioners for expert clinical diagnosis and deciding the optimal
treatment plan. At present, convolutional neural networks (CNN) are the
preferred choice for medical image analysis. In addition, with the rapid
advancements in three-dimensional (3D) imaging systems and the availability of
excellent hardware and software support to process large volumes of data, 3D
deep learning methods are gaining popularity in medical image analysis. Here,
we present an extensive review of the recently evolved 3D deep learning methods
in medical image segmentation. Furthermore, the research gaps and future
directions in 3D medical image segmentation are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1"&gt;S. Niyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1"&gt;S J Pawan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M Anand Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1"&gt;Jeny Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Center Federated Learning. (arXiv:2108.08647v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08647</id>
        <link href="http://arxiv.org/abs/2108.08647"/>
        <updated>2021-08-20T01:53:52.105Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) can protect data privacy in distributed learning
since it merely collects local gradients from users without access to their
data. However, FL is fragile in the presence of heterogeneity that is commonly
encountered in practical settings, e.g., non-IID data over different users.
Existing FL approaches usually update a single global model to capture the
shared knowledge of all users by aggregating their gradients, regardless of the
discrepancy between their data distributions. By comparison, a mixture of
multiple global models could capture the heterogeneity across various users if
assigning the users to different global models (i.e., centers) in FL. To this
end, we propose a novel multi-center aggregation mechanism . It learns multiple
global models from data, and simultaneously derives the optimal matching
between users and centers. We then formulate it as a bi-level optimization
problem that can be efficiently solved by a stochastic expectation maximization
(EM) algorithm. Experiments on multiple benchmark datasets of FL show that our
method outperforms several popular FL competitors. The source code are open
source on Github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1"&gt;Ming Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Guodong Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xianzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengqi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11320</id>
        <link href="http://arxiv.org/abs/2107.11320"/>
        <updated>2021-08-20T01:53:52.099Z</updated>
        <summary type="html"><![CDATA[Forest carbon offsets are increasingly popular and can play a significant
role in financing climate mitigation, forest conservation, and reforestation.
Measuring how much carbon is stored in forests is, however, still largely done
via expensive, time-consuming, and sometimes unaccountable field measurements.
To overcome these limitations, many verification bodies are leveraging machine
learning (ML) algorithms to estimate forest carbon from satellite or aerial
imagery. Aerial imagery allows for tree species or family classification, which
improves the satellite imagery-based forest type classification. However,
aerial imagery is significantly more expensive to collect and it is unclear by
how much the higher resolution improves the forest carbon estimation. This
proposal paper describes the first systematic comparison of forest carbon
estimation from aerial imagery, satellite imagery, and ground-truth field
measurements via deep learning-based algorithms for a tropical reforestation
project. Our initial results show that forest carbon estimates from satellite
imagery can overestimate above-ground biomass by up to 10-times for tropical
reforestation projects. The significant difference between aerial and
satellite-derived forest carbon measurements shows the potential for aerial
imagery-based ML algorithms and raises the importance to extend this study to a
global benchmark between options for carbon measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1"&gt;Gyri Reiersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1"&gt;David Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1"&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Order Optimal One-Shot Federated Learning for non-Convex Loss Functions. (arXiv:2108.08677v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08677</id>
        <link href="http://arxiv.org/abs/2108.08677"/>
        <updated>2021-08-20T01:53:52.092Z</updated>
        <summary type="html"><![CDATA[We consider the problem of federated learning in a one-shot setting in which
there are $m$ machines, each observing $n$ samples function from an unknown
distribution on non-convex loss functions. Let $F:[-1,1]^d\to\mathbb{R}$ be the
expected loss function with respect to this unknown distribution. The goal is
to find an estimate of the minimizer of $F$. Based on its observations, each
machine generates a signal of bounded length $B$ and sends it to a server. The
sever collects signals of all machines and outputs an estimate of the minimizer
of $F$. We propose a distributed learning algorithm, called Multi-Resolution
Estimator for Non-Convex loss function (MRE-NC), whose expected error is
bounded by $\max\big(1/\sqrt{n}(mB)^{1/d}, 1/\sqrt{mn}\big)$, up to
polylogarithmic factors. We also provide a matching lower bound on the
performance of any algorithm, showing that MRE-NC is order optimal in terms of
$n$ and $m$. Experiments on synthetic and real data show the effectiveness of
MRE-NC in distributed learning of model's parameters for non-convex loss
functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharifnassab_A/0/1/0/all/0/1"&gt;Arsalan Sharifnassab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1"&gt;Saber Salehkaleybar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golestani_S/0/1/0/all/0/1"&gt;S. Jamaloddin Golestani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Multilevel Circulant Matrix Approximate to Speed Up Kernel Logistic Regression. (arXiv:2108.08605v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08605</id>
        <link href="http://arxiv.org/abs/2108.08605"/>
        <updated>2021-08-20T01:53:52.085Z</updated>
        <summary type="html"><![CDATA[Kernel logistic regression (KLR) is a classical nonlinear classifier in
statistical machine learning. Newton method with quadratic convergence rate can
solve KLR problem more effectively than the gradient method. However, an
obvious limitation of Newton method for training large-scale problems is the
$O(n^{3})$ time complexity and $O(n^{2})$ space complexity, where $n$ is the
number of training instances. In this paper, we employ the multilevel circulant
matrix (MCM) approximate kernel matrix to save in storage space and accelerate
the solution of the KLR. Combined with the characteristics of MCM and our
ingenious design, we propose an MCM approximate Newton iterative method. We
first simplify the Newton direction according to the semi-positivity of the
kernel matrix and then perform a two-step approximation of the Newton direction
by using MCM. Our method reduces the time complexity of each iteration to $O(n
\log n)$ by using the multidimensional fast Fourier transform (mFFT). In
addition, the space complexity can be reduced to $O(n)$ due to the built-in
periodicity of MCM. Experimental results on some large-scale binary and
multi-classification problems show that our method makes KLR scalable for
large-scale problems, with less memory consumption, and converges to test
accuracy without sacrifice in a shorter time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junna~Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuisheng~Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_%7E/0/1/0/all/0/1"&gt;~Cui~Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08375</id>
        <link href="http://arxiv.org/abs/2108.08375"/>
        <updated>2021-08-20T01:53:52.068Z</updated>
        <summary type="html"><![CDATA[This paper studies the relative importance of attention heads in
Transformer-based models to aid their interpretability in cross-lingual and
multi-lingual tasks. Prior research has found that only a few attention heads
are important in each mono-lingual Natural Language Processing (NLP) task and
pruning the remaining heads leads to comparable or improved performance of the
model. However, the impact of pruning attention heads is not yet clear in
cross-lingual and multi-lingual tasks. Through extensive experiments, we show
that (1) pruning a number of attention heads in a multi-lingual
Transformer-based model has, in general, positive effects on its performance in
cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned
can be ranked using gradients and identified with a few trial experiments. Our
experiments focus on sequence labeling tasks, with potential applicability on
other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine
two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and
XLM-R, on three tasks across 9 languages each. We also discuss the validity of
our findings and their extensibility to truly resource-scarce languages and
other task settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weicheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1"&gt;Renze Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AIRCHITECT: Learning Custom Architecture Design and Mapping Space. (arXiv:2108.08295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08295</id>
        <link href="http://arxiv.org/abs/2108.08295"/>
        <updated>2021-08-20T01:53:52.060Z</updated>
        <summary type="html"><![CDATA[Design space exploration is an important but costly step involved in the
design/deployment of custom architectures to squeeze out maximum possible
performance and energy efficiency. Conventionally, optimizations require
iterative sampling of the design space using simulation or heuristic tools. In
this paper we investigate the possibility of learning the optimization task
using machine learning and hence using the learnt model to predict optimal
parameters for the design and mapping space of custom architectures, bypassing
any exploration step. We use three case studies involving the optimal array
design, SRAM buffer sizing, mapping, and schedule determination for
systolic-array-based custom architecture design and mapping space. Within the
purview of these case studies, we show that it is possible to capture the
design space and train a model to "generalize" prediction the optimal design
and mapping parameters when queried with workload and design constraints. We
perform systematic design-aware and statistical analysis of the optimization
space for our case studies and highlight the patterns in the design space. We
formulate the architecture design and mapping as a machine learning problem
that allows us to leverage existing ML models for training and inference. We
design and train a custom network architecture called AIRCHITECT, which is
capable of learning the architecture design space with as high as 94.3% test
accuracy and predicting optimal configurations which achieve on average
(GeoMean) of 99.9% the best possible performance on a test dataset with $10^5$
GEMM workloads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samajdar_A/0/1/0/all/0/1"&gt;Ananda Samajdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1"&gt;Jan Moritz Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denton_M/0/1/0/all/0/1"&gt;Matthew Denton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1"&gt;Tushar Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stroke-Based Scene Text Erasing Using Synthetic Data for Training. (arXiv:2104.11493v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11493</id>
        <link href="http://arxiv.org/abs/2104.11493"/>
        <updated>2021-08-20T01:53:52.053Z</updated>
        <summary type="html"><![CDATA[Scene text erasing, which replaces text regions with reasonable content in
natural images, has drawn significant attention in the computer vision
community in recent years. There are two potential subtasks in scene text
erasing: text detection and image inpainting. Either subtask requires
considerable data to achieve better performance; however, the lack of a
large-scale real-world scene-text removal dataset does not allow existing
methods to work according to their potential. To avoid the limitation of the
lack of pairwise real-world data, we enhance and make considerable use of the
synthetic text and subsequently train our model only on the dataset generated
by the improved synthetic text engine. Our proposed network contains a stroke
mask prediction module and background inpainting module that can extract the
text stroke as a relatively small hole from the cropped text image to maintain
more background content for better inpainting results. This model can partially
erase text instances in a scene image with a bounding box or work with an
existing scene-text detector for automatic scene text erasing. The experimental
results from the qualitative and quantitative evaluation of the SCUT-Syn,
ICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly
outperforms existing state-of-the-art methods even when they were trained on
real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhengmi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyazaki_T/0/1/0/all/0/1"&gt;Tomo Miyazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugaya_Y/0/1/0/all/0/1"&gt;Yoshihiro Sugaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omachi_S/0/1/0/all/0/1"&gt;Shinichiro Omachi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation. (arXiv:2108.01866v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01866</id>
        <link href="http://arxiv.org/abs/2108.01866"/>
        <updated>2021-08-20T01:53:52.047Z</updated>
        <summary type="html"><![CDATA[We present a novel pyramidal output representation to ensure parsimony with
our "specialize and fuse" process for semantic segmentation. A pyramidal
"output" representation consists of coarse-to-fine levels, where each level is
"specialize" in a different class distribution (e.g., more stuff than things
classes at coarser levels). Two types of pyramidal outputs (i.e., unity and
semantic pyramid) are "fused" into the final semantic output, where the unity
pyramid indicates unity-cells (i.e., all pixels in such cell share the same
semantic label). The process ensures parsimony by predicting a relatively small
number of labels for unity-cells (e.g., a large cell of grass) to build the
final semantic output. In addition to the "output" representation, we design a
coarse-to-fine contextual module to aggregate the "features" representation
from different levels. We validate the effectiveness of each key module in our
method through comprehensive ablation studies. Finally, our approach achieves
state-of-the-art performance on three widely-used semantic segmentation
datasets -- ADE20K, COCO-Stuff, and Pascal-Context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1"&gt;Chi-Wei Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hwann-Tzong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Min Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11857</id>
        <link href="http://arxiv.org/abs/2106.11857"/>
        <updated>2021-08-20T01:53:52.029Z</updated>
        <summary type="html"><![CDATA[We present HybVIO, a novel hybrid approach for combining filtering-based
visual-inertial odometry (VIO) with optimization-based SLAM. The core of our
method is highly robust, independent VIO with improved IMU bias modeling,
outlier rejection, stationarity detection, and feature track selection, which
is adjustable to run on embedded hardware. Long-term consistency is achieved
with a loosely-coupled SLAM module. In academic benchmarks, our solution yields
excellent performance in all categories, especially in the real-time use case,
where we outperform the current state-of-the-art. We also demonstrate the
feasibility of VIO for vehicular tracking on consumer-grade hardware using a
custom dataset, and show good performance in comparison to current commercial
VISLAM alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1"&gt;Otto Seiskari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1"&gt;Pekka Rantalankila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1"&gt;Jerry Ylilammi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08468</id>
        <link href="http://arxiv.org/abs/2108.08468"/>
        <updated>2021-08-20T01:53:52.022Z</updated>
        <summary type="html"><![CDATA[We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student's performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Chen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tony Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hanqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiwei Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08655</id>
        <link href="http://arxiv.org/abs/2108.08655"/>
        <updated>2021-08-20T01:53:52.000Z</updated>
        <summary type="html"><![CDATA[Actor-critic algorithms are widely used in reinforcement learning, but are
challenging to mathematically analyze due to the online arrival of non-i.i.d.
data samples. The distribution of the data samples dynamically changes as the
model is updated, introducing a complex feedback loop between the data
distribution and the reinforcement learning algorithm. We prove that, under a
time rescaling, the online actor-critic algorithm with tabular parametrization
converges to an ordinary differential equations (ODEs) as the number of updates
becomes large. The proof first establishes the geometric ergodicity of the data
samples under a fixed actor policy. Then, using a Poisson equation, we prove
that the fluctuations of the data samples around a dynamic probability measure,
which is a function of the evolving actor model, vanish as the number of
updates become large. Once the ODE limit has been derived, we study its
convergence properties using a two time-scale analysis which asymptotically
de-couples the critic ODE from the actor ODE. The convergence of the critic to
the solution of the Bellman equation and the actor to the optimal policy are
proven. In addition, a convergence rate to this global minimum is also
established. Our convergence analysis holds under specific choices for the
learning rates and exploration rates in the actor-critic algorithm, which could
provide guidance for the implementation of actor-critic algorithms in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sirignano_J/0/1/0/all/0/1"&gt;Justin Sirignano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03151</id>
        <link href="http://arxiv.org/abs/2108.03151"/>
        <updated>2021-08-20T01:53:51.990Z</updated>
        <summary type="html"><![CDATA[Appearance and motion are two important sources of information in video
object segmentation (VOS). Previous methods mainly focus on using simplex
solutions, lowering the upper bound of feature collaboration among and across
these two cues. In this paper, we study a novel framework, termed the FSNet
(Full-duplex Strategy Network), which designs a relational cross-attention
module (RCAM) to achieve the bidirectional message propagation across embedding
subspaces. Furthermore, the bidirectional purification module (BPM) is
introduced to update the inconsistent features between the spatial-temporal
embeddings, effectively improving the model robustness. By considering the
mutual restraint within the full-duplex strategy, our FSNet performs the
cross-modal feature-passing (i.e., transmission and receiving) simultaneously
before the fusion and decoding stage, making it robust to various challenging
scenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five
popular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and
DAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both
the VOS and video salient object detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Keren Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhe Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianbing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (arXiv:2108.08487v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08487</id>
        <link href="http://arxiv.org/abs/2108.08487"/>
        <updated>2021-08-20T01:53:51.969Z</updated>
        <summary type="html"><![CDATA[Recently, the generalization behavior of Convolutional Neural Networks (CNN)
is gradually transparent through explanation techniques with the frequency
components decomposition. However, the importance of the phase spectrum of the
image for a robust vision system is still ignored. In this paper, we notice
that the CNN tends to converge at the local optimum which is closely related to
the high-frequency components of the training images, while the amplitude
spectrum is easily disturbed such as noises or common corruptions. In contrast,
more empirical studies found that humans rely on more phase components to
achieve robust recognition. This observation leads to more explanations of the
CNN's generalization behaviors in both robustness to common perturbations and
out-of-distribution detection, and motivates a new perspective on data
augmentation designed by re-combing the phase spectrum of the current image and
the amplitude spectrum of the distracter image. That is, the generated samples
force the CNN to pay more attention to the structured information from phase
components and keep robust to the variation of the amplitude. Experiments on
several image datasets indicate that the proposed method achieves
state-of-the-art performances on multiple generalizations and calibration
tasks, including adaptability for common corruptions and surface variations,
out-of-distribution detection, and adversarial attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1"&gt;Peixi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Li Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lin Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02613</id>
        <link href="http://arxiv.org/abs/2108.02613"/>
        <updated>2021-08-20T01:53:51.962Z</updated>
        <summary type="html"><![CDATA[Point cloud registration is a fundamental problem in 3D computer vision. In
this paper, we cast point cloud registration into a planning problem in
reinforcement learning, which can seek the transformation between the source
and target point clouds through trial and error. By modeling the point cloud
registration process as a Markov decision process (MDP), we develop a latent
dynamic model of point clouds, consisting of a transformation network and
evaluation network. The transformation network aims to predict the new
transformed feature of the point cloud after performing a rigid transformation
(i.e., action) on it while the evaluation network aims to predict the alignment
precision between the transformed source point cloud and target point cloud as
the reward signal. Once the dynamic model of the point cloud is trained, we
employ the cross-entropy method (CEM) to iteratively update the planning policy
by maximizing the rewards in the point cloud registration process. Thus, the
optimal policy, i.e., the transformation between the source and target point
clouds, can be obtained via gradually narrowing the search space of the
transformation. Experimental results on ModelNet40 and 7Scene benchmark
datasets demonstrate that our method can yield good registration performance in
an unsupervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haobo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jianjun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Modeling for Distribution Grids Under Partial Observability. (arXiv:2108.08350v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.08350</id>
        <link href="http://arxiv.org/abs/2108.08350"/>
        <updated>2021-08-20T01:53:51.955Z</updated>
        <summary type="html"><![CDATA[Accurately modeling power distribution grids is crucial for designing
effective monitoring and decision making algorithms. This paper addresses the
partial observability issue of data-driven distribution modeling in order to
improve the accuracy of line parameter estimation. Inspired by the sparse
changes in residential loads, we advocate to regularize the group sparsity of
the unobservable injections in a bi-linear estimation problem. The alternating
minimization scheme of guaranteed convergence is proposed to take advantage of
convex subproblems with efficient solutions. Numerical results using real-world
load data on the single-phase equivalent of the IEEE 123-bus test case have
demonstrated the accuracy improvements of the proposed solution over existing
work for both parameter estimation and voltage modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shanny Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect: A Data-driven Approach for Network Intrusion Detection. (arXiv:2108.08394v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08394</id>
        <link href="http://arxiv.org/abs/2108.08394"/>
        <updated>2021-08-20T01:53:51.948Z</updated>
        <summary type="html"><![CDATA[With massive data being generated daily and the ever-increasing
interconnectivity of the world's Internet infrastructures, a machine learning
based intrusion detection system (IDS) has become a vital component to protect
our economic and national security. In this paper, we perform a comprehensive
study on NSL-KDD, a network traffic dataset, by visualizing patterns and
employing different learning-based models to detect cyber attacks. Unlike
previous shallow learning and deep learning models that use the single learning
model approach for intrusion detection, we adopt a hierarchy strategy, in which
the intrusion and normal behavior are classified firstly, and then the specific
types of attacks are classified. We demonstrate the advantage of the
unsupervised representation learning model in binary intrusion detection tasks.
Besides, we alleviate the data imbalance problem with SVM-SMOTE oversampling
technique in 4-class classification and further demonstrate the effectiveness
and the drawback of the oversampling mechanism with a deep neural network as a
base model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tauscher_Z/0/1/0/all/0/1"&gt;Zachary Tauscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yushan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Houbing Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07791</id>
        <link href="http://arxiv.org/abs/2107.07791"/>
        <updated>2021-08-20T01:53:51.828Z</updated>
        <summary type="html"><![CDATA[We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1"&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Shreyas Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1"&gt;Oliver Stromann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Analysis with Adaptive Adversaries. (arXiv:2102.08446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08446</id>
        <link href="http://arxiv.org/abs/2102.08446"/>
        <updated>2021-08-20T01:53:51.809Z</updated>
        <summary type="html"><![CDATA[We prove novel algorithmic guarantees for several online problems in the
smoothed analysis model. In this model, at each time an adversary chooses an
input distribution with density function bounded above by $\tfrac{1}{\sigma}$
times that of the uniform distribution; nature then samples an input from this
distribution. Crucially, our results hold for {\em adaptive} adversaries that
can choose an input distribution based on the decisions of the algorithm and
the realizations of the inputs in the previous time steps.

This paper presents a general technique for proving smoothed algorithmic
guarantees against adaptive adversaries, in effect reducing the setting of
adaptive adversaries to the simpler case of oblivious adversaries. We apply
this technique to prove strong smoothed guarantees for three problems:

-Online learning: We consider the online prediction problem, where instances
are generated from an adaptive sequence of $\sigma$-smooth distributions and
the hypothesis class has VC dimension $d$. We bound the regret by
$\tilde{O}\big(\sqrt{T d\ln(1/\sigma)} + d\sqrt{\ln(T/\sigma)}\big)$. This
answers open questions of [RST11,Hag18].

-Online discrepancy minimization: We consider the online Koml\'os problem,
where the input is generated from an adaptive sequence of $\sigma$-smooth and
isotropic distributions on the $\ell_2$ unit ball. We bound the $\ell_\infty$
norm of the discrepancy vector by $\tilde{O}\big(\ln^2\!\big(
\frac{nT}{\sigma}\big) \big)$.

-Dispersion in online optimization: We consider online optimization of
piecewise Lipschitz functions where functions with $\ell$ discontinuities are
chosen by a smoothed adaptive adversary and show that the resulting sequence is
$\big( {\sigma}/{\sqrt{T\ell}}, \tilde O\big(\sqrt{T\ell}
\big)\big)$-dispersed. This matches the parameters of [BDV18] for oblivious
adversaries, up to log factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haghtalab_N/0/1/0/all/0/1"&gt;Nika Haghtalab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roughgarden_T/0/1/0/all/0/1"&gt;Tim Roughgarden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1"&gt;Abhishek Shetty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06985</id>
        <link href="http://arxiv.org/abs/2012.06985"/>
        <updated>2021-08-20T01:53:51.794Z</updated>
        <summary type="html"><![CDATA[Collecting labeled data for the task of semantic segmentation is expensive
and time-consuming, as it requires dense pixel-level annotations. While recent
Convolutional Neural Network (CNN) based semantic segmentation approaches have
achieved impressive results by using large amounts of labeled training data,
their performance drops significantly as the amount of labeled data decreases.
This happens because deep CNNs trained with the de facto cross-entropy loss can
easily overfit to small amounts of labeled data. To address this issue, we
propose a simple and effective contrastive learning-based training strategy in
which we first pretrain the network using a pixel-wise, label-based contrastive
loss, and then fine-tune it using the cross-entropy loss. This approach
increases intra-class compactness and inter-class separability, thereby
resulting in a better pixel classifier. We demonstrate the effectiveness of the
proposed training strategy using the Cityscapes and PASCAL VOC 2012
segmentation datasets. Our results show that pretraining with the proposed
contrastive loss results in large performance gains (more than 20% absolute
improvement in some settings) when the amount of labeled data is limited. In
many settings, the proposed contrastive pretraining strategy, which does not
use any additional data, is able to match or outperform the widely-used
ImageNet pretraining strategy that uses more than a million additional labeled
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangyun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1"&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1"&gt;Philip Mansfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1"&gt;Bradley Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1"&gt;Lior Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10511</id>
        <link href="http://arxiv.org/abs/2101.10511"/>
        <updated>2021-08-20T01:53:51.786Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel task together with a new benchmark for detecting
generic, taxonomy-free event boundaries that segment a whole video into chunks.
Conventional work in temporal video segmentation and action detection focuses
on localizing pre-defined action categories and thus does not scale to generic
videos. Cognitive Science has known since last century that humans consistently
segment videos into meaningful temporal chunks. This segmentation happens
naturally, without pre-defined event categories and without being explicitly
asked to do so. Here, we repeat these cognitive experiments on mainstream CV
datasets; with our novel annotation guideline which addresses the complexities
of taxonomy-free event boundary annotation, we introduce the task of Generic
Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our
Kinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8
of EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event
change, and respect human perception diversity. We view GEBD as an important
stepping stone towards understanding the video as a whole, and believe it has
been previously neglected due to a lack of proper task definition and
annotations. Through experiment and human study we demonstrate the value of the
annotations. Further, we benchmark supervised and un-supervised GEBD approaches
on the TAPOS dataset and our Kinetics-GEBD. We release our annotations and
baseline codes at CVPR'21 LOVEU Challenge:
https://sites.google.com/view/loveucvpr21.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1"&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Stan Weixian Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1"&gt;Deepti Ghadiyaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1"&gt;Matt Feiszli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Reciprocal Points Learning for Open Set Recognition. (arXiv:2103.00953v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00953</id>
        <link href="http://arxiv.org/abs/2103.00953"/>
        <updated>2021-08-20T01:53:51.768Z</updated>
        <summary type="html"><![CDATA[Open set recognition (OSR), aiming to simultaneously classify the seen
classes and identify the unseen classes as 'unknown', is essential for reliable
machine learning.The key challenge of OSR is how to reduce the empirical
classification risk on the labeled known data and the open space risk on the
potential unknown data simultaneously. To handle the challenge, we formulate
the open space risk problem from the perspective of multi-class integration,
and model the unexploited extra-class space with a novel concept Reciprocal
Point. Follow this, a novel learning framework, termed Adversarial Reciprocal
Point Learning (ARPL), is proposed to minimize the overlap of known
distribution and unknown distributions without loss of known classification
accuracy. Specifically, each reciprocal point is learned by the extra-class
space with the corresponding known category, and the confrontation among
multiple known categories are employed to reduce the empirical classification
risk. Then, an adversarial margin constraint is proposed to reduce the open
space risk by limiting the latent open space constructed by reciprocal points.
To further estimate the unknown distribution from open space, an instantiated
adversarial enhancement method is designed to generate diverse and confusing
training samples, based on the adversarial mechanism between the reciprocal
points and known classes. This can effectively enhance the model
distinguishability to the unknown classes. Extensive experimental results on
various benchmark datasets indicate that the proposed method is significantly
superior to other existing approaches and achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1"&gt;Peixi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search. (arXiv:2103.12424v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12424</id>
        <link href="http://arxiv.org/abs/2103.12424"/>
        <updated>2021-08-20T01:53:51.761Z</updated>
        <summary type="html"><![CDATA[A myriad of recent breakthroughs in hand-crafted neural architectures for
visual recognition have highlighted the urgent need to explore hybrid
architectures consisting of diversified building blocks. Meanwhile, neural
architecture search methods are surging with an expectation to reduce human
efforts. However, whether NAS methods can efficiently and effectively handle
diversified search spaces with disparate candidates (e.g. CNNs and
transformers) is still an open question. In this work, we present Block-wisely
Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS
method that addresses the problem of inaccurate architecture rating caused by
large weight-sharing space and biased supervision in previous methods. More
specifically, we factorize the search space into blocks and utilize a novel
self-supervised training scheme, named ensemble bootstrapping, to train each
block separately before searching them as a whole towards the population
center. Additionally, we present HyTra search space, a fabric-like hybrid
CNN-transformer search space with searchable down-sampling positions. On this
challenging search space, our searched model, BossNet-T, achieves up to 82.5%
accuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute
time. Moreover, our method achieves superior architecture rating accuracy with
0.78 and 0.76 Spearman correlation on the canonical MBConv search space with
ImageNet and on NATS-Bench size search space with CIFAR-100, respectively,
surpassing state-of-the-art NAS methods. Code:
https://github.com/changlin31/BossNAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jiefeng Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threshold Phenomena in Learning Halfspaces with Massart Noise. (arXiv:2108.08767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08767</id>
        <link href="http://arxiv.org/abs/2108.08767"/>
        <updated>2021-08-20T01:53:51.753Z</updated>
        <summary type="html"><![CDATA[We study the problem of PAC learning halfspaces on $\mathbb{R}^d$ with
Massart noise under Gaussian marginals. In the Massart noise model, an
adversary is allowed to flip the label of each point $\mathbf{x}$ with
probability $\eta(\mathbf{x}) \leq \eta$, for some parameter $\eta \in
[0,1/2]$.

The goal of the learner is to output a hypothesis with missclassification
error $\mathrm{opt} + \epsilon$, where $\mathrm{opt}$ is the error of the
target halfspace. Prior work studied this problem assuming that the target
halfspace is homogeneous and that the parameter $\eta$ is strictly smaller than
$1/2$. We explore how the complexity of the problem changes when either of
these assumptions is removed, establishing the following threshold phenomena:

For $\eta = 1/2$, we prove a lower bound of $d^{\Omega (\log(1/\epsilon))}$
on the complexity of any Statistical Query (SQ) algorithm for the problem,
which holds even for homogeneous halfspaces. On the positive side, we give a
new learning algorithm for arbitrary halfspaces in this regime with sample
complexity and running time $O_\epsilon(1) \, d^{O(\log(1/\epsilon))}$.

For $\eta <1/2$, we establish a lower bound of $d^{\Omega(\log(1/\gamma))}$
on the SQ complexity of the problem, where $\gamma = \max\{\epsilon,
\min\{\mathbf{Pr}[f(\mathbf{x}) = 1], \mathbf{Pr}[f(\mathbf{x}) = -1]\} \}$ and
$f$ is the target halfspace. In particular, this implies an SQ lower bound of
$d^{\Omega (\log(1/\epsilon) )}$ for learning arbitrary Massart halfspaces
(even for small constant $\eta$). We complement this lower bound with a new
learning algorithm for this regime with sample complexity and runtime
$d^{O_{\eta}(\log(1/\gamma))} \mathrm{poly}(1/\epsilon)$.

Taken together, our results qualitatively characterize the complexity of
learning halfspaces in the Massart model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1"&gt;Vasilis Kontonis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1"&gt;Nikos Zarifis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trends in Neural Architecture Search: Towards the Acceleration of Search. (arXiv:2108.08474v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08474</id>
        <link href="http://arxiv.org/abs/2108.08474"/>
        <updated>2021-08-20T01:53:51.742Z</updated>
        <summary type="html"><![CDATA[In modern deep learning research, finding optimal (or near optimal) neural
network models is one of major research directions and it is widely studied in
many applications. In this paper, the main research trends of neural
architecture search (NAS) are classified as neuro-evolutionary algorithms,
reinforcement learning based algorithms, and one-shot architecture search
approaches. Furthermore, each research trend is introduced and finally all the
major three trends are compared. Lastly, the future research directions of NAS
research trends are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngkee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1"&gt;Won Joon Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Youn Kyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Soyi Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joongheon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08481</id>
        <link href="http://arxiv.org/abs/2108.08481"/>
        <updated>2021-08-20T01:53:51.735Z</updated>
        <summary type="html"><![CDATA[The classical development of neural networks has primarily focused on
learning mappings between finite dimensional Euclidean spaces or finite sets.
We propose a generalization of neural networks tailored to learn operators
mapping between infinite dimensional function spaces. We formulate the
approximation of operators by composition of a class of linear integral
operators and nonlinear activation functions, so that the composed operator can
approximate complex nonlinear operators. Furthermore, we introduce four classes
of operator parameterizations: graph-based operators, low-rank operators,
multipole graph-based operators, and Fourier operators and describe efficient
algorithms for computing with each one. The proposed neural operators are
resolution-invariant: they share the same network parameters between different
discretizations of the underlying function spaces and can be used for zero-shot
super-resolutions. Numerically, the proposed models show superior performance
compared to existing machine learning based methodologies on Burgers' equation,
Darcy flow, and the Navier-Stokes equation, while being several order of
magnitude faster compared to conventional PDE solvers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1"&gt;Nikola Kovachki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zongyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Burigede Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1"&gt;Kaushik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew Stuart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08485</id>
        <link href="http://arxiv.org/abs/2108.08485"/>
        <updated>2021-08-20T01:53:51.717Z</updated>
        <summary type="html"><![CDATA[Although automated metrics are commonly used to evaluate NLG systems, they
often correlate poorly with human judgements. Newer metrics such as BERTScore
have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which
rely on n-gram matching. These newer methods, however, are still limited in
that they do not consider the generation context, so they cannot properly
reward generated text that is correct but deviates from the given reference.

In this paper, we propose Language Model Augmented Relevance Score (MARS), a
new context-aware metric for NLG evaluation. MARS leverages off-the-shelf
language models, guided by reinforcement learning, to create augmented
references that consider both the generation context and available human
references, which are then used as additional references to score generated
text. Compared with seven existing metrics in three common NLG tasks, MARS not
only achieves higher correlation with human reference judgements, but also
differentiates well-formed candidates from adversarial samples to a larger
degree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair and Consistent Federated Learning. (arXiv:2108.08435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08435</id>
        <link href="http://arxiv.org/abs/2108.08435"/>
        <updated>2021-08-20T01:53:51.710Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has gain growing interests for its capability of
learning from distributed data sources collectively without the need of
accessing the raw data samples across different sources. So far FL research has
mostly focused on improving the performance, how the algorithmic disparity will
be impacted for the model learned from FL and the impact of algorithmic
disparity on the utility inconsistency are largely unexplored. In this paper,
we propose an FL framework to jointly consider performance consistency and
algorithmic fairness across different local clients (data sources). We derive
our framework from a constrained multi-objective optimization perspective, in
which we learn a model satisfying fairness constraints on all clients with
consistent performance. Specifically, we treat the algorithm prediction loss at
each local client as an objective and maximize the worst-performing client with
fairness constraints through optimizing a surrogate maximum function with all
objectives involved. A gradient-based procedure is employed to achieve the
Pareto optimality of this optimization problem. Theoretical analysis is
provided to prove that our method can converge to a Pareto solution that
achieves the min-max performance with fairness constraints on all clients.
Comprehensive experiments on synthetic and real-world datasets demonstrate the
superiority that our approach over baselines and its effectiveness in achieving
both fairness and consistency across all local clients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Sen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weishen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Video Representation Learning with Meta-Contrastive Network. (arXiv:2108.08426v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08426</id>
        <link href="http://arxiv.org/abs/2108.08426"/>
        <updated>2021-08-20T01:53:51.703Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning has been successfully applied to pre-train video
representations, which aims at efficient adaptation from pre-training domain to
downstream tasks. Existing approaches merely leverage contrastive loss to learn
instance-level discrimination. However, lack of category information will lead
to hard-positive problem that constrains the generalization ability of this
kind of methods. We find that the multi-task process of meta learning can
provide a solution to this problem. In this paper, we propose a
Meta-Contrastive Network (MCN), which combines the contrastive learning and
meta learning, to enhance the learning ability of existing self-supervised
approaches. Our method contains two training stages based on model-agnostic
meta learning (MAML), each of which consists of a contrastive branch and a meta
branch. Extensive evaluations demonstrate the effectiveness of our method. For
two downstream tasks, i.e., video action recognition and video retrieval, MCN
outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be
more specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8%
and 54.5% for video action recognition, as well as 52.5% and 23.7% for video
retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuanze Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blockchain Phishing Scam Detection via Multi-channel Graph Classification. (arXiv:2108.08456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08456</id>
        <link href="http://arxiv.org/abs/2108.08456"/>
        <updated>2021-08-20T01:53:51.697Z</updated>
        <summary type="html"><![CDATA[With the popularity of blockchain technology, the financial security issues
of blockchain transaction networks have become increasingly serious. Phishing
scam detection methods will protect possible victims and build a healthier
blockchain ecosystem. Usually, the existing works define phishing scam
detection as a node classification task by learning the potential features of
users through graph embedding methods such as random walk or graph neural
network (GNN). However, these detection methods are suffered from high
complexity due to the large scale of the blockchain transaction network,
ignoring temporal information of the transaction. Addressing this problem, we
defined the transaction pattern graphs for users and transformed the phishing
scam detection into a graph classification task. To extract richer information
from the input graph, we proposed a multi-channel graph classification model
(MCGC) with multiple feature extraction channels for GNN. The transaction
pattern graphs and MCGC are more able to detect potential phishing scammers by
extracting the transaction pattern features of the target users. Extensive
experiments on seven benchmark and Ethereum datasets demonstrate that the
proposed MCGC can not only achieve state-of-the-art performance in the graph
classification task but also achieve effective phishing scam detection based on
the target users' transaction pattern graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dunjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinyin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BARF: Bundle-Adjusting Neural Radiance Fields. (arXiv:2104.06405v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06405</id>
        <link href="http://arxiv.org/abs/2104.06405"/>
        <updated>2021-08-20T01:53:51.690Z</updated>
        <summary type="html"><![CDATA[Neural Radiance Fields (NeRF) have recently gained a surge of interest within
the computer vision community for its power to synthesize photorealistic novel
views of real-world scenes. One limitation of NeRF, however, is its requirement
of accurate camera poses to learn the scene representations. In this paper, we
propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from
imperfect (or even unknown) camera poses -- the joint problem of learning
neural 3D representations and registering camera frames. We establish a
theoretical connection to classical image alignment and show that
coarse-to-fine registration is also applicable to NeRF. Furthermore, we show
that na\"ively applying positional encoding in NeRF has a negative impact on
registration with a synthesis-based objective. Experiments on synthetic and
real-world data show that BARF can effectively optimize the neural scene
representations and resolve large camera pose misalignment at the same time.
This enables view synthesis and localization of video sequences from unknown
camera poses, opening up new avenues for visual localization systems (e.g.
SLAM) and potential applications for dense 3D mapping and reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen-Hsuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wei-Chiu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1"&gt;Simon Lucey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image Translation. (arXiv:2105.14110v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14110</id>
        <link href="http://arxiv.org/abs/2105.14110"/>
        <updated>2021-08-20T01:53:51.683Z</updated>
        <summary type="html"><![CDATA[While attention-based transformer networks achieve unparalleled success in
nearly all language tasks, the large number of tokens (pixels) found in images
coupled with the quadratic activation memory usage makes them prohibitive for
problems in computer vision. As such, while language-to-language translation
has been revolutionized by the transformer model, convolutional networks remain
the de facto solution for image-to-image translation. The recently proposed
MLP-Mixer architecture alleviates some of the computational issues associated
with attention-based networks while still retaining the long-range connections
that make transformer models desirable. Leveraging this memory-efficient
alternative to self-attention, we propose a new exploratory model in unpaired
image-to-image translation called MixerGAN: a simpler MLP-based architecture
that considers long-distance relationships between pixels without the need for
expensive attention mechanisms. Quantitative and qualitative analysis shows
that MixerGAN achieves competitive results when compared to prior
convolutional-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1"&gt;George Cazenavette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevara_M/0/1/0/all/0/1"&gt;Manuel Ladron De Guevara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07791</id>
        <link href="http://arxiv.org/abs/2107.07791"/>
        <updated>2021-08-20T01:53:51.664Z</updated>
        <summary type="html"><![CDATA[We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1"&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Shreyas Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1"&gt;Oliver Stromann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09701</id>
        <link href="http://arxiv.org/abs/2106.09701"/>
        <updated>2021-08-20T01:53:51.658Z</updated>
        <summary type="html"><![CDATA[Modern computer vision applications suffer from catastrophic forgetting when
incrementally learning new concepts over time. The most successful approaches
to alleviate this forgetting require extensive replay of previously seen data,
which is problematic when memory constraints or data legality concerns exist.
In this work, we consider the high-impact problem of Data-Free
Class-Incremental Learning (DFCIL), where an incremental learning agent must
learn new concepts over time without storing generators or training data from
past tasks. One approach for DFCIL is to replay synthetic images produced by
inverting a frozen copy of the learner's classification model, but we show this
approach fails for common class-incremental benchmarks when using standard
distillation strategies. We diagnose the cause of this failure and propose a
novel incremental distillation strategy for DFCIL, contributing a modified
cross-entropy training and importance-weighted feature distillation, and show
that our method results in up to a 25.1% increase in final task accuracy
(absolute difference) compared to SOTA DFCIL methods for common
class-incremental benchmarks. Our method even outperforms several standard
replay based methods which store a coreset of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;James Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1"&gt;Jonathan Balloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yilin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hongxia Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOCCA: Multi-Layer One-Class ClassificAtion for Anomaly Detection. (arXiv:2012.12111v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12111</id>
        <link href="http://arxiv.org/abs/2012.12111"/>
        <updated>2021-08-20T01:53:51.651Z</updated>
        <summary type="html"><![CDATA[Anomalies are ubiquitous in all scientific fields and can express an
unexpected event due to incomplete knowledge about the data distribution or an
unknown process that suddenly comes into play and distorts observations. Due to
such events' rarity, to train deep learning models on the Anomaly Detection
(AD) task, scientists only rely on "normal" data, i.e., non-anomalous samples.
Thus, letting the neural network infer the distribution beneath the input data.
In such a context, we propose a novel framework, named Multi-layer One-Class
ClassificAtion (MOCCA),to train and test deep learning models on the AD task.
Specifically, we applied it to autoencoders. A key novelty in our work stems
from the explicit optimization of intermediate representations for the AD task.
Indeed, differently from commonly used approaches that consider a neural
network as a single computational block, i.e., using the output of the last
layer only, MOCCA explicitly leverages the multi-layer structure of deep
architectures. Each layer's feature space is optimized for AD during training,
while in the test phase, the deep representations extracted from the trained
layers are combined to detect anomalies. With MOCCA, we split the training
process into two steps. First, the autoencoder is trained on the reconstruction
task only. Then, we only retain the encoder tasked with minimizing the L_2
distance between the output representation and a reference point, the
anomaly-free training data centroid, at each considered layer. Subsequently, we
combine the deep features extracted at the various trained layers of the
encoder model to detect anomalies at inference time. To assess the performance
of the models trained with MOCCA, we conduct extensive experiments on publicly
available datasets. We show that our proposed method reaches comparable or
superior performance to state-of-the-art approaches available in the
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Massoli_F/0/1/0/all/0/1"&gt;Fabio Valerio Massoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1"&gt;Fabrizio Falchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1"&gt;Alperen Kantarci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akti_S/0/1/0/all/0/1"&gt;&amp;#x15e;eymanur Akti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1"&gt;Hazim Kemal Ekenel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1"&gt;Giuseppe Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal and Flexible Optical Aberration Correction Using Deep-Prior Based Deconvolution. (arXiv:2104.03078v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03078</id>
        <link href="http://arxiv.org/abs/2104.03078"/>
        <updated>2021-08-20T01:53:51.644Z</updated>
        <summary type="html"><![CDATA[High quality imaging usually requires bulky and expensive lenses to
compensate geometric and chromatic aberrations. This poses high constraints on
the optical hash or low cost applications. Although one can utilize algorithmic
reconstruction to remove the artifacts of low-end lenses, the degeneration from
optical aberrations is spatially varying and the computation has to trade off
efficiency for performance. For example, we need to conduct patch-wise
optimization or train a large set of local deep neural networks to achieve high
reconstruction performance across the whole image. In this paper, we propose a
PSF aware plug-and-play deep network, which takes the aberrant image and PSF
map as input and produces the latent high quality version via incorporating
lens-specific deep priors, thus leading to a universal and flexible optical
aberration correction method. Specifically, we pre-train a base model from a
set of diverse lenses and then adapt it to a given lens by quickly refining the
parameters, which largely alleviates the time and memory consumption of model
learning. The approach is of high efficiency in both training and testing
stages. Extensive results verify the promising applications of our proposed
approach for compact low-end cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1"&gt;Jinli Suo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weihang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qionghai Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention in Attention Network for Image Super-Resolution. (arXiv:2104.09497v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09497</id>
        <link href="http://arxiv.org/abs/2104.09497"/>
        <updated>2021-08-20T01:53:51.637Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks have allowed remarkable advances in single
image super-resolution (SISR) over the last decade. Among recent advances in
SISR, attention mechanisms are crucial for high-performance SR models. However,
the attention mechanism remains unclear on why it works and how it works in
SISR. In this work, we attempt to quantify and visualize attention mechanisms
in SISR and show that not all attention modules are equally beneficial. We then
propose attention in attention network (A$^2$N) for more efficient and accurate
SISR. Specifically, A$^2$N consists of a non-attention branch and a coupling
attention branch. A dynamic attention module is proposed to generate weights
for these two branches to suppress unwanted attention adjustments dynamically,
where the weights change adaptively according to the input features. This
allows attention modules to specialize to beneficial examples without otherwise
penalties and thus greatly improve the capacity of the attention network with
few parameters overhead. Experimental results demonstrate that our final model
A$^2$N could achieve superior trade-off performances comparing with
state-of-the-art networks of similar sizes. Codes are available at
https://github.com/haoyuc/A2N.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region Similarity Representation Learning. (arXiv:2103.12902v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12902</id>
        <link href="http://arxiv.org/abs/2103.12902"/>
        <updated>2021-08-20T01:53:51.617Z</updated>
        <summary type="html"><![CDATA[We present Region Similarity Representation Learning (ReSim), a new approach
to self-supervised representation learning for localization-based tasks such as
object detection and segmentation. While existing work has largely focused on
solely learning global representations for an entire image, ReSim learns both
regional representations for localization as well as semantic image-level
representations. ReSim operates by sliding a fixed-sized window across the
overlapping area between two views (e.g., image crops), aligning these areas
with their corresponding convolutional feature map regions, and then maximizing
the feature similarity across views. As a result, ReSim learns spatially and
semantically consistent feature representation throughout the convolutional
feature maps of a neural network. A shift or scale of an image region, e.g., a
shift or scale of an object, has a corresponding change in the feature maps;
this allows downstream tasks to leverage these representations for
localization. Through object detection, instance segmentation, and dense pose
estimation experiments, we illustrate how ReSim learns representations which
significantly improve the localization and classification performance compared
to a competitive MoCo-v2 baseline: $+2.7$ AP$^{\text{bb}}_{75}$ VOC, $+1.1$
AP$^{\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\text{mk}}$ Cityscapes. Code and
pre-trained models are released at: \url{https://github.com/Tete-Xiao/ReSim}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tete Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1"&gt;Colorado J Reed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[eGAN: Unsupervised approach to class imbalance using transfer learning. (arXiv:2104.04162v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04162</id>
        <link href="http://arxiv.org/abs/2104.04162"/>
        <updated>2021-08-20T01:53:51.610Z</updated>
        <summary type="html"><![CDATA[Class imbalance is an inherent problem in many machine learning
classification tasks. This often leads to trained models that are unusable for
any practical purpose. In this study we explore an unsupervised approach to
address these imbalances by leveraging transfer learning from pre-trained image
classification models to encoder-based Generative Adversarial Network (eGAN).
To the best of our knowledge, this is the first work to tackle this problem
using GAN without needing to augment with synthesized fake images.

In the proposed approach we use the discriminator network to output a
negative or positive score. We classify as minority, test samples with negative
scores and as majority those with positive scores. Our approach eliminates
epistemic uncertainty in model predictions, as the P(minority) + P(majority)
need not sum up to 1. The impact of transfer learning and combinations of
different pre-trained image classification models at the generator and
discriminator is also explored. Best result of 0.69 F1-score was obtained on
CIFAR-10 classification task with imbalance ratio of 1:2500.

Our approach also provides a mechanism of thresholding the specificity or
sensitivity of our machine learning system. Keywords: Class imbalance, Transfer
Learning, GAN, nash equilibrium]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1"&gt;Ademola Okerinde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1"&gt;Lior Shamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;William Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theis_T/0/1/0/all/0/1"&gt;Tom Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nafi_N/0/1/0/all/0/1"&gt;Nasik Nafi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Ikshana Hypothesis of Human Scene Understanding Mechanism. (arXiv:2101.10837v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10837</id>
        <link href="http://arxiv.org/abs/2101.10837"/>
        <updated>2021-08-20T01:53:51.603Z</updated>
        <summary type="html"><![CDATA[In recent years, deep neural networks (DNNs) achieved state-of-the-art
performance on several computer vision tasks. However, the one typical drawback
of these DNNs is the requirement of massive labeled data. Even though few-shot
learning methods address this problem, they often use techniques such as
meta-learning and metric-learning on top of the existing methods. In this work,
we address this problem from a neuroscience perspective by proposing a
hypothesis named Ikshana, which is supported by several findings in
neuroscience. Our hypothesis approximates the refining process of conceptual
gist in the human brain while understanding a natural scene/image. While our
hypothesis holds no particular novelty in neuroscience, it provides a novel
perspective for designing DNNs for vision tasks. By following the Ikshana
hypothesis, we propose a novel neural-inspired CNN architecture named
IkshanaNet. The empirical results demonstrate the effectiveness of our method
by outperforming several baselines on the entire and subsets of the Cityscapes
and the CamVid semantic segmentation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daliparthi_V/0/1/0/all/0/1"&gt;Venkata Satya Sai Ajay Daliparthi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04351</id>
        <link href="http://arxiv.org/abs/2108.04351"/>
        <updated>2021-08-20T01:53:51.595Z</updated>
        <summary type="html"><![CDATA[This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02833</id>
        <link href="http://arxiv.org/abs/2108.02833"/>
        <updated>2021-08-20T01:53:51.589Z</updated>
        <summary type="html"><![CDATA[The growing number of action classes has posed a new challenge for video
understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.
The ZSAR task aims to recognize target (unseen) actions without training
examples by leveraging semantic representations to bridge seen and unseen
actions. However, due to the complexity and diversity of actions, it remains
challenging to semantically represent action classes and transfer knowledge
from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by
an effective human memory technique Elaborative Rehearsal (ER), which involves
elaborating a new concept and relating it to known concepts. Specifically, we
expand each action class as an Elaborative Description (ED) sentence, which is
more discriminative than a class name and less costly than manual-defined
attributes. Besides directly aligning class semantics with videos, we
incorporate objects from the video as Elaborative Concepts (EC) to improve
video semantics and generalization from seen actions to unseen actions. Our
ER-enhanced ZSAR model achieves state-of-the-art results on three existing
benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics
dataset to overcome limitations of current benchmarks and demonstrate the first
case where ZSAR performance is comparable to few-shot learning baselines on
this more realistic setting. We will release our codes and collected EDs at
https://github.com/DeLightCMU/ElaborativeRehearsal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Dong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions. (arXiv:2101.11529v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11529</id>
        <link href="http://arxiv.org/abs/2101.11529"/>
        <updated>2021-08-20T01:53:51.571Z</updated>
        <summary type="html"><![CDATA[The lack of fine-grained joints (facial joints, hand fingers) is a
fundamental performance bottleneck for state of the art skeleton action
recognition models. Despite this bottleneck, community's efforts seem to be
invested only in coming up with novel architectures. To specifically address
this bottleneck, we introduce two new pose based human action datasets -
NTU60-X and NTU120-X. Our datasets extend the largest existing action
recognition dataset, NTU-RGBD. In addition to the 25 body joints for each
skeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and
facial joints, enabling a richer skeleton representation. We appropriately
modify the state of the art approaches to enable training using the introduced
datasets. Our results demonstrate the effectiveness of these NTU-X datasets in
overcoming the aforementioned bottleneck and improve state of the art
performance, overall and on previously worst performing action categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_N/0/1/0/all/0/1"&gt;Neel Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1"&gt;Anirudh Thatipelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09493</id>
        <link href="http://arxiv.org/abs/2104.09493"/>
        <updated>2021-08-20T01:53:51.564Z</updated>
        <summary type="html"><![CDATA[Active learning algorithms select a subset of data for annotation to maximize
the model performance on a budget. One such algorithm is Expected Gradient
Length, which as the name suggests uses the approximate gradient induced per
example in the sampling process. While Expected Gradient Length has been
successfully used for classification and regression, the formulation for
regression remains intuitively driven. Hence, our theoretical contribution
involves deriving this formulation, thereby supporting the experimental
evidence. Subsequently, we show that expected gradient length in regression is
equivalent to Bayesian uncertainty. If certain assumptions are infeasible, our
algorithmic contribution (EGL++) approximates the effect of ensembles with a
single deterministic network. Instead of computing multiple possible inferences
per input, we leverage previously annotated samples to quantify the probability
of previous labels being the true label. Such an approach allows us to extend
expected gradient length to a new task: human pose estimation. We perform
experimental validation on two human pose datasets (MPII and LSP/LSPET),
highlighting the interpretability and competitiveness of EGL++ with different
active learning algorithms for human pose estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1"&gt;Megh Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier. (arXiv:2101.00562v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00562</id>
        <link href="http://arxiv.org/abs/2101.00562"/>
        <updated>2021-08-20T01:53:51.558Z</updated>
        <summary type="html"><![CDATA[Recent papers have suggested that transfer learning can outperform
sophisticated meta-learning methods for few-shot image classification. We take
this hypothesis to its logical conclusion, and suggest the use of an ensemble
of high-quality, pre-trained feature extractors for few-shot image
classification. We show experimentally that a library of pre-trained feature
extractors combined with a simple feed-forward network learned with an
L2-regularizer can be an excellent option for solving cross-domain few-shot
image classification. Our experimental results suggest that this simpler
sample-efficient approach far outperforms several well-established
meta-learning algorithms on a variety of few-shot tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Arkabandhu Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Mingchao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1"&gt;Chris Jermaine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple GAN Inversion for Exemplar-based Image-to-Image Translation. (arXiv:2103.14471v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14471</id>
        <link href="http://arxiv.org/abs/2103.14471"/>
        <updated>2021-08-20T01:53:51.551Z</updated>
        <summary type="html"><![CDATA[Existing state-of-the-art techniques in exemplar-based image-to-image
translation hold several critical concerns. Existing methods related to
exemplar-based image-to-image translation are impossible to translate on an
image tuple input (source, target) that is not aligned. Additionally, we can
confirm that the existing method exhibits limited generalization ability to
unseen images. In order to overcome this limitation, we propose Multiple GAN
Inversion for Exemplar-based Image-to-Image Translation. Our novel Multiple GAN
Inversion avoids human intervention by using a self-deciding algorithm to
choose the number of layers using Fr\'echet Inception Distance(FID), which
selects more plausible image reconstruction results among multiple hypotheses
without any training or supervision. Experimental results have in fact, shown
the advantage of the proposed method compared to existing state-of-the-art
exemplar-based image-to-image translation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1"&gt;Taewon Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14118</id>
        <link href="http://arxiv.org/abs/2106.14118"/>
        <updated>2021-08-20T01:53:51.544Z</updated>
        <summary type="html"><![CDATA[State of the art architectures for untrimmed video Temporal Action
Localization (TAL) have only considered RGB and Flow modalities, leaving the
information-rich audio modality totally unexploited. Audio fusion has been
explored for the related but arguably easier problem of trimmed (clip-level)
action recognition. However, TAL poses a unique set of challenges. In this
paper, we propose simple but effective fusion-based approaches for TAL. To the
best of our knowledge, our work is the first to jointly consider audio and
video modalities for supervised TAL. We experimentally show that our schemes
consistently improve performance for state of the art video-only TAL
approaches. Specifically, they help achieve new state of the art performance on
large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14
(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion
schemes, modality combinations and TAL architectures. Our code, models and
associated data are available at https://github.com/skelemoa/tal-hmo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1"&gt;Anurag Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1"&gt;Jazib Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1"&gt;Dolton Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransCenter: Transformers with Dense Queries for Multiple-Object Tracking. (arXiv:2103.15145v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15145</id>
        <link href="http://arxiv.org/abs/2103.15145"/>
        <updated>2021-08-20T01:53:51.538Z</updated>
        <summary type="html"><![CDATA[Transformer networks have proven extremely powerful for a wide variety of
tasks since they were introduced. Computer vision is not an exception, as the
use of transformers has become very popular in the vision community in recent
years. Despite this wave, multiple-object tracking (MOT) exhibits for now some
sort of incompatibility with transformers. We argue that the standard
representation - bounding boxes with insufficient sparse queries - is not
optimal to learning transformers for MOT. Inspired by recent research, we
propose TransCenter, the first transformer-based MOT architecture for dense
heatmap predictions. Methodologically, we propose the use of dense pixel-level
multi-scale queries in a transformer dual-decoder network, to be able to
globally and robustly infer the heatmap of targets' centers and associate them
through time. TransCenter outperforms the current state-of-the-art in standard
benchmarks both in MOT17 and MOT20. Our ablation study demonstrates the
advantage in the proposed architecture compared to more naive alternatives. The
code will be made publicly available at
https://github.com/yihongxu/transcenter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yihong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1"&gt;Yutong Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delorme_G/0/1/0/all/0/1"&gt;Guillaume Delorme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data. (arXiv:2103.17204v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17204</id>
        <link href="http://arxiv.org/abs/2103.17204"/>
        <updated>2021-08-20T01:53:51.518Z</updated>
        <summary type="html"><![CDATA[Research in unpaired video translation has mainly focused on short-term
temporal consistency by conditioning on neighboring frames. However for
transfer from simulated to photorealistic sequences, available information on
the underlying geometry offers potential for achieving global consistency
across views. We propose a novel approach which combines unpaired image
translation with neural rendering to transfer simulated to photorealistic
surgical abdominal scenes. By introducing global learnable textures and a
lighting-invariant view-consistency loss, our method produces consistent
translations of arbitrary views and thus enables long-term consistent video
synthesis. We design and test our model to generate video sequences from
minimally-invasive surgical abdominal scenes. Because labeled data is often
limited in this domain, photorealistic data where ground truth information from
the simulated domain is preserved is especially relevant. By extending existing
image-based methods to view-consistent videos, we aim to impact the
applicability of simulated training and evaluation environments for surgical
applications. Code and data: this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1"&gt;Dominik Rivoir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_M/0/1/0/all/0/1"&gt;Micha Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Docea_R/0/1/0/all/0/1"&gt;Reuben Docea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolbinger_F/0/1/0/all/0/1"&gt;Fiona Kolbinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riediger_C/0/1/0/all/0/1"&gt;Carina Riediger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weitz_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1"&gt;Stefanie Speidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the hidden treasure of dialog in video question answering. (arXiv:2103.14517v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14517</id>
        <link href="http://arxiv.org/abs/2103.14517"/>
        <updated>2021-08-20T01:53:51.511Z</updated>
        <summary type="html"><![CDATA[High-level understanding of stories in video such as movies and TV shows from
raw data is extremely challenging. Modern video question answering (VideoQA)
systems often use additional human-made sources like plot synopses, scripts,
video descriptions or knowledge bases. In this work, we present a new approach
to understand the whole story without such external sources. The secret lies in
the dialog: unlike any prior work, we treat dialog as a noisy source to be
converted into text description via dialog summarization, much like recent
methods treat video. The input of each modality is encoded by transformers
independently, and a simple fusion method combines all modalities, using soft
temporal attention for localization over long inputs. Our model outperforms the
state of the art on the KnowIT VQA dataset by a large margin, without using
question-specific human annotation or human-made plot summaries. It even
outperforms human evaluators who have never watched any whole episode before.
Code is available at https://engindeniz.github.io/dialogsummary-videoqa]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Engin_D/0/1/0/all/0/1"&gt;Deniz Engin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Schnitzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_N/0/1/0/all/0/1"&gt;Ngoc Q. K. Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1"&gt;Yannis Avrithis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrasting Contrastive Self-Supervised Representation Learning Pipelines. (arXiv:2103.14005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14005</id>
        <link href="http://arxiv.org/abs/2103.14005"/>
        <updated>2021-08-20T01:53:51.504Z</updated>
        <summary type="html"><![CDATA[In the past few years, we have witnessed remarkable breakthroughs in
self-supervised representation learning. Despite the success and adoption of
representations learned through this paradigm, much is yet to be understood
about how different training methods and datasets influence performance on
downstream tasks. In this paper, we analyze contrastive approaches as one of
the most successful and popular variants of self-supervised representation
learning. We perform this analysis from the perspective of the training
algorithms, pre-training datasets and end tasks. We examine over 700 training
experiments including 30 encoders, 4 pre-training datasets and 20 diverse
downstream tasks. Our experiments address various questions regarding the
performance of self-supervised models compared to their supervised
counterparts, current benchmarks used for evaluation, and the effect of the
pre-training data on end task performance. Our Visual Representation Benchmark
(ViRB) is available at: https://github.com/allenai/virb.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1"&gt;Klemen Kotar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1"&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1"&gt;Kiana Ehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Self-Similarity in Space and Time as Generalized Motion for Action Recognition. (arXiv:2102.07092v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07092</id>
        <link href="http://arxiv.org/abs/2102.07092"/>
        <updated>2021-08-20T01:53:51.497Z</updated>
        <summary type="html"><![CDATA[Spatio-temporal convolution often fails to learn motion dynamics in videos
and thus an effective motion representation is required for video understanding
in the wild. In this paper, we propose a rich and robust motion representation
based on spatio-temporal self-similarity (STSS). Given a sequence of frames,
STSS represents each local region as similarities to its neighbors in space and
time. By converting appearance features into relational values, it enables the
learner to better recognize structural patterns in space and time. We leverage
the whole volume of STSS and let our model learn to extract an effective motion
representation from it. The proposed neural block, dubbed SELFY, can be easily
inserted into neural architectures and trained end-to-end without additional
supervision. With a sufficient volume of the neighborhood in space and time, it
effectively captures long-term interaction and fast motion in the video,
leading to robust action recognition. Our experimental analysis demonstrates
its superiority over previous methods for motion modeling as well as its
complementarity to spatio-temporal features from direct convolution. On the
standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48,
and FineGym, the proposed method achieves the state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1"&gt;Heeseung Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Manjin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1"&gt;Suha Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;Minsu Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Proxy-based Loss for Deep Metric Learning. (arXiv:2103.13538v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13538</id>
        <link href="http://arxiv.org/abs/2103.13538"/>
        <updated>2021-08-20T01:53:51.490Z</updated>
        <summary type="html"><![CDATA[Proxy-based metric learning losses are superior to pair-based losses due to
their fast convergence and low training complexity. However, existing
proxy-based losses focus on learning class-discriminative features while
overlooking the commonalities shared across classes which are potentially
useful in describing and matching samples. Moreover, they ignore the implicit
hierarchy of categories in real-world datasets, where similar subordinate
classes can be grouped together. In this paper, we present a framework that
leverages this implicit hierarchy by imposing a hierarchical structure on the
proxies and can be used with any existing proxy-based loss. This allows our
model to capture both class-discriminative features and class-shared
characteristics without breaking the implicit data hierarchy. We evaluate our
method on five established image retrieval datasets such as In-Shop and SOP.
Results demonstrate that our hierarchical proxy-based loss framework improves
the performance of existing proxy-based losses, especially on large datasets
which exhibit strong hierarchical structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1"&gt;Muhammet Bastan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinliang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_D/0/1/0/all/0/1"&gt;Doug Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1"&gt;Dimitris Samaras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering. (arXiv:2104.00633v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00633</id>
        <link href="http://arxiv.org/abs/2104.00633"/>
        <updated>2021-08-20T01:53:51.471Z</updated>
        <summary type="html"><![CDATA[We present RePOSE, a fast iterative refinement method for 6D object pose
estimation. Prior methods perform refinement by feeding zoomed-in input and
rendered RGB images into a CNN and directly regressing an update of a refined
pose. Their runtime is slow due to the computational cost of CNN, which is
especially prominent in multiple-object pose refinement. To overcome this
problem, RePOSE leverages image rendering for fast feature extraction using a
3D model with a learnable texture. We call this deep texture rendering, which
uses a shallow multi-layer perceptron to directly regress a view-invariant
image representation of an object. Furthermore, we utilize differentiable
Levenberg-Marquardt (LM) optimization to refine a pose fast and accurately by
minimizing the feature-metric error between the input and rendered image
representations without the need of zooming in. These image representations are
trained such that differentiable LM optimization converges within few
iterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art
accuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute
improvement over the prior art, and comparable result on the YCB-Video dataset
with a much faster runtime. The code is available at
https://github.com/sh8/repose.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1"&gt;Shun Iwase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xingyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1"&gt;Rawal Khirodkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yokota_R/0/1/0/all/0/1"&gt;Rio Yokota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris M. Kitani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relaxed Transformer Decoders for Direct Action Proposal Generation. (arXiv:2102.01894v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01894</id>
        <link href="http://arxiv.org/abs/2102.01894"/>
        <updated>2021-08-20T01:53:51.464Z</updated>
        <summary type="html"><![CDATA[Temporal action proposal generation is an important and challenging task in
video understanding, which aims at detecting all temporal segments containing
action instances of interest. The existing proposal generation approaches are
generally based on pre-defined anchor windows or heuristic bottom-up boundary
matching strategies. This paper presents a simple and efficient framework
(RTD-Net) for direct action proposal generation, by re-purposing a
Transformer-alike architecture. To tackle the essential visual difference
between time and space, we make three important improvements over the original
transformer detection framework (DETR). First, to deal with slowness prior in
videos, we replace the original Transformer encoder with a boundary attentive
module to better capture long-range temporal information. Second, due to the
ambiguous temporal boundary and relatively sparse annotations, we present a
relaxed matching scheme to relieve the strict criteria of single assignment to
each groundtruth. Finally, we devise a three-branch head to further improve the
proposal confidence estimation by explicitly predicting its completeness.
Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate
the effectiveness of RTD-Net, on both tasks of temporal action proposal
generation and temporal action detection. Moreover, due to its simplicity in
design, our framework is more efficient than previous proposal generation
methods, without non-maximum suppression post-processing. The code and models
are made available at https://github.com/MCG-NJU/RTD-Action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiaqi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gangshan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Classification By Few-Iteration Meta-Learning. (arXiv:2010.00511v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00511</id>
        <link href="http://arxiv.org/abs/2010.00511"/>
        <updated>2021-08-20T01:53:51.453Z</updated>
        <summary type="html"><![CDATA[Learning in a low-data regime from only a few labeled examples is an
important, but challenging problem. Recent advancements within meta-learning
have demonstrated encouraging performance, in particular, for the task of
few-shot classification. We propose a novel optimization-based meta-learning
approach for few-shot classification. It consists of an embedding network,
providing a general representation of the image, and a base learner module. The
latter learns a linear classifier during the inference through an unrolled
optimization procedure. We design an inner learning objective composed of (i) a
robust classification loss on the support set and (ii) an entropy loss,
allowing transductive learning from unlabeled query samples. By employing an
efficient initialization module and a Steepest Descent based optimization
algorithm, our base learner predicts a powerful classifier within only a few
iterations. Further, our strategy enables important aspects of the base learner
objective to be learned during meta-training. To the best of our knowledge,
this work is the first to integrate both induction and transduction into the
base learner in an optimization-based meta-learning framework. We perform a
comprehensive experimental analysis, demonstrating the effectiveness of our
approach on four few-shot classification datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1"&gt;Ardhendu Shekhar Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01242</id>
        <link href="http://arxiv.org/abs/2010.01242"/>
        <updated>2021-08-20T01:53:51.446Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have developed to become powerful models
for various computer vision tasks ranging from object detection to semantic
segmentation. However, most of the state-of-the-art CNNs cannot be deployed
directly on edge devices such as smartphones and drones, which need low latency
under limited power and memory bandwidth. One popular, straightforward approach
to compressing CNNs is network slimming, which imposes $\ell_1$ regularization
on the channel-associated scaling factors via the batch normalization layers
during training. Network slimming thereby identifies insignificant channels
that can be pruned for inference. In this paper, we propose replacing the
$\ell_1$ penalty with an alternative nonconvex, sparsity-inducing penalty in
order to yield a more compressed and/or accurate CNN architecture. We
investigate $\ell_p (0 < p < 1)$, transformed $\ell_1$ (T$\ell_1$), minimax
concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) due to
their recent successes and popularity in solving sparse optimization problems,
such as compressed sensing and variable selection. We demonstrate the
effectiveness of network slimming with nonconvex penalties on three neural
network architectures -- VGG-19, DenseNet-40, and ResNet-164 -- on standard
image classification datasets. Based on the numerical experiments, T$\ell_1$
preserves model accuracy against channel pruning, $\ell_{1/2, 3/4}$ yield
better compressed models with similar accuracies after retraining as $\ell_1$,
and MCP and SCAD provide more accurate models after retraining with similar
compression as $\ell_1$. Network slimming with T$\ell_1$ regularization also
outperforms the latest Bayesian modification of network slimming in compressing
a CNN architecture in terms of memory storage while preserving its model
accuracy after channel pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1"&gt;Kevin Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1"&gt;Fredrick Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yingyong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11857</id>
        <link href="http://arxiv.org/abs/2011.11857"/>
        <updated>2021-08-20T01:53:51.438Z</updated>
        <summary type="html"><![CDATA[Adversarial attack algorithms are dominated by penalty methods, which are
slow in practice, or more efficient distance-customized methods, which are
heavily tailored to the properties of the distance considered. We propose a
white-box attack algorithm to generate minimally perturbed adversarial examples
based on Augmented Lagrangian principles. We bring several algorithmic
modifications, which have a crucial effect on performance. Our attack enjoys
the generality of penalty methods and the computational efficiency of
distance-customized algorithms, and can be readily used for a wide set of
distances. We compare our attack to state-of-the-art methods on three datasets
and several models, and consistently obtain competitive performances with
similar or lower computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Rony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1"&gt;Eric Granger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rescaling CNN through Learnable Repetition of Network Parameters. (arXiv:2101.05650v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05650</id>
        <link href="http://arxiv.org/abs/2101.05650"/>
        <updated>2021-08-20T01:53:51.420Z</updated>
        <summary type="html"><![CDATA[Deeper and wider CNNs are known to provide improved performance for deep
learning tasks. However, most such networks have poor performance gain per
parameter increase. In this paper, we investigate whether the gain observed in
deeper models is purely due to the addition of more optimization parameters or
whether the physical size of the network as well plays a role. Further, we
present a novel rescaling strategy for CNNs based on learnable repetition of
its parameters. Based on this strategy, we rescale CNNs without changing their
parameter count, and show that learnable sharing of weights itself can provide
significant boost in the performance of any given model without changing its
parameter count. We show that small base networks when rescaled, can provide
performance comparable to deeper networks with as low as 6% of optimization
parameters of the deeper one.

The relevance of weight sharing is further highlighted through the example of
group-equivariant CNNs. We show that the significant improvements obtained with
group-equivariant CNNs over the regular CNNs on classification problems are
only partly due to the added equivariance property, and part of it comes from
the learnable repetition of network weights. For rot-MNIST dataset, we show
that up to 40% of the relative gain reported by state-of-the-art methods for
rotation equivariance could actually be due to just the learnt repetition of
weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1"&gt;Arnav Chavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamba_U/0/1/0/all/0/1"&gt;Udbhav Bamba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_R/0/1/0/all/0/1"&gt;Rishabh Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepak Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Real-to-Sim Scene Generation. (arXiv:2011.14488v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14488</id>
        <link href="http://arxiv.org/abs/2011.14488"/>
        <updated>2021-08-20T01:53:51.399Z</updated>
        <summary type="html"><![CDATA[Synthetic data is emerging as a promising solution to the scalability issue
of supervised deep learning, especially when real data are difficult to acquire
or hard to annotate. Synthetic data generation, however, can itself be
prohibitively expensive when domain experts have to manually and painstakingly
oversee the process. Moreover, neural networks trained on synthetic data often
do not perform well on real data because of the domain gap. To solve these
challenges, we propose Sim2SG, a self-supervised automatic scene generation
technique for matching the distribution of real data. Importantly, Sim2SG does
not require supervision from the real-world dataset, thus making it applicable
in situations for which such annotations are difficult to obtain. Sim2SG is
designed to bridge both the content and appearance gaps, by matching the
content of real data, and by matching the features in the source and target
domains. We select scene graph (SG) generation as the downstream task, due to
the limited availability of labeled datasets. Experiments demonstrate
significant improvements over leading baselines in reducing the domain gap both
qualitatively and quantitatively, on several synthetic datasets as well as the
real-world KITTI dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Aayush Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debnath_S/0/1/0/all/0/1"&gt;Shoubhik Debnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lafleche_J/0/1/0/all/0/1"&gt;Jean-Francois Lafleche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cameracci_E/0/1/0/all/0/1"&gt;Eric Cameracci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+State_G/0/1/0/all/0/1"&gt;Gavriel State&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1"&gt;Stan Birchfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10623</id>
        <link href="http://arxiv.org/abs/2009.10623"/>
        <updated>2021-08-20T01:53:51.392Z</updated>
        <summary type="html"><![CDATA[From CNNs to attention mechanisms, encoding inductive biases into neural
networks has been a fruitful source of improvement in machine learning. Adding
auxiliary losses to the main objective function is a general way of encoding
biases that can help networks learn better representations. However, since
auxiliary losses are minimized only on training data, they suffer from the same
generalization gap as regular task losses. Moreover, by adding a term to the
loss function, the model optimizes a different objective than the one we care
about. In this work we address both problems: first, we take inspiration from
\textit{transductive learning} and note that after receiving an input but
before making a prediction, we can fine-tune our networks on any unsupervised
loss. We call this process {\em tailoring}, because we customize the model to
each input to ensure our prediction satisfies the inductive bias. Second, we
formulate {\em meta-tailoring}, a nested optimization similar to that in
meta-learning, and train our models to perform well on the task objective after
adapting them using an unsupervised loss. The advantages of tailoring and
meta-tailoring are discussed theoretically and demonstrated empirically on a
diverse set of examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1"&gt;Ferran Alet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1"&gt;Maria Bauza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1"&gt;Nurullah Giray Kuru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1"&gt;Tomas Lozano-Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1"&gt;Leslie Pack Kaelbling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11078</id>
        <link href="http://arxiv.org/abs/2103.11078"/>
        <updated>2021-08-20T01:53:51.385Z</updated>
        <summary type="html"><![CDATA[Generating high-fidelity talking head video by fitting with the input audio
sequence is a challenging problem that receives considerable attentions
recently. In this paper, we address this problem with the aid of neural scene
representation networks. Our method is completely different from existing
methods that rely on intermediate representations like 2D landmarks or 3D face
models to bridge the gap between audio input and video output. Specifically,
the feature of input audio signal is directly fed into a conditional implicit
function to generate a dynamic neural radiance field, from which a
high-fidelity talking-head video corresponding to the audio signal is
synthesized using volume rendering. Another advantage of our framework is that
not only the head (with hair) region is synthesized as previous methods did,
but also the upper body is generated via two individual neural radiance fields.
Experimental results demonstrate that our novel framework can (1) produce
high-fidelity and natural results, and (2) support free adjustment of audio
signals, viewing directions, and background images. Code is available at
https://github.com/YudongGuo/AD-NeRF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yudong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Sen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong-Jin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal multifilters image processing of astronomical images from scanned photographic plates. (arXiv:1007.3881v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1007.3881</id>
        <link href="http://arxiv.org/abs/1007.3881"/>
        <updated>2021-08-20T01:53:51.378Z</updated>
        <summary type="html"><![CDATA[In this paper orthogonal multifilters for astronomical image processing are
presented. We obtained new orthogonal multifilters based on the orthogonal
wavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as
a more powerful multiscale analysis tool. It adds several degrees of freedom in
multifilter design and makes it possible to have several useful properties such
as symmetry, orthogonality, short support, and a higher number of vanishing
moments simultaneously. Multifilter decomposition of scanned photographic
plates with astronomical images is made.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1"&gt;Vasil Kolev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Scene Graphs for Human-Object Interaction Detection. (arXiv:2108.08584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08584</id>
        <link href="http://arxiv.org/abs/2108.08584"/>
        <updated>2021-08-20T01:53:51.358Z</updated>
        <summary type="html"><![CDATA[Human-Object Interaction (HOI) detection is a fundamental visual task aiming
at localizing and recognizing interactions between humans and objects. Existing
works focus on the visual and linguistic features of humans and objects.
However, they do not capitalise on the high-level and semantic relationships
present in the image, which provides crucial contextual and detailed relational
knowledge for HOI inference. We propose a novel method to exploit this
information, through the scene graph, for the Human-Object Interaction (SG2HOI)
detection task. Our method, SG2HOI, incorporates the SG information in two
ways: (1) we embed a scene graph into a global context clue, serving as the
scene-specific environmental context; and (2) we build a relation-aware
message-passing module to gather relationships from objects' neighborhood and
transfer them into interactions. Empirical evaluation shows that our SG2HOI
method outperforms the state-of-the-art methods on two benchmark HOI datasets:
V-COCO and HICO-DET. Code will be available at https://github.com/ht014/SG2HOI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design of a Simple Orthogonal Multiwavelet Filter by Matrix Spectral Factorization. (arXiv:1910.07133v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.07133</id>
        <link href="http://arxiv.org/abs/1910.07133"/>
        <updated>2021-08-20T01:53:51.351Z</updated>
        <summary type="html"><![CDATA[We consider the design of an orthogonal symmetric/antisymmetric multiwavelet
from its matrix product filter by matrix spectral factorization (MSF). As a
test problem, we construct a simple matrix product filter with desirable
properties, and factor it using Bauer's method, which in this case can be done
in closed form. The corresponding orthogonal multiwavelet function is derived
using algebraic techniques which allow symmetry to be considered. This leads to
the known orthogonal multiwavelet SA1, which can also be derived directly. We
also give a lifting scheme for SA1, investigate the influence of the number of
significant digits in the calculations, and show some numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1"&gt;Vasil Kolev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooklev_T/0/1/0/all/0/1"&gt;Todor Cooklev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keinert_F/0/1/0/all/0/1"&gt;Fritz Keinert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Location-aware Single Image Reflection Removal. (arXiv:2012.07131v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07131</id>
        <link href="http://arxiv.org/abs/2012.07131"/>
        <updated>2021-08-20T01:53:51.339Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel location-aware deep-learning-based single image
reflection removal method. Our network has a reflection detection module to
regress a probabilistic reflection confidence map, taking multi-scale Laplacian
features as inputs. This probabilistic map tells if a region is
reflection-dominated or transmission-dominated, and it is used as a cue for the
network to control the feature flow when predicting the reflection and
transmission layers. We design our network as a recurrent network to
progressively refine reflection removal results at each iteration. The novelty
is that we leverage Laplacian kernel parameters to emphasize the boundaries of
strong reflections. It is beneficial to strong reflection detection and
substantially improves the quality of reflection removal results. Extensive
experiments verify the superior performance of the proposed method over
state-of-the-art approaches. Our code and the pre-trained model can be found at
https://github.com/zdlarr/Location-aware-SIRR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zheng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weiwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1"&gt;Rynson W.H. Lau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Detection and Segmentation via Multi-view Consensus. (arXiv:2012.05119v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05119</id>
        <link href="http://arxiv.org/abs/2012.05119"/>
        <updated>2021-08-20T01:53:51.333Z</updated>
        <summary type="html"><![CDATA[Self-supervised detection and segmentation of foreground objects aims for
accuracy without annotated training data. However, existing approaches
predominantly rely on restrictive assumptions on appearance and motion. For
scenes with dynamic activities and camera motion, we propose a multi-camera
framework in which geometric constraints are embedded in the form of multi-view
consistency during training via coarse 3D localization in a voxel grid and
fine-grained offset regression. In this manner, we learn a joint distribution
of proposals over multiple views. At inference time, our method operates on
single RGB images. We outperform state-of-the-art techniques both on images
that visually depart from those of standard benchmarks and on those of the
classical Human3.6M dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katircioglu_I/0/1/0/all/0/1"&gt;Isinsu Katircioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1"&gt;Helge Rhodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sporri_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Sp&amp;#xf6;rri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-based Localization of Moments in a Video Corpus. (arXiv:2008.08716v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08716</id>
        <link href="http://arxiv.org/abs/2008.08716"/>
        <updated>2021-08-20T01:53:51.326Z</updated>
        <summary type="html"><![CDATA[Prior works on text-based video moment localization focus on temporally
grounding the textual query in an untrimmed video. These works assume that the
relevant video is already known and attempt to localize the moment on that
relevant video only. Different from such works, we relax this assumption and
address the task of localizing moments in a corpus of videos for a given
sentence query. This task poses a unique challenge as the system is required to
perform: (i) retrieval of the relevant video where only a segment of the video
corresponds with the queried sentence, and (ii) temporal localization of moment
in the relevant video based on sentence query. Towards overcoming this
challenge, we propose Hierarchical Moment Alignment Network (HMAN) which learns
an effective joint embedding space for moments and sentences. In addition to
learning subtle differences between intra-video moments, HMAN focuses on
distinguishing inter-video global semantic concepts based on sentence queries.
Qualitative and quantitative results on three benchmark text-based video moment
retrieval datasets - Charades-STA, DiDeMo, and ActivityNet Captions -
demonstrate that our method achieves promising performance on the proposed task
of temporal localization of moments in a corpus of videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sudipta Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1"&gt;Niluthpol Chowdhury Mithun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gravity-Aware Monocular 3D Human-Object Reconstruction. (arXiv:2108.08844v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08844</id>
        <link href="http://arxiv.org/abs/2108.08844"/>
        <updated>2021-08-20T01:53:51.309Z</updated>
        <summary type="html"><![CDATA[This paper proposes GraviCap, i.e., a new approach for joint markerless 3D
human motion capture and object trajectory estimation from monocular RGB
videos. We focus on scenes with objects partially observed during a free
flight. In contrast to existing monocular methods, we can recover scale, object
trajectories as well as human bone lengths in meters and the ground plane's
orientation, thanks to the awareness of the gravity constraining object
motions. Our objective function is parametrised by the object's initial
velocity and position, gravity direction and focal length, and jointly
optimised for one or several free flight episodes. The proposed human-object
interaction constraints ensure geometric consistency of the 3D reconstructions
and improved physical plausibility of human poses compared to the unconstrained
case. We evaluate GraviCap on a new dataset with ground-truth annotations for
persons and different objects undergoing free flights. In the experiments, our
approach achieves state-of-the-art accuracy in 3D human motion capture on
various metrics. We urge the reader to watch our supplementary video. Both the
source code and the dataset are released; see
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1"&gt;Rishabh Dabral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1"&gt;Soshi Shimada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Arjun Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. (arXiv:2108.08839v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08839</id>
        <link href="http://arxiv.org/abs/2108.08839"/>
        <updated>2021-08-20T01:53:51.301Z</updated>
        <summary type="html"><![CDATA[Point clouds captured in real-world applications are often incomplete due to
the limited sensor resolution, single viewpoint, and occlusion. Therefore,
recovering the complete point clouds from partial ones becomes an indispensable
task in many practical applications. In this paper, we present a new method
that reformulates point cloud completion as a set-to-set translation problem
and design a new model, called PoinTr that adopts a transformer encoder-decoder
architecture for point cloud completion. By representing the point cloud as a
set of unordered groups of points with position embeddings, we convert the
point cloud to a sequence of point proxies and employ the transformers for
point cloud generation. To facilitate transformers to better leverage the
inductive bias about 3D geometric structures of point clouds, we further devise
a geometry-aware block that models the local geometric relationships
explicitly. The migration of transformers enables our model to better learn
structural knowledge and preserve detailed information for point cloud
completion. Furthermore, we propose two more challenging benchmarks with more
diverse incomplete point clouds that can better reflect the real-world
scenarios to promote future research. Experimental results show that our method
outperforms state-of-the-art methods by a large margin on both the new
benchmarks and the existing ones. Code is available at
https://github.com/yuxumin/PoinTr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xumin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zuyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel-based framework to estimate deformations of pneumothorax lung using relative position of anatomical landmarks. (arXiv:2102.12505v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12505</id>
        <link href="http://arxiv.org/abs/2102.12505"/>
        <updated>2021-08-20T01:53:51.294Z</updated>
        <summary type="html"><![CDATA[In video-assisted thoracoscopic surgeries, successful procedures of nodule
resection are highly dependent on the precise estimation of lung deformation
between the inflated lung in the computed tomography (CT) images during
preoperative planning and the deflated lung in the treatment views during
surgery. Lungs in the pneumothorax state during surgery have a large volume
change from normal lungs, making it difficult to build a mechanical model. The
purpose of this study is to develop a deformation estimation method of the 3D
surface of a deflated lung from a few partial observations. To estimate
deformations for a largely deformed lung, a kernel regression-based solution
was introduced. The proposed method used a few landmarks to capture the partial
deformation between the 3D surface mesh obtained from preoperative CT and the
intraoperative anatomical positions. The deformation for each vertex of the
entire mesh model was estimated per-vertex as a relative position from the
landmarks. The landmarks were placed in the anatomical position of the lung's
outer contour. The method was applied on nine datasets of the left lungs of
live Beagle dogs. Contrast-enhanced CT images of the lungs were acquired. The
proposed method achieved a local positional error of vertices of 2.74 mm,
Hausdorff distance of 6.11 mm, and Dice similarity coefficient of 0.94.
Moreover, the proposed method could estimate lung deformations from a small
number of training cases and a small observation area. This study contributes
to the data-driven modeling of pneumothorax deformation of the lung.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_U/0/1/0/all/0/1"&gt;Utako Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakao_M/0/1/0/all/0/1"&gt;Megumi Nakao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohzeki_M/0/1/0/all/0/1"&gt;Masayuki Ohzeki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tokuno_J/0/1/0/all/0/1"&gt;Junko Tokuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Yoshikawa_T/0/1/0/all/0/1"&gt;Toyofumi Fengshi Chen-Yoshikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Tetsuya Matsuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Object Tracking with Hallucinated and Unlabeled Videos. (arXiv:2108.08836v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08836</id>
        <link href="http://arxiv.org/abs/2108.08836"/>
        <updated>2021-08-20T01:53:51.286Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore learning end-to-end deep neural trackers without
tracking annotations. This is important as large-scale training data is
essential for training deep neural trackers while tracking annotations are
expensive to acquire. In place of tracking annotations, we first hallucinate
videos from images with bounding box annotations using zoom-in/out motion
transformations to obtain free tracking labels. We add video simulation
augmentations to create a diverse tracking dataset, albeit with simple motion.
Next, to tackle harder tracking cases, we mine hard examples across an
unlabeled pool of real videos with a tracker trained on our hallucinated video
data. For hard example mining, we propose an optimization-based connecting
process to first identify and then rectify hard examples from the pool of
unlabeled videos. Finally, we train our tracker jointly on hallucinated data
and mined hard video examples. Our weakly supervised tracker achieves
state-of-the-art performance on the MOT17 and TAO-person datasets. On MOT17, we
further demonstrate that the combination of our self-generated data and the
existing manually-annotated data leads to additional improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McKee_D/0/1/0/all/0/1"&gt;Daniel McKee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1"&gt;Bing Shuai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berneshawi_A/0/1/0/all/0/1"&gt;Andrew Berneshawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Manchen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1"&gt;Davide Modolo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1"&gt;Svetlana Lazebnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12815</id>
        <link href="http://arxiv.org/abs/2011.12815"/>
        <updated>2021-08-20T01:53:51.274Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been tremendously successful in
solving imaging inverse problems. To understand their success, an effective
strategy is to construct simpler and mathematically more tractable
convolutional sparse coding (CSC) models that share essential ingredients with
CNNs. Existing CSC methods, however, underperform leading CNNs in challenging
inverse problems. We hypothesize that the performance gap may be attributed in
part to how they process images at different spatial scales: While many CNNs
use multiscale feature representations, existing CSC models mostly rely on
single-scale dictionaries. To close the performance gap, we thus propose a
multiscale convolutional dictionary structure. The proposed dictionary
structure is derived from the U-Net, arguably the most versatile and widely
used CNN for image-to-image learning problems. We show that incorporating the
proposed multiscale dictionary in an otherwise standard CSC framework yields
performance competitive with state-of-the-art CNNs across a range of
challenging inverse problems including CT and MRI reconstruction. Our work thus
demonstrates the effectiveness and scalability of the multiscale CSC approach
in solving challenging inverse problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1"&gt;Anadi Chaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1"&gt;David Belius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1"&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Relation Detection via Tracklet based Visual Transformer. (arXiv:2108.08669v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08669</id>
        <link href="http://arxiv.org/abs/2108.08669"/>
        <updated>2021-08-20T01:53:51.267Z</updated>
        <summary type="html"><![CDATA[Video Visual Relation Detection (VidVRD), has received significant attention
of our community over recent years. In this paper, we apply the
state-of-the-art video object tracklet detection pipeline MEGA and deepSORT to
generate tracklet proposals. Then we perform VidVRD in a tracklet-based manner
without any pre-cutting operations. Specifically, we design a tracklet-based
visual Transformer. It contains a temporal-aware decoder which performs feature
interactions between the tracklets and learnable predicate query embeddings,
and finally predicts the relations. Experimental results strongly demonstrate
the superiority of our method, which outperforms other methods by a large
margin on the Video Relation Understanding (VRU) Grand Challenge in ACM
Multimedia 2021. Codes are released at
https://github.com/Dawn-LX/VidVRD-tracklets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1"&gt;Kaifeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jun Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[C-Net: A Reliable Convolutional Neural Network for Biomedical Image Classification. (arXiv:2011.00081v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00081</id>
        <link href="http://arxiv.org/abs/2011.00081"/>
        <updated>2021-08-20T01:53:51.249Z</updated>
        <summary type="html"><![CDATA[Cancers are the leading cause of death in many countries. Early diagnosis
plays a crucial role in having proper treatment for this debilitating disease.
The automated classification of the type of cancer is a challenging task since
pathologists must examine a huge number of histopathological images to detect
infinitesimal abnormalities. In this study, we propose a novel convolutional
neural network (CNN) architecture composed of a Concatenation of multiple
Networks, called C-Net, to classify biomedical images. The model incorporates
multiple CNNs including Outer, Middle, and Inner. The first two parts of the
architecture contain six networks that serve as feature extractors to feed into
the Inner network to classify the images in terms of malignancy and benignancy.
The C-Net is applied for histopathological image classification on two public
datasets, including BreakHis and Osteosarcoma. To evaluate the performance, the
model is tested using several evaluation metrics for its reliability. The C-Net
model outperforms all other models on the individual metrics for both datasets
and achieves zero misclassification. This approach has the potential to be
extended to additional classification tasks, as experimental results
demonstrate utilizing extensive evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Barzekar_H/0/1/0/all/0/1"&gt;Hosein Barzekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zeyun Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singular Value Decomposition of Images from Scanned Photographic Plates. (arXiv:1310.1869v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1310.1869</id>
        <link href="http://arxiv.org/abs/1310.1869"/>
        <updated>2021-08-20T01:53:51.242Z</updated>
        <summary type="html"><![CDATA[We want to approximate the mxn image A from scanned astronomical photographic
plates (from the Sofia Sky Archive Data Center) by using far fewer entries than
in the original matrix. By using rank of a matrix, k we remove the redundant
information or noise and use as Wiener filter, when rank k<m or k<n. With this
approximation more than 98% compression ration of image of astronomical plate
without that image details, is obtained. The SVD of images from scanned
photographic plates (SPP) is considered and its possible image compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1"&gt;Vasil Kolev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkova_K/0/1/0/all/0/1"&gt;Katya Tsvetkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_M/0/1/0/all/0/1"&gt;Milcho Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. (arXiv:2003.04390v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04390</id>
        <link href="http://arxiv.org/abs/2003.04390"/>
        <updated>2021-08-20T01:53:51.235Z</updated>
        <summary type="html"><![CDATA[Meta-learning has been the most common framework for few-shot learning in
recent years. It learns the model from collections of few-shot classification
tasks, which is believed to have a key advantage of making the training
objective consistent with the testing objective. However, some recent works
report that by training for whole-classification, i.e. classification on the
whole label-set, it can get comparable or even better embedding than many
meta-learning algorithms. The edge between these two lines of works has yet
been underexplored, and the effectiveness of meta-learning in few-shot learning
remains unclear. In this paper, we explore a simple process: meta-learning over
a whole-classification pre-trained model on its evaluation metric. We observe
this simple method achieves competitive performance to state-of-the-art methods
on standard benchmarks. Our further analysis shed some light on understanding
the trade-offs between the meta-learning objective and the whole-classification
objective in few-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinbo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huijuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction. (arXiv:2108.08623v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08623</id>
        <link href="http://arxiv.org/abs/2108.08623"/>
        <updated>2021-08-20T01:53:51.229Z</updated>
        <summary type="html"><![CDATA[To reconstruct a 3D scene from a set of calibrated views, traditional
multi-view stereo techniques rely on two distinct stages: local depth maps
computation and global depth maps fusion. Recent studies concentrate on deep
neural architectures for depth estimation by using conventional depth fusion
method or direct 3D reconstruction network by regressing Truncated Signed
Distance Function (TSDF). In this paper, we advocate that replicating the
traditional two stages framework with deep neural networks improves both the
interpretability and the accuracy of the results. As mentioned, our network
operates in two steps: 1) the local computation of the local depth maps with a
deep MVS technique, and, 2) the depth maps and images' features fusion to build
a single TSDF volume. In order to improve the matching performance between
images acquired from very different viewpoints (e.g., large-baseline and
rotations), we introduce a rotation-invariant 3D convolution kernel called
PosedConv. The effectiveness of the proposed architecture is underlined via a
large series of experiments conducted on the ScanNet dataset where our approach
compares favorably against both traditional and deep learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1"&gt;Jaesung Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1"&gt;Sunghoon Im&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1"&gt;Francois Rameau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Minjun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1"&gt;In So Kweon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks. (arXiv:2108.08755v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08755</id>
        <link href="http://arxiv.org/abs/2108.08755"/>
        <updated>2021-08-20T01:53:51.198Z</updated>
        <summary type="html"><![CDATA[Category-level 6D pose estimation, aiming to predict the location and
orientation of unseen object instances, is fundamental to many scenarios such
as robotic manipulation and augmented reality, yet still remains unsolved.
Precisely recovering instance 3D model in the canonical space and accurately
matching it with the observation is an essential point when estimating 6D pose
for unseen objects. In this paper, we achieve accurate category-level 6D pose
estimation via cascaded relation and recurrent reconstruction networks.
Specifically, a novel cascaded relation network is dedicated for advanced
representation learning to explore the complex and informative relations among
instance RGB image, instance point cloud and category shape prior. Furthermore,
we design a recurrent reconstruction network for iterative residual refinement
to progressively improve the reconstruction and correspondence estimations from
coarse to fine. Finally, the instance 6D pose is obtained leveraging the
estimated dense correspondences between the instance point cloud and the
reconstructed 3D model in the canonical space. We have conducted extensive
experiments on two well-acknowledged benchmarks of category-level 6D pose
estimation, with significant performance improvement over existing approaches.
On the representatively strict evaluation metrics of $3D_{75}$ and $5^{\circ}2
cm$, our method exceeds the latest state-of-the-art SPD by $4.9\%$ and $17.7\%$
on the CAMERA25 dataset, and by $2.7\%$ and $8.5\%$ on the REAL275 dataset.
Codes are available at https://wangjiaze.cn/projects/6DPoseEstimation.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03744</id>
        <link href="http://arxiv.org/abs/2004.03744"/>
        <updated>2021-08-20T01:53:51.191Z</updated>
        <summary type="html"><![CDATA[The recently proposed SNLI-VE corpus for recognising visual-textual
entailment is a large, real-world dataset for fine-grained multimodal
reasoning. However, the automatic way in which SNLI-VE has been assembled (via
combining parts of two related datasets) gives rise to a large number of errors
in the labels of this corpus. In this paper, we first present a data collection
effort to correct the class with the highest error rate in SNLI-VE. Secondly,
we re-evaluate an existing model on the corrected corpus, which we call
SNLI-VE-2.0, and provide a quantitative comparison with its performance on the
non-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends
human-written natural language explanations to SNLI-VE-2.0. Finally, we train
models that learn from these explanations at training time, and output such
explanations at testing time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1"&gt;Virginie Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation. (arXiv:2004.08878v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08878</id>
        <link href="http://arxiv.org/abs/2004.08878"/>
        <updated>2021-08-20T01:53:51.184Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to adapt existing models of the
source domain to a new target domain with only unlabeled data. Most existing
methods suffer from noticeable negative transfer resulting from either the
error-prone discriminator network or the unreasonable teacher model. Besides,
the local regional consistency in UDA has been largely neglected, and only
extracting the global-level pattern information is not powerful enough for
feature alignment due to the abuse use of contexts. To this end, we propose an
uncertainty-aware consistency regularization method for cross-domain semantic
segmentation. Firstly, we introduce an uncertainty-guided consistency loss with
a dynamic weighting scheme by exploiting the latent uncertainty information of
the target samples. As such, more meaningful and reliable knowledge from the
teacher model can be transferred to the student model. We further reveal the
reason why the current consistency regularization is often unstable in
minimizing the domain discrepancy. Besides, we design a ClassDrop mask
generation algorithm to produce strong class-wise perturbations. Guided by this
mask, we propose a ClassOut strategy to realize effective regional consistency
in a fine-grained manner. Experiments demonstrate that our method outperforms
the state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV
$\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $ Cityscapes, Virtual KITTI
$\rightarrow$ KITTI and Cityscapes $\rightarrow$ KITTI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhengyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Qiqi Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuequan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianping Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Controllable and Photorealistic Region-wise Image Manipulation. (arXiv:2108.08674v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08674</id>
        <link href="http://arxiv.org/abs/2108.08674"/>
        <updated>2021-08-20T01:53:51.177Z</updated>
        <summary type="html"><![CDATA[Adaptive and flexible image editing is a desirable function of modern
generative models. In this work, we present a generative model with
auto-encoder architecture for per-region style manipulation. We apply a code
consistency loss to enforce an explicit disentanglement between content and
style latent representations, making the content and style of generated samples
consistent with their corresponding content and style references. The model is
also constrained by a content alignment loss to ensure the foreground editing
will not interfere background contents. As a result, given interested region
masks provided by users, our model supports foreground region-wise style
transfer. Specially, our model receives no extra annotations such as semantic
labels except for self-supervision. Extensive experiments show the
effectiveness of the proposed method and exhibit the flexibility of the
proposed model for various applications, including region-wise style editing,
latent space interpolation, cross-domain style transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1"&gt;Ansheng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chenglin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qixuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing. (arXiv:2108.08807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08807</id>
        <link href="http://arxiv.org/abs/2108.08807"/>
        <updated>2021-08-20T01:53:51.170Z</updated>
        <summary type="html"><![CDATA[We present Neural Generalized Implicit Functions(Neural-GIF), to animate
people in clothing as a function of the body pose. Given a sequence of scans of
a subject in various poses, we learn to animate the character for new poses.
Existing methods have relied on template-based representations of the human
body (or clothing). However such models usually have fixed and limited
resolutions, require difficult data pre-processing steps and cannot be used
with complex clothing. We draw inspiration from template-based methods, which
factorize motion into articulation and non-rigid deformation, but generalize
this concept for implicit shape learning to obtain a more flexible model. We
learn to map every point in the space to a canonical space, where a learned
deformation field is applied to model non-rigid effects, before evaluating the
signed distance field. Our formulation allows the learning of complex and
non-rigid deformations of clothing and soft tissue, without computing a
template registration as it is common with current approaches. Neural-GIF can
be trained on raw 3D scans and reconstructs detailed complex surface geometry
and deformations. Moreover, the model can generalize to new poses. We evaluate
our method on a variety of characters from different public datasets in diverse
clothing styles and show significant improvements over baseline methods,
quantitatively and qualitatively. We also extend our model to multiple shape
setting. To stimulate further research, we will make the model, code and data
publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1"&gt;Garvita Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1"&gt;Nikolaos Sarafianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1"&gt;Tony Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_Moll1_G/0/1/0/all/0/1"&gt;Gerard Pons-Moll1&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08810</id>
        <link href="http://arxiv.org/abs/2108.08810"/>
        <updated>2021-08-20T01:53:51.164Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have so far been the de-facto model for
visual data. Recent work has shown that (Vision) Transformer models (ViT) can
achieve comparable or even superior performance on image classification tasks.
This raises a central question: how are Vision Transformers solving these
tasks? Are they acting like convolutional networks, or learning entirely
different visual representations? Analyzing the internal representation
structure of ViTs and CNNs on image classification benchmarks, we find striking
differences between the two architectures, such as ViT having more uniform
representations across all layers. We explore how these differences arise,
finding crucial roles played by self-attention, which enables early aggregation
of global information, and ViT residual connections, which strongly propagate
features from lower to higher layers. We study the ramifications for spatial
localization, demonstrating ViTs successfully preserve input spatial
information, with noticeable effects from different classification methods.
Finally, we study the effect of (pretraining) dataset scale on intermediate
features and transfer learning, and conclude with a discussion on connections
to new architectures such as the MLP-Mixer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1"&gt;Maithra Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1"&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1"&gt;Simon Kornblith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1"&gt;Alexey Dosovitskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Match Features with Seeded Graph Matching Network. (arXiv:2108.08771v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08771</id>
        <link href="http://arxiv.org/abs/2108.08771"/>
        <updated>2021-08-20T01:53:51.157Z</updated>
        <summary type="html"><![CDATA[Matching local features across images is a fundamental problem in computer
vision. Targeting towards high accuracy and efficiency, we propose Seeded Graph
Matching Network, a graph neural network with sparse structure to reduce
redundant connectivity and learn compact representation. The network consists
of 1) Seeding Module, which initializes the matching by generating a small set
of reliable matches as seeds. 2) Seeded Graph Neural Network, which utilizes
seed matches to pass messages within/across images and predicts assignment
costs. Three novel operations are proposed as basic elements for message
passing: 1) Attentional Pooling, which aggregates keypoint features within the
image to seed matches. 2) Seed Filtering, which enhances seed features and
exchanges messages across images. 3) Attentional Unpooling, which propagates
seed features back to original keypoints. Experiments show that our method
reduces computational and memory complexity significantly compared with typical
attention-based networks while competitive or higher performance is achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongkai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zixin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Lei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xuyang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zeyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1"&gt;Chiew-Lan Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1"&gt;Long Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Attention for Unbiased Visual Recognition. (arXiv:2108.08782v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08782</id>
        <link href="http://arxiv.org/abs/2108.08782"/>
        <updated>2021-08-20T01:53:51.150Z</updated>
        <summary type="html"><![CDATA[Attention module does not always help deep models learn causal features that
are robust in any confounding context, e.g., a foreground object feature is
invariant to different backgrounds. This is because the confounders trick the
attention to capture spurious correlations that benefit the prediction when the
training and testing data are IID (identical & independent distribution); while
harm the prediction when the data are OOD (out-of-distribution). The sole
fundamental solution to learn causal attention is by causal intervention, which
requires additional annotations of the confounders, e.g., a "dog" model is
learned within "grass+dog" and "road+dog" respectively, so the "grass" and
"road" contexts will no longer confound the "dog" recognition. However, such
annotation is not only prohibitively expensive, but also inherently
problematic, as the confounders are elusive in nature. In this paper, we
propose a causal attention module (CaaM) that self-annotates the confounders in
unsupervised fashion. In particular, multiple CaaMs can be stacked and
integrated in conventional attention CNN and self-attention Vision Transformer.
In OOD settings, deep models with CaaM outperform those without it
significantly; even in IID settings, the attention localization is also
improved by CaaM, showing a great potential in applications that require robust
visual saliency. Codes are available at \url{https://github.com/Wangt-CN/CaaM}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qianru Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition. (arXiv:2108.08633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08633</id>
        <link href="http://arxiv.org/abs/2108.08633"/>
        <updated>2021-08-20T01:53:51.120Z</updated>
        <summary type="html"><![CDATA[For a given video-based Human-Object Interaction scene, modeling the
spatio-temporal relationship between humans and objects are the important cue
to understand the contextual information presented in the video. With the
effective spatio-temporal relationship modeling, it is possible not only to
uncover contextual information in each frame but also to directly capture
inter-time dependencies. It is more critical to capture the position changes of
human and objects over the spatio-temporal dimension when their appearance
features may not show up significant changes over time. The full use of
appearance features, the spatial location and the semantic information are also
the key to improve the video-based Human-Object Interaction recognition
performance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks
(STIGPN) are constructed, which encode the videos with a graph composed of
human and object nodes. These nodes are connected by two types of relations:
(i) spatial relations modeling the interactions between human and the
interacted objects within each frame. (ii) inter-time relations capturing the
long range dependencies between human and the interacted objects across frame.
With the graph, STIGPN learn spatio-temporal features directly from the whole
video-based Human-Object Interaction scenes. Multi-modal features and a
multi-stream fusion strategy are used to enhance the reasoning capability of
STIGPN. Two Human-Object Interaction video datasets, including CAD-120 and
Something-Else, are used to evaluate the proposed architectures, and the
state-of-the-art performance demonstrates the superiority of STIGPN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1"&gt;Peiyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1"&gt;Cong Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation. (arXiv:2108.08829v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08829</id>
        <link href="http://arxiv.org/abs/2108.08829"/>
        <updated>2021-08-20T01:53:51.103Z</updated>
        <summary type="html"><![CDATA[Self-supervised monocular depth estimation has been widely studied, owing to
its practical importance and recent promising improvements. However, most works
suffer from limited supervision of photometric consistency, especially in weak
texture regions and at object boundaries. To overcome this weakness, we propose
novel ideas to improve self-supervised monocular depth estimation by leveraging
cross-domain information, especially scene semantics. We focus on incorporating
implicit semantic knowledge into geometric representation enhancement and
suggest two ideas: a metric learning approach that exploits the
semantics-guided local geometry to optimize intermediate depth representations
and a novel feature fusion module that judiciously utilizes cross-modality
between two heterogeneous feature representations. We comprehensively evaluate
our methods on the KITTI dataset and demonstrate that our method outperforms
state-of-the-art methods. The source code is available at
https://github.com/hyBlue/FSRE-Depth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1"&gt;Hyunyoung Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1"&gt;Eunhyeok Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sungjoo Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplierless Modules for Forward and Backward Integer Wavelet Transform. (arXiv:1010.4059v3 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1010.4059</id>
        <link href="http://arxiv.org/abs/1010.4059"/>
        <updated>2021-08-20T01:53:51.083Z</updated>
        <summary type="html"><![CDATA[This article is about the architecture of a lossless wavelet filter bank with
reprogrammable logic. It is based on second generation of wavelets with a
reduced of number of operations. A new basic structure for parallel
architecture and modules to forward and backward integer discrete wavelet
transform is proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1"&gt;Vasil Kolev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Segmentation with Optimal Transport Matching and Message Flow. (arXiv:2108.08518v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08518</id>
        <link href="http://arxiv.org/abs/2108.08518"/>
        <updated>2021-08-20T01:53:51.064Z</updated>
        <summary type="html"><![CDATA[We address the challenging task of few-shot segmentation in this work. It is
essential for few-shot semantic segmentation to fully utilize the support
information. Previous methods typically adapt masked average pooling over the
support feature to extract the support clues as a global vector, usually
dominated by the salient part and loses some important clues. In this work, we
argue that every support pixel's information is desired to be transferred to
all query pixels and propose a Correspondence Matching Network (CMNet) with an
Optimal Transport Matching module to mine out the correspondence between the
query and support images. Besides, it is important to fully utilize both local
and global information from the annotated support images. To this end, we
propose a Message Flow module to propagate the message along the inner-flow
within the same image and cross-flow between support and query images, which
greatly help enhance the local feature representations. We further address the
few-shot segmentation as a multi-task learning problem to alleviate the domain
gap issue between different datasets. Experiments on PASCAL VOC 2012, MS COCO,
and FSS-1000 datasets show that our network achieves new state-of-the-art
few-shot segmentation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weide Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Henghui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1"&gt;Tzu-Yi Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video. (arXiv:1910.00618v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.00618</id>
        <link href="http://arxiv.org/abs/1910.00618"/>
        <updated>2021-08-20T01:53:51.058Z</updated>
        <summary type="html"><![CDATA[Pushing is a fundamental robotic skill. Existing work has shown how to
exploit models of pushing to achieve a variety of tasks, including grasping
under uncertainty, in-hand manipulation and clearing clutter. Such models,
however, are approximate, which limits their applicability. Learning-based
methods can reason directly from raw sensory data with accuracy, and have the
potential to generalize to a wider diversity of scenarios. However, developing
and testing such methods requires rich-enough datasets. In this paper we
introduce Omnipush, a dataset with high variety of planar pushing behavior. In
particular, we provide 250 pushes for each of 250 objects, all recorded with
RGB-D and a high precision tracking system. The objects are constructed so as
to systematically explore key factors that affect pushing -- the shape of the
object and its mass distribution -- which have not been broadly explored in
previous datasets, and allow to study generalization in model learning.
Omnipush includes a benchmark for meta-learning dynamic models, which requires
algorithms that make good predictions and estimate their own uncertainty. We
also provide an RGB video prediction benchmark and propose other relevant tasks
that can be suited with this dataset.

Data and code are available at
\url{https://web.mit.edu/mcube/omnipush-dataset/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1"&gt;Maria Bauza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1"&gt;Ferran Alet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1"&gt;Tomas Lozano-Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1"&gt;Leslie P. Kaelbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alberto Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wind Turbine Blade Surface Damage Detection based on Aerial Imagery and VGG16-RCNN Framework. (arXiv:2108.08636v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.08636</id>
        <link href="http://arxiv.org/abs/2108.08636"/>
        <updated>2021-08-20T01:53:51.051Z</updated>
        <summary type="html"><![CDATA[In this manuscript, an image analytics based deep learning framework for wind
turbine blade surface damage detection is proposed. Turbine blade(s) which
carry approximately one-third of a turbine weight are susceptible to damage and
can cause sudden malfunction of a grid-connected wind energy conversion system.
The surface damage detection of wind turbine blade requires a large dataset so
as to detect a type of damage at an early stage. Turbine blade images are
captured via aerial imagery. Upon inspection, it is found that the image
dataset was limited and hence image augmentation is applied to improve blade
image dataset. The approach is modeled as a multi-class supervised learning
problem and deep learning methods like Convolutional neural network (CNN),
VGG16-RCNN and AlexNet are tested for determining the potential capability of
turbine blade surface damage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1"&gt;Juhi Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_L/0/1/0/all/0/1"&gt;Lagan Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dhiman_H/0/1/0/all/0/1"&gt;Harsh S. Dhiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieval and Localization with Observation Constraints. (arXiv:2108.08516v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08516</id>
        <link href="http://arxiv.org/abs/2108.08516"/>
        <updated>2021-08-20T01:53:51.044Z</updated>
        <summary type="html"><![CDATA[Accurate visual re-localization is very critical to many artificial
intelligence applications, such as augmented reality, virtual reality, robotics
and autonomous driving. To accomplish this task, we propose an integrated
visual re-localization method called RLOCS by combining image retrieval,
semantic consistency and geometry verification to achieve accurate estimations.
The localization pipeline is designed as a coarse-to-fine paradigm. In the
retrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ
DBSCAN followed by spatial verification to obtain a better initial coarse pose.
We design a module called observation constraints, which combines geometry
information and semantic consistency for filtering outliers. Comprehensive
experiments are conducted on open datasets, including retrieval on R-Oxford5k
and R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen
Day-Night and InLoc. By creatively modifying separate modules in the total
pipeline, our method achieves many performance improvements on the challenging
localization benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Huanhuan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shuang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuchen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xudong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jijunnan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yandong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Compositional Learning for Low-shot Scene Graph Generation. (arXiv:2108.08600v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08600</id>
        <link href="http://arxiv.org/abs/2108.08600"/>
        <updated>2021-08-20T01:53:51.037Z</updated>
        <summary type="html"><![CDATA[Scene graphs provide valuable information to many downstream tasks. Many
scene graph generation (SGG) models solely use the limited annotated relation
triples for training, leading to their underperformance on low-shot (few and
zero) scenarios, especially on the rare predicates. To address this problem, we
propose a novel semantic compositional learning strategy that makes it possible
to construct additional, realistic relation triples with objects from different
images. Specifically, our strategy decomposes a relation triple by identifying
and removing the unessential component and composes a new relation triple by
fusing with a semantically or visually similar object from a visual components
dictionary, whilst ensuring the realisticity of the newly composed triple.
Notably, our strategy is generic and can be combined with existing SGG models
to significantly improve their performance. We performed a comprehensive
evaluation on the benchmark dataset Visual Genome. For three recent SGG models,
adding our strategy improves their performance by close to 50\%, and all of
them substantially exceed the current state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis. (arXiv:2108.08827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08827</id>
        <link href="http://arxiv.org/abs/2108.08827"/>
        <updated>2021-08-20T01:53:51.030Z</updated>
        <summary type="html"><![CDATA[Autoregressive models and their sequential factorization of the data
likelihood have recently demonstrated great potential for image representation
and synthesis. Nevertheless, they incorporate image context in a linear 1D
order by attending only to previously synthesized image patches above or to the
left. Not only is this unidirectional, sequential bias of attention unnatural
for images as it disregards large parts of a scene until synthesis is almost
complete. It also processes the entire image on a single scale, thus ignoring
more global contextual information up to the gist of the entire scene. As a
remedy we incorporate a coarse-to-fine hierarchy of context by combining the
autoregressive formulation with a multinomial diffusion process: Whereas a
multistage diffusion process successively removes information to coarsen an
image, we train a (short) Markov chain to invert this process. In each stage,
the resulting autoregressive ImageBART model progressively incorporates context
from previous stages in a coarse-to-fine manner. Experiments show greatly
improved image modification capabilities over autoregressive models while also
providing high-fidelity image generation, both of which are enabled through
efficient training in a compressed latent space. Specifically, our approach can
take unrestricted, user-provided masks into account to perform local image
editing. Thus, in contrast to pure autoregressive models, it can solve
free-form image inpainting and, in the case of conditional models, local,
text-guided image modification without requiring mask-specific training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1"&gt;Patrick Esser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1"&gt;Robin Rombach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1"&gt;Andreas Blattmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary-task learning for geographic data with autoregressive embeddings. (arXiv:2006.10461v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10461</id>
        <link href="http://arxiv.org/abs/2006.10461"/>
        <updated>2021-08-20T01:53:51.003Z</updated>
        <summary type="html"><![CDATA[Machine learning is gaining popularity in a broad range of areas working with
geographic data, such as ecology or atmospheric sciences. Here, data often
exhibit spatial effects, which can be difficult to learn for neural networks.
In this study, we propose SXL, a method for embedding information on the
autoregressive nature of spatial data directly into the learning process using
auxiliary tasks. We utilize the local Moran's I, a popular measure of local
spatial autocorrelation, to "nudge" the model to learn the direction and
magnitude of local spatial effects, complementing the learning of the primary
task. We further introduce a novel expansion of Moran's I to multiple
resolutions, thus capturing spatial interactions over longer and shorter
distances simultaneously. The novel multi-resolution Moran's I can be
constructed easily and as a multi-dimensional tensor offers seamless
integration into existing machine learning frameworks. Throughout a range of
experiments using real-world data, we highlight how our method consistently
improves the training of neural networks in unsupervised and supervised
learning tasks. In generative spatial modeling experiments, we propose a novel
loss for auxiliary task GANs utilizing task uncertainty weights. Our proposed
method outperforms domain-specific spatial interpolation benchmarks,
highlighting its potential for downstream applications. This study bridges
expertise from geographic information science and machine learning, showing how
this integration of disciplines can help to address domain-specific challenges.
The code for our experiments is available on Github:
https://github.com/konstantinklemmer/sxl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1"&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1"&gt;Daniel B. Neill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimal Cases for Computing the Generalized Relative Pose using Affine Correspondences. (arXiv:2007.10700v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10700</id>
        <link href="http://arxiv.org/abs/2007.10700"/>
        <updated>2021-08-20T01:53:50.994Z</updated>
        <summary type="html"><![CDATA[We propose three novel solvers for estimating the relative pose of a
multi-camera system from affine correspondences (ACs). A new constraint is
derived interpreting the relationship of ACs and the generalized camera model.
Using the constraint, we demonstrate efficient solvers for two types of motions
assumed. Considering that the cameras undergo planar motion, we propose a
minimal solution using a single AC and a solver with two ACs to overcome the
degenerate case. Also, we propose a minimal solution using two ACs with known
vertical direction, e.g., from an IMU. Since the proposed methods require
significantly fewer correspondences than state-of-the-art algorithms, they can
be efficiently used within RANSAC for outlier removal and initial motion
estimation. The solvers are tested both on synthetic data and on real-world
scenes from the KITTI odometry benchmark. It is shown that the accuracy of the
estimated poses is superior to the state-of-the-art techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1"&gt;Banglei Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Ji Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1"&gt;Daniel Barath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1"&gt;Friedrich Fraundorfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12245</id>
        <link href="http://arxiv.org/abs/2006.12245"/>
        <updated>2021-08-20T01:53:50.988Z</updated>
        <summary type="html"><![CDATA[We develop a transductive meta-learning method that uses unlabelled instances
to improve few-shot image classification performance. Our approach combines a
regularized Mahalanobis-distance-based soft k-means clustering procedure with a
modified state of the art neural adaptive feature extractor to achieve improved
test-time classification accuracy using unlabelled data. We evaluate our method
on transductive few-shot learning tasks, in which the goal is to jointly
predict labels for query (test) examples given a set of support (training)
examples. We achieve state-of-the-art performance on the Meta-Dataset,
mini-ImageNet and tiered-ImageNet benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1"&gt;Peyman Bateni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1"&gt;Jarred Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1"&gt;Jan-Willem van de Meent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1"&gt;Frank Wood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. (arXiv:2108.08505v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08505</id>
        <link href="http://arxiv.org/abs/2108.08505"/>
        <updated>2021-08-20T01:53:50.981Z</updated>
        <summary type="html"><![CDATA[Perceptual quality assessment of the videos acquired in the wilds is of vital
importance for quality assurance of video services. The inaccessibility of
reference videos with pristine quality and the complexity of authentic
distortions pose great challenges for this kind of blind video quality
assessment (BVQA) task. Although model-based transfer learning is an effective
and efficient paradigm for the BVQA task, it remains to be a challenge to
explore what and how to bridge the domain shifts for better video
representation. In this work, we propose to transfer knowledge from image
quality assessment (IQA) databases with authentic distortions and large-scale
action recognition with rich motion patterns. We rely on both groups of data to
learn the feature extractor. We train the proposed model on the target VQA
databases using a mixed list-wise ranking loss function. Extensive experiments
on six databases demonstrate that our method performs very competitively under
both individual database and mixed database training settings. We also verify
the rationality of each component of the proposed method and explore a simple
manner for further improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weixia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tian_M/0/1/0/all/0/1"&gt;Meng Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xianpei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08536</id>
        <link href="http://arxiv.org/abs/2108.08536"/>
        <updated>2021-08-20T01:53:50.975Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims
at inferring novel object categories in an unlabeled set by leveraging from
prior knowledge of a labeled set containing different, but related classes.
Existing approaches tackle this problem by considering multiple objective
functions, usually involving specialized loss terms for the labeled and the
unlabeled samples respectively, and often requiring auxiliary regularization
terms. In this paper, we depart from this traditional scheme and introduce a
UNified Objective function (UNO) for discovering novel classes, with the
explicit purpose of favoring synergy between supervised and unsupervised
learning. Using a multi-view self-labeling strategy, we generate pseudo-labels
that can be treated homogeneously with ground truth labels. This leads to a
single classification objective operating on both known and unknown classes.
Despite its simplicity, UNO outperforms the state of the art by a significant
margin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The
project page is available at: \url{https://ncd-uno.github.io}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1"&gt;Enrico Fini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1"&gt;Moin Nabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Federated Learning for Heterogeneous Pancreas Segmentation. (arXiv:2108.08537v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08537</id>
        <link href="http://arxiv.org/abs/2108.08537"/>
        <updated>2021-08-20T01:53:50.957Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) for medical image segmentation becomes more
challenging in multi-task settings where clients might have different
categories of labels represented in their data. For example, one client might
have patient data with "healthy'' pancreases only while datasets from other
clients may contain cases with pancreatic tumors. The vanilla federated
averaging algorithm makes it possible to obtain more generalizable deep
learning-based segmentation models representing the training data from multiple
institutions without centralizing datasets. However, it might be sub-optimal
for the aforementioned multi-task scenarios. In this paper, we investigate
heterogeneous optimization methods that show improvements for the automated
segmentation of pancreas and pancreatic tumors in abdominal CT images with FL
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pochuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1"&gt;Holger R. Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Daguang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oda_M/0/1/0/all/0/1"&gt;Masahiro Oda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weichung Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuh_C/0/1/0/all/0/1"&gt;Chiou-Shann Fuh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Po-Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kao-Lang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wei-Chih Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1"&gt;Kensaku Mori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DESYR: Definition and Syntactic Representation Based Claim Detection on the Web. (arXiv:2108.08759v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08759</id>
        <link href="http://arxiv.org/abs/2108.08759"/>
        <updated>2021-08-20T01:53:50.948Z</updated>
        <summary type="html"><![CDATA[The formulation of a claim rests at the core of argument mining. To demarcate
between a claim and a non-claim is arduous for both humans and machines, owing
to latent linguistic variance between the two and the inadequacy of extensive
definition-based formalization. Furthermore, the increase in the usage of
online social media has resulted in an explosion of unsolicited information on
the web presented as informal text. To account for the aforementioned, in this
paper, we proposed DESYR. It is a framework that intends on annulling the said
issues for informal web-based text by leveraging a combination of hierarchical
representation learning (dependency-inspired Poincare embedding),
definition-based alignment, and feature projection. We do away with fine-tuning
computer-heavy language models in favor of fabricating a more domain-centric
but lighter approach. Experimental results indicate that DESYR builds upon the
state-of-the-art system across four benchmark claim datasets, most of which
were constructed with informal texts. We see an increase of 3 claim-F1 points
on the LESA-Twitter dataset, an increase of 1 claim-F1 point and 9 macro-F1
points on the Online Comments(OC) dataset, an increase of 24 claim-F1 points
and 17 macro-F1 points on the Web Discourse(WD) dataset, and an increase of 8
claim-F1 points and 5 macro-F1 points on the Micro Texts(MT) dataset. We also
perform an extensive analysis of the results. We make a 100-D pre-trained
version of our Poincare-variant along with the source code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1"&gt;Megha Sundriyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Parantak Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1"&gt;Md Shad Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Shubhashis Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting. (arXiv:2108.08784v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08784</id>
        <link href="http://arxiv.org/abs/2108.08784"/>
        <updated>2021-08-20T01:53:50.941Z</updated>
        <summary type="html"><![CDATA[Datasets for training crowd counting deep networks are typically heavy-tailed
in count distribution and exhibit discontinuities across the count range. As a
result, the de facto statistical measures (MSE, MAE) exhibit large variance and
tend to be unreliable indicators of performance across the count range. To
address these concerns in a holistic manner, we revise processes at various
stages of the standard crowd counting pipeline. To enable principled and
balanced minibatch sampling, we propose a novel smoothed Bayesian sample
stratification approach. We propose a novel cost function which can be readily
incorporated into existing crowd counting deep networks to encourage
strata-aware optimization. We analyze the performance of representative crowd
counting approaches across standard datasets at per strata level and in
aggregate. We analyze the performance of crowd counting approaches across
standard datasets and demonstrate that our proposed modifications noticeably
reduce error standard deviation. Our contributions represent a nuanced,
statistically balanced and fine-grained characterization of performance for
crowd counting approaches. Code, pretrained models and interactive
visualizations can be viewed at our project page https://deepcount.iiit.ac.in/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1"&gt;Sravya Vardhani Shivapuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khamkar_M/0/1/0/all/0/1"&gt;Mansi Pradeep Khamkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_D/0/1/0/all/0/1"&gt;Divij Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Kernel Consistency for Blind Video Super-Resolution. (arXiv:2108.08305v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08305</id>
        <link href="http://arxiv.org/abs/2108.08305"/>
        <updated>2021-08-20T01:53:50.934Z</updated>
        <summary type="html"><![CDATA[Deep learning-based blind super-resolution (SR) methods have recently
achieved unprecedented performance in upscaling frames with unknown
degradation. These models are able to accurately estimate the unknown
downscaling kernel from a given low-resolution (LR) image in order to leverage
the kernel during restoration. Although these approaches have largely been
successful, they are predominantly image-based and therefore do not exploit the
temporal properties of the kernels across multiple video frames. In this paper,
we investigated the temporal properties of the kernels and highlighted its
importance in the task of blind video super-resolution. Specifically, we
measured the kernel temporal consistency of real-world videos and illustrated
how the estimated kernels might change per frame in videos of varying
dynamicity of the scene and its objects. With this new insight, we revisited
previous popular video SR approaches, and showed that previous assumptions of
using a fixed kernel throughout the restoration process can lead to visual
artifacts when upscaling real-world videos. In order to counteract this, we
tailored existing single-image and video SR techniques to leverage kernel
consistency during both kernel estimation and video upscaling processes.
Extensive experiments on synthetic and real-world videos show substantial
restoration gains quantitatively and qualitatively, achieving the new
state-of-the-art in blind video SR and underlining the potential of exploiting
kernel temporal consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Lichuan Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1"&gt;Royson Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdelfattah_M/0/1/0/all/0/1"&gt;Mohamed S. Abdelfattah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_H/0/1/0/all/0/1"&gt;Hongkai Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08728</id>
        <link href="http://arxiv.org/abs/2108.08728"/>
        <updated>2021-08-20T01:53:50.925Z</updated>
        <summary type="html"><![CDATA[Attention mechanism has demonstrated great potential in fine-grained visual
recognition tasks. In this paper, we present a counterfactual attention
learning method to learn more effective attention based on causal inference.
Unlike most existing methods that learn visual attention based on conventional
likelihood, we propose to learn the attention with counterfactual causality,
which provides a tool to measure the attention quality and a powerful
supervisory signal to guide the learning process. Specifically, we analyze the
effect of the learned visual attention on network prediction through
counterfactual intervention and maximize the effect to encourage the network to
learn more useful attention for fine-grained image recognition. Empirically, we
evaluate our method on a wide range of fine-grained recognition tasks where
attention plays a crucial role, including fine-grained image categorization,
person re-identification, and vehicle re-identification. The consistent
improvement on all benchmarks demonstrates the effectiveness of our method.
Code is available at https://github.com/raoyongming/CAL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08688</id>
        <link href="http://arxiv.org/abs/2108.08688"/>
        <updated>2021-08-20T01:53:50.916Z</updated>
        <summary type="html"><![CDATA[CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal
model that jointly learns representations of images and texts. The model is
trained on a massive amount of English data and shows impressive performance on
zero-shot classification tasks. Training the same model on a different language
is not trivial, since data in other languages might be not enough and the model
needs high-quality translations of the texts to guarantee a good performance.
In this paper, we present the first CLIP model for the Italian Language
(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show
that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image
retrieval and zero-shot classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1"&gt;Giuseppe Attanasio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pisoni_R/0/1/0/all/0/1"&gt;Raphael Pisoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1"&gt;Silvia Terragni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1"&gt;Gabriele Sarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshmi_S/0/1/0/all/0/1"&gt;Sri Lakshmi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification. (arXiv:2007.03584v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03584</id>
        <link href="http://arxiv.org/abs/2007.03584"/>
        <updated>2021-08-20T01:53:50.898Z</updated>
        <summary type="html"><![CDATA[Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on
person image representation and re-identification task via feature erasing.
However, BDB drops the features \textbf{randomly} which may lead to sub-optimal
results. In this paper, we propose a novel Self-Thresholding attention guided
Adaptive DropBlock network (STADB) for person re-ID which can
\textbf{adaptively} erase the most discriminative regions. Specifically, STADB
first obtains an attention map by channel-wise pooling and returns a drop mask
by thresholding the attention map. Then, the input features and
self-thresholding attention guided drop mask are multiplied to generate the
dropped feature maps. In addition, STADB utilizes the spatial and channel
attention to learn a better feature map and iteratively trains the feature
dropping module for person re-ID. Experiments on several benchmark datasets
demonstrate that the proposed STADB outperforms many other related methods for
person re-ID. The source code of this paper is released at:
\textcolor{red}{\url{https://github.com/wangxiao5791509/STADB_ReID}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1"&gt;Aihua Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Theory-inspired Strategy for Automatic Network Pruning. (arXiv:2108.08532v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08532</id>
        <link href="http://arxiv.org/abs/2108.08532"/>
        <updated>2021-08-20T01:53:50.891Z</updated>
        <summary type="html"><![CDATA[Despite superior performance on many computer vision tasks, deep convolution
neural networks are well known to be compressed on devices that have resource
constraints. Most existing network pruning methods require laborious human
efforts and prohibitive computation resources, especially when the constraints
are changed. This practically limits the application of model compression when
the model needs to be deployed on a wide range of devices. Besides, existing
methods are still challenged by the missing theoretical guidance. In this paper
we propose an information theory-inspired strategy for automatic model
compression. The principle behind our method is the information bottleneck
theory, i.e., the hidden representation should compress information with each
other. We thus introduce the normalized Hilbert-Schmidt Independence Criterion
(nHSIC) on network activations as a stable and generalized indicator of layer
importance. When a certain resource constraint is given, we integrate the HSIC
indicator with the constraint to transform the architecture search problem into
a linear programming problem with quadratic constraints. Such a problem is
easily solved by a convex optimization method with a few seconds. We also
provide a rigorous proof to reveal that optimizing the normalized HSIC
simultaneously minimizes the mutual information between different layers.
Without any search process, our method achieves better compression tradeoffs
comparing to the state-of-the-art compression algorithms. For instance, with
ResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on
ImageNet. Codes are avaliable at
https://github.com/MAC-AutoML/ITPruner/tree/master.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuexiao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1"&gt;Teng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces. (arXiv:2108.08653v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08653</id>
        <link href="http://arxiv.org/abs/2108.08653"/>
        <updated>2021-08-20T01:53:50.883Z</updated>
        <summary type="html"><![CDATA[3D Shape representation has substantial effects on 3D shape reconstruction.
Primitive-based representations approximate a 3D shape mainly by a set of
simple implicit primitives, but the low geometrical complexity of the
primitives limits the shape resolution. Moreover, setting a sufficient number
of primitives for an arbitrary shape is challenging. To overcome these issues,
we propose a constrained implicit algebraic surface as the primitive with few
learnable coefficients and higher geometrical complexities and a deep neural
network to produce these primitives. Our experiments demonstrate the
superiorities of our method in terms of representation power compared to the
state-of-the-art methods in single RGB image 3D shape reconstruction.
Furthermore, we show that our method can semantically learn segments of 3D
shapes in an unsupervised manner. The code is publicly available from
https://myavartanoo.github.io/3dias/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1"&gt;Mohsen Yavartanoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1"&gt;JaeYoung Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1"&gt;Reyhaneh Neshatavar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyoung Mu Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Click to Move: Controlling Video Generation with Sparse Motion. (arXiv:2108.08815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08815</id>
        <link href="http://arxiv.org/abs/2108.08815"/>
        <updated>2021-08-20T01:53:50.877Z</updated>
        <summary type="html"><![CDATA[This paper introduces Click to Move (C2M), a novel framework for video
generation where the user can control the motion of the synthesized video
through mouse clicks specifying simple object trajectories of the key objects
in the scene. Our model receives as input an initial frame, its corresponding
segmentation map and the sparse motion vectors encoding the input provided by
the user. It outputs a plausible video sequence starting from the given frame
and with a motion that is consistent with user input. Notably, our proposed
deep architecture incorporates a Graph Convolution Network (GCN) modelling the
movements of all the objects in the scene in a holistic manner and effectively
combining the sparse user motion information and image features. Experimental
results show that C2M outperforms existing methods on two publicly available
datasets, thus demonstrating the effectiveness of our GCN framework at
modelling object interactions. The source code is publicly available at
https://github.com/PierfrancescoArdino/C2M.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ardino_P/0/1/0/all/0/1"&gt;Pierfrancesco Ardino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1"&gt;Marco De Nadai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1"&gt;Bruno Lepri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation. (arXiv:2108.08574v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08574</id>
        <link href="http://arxiv.org/abs/2108.08574"/>
        <updated>2021-08-20T01:53:50.869Z</updated>
        <summary type="html"><![CDATA[Self-supervised monocular depth estimation has achieved impressive
performance on outdoor datasets. Its performance however degrades notably in
indoor environments because of the lack of textures. Without rich textures, the
photometric consistency is too weak to train a good depth network. Inspired by
the early works on indoor modeling, we leverage the structural regularities
exhibited in indoor scenes, to train a better depth network. Specifically, we
adopt two extra supervisory signals for self-supervised training: 1) the
Manhattan normal constraint and 2) the co-planar constraint. The Manhattan
normal constraint enforces the major surfaces (the floor, ceiling, and walls)
to be aligned with dominant directions. The co-planar constraint states that
the 3D points be well fitted by a plane if they are located within the same
planar region. To generate the supervisory signals, we adopt two components to
classify the major surface normal into dominant directions and detect the
planar regions on the fly during training. As the predicted depth becomes more
accurate after more training epochs, the supervisory signals also improve and
in turn feedback to obtain a better depth model. Through extensive experiments
on indoor benchmark datasets, the results show that our network outperforms the
state-of-the-art methods. The source code is available at
https://github.com/SJTU-ViSYS/StructDepth .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zeyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Danping Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenxian Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Box-Adapt: Domain-Adaptive Medical Image Segmentation using Bounding BoxSupervision. (arXiv:2108.08432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08432</id>
        <link href="http://arxiv.org/abs/2108.08432"/>
        <updated>2021-08-20T01:53:50.850Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved remarkable success in medicalimage segmentation,
but it usually requires a large numberof images labeled with fine-grained
segmentation masks, andthe annotation of these masks can be very expensive
andtime-consuming. Therefore, recent methods try to use un-supervised domain
adaptation (UDA) methods to borrow in-formation from labeled data from other
datasets (source do-mains) to a new dataset (target domain). However, due tothe
absence of labels in the target domain, the performance ofUDA methods is much
worse than that of the fully supervisedmethod. In this paper, we propose a
weakly supervised do-main adaptation setting, in which we can partially label
newdatasets with bounding boxes, which are easier and cheaperto obtain than
segmentation masks. Accordingly, we proposea new weakly-supervised domain
adaptation method calledBox-Adapt, which fully explores the fine-grained
segmenta-tion mask in the source domain and the weak bounding boxin the target
domain. Our Box-Adapt is a two-stage methodthat first performs joint training
on the source and target do-mains, and then conducts self-training with the
pseudo-labelsof the target domain. We demonstrate the effectiveness of
ourmethod in the liver segmentation task. Weakly supervised do-main adaptation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1"&gt;Kayhan Batmanghelich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VIL-100: A New Dataset and A Baseline Model for Video Instance Lane Detection. (arXiv:2108.08482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08482</id>
        <link href="http://arxiv.org/abs/2108.08482"/>
        <updated>2021-08-20T01:53:50.843Z</updated>
        <summary type="html"><![CDATA[Lane detection plays a key role in autonomous driving. While car cameras
always take streaming videos on the way, current lane detection works mainly
focus on individual images (frames) by ignoring dynamics along the video. In
this work, we collect a new video instance lane detection (VIL-100) dataset,
which contains 100 videos with in total 10,000 frames, acquired from different
real traffic scenarios. All the frames in each video are manually annotated to
a high-quality instance-level lane annotation, and a set of frame-level and
video-level metrics are included for quantitative performance evaluation.
Moreover, we propose a new baseline model, named multi-level memory aggregation
network (MMA-Net), for video instance lane detection. In our approach, the
representation of current frame is enhanced by attentively aggregating both
local and global memory features from other frames. Experiments on the new
collected dataset show that the proposed MMA-Net outperforms state-of-the-art
lane detection methods and video object segmentation methods. We release our
dataset and code at https://github.com/yujun0-0/MMA-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yujun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingxia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Song Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains. (arXiv:2108.08356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08356</id>
        <link href="http://arxiv.org/abs/2108.08356"/>
        <updated>2021-08-20T01:53:50.836Z</updated>
        <summary type="html"><![CDATA[In this work, for the first time, we address the problem of universal
cross-domain retrieval, where the test data can belong to classes or domains
which are unseen during training. Due to dynamically increasing number of
categories and practical constraint of training on every possible domain, which
requires large amounts of data, generalizing to both unseen classes and domains
is important. Towards that goal, we propose SnMpNet (Semantic Neighbourhood and
Mixture Prediction Network), which incorporates two novel losses to account for
the unseen classes and domains encountered during testing. Specifically, we
introduce a novel Semantic Neighborhood loss to bridge the knowledge gap
between seen and unseen classes and ensure that the latent space embedding of
the unseen classes is semantically meaningful with respect to its neighboring
classes. We also introduce a mix-up based supervision at image-level as well as
semantic-level of the data for training with the Mixture Prediction loss, which
helps in efficient retrieval when the query belongs to an unseen domain. These
losses are incorporated on the SE-ResNet50 backbone to obtain SnMpNet.
Extensive experiments on two large-scale datasets, Sketchy Extended and
DomainNet, and thorough comparisons with state-of-the-art justify the
effectiveness of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Soumava Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_T/0/1/0/all/0/1"&gt;Titir Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Soma Biswas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (arXiv:2108.08487v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08487</id>
        <link href="http://arxiv.org/abs/2108.08487"/>
        <updated>2021-08-20T01:53:50.829Z</updated>
        <summary type="html"><![CDATA[Recently, the generalization behavior of Convolutional Neural Networks (CNN)
is gradually transparent through explanation techniques with the frequency
components decomposition. However, the importance of the phase spectrum of the
image for a robust vision system is still ignored. In this paper, we notice
that the CNN tends to converge at the local optimum which is closely related to
the high-frequency components of the training images, while the amplitude
spectrum is easily disturbed such as noises or common corruptions. In contrast,
more empirical studies found that humans rely on more phase components to
achieve robust recognition. This observation leads to more explanations of the
CNN's generalization behaviors in both robustness to common perturbations and
out-of-distribution detection, and motivates a new perspective on data
augmentation designed by re-combing the phase spectrum of the current image and
the amplitude spectrum of the distracter image. That is, the generated samples
force the CNN to pay more attention to the structured information from phase
components and keep robust to the variation of the amplitude. Experiments on
several image datasets indicate that the proposed method achieves
state-of-the-art performances on multiple generalizations and calibration
tasks, including adaptability for common corruptions and surface variations,
out-of-distribution detection, and adversarial attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1"&gt;Peixi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Li Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lin Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images. (arXiv:2108.08775v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08775</id>
        <link href="http://arxiv.org/abs/2108.08775"/>
        <updated>2021-08-20T01:53:50.823Z</updated>
        <summary type="html"><![CDATA[The world is going through a challenging phase due to the disastrous effect
caused by the COVID-19 pandemic on the healthcare system and the economy. The
rate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of
COVID-19 have put the healthcare systems in disruption across the globe. Due to
this, the task of accurately screening COVID-19 cases has become of utmost
priority. Since the virus infects the respiratory system, Chest X-Ray is an
imaging modality that is adopted extensively for the initial screening. We have
performed a comprehensive study that uses CXR images to identify COVID-19 cases
and realized the necessity of having a more generalizable model. We utilize
MobileNetV2 architecture as the feature extractor and integrate it into Capsule
Networks to construct a fully automated and lightweight model termed as
MobileCaps. MobileCaps is trained and evaluated on the publicly available
dataset with the model ensembling and Bayesian optimization strategies to
efficiently classify CXR images of patients with COVID-19 from non-COVID-19
pneumonia and healthy cases. The proposed model is further evaluated on two
additional RT-PCR confirmed datasets to demonstrate the generalizability. We
also introduce MobileCaps-S and leverage it for performing severity assessment
of CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema
(RALE) scoring technique. Our classification model achieved an overall recall
of 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,
non-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity
assessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that
the proposed models have fewer trainable parameters than the state-of-the-art
models reported in the literature, we believe our models will go a long way in
aiding healthcare systems in the battle against the pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1"&gt;S J Pawan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1"&gt;Rahul Sankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prabhudev_A/0/1/0/all/0/1"&gt;Amithash M Prabhudev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahesh_P/0/1/0/all/0/1"&gt;P A Mahesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prakashini_K/0/1/0/all/0/1"&gt;K Prakashini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1"&gt;Sudha Kiran Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1"&gt;Jeny Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (arXiv:2108.08421v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08421</id>
        <link href="http://arxiv.org/abs/2108.08421"/>
        <updated>2021-08-20T01:53:50.804Z</updated>
        <summary type="html"><![CDATA[Vision systems that deploy Deep Neural Networks (DNNs) are known to be
vulnerable to adversarial examples. Recent research has shown that checking the
intrinsic consistencies in the input data is a promising way to detect
adversarial attacks (e.g., by checking the object co-occurrence relationships
in complex scenes). However, existing approaches are tied to specific models
and do not offer generalizability. Motivated by the observation that language
descriptions of natural scene images have already captured the object
co-occurrence relationships that can be learned by a language model, we develop
a novel approach to perform context consistency checks using such language
models. The distinguishing aspect of our approach is that it is independent of
the deployed object detector and yet offers very high accuracy in terms of
detecting adversarial examples in practical scenes with multiple objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Mingjun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shasha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zikui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chengyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;M. Salman Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Srikanth V. Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Video Representation Learning with Meta-Contrastive Network. (arXiv:2108.08426v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08426</id>
        <link href="http://arxiv.org/abs/2108.08426"/>
        <updated>2021-08-20T01:53:50.796Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning has been successfully applied to pre-train video
representations, which aims at efficient adaptation from pre-training domain to
downstream tasks. Existing approaches merely leverage contrastive loss to learn
instance-level discrimination. However, lack of category information will lead
to hard-positive problem that constrains the generalization ability of this
kind of methods. We find that the multi-task process of meta learning can
provide a solution to this problem. In this paper, we propose a
Meta-Contrastive Network (MCN), which combines the contrastive learning and
meta learning, to enhance the learning ability of existing self-supervised
approaches. Our method contains two training stages based on model-agnostic
meta learning (MAML), each of which consists of a contrastive branch and a meta
branch. Extensive evaluations demonstrate the effectiveness of our method. For
two downstream tasks, i.e., video action recognition and video retrieval, MCN
outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be
more specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8%
and 54.5% for video action recognition, as well as 52.5% and 23.7% for video
retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuanze Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03744</id>
        <link href="http://arxiv.org/abs/2004.03744"/>
        <updated>2021-08-20T01:53:50.788Z</updated>
        <summary type="html"><![CDATA[The recently proposed SNLI-VE corpus for recognising visual-textual
entailment is a large, real-world dataset for fine-grained multimodal
reasoning. However, the automatic way in which SNLI-VE has been assembled (via
combining parts of two related datasets) gives rise to a large number of errors
in the labels of this corpus. In this paper, we first present a data collection
effort to correct the class with the highest error rate in SNLI-VE. Secondly,
we re-evaluate an existing model on the corrected corpus, which we call
SNLI-VE-2.0, and provide a quantitative comparison with its performance on the
non-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends
human-written natural language explanations to SNLI-VE-2.0. Finally, we train
models that learn from these explanations at training time, and output such
explanations at testing time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1"&gt;Virginie Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressed sensing of astronomical images:orthogonal wavelets domains. (arXiv:1111.6276v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1111.6276</id>
        <link href="http://arxiv.org/abs/1111.6276"/>
        <updated>2021-08-20T01:53:50.781Z</updated>
        <summary type="html"><![CDATA[A simple approach for orthogonal wavelets in compressed sensing (CS)
applications is presented. We compare efficient algorithm for different
orthogonal wavelet measurement matrices in CS for image processing from scanned
photographic plates (SPP). Some important characteristics were obtained for
astronomical image processing of SPP. The best orthogonal wavelet choice for
measurement matrix construction in CS for image compression of images of SPP is
given. The image quality measure for linear and nonlinear image compression
method is defined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1"&gt;Vasil Kolev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Smooth Pose Sequences for Diverse Human Motion Prediction. (arXiv:2108.08422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08422</id>
        <link href="http://arxiv.org/abs/2108.08422"/>
        <updated>2021-08-20T01:53:50.773Z</updated>
        <summary type="html"><![CDATA[Recent progress in stochastic motion prediction, i.e., predicting multiple
possible future human motions given a single past pose sequence, has led to
producing truly diverse future motions and even providing control over the
motion of some body parts. However, to achieve this, the state-of-the-art
method requires learning several mappings for diversity and a dedicated model
for controllable motion prediction. In this paper, we introduce a unified deep
generative network for both diverse and controllable motion prediction. To this
end, we leverage the intuition that realistic human motions consist of smooth
sequences of valid poses, and that, given limited data, learning a pose prior
is much more tractable than a motion one. We therefore design a generator that
predicts the motion of different body parts sequentially, and introduce a
normalizing flow based pose prior, together with a joint angle loss, to achieve
motion realism.Our experiments on two standard benchmark datasets, Human3.6M
and HumanEva-I, demonstrate that our approach outperforms the state-of-the-art
baselines in terms of both sample diversity and accuracy. The code is available
at https://github.com/wei-mao-2019/gsps]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1"&gt;Wei Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miaomiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More but Correct: Generating Diversified and Entity-revised Medical Response. (arXiv:2108.01266v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01266</id>
        <link href="http://arxiv.org/abs/2108.01266"/>
        <updated>2021-08-20T01:53:50.756Z</updated>
        <summary type="html"><![CDATA[Medical Dialogue Generation (MDG) is intended to build a medical dialogue
system for intelligent consultation, which can communicate with patients in
real-time, thereby improving the efficiency of clinical diagnosis with broad
application prospects. This paper presents our proposed framework for the
Chinese MDG organized by the 2021 China conference on knowledge graph and
semantic computing (CCKS) competition, which requires generating
context-consistent and medically meaningful responses conditioned on the
dialogue history. In our framework, we propose a pipeline system composed of
entity prediction and entity-aware dialogue generation, by adding predicted
entities to the dialogue model with a fusion mechanism, thereby utilizing
information from different sources. At the decoding stage, we propose a new
decoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve
entity correctness and promote the length and quality of the final response.
The proposed method wins both the CCKS and the International Conference on
Learning Representations (ICLR) 2021 Workshop Machine Learning for Preventing
and Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which
demonstrate the practicality and effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Encheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongru Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1"&gt;Yixuan Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yongping Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Meiling Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-S3Net: Graph-based Panoptic Sparse Semantic Segmentation Network. (arXiv:2108.08401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08401</id>
        <link href="http://arxiv.org/abs/2108.08401"/>
        <updated>2021-08-20T01:53:50.720Z</updated>
        <summary type="html"><![CDATA[Panoptic segmentation as an integrated task of both static environmental
understanding and dynamic object identification, has recently begun to receive
broad research interest. In this paper, we propose a new computationally
efficient LiDAR based panoptic segmentation framework, called GP-S3Net.
GP-S3Net is a proposal-free approach in which no object proposals are needed to
identify the objects in contrast to conventional two-stage panoptic systems,
where a detection network is incorporated for capturing instance information.
Our new design consists of a novel instance-level network to process the
semantic results by constructing a graph convolutional network to identify
objects (foreground), which later on are fused with the background classes.
Through the fine-grained clusters of the foreground objects from the semantic
segmentation backbone, over-segmentation priors are generated and subsequently
processed by 3D sparse convolution to embed each cluster. Each cluster is
treated as a node in the graph and its corresponding embedding is used as its
node feature. Then a GCNN predicts whether edges exist between each cluster
pair. We utilize the instance label to generate ground truth edge labels for
each constructed graph in order to supervise the learning. Extensive
experiments demonstrate that GP-S3Net outperforms the current state-of-the-art
approaches, by a significant margin across available datasets such as, nuScenes
and SemanticPOSS, ranking first on the competitive public SemanticKITTI
leaderboard upon publication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1"&gt;Ryan Razani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1"&gt;Ran Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1"&gt;Enxu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taghavi_E/0/1/0/all/0/1"&gt;Ehsan Taghavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yuan Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bingbing_L/0/1/0/all/0/1"&gt;Liu Bingbing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image2Lego: Customized LEGO Set Generation from Images. (arXiv:2108.08477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08477</id>
        <link href="http://arxiv.org/abs/2108.08477"/>
        <updated>2021-08-20T01:53:50.713Z</updated>
        <summary type="html"><![CDATA[Although LEGO sets have entertained generations of children and adults, the
challenge of designing customized builds matching the complexity of real-world
or imagined scenes remains too great for the average enthusiast. In order to
make this feat possible, we implement a system that generates a LEGO brick
model from 2D images. We design a novel solution to this problem that uses an
octree-structured autoencoder trained on 3D voxelized models to obtain a
feasible latent representation for model reconstruction, and a separate network
trained to predict this latent representation from 2D images. LEGO models are
obtained by algorithmic conversion of the 3D voxelized model to bricks. We
demonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An
octree architecture enables the flexibility to produce multiple resolutions to
best fit a user's creative vision or design needs. In order to demonstrate the
broad applicability of our system, we generate step-by-step building
instructions and animations for LEGO models of objects and human faces.
Finally, we test these automatically generated LEGO sets by constructing
physical builds using real LEGO bricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lennon_K/0/1/0/all/0/1"&gt;Kyle Lennon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fransen_K/0/1/0/all/0/1"&gt;Katharina Fransen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1"&gt;Alexander O&amp;#x27;Brien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yumeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1"&gt;Matthew Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arefeen_Y/0/1/0/all/0/1"&gt;Yamin Arefeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Nikhil Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Reinforced Attention Learning for Visual Place Recognition. (arXiv:2108.08443v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08443</id>
        <link href="http://arxiv.org/abs/2108.08443"/>
        <updated>2021-08-20T01:53:50.705Z</updated>
        <summary type="html"><![CDATA[Large-scale visual place recognition (VPR) is inherently challenging because
not all visual cues in the image are beneficial to the task. In order to
highlight the task-relevant visual cues in the feature embedding, the existing
attention mechanisms are either based on artificial rules or trained in a
thorough data-driven manner. To fill the gap between the two types, we propose
a novel Semantic Reinforced Attention Learning Network (SRALNet), in which the
inferred attention can benefit from both semantic priors and data-driven
fine-tuning. The contribution lies in two-folds. (1) To suppress misleading
local features, an interpretable local weighting scheme is proposed based on
hierarchical feature distribution. (2) By exploiting the interpretability of
the local weighting scheme, a semantic constrained initialization is proposed
so that the local attention can be reinforced by semantic priors. Experiments
demonstrate that our method outperforms state-of-the-art techniques on
city-scale VPR benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1"&gt;Guohao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yufeng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhenyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Danwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Binary Local Image Description for Resource Limited Devices. (arXiv:2108.08380v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08380</id>
        <link href="http://arxiv.org/abs/2108.08380"/>
        <updated>2021-08-20T01:53:50.699Z</updated>
        <summary type="html"><![CDATA[The advent of a panoply of resource limited devices opens up new challenges
in the design of computer vision algorithms with a clear compromise between
accuracy and computational requirements. In this paper we present new binary
image descriptors that emerge from the application of triplet ranking loss,
hard negative mining and anchor swapping to traditional features based on pixel
differences and image gradients. These descriptors, BAD (Box Average
Difference) and HashSIFT, establish new operating points in the
state-of-the-art's accuracy vs.\ resources trade-off curve. In our experiments
we evaluate the accuracy, execution time and energy consumption of the proposed
descriptors. We show that BAD bears the fastest descriptor implementation in
the literature while HashSIFT approaches in accuracy that of the top deep
learning-based descriptors, being computationally more efficient. We have made
the source code public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1"&gt;Iago Su&amp;#xe1;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Buenaposada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1"&gt;Luis Baumela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-Species Cell Detection: Datasets on pulmonary hemosiderophages in equine, human and feline specimens. (arXiv:2108.08529v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.08529</id>
        <link href="http://arxiv.org/abs/2108.08529"/>
        <updated>2021-08-20T01:53:50.679Z</updated>
        <summary type="html"><![CDATA[Pulmonary hemorrhage (P-Hem) occurs among multiple species and can have
various causes. Cytology of bronchoalveolarlavage fluid (BALF) using a 5-tier
scoring system of alveolar macrophages based on their hemosiderin content is
considered the most sensitive diagnostic method. We introduce a novel, fully
annotated multi-species P-Hem dataset which consists of 74 cytology whole slide
images (WSIs) with equine, feline and human samples. To create this
high-quality and high-quantity dataset, we developed an annotation pipeline
combining human expertise with deep learning and data visualisation techniques.
We applied a deep learning-based object detection approach trained on 17
expertly annotated equine WSIs, to the remaining 39 equine, 12 human and 7
feline WSIs. The resulting annotations were semi-automatically screened for
errors on multiple types of specialised annotation maps and finally reviewed by
a trained pathologists. Our dataset contains a total of 297,383
hemosiderophages classified into five grades. It is one of the largest publicly
availableWSIs datasets with respect to the number of annotations, the scanned
area and the number of species covered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1"&gt;Christian Marzahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_J/0/1/0/all/0/1"&gt;Jenny Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stayt_J/0/1/0/all/0/1"&gt;Jason Stayt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bienzle_D/0/1/0/all/0/1"&gt;Dorothee Bienzle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welker_L/0/1/0/all/0/1"&gt;Lutz Welker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilm_F/0/1/0/all/0/1"&gt;Frauke Wilm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Voigt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1"&gt;Marc Aubreville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1"&gt;Robert Klopfleisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1"&gt;Katharina Breininger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1"&gt;Christof A. Bertram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STAR: Noisy Semi-Supervised Transfer Learning for Visual Classification. (arXiv:2108.08362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08362</id>
        <link href="http://arxiv.org/abs/2108.08362"/>
        <updated>2021-08-20T01:53:50.672Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning (SSL) has proven to be effective at leveraging
large-scale unlabeled data to mitigate the dependency on labeled data in order
to learn better models for visual recognition and classification tasks.
However, recent SSL methods rely on unlabeled image data at a scale of billions
to work well. This becomes infeasible for tasks with relatively fewer unlabeled
data in terms of runtime, memory and data acquisition. To address this issue,
we propose noisy semi-supervised transfer learning, an efficient SSL approach
that integrates transfer learning and self-training with noisy student into a
single framework, which is tailored for tasks that can leverage unlabeled image
data on a scale of thousands. We evaluate our method on both binary and
multi-class classification tasks, where the objective is to identify whether an
image displays people practicing sports or the type of sport, as well as to
identify the pose from a pool of popular yoga poses. Extensive experiments and
ablation studies demonstrate that by leveraging unlabeled data, our proposed
framework significantly improves visual classification, especially in
multi-class classification settings compared to state-of-the-art methods.
Moreover, incorporating transfer learning not only improves classification
performance, but also requires 6x less compute time and 5x less memory. We also
show that our method boosts robustness of visual classification models, even
without specifically optimizing for adversarial robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1"&gt;Hasib Zunair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gobeil_Y/0/1/0/all/0/1"&gt;Yan Gobeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mercier_S/0/1/0/all/0/1"&gt;Samuel Mercier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning From How Human Correct. (arXiv:2102.00225v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00225</id>
        <link href="http://arxiv.org/abs/2102.00225"/>
        <updated>2021-08-20T01:53:50.663Z</updated>
        <summary type="html"><![CDATA[In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% baseline is based on BERT training on the corrected dataset, which is
hard to surpass.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End License Plate Recognition Pipeline for Real-time Low Resource Video Based Applications. (arXiv:2108.08339v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08339</id>
        <link href="http://arxiv.org/abs/2108.08339"/>
        <updated>2021-08-20T01:53:50.655Z</updated>
        <summary type="html"><![CDATA[Automatic License Plate Recognition systems aim to provide an end-to-end
solution towards detecting, localizing, and recognizing license plate
characters from vehicles appearing in video frames. However, deploying such
systems in the real world requires real-time performance in low-resource
environments. In our paper, we propose a novel two-stage detection pipeline
paired with Vision API that aims to provide real-time inference speed along
with consistently accurate detection and recognition performance. We used a
haar-cascade classifier as a filter on top of our backbone MobileNet SSDv2
detection model. This reduces inference time by only focusing on high
confidence detections and using them for recognition. We also impose a temporal
frame separation strategy to identify multiple vehicle license plates in the
same clip. Furthermore, there are no publicly available Bangla license plate
datasets, for which we created an image dataset and a video dataset containing
license plates in the wild. We trained our models on the image dataset and
achieved an AP(0.5) score of 86% and tested our pipeline on the video dataset
and observed reasonable detection and recognition performance (82.7% detection
rate, and 60.8% OCR F1 score) with real-time processing speed (27.2 frames per
second).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashrafee_A/0/1/0/all/0/1"&gt;Alif Ashrafee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Mohammed Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1"&gt;Mohammad Sabik Irbaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1"&gt;MD Abdullah Al Nasim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04632</id>
        <link href="http://arxiv.org/abs/2106.04632"/>
        <updated>2021-08-20T01:53:50.648Z</updated>
        <summary type="html"><![CDATA[Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-benchmark.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Licheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1"&gt;Rohit Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara Lee Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality assessment of image matchers for DSM generation -- a comparative study based on UAV images. (arXiv:2108.08369v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08369</id>
        <link href="http://arxiv.org/abs/2108.08369"/>
        <updated>2021-08-20T01:53:50.628Z</updated>
        <summary type="html"><![CDATA[Recently developed automatic dense image matching algorithms are now being
implemented for DSM/DTM production, with their pixel-level surface generation
capability offering the prospect of partially alleviating the need for manual
and semi-automatic stereoscopic measurements. In this paper, five
commercial/public software packages for 3D surface generation are evaluated,
using 5cm GSD imagery recorded from a UAV. Generated surface models are
assessed against point clouds generated from mobile LiDAR and manual
stereoscopic measurements. The software packages considered are APS, MICMAC,
SURE, Pix4UAV and an SGM implementation from DLR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruen_A/0/1/0/all/0/1"&gt;Armin Gruen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_C/0/1/0/all/0/1"&gt;Cive Fraser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08597</id>
        <link href="http://arxiv.org/abs/2108.08597"/>
        <updated>2021-08-20T01:53:50.622Z</updated>
        <summary type="html"><![CDATA[Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique is to apply named entity disambiguation (NED)
systems to the question, and retrieve KB facts for the disambiguated entities.
This work presents ECQA, an efficient method that prunes irrelevant parts of
the search space using KB-aware signals. ECQA is based on top-k query
processing over score-ordered lists of KB items that combine signals about
lexical matching, relevance to the question, coherence among candidate items,
and connectivity in the KB graph. Experiments with two recent QA benchmarks
demonstrate the superiority of ECQA over state-of-the-art baselines with
respect to answer presence, size of the search space, and runtimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1"&gt;Philipp Christmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1"&gt;Gerhard Weikum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06467</id>
        <link href="http://arxiv.org/abs/2010.06467"/>
        <updated>2021-08-20T01:53:50.615Z</updated>
        <summary type="html"><![CDATA[The goal of text ranking is to generate an ordered list of texts retrieved
from a corpus in response to a query. Although the most common formulation of
text ranking is search, instances of the task can also be found in many natural
language processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has been responsible for a paradigm shift in natural language
processing (NLP), information retrieval (IR), and beyond. In this survey, we
provide a synthesis of existing work as a single point of entry for
practitioners who wish to gain a better understanding of how to apply
transformers to text ranking problems and researchers who wish to pursue work
in this area. We cover a wide range of modern techniques, grouped into two
high-level categories: transformer models that perform reranking in multi-stage
architectures and dense retrieval techniques that perform ranking directly.
There are two themes that pervade our survey: techniques for handling long
documents, beyond typical sentence-by-sentence processing in NLP, and
techniques for addressing the tradeoff between effectiveness (i.e., result
quality) and efficiency (e.g., query latency, model and index size). Although
transformer architectures and pretraining techniques are recent innovations,
many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of
pretrained transformers for text ranking, this survey also attempts to
prognosticate where the field is heading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1"&gt;Rodrigo Nogueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1"&gt;Andrew Yates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive and Selective Fusion Network for High Dynamic Range Imaging. (arXiv:2108.08585v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08585</id>
        <link href="http://arxiv.org/abs/2108.08585"/>
        <updated>2021-08-20T01:53:50.607Z</updated>
        <summary type="html"><![CDATA[This paper considers the problem of generating an HDR image of a scene from
its LDR images. Recent studies employ deep learning and solve the problem in an
end-to-end fashion, leading to significant performance improvements. However,
it is still hard to generate a good quality image from LDR images of a dynamic
scene captured by a hand-held camera, e.g., occlusion due to the large motion
of foreground objects, causing ghosting artifacts. The key to success relies on
how well we can fuse the input images in their feature space, where we wish to
remove the factors leading to low-quality image generation while performing the
fundamental computations for HDR image generation, e.g., selecting the
best-exposed image/region. We propose a novel method that can better fuse the
features based on two ideas. One is multi-step feature fusion; our network
gradually fuses the features in a stack of blocks having the same structure.
The other is the design of the component block that effectively performs two
operations essential to the problem, i.e., comparing and selecting appropriate
images/regions. Experimental results show that the proposed method outperforms
the previous state-of-the-art methods on the standard benchmark tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kin-man Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1"&gt;Takayuki Okatani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50. (arXiv:2108.08473v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08473</id>
        <link href="http://arxiv.org/abs/2108.08473"/>
        <updated>2021-08-20T01:53:50.599Z</updated>
        <summary type="html"><![CDATA[In this work, deep learning algorithms are used to classify fundus images in
terms of diabetic retinopathy severity. Six different combinations of two model
architectures, the Dense Convolutional Network-121 and the Residual Neural
Network-50 and three image types, RGB, Green, and High Contrast, were tested to
find the highest performing combination. We achieved an average validation loss
of 0.17 and a max validation accuracy of 85 percent. By testing out multiple
combinations, certain combinations of parameters performed better than others,
though minimal variance was found overall. Green filtration was shown to
perform the poorest, while amplified contrast appeared to have a negligible
effect in comparison to RGB analysis. ResNet50 proved to be less of a robust
model as opposed to DenseNet121.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jonathan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1"&gt;Bowen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ram_R/0/1/0/all/0/1"&gt;Rahul Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;David Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FSNet: A Failure Detection Framework for Semantic Segmentation. (arXiv:2108.08748v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08748</id>
        <link href="http://arxiv.org/abs/2108.08748"/>
        <updated>2021-08-20T01:53:50.580Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is an important task that helps autonomous vehicles
understand their surroundings and navigate safely. During deployment, even the
most mature segmentation models are vulnerable to various external factors that
can degrade the segmentation performance with potentially catastrophic
consequences for the vehicle and its surroundings. To address this issue, we
propose a failure detection framework to identify pixel-level
misclassification. We do so by exploiting internal features of the segmentation
model and training it simultaneously with a failure detection network. During
deployment, the failure detector can flag areas in the image where the
segmentation model have failed to segment correctly. We evaluate the proposed
approach against state-of-the-art methods and achieve 12.30%, 9.46%, and 9.65%
performance improvement in the AUPR-Error metric for Cityscapes, BDD100K, and
Mapillary semantic segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_Q/0/1/0/all/0/1"&gt;Quazi Marufur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1"&gt;Niko S&amp;#xfc;nderhauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corke_P/0/1/0/all/0/1"&gt;Peter Corke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1"&gt;Feras Dayoub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient remedies for outlier detection with variational autoencoders. (arXiv:2108.08760v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08760</id>
        <link href="http://arxiv.org/abs/2108.08760"/>
        <updated>2021-08-20T01:53:50.573Z</updated>
        <summary type="html"><![CDATA[Deep networks often make confident, yet incorrect, predictions when tested
with outlier data that is far removed from their training distributions.
Likelihoods computed by deep generative models are a candidate metric for
outlier detection with unlabeled data. Yet, previous studies have shown that
such likelihoods are unreliable and can be easily biased by simple
transformations to input data. Here, we examine outlier detection with
variational autoencoders (VAEs), among the simplest class of deep generative
models. First, we show that a theoretically-grounded correction readily
ameliorates a key bias with VAE likelihood estimates. The bias correction is
model-free, sample-specific, and accurately computed with the Bernoulli and
continuous Bernoulli visible distributions. Second, we show that a well-known
preprocessing technique, contrast normalization, extends the effectiveness of
bias correction to natural image datasets. Third, we show that the variance of
the likelihoods computed over an ensemble of VAEs also enables robust outlier
detection. We perform a comprehensive evaluation of our remedies with nine
(grayscale and natural) image datasets, and demonstrate significant advantages,
in terms of both speed and accuracy, over four other state-of-the-art methods.
Our lightweight remedies are biologically inspired and may serve to achieve
efficient outlier detection with many types of deep generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1"&gt;Kushal Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1"&gt;Pradeep Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1"&gt;Devarajan Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrent Discrimination and Alignment for Self-Supervised Feature Learning. (arXiv:2108.08562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08562</id>
        <link href="http://arxiv.org/abs/2108.08562"/>
        <updated>2021-08-20T01:53:50.566Z</updated>
        <summary type="html"><![CDATA[Existing self-supervised learning methods learn representation by means of
pretext tasks which are either (1) discriminating that explicitly specify which
features should be separated or (2) aligning that precisely indicate which
features should be closed together, but ignore the fact how to jointly and
principally define which features to be repelled and which ones to be
attracted. In this work, we combine the positive aspects of the discriminating
and aligning methods, and design a hybrid method that addresses the above
issue. Our method explicitly specifies the repulsion and attraction mechanism
respectively by discriminative predictive task and concurrently maximizing
mutual information between paired views sharing redundant information. We
qualitatively and quantitatively show that our proposed model learns better
features that are more effective for the diverse downstream tasks ranging from
classification to semantic segmentation. Our experiments on nine established
benchmarks show that the proposed model consistently outperforms the existing
state-of-the-art results of self-supervised and transfer learning protocol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility. (arXiv:2108.08378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08378</id>
        <link href="http://arxiv.org/abs/2108.08378"/>
        <updated>2021-08-20T01:53:50.557Z</updated>
        <summary type="html"><![CDATA[We present a novel framework for mesh reconstruction from unstructured point
clouds by taking advantage of the learned visibility of the 3D points in the
virtual views and traditional graph-cut based mesh generation. Specifically, we
first propose a three-step network that explicitly employs depth completion for
visibility prediction. Then the visibility information of multiple views is
aggregated to generate a 3D mesh model by solving an optimization problem
considering visibility in which a novel adaptive visibility weighting in
surface determination is also introduced to suppress line of sight with a large
incident angle. Compared to other learning-based approaches, our pipeline only
exercises the learning on a 2D binary classification task, \ie, points visible
or not in a view, which is much more generalizable and practically more
efficient and capable to deal with a large number of points. Experiments
demonstrate that our method with favorable transferability and robustness, and
achieve competing performances \wrt state-of-the-art learning-based approaches
on small complex objects and outperforms on large indoor and outdoor scenes.
Code is available at https://github.com/GDAOSU/vis2mesh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhaopeng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08618</id>
        <link href="http://arxiv.org/abs/2108.08618"/>
        <updated>2021-08-20T01:53:50.548Z</updated>
        <summary type="html"><![CDATA[Radiomics uses quantitative medical imaging features to predict clinical
outcomes. While many radiomics methods have been described in the literature,
these are generally designed for a single application. The aim of this study is
to generalize radiomics across applications by proposing a framework to
automatically construct and optimize the radiomics workflow per application. To
this end, we formulate radiomics as a modular workflow, consisting of several
components: image and segmentation preprocessing, feature extraction, feature
and sample preprocessing, and machine learning. For each component, a
collection of common algorithms is included. To optimize the workflow per
application, we employ automated machine learning using a random search and
ensembling. We evaluate our method in twelve different clinical applications,
resulting in the following area under the curves: 1) liposarcoma (0.83); 2)
desmoid-type fibromatosis (0.82); 3) primary liver tumors (0.81); 4)
gastrointestinal stromal tumors (0.77); 5) colorectal liver metastases (0.68);
6) melanoma metastases (0.51); 7) hepatocellular carcinoma (0.75); 8)
mesenteric fibrosis (0.81); 9) prostate cancer (0.72); 10) glioma (0.70); 11)
Alzheimer's disease (0.87); and 12) head and neck cancer (0.84). Concluding,
our method fully automatically constructs and optimizes the radiomics workflow,
thereby streamlining the search for radiomics biomarkers in new applications.
To facilitate reproducibility and future research, we publicly release six
datasets, the software implementation of our framework (open-source), and the
code to reproduce this study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Starmans_M/0/1/0/all/0/1"&gt;Martijn P. A. Starmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Voort_S/0/1/0/all/0/1"&gt;Sebastian R. van der Voort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phil_T/0/1/0/all/0/1"&gt;Thomas Phil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Timbergen_M/0/1/0/all/0/1"&gt;Milea J. M. Timbergen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vos_M/0/1/0/all/0/1"&gt;Melissa Vos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Padmos_G/0/1/0/all/0/1"&gt;Guillaume A. Padmos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kessels_W/0/1/0/all/0/1"&gt;Wouter Kessels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hanff_D/0/1/0/all/0/1"&gt;David Hanff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grunhagen_D/0/1/0/all/0/1"&gt;Dirk J. Grunhagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verhoef_C/0/1/0/all/0/1"&gt;Cornelis Verhoef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sleijfer_S/0/1/0/all/0/1"&gt;Stefan Sleijfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bent_M/0/1/0/all/0/1"&gt;Martin J. van den Bent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smits_M/0/1/0/all/0/1"&gt;Marion Smits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dwarkasing_R/0/1/0/all/0/1"&gt;Roy S. Dwarkasing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Els_C/0/1/0/all/0/1"&gt;Christopher J. Els&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fiduzi_F/0/1/0/all/0/1"&gt;Federico Fiduzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leenders_G/0/1/0/all/0/1"&gt;Geert J. L. H. van Leenders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blazevic_A/0/1/0/all/0/1"&gt;Anela Blazevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hofland_J/0/1/0/all/0/1"&gt;Johannes Hofland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brabander_T/0/1/0/all/0/1"&gt;Tessa Brabander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gils_R/0/1/0/all/0/1"&gt;Renza A. H. van Gils&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Franssen_G/0/1/0/all/0/1"&gt;Gaston J. H. Franssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feelders_R/0/1/0/all/0/1"&gt;Richard A. Feelders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herder_W/0/1/0/all/0/1"&gt;Wouter W. de Herder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buisman_F/0/1/0/all/0/1"&gt;Florian E. Buisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Willemssen_F/0/1/0/all/0/1"&gt;Francois E. J. A. Willemssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koerkamp_B/0/1/0/all/0/1"&gt;Bas Groot Koerkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Angus_L/0/1/0/all/0/1"&gt;Lindsay Angus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Veldt_A/0/1/0/all/0/1"&gt;Astrid A. M. van der Veldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajicic_A/0/1/0/all/0/1"&gt;Ana Rajicic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Odink_A/0/1/0/all/0/1"&gt;Arlette E. Odink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deen_M/0/1/0/all/0/1"&gt;Mitchell Deen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+T%2E_J/0/1/0/all/0/1"&gt;Jose M. Castillo T.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Veenland_J/0/1/0/all/0/1"&gt;Jifke Veenland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schoots_I/0/1/0/all/0/1"&gt;Ivo Schoots&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Renckens_M/0/1/0/all/0/1"&gt;Michel Renckens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doukas_M/0/1/0/all/0/1"&gt;Michail Doukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Man_R/0/1/0/all/0/1"&gt;Rob A. de Man&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+IJzermans_J/0/1/0/all/0/1"&gt;Jan N. M. IJzermans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miclea_R/0/1/0/all/0/1"&gt;Razvan L. Miclea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vermeulen_P/0/1/0/all/0/1"&gt;Peter B. Vermeulen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bron_E/0/1/0/all/0/1"&gt;Esther E. Bron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thomeer_M/0/1/0/all/0/1"&gt;Maarten G. Thomeer&lt;/a&gt;, et al. (3 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs. (arXiv:2108.08841v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08841</id>
        <link href="http://arxiv.org/abs/2108.08841"/>
        <updated>2021-08-20T01:53:50.522Z</updated>
        <summary type="html"><![CDATA[Controllable scene synthesis consists of generating 3D information that
satisfy underlying specifications. Thereby, these specifications should be
abstract, i.e. allowing easy user interaction, whilst providing enough
interface for detailed control. Scene graphs are representations of a scene,
composed of objects (nodes) and inter-object relationships (edges), proven to
be particularly suited for this task, as they allow for semantic control on the
generated content. Previous works tackling this task often rely on synthetic
data, and retrieve object meshes, which naturally limits the generation
capabilities. To circumvent this issue, we instead propose the first work that
directly generates shapes from a scene graph in an end-to-end manner. In
addition, we show that the same model supports scene modification, using the
respective scene graph as interface. Leveraging Graph Convolutional Networks
(GCN) we train a variational Auto-Encoder on top of the object and edge
categories, as well as 3D shapes and scene layouts, allowing latter sampling of
new scenes and shapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1"&gt;Helisa Dhamo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1"&gt;Fabian Manhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Vivid and Diverse Image Colorization with Generative Color Prior. (arXiv:2108.08826v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08826</id>
        <link href="http://arxiv.org/abs/2108.08826"/>
        <updated>2021-08-20T01:53:50.515Z</updated>
        <summary type="html"><![CDATA[Colorization has attracted increasing interest in recent years. Classic
reference-based methods usually rely on external color images for plausible
results. A large image database or online search engine is inevitably required
for retrieving such exemplars. Recent deep-learning-based methods could
automatically colorize images at a low cost. However, unsatisfactory artifacts
and incoherent colors are always accompanied. In this work, we aim at
recovering vivid colors by leveraging the rich and diverse color priors
encapsulated in a pretrained Generative Adversarial Networks (GAN).
Specifically, we first "retrieve" matched features (similar to exemplars) via a
GAN encoder and then incorporate these features into the colorization process
with feature modulations. Thanks to the powerful generative color prior and
delicate designs, our method could produce vivid colors with a single forward
pass. Moreover, it is highly convenient to obtain diverse results by modifying
GAN latent codes. Our method also inherits the merit of interpretable controls
of GANs and could attain controllable and smooth transitions by walking through
GAN latent space. Extensive experiments and user studies demonstrate that our
method achieves superior performance than previous works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanze Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables. (arXiv:2108.08697v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08697</id>
        <link href="http://arxiv.org/abs/2108.08697"/>
        <updated>2021-08-20T01:53:50.507Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning-based image enhancement algorithms achieved
state-of-the-art (SOTA) performance on several publicly available datasets.
However, most existing methods fail to meet practical requirements either for
visual perception or for computation efficiency, especially for high-resolution
images. In this paper, we propose a novel real-time image enhancer via
learnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well
considers global scenario and local spatial information. Specifically, we
introduce a light weight two-head weight predictor that has two outputs. One is
a 1D weight vector used for image-level scenario adaptation, the other is a 3D
weight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D
LUTs and fuse them according to the aforementioned weights in an end-to-end
manner. The fused LUT is then used to transform the source image into the
target tone in an efficient way. Extensive results show that our model
outperforms SOTA image enhancement methods on public datasets both subjectively
and objectively, and that our model only takes about 4ms to process a 4K
resolution image on one NVIDIA V100 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jingyang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yipeng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1"&gt;Fenglong Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Youliang Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially-Adaptive Image Restoration using Distortion-Guided Networks. (arXiv:2108.08617v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08617</id>
        <link href="http://arxiv.org/abs/2108.08617"/>
        <updated>2021-08-20T01:53:50.499Z</updated>
        <summary type="html"><![CDATA[We present a general learning-based solution for restoring images suffering
from spatially-varying degradations. Prior approaches are typically
degradation-specific and employ the same processing across different images and
different pixels within. However, we hypothesize that such spatially rigid
processing is suboptimal for simultaneously restoring the degraded pixels as
well as reconstructing the clean regions of the image. To overcome this
limitation, we propose SPAIR, a network design that harnesses
distortion-localization information and dynamically adjusts computation to
difficult regions in the image. SPAIR comprises of two components, (1) a
localization network that identifies degraded pixels, and (2) a restoration
network that exploits knowledge from the localization network in filter and
feature domain to selectively and adaptively restore degraded pixels. Our key
idea is to exploit the non-uniformity of heavy degradations in spatial-domain
and suitably embed this knowledge within distortion-guided modules performing
sparse normalization, feature extraction and attention. Our architecture is
agnostic to physical formation model and generalizes across several types of
spatially-varying degradations. We demonstrate the efficacy of SPAIR
individually on four restoration tasks-removal of rain-streaks, raindrops,
shadows and motion blur. Extensive qualitative and quantitative comparisons
with prior art on 11 benchmark datasets demonstrate that our
degradation-agnostic network design offers significant performance gains over
state-of-the-art degradation-specific architectures. Code available at
https://github.com/human-analysis/spatially-adaptive-image-restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1"&gt;Kuldeep Purohit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1"&gt;Maitreya Suin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1"&gt;A. N. Rajagopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1"&gt;Vishnu Naresh Boddeti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Superpixels for High-resolution Images with Decoupled Patch Calibration. (arXiv:2108.08607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08607</id>
        <link href="http://arxiv.org/abs/2108.08607"/>
        <updated>2021-08-20T01:53:50.483Z</updated>
        <summary type="html"><![CDATA[Superpixel segmentation has recently seen important progress benefiting from
the advances in differentiable deep learning. However, the very high-resolution
superpixel segmentation still remains challenging due to the expensive memory
and computation cost, making the current advanced superpixel networks fail to
process. In this paper, we devise Patch Calibration Networks (PCNet), aiming to
efficiently and accurately implement high-resolution superpixel segmentation.
PCNet follows the principle of producing high-resolution output from
low-resolution input for saving GPU memory and relieving computation cost. To
recall the fine details destroyed by the down-sampling operation, we propose a
novel Decoupled Patch Calibration (DPC) branch for collaboratively augment the
main superpixel generation branch. In particular, DPC takes a local patch from
the high-resolution images and dynamically generates a binary mask to impose
the network to focus on region boundaries. By sharing the parameters of DPC and
main branches, the fine-detailed knowledge learned from high-resolution patches
will be transferred to help calibrate the destroyed information. To the best of
our knowledge, we make the first attempt to consider the deep-learning-based
superpixel generation for high-resolution cases. To facilitate this research,
we build evaluation benchmarks from two public datasets and one new constructed
one, covering a wide range of diversities from fine-grained human parts to
cityscapes. Extensive experiments demonstrate that our PCNet can not only
perform favorably against the state-of-the-arts in the quantitative results but
also improve the resolution upper bound from 3K to 5K on 1080Ti GPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yuchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xueming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Li Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to cheat with metrics in single-image HDR reconstruction. (arXiv:2108.08713v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08713</id>
        <link href="http://arxiv.org/abs/2108.08713"/>
        <updated>2021-08-20T01:53:50.475Z</updated>
        <summary type="html"><![CDATA[Single-image high dynamic range (SI-HDR) reconstruction has recently emerged
as a problem well-suited for deep learning methods. Each successive technique
demonstrates an improvement over existing methods by reporting higher image
quality scores. This paper, however, highlights that such improvements in
objective metrics do not necessarily translate to visually superior images. The
first problem is the use of disparate evaluation conditions in terms of data
and metric parameters, calling for a standardized protocol to make it possible
to compare between papers. The second problem, which forms the main focus of
this paper, is the inherent difficulty in evaluating SI-HDR reconstructions
since certain aspects of the reconstruction problem dominate objective
differences, thereby introducing a bias. Here, we reproduce a typical
evaluation using existing as well as simulated SI-HDR methods to demonstrate
how different aspects of the problem affect objective quality metrics.
Surprisingly, we found that methods that do not even reconstruct HDR
information can compete with state-of-the-art deep learning methods. We show
how such results are not representative of the perceived quality and that
SI-HDR reconstruction needs better evaluation protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eilertsen_G/0/1/0/all/0/1"&gt;Gabriel Eilertsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajisharif_S/0/1/0/all/0/1"&gt;Saghi Hajisharif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanji_P/0/1/0/all/0/1"&gt;Param Hanji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsirikoglou_A/0/1/0/all/0/1"&gt;Apostolia Tsirikoglou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1"&gt;Rafal K. Mantiuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unger_J/0/1/0/all/0/1"&gt;Jonas Unger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Inpainting using Partial Convolution. (arXiv:2108.08791v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08791</id>
        <link href="http://arxiv.org/abs/2108.08791"/>
        <updated>2021-08-20T01:53:50.465Z</updated>
        <summary type="html"><![CDATA[Image Inpainting is one of the very popular tasks in the field of image
processing with broad applications in computer vision. In various practical
applications, images are often deteriorated by noise due to the presence of
corrupted, lost, or undesirable information. There have been various
restoration techniques used in the past with both classical and deep learning
approaches for handling such issues. Some traditional methods include image
restoration by filling gap pixels using the nearby known pixels or using the
moving average over the same. The aim of this paper is to perform image
inpainting using robust deep learning methods that use partial convolution
layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1"&gt;Harsh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1"&gt;Amey Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahni_S/0/1/0/all/0/1"&gt;Shivam Sahni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_U/0/1/0/all/0/1"&gt;Udit Vyas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08614</id>
        <link href="http://arxiv.org/abs/2108.08614"/>
        <updated>2021-08-20T01:53:50.459Z</updated>
        <summary type="html"><![CDATA[Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but barely utilize semantic
data and knowledge. This paper presents the first QA system that can seamlessly
operate over RDF datasets and text corpora, or both together, in a unified
framework. Our method, called UNIQORN, builds a context graph on the fly, by
retrieving question-relevant triples from the RDF data and/or the text corpus,
where the latter case is handled by automatic information extraction. The
resulting graph is typically rich but highly noisy. UNIQORN copes with this
input by advanced graph algorithms for Group Steiner Trees, that identify the
best answer candidates in the context graph. Experimental results on several
benchmarks of complex questions with multiple entities and relations, show that
UNIQORN, an unsupervised method with only five parameters, produces results
comparable to the state-of-the-art on KGs, text corpora, and heterogeneous
sources. The graph-based methodology provides user-interpretable evidence for
the complete answering process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1"&gt;Soumajit Pramanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1"&gt;Jesujoba Alabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1"&gt;Gerhard Weikum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Stylization and Domain-aware Contrastive Learning for Domain Generalization. (arXiv:2108.08596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08596</id>
        <link href="http://arxiv.org/abs/2108.08596"/>
        <updated>2021-08-20T01:53:50.452Z</updated>
        <summary type="html"><![CDATA[Domain generalization aims to enhance the model robustness against domain
shift without accessing the target domain. Since the available source domains
for training are limited, recent approaches focus on generating samples of
novel domains. Nevertheless, they either struggle with the optimization problem
when synthesizing abundant domains or cause the distortion of class semantics.
To these ends, we propose a novel domain generalization framework where feature
statistics are utilized for stylizing original features to ones with novel
domain properties. To preserve class information during stylization, we first
decompose features into high and low frequency components. Afterward, we
stylize the low frequency components with the novel domain styles sampled from
the manipulated statistics, while preserving the shape cues in high frequency
ones. As the final step, we re-merge both components to synthesize novel domain
features. To enhance domain robustness, we utilize the stylized features to
maintain the model consistency in terms of features as well as outputs. We
achieve the feature consistency with the proposed domain-aware supervised
contrastive loss, which ensures domain invariance while increasing class
discriminability. Experimental results demonstrate the effectiveness of the
proposed feature stylization and the domain-aware contrastive loss. Through
quantitative comparisons, we verify the lead of our method upon existing
state-of-the-art methods on two benchmarks, PACS and Office-Home.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1"&gt;Seogkyu Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1"&gt;Kibeom Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1"&gt;Pilhyeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jewook Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1"&gt;Hyeran Byun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders. (arXiv:2108.08557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08557</id>
        <link href="http://arxiv.org/abs/2108.08557"/>
        <updated>2021-08-20T01:53:50.445Z</updated>
        <summary type="html"><![CDATA[Human Pose Estimation (HPE) aims at retrieving the 3D position of human
joints from images or videos. We show that current 3D HPE methods suffer a lack
of viewpoint equivariance, namely they tend to fail or perform poorly when
dealing with viewpoints unseen at training time. Deep learning methods often
rely on either scale-invariant, translation-invariant, or rotation-invariant
operations, such as max-pooling. However, the adoption of such procedures does
not necessarily improve viewpoint generalization, rather leading to more
data-dependent methods. To tackle this issue, we propose a novel capsule
autoencoder network with fast Variational Bayes capsule routing, named DECA. By
modeling each joint as a capsule entity, combined with the routing algorithm,
our approach can preserve the joints' hierarchical and geometrical structure in
the feature space, independently from the viewpoint. By achieving viewpoint
equivariance, we drastically reduce the network data dependency at training
time, resulting in an improved ability to generalize for unseen viewpoints. In
the experimental validation, we outperform other methods on depth images from
both seen and unseen viewpoints, both top-view, and front-view. In the RGB
domain, the same network gives state-of-the-art results on the challenging
viewpoint transfer task, also establishing a new framework for top-view HPE.
The code can be found at https://github.com/mmlab-cv/DECA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1"&gt;Nicola Garau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1"&gt;Niccol&amp;#xf2; Bisagno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brodka_P/0/1/0/all/0/1"&gt;Piotr Br&amp;#xf3;dka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1"&gt;Nicola Conci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch-Based Cervical Cancer Segmentation using Distance from Boundary of Tissue. (arXiv:2108.08508v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08508</id>
        <link href="http://arxiv.org/abs/2108.08508"/>
        <updated>2021-08-20T01:53:50.438Z</updated>
        <summary type="html"><![CDATA[Pathological diagnosis is used for examining cancer in detail, and its
automation is in demand. To automatically segment each cancer area, a
patch-based approach is usually used since a Whole Slide Image (WSI) is huge.
However, this approach loses the global information needed to distinguish
between classes. In this paper, we utilized the Distance from the Boundary of
tissue (DfB), which is global information that can be extracted from the
original image. We experimentally applied our method to the three-class
classification of cervical cancer, and found that it improved the total
performance compared with the conventional method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Araki_K/0/1/0/all/0/1"&gt;Kengo Araki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rokutan_Kurata_M/0/1/0/all/0/1"&gt;Mariyo Rokutan-Kurata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Terada_K/0/1/0/all/0/1"&gt;Kazuhiro Terada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshizawa_A/0/1/0/all/0/1"&gt;Akihiko Yoshizawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bise_R/0/1/0/all/0/1"&gt;Ryoma Bise&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08688</id>
        <link href="http://arxiv.org/abs/2108.08688"/>
        <updated>2021-08-20T01:53:50.419Z</updated>
        <summary type="html"><![CDATA[CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal
model that jointly learns representations of images and texts. The model is
trained on a massive amount of English data and shows impressive performance on
zero-shot classification tasks. Training the same model on a different language
is not trivial, since data in other languages might be not enough and the model
needs high-quality translations of the texts to guarantee a good performance.
In this paper, we present the first CLIP model for the Italian Language
(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show
that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image
retrieval and zero-shot classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1"&gt;Giuseppe Attanasio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pisoni_R/0/1/0/all/0/1"&gt;Raphael Pisoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1"&gt;Silvia Terragni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1"&gt;Gabriele Sarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshmi_S/0/1/0/all/0/1"&gt;Sri Lakshmi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08504</id>
        <link href="http://arxiv.org/abs/2108.08504"/>
        <updated>2021-08-20T01:53:50.413Z</updated>
        <summary type="html"><![CDATA[The performance of a computer vision model depends on the size and quality of
its training data. Recent studies have unveiled previously-unknown composition
biases in common image datasets which then lead to skewed model outputs, and
have proposed methods to mitigate these biases. However, most existing works
assume that human-generated annotations can be considered gold-standard and
unbiased. In this paper, we reveal that this assumption can be problematic, and
that special care should be taken to prevent models from learning such
annotation biases. We focus on facial expression recognition and compare the
label biases between lab-controlled and in-the-wild datasets. We demonstrate
that many expression datasets contain significant annotation biases between
genders, especially when it comes to the happy and angry expressions, and that
traditional methods cannot fully mitigate such biases in trained models. To
remove expression annotation bias, we propose an AU-Calibrated Facial
Expression Recognition (AUC-FER) framework that utilizes facial action units
(AUs) and incorporates the triplet loss into the objective function.
Experimental results suggest that the proposed method is more effective in
removing expression annotation bias than existing techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Shapes Local Geometry Codes Learning with SDF. (arXiv:2108.08593v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08593</id>
        <link href="http://arxiv.org/abs/2108.08593"/>
        <updated>2021-08-20T01:53:50.406Z</updated>
        <summary type="html"><![CDATA[A signed distance function (SDF) as the 3D shape description is one of the
most effective approaches to represent 3D geometry for rendering and
reconstruction. Our work is inspired by the state-of-the-art method DeepSDF
that learns and analyzes the 3D shape as the iso-surface of its shell and this
method has shown promising results especially in the 3D shape reconstruction
and compression domain. In this paper, we consider the degeneration problem of
reconstruction coming from the capacity decrease of the DeepSDF model, which
approximates the SDF with a neural network and a single latent code. We propose
Local Geometry Code Learning (LGCL), a model that improves the original DeepSDF
results by learning from a local shape geometry of the full 3D shape. We add an
extra graph neural network to split the single transmittable latent code into a
set of local latent codes distributed on the 3D shape. Mentioned latent codes
are used to approximate the SDF in their local regions, which will alleviate
the complexity of the approximation compared to the original DeepSDF.
Furthermore, we introduce a new geometric loss function to facilitate the
training of these local latent codes. Note that other local shape adjusting
methods use the 3D voxel representation, which in turn is a problem highly
difficult to solve or even is insolvable. In contrast, our architecture is
based on graph processing implicitly and performs the learning regression
process directly in the latent code space, thus make the proposed architecture
more flexible and also simple for realization. Our experiments on 3D shape
reconstruction demonstrate that our LGCL method can keep more details with a
significantly smaller size of the SDF decoder and outperforms considerably the
original DeepSDF method under the most important quantitative metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1"&gt;Shun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yongmei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozerov_M/0/1/0/all/0/1"&gt;Mikhail G. Mozerov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Video Compression with Residual Prediction and Loop Filter. (arXiv:2108.08551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08551</id>
        <link href="http://arxiv.org/abs/2108.08551"/>
        <updated>2021-08-20T01:53:50.389Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a learned video codec with a residual prediction
network (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we
exploit the residual of previous multiple frames to further eliminate the
redundancy of the current frame residual. For the LF-Net, the features from
residual decoding network and the motion compensation network are used to aid
the reconstruction quality. To reduce the complexity, a light ResNet structure
is used as the backbone for both RP-Net and LF-Net. Experimental results
illustrate that we can save about 10% BD-rate compared with previous learned
video compression frameworks. Moreover, we can achieve faster coding speed due
to the ResNet backbone. This project is available at
https://github.com/chaoliu18/RPLVC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1"&gt;Heming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katto_J/0/1/0/all/0/1"&gt;Jiro Katto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaoyang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yibo Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation. (arXiv:2108.08367v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08367</id>
        <link href="http://arxiv.org/abs/2108.08367"/>
        <updated>2021-08-20T01:53:50.382Z</updated>
        <summary type="html"><![CDATA[Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (e.g.
the 3D rotation and translation) in a cluttered environment from a single RGB
image is a challenging problem. While end-to-end methods have recently
demonstrated promising results at high efficiency, they are still inferior when
compared with elaborate P$n$P/RANSAC-based approaches in terms of pose
accuracy. In this work, we address this shortcoming by means of a novel
reasoning about self-occlusion, in order to establish a two-layer
representation for 3D objects which considerably enhances the accuracy of
end-to-end 6D pose estimation. Our framework, named SO-Pose, takes a single RGB
image as input and respectively generates 2D-3D correspondences as well as
self-occlusion information harnessing a shared encoder and two separate
decoders. Both outputs are then fused to directly regress the 6DoF pose
parameters. Incorporating cross-layer consistencies that align correspondences,
self-occlusion and 6D pose, we can further improve accuracy and robustness,
surpassing or rivaling all other state-of-the-art approaches on various
challenging datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1"&gt;Yan Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1"&gt;Fabian Manhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D3D-HOI: Dynamic 3D Human-Object Interactions from Videos. (arXiv:2108.08420v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08420</id>
        <link href="http://arxiv.org/abs/2108.08420"/>
        <updated>2021-08-20T01:53:50.375Z</updated>
        <summary type="html"><![CDATA[We introduce D3D-HOI: a dataset of monocular videos with ground truth
annotations of 3D object pose, shape and part motion during human-object
interactions. Our dataset consists of several common articulated objects
captured from diverse real-world scenes and camera viewpoints. Each manipulated
object (e.g., microwave oven) is represented with a matching 3D parametric
model. This data allows us to evaluate the reconstruction quality of
articulated objects and establish a benchmark for this challenging task. In
particular, we leverage the estimated 3D human pose for more accurate inference
of the object spatial layout and dynamics. We evaluate this approach on our
dataset, demonstrating that human-object relations can significantly reduce the
ambiguity of articulated object reconstructions from challenging real-world
videos. Code and dataset are available at
https://github.com/facebookresearch/d3d-hoi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1"&gt;Hanbyul Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1"&gt;Greg Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1"&gt;Manolis Savva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction. (arXiv:2108.08478v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08478</id>
        <link href="http://arxiv.org/abs/2108.08478"/>
        <updated>2021-08-20T01:53:50.368Z</updated>
        <summary type="html"><![CDATA[While single-view 3D reconstruction has made significant progress benefiting
from deep shape representations in recent years, garment reconstruction is
still not solved well due to open surfaces, diverse topologies and complex
geometric details. In this paper, we propose a novel learnable Anchored
Unsigned Distance Function (AnchorUDF) representation for 3D garment
reconstruction from a single image. AnchorUDF represents 3D shapes by
predicting unsigned distance fields (UDFs) to enable open garment surface
modeling at arbitrary resolution. To capture diverse garment topologies,
AnchorUDF not only computes pixel-aligned local image features of query points,
but also leverages a set of anchor points located around the surface to enrich
3D position features for query points, which provides stronger 3D space context
for the distance function. Furthermore, in order to obtain more accurate point
projection direction at inference, we explicitly align the spatial gradient
direction of AnchorUDF with the ground-truth direction to the surface during
training. Extensive experiments on two public 3D garment datasets, i.e., MGN
and Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art
performance on single-view garment reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shengcai Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03599</id>
        <link href="http://arxiv.org/abs/2105.03599"/>
        <updated>2021-08-20T01:53:50.338Z</updated>
        <summary type="html"><![CDATA[Recently, the retrieval models based on dense representations have been
gradually applied in the first stage of the document retrieval tasks, showing
better performance than traditional sparse vector space models. To obtain high
efficiency, the basic structure of these models is Bi-encoder in most cases.
However, this simple structure may cause serious information loss during the
encoding of documents since the queries are agnostic. To address this problem,
we design a method to mimic the queries on each of the documents by an
iterative clustering process and represent the documents by multiple pseudo
queries (i.e., the cluster centroids). To boost the retrieval process using
approximate nearest neighbor search library, we also optimize the matching
function with a two-step score calculation procedure. Experimental results on
several popular ranking and QA datasets show that our model can achieve
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hongyin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xingwu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1"&gt;Beihong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08708</id>
        <link href="http://arxiv.org/abs/2108.08708"/>
        <updated>2021-08-20T01:53:50.331Z</updated>
        <summary type="html"><![CDATA[This paper describes a novel dataset consisting of sentences with semantic
similarity annotations. The data originate from the journalistic domain in the
Czech language. We describe the process of collecting and annotating the data
in detail. The dataset contains 138,556 human annotations divided into train
and test sets. In total, 485 journalism students participated in the creation
process. To increase the reliability of the test set, we compute the annotation
as an average of 9 individual annotations. We evaluate the quality of the
dataset by measuring inter and intra annotation annotators' agreements. Beside
agreement numbers, we provide detailed statistics of the collected dataset. We
conclude our paper with a baseline experiment of building a system for
predicting the semantic similarity of sentences. Due to the massive number of
training annotations (116 956), the model can perform significantly better than
an average annotator (0,92 versus 0,86 of Person's correlation coefficients).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1"&gt;Jakub Sido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1"&gt;Michal Sej&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Pra&amp;#x17e;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1"&gt;Miloslav Konop&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1"&gt;V&amp;#xe1;clav Moravec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08676</id>
        <link href="http://arxiv.org/abs/2108.08676"/>
        <updated>2021-08-20T01:53:50.324Z</updated>
        <summary type="html"><![CDATA[Existing system dealing with online complaint provides a final decision
without explanations. We propose to analyse the complaint text of internet
fraud in a fine-grained manner. Considering the complaint text includes
multiple clauses with various functions, we propose to identify the role of
each clause and classify them into different types of fraud element. We
construct a large labeled dataset originated from a real finance service
platform. We build an element identification model on top of BERT and propose
additional two modules to utilize the context of complaint text for better
element label classification, namely, global context encoder and label refiner.
Experimental results show the effectiveness of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jingchao Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhongyu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yaqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Heng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liaosa Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1"&gt;Weiqiang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08467</id>
        <link href="http://arxiv.org/abs/2108.08467"/>
        <updated>2021-08-20T01:53:50.316Z</updated>
        <summary type="html"><![CDATA[Computer-aided medical image analysis plays a significant role in assisting
medical practitioners for expert clinical diagnosis and deciding the optimal
treatment plan. At present, convolutional neural networks (CNN) are the
preferred choice for medical image analysis. In addition, with the rapid
advancements in three-dimensional (3D) imaging systems and the availability of
excellent hardware and software support to process large volumes of data, 3D
deep learning methods are gaining popularity in medical image analysis. Here,
we present an extensive review of the recently evolved 3D deep learning methods
in medical image segmentation. Furthermore, the research gaps and future
directions in 3D medical image segmentation are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1"&gt;S. Niyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1"&gt;S J Pawan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M Anand Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1"&gt;Jeny Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Multi-Modal Video Reasoning and Analyzing Competition. (arXiv:2108.08344v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08344</id>
        <link href="http://arxiv.org/abs/2108.08344"/>
        <updated>2021-08-20T01:53:50.258Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the Multi-Modal Video Reasoning and Analyzing
Competition (MMVRAC) workshop in conjunction with ICCV 2021. This competition
is composed of four different tracks, namely, video question answering,
skeleton-based action recognition, fisheye video-based action recognition, and
person re-identification, which are based on two datasets: SUTD-TrafficQA and
UAV-Human. We summarize the top-performing methods submitted by the
participants in this competition and show their results achieved in the
competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Haoran Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;He Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Li Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianjiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1"&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1"&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhicheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rongchang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Mang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiahao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaxu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianbin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models. (arXiv:2108.08451v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08451</id>
        <link href="http://arxiv.org/abs/2108.08451"/>
        <updated>2021-08-20T01:53:50.243Z</updated>
        <summary type="html"><![CDATA[Spoken Language Understanding (SLU) is one essential step in building a
dialogue system. Due to the expensive cost of obtaining the labeled data, SLU
suffers from the data scarcity problem. Therefore, in this paper, we focus on
data augmentation for slot filling task in SLU. To achieve that, we aim at
generating more diverse data based on existing data. Specifically, we try to
exploit the latent language knowledge from pretrained language models by
finetuning them. We propose two strategies for finetuning process: value-based
and context-based augmentation. Experimental results on two public SLU datasets
have shown that compared with existing data augmentation methods, our proposed
method can generate more diverse sentences and significantly improve the
performance on SLU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haitao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Lu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1"&gt;Chengqing Zong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08485</id>
        <link href="http://arxiv.org/abs/2108.08485"/>
        <updated>2021-08-20T01:53:50.215Z</updated>
        <summary type="html"><![CDATA[Although automated metrics are commonly used to evaluate NLG systems, they
often correlate poorly with human judgements. Newer metrics such as BERTScore
have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which
rely on n-gram matching. These newer methods, however, are still limited in
that they do not consider the generation context, so they cannot properly
reward generated text that is correct but deviates from the given reference.

In this paper, we propose Language Model Augmented Relevance Score (MARS), a
new context-aware metric for NLG evaluation. MARS leverages off-the-shelf
language models, guided by reinforcement learning, to create augmented
references that consider both the generation context and available human
references, which are then used as additional references to score generated
text. Compared with seven existing metrics in three common NLG tasks, MARS not
only achieves higher correlation with human reference judgements, but also
differentiates well-formed candidates from adversarial samples to a larger
degree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Fabric: Tubelet Compositions for Video Relation Detection. (arXiv:2108.08363v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08363</id>
        <link href="http://arxiv.org/abs/2108.08363"/>
        <updated>2021-08-20T01:53:50.207Z</updated>
        <summary type="html"><![CDATA[This paper strives to classify and detect the relationship between object
tubelets appearing within a video as a <subject-predicate-object> triplet.
Where existing works treat object proposals or tubelets as single entities and
model their relations a posteriori, we propose to classify and detect
predicates for pairs of object tubelets a priori. We also propose Social
Fabric: an encoding that represents a pair of object tubelets as a composition
of interaction primitives. These primitives are learned over all relations,
resulting in a compact representation able to localize and classify relations
from the pool of co-occurring object tubelets across all timespans in a video.
The encoding enables our two-stage network. In the first stage, we train Social
Fabric to suggest proposals that are likely interacting. We use the Social
Fabric in the second stage to simultaneously fine-tune and predict predicate
labels for the tubelets. Experiments demonstrate the benefit of early video
relation modeling, our encoding and the two-stage architecture, leading to a
new state-of-the-art on two benchmarks. We also show how the encoding enables
query-by-primitive-example to search for spatio-temporal video relations. Code:
https://github.com/shanshuo/Social-Fabric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zenglin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1"&gt;Pascal Mettes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stylized Story Generation with Style-Guided Planning. (arXiv:2105.08625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08625</id>
        <link href="http://arxiv.org/abs/2105.08625"/>
        <updated>2021-08-20T01:53:50.200Z</updated>
        <summary type="html"><![CDATA[Current storytelling systems focus more ongenerating stories with coherent
plots regard-less of the narration style, which is impor-tant for controllable
text generation. There-fore, we propose a new task, stylized story gen-eration,
namely generating stories with speci-fied style given a leading context. To
tacklethe problem, we propose a novel generationmodel that first plans the
stylized keywordsand then generates the whole story with theguidance of the
keywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency
between the generated story andthe specified style. Experiments
demonstratesthat our model can controllably generateemo-tion-driven
orevent-driven stories based onthe ROCStories dataset (Mostafazadeh et
al.,2016). Our study presents insights for stylizedstory generation in further
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiangzhe Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jialiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_Z/0/1/0/all/0/1"&gt;Ziquan Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jian Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Legislative Recipe: Syntax for Machine-Readable Legislation. (arXiv:2108.08678v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08678</id>
        <link href="http://arxiv.org/abs/2108.08678"/>
        <updated>2021-08-20T01:53:50.192Z</updated>
        <summary type="html"><![CDATA[Legal interpretation is a linguistic venture. In judicial opinions, for
example, courts are often asked to interpret the text of statutes and
legislation. As time has shown, this is not always as easy as it sounds.
Matters can hinge on vague or inconsistent language and, under the surface,
human biases can impact the decision-making of judges. This raises an important
question: what if there was a method of extracting the meaning of statutes
consistently? That is, what if it were possible to use machines to encode
legislation in a mathematically precise form that would permit clearer
responses to legal questions? This article attempts to unpack the notion of
machine-readability, providing an overview of both its historical and recent
developments. The paper will reflect on logic syntax and symbolic language to
assess the capacity and limits of representing legal knowledge. In doing so,
the paper seeks to move beyond existing literature to discuss the implications
of various approaches to machine-readable legislation. Importantly, this study
hopes to highlight the challenges encountered in this burgeoning ecosystem of
machine-readable legislation against existing human-readable counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Megan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1"&gt;Bryan Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08787</id>
        <link href="http://arxiv.org/abs/2108.08787"/>
        <updated>2021-08-20T01:53:50.169Z</updated>
        <summary type="html"><![CDATA[We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual
retrieval in eleven typologically diverse languages, designed to evaluate
ranking with learned dense representations. The goal of this resource is to
spur research in dense retrieval techniques in non-English languages, motivated
by recent observations that existing techniques for representation learning
perform poorly when applied to out-of-distribution data. As a starting point,
we provide zero-shot baselines for this new dataset based on a multi-lingual
adaptation of DPR that we call "mDPR". Experiments show that although the
effectiveness of mDPR is much lower than BM25, dense representations
nevertheless appear to provide valuable relevance signals, improving BM25
results in sparse-dense hybrids. In addition to analyses of our results, we
also discuss future challenges and present a research agenda in multi-lingual
dense retrieval. Mr. TyDi can be downloaded at
https://github.com/castorini/mr.tydi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xueguang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08556</id>
        <link href="http://arxiv.org/abs/2108.08556"/>
        <updated>2021-08-20T01:53:50.131Z</updated>
        <summary type="html"><![CDATA[This paper reports the Machine Translation (MT) systems submitted by the
IIITT team for the English->Marathi and English->Irish language pairs LoResMT
2021 shared task. The task focuses on getting exceptional translations for
rather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,
a pretrained multilingual NMT model for English->Marathi, using external
parallel corpus as input for additional training. We have used a pretrained
Helsinki-NLP Opus MT English->Irish model for the latter language pair. Our
approaches yield relatively promising results on the BLEU metrics. Under the
team name IIITT, our systems ranked 1, 1, and 2 in English->Marathi,
Irish->English, and English->Irish, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puranik_K/0/1/0/all/0/1"&gt;Karthik Puranik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1"&gt;Adeep Hande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1"&gt;Ruba Priyadharshini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durairaj_T/0/1/0/all/0/1"&gt;Thenmozi Durairaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1"&gt;Anbukkarasi Sampath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1"&gt;Kingston Pal Thamburaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation. (arXiv:2108.08762v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.08762</id>
        <link href="http://arxiv.org/abs/2108.08762"/>
        <updated>2021-08-20T01:53:50.120Z</updated>
        <summary type="html"><![CDATA[Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1"&gt;Silvan Mertes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangelova_S/0/1/0/all/0/1"&gt;Stanislava Rangelova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flutura_S/0/1/0/all/0/1"&gt;Simon Flutura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08375</id>
        <link href="http://arxiv.org/abs/2108.08375"/>
        <updated>2021-08-20T01:53:50.110Z</updated>
        <summary type="html"><![CDATA[This paper studies the relative importance of attention heads in
Transformer-based models to aid their interpretability in cross-lingual and
multi-lingual tasks. Prior research has found that only a few attention heads
are important in each mono-lingual Natural Language Processing (NLP) task and
pruning the remaining heads leads to comparable or improved performance of the
model. However, the impact of pruning attention heads is not yet clear in
cross-lingual and multi-lingual tasks. Through extensive experiments, we show
that (1) pruning a number of attention heads in a multi-lingual
Transformer-based model has, in general, positive effects on its performance in
cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned
can be ranked using gradients and identified with a few trial experiments. Our
experiments focus on sequence labeling tasks, with potential applicability on
other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine
two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and
XLM-R, on three tasks across 9 languages each. We also discuss the validity of
our findings and their extensibility to truly resource-scarce languages and
other task settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weicheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1"&gt;Renze Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions. (arXiv:2101.11529v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11529</id>
        <link href="http://arxiv.org/abs/2101.11529"/>
        <updated>2021-08-20T01:53:50.100Z</updated>
        <summary type="html"><![CDATA[The lack of fine-grained joints (facial joints, hand fingers) is a
fundamental performance bottleneck for state of the art skeleton action
recognition models. Despite this bottleneck, community's efforts seem to be
invested only in coming up with novel architectures. To specifically address
this bottleneck, we introduce two new pose based human action datasets -
NTU60-X and NTU120-X. Our datasets extend the largest existing action
recognition dataset, NTU-RGBD. In addition to the 25 body joints for each
skeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and
facial joints, enabling a richer skeleton representation. We appropriately
modify the state of the art approaches to enable training using the introduced
datasets. Our results demonstrate the effectiveness of these NTU-X datasets in
overcoming the aforementioned bottleneck and improve state of the art
performance, overall and on previously worst performing action categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_N/0/1/0/all/0/1"&gt;Neel Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1"&gt;Anirudh Thatipelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting. (arXiv:2108.08784v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08784</id>
        <link href="http://arxiv.org/abs/2108.08784"/>
        <updated>2021-08-20T01:53:50.079Z</updated>
        <summary type="html"><![CDATA[Datasets for training crowd counting deep networks are typically heavy-tailed
in count distribution and exhibit discontinuities across the count range. As a
result, the de facto statistical measures (MSE, MAE) exhibit large variance and
tend to be unreliable indicators of performance across the count range. To
address these concerns in a holistic manner, we revise processes at various
stages of the standard crowd counting pipeline. To enable principled and
balanced minibatch sampling, we propose a novel smoothed Bayesian sample
stratification approach. We propose a novel cost function which can be readily
incorporated into existing crowd counting deep networks to encourage
strata-aware optimization. We analyze the performance of representative crowd
counting approaches across standard datasets at per strata level and in
aggregate. We analyze the performance of crowd counting approaches across
standard datasets and demonstrate that our proposed modifications noticeably
reduce error standard deviation. Our contributions represent a nuanced,
statistically balanced and fine-grained characterization of performance for
crowd counting approaches. Code, pretrained models and interactive
visualizations can be viewed at our project page https://deepcount.iiit.ac.in/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1"&gt;Sravya Vardhani Shivapuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khamkar_M/0/1/0/all/0/1"&gt;Mansi Pradeep Khamkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_D/0/1/0/all/0/1"&gt;Divij Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08411</id>
        <link href="http://arxiv.org/abs/2108.08411"/>
        <updated>2021-08-20T01:53:50.069Z</updated>
        <summary type="html"><![CDATA[Twitch chats pose a unique problem in natural language understanding due to a
large presence of neologisms, specifically emotes. There are a total of 8.06
million emotes, over 400k of which were used in the week studied. There is
virtually no information on the meaning or sentiment of emotes, and with a
constant influx of new emotes and drift in their frequencies, it becomes
impossible to maintain an updated manually-labeled dataset. Our paper makes a
two fold contribution. First we establish a new baseline for sentiment analysis
on Twitch data, outperforming the previous supervised benchmark by 7.9% points.
Secondly, we introduce a simple but powerful unsupervised framework based on
word embeddings and k-NN to enrich existing models with out-of-vocabulary
knowledge. This framework allows us to auto-generate a pseudo-dictionary of
emotes and we show that we can nearly match the supervised benchmark above even
when injecting such emote knowledge into sentiment classifiers trained on
extraneous datasets such as movie reviews or Twitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1"&gt;Pavel Dolin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+dHauthuille_L/0/1/0/all/0/1"&gt;Luc d&amp;#x27;Hauthuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vattani_A/0/1/0/all/0/1"&gt;Andrea Vattani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation. (arXiv:2108.08447v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08447</id>
        <link href="http://arxiv.org/abs/2108.08447"/>
        <updated>2021-08-20T01:53:50.056Z</updated>
        <summary type="html"><![CDATA[Conditional masked language models (CMLM) have shown impressive progress in
non-autoregressive machine translation (NAT). They learn the conditional
translation model by predicting the random masked subset in the target
sentence. Based on the CMLM framework, we introduce Multi-view Subset
Regularization (MvSR), a novel regularization method to improve the performance
of the NAT model. Specifically, MvSR consists of two parts: (1) \textit{shared
mask consistency}: we forward the same target with different mask strategies,
and encourage the predictions of shared mask positions to be consistent with
each other. (2) \textit{model consistency}, we maintain an exponential moving
average of the model weights, and enforce the predictions to be consistent
between the average model and the online model. Without changing the CMLM-based
architecture, our approach achieves remarkable performance on three public
benchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover,
compared with the stronger Transformer baseline, we reduce the gap to 0.01-0.44
BLEU scores on small datasets (WMT16 RO$\leftrightarrow$EN and IWSLT
DE$\rightarrow$EN).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zexian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaohui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14118</id>
        <link href="http://arxiv.org/abs/2106.14118"/>
        <updated>2021-08-20T01:53:50.013Z</updated>
        <summary type="html"><![CDATA[State of the art architectures for untrimmed video Temporal Action
Localization (TAL) have only considered RGB and Flow modalities, leaving the
information-rich audio modality totally unexploited. Audio fusion has been
explored for the related but arguably easier problem of trimmed (clip-level)
action recognition. However, TAL poses a unique set of challenges. In this
paper, we propose simple but effective fusion-based approaches for TAL. To the
best of our knowledge, our work is the first to jointly consider audio and
video modalities for supervised TAL. We experimentally show that our schemes
consistently improve performance for state of the art video-only TAL
approaches. Specifically, they help achieve new state of the art performance on
large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14
(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion
schemes, modality combinations and TAL architectures. Our code, models and
associated data are available at https://github.com/skelemoa/tal-hmo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1"&gt;Anurag Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1"&gt;Jazib Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1"&gt;Dolton Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Dialog History into End-to-End Spoken Language Understanding Systems. (arXiv:2108.08405v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08405</id>
        <link href="http://arxiv.org/abs/2108.08405"/>
        <updated>2021-08-20T01:53:50.003Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) systems that process
human-human or human-computer interactions are often context independent and
process each turn of a conversation independently. Spoken conversations on the
other hand, are very much context dependent, and dialog history contains useful
information that can improve the processing of each conversational turn. In
this paper, we investigate the importance of dialog history and how it can be
effectively integrated into end-to-end SLU systems. While processing a spoken
utterance, our proposed RNN transducer (RNN-T) based SLU model has access to
its dialog history in the form of decoded transcripts and SLU labels of
previous turns. We encode the dialog history as BERT embeddings, and use them
as an additional input to the SLU model along with the speech features for the
current utterance. We evaluate our approach on a recently released spoken
dialog data set, the HarperValleyBank corpus. We observe significant
improvements: 8% for dialog action and 30% for caller intent recognition tasks,
in comparison to a competitive context independent end-to-end baseline system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1"&gt;Jatin Ganhotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Samuel Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hong-Kwang J. Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Sachindra Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1"&gt;George Saon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1"&gt;Zolt&amp;#xe1;n T&amp;#xfc;ske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1"&gt;Brian Kingsbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08468</id>
        <link href="http://arxiv.org/abs/2108.08468"/>
        <updated>2021-08-20T01:53:49.979Z</updated>
        <summary type="html"><![CDATA[We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student's performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Chen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tony Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hanqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiwei Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition. (arXiv:2108.08633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08633</id>
        <link href="http://arxiv.org/abs/2108.08633"/>
        <updated>2021-08-20T01:53:49.832Z</updated>
        <summary type="html"><![CDATA[For a given video-based Human-Object Interaction scene, modeling the
spatio-temporal relationship between humans and objects are the important cue
to understand the contextual information presented in the video. With the
effective spatio-temporal relationship modeling, it is possible not only to
uncover contextual information in each frame but also to directly capture
inter-time dependencies. It is more critical to capture the position changes of
human and objects over the spatio-temporal dimension when their appearance
features may not show up significant changes over time. The full use of
appearance features, the spatial location and the semantic information are also
the key to improve the video-based Human-Object Interaction recognition
performance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks
(STIGPN) are constructed, which encode the videos with a graph composed of
human and object nodes. These nodes are connected by two types of relations:
(i) spatial relations modeling the interactions between human and the
interacted objects within each frame. (ii) inter-time relations capturing the
long range dependencies between human and the interacted objects across frame.
With the graph, STIGPN learn spatio-temporal features directly from the whole
video-based Human-Object Interaction scenes. Multi-modal features and a
multi-stream fusion strategy are used to enhance the reasoning capability of
STIGPN. Two Human-Object Interaction video datasets, including CAD-120 and
Something-Else, are used to evaluate the proposed architectures, and the
state-of-the-art performance demonstrates the superiority of STIGPN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1"&gt;Peiyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1"&gt;Cong Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Video Compression with Residual Prediction and Loop Filter. (arXiv:2108.08551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08551</id>
        <link href="http://arxiv.org/abs/2108.08551"/>
        <updated>2021-08-20T01:53:49.822Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a learned video codec with a residual prediction
network (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we
exploit the residual of previous multiple frames to further eliminate the
redundancy of the current frame residual. For the LF-Net, the features from
residual decoding network and the motion compensation network are used to aid
the reconstruction quality. To reduce the complexity, a light ResNet structure
is used as the backbone for both RP-Net and LF-Net. Experimental results
illustrate that we can save about 10% BD-rate compared with previous learned
video compression frameworks. Moreover, we can achieve faster coding speed due
to the ResNet backbone. This project is available at
https://github.com/chaoliu18/RPLVC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1"&gt;Heming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katto_J/0/1/0/all/0/1"&gt;Jiro Katto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaoyang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yibo Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08504</id>
        <link href="http://arxiv.org/abs/2108.08504"/>
        <updated>2021-08-20T01:53:49.791Z</updated>
        <summary type="html"><![CDATA[The performance of a computer vision model depends on the size and quality of
its training data. Recent studies have unveiled previously-unknown composition
biases in common image datasets which then lead to skewed model outputs, and
have proposed methods to mitigate these biases. However, most existing works
assume that human-generated annotations can be considered gold-standard and
unbiased. In this paper, we reveal that this assumption can be problematic, and
that special care should be taken to prevent models from learning such
annotation biases. We focus on facial expression recognition and compare the
label biases between lab-controlled and in-the-wild datasets. We demonstrate
that many expression datasets contain significant annotation biases between
genders, especially when it comes to the happy and angry expressions, and that
traditional methods cannot fully mitigate such biases in trained models. To
remove expression annotation bias, we propose an AU-Calibrated Facial
Expression Recognition (AUC-FER) framework that utilizes facial action units
(AUs) and incorporates the triplet loss into the objective function.
Experimental results suggest that the proposed method is more effective in
removing expression annotation bias than existing techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards More Efficient Federated Learning with Better Optimization Objects. (arXiv:2108.08577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08577</id>
        <link href="http://arxiv.org/abs/2108.08577"/>
        <updated>2021-08-20T01:53:49.781Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a privacy-protected machine learning paradigm that
allows model to be trained directly at the edge without uploading data. One of
the biggest challenges faced by FL in practical applications is the
heterogeneity of edge node data, which will slow down the convergence speed and
degrade the performance of the model. For the above problems, a representative
solution is to add additional constraints in the local training, such as
FedProx, FedCurv and FedCL. However, the above algorithms still have room for
improvement. We propose to use the aggregation of all models obtained in the
past as new constraint target to further improve the performance of such
algorithms. Experiments in various settings demonstrate that our method
significantly improves the convergence speed and performance of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zirui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Ziyi Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03599</id>
        <link href="http://arxiv.org/abs/2105.03599"/>
        <updated>2021-08-20T01:53:49.470Z</updated>
        <summary type="html"><![CDATA[Recently, the retrieval models based on dense representations have been
gradually applied in the first stage of the document retrieval tasks, showing
better performance than traditional sparse vector space models. To obtain high
efficiency, the basic structure of these models is Bi-encoder in most cases.
However, this simple structure may cause serious information loss during the
encoding of documents since the queries are agnostic. To address this problem,
we design a method to mimic the queries on each of the documents by an
iterative clustering process and represent the documents by multiple pseudo
queries (i.e., the cluster centroids). To boost the retrieval process using
approximate nearest neighbor search library, we also optimize the matching
function with a two-step score calculation procedure. Experimental results on
several popular ranking and QA datasets show that our model can achieve
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hongyin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xingwu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1"&gt;Beihong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08787</id>
        <link href="http://arxiv.org/abs/2108.08787"/>
        <updated>2021-08-20T01:53:49.443Z</updated>
        <summary type="html"><![CDATA[We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual
retrieval in eleven typologically diverse languages, designed to evaluate
ranking with learned dense representations. The goal of this resource is to
spur research in dense retrieval techniques in non-English languages, motivated
by recent observations that existing techniques for representation learning
perform poorly when applied to out-of-distribution data. As a starting point,
we provide zero-shot baselines for this new dataset based on a multi-lingual
adaptation of DPR that we call "mDPR". Experiments show that although the
effectiveness of mDPR is much lower than BM25, dense representations
nevertheless appear to provide valuable relevance signals, improving BM25
results in sparse-dense hybrids. In addition to analyses of our results, we
also discuss future challenges and present a research agenda in multi-lingual
dense retrieval. Mr. TyDi can be downloaded at
https://github.com/castorini/mr.tydi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xueguang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChMusic: A Traditional Chinese Music Dataset for Evaluation of Instrument Recognition. (arXiv:2108.08470v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.08470</id>
        <link href="http://arxiv.org/abs/2108.08470"/>
        <updated>2021-08-20T01:53:49.431Z</updated>
        <summary type="html"><![CDATA[Musical instruments recognition is a widely used application for music
information retrieval. As most of previous musical instruments recognition
dataset focus on western musical instruments, it is difficult for researcher to
study and evaluate the area of traditional Chinese musical instrument
recognition. This paper propose a traditional Chinese music dataset for
training model and performance evaluation, named ChMusic. This dataset is free
and publicly available, 11 traditional Chinese musical instruments and 55
traditional Chinese music excerpts are recorded in this dataset. Then an
evaluation standard is proposed based on ChMusic dataset. With this standard,
researchers can compare their results following the same rule, and results from
different researchers will become comparable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xia Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Haidi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1"&gt;Haoran Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiReN: Sign-Aware Recommendation Using Graph Neural Networks. (arXiv:2108.08735v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08735</id>
        <link href="http://arxiv.org/abs/2108.08735"/>
        <updated>2021-08-20T01:53:49.411Z</updated>
        <summary type="html"><![CDATA[In recent years, many recommender systems using network embedding (NE) such
as graph neural networks (GNNs) have been extensively studied in the sense of
improving recommendation accuracy. However, such attempts have focused mostly
on utilizing only the information of positive user-item interactions with high
ratings. Thus, there is a challenge on how to make use of low rating scores for
representing users' preferences since low ratings can be still informative in
designing NE-based recommender systems. In this study, we present SiReN, a new
sign-aware recommender system based on GNN models. Specifically, SiReN has
three key components: 1) constructing a signed bipartite graph for more
precisely representing users' preferences, which is split into two
edge-disjoint graphs with positive and negative edges each, 2) generating two
embeddings for the partitioned graphs with positive and negative edges via a
GNN model and a multi-layer perceptron (MLP), respectively, and then using an
attention model to obtain the final embeddings, and 3) establishing a
sign-aware Bayesian personalized ranking (BPR) loss function in the process of
optimization. Through comprehensive experiments, we empirically demonstrate
that SiReN consistently outperforms state-of-the-art NE-aided recommendation
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1"&gt;Changwon Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1"&gt;Kyeong-Joong Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Sungsu Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won-Yong Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06467</id>
        <link href="http://arxiv.org/abs/2010.06467"/>
        <updated>2021-08-20T01:53:49.398Z</updated>
        <summary type="html"><![CDATA[The goal of text ranking is to generate an ordered list of texts retrieved
from a corpus in response to a query. Although the most common formulation of
text ranking is search, instances of the task can also be found in many natural
language processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has been responsible for a paradigm shift in natural language
processing (NLP), information retrieval (IR), and beyond. In this survey, we
provide a synthesis of existing work as a single point of entry for
practitioners who wish to gain a better understanding of how to apply
transformers to text ranking problems and researchers who wish to pursue work
in this area. We cover a wide range of modern techniques, grouped into two
high-level categories: transformer models that perform reranking in multi-stage
architectures and dense retrieval techniques that perform ranking directly.
There are two themes that pervade our survey: techniques for handling long
documents, beyond typical sentence-by-sentence processing in NLP, and
techniques for addressing the tradeoff between effectiveness (i.e., result
quality) and efficiency (e.g., query latency, model and index size). Although
transformer architectures and pretraining techniques are recent innovations,
many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of
pretrained transformers for text ranking, this survey also attempts to
prognosticate where the field is heading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1"&gt;Rodrigo Nogueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1"&gt;Andrew Yates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08614</id>
        <link href="http://arxiv.org/abs/2108.08614"/>
        <updated>2021-08-20T01:53:49.380Z</updated>
        <summary type="html"><![CDATA[Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but barely utilize semantic
data and knowledge. This paper presents the first QA system that can seamlessly
operate over RDF datasets and text corpora, or both together, in a unified
framework. Our method, called UNIQORN, builds a context graph on the fly, by
retrieving question-relevant triples from the RDF data and/or the text corpus,
where the latter case is handled by automatic information extraction. The
resulting graph is typically rich but highly noisy. UNIQORN copes with this
input by advanced graph algorithms for Group Steiner Trees, that identify the
best answer candidates in the context graph. Experimental results on several
benchmarks of complex questions with multiple entities and relations, show that
UNIQORN, an unsupervised method with only five parameters, produces results
comparable to the state-of-the-art on KGs, text corpora, and heterogeneous
sources. The graph-based methodology provides user-interpretable evidence for
the complete answering process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1"&gt;Soumajit Pramanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1"&gt;Jesujoba Alabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1"&gt;Gerhard Weikum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture-Based Correction for Position and Trust Bias in Counterfactual Learning to Rank. (arXiv:2108.08538v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08538</id>
        <link href="http://arxiv.org/abs/2108.08538"/>
        <updated>2021-08-20T01:53:49.360Z</updated>
        <summary type="html"><![CDATA[In counterfactual learning to rank (CLTR) user interactions are used as a
source of supervision. Since user interactions come with bias, an important
focus of research in this field lies in developing methods to correct for the
bias of interactions. Inverse propensity scoring (IPS) is a popular method
suitable for correcting position bias. Affine correction (AC) is a
generalization of IPS that corrects for position bias and trust bias. IPS and
AC provably remove bias, conditioned on an accurate estimation of the bias
parameters. Estimating the bias parameters, in turn, requires an accurate
estimation of the relevance probabilities. This cyclic dependency introduces
practical limitations in terms of sensitivity, convergence and efficiency.

We propose a new correction method for position and trust bias in CLTR in
which, unlike the existing methods, the correction does not rely on relevance
estimation. Our proposed method, mixture-based correction (MBC), is based on
the assumption that the distribution of the CTRs over the items being ranked is
a mixture of two distributions: the distribution of CTRs for relevant items and
the distribution of CTRs for non-relevant items. We prove that our method is
unbiased. The validity of our proof is not conditioned on accurate bias
parameter estimation. Our experiments show that MBC, when used in different
bias settings and accompanied by different LTR algorithms, outperforms AC, the
state-of-the-art method for correcting position and trust bias, in some
settings, while performing on par in other settings. Furthermore, MBC is orders
of magnitude more efficient than AC in terms of the training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vardasbi_A/0/1/0/all/0/1"&gt;Ali Vardasbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Ilya Markov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08468</id>
        <link href="http://arxiv.org/abs/2108.08468"/>
        <updated>2021-08-20T01:53:49.335Z</updated>
        <summary type="html"><![CDATA[We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student's performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Chen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tony Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hanqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiwei Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08597</id>
        <link href="http://arxiv.org/abs/2108.08597"/>
        <updated>2021-08-20T01:53:49.296Z</updated>
        <summary type="html"><![CDATA[Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique is to apply named entity disambiguation (NED)
systems to the question, and retrieve KB facts for the disambiguated entities.
This work presents ECQA, an efficient method that prunes irrelevant parts of
the search space using KB-aware signals. ECQA is based on top-k query
processing over score-ordered lists of KB items that combine signals about
lexical matching, relevance to the question, coherence among candidate items,
and connectivity in the KB graph. Experiments with two recent QA benchmarks
demonstrate the superiority of ECQA over state-of-the-art baselines with
respect to answer presence, size of the search space, and runtimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1"&gt;Philipp Christmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1"&gt;Gerhard Weikum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion. (arXiv:2108.08513v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08513</id>
        <link href="http://arxiv.org/abs/2108.08513"/>
        <updated>2021-08-20T01:53:49.239Z</updated>
        <summary type="html"><![CDATA[BERT-based information retrieval models are expensive, in both time (query
latency) and computational resources (energy, hardware cost), making many of
these models impractical especially under resource constraints. The reliance on
a query encoder that only performs tokenization and on the pre-processing of
passage representations at indexing, has allowed the recently proposed TILDE
method to overcome the high query latency issue typical of BERT-based models.
This however is at the expense of a lower effectiveness compared to other
BERT-based re-rankers and dense retrievers. In addition, the original TILDE
method is characterised by indexes with a very high memory footprint, as it
expands each passage into the size of the BERT vocabulary. In this paper, we
propose TILDEv2, a new model that stems from the original TILDE but that
addresses its limitations. TILDEv2 relies on contextualized exact term matching
with expanded passages. This requires to only store in the index the score of
tokens that appear in the expanded passages (rather than all the vocabulary),
thus producing indexes that are 99% smaller than those of TILDE. This matching
mechanism also improves ranking effectiveness by 24%, without adding to the
query latency. This makes TILDEv2 the state-of-the-art passage re-ranking
method for CPU-only environments, capable of maintaining query latency below
100ms on commodity hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1"&gt;Shengyao Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1"&gt;Guido Zuccon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teachers Do More Than Teach: Compressing Image-to-Image Models. (arXiv:2103.03467v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03467</id>
        <link href="http://arxiv.org/abs/2103.03467"/>
        <updated>2021-08-19T01:35:03.930Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have achieved huge success in
generating high-fidelity images, however, they suffer from low efficiency due
to tremendous computational cost and bulky memory usage. Recent efforts on
compression GANs show noticeable progress in obtaining smaller generators by
sacrificing image quality or involving a time-consuming searching process. In
this work, we aim to address these issues by introducing a teacher network that
provides a search space in which efficient network architectures can be found,
in addition to performing knowledge distillation. First, we revisit the search
space of generative models, introducing an inception-based residual block into
generators. Second, to achieve target computation cost, we propose a one-step
pruning algorithm that searches a student architecture from the teacher model
and substantially reduces searching cost. It requires no l1 sparsity
regularization and its associated hyper-parameters, simplifying the training
procedure. Finally, we propose to distill knowledge through maximizing feature
similarity between teacher and student via an index named Global Kernel
Alignment (GKA). Our compressed networks achieve similar or even better image
fidelity (FID, mIoU) than the original models with much-reduced computational
cost, e.g., MACs. Code will be released at
https://github.com/snap-research/CAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1"&gt;Oliver J. Woodford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiazhuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1"&gt;Sergey Tulyakov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03761</id>
        <link href="http://arxiv.org/abs/2105.03761"/>
        <updated>2021-08-19T01:35:03.923Z</updated>
        <summary type="html"><![CDATA[Recently, there has been an increasing number of efforts to introduce models
capable of generating natural language explanations (NLEs) for their
predictions on vision-language (VL) tasks. Such models are appealing, because
they can provide human-friendly and comprehensive explanations. However, there
is a lack of comparison between existing methods, which is due to a lack of
re-usable evaluation frameworks and a scarcity of datasets. In this work, we
introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable
vision-language tasks that establishes a unified evaluation framework and
provides the first comprehensive comparison of existing approaches that
generate NLEs for VL tasks. It spans four models and three datasets and both
automatic metrics and human evaluation are used to assess model-generated
explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs
(over 430k instances). We also propose a new model that combines UNITER, which
learns joint embeddings of images and text, and GPT-2, a pre-trained language
model that is well-suited for text generation. It surpasses the previous state
of the art by a large margin across all datasets. Code and data are available
here: https://github.com/maximek3/e-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1"&gt;Maxime Kayser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1"&gt;Leonard Salewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1"&gt;Cornelius Emde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1"&gt;Virginie Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00274</id>
        <link href="http://arxiv.org/abs/2108.00274"/>
        <updated>2021-08-19T01:35:03.916Z</updated>
        <summary type="html"><![CDATA[3D ultrasound (US) is widely used for its rich diagnostic information.
However, it is criticized for its limited field of view. 3D freehand US
reconstruction is promising in addressing the problem by providing broad range
and freeform scan. The existing deep learning based methods only focus on the
basic cases of skill sequences, and the model relies on the training data
heavily. The sequences in real clinical practice are a mix of diverse skills
and have complex scanning paths. Besides, deep models should adapt themselves
to the testing cases with prior knowledge for better robustness, rather than
only fit to the training cases. In this paper, we propose a novel approach to
sensorless freehand 3D US reconstruction considering the complex skill
sequences. Our contribution is three-fold. First, we advance a novel online
learning framework by designing a differentiable reconstruction algorithm. It
realizes an end-to-end optimization from section sequences to the reconstructed
volume. Second, a self-supervised learning method is developed to explore the
context information that reconstructed by the testing data itself, promoting
the perception of the model. Third, inspired by the effectiveness of shape
prior, we also introduce adversarial training to strengthen the learning of
anatomical shape prior in the reconstructed volume. By mining the context and
structural cues of the testing data, our online learning methods can drive the
model to handle complex skill sequences. Experimental results on developmental
dysplasia of the hip US and fetal US datasets show that, our proposed method
can outperform the start-of-the-art methods regarding the shift errors and path
similarities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mingyuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoqiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xindi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HuMoR: 3D Human Motion Model for Robust Pose Estimation. (arXiv:2105.04668v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04668</id>
        <link href="http://arxiv.org/abs/2105.04668"/>
        <updated>2021-08-19T01:35:03.909Z</updated>
        <summary type="html"><![CDATA[We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal
pose and shape. Though substantial progress has been made in estimating 3D
human motion and shape from dynamic observations, recovering plausible pose
sequences in the presence of noise and occlusions remains a challenge. For this
purpose, we propose an expressive generative model in the form of a conditional
variational autoencoder, which learns a distribution of the change in pose at
each step of a motion sequence. Furthermore, we introduce a flexible
optimization-based approach that leverages HuMoR as a motion prior to robustly
estimate plausible pose and shape from ambiguous observations. Through
extensive evaluations, we demonstrate that our model generalizes to diverse
motions and body shapes after training on a large motion capture dataset, and
enables motion reconstruction from multiple input modalities including 3D
keypoints and RGB(-D) videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1"&gt;Davis Rempe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1"&gt;Tolga Birdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1"&gt;Aaron Hertzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1"&gt;Srinath Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas J. Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis. (arXiv:2105.14211v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14211</id>
        <link href="http://arxiv.org/abs/2105.14211"/>
        <updated>2021-08-19T01:35:03.900Z</updated>
        <summary type="html"><![CDATA[Conditional image synthesis aims to create an image according to some
multi-modal guidance in the forms of textual descriptions, reference images,
and image blocks to preserve, as well as their combinations. In this paper,
instead of investigating these control signals separately, we propose a new
two-stage architecture, UFC-BERT, to unify any number of multi-modal controls.
In UFC-BERT, both the diverse control signals and the synthesized image are
uniformly represented as a sequence of discrete tokens to be processed by
Transformer. Different from existing two-stage autoregressive approaches such
as DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the
second stage to enhance the holistic consistency of the synthesized image, to
support preserving specified image blocks, and to improve the synthesis speed.
Further, we design a progressive algorithm that iteratively improves the
non-autoregressively generated image, with the help of two estimators developed
for evaluating the compliance with the controls and evaluating the fidelity of
the synthesized image, respectively. Extensive experiments on a newly collected
large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal
CelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply
with flexible multi-modal controls.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhikang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.05067</id>
        <link href="http://arxiv.org/abs/2108.05067"/>
        <updated>2021-08-19T01:35:03.893Z</updated>
        <summary type="html"><![CDATA[Medical imaging technologies, including computed tomography (CT) or chest
X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.
Since manual report writing is usually too time-consuming, a more intelligent
auxiliary medical system that could generate medical reports automatically and
immediately is urgently needed. In this article, we propose to use the medical
visual language BERT (Medical-VLBERT) model to identify the abnormality on the
COVID-19 scans and generate the medical report automatically based on the
detected lesion regions. To produce more accurate medical reports and minimize
the visual-and-linguistic differences, this model adopts an alternate learning
strategy with two procedures that are knowledge pretraining and transferring.
To be more precise, the knowledge pretraining procedure is to memorize the
knowledge from medical texts, while the transferring procedure is to utilize
the acquired knowledge for professional medical sentences generations through
observations of medical images. In practice, for automatic medical report
generation on the COVID-19 cases, we constructed a dataset of 368 medical
findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital
of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun
Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of
the COVID-19 training samples, our model was first trained on the large-scale
Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for
further fine-tuning. The experimental results showed that Medical-VLBERT
achieved state-of-the-art performances on terminology prediction and report
generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The
Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yinghong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fuyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuixing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICE: Inter-instance Contrastive Encoding for Unsupervised Person Re-identification. (arXiv:2103.16364v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16364</id>
        <link href="http://arxiv.org/abs/2103.16364"/>
        <updated>2021-08-19T01:35:03.871Z</updated>
        <summary type="html"><![CDATA[Unsupervised person re-identification (ReID) aims at learning discriminative
identity features without annotations. Recently, self-supervised contrastive
learning has gained increasing attention for its effectiveness in unsupervised
representation learning. The main idea of instance contrastive learning is to
match a same instance in different augmented views. However, the relationship
between different instances has not been fully explored in previous contrastive
methods, especially for instance-level contrastive loss. To address this issue,
we propose Inter-instance Contrastive Encoding (ICE) that leverages
inter-instance pairwise similarity scores to boost previous class-level
contrastive ReID methods. We first use pairwise similarity ranking as one-hot
hard pseudo labels for hard instance contrast, which aims at reducing
intra-class variance. Then, we use similarity scores as soft pseudo labels to
enhance the consistency between augmented and original views, which makes our
model more robust to augmentation perturbations. Experiments on several
large-scale person ReID datasets validate the effectiveness of our proposed
unsupervised method ICE, which is competitive with even supervised methods.
Code is made available at https://github.com/chenhao2345/ICE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lagadec_B/0/1/0/all/0/1"&gt;Benoit Lagadec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1"&gt;Francois Bremond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TeliNet: Classifying CT scan images for COVID-19 diagnosis. (arXiv:2107.04930v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04930</id>
        <link href="http://arxiv.org/abs/2107.04930"/>
        <updated>2021-08-19T01:35:03.843Z</updated>
        <summary type="html"><![CDATA[COVID-19 has led to hundreds of millions of cases and millions of deaths
worldwide since its onset. The fight against this pandemic is on-going on
multiple fronts. While vaccinations are picking up speed, there are still
billions of unvaccinated people. In this fight against the virus, diagnosis of
the disease and isolation of the patients to prevent any spread play a huge
role. Machine Learning approaches have assisted in the diagnosis of COVID-19
cases by analyzing chest X-rays and CT-scan images of patients. To push
algorithm development and research in this direction of radiological diagnosis,
a challenge to classify CT-scan series was organized in conjunction with ICCV,
2021. In this research we present a simple and shallow Convolutional Neural
Network based approach, TeliNet, to classify these CT-scan images of COVID-19
patients presented as part of this competition. Our results outperform the F1
`macro' score of the competition benchmark and VGGNet approaches. Our proposed
solution is also more lightweight in comparison to the other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1"&gt;Mohammad Nayeem Teli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-08-19T01:35:03.835Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification and reconstruction of spatially overlapping phase images using diffractive optical networks. (arXiv:2108.07977v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2108.07977</id>
        <link href="http://arxiv.org/abs/2108.07977"/>
        <updated>2021-08-19T01:35:03.816Z</updated>
        <summary type="html"><![CDATA[Diffractive optical networks unify wave optics and deep learning to
all-optically compute a given machine learning or computational imaging task as
the light propagates from the input to the output plane. Here, we report the
design of diffractive optical networks for the classification and
reconstruction of spatially overlapping, phase-encoded objects. When two
different phase-only objects spatially overlap, the individual object functions
are perturbed since their phase patterns are summed up. The retrieval of the
underlying phase images from solely the overlapping phase distribution presents
a challenging problem, the solution of which is generally not unique. We show
that through a task-specific training process, passive diffractive networks
composed of successive transmissive layers can all-optically and simultaneously
classify two different randomly-selected, spatially overlapping phase images at
the input. After trained with ~550 million unique combinations of phase-encoded
handwritten digits from the MNIST dataset, our blind testing results reveal
that the diffractive network achieves an accuracy of >85.8% for all-optical
classification of two overlapping phase images of new handwritten digits. In
addition to all-optical classification of overlapping phase objects, we also
demonstrate the reconstruction of these phase images based on a shallow
electronic neural network that uses the highly compressed output of the
diffractive network as its input (with e.g., ~20-65 times less number of
pixels) to rapidly reconstruct both of the phase images, despite their spatial
overlap and related phase ambiguity. The presented phase image classification
and reconstruction framework might find applications in e.g., computational
imaging, microscopy and quantitative phase imaging fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1"&gt;Deniz Mengu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Veli_M/0/1/0/all/0/1"&gt;Muhammed Veli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rivenson_Y/0/1/0/all/0/1"&gt;Yair Rivenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1"&gt;Aydogan Ozcan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Urban Driving by Imitating a Reinforcement Learning Coach. (arXiv:2108.08265v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08265</id>
        <link href="http://arxiv.org/abs/2108.08265"/>
        <updated>2021-08-19T01:35:03.800Z</updated>
        <summary type="html"><![CDATA[End-to-end approaches to autonomous driving commonly rely on expert
demonstrations. Although humans are good drivers, they are not good coaches for
end-to-end algorithms that demand dense on-policy supervision. On the contrary,
automated experts that leverage privileged information can efficiently generate
large scale on-policy and off-policy demonstrations. However, existing
automated experts for urban driving make heavy use of hand-crafted rules and
perform suboptimally even on driving simulators, where ground-truth information
is available. To address these issues, we train a reinforcement learning expert
that maps bird's-eye view images to continuous low-level actions. While setting
a new performance upper-bound on CARLA, our expert is also a better coach that
provides informative supervision signals for imitation learning agents to learn
from. Supervised by our reinforcement learning coach, a baseline end-to-end
agent with monocular camera-input achieves expert-level performance. Our
end-to-end agent achieves a 78% success rate while generalizing to a new town
and new weather on the NoCrash-dense benchmark and state-of-the-art performance
on the more challenging CARLA LeaderBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhejun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1"&gt;Alexander Liniger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral Denoising Using Unsupervised Disentangled Spatio-Spectral Deep Priors. (arXiv:2102.12310v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12310</id>
        <link href="http://arxiv.org/abs/2102.12310"/>
        <updated>2021-08-19T01:35:03.793Z</updated>
        <summary type="html"><![CDATA[Image denoising is often empowered by accurate prior information. In recent
years, data-driven neural network priors have shown promising performance for
RGB natural image denoising. Compared to classic handcrafted priors (e.g.,
sparsity and total variation), the "deep priors" are learned using a large
number of training samples -- which can accurately model the complex image
generating process. However, data-driven priors are hard to acquire for
hyperspectral images (HSIs) due to the lack of training data. A remedy is to
use the so-called unsupervised deep image prior (DIP). Under the unsupervised
DIP framework, it is hypothesized and empirically demonstrated that proper
neural network structures are reasonable priors of certain types of images, and
the network weights can be learned without training data. Nonetheless, the most
effective unsupervised DIP structures were proposed for natural images instead
of HSIs. The performance of unsupervised DIP-based HSI denoising is limited by
a couple of serious challenges, namely, network structure design and network
complexity. This work puts forth an unsupervised DIP framework that is based on
the classic spatio-spectral decomposition of HSIs. Utilizing the so-called
linear mixture model of HSIs, two types of unsupervised DIPs, i.e., U-Net-like
network and fully-connected networks, are employed to model the abundance maps
and endmembers contained in the HSIs, respectively. This way, empirically
validated unsupervised DIP structures for natural images can be easily
incorporated for HSI denoising. Besides, the decomposition also substantially
reduces network complexity. An efficient alternating optimization algorithm is
proposed to handle the formulated denoising problem. Semi-real and real data
experiments are employed to showcase the effectiveness of the proposed
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yu-Chun Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xi-Le Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian-Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yu-Bang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions. (arXiv:2105.07404v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07404</id>
        <link href="http://arxiv.org/abs/2105.07404"/>
        <updated>2021-08-19T01:35:03.774Z</updated>
        <summary type="html"><![CDATA[Spatio-temporal action detection is an important and challenging problem in
video understanding. The existing action detection benchmarks are limited in
aspects of small numbers of instances in a trimmed video or low-level atomic
actions. This paper aims to present a new multi-person dataset of
spatio-temporal localized sports actions, coined as MultiSports. We first
analyze the important ingredients of constructing a realistic and challenging
dataset for spatio-temporal action detection by proposing three criteria: (1)
multi-person scenes and motion dependent identification, (2) with well-defined
boundaries, (3) relatively fine-grained classes of high complexity. Based on
these guide-lines, we build the dataset of MultiSports v1.0 by selecting 4
sports classes, collecting 3200 video clips, and annotating 37701 action
instances with 902k bounding boxes. Our datasets are characterized with
important properties of high diversity, dense annotation, and high quality. Our
Multi-Sports, with its realistic setting and detailed annotations, exposes the
intrinsic challenges of spatio-temporal action detection. To benchmark this, we
adapt several baseline methods to our dataset and give an in-depth analysis on
the action detection results in our dataset. We hope our MultiSports can serve
as a standard benchmark for spatio-temporal action detection in the future. Our
dataset website is at https://deeperaction.github.io/multisports/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Runyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhenzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gangshan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. (arXiv:2107.11443v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11443</id>
        <link href="http://arxiv.org/abs/2107.11443"/>
        <updated>2021-08-19T01:35:03.761Z</updated>
        <summary type="html"><![CDATA[Video activity localisation has recently attained increasing attention due to
its practical values in automatically localising the most salient visual
segments corresponding to their language descriptions (sentences) from
untrimmed and unstructured videos. For supervised model training, a temporal
annotation of both the start and end time index of each video segment for a
sentence (a video moment) must be given. This is not only very expensive but
also sensitive to ambiguity and subjective annotation bias, a much harder task
than image labelling. In this work, we develop a more accurate
weakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)
in video moment proposal generation and matching when only a paragraph
description of activities without per-sentence temporal annotation is
available. Specifically, we explore two cross-sentence relational constraints:
(1) Temporal ordering and (2) semantic consistency among sentences in a
paragraph description of video activities. Existing weakly-supervised
techniques only consider within-sentence video segment correlations in training
without considering cross-sentence paragraph context. This can mislead due to
ambiguous expressions of individual sentences with visually indiscriminate
video moment proposals in isolation. Experiments on two publicly available
activity localisation datasets show the advantages of our approach over the
state-of-the-art weakly supervised methods, especially so when the video
activity descriptions become more complex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiabo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shaogang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hailin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space. (arXiv:2108.08244v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08244</id>
        <link href="http://arxiv.org/abs/2108.08244"/>
        <updated>2021-08-19T01:35:03.692Z</updated>
        <summary type="html"><![CDATA[Increased drone proliferation in civilian and professional settings has
created new threat vectors for airports and national infrastructures. The
economic damage for a single major airport from drone incursions is estimated
to be millions per day. Due to the lack of diverse drone training data,
accurate training of deep learning detection algorithms under scarce data is an
open challenge. Existing methods largely rely on collecting diverse and
comprehensive experimental drone footage data, artificially induced data
augmentation, transfer and meta-learning, as well as physics-informed learning.
However, these methods cannot guarantee capturing diverse drone designs and
fully understanding the deep feature space of drones. Here, we show how
understanding the general distribution of the drone data via a Generative
Adversarial Network (GAN) and explaining the missing features using Topological
Data Analysis (TDA) - can allow us to acquire missing data to achieve rapid and
more accurate learning. We demonstrate our results on a drone image dataset,
which contains both real drone images as well as simulated images from
computer-aided design. When compared to random data collection (usual practice
- discriminator accuracy of 94.67\% after 200 epochs), our proposed GAN-TDA
informed data collection method offers a significant 4\% improvement (99.42\%
after 200 epochs). We believe that this approach of exploiting general data
distribution knowledge form neural networks can be applied to a wide range of
scarce data open challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Schyler C. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhuangkun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsourdos_A/0/1/0/all/0/1"&gt;Antonios Tsourdos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Weisi Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rendering and Tracking the Directional TSDF: Modeling Surface Orientation for Coherent Maps. (arXiv:2108.08115v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08115</id>
        <link href="http://arxiv.org/abs/2108.08115"/>
        <updated>2021-08-19T01:35:03.685Z</updated>
        <summary type="html"><![CDATA[Dense real-time tracking and mapping from RGB-D images is an important tool
for many robotic applications, such as navigation or grasping. The recently
presented Directional Truncated Signed Distance Function (DTSDF) is an
augmentation of the regular TSDF and shows potential for more coherent maps and
improved tracking performance. In this work, we present methods for rendering
depth- and color maps from the DTSDF, making it a true drop-in replacement for
the regular TSDF in established trackers. We evaluate and show, that our method
increases re-usability of mapped scenes. Furthermore, we add color integration
which notably improves color-correctness at adjacent surfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Splietker_M/0/1/0/all/0/1"&gt;Malte Splietker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pixel-Perfect Structure-from-Motion with Featuremetric Refinement. (arXiv:2108.08291v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08291</id>
        <link href="http://arxiv.org/abs/2108.08291"/>
        <updated>2021-08-19T01:35:03.613Z</updated>
        <summary type="html"><![CDATA[Finding local features that are repeatable across multiple views is a
cornerstone of sparse 3D reconstruction. The classical image matching paradigm
detects keypoints per-image once and for all, which can yield poorly-localized
features and propagate large errors to the final geometry. In this paper, we
refine two key steps of structure-from-motion by a direct alignment of
low-level image information from multiple views: we first adjust the initial
keypoint locations prior to any geometric estimation, and subsequently refine
points and camera poses as a post-processing. This refinement is robust to
large detection noise and appearance changes, as it optimizes a featuremetric
error based on dense features predicted by a neural network. This significantly
improves the accuracy of camera poses and scene geometry for a wide range of
keypoint detectors, challenging viewing conditions, and off-the-shelf deep
features. Our system easily scales to large image collections, enabling
pixel-perfect crowd-sourced localization at scale. Our code is publicly
available at https://github.com/cvg/pixel-perfect-sfm as an add-on to the
popular SfM software COLMAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindenberger_P/0/1/0/all/0/1"&gt;Philipp Lindenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarlin_P/0/1/0/all/0/1"&gt;Paul-Edouard Sarlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1"&gt;Viktor Larsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsely Activated Networks. (arXiv:1907.06592v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.06592</id>
        <link href="http://arxiv.org/abs/1907.06592"/>
        <updated>2021-08-19T01:35:03.597Z</updated>
        <summary type="html"><![CDATA[Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1"&gt;Paschalis Bizopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1"&gt;Dimitrios Koutsouris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14072</id>
        <link href="http://arxiv.org/abs/2107.14072"/>
        <updated>2021-08-19T01:35:03.579Z</updated>
        <summary type="html"><![CDATA[A core objective of the TERRA-REF project was to generate an open-access
reference dataset for the evaluation of sensing technologies to study plants
under field conditions. The TERRA-REF program deployed a suite of
high-resolution, cutting edge technology sensors on a gantry system with the
aim of scanning 1 hectare (10$^4$) at around 1 mm$^2$ spatial resolution
multiple times per week. The system contains co-located sensors including a
stereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D
structure, and two hyperspectral cameras covering wavelengths of 300-2500nm.
This sensor data is provided alongside over sixty types of traditional plant
phenotype measurements that can be used to train new machine learning models.
Associated weather and environmental measurements, information about agronomic
management and experimental design, and the genomic sequences of hundreds of
plant varieties have been collected and are available alongside the sensor and
plant phenotype data.

Over the course of four years and ten growing seasons, the TERRA-REF system
generated over 1 PB of sensor data and almost 45 million files. The subset that
has been released to the public domain accounts for two seasons and about half
of the total data volume. This provides an unprecedented opportunity for
investigations far beyond the core biological scope of the project.

The focus of this paper is to provide the Computer Vision and Machine
Learning communities an overview of the available data and some potential
applications of this one of a kind data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1"&gt;David LeBauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1"&gt;Max Burnette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1"&gt;Noah Fahlgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1"&gt;Rob Kooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1"&gt;Kenton McHenry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05528</id>
        <link href="http://arxiv.org/abs/2105.05528"/>
        <updated>2021-08-19T01:35:03.572Z</updated>
        <summary type="html"><![CDATA[The use of gait for person identification has important advantages such as
being non-invasive, unobtrusive, not requiring cooperation and being less
likely to be obscured compared to other biometrics. Existing methods for gait
recognition require cooperative gait scenarios, in which a single person is
walking multiple times in a straight line in front of a camera. We aim to
address the challenges of real-world scenarios in which camera feeds capture
multiple people, who in most cases pass in front of the camera only once. We
address privacy concerns by using only motion information of walking
individuals, with no identifiable appearance-based information. As such, we
propose a novel weakly supervised learning framework, WildGait, which consists
of training a Spatio-Temporal Graph Convolutional Network on a large number of
automatically annotated skeleton sequences obtained from raw, real-world,
surveillance streams to learn useful gait signatures. We collected the training
data and compiled the largest dataset of walking skeletons called Uncooperative
Wild Gait, containing over 38k tracklets of anonymized walking 2D skeletons. We
release the dataset for public use. Our results show that, with fine-tuning, we
surpass the current state-of-the-art pose-based gait recognition solutions. Our
proposed method is reliable in training gait recognition methods in
unconstrained environments, especially in settings with scarce amounts of
annotated data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1"&gt;Adrian Cosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1"&gt;Emilian Radoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Human Trajectory Prediction in Raw Videos. (arXiv:2108.08259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08259</id>
        <link href="http://arxiv.org/abs/2108.08259"/>
        <updated>2021-08-19T01:35:03.526Z</updated>
        <summary type="html"><![CDATA[Human trajectory prediction has received increased attention lately due to
its importance in applications such as autonomous vehicles and indoor robots.
However, most existing methods make predictions based on human-labeled
trajectories and ignore the errors and noises in detection and tracking. In
this paper, we study the problem of human trajectory forecasting in raw videos,
and show that the prediction accuracy can be severely affected by various types
of tracking errors. Accordingly, we propose a simple yet effective strategy to
correct the tracking failures by enforcing prediction consistency over time.
The proposed "re-tracking" algorithm can be applied to any existing tracking
and prediction pipelines. Experiments on public benchmark datasets demonstrate
that the proposed method can improve both tracking and prediction performance
in challenging real-world scenarios. The code and data are available at
https://git.io/retracking-prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rui Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zihan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neuromorphic Computing for Content-based Image Retrieval. (arXiv:2008.01380v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01380</id>
        <link href="http://arxiv.org/abs/2008.01380"/>
        <updated>2021-08-19T01:35:03.498Z</updated>
        <summary type="html"><![CDATA[Neuromorphic computing mimics the neural activity of the brain through
emulating spiking neural networks. In numerous machine learning tasks,
neuromorphic chips are expected to provide superior solutions in terms of cost
and power efficiency. Here, we explore the application of Loihi, a neuromorphic
computing chip developed by Intel, for the computer vision task of image
retrieval. We evaluated the functionalities and the performance metrics that
are critical in content-based visual search and recommender systems using
deep-learning embeddings. Our results show that the neuromorphic solution is
about 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and
12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a
lightweight convolutional neural network without batching while maintaining the
same level of matching accuracy. The study validates the potential of
neuromorphic computing in low-power image retrieval, as a complementary
paradigm to the existing von Neumann architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Te-Yuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1"&gt;Ata Mahjoubfar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prusinski_D/0/1/0/all/0/1"&gt;Daniel Prusinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_L/0/1/0/all/0/1"&gt;Luis Stevens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02672</id>
        <link href="http://arxiv.org/abs/2101.02672"/>
        <updated>2021-08-19T01:35:03.472Z</updated>
        <summary type="html"><![CDATA[Existing point-cloud based 3D object detectors use convolution-like operators
to process information in a local neighbourhood with fixed-weight kernels and
aggregate global context hierarchically. However, non-local neural networks and
self-attention for 2D vision have shown that explicitly modeling long-range
interactions can lead to more robust and competitive models. In this paper, we
propose two variants of self-attention for contextual modeling in 3D object
detection by augmenting convolutional features with self-attention features. We
first incorporate the pairwise self-attention mechanism into the current
state-of-the-art BEV, voxel and point-based detectors and show consistent
improvement over strong baseline models of up to 1.5 3D AP while simultaneously
reducing their parameter footprint and computational cost by 15-80% and 30-50%,
respectively, on the KITTI validation set. We next propose a self-attention
variant that samples a subset of the most representative features by learning
deformations over randomly sampled locations. This not only allows us to scale
explicit global contextual modeling to larger point-clouds, but also leads to
more discriminative and informative feature descriptors. Our method can be
flexibly applied to most state-of-the-art detectors with increased accuracy and
parameter and compute efficiency. We show our proposed method improves 3D
object detection performance on KITTI, nuScenes and Waymo Open datasets. Code
is available at https://github.com/AutoVision-cloud/SA-Det3D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Prarthana Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chengjie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1"&gt;Krzysztof Czarnecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14956</id>
        <link href="http://arxiv.org/abs/2011.14956"/>
        <updated>2021-08-19T01:35:03.464Z</updated>
        <summary type="html"><![CDATA[Learning from noisy labels is an important concern because of the lack of
accurate ground-truth labels in plenty of real-world scenarios. In practice,
various approaches for this concern first make some corrections corresponding
to potentially noisy-labeled instances, and then update predictive model with
information of the made corrections. However, in specific areas, such as
medical histopathology whole slide image analysis (MHWSIA), it is often
difficult or even impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. In this paper, we
focus on alleviating these two problems. For the problem 1), we present
one-step abductive multi-target learning (OSAMTL) that imposes a one-step
logical reasoning upon machine learning via a multi-target learning procedure
to constrain the predictions of the learning model to be subject to our prior
knowledge about the true target. For the problem 2), we propose a logical
assessment formula (LAF) that evaluates the logical rationality of the outputs
of an approach by estimating the consistencies between the predictions of the
learning model and the logical facts narrated from the results of the one-step
logical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori
(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable
the machine learning model achieving logically more rational predictions, which
is beyond various state-of-the-art approaches in handling complex noisy labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongquan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yiming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jiayi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhongxi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08217</id>
        <link href="http://arxiv.org/abs/2108.08217"/>
        <updated>2021-08-19T01:35:03.456Z</updated>
        <summary type="html"><![CDATA[With the rise and development of deep learning over the past decade, there
has been a steady momentum of innovation and breakthroughs that convincingly
push the state-of-the-art of cross-modal analytics between vision and language
in multimedia field. Nevertheless, there has not been an open-source codebase
in support of training and deploying numerous neural network models for
cross-modal analytics in a unified and modular fashion. In this work, we
propose X-modaler -- a versatile and high-performance codebase that
encapsulates the state-of-the-art cross-modal analytics into several
general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,
decoder, and decode strategy). Each stage is empowered with the functionality
that covers a series of modules widely adopted in state-of-the-arts and allows
seamless switching in between. This way naturally enables a flexible
implementation of state-of-the-art algorithms for image captioning, video
captioning, and vision-language pre-training, aiming to facilitate the rapid
development of research community. Meanwhile, since the effective modular
designs in several stages (e.g., cross-modal interaction) are shared across
different vision-language tasks, X-modaler can be simply extended to power
startup prototypes for other tasks in cross-modal analytics, including visual
question answering, visual commonsense reasoning, and cross-modal retrieval.
X-modaler is an Apache-licensed codebase, and its source codes, sample projects
and pre-trained models are available on-line:
https://github.com/YehLi/xmodaler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yehao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval. (arXiv:2103.15049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15049</id>
        <link href="http://arxiv.org/abs/2103.15049"/>
        <updated>2021-08-19T01:35:03.450Z</updated>
        <summary type="html"><![CDATA[Video-Text Retrieval has been a hot research topic with the growth of
multimedia data on the internet. Transformer for video-text learning has
attracted increasing attention due to its promising performance. However,
existing cross-modal transformer approaches typically suffer from two major
limitations: 1) Exploitation of the transformer architecture where different
layers have different feature characteristics is limited; 2) End-to-end
training mechanism limits negative sample interactions in a mini-batch. In this
paper, we propose a novel approach named Hierarchical Transformer (HiT) for
video-text retrieval. HiT performs Hierarchical Cross-modal Contrastive
Matching in both feature-level and semantic-level, achieving multi-view and
comprehensive retrieval results. Moreover, inspired by MoCo, we propose
Momentum Cross-modal Contrast for cross-modal learning to enable large-scale
negative sample interactions on-the-fly, which contributes to the generation of
more precise and discriminative representations. Experimental results on the
three major Video-Text Retrieval benchmark datasets demonstrate the advantages
of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Song Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Haoqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shengsheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiru Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenkui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongyuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03788</id>
        <link href="http://arxiv.org/abs/2108.03788"/>
        <updated>2021-08-19T01:35:03.433Z</updated>
        <summary type="html"><![CDATA[This paper presents a three-tier modality alignment approach to learning
text-image joint embedding, coined as JEMA, for cross-modal retrieval of
cooking recipes and food images. The first tier improves recipe text embedding
by optimizing the LSTM networks with term extraction and ranking enhanced
sequence patterns, and optimizes the image embedding by combining the
ResNeXt-101 image encoder with the category embedding using wideResNet-50 with
word2vec. The second tier modality alignment optimizes the textual-visual joint
embedding loss function using a double batch-hard triplet loss with soft-margin
optimization. The third modality alignment incorporates two types of
cross-modality alignments as the auxiliary loss regularizations to further
reduce the alignment errors in the joint learning of the two modality-specific
embedding functions. The category-based cross-modal alignment aims to align the
image category with the recipe category as a loss regularization to the joint
embedding. The cross-modal discriminator-based alignment aims to add the
visual-textual embedding distribution alignment to further regularize the joint
embedding loss. Extensive experiments with the one-million recipes benchmark
dataset Recipe1M demonstrate that the proposed JEMA approach outperforms the
state-of-the-art cross-modal embedding methods for both image-to-recipe and
recipe-to-image retrievals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation. (arXiv:2108.08202v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08202</id>
        <link href="http://arxiv.org/abs/2108.08202"/>
        <updated>2021-08-19T01:35:03.426Z</updated>
        <summary type="html"><![CDATA[Internet video delivery has undergone a tremendous explosion of growth over
the past few years. However, the quality of video delivery system greatly
depends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to
improve the quality of video delivery recently. These methods divide a video
into chunks, and stream LR video chunks and corresponding content-aware models
to the client. The client runs the inference of models to super-resolve the LR
chunks. Consequently, a large number of models are streamed in order to deliver
a video. In this paper, we first carefully study the relation between models of
different chunks, then we tactfully design a joint training framework along
with the Content-aware Feature Modulation (CaFM) layer to compress these models
for neural video delivery. {\bf With our method, each video chunk only requires
less than $1\% $ of original parameters to be streamed, achieving even better
SR performance.} We conduct extensive experiments across various SR backbones,
video time length, and scaling factors to demonstrate the advantages of our
method. Besides, our method can be also viewed as a new approach of video
coding. Our primary experiments achieve better video quality compared with the
commercial H.264 and H.265 standard under the same storage cost, showing the
great potential of the proposed method. Code is available
at:\url{https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kaixin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shizun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_E/0/1/0/all/0/1"&gt;Enhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yurong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chuang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Ming Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reparametrization of Multi-Frame Super-Resolution and Denoising. (arXiv:2108.08286v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08286</id>
        <link href="http://arxiv.org/abs/2108.08286"/>
        <updated>2021-08-19T01:35:03.419Z</updated>
        <summary type="html"><![CDATA[We propose a deep reparametrization of the maximum a posteriori formulation
commonly employed in multi-frame image restoration tasks. Our approach is
derived by introducing a learned error metric and a latent representation of
the target image, which transforms the MAP objective to a deep feature space.
The deep reparametrization allows us to directly model the image formation
process in the latent space, and to integrate learned image priors into the
prediction. Our approach thereby leverages the advantages of deep learning,
while also benefiting from the principled multi-frame fusion provided by the
classical MAP formulation. We validate our approach through comprehensive
experiments on burst denoising and burst super-resolution datasets. Our
approach sets a new state-of-the-art for both tasks, demonstrating the
generality and effectiveness of the proposed formulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhat_G/0/1/0/all/0/1"&gt;Goutam Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMiner: Discovering Interpretable Representations for Mammogram Classification and Explanation. (arXiv:1805.12323v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.12323</id>
        <link href="http://arxiv.org/abs/1805.12323"/>
        <updated>2021-08-19T01:35:03.413Z</updated>
        <summary type="html"><![CDATA[We propose DeepMiner, a framework to discover interpretable representations
in deep neural networks and to build explanations for medical predictions. By
probing convolutional neural networks (CNNs) trained to classify cancer in
mammograms, we show that many individual units in the final convolutional layer
of a CNN respond strongly to diseased tissue concepts specified by the BI-RADS
lexicon. After expert annotation of the interpretable units, our proposed
method is able to generate explanations for CNN mammogram classification that
are consistent with ground truth radiology reports on the Digital Database for
Screening Mammography. We show that DeepMiner not only enables better
understanding of the nuances of CNN classification decisions but also possibly
discovers new visual knowledge relevant to medical diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jimmy Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peck_D/0/1/0/all/0/1"&gt;Diondra Peck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_S/0/1/0/all/0/1"&gt;Scott Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dialani_V/0/1/0/all/0/1"&gt;Vandana Dialani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patterson_G/0/1/0/all/0/1"&gt;Genevieve Patterson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08212</id>
        <link href="http://arxiv.org/abs/2108.08212"/>
        <updated>2021-08-19T01:35:03.404Z</updated>
        <summary type="html"><![CDATA[Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yangdi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1"&gt;Yang Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wenbo He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08236</id>
        <link href="http://arxiv.org/abs/2108.08236"/>
        <updated>2021-08-19T01:35:03.397Z</updated>
        <summary type="html"><![CDATA[Recent advances in trajectory prediction have shown that explicit reasoning
about agents' intent is important to accurately forecast their motion. However,
the current research activities are not directly applicable to intelligent and
safety critical systems. This is mainly because very few public datasets are
available, and they only consider pedestrian-specific intents for a short
temporal horizon from a restricted egocentric view. To this end, we propose
LOKI (LOng term and Key Intentions), a novel large-scale dataset that is
designed to tackle joint trajectory and intention prediction for heterogeneous
traffic agents (pedestrians and vehicles) in an autonomous driving setting. The
LOKI dataset is created to discover several factors that may affect intention,
including i) agent's own will, ii) social interactions, iii) environmental
constraints, and iv) contextual information. We also propose a model that
jointly performs trajectory and intention prediction, showing that recurrently
reasoning about intention can assist with trajectory prediction. We show our
method outperforms state-of-the-art trajectory prediction methods by upto
$27\%$ and also provide a baseline for frame-wise intention estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1"&gt;Harshayu Girase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1"&gt;Haiming Gang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1"&gt;Srikanth Malla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1"&gt;Akira Kanehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1"&gt;Karttikeya Mangalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1"&gt;Chiho Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attention Module for Convolutional Neural Networks. (arXiv:2108.08205v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08205</id>
        <link href="http://arxiv.org/abs/2108.08205"/>
        <updated>2021-08-19T01:35:03.378Z</updated>
        <summary type="html"><![CDATA[Attention mechanism has been regarded as an advanced technique to capture
long-range feature interactions and to boost the representation capability for
convolutional neural networks. However, we found two ignored problems in
current attentional activations-based models: the approximation problem and the
insufficient capacity problem of the attention maps. To solve the two problems
together, we initially propose an attention module for convolutional neural
networks by developing an AW-convolution, where the shape of attention maps
matches that of the weights rather than the activations. Our proposed attention
module is a complementary method to previous attention-based schemes, such as
those that apply the attention mechanism to explore the relationship between
channel-wise and spatial features. Experiments on several datasets for image
classification and object detection tasks show the effectiveness of our
proposed attention module. In particular, our proposed attention module
achieves 1.00% Top-1 accuracy improvement on ImageNet classification over a
ResNet101 baseline and 0.63 COCO-style Average Precision improvement on the
COCO object detection on top of a Faster R-CNN baseline with the backbone of
ResNet101-FPN. When integrating with the previous attentional activations-based
models, our proposed attention module can further increase their Top-1 accuracy
on ImageNet classification by up to 0.57% and COCO-style Average Precision on
the COCO object detection by up to 0.45. Code and pre-trained models will be
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baozhou_Z/0/1/0/all/0/1"&gt;Zhu Baozhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofstee_P/0/1/0/all/0/1"&gt;Peter Hofstee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Ars_Z/0/1/0/all/0/1"&gt;Zaid Al-Ars&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Min-Max affine approximants of convex or concave real valued functions from $\mathbb R^k$, Chebyshev equioscillation and graphics. (arXiv:1812.02302v10 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.02302</id>
        <link href="http://arxiv.org/abs/1812.02302"/>
        <updated>2021-08-19T01:35:03.372Z</updated>
        <summary type="html"><![CDATA[We study Min-Max affine approximants of a continuous convex or concave
function $f:\Delta\subset \mathbb R^k\xrightarrow{} \mathbb R$ where $\Delta$
is a convex compact subset of $\mathbb R^k$. In the case when $\Delta$ is a
simplex we prove that there is a vertical translate of the supporting
hyperplane in $\mathbb R^{k+1}$ of the graph of $f$ at the vertices which is
the unique best affine approximant to $f$ on $\Delta$. For $k=1$, this result
provides an extension of the Chebyshev equioscillation theorem for linear
approximants. Our result has interesting connections to the computer graphics
problem of rapid rendering of projective transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Damelin_S/0/1/0/all/0/1"&gt;Steven B. Damelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ragozin_D/0/1/0/all/0/1"&gt;David L. Ragozin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Werman_M/0/1/0/all/0/1"&gt;Michael Werman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-patch Feature Pyramid Network for Weakly Supervised Object Detection in Optical Remote Sensing Images. (arXiv:2108.08063v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08063</id>
        <link href="http://arxiv.org/abs/2108.08063"/>
        <updated>2021-08-19T01:35:03.365Z</updated>
        <summary type="html"><![CDATA[Object detection is a challenging task in remote sensing because objects only
occupy a few pixels in the images, and the models are required to
simultaneously learn object locations and detection. Even though the
established approaches well perform for the objects of regular sizes, they
achieve weak performance when analyzing small ones or getting stuck in the
local minima (e.g. false object parts). Two possible issues stand in their way.
First, the existing methods struggle to perform stably on the detection of
small objects because of the complicated background. Second, most of the
standard methods used hand-crafted features, and do not work well on the
detection of objects parts of which are missing. We here address the above
issues and propose a new architecture with a multiple patch feature pyramid
network (MPFP-Net). Different from the current models that during training only
pursue the most discriminative patches, in MPFPNet the patches are divided into
class-affiliated subsets, in which the patches are related and based on the
primary loss function, a sequence of smooth loss functions are determined for
the subsets to improve the model for collecting small object parts. To enhance
the feature representation for patch selection, we introduce an effective
method to regularize the residual values and make the fusion transition layers
strictly norm-preserving. The network contains bottom-up and crosswise
connections to fuse the features of different scales to achieve better
accuracy, compared to several state-of-the-art object detection models. Also,
the developed architecture is more efficient than the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1"&gt;Pourya Shamsolmoali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1"&gt;Masoumeh Zareapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research on Gender-related Fingerprint Features. (arXiv:2108.08233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08233</id>
        <link href="http://arxiv.org/abs/2108.08233"/>
        <updated>2021-08-19T01:35:03.358Z</updated>
        <summary type="html"><![CDATA[Fingerprint is an important biological feature of human body, which contains
abundant gender information. At present, the academic research of fingerprint
gender characteristics is generally at the level of understanding, while the
standardization research is quite limited. In this work, we propose a more
robust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid
gender information from fingerprints. By replacing the normal convolution
operations with the atrous convolution in the backbone, prior knowledge is
provided to keep the edge details and the global reception field can be
extended. We explored the results in 3 ways: 1) The efficiency of the
DDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9
mainstream classifiers are evaluated in our dataset with fair implementation
details. Experimental results demonstrate that the combination of our approach
outperforms other combinations in terms of average accuracy and separate-gender
accuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for
separate-gender accuracy. 2) The effect of fingers. It is found that the best
performance of classifying gender with separate fingers is achieved by the
right ring finger. 3) The effect of specific features. Based on the
observations of the concentrations of fingerprints visualized by our approach,
it can be inferred that loops and whorls (level 1), bifurcations (level 2), as
well as line shapes (level 3) are connected with gender. Finally, we will open
source the dataset that contains 6000 fingerprint images]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Huawei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiashu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1"&gt;Huaiguang Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNeRF: GAN-based Neural Radiance Field without Posed Camera. (arXiv:2103.15606v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15606</id>
        <link href="http://arxiv.org/abs/2103.15606"/>
        <updated>2021-08-19T01:35:03.342Z</updated>
        <summary type="html"><![CDATA[We introduce GNeRF, a framework to marry Generative Adversarial Networks
(GAN) with Neural Radiance Field (NeRF) reconstruction for the complex
scenarios with unknown and even randomly initialized camera poses. Recent
NeRF-based advances have gained popularity for remarkable realistic novel view
synthesis. However, most of them heavily rely on accurate camera poses
estimation, while few recent methods can only optimize the unknown camera poses
in roughly forward-facing scenes with relatively short camera trajectories and
require rough camera poses initialization. Differently, our GNeRF only utilizes
randomly initialized poses for complex outside-in scenarios. We propose a novel
two-phases end-to-end framework. The first phase takes the use of GANs into the
new realm for optimizing coarse camera poses and radiance fields jointly, while
the second phase refines them with additional photometric loss. We overcome
local minima using a hybrid and iterative optimization scheme. Extensive
experiments on a variety of synthetic and natural scenes demonstrate the
effectiveness of GNeRF. More impressively, our approach outperforms the
baselines favorably in those scenes with repeated patterns or even low textures
that are regarded as extremely challenging before.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Quan Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anpei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haimin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minye Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images. (arXiv:2108.08157v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08157</id>
        <link href="http://arxiv.org/abs/2108.08157"/>
        <updated>2021-08-19T01:35:03.335Z</updated>
        <summary type="html"><![CDATA[Recently, FCNs have attracted widespread attention in the CD field. In
pursuit of better CD performance, it has become a tendency to design deeper and
more complicated FCNs, which inevitably brings about huge numbers of parameters
and an unbearable computational burden. With the goal of designing a quite deep
architecture to obtain more precise CD results while simultaneously decreasing
parameter numbers to improve efficiency, in this work, we present a very deep
and efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the
numerous parameters associated with deep architecture, an efficient convolution
consisting of depth-wise convolution and group convolution with a channel
shuffle mechanism is introduced to replace standard convolutional layers. In
terms of the specific network architecture, EffCDNet does not use mainstream
UNet-like architecture, but rather adopts the architecture with a very deep
encoder and a lightweight decoder. In the very deep encoder, two very deep
siamese streams stacked by efficient convolution first extract two highly
representative and informative feature maps from input image-pairs.
Subsequently, an efficient ASPP module is designed to capture multi-scale
change information. In the lightweight decoder, a recurrent criss-cross
self-attention (RCCA) module is applied to efficiently utilize non-local
similar feature representations to enhance discriminability for each pixel,
thus effectively separating the changed and unchanged regions. Moreover, to
tackle the optimization problem in confused pixels, two novel loss functions
based on information entropy are presented. On two challenging CD datasets, our
approach outperforms other SOTA FCN-based methods, with only benchmark-level
parameter numbers and quite low computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongruixuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11574</id>
        <link href="http://arxiv.org/abs/2104.11574"/>
        <updated>2021-08-19T01:35:03.328Z</updated>
        <summary type="html"><![CDATA[Capillaries are the smallest vessels in the body responsible for the delivery
of oxygen and nutrients to the surrounding cells. Various diseases have been
shown to alter the density of nutritive capillaries and the flow velocity of
erythrocytes. In previous studies, capillary density and flow velocity have
been assessed manually by trained specialists. Manual analysis of a standard
20-second long microvascular video takes on average 20 minutes and requires
extensive training. Several studies have reported that manual analysis hinders
the application of microvascular microscopy in a clinical setting. In this
paper, we present a fully automated state-of-the-art system, called
CapillaryNet, that can quantify skin nutritive capillary density and red blood
cell velocity from handheld microscopy videos. Moreover, CapillaryNet measures
several novel microvascular parameters that researchers were previously unable
to quantify, i.e. capillary hematocrit and Intra-capillary flow velocity
heterogeneity. Our system has been used to analyze skin microcirculation videos
from various patient groups (COVID-19, pancreatitis, and acute heart diseases).
Our proposed system excels from existing capillary detection systems as it
combines the speed of traditional computer vision algorithms and the accuracy
of convolutional neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1"&gt;Anastasiya Dykyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1"&gt;Paulo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1"&gt;Eric Jul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector. (arXiv:2108.08258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08258</id>
        <link href="http://arxiv.org/abs/2108.08258"/>
        <updated>2021-08-19T01:35:03.318Z</updated>
        <summary type="html"><![CDATA[Stereo-based 3D detection aims at detecting 3D object bounding boxes from
stereo images using intermediate depth maps or implicit 3D geometry
representations, which provides a low-cost solution for 3D perception. However,
its performance is still inferior compared with LiDAR-based detection
algorithms. To detect and localize accurate 3D bounding boxes, LiDAR-based
models can encode accurate object boundaries and surface normal directions from
LiDAR point clouds. However, the detection results of stereo-based detectors
are easily affected by the erroneous depth features due to the limitation of
stereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry
Aware Stereo Detector) to learn stereo-based 3D detectors under the guidance of
high-level geometry-aware representations of LiDAR-based detection models. In
addition, we found existing voxel-based stereo detectors failed to learn
semantic features effectively from indirect 3D supervisions. We attach an
auxiliary 2D detection head to provide direct 2D semantic supervisions.
Experiment results show that the above two strategies improved the geometric
and semantic representation capabilities. Compared with the state-of-the-art
stereo detector, our method has improved the 3D detection performance of cars,
pedestrians, cyclists by 10.44%, 5.69%, 5.97% mAP respectively on the official
KITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is
further narrowed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaoyang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shaoshuai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13467</id>
        <link href="http://arxiv.org/abs/2107.13467"/>
        <updated>2021-08-19T01:35:03.309Z</updated>
        <summary type="html"><![CDATA[The unsupervised domain adaptation (UDA) has been widely adopted to alleviate
the data scalability issue, while the existing works usually focus on
classifying independently discrete labels. However, in many tasks (e.g.,
medical diagnosis), the labels are discrete and successively distributed. The
UDA for ordinal classification requires inducing non-trivial ordinal
distribution prior to the latent space. Target for this, the partially ordered
set (poset) is defined for constraining the latent vector. Instead of the
typically i.i.d. Gaussian latent prior, in this work, a recursively conditional
Gaussian (RCG) set is adapted for ordered constraint modeling, which admits a
tractable joint distribution prior. Furthermore, we are able to control the
density of content vector that violates the poset constraints by a simple
"three-sigma rule". We explicitly disentangle the cross-domain images into a
shared ordinal prior induced ordinal content space and two separate
source/target ordinal-unrelated spaces, and the self-training is worked on the
shared space exclusively for ordinal-aware domain alignment. Extensive
experiments on UDA medical diagnoses and facial age estimation demonstrate its
effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yubin Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1"&gt;Pengyi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12533</id>
        <link href="http://arxiv.org/abs/2104.12533"/>
        <updated>2021-08-19T01:35:03.285Z</updated>
        <summary type="html"><![CDATA[The past year has witnessed the rapid development of applying the Transformer
module to vision problems. While some researchers have demonstrated that
Transformer-based models enjoy a favorable ability of fitting data, there are
still growing number of evidences showing that these models suffer over-fitting
especially when the training data is limited. This paper offers an empirical
study by performing step-by-step operations to gradually transit a
Transformer-based model to a convolution-based model. The results we obtain
during the transition process deliver useful messages for improving visual
recognition. Based on these observations, we propose a new architecture named
Visformer, which is abbreviated from the `Vision-friendly Transformer'. With
the same computational complexity, Visformer outperforms both the
Transformer-based and convolution-based models in terms of ImageNet
classification accuracy, and the advantage becomes more significant when the
model complexity is lower or the training set is smaller. The code is available
at https://github.com/danczs/Visformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengsu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lingxi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1"&gt;Jianwei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuefeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Longhui Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FOX-NAS: Fast, On-device and Explainable Neural Architecture Search. (arXiv:2108.08189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08189</id>
        <link href="http://arxiv.org/abs/2108.08189"/>
        <updated>2021-08-19T01:35:03.279Z</updated>
        <summary type="html"><![CDATA[Neural architecture search can discover neural networks with good
performance, and One-Shot approaches are prevalent. One-Shot approaches
typically require a supernet with weight sharing and predictors that predict
the performance of architecture. However, the previous methods take much time
to generate performance predictors thus are inefficient. To this end, we
propose FOX-NAS that consists of fast and explainable predictors based on
simulated annealing and multivariate regression. Our method is
quantization-friendly and can be efficiently deployed to the edge. The
experiments on different hardware show that FOX-NAS models outperform some
other popular neural network architectures. For example, FOX-NAS matches
MobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on
the edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer
Vision Challenge (LPCVC), DSP classification track. See all evaluation results
at https://lpcv.ai/competitions/2020. Search code and pre-trained models are
released at https://github.com/great8nctu/FOX-NAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chia-Hsiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yu-Shin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Yuan-Yao Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hung-Yueh Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kai-Chiang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression. (arXiv:2108.08211v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08211</id>
        <link href="http://arxiv.org/abs/2108.08211"/>
        <updated>2021-08-19T01:35:03.270Z</updated>
        <summary type="html"><![CDATA[Based on the powerful feature extraction ability of deep learning
architecture, recently, deep-learning based watermarking algorithms have been
widely studied. The basic framework of such algorithm is the auto-encoder like
end-to-end architecture with an encoder, a noise layer and a decoder. The key
to guarantee robustness is the adversarial training with the differential noise
layer. However, we found that none of the existing framework can well ensure
the robustness against JPEG compression, which is non-differential but is an
essential and important image processing operation. To address such
limitations, we proposed a novel end-to-end training architecture, which
utilizes Mini-Batch of Real and Simulated JPEG compression (MBRS) to enhance
the JPEG robustness. Precisely, for different mini-batches, we randomly choose
one of real JPEG, simulated JPEG and noise-free layer as the noise layer.
Besides, we suggest to utilize the Squeeze-and-Excitation blocks which can
learn better feature in embedding and extracting stage, and propose a "message
processor" to expand the message in a more appreciate way. Meanwhile, to
improve the robustness against crop attack, we propose an additive diffusion
block into the network. The extensive experimental results have demonstrated
the superior performance of the proposed scheme compared with the
state-of-the-art algorithms. Under the JPEG compression with quality factor
Q=50, our models achieve a bit error rate less than 0.01% for extracted
messages, with PSNR larger than 36 for the encoded images, which shows the
well-enhanced robustness against JPEG attack. Besides, under many other
distortions such as Gaussian filter, crop, cropout and dropout, the proposed
framework also obtains strong robustness. The code implemented by PyTorch
\cite{2011torch7} is avaiable in https://github.com/jzyustc/MBRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhaoyang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1"&gt;Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Visual-Inertial Cooperative Localization. (arXiv:2103.12770v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12770</id>
        <link href="http://arxiv.org/abs/2103.12770"/>
        <updated>2021-08-19T01:35:03.254Z</updated>
        <summary type="html"><![CDATA[In this paper we present a consistent and distributed state estimator for
multi-robot cooperative localization (CL) which efficiently fuses environmental
features and loop-closure constraints across time and robots. In particular, we
leverage covariance intersection (CI) to allow each robot to only estimate its
own state and autocovariance and compensate for the unknown correlations
between robots. Two novel multi-robot methods for utilizing common
environmental SLAM features are introduced and evaluated in terms of accuracy
and efficiency. Moreover, we adapt CI to enable drift-free estimation through
the use of loop-closure measurement constraints to other robots' historical
poses without a significant increase in computational cost. The proposed
distributed CL estimator is validated against its non-realtime centralized
counterpart extensively in both simulations and real-world experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Pengxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geneva_P/0/1/0/all/0/1"&gt;Patrick Geneva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1"&gt;Wei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guoquan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02523</id>
        <link href="http://arxiv.org/abs/2011.02523"/>
        <updated>2021-08-19T01:35:03.246Z</updated>
        <summary type="html"><![CDATA[For many fundamental scene understanding tasks, it is difficult or impossible
to obtain per-pixel ground truth labels from real images. We address this
challenge by introducing Hypersim, a photorealistic synthetic dataset for
holistic indoor scene understanding. To create our dataset, we leverage a large
repository of synthetic scenes created by professional artists, and we generate
77,400 images of 461 indoor scenes with detailed per-pixel labels and
corresponding ground truth geometry. Our dataset: (1) relies exclusively on
publicly available 3D assets; (2) includes complete scene geometry, material
information, and lighting information for every scene; (3) includes dense
per-pixel semantic instance segmentations and complete camera information for
every image; and (4) factors every image into diffuse reflectance, diffuse
illumination, and a non-diffuse residual term that captures view-dependent
lighting effects.

We analyze our dataset at the level of scenes, objects, and pixels, and we
analyze costs in terms of money, computation time, and annotation effort.
Remarkably, we find that it is possible to generate our entire dataset from
scratch, for roughly half the cost of training a popular open-source natural
language processing model. We also evaluate sim-to-real transfer performance on
two real-world scene understanding tasks - semantic segmentation and 3D shape
prediction - where we find that pre-training on our dataset significantly
improves performance on both tasks, and achieves state-of-the-art performance
on the most challenging Pix3D test set. All of our rendered image data, as well
as all the code we used to generate our dataset and perform our experiments, is
available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_M/0/1/0/all/0/1"&gt;Mike Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1"&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1"&gt;Anurag Ranjan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Atulit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1"&gt;Miguel Angel Bautista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paczan_N/0/1/0/all/0/1"&gt;Nathan Paczan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_R/0/1/0/all/0/1"&gt;Russ Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1"&gt;Joshua M. Susskind&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Spatial Dimensions of Vision Transformers. (arXiv:2103.16302v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16302</id>
        <link href="http://arxiv.org/abs/2103.16302"/>
        <updated>2021-08-19T01:35:03.239Z</updated>
        <summary type="html"><![CDATA[Vision Transformer (ViT) extends the application range of transformers from
language processing to computer vision tasks as being an alternative
architecture against the existing convolutional neural networks (CNN). Since
the transformer-based architecture has been innovative for computer vision
modeling, the design convention towards an effective architecture has been less
studied yet. From the successful design principles of CNN, we investigate the
role of spatial dimension conversion and its effectiveness on transformer-based
architecture. We particularly attend to the dimension reduction principle of
CNNs; as the depth increases, a conventional CNN increases channel dimension
and decreases spatial dimensions. We empirically show that such a spatial
dimension reduction is beneficial to a transformer architecture as well, and
propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT
model. We show that PiT achieves the improved model capability and
generalization performance against ViT. Throughout the extensive experiments,
we further show PiT outperforms the baseline on several tasks such as image
classification, object detection, and robustness evaluation. Source codes and
ImageNet models are available at https://github.com/naver-ai/pit]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1"&gt;Byeongho Heo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1"&gt;Junsuk Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Conditioned Probabilistic Learning of Video Rescaling. (arXiv:2107.11639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11639</id>
        <link href="http://arxiv.org/abs/2107.11639"/>
        <updated>2021-08-19T01:35:03.222Z</updated>
        <summary type="html"><![CDATA[Bicubic downscaling is a prevalent technique used to reduce the video storage
burden or to accelerate the downstream processing speed. However, the inverse
upscaling step is non-trivial, and the downscaled video may also deteriorate
the performance of downstream tasks. In this paper, we propose a
self-conditioned probabilistic framework for video rescaling to learn the
paired downscaling and upscaling procedures simultaneously. During the
training, we decrease the entropy of the information lost in the downscaling by
maximizing its probability conditioned on the strong spatial-temporal prior
information within the downscaled video. After optimization, the downscaled
video by our framework preserves more meaningful information, which is
beneficial for both the upscaling step and the downstream tasks, e.g., video
action recognition task. We further extend the framework to a lossy video
compression system, in which a gradient estimator for non-differential
industrial lossy codecs is proposed for the end-to-end training of the whole
system. Extensive experimental results demonstrate the superiority of our
approach on video rescaling, video compression, and efficient action
recognition tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhaohui Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhiyong Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Photo Scan: Semi-Supervised Learning for dealing with the real-world degradation in Smartphone Photo Scanning. (arXiv:2102.06120v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06120</id>
        <link href="http://arxiv.org/abs/2102.06120"/>
        <updated>2021-08-19T01:35:03.215Z</updated>
        <summary type="html"><![CDATA[Physical photographs now can be conveniently scanned by smartphones and
stored forever as a digital version, yet the scanned photos are not restored
well. One solution is to train a supervised deep neural network on many digital
photos and the corresponding scanned photos. However, it requires a high labor
cost, leading to limited training data. Previous works create training pairs by
simulating degradation using image processing techniques. Their synthetic
images are formed with perfectly scanned photos in latent space. Even so, the
real-world degradation in smartphone photo scanning remains unsolved since it
is more complicated due to lens defocus, lighting conditions, losing details
via printing. Besides, locally structural misalignment still occurs in data due
to distorted shapes captured in a 3-D world, reducing restoration performance
and the reliability of the quantitative evaluation. To solve these problems, we
propose a semi-supervised Deep Photo Scan (DPScan). First, we present a way of
producing real-world degradation and provide the DIV2K-SCAN dataset for
smartphone-scanned photo restoration. Also, Local Alignment is proposed to
reduce the minor misalignment remaining in data. Second, we simulate many
different variants of the real-world degradation using low-level image
transformation to gain a generalization in smartphone-scanned image properties,
then train a degradation network to generalize all styles of degradation and
provide pseudo-scanned photos for unscanned images as if they were scanned by a
smartphone. Finally, we propose a Semi-Supervised Learning that allows our
restoration network to be trained on both scanned and unscanned images,
diversifying training image content. As a result, the proposed DPScan
quantitatively and qualitatively outperforms its baseline architecture,
state-of-the-art academic research, and industrial products in smartphone photo
scanning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1"&gt;Man M. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjia Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidential Deep Learning for Open Set Action Recognition. (arXiv:2107.10161v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10161</id>
        <link href="http://arxiv.org/abs/2107.10161"/>
        <updated>2021-08-19T01:35:03.208Z</updated>
        <summary type="html"><![CDATA[In a real-world scenario, human actions are typically out of the distribution
from training data, which requires a model to both recognize the known actions
and reject the unknown. Different from image data, video actions are more
challenging to be recognized in an open-set setting due to the uncertain
temporal dynamics and static bias of human actions. In this paper, we propose a
Deep Evidential Action Recognition (DEAR) method to recognize actions in an
open testing set. Specifically, we formulate the action recognition problem
from the evidential deep learning (EDL) perspective and propose a novel model
calibration method to regularize the EDL training. Besides, to mitigate the
static bias of video representation, we propose a plug-and-play module to
debias the learned representation through contrastive learning. Experimental
results show that our DEAR method achieves consistent performance gain on
multiple mainstream action recognition models and benchmarks. Code and
pre-trained models are available at
{\small{\url{https://www.rit.edu/actionlab/dear}}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1"&gt;Wentao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Yu Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2106.15277v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15277</id>
        <link href="http://arxiv.org/abs/2106.15277"/>
        <updated>2021-08-19T01:35:03.200Z</updated>
        <summary type="html"><![CDATA[3D LiDAR (light detection and ranging) semantic segmentation is important in
scene understanding for many applications, such as auto-driving and robotics.
For example, for autonomous cars equipped with RGB cameras and LiDAR, it is
crucial to fuse complementary information from different sensors for robust and
accurate segmentation. Existing fusion-based methods, however, may not achieve
promising performance due to the vast difference between the two modalities. In
this work, we investigate a collaborative fusion scheme called perception-aware
multi-sensor fusion (PMF) to exploit perceptual information from two
modalities, namely, appearance information from RGB images and spatio-depth
information from point clouds. To this end, we first project point clouds to
the camera coordinates to provide spatio-depth information for RGB images.
Then, we propose a two-stream network to extract features from the two
modalities, separately, and fuse the features by effective residual-based
fusion modules. Moreover, we propose additional perception-aware losses to
measure the perceptual difference between the two modalities. Extensive
experiments on two benchmark data sets show the superiority of our method. For
example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8 in
mIoU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1"&gt;Zhuangwei Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10437</id>
        <link href="http://arxiv.org/abs/2106.10437"/>
        <updated>2021-08-19T01:35:03.194Z</updated>
        <summary type="html"><![CDATA[Recently, there has been discussions on the ill-posed nature of
super-resolution that multiple possible reconstructions exist for a given
low-resolution image. Using normalizing flows, SRflow[23] achieves
state-of-the-art perceptual quality by learning the distribution of the output
instead of a deterministic output to one estimate. In this paper, we adapt the
concepts of SRFlow to improve GAN-based super-resolution by properly
implementing the one-to-many property. We modify the generator to estimate a
distribution as a mapping from random noise. We improve the content loss that
hampers the perceptual training objectives. We also propose additional training
techniques to further enhance the perceptual quality of generated images. Using
our proposed methods, we were able to improve the performance of ESRGAN[1] in
x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual
extreme SR by applying our methods to RFB-ESRGAN[21].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sieun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Perturbations with Normalizing Flows for Improved Generalization. (arXiv:2108.07958v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.07958</id>
        <link href="http://arxiv.org/abs/2108.07958"/>
        <updated>2021-08-19T01:35:03.169Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a widely adopted technique for avoiding overfitting when
training deep neural networks. However, this approach requires domain-specific
knowledge and is often limited to a fixed set of hard-coded transformations.
Recently, several works proposed to use generative models for generating
semantically meaningful perturbations to train a classifier. However, because
accurate encoding and decoding are critical, these methods, which use
architectures that approximate the latent-variable inference, remained limited
to pilot studies on small datasets.

Exploiting the exactly reversible encoder-decoder structure of normalizing
flows, we perform on-manifold perturbations in the latent space to define fully
unsupervised data augmentations. We demonstrate that such perturbations match
the performance of advanced data augmentation techniques -- reaching 96.6% test
accuracy for CIFAR-10 using ResNet-18 and outperform existing methods,
particularly in low data regimes -- yielding 10--25% relative improvement of
test accuracy from classical training. We find that our latent adversarial
perturbations adaptive to the classifier throughout its training are most
effective, yielding the first test accuracy improvement results on real-world
datasets -- CIFAR-10/100 -- via latent-space perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yuksel_O/0/1/0/all/0/1"&gt;Oguz Kaan Yuksel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1"&gt;Tatjana Chavdarova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Focus for Efficient Video Recognition. (arXiv:2105.03245v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03245</id>
        <link href="http://arxiv.org/abs/2105.03245"/>
        <updated>2021-08-19T01:35:03.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the spatial redundancy in video recognition with
the aim to improve the computational efficiency. It is observed that the most
informative region in each frame of a video is usually a small image patch,
which shifts smoothly across frames. Therefore, we model the patch localization
problem as a sequential decision task, and propose a reinforcement learning
based approach for efficient spatially adaptive video recognition (AdaFocus).
In specific, a light-weighted ConvNet is first adopted to quickly process the
full video sequence, whose features are used by a recurrent policy network to
localize the most task-relevant regions. Then the selected patches are inferred
by a high-capacity network for the final prediction. During offline inference,
once the informative patch sequence has been generated, the bulk of computation
can be done in parallel, and is efficient on modern GPU devices. In addition,
we demonstrate that the proposed method can be easily extended by further
considering the temporal redundancy, e.g., dynamically skipping less valuable
frames. Extensive experiments on five benchmark datasets, i.e., ActivityNet,
FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is
significantly more efficient than the competitive baselines. Code is available
at https://github.com/blackfeather-wang/AdaFocus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhaoxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haojun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yizeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VariTex: Variational Neural Face Textures. (arXiv:2104.05988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05988</id>
        <link href="http://arxiv.org/abs/2104.05988"/>
        <updated>2021-08-19T01:35:03.155Z</updated>
        <summary type="html"><![CDATA[Deep generative models can synthesize photorealistic images of human faces
with novel identities. However, a key challenge to the wide applicability of
such techniques is to provide independent control over semantically meaningful
parameters: appearance, head pose, face shape, and facial expressions. In this
paper, we propose VariTex - to the best of our knowledge the first method that
learns a variational latent feature space of neural face textures, which allows
sampling of novel identities. We combine this generative model with a
parametric face model and gain explicit control over head pose and facial
expressions. To generate complete images of human heads, we propose an additive
decoder that adds plausible details such as hair. A novel training scheme
enforces a pose-independent latent space and in consequence, allows learning a
one-to-many mapping between latent codes and pose-conditioned exterior regions.
The resulting method can generate geometrically consistent images of novel
identities under fine-grained control over head pose, face shape, and facial
expressions. This facilitates a broad range of downstream tasks, like sampling
novel identities, changing the head pose, expression transfer, and more. Code
and models are available for research on https://mcbuehler.github.io/VariTex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1"&gt;Marcel C. B&amp;#xfc;hler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gengyan Li&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1"&gt;Thabo Beeler&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt; (1) ((1) ETH Zurich, (2) Google)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The surprising impact of mask-head architecture on novel class segmentation. (arXiv:2104.00613v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00613</id>
        <link href="http://arxiv.org/abs/2104.00613"/>
        <updated>2021-08-19T01:35:03.148Z</updated>
        <summary type="html"><![CDATA[Instance segmentation models today are very accurate when trained on large
annotated datasets, but collecting mask annotations at scale is prohibitively
expensive. We address the partially supervised instance segmentation problem in
which one can train on (significantly cheaper) bounding boxes for all
categories but use masks only for a subset of categories. In this work, we
focus on a popular family of models which apply differentiable cropping to a
feature map and predict a mask based on the resulting crop. Under this family,
we study Mask R-CNN and discover that instead of its default strategy of
training the mask-head with a combination of proposals and groundtruth boxes,
training the mask-head with only groundtruth boxes dramatically improves its
performance on novel classes. This training strategy also allows us to take
advantage of alternative mask-head architectures, which we exploit by replacing
the typical mask-head of 2-4 layers with significantly deeper off-the-shelf
architectures (e.g. ResNet, Hourglass models). While many of these
architectures perform similarly when trained in fully supervised mode, our main
finding is that they can generalize to novel classes in dramatically different
ways. We call this ability of mask-heads to generalize to unseen classes the
strong mask generalization effect and show that without any specialty modules
or losses, we can achieve state-of-the-art results in the partially supervised
COCO instance segmentation benchmark. Finally, we demonstrate that our effect
is general, holding across underlying detection methodologies (including
anchor-based, anchor-free or no detector at all) and across different backbone
networks. Code and pre-trained models are available at https://git.io/deepmac.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1"&gt;Vighnesh Birodkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhichao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathod_V/0/1/0/all/0/1"&gt;Vivek Rathod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jonathan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10772</id>
        <link href="http://arxiv.org/abs/2102.10772"/>
        <updated>2021-08-19T01:35:03.140Z</updated>
        <summary type="html"><![CDATA[We propose UniT, a Unified Transformer model to simultaneously learn the most
prominent tasks across different domains, ranging from object detection to
natural language understanding and multimodal reasoning. Based on the
transformer encoder-decoder architecture, our UniT model encodes each input
modality with an encoder and makes predictions on each task with a shared
decoder over the encoded input representations, followed by task-specific
output heads. The entire model is jointly trained end-to-end with losses from
each task. Compared to previous efforts on multi-task learning with
transformers, we share the same model parameters across all tasks instead of
separately fine-tuning task-specific models and handle a much higher variety of
tasks across different domains. In our experiments, we learn 7 tasks jointly
over 8 datasets, achieving strong performance on each task with significantly
fewer parameters. Our code is available in MMF at https://mmf.sh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ronghang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Vision Transformers with Hierarchical Pooling. (arXiv:2103.10619v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10619</id>
        <link href="http://arxiv.org/abs/2103.10619"/>
        <updated>2021-08-19T01:35:03.123Z</updated>
        <summary type="html"><![CDATA[The recently proposed Visual image Transformers (ViT) with pure attention
have achieved promising performance on image recognition tasks, such as image
classification. However, the routine of the current ViT model is to maintain a
full-length patch sequence during inference, which is redundant and lacks
hierarchical representation. To this end, we propose a Hierarchical Visual
Transformer (HVT) which progressively pools visual tokens to shrink the
sequence length and hence reduces the computational cost, analogous to the
feature maps downsampling in Convolutional Neural Networks (CNNs). It brings a
great benefit that we can increase the model capacity by scaling dimensions of
depth/width/resolution/patch size without introducing extra computational
complexity due to the reduced sequence length. Moreover, we empirically find
that the average pooled visual tokens contain more discriminative information
than the single class token. To demonstrate the improved scalability of our
HVT, we conduct extensive experiments on the image classification task. With
comparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and
CIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zizheng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03194</id>
        <link href="http://arxiv.org/abs/2106.03194"/>
        <updated>2021-08-19T01:35:03.115Z</updated>
        <summary type="html"><![CDATA[Implicit neural networks, a.k.a., deep equilibrium networks, are a class of
implicit-depth learning models where function evaluation is performed by
solving a fixed point equation. They generalize classic feedforward models and
are equivalent to infinite-depth weight-tied feedforward networks. While
implicit models show improved accuracy and significant reduction in memory
consumption, they can suffer from ill-posedness and convergence instability.

This paper provides a new framework to design well-posed and robust implicit
neural networks based upon contraction theory for the non-Euclidean norm
$\ell_\infty$. Our framework includes (i) a novel condition for well-posedness
based on one-sided Lipschitz constants, (ii) an average iteration for computing
fixed-points, and (iii) explicit estimates on input-output Lipschitz constants.
Additionally, we design a training problem with the well-posedness condition
and the average iteration as constraints and, to achieve robust models, with
the input-output Lipschitz constant as a regularizer. Our $\ell_\infty$
well-posedness condition leads to a larger polytopic training search space than
existing conditions and our average iteration enjoys accelerated convergence.
Finally, we perform several numerical experiments for function estimation and
digit classification through the MNIST data set. Our numerical results
demonstrate improved accuracy and robustness of the implicit models with
smaller input-output Lipschitz bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1"&gt;Saber Jafarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1"&gt;Alexander Davydov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1"&gt;Anton V. Proskurnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1"&gt;Francesco Bullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Articulated Radiance Field. (arXiv:2104.03110v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03110</id>
        <link href="http://arxiv.org/abs/2104.03110"/>
        <updated>2021-08-19T01:35:03.108Z</updated>
        <summary type="html"><![CDATA[We present Neural Articulated Radiance Field (NARF), a novel deformable 3D
representation for articulated objects learned from images. While recent
advances in 3D implicit representation have made it possible to learn models of
complex objects, learning pose-controllable representations of articulated
objects remains a challenge, as current methods require 3D shape supervision
and are unable to render appearance. In formulating an implicit representation
of 3D articulated objects, our method considers only the rigid transformation
of the most relevant object part in solving for the radiance field at each 3D
location. In this way, the proposed method represents pose-dependent changes
without significantly increasing the computational complexity. NARF is fully
differentiable and can be trained from images with pose annotations. Moreover,
through the use of an autoencoder, it can learn appearance variations over
multiple instances of an object class. Experiments show that the proposed
method is efficient and can generalize well to novel poses. The code is
available for research purposes at https://github.com/nogu-atsu/NARF]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noguchi_A/0/1/0/all/0/1"&gt;Atsuhiro Noguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Admix: Enhancing the Transferability of Adversarial Attacks. (arXiv:2102.00436v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00436</id>
        <link href="http://arxiv.org/abs/2102.00436"/>
        <updated>2021-08-19T01:35:03.100Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are known to be extremely vulnerable to adversarial
examples under white-box setting. Moreover, the malicious adversaries crafted
on the surrogate (source) model often exhibit black-box transferability on
other models with the same learning task but having different architectures.
Recently, various methods are proposed to boost the adversarial
transferability, among which the input transformation is one of the most
effective approaches. We investigate in this direction and observe that
existing transformations are all applied on a single image, which might limit
the adversarial transferability. To this end, we propose a new input
transformation based attack method called Admix that considers the input image
and a set of images randomly sampled from other categories. Instead of directly
calculating the gradient on the original input, Admix calculates the gradient
on the input image admixed with a small portion of each add-in image while
using the original label of the input to craft more transferable adversaries.
Empirical evaluations on standard ImageNet dataset demonstrate that Admix could
achieve significantly better transferability than existing input transformation
methods under both single model setting and ensemble-model setting. By
incorporating with existing input transformations, our method could further
improve the transferability and outperforms the state-of-the-art combination of
input transformations by a clear margin when attacking nine advanced defense
models under ensemble-model setting. Code is available at
https://github.com/JHL-HUST/Admix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuanran He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bifurcated backbone strategy for RGB-D salient object detection. (arXiv:2007.02713v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02713</id>
        <link href="http://arxiv.org/abs/2007.02713"/>
        <updated>2021-08-19T01:35:03.094Z</updated>
        <summary type="html"><![CDATA[Multi-level feature fusion is a fundamental topic in computer vision. It has
been exploited to detect, segment and classify objects at various scales. When
multi-level features meet multi-modal cues, the optimal feature aggregation and
multi-modal learning strategy become a hot potato. In this paper, we leverage
the inherent multi-modal and multi-level nature of RGB-D salient object
detection to devise a novel cascaded refinement network. In particular, first,
we propose to regroup the multi-level features into teacher and student
features using a bifurcated backbone strategy (BBS). Second, we introduce a
depth-enhanced module (DEM) to excavate informative depth cues from the channel
and spatial views. Then, RGB and depth modalities are fused in a complementary
way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is
simple, efficient, and backbone-independent. Extensive experiments show that
BBS-Net significantly outperforms eighteen SOTA models on eight challenging
datasets under five evaluation measures, demonstrating the superiority of our
approach ($\sim 4 \%$ improvement in S-measure $vs.$ the top-ranked model:
DMRA-iccv2019). In addition, we provide a comprehensive analysis on the
generalization ability of different RGB-D datasets and provide a powerful
training set for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1"&gt;Yingjie Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jufeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1"&gt;Ali Borji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junwei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Shift Equivariance Impacts Metric Learning for Instance Segmentation. (arXiv:2101.05846v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05846</id>
        <link href="http://arxiv.org/abs/2101.05846"/>
        <updated>2021-08-19T01:35:03.087Z</updated>
        <summary type="html"><![CDATA[Metric learning has received conflicting assessments concerning its
suitability for solving instance segmentation tasks. It has been dismissed as
theoretically flawed due to the shift equivariance of the employed CNNs and
their respective inability to distinguish same-looking objects. Yet it has been
shown to yield state of the art results for a variety of tasks, and practical
issues have mainly been reported in the context of tile-and-stitch approaches,
where discontinuities at tile boundaries have been observed. To date, neither
of the reported issues have undergone thorough formal analysis. In our work, we
contribute a comprehensive formal analysis of the shift equivariance properties
of encoder-decoder-style CNNs, which yields a clear picture of what can and
cannot be achieved with metric learning in the face of same-looking objects. In
particular, we prove that a standard encoder-decoder network that takes
$d$-dimensional images as input, with $l$ pooling layers and pooling factor
$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and
we show that this upper limit can be reached. Furthermore, we show that to
avoid discontinuities in a tile-and-stitch approach, assuming standard batch
size 1, it is necessary to employ valid convolutions in combination with a
training output window size strictly greater than $f^l$, while at test-time it
is necessary to crop tiles to size $n\cdot f^l$ before stitching, with $n\geq
1$. We complement these theoretical findings by discussing a number of
insightful special cases for which we show empirical results on synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rumberger_J/0/1/0/all/0/1"&gt;Josef Lorenz Rumberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaoyan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirsch_P/0/1/0/all/0/1"&gt;Peter Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohmen_M/0/1/0/all/0/1"&gt;Melanie Dohmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guarino_V/0/1/0/all/0/1"&gt;Vanessa Emanuela Guarino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mokarian_A/0/1/0/all/0/1"&gt;Ashkan Mokarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mais_L/0/1/0/all/0/1"&gt;Lisa Mais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1"&gt;Jan Funke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1"&gt;Dagmar Kainmueller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09854</id>
        <link href="http://arxiv.org/abs/2012.09854"/>
        <updated>2021-08-19T01:35:03.069Z</updated>
        <summary type="html"><![CDATA[We present Worldsheet, a method for novel view synthesis using just a single
RGB image as input. The main insight is that simply shrink-wrapping a planar
mesh sheet onto the input image, consistent with the learned intermediate
depth, captures underlying geometry sufficient to generate photorealistic
unseen views with large viewpoint changes. To operationalize this, we propose a
novel differentiable texture sampler that allows our wrapped mesh sheet to be
textured and rendered differentiably into an image from a target viewpoint. Our
approach is category-agnostic, end-to-end trainable without using any 3D
supervision, and requires a single image at test time. We also explore a simple
extension by stacking multiple layers of Worldsheets to better handle
occlusions. Worldsheet consistently outperforms prior state-of-the-art methods
on single-image view synthesis across several datasets. Furthermore, this
simple idea captures novel views surprisingly well on a wide range of
high-resolution in-the-wild images, converting them into navigable 3D pop-ups.
Video results and code are available at https://worldsheet.github.io.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ronghang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1"&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1"&gt;Alexander C. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Resize Images for Computer Vision Tasks. (arXiv:2103.09950v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09950</id>
        <link href="http://arxiv.org/abs/2103.09950"/>
        <updated>2021-08-19T01:35:03.058Z</updated>
        <summary type="html"><![CDATA[For all the ways convolutional neural nets have revolutionized computer
vision in recent years, one important aspect has received surprisingly little
attention: the effect of image size on the accuracy of tasks being trained for.
Typically, to be efficient, the input images are resized to a relatively small
spatial resolution (e.g. 224x224), and both training and inference are carried
out at this resolution. The actual mechanism for this re-scaling has been an
afterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic
are commonly used in most machine learning software frameworks. But do these
resizers limit the on task performance of the trained networks? The answer is
yes. Indeed, we show that the typical linear resizer can be replaced with
learned resizers that can substantially improve performance. Importantly, while
the classical resizers typically result in better perceptual quality of the
downscaled images, our proposed learned resizers do not necessarily give better
visual quality, but instead improve task performance. Our learned image resizer
is jointly trained with a baseline vision model. This learned CNN-based resizer
creates machine friendly visual manipulations that lead to a consistent
improvement of the end task metric over the baseline model. Specifically, here
we focus on the classification task with the ImageNet dataset, and experiment
with four different models to learn resizers adapted to each model. Moreover,
we show that the proposed resizer can also be useful for fine-tuning the
classification baselines for other vision tasks. To this end, we experiment
with three different baselines to develop image quality assessment (IQA) models
on the AVA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1"&gt;Hossein Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1"&gt;Peyman Milanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04353</id>
        <link href="http://arxiv.org/abs/2108.04353"/>
        <updated>2021-08-19T01:35:03.048Z</updated>
        <summary type="html"><![CDATA[The great success of Convolutional Neural Networks (CNN) for facial attribute
prediction relies on a large amount of labeled images. Facial image datasets
are usually annotated by some commonly used attributes (e.g., gender), while
labels for the other attributes (e.g., big nose) are limited which causes their
prediction challenging. To address this problem, we use a new Multi-Task
Learning (MTL) paradigm in which a facial attribute predictor uses the
knowledge of other related attributes to obtain a better generalization
performance. Here, we leverage MLT paradigm in two problem settings. First, it
is assumed that the structure of the tasks (e.g., grouping pattern of facial
attributes) is known as a prior knowledge, and parameters of the tasks (i.e.,
predictors) within the same group are represented by a linear combination of a
limited number of underlying basis tasks. Here, a sparsity constraint on the
coefficients of this linear combination is also considered such that each task
is represented in a more structured and simpler manner. Second, it is assumed
that the structure of the tasks is unknown, and then structure and parameters
of the tasks are learned jointly by using a Laplacian regularization framework.
Our MTL methods are compared with competing methods for facial attribute
prediction to show its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1"&gt;Fariborz Taherkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1"&gt;Ali Dabouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving Search Space for Neural Architecture Search. (arXiv:2011.10904v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10904</id>
        <link href="http://arxiv.org/abs/2011.10904"/>
        <updated>2021-08-19T01:35:03.027Z</updated>
        <summary type="html"><![CDATA[The automation of neural architecture design has been a coveted alternative
to human experts. Recent works have small search space, which is easier to
optimize but has a limited upper bound of the optimal solution. Extra human
design is needed for those methods to propose a more suitable space with
respect to the specific task and algorithm capacity. To further enhance the
degree of automation for neural architecture search, we present a Neural
Search-space Evolution (NSE) scheme that iteratively amplifies the results from
the previous effort by maintaining an optimized search space subset. This
design minimizes the necessity of a well-designed search space. We further
extend the flexibility of obtainable architectures by introducing a learnable
multi-branch setting. By employing the proposed method, a consistent
performance gain is achieved during a progressive search over upcoming search
spaces. We achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs,
which yielded a state-of-the-art performance among previous auto-generated
architectures that do not involve knowledge distillation or weight pruning.
When the latency constraint is adopted, our result also performs better than
the previous best-performing mobile models with a 77.9% Top-1 retrain accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ci_Y/0/1/0/all/0/1"&gt;Yuanzheng Ci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Ming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Target Candidate Association to Keep Track of What Not to Track. (arXiv:2103.16556v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16556</id>
        <link href="http://arxiv.org/abs/2103.16556"/>
        <updated>2021-08-19T01:35:03.019Z</updated>
        <summary type="html"><![CDATA[The presence of objects that are confusingly similar to the tracked target,
poses a fundamental challenge in appearance-based visual tracking. Such
distractor objects are easily misclassified as the target itself, leading to
eventual tracking failure. While most methods strive to suppress distractors
through more powerful appearance models, we take an alternative approach.

We propose to keep track of distractor objects in order to continue tracking
the target. To this end, we introduce a learned association network, allowing
us to propagate the identities of all target candidates from frame-to-frame. To
tackle the problem of lacking ground-truth correspondences between distractor
objects in visual tracking, we propose a training strategy that combines
partial annotations with self-supervision. We conduct comprehensive
experimental validation and analysis of our approach on several challenging
datasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving
an AUC score of 67.1% on LaSOT and a +5.8% absolute gain on the OxUvA long-term
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_C/0/1/0/all/0/1"&gt;Christoph Mayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1"&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Hyperspherical Uniformity. (arXiv:2103.01649v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01649</id>
        <link href="http://arxiv.org/abs/2103.01649"/>
        <updated>2021-08-19T01:35:02.970Z</updated>
        <summary type="html"><![CDATA[Due to the over-parameterization nature, neural networks are a powerful tool
for nonlinear function approximation. In order to achieve good generalization
on unseen data, a suitable inductive bias is of great importance for neural
networks. One of the most straightforward ways is to regularize the neural
network with some additional objectives. L2 regularization serves as a standard
regularization for neural networks. Despite its popularity, it essentially
regularizes one dimension of the individual neuron, which is not strong enough
to control the capacity of highly over-parameterized neural networks. Motivated
by this, hyperspherical uniformity is proposed as a novel family of relational
regularizations that impact the interaction among neurons. We consider several
geometrically distinct ways to achieve hyperspherical uniformity. The
effectiveness of hyperspherical uniformity is justified by theoretical insights
and empirical evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Collation: Matching illustrations in manuscripts. (arXiv:2108.08109v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08109</id>
        <link href="http://arxiv.org/abs/2108.08109"/>
        <updated>2021-08-19T01:35:02.959Z</updated>
        <summary type="html"><![CDATA[Illustrations are an essential transmission instrument. For an historian, the
first step in studying their evolution in a corpus of similar manuscripts is to
identify which ones correspond to each other. This image collation task is
daunting for manuscripts separated by many lost copies, spreading over
centuries, which might have been completely re-organized and greatly modified
to adapt to novel knowledge or belief and include hundreds of illustrations.
Our contributions in this paper are threefold. First, we introduce the task of
illustration collation and a large annotated public dataset to evaluate
solutions, including 6 manuscripts of 2 different texts with more than 2 000
illustrations and 1 200 annotated correspondences. Second, we analyze state of
the art similarity measures for this task and show that they succeed in simple
cases but struggle for large manuscripts when the illustrations have undergone
very significant changes and are discriminated only by fine details. Finally,
we show clear evidence that significant performance boosts can be expected by
exploiting cycle-consistent correspondences. Our code and data are available on
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaoua_R/0/1/0/all/0/1"&gt;Ryad Kaoua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durr_A/0/1/0/all/0/1"&gt;Alexandra Durr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaris_S/0/1/0/all/0/1"&gt;Stavros Lazaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1"&gt;David Picard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1"&gt;Mathieu Aubry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13797</id>
        <link href="http://arxiv.org/abs/2107.13797"/>
        <updated>2021-08-19T01:35:02.928Z</updated>
        <summary type="html"><![CDATA[In recent years, federated learning (FL) has been widely applied for
supporting decentralized collaborative learning scenarios. Among existing FL
models, federated logistic regression (FLR) is a widely used statistic model
and has been used in various industries. To ensure data security and user
privacy, FLR leverages homomorphic encryption (HE) to protect the exchanged
data among different collaborative parties. However, HE introduces significant
computational overhead (i.e., the cost of data encryption/decryption and
calculation over encrypted data), which eventually becomes the performance
bottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based
solution to improve the performance of FLR. The core idea of HAFLO is to
summarize a set of performance-critical homomorphic operators (HO) used by FLR
and accelerate the execution of these operators through a joint optimization of
storage, IO, and computation. The preliminary results show that our
acceleration on FATE, a popular FL framework, achieves a 49.9$\times$ speedup
for heterogeneous LR and 88.4$\times$ for homogeneous LR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiaodian Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wanhang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xinyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shuihai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-DARTS: Towards Stable Architecture Search. (arXiv:2108.08128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08128</id>
        <link href="http://arxiv.org/abs/2108.08128"/>
        <updated>2021-08-19T01:35:02.920Z</updated>
        <summary type="html"><![CDATA[Differentiable architecture search (DARTS) marks a milestone in Neural
Architecture Search (NAS), boasting simplicity and small search costs. However,
DARTS still suffers from frequent performance collapse, which happens when some
operations, such as skip connections, zeroes and poolings, dominate the
architecture. In this paper, we are the first to point out that the phenomenon
is attributed to bi-level optimization. We propose Single-DARTS which merely
uses single-level optimization, updating network weights and architecture
parameters simultaneously with the same data batch. Even single-level
optimization has been previously attempted, no literature provides a systematic
explanation on this essential point. Replacing the bi-level optimization,
Single-DARTS obviously alleviates performance collapse as well as enhances the
stability of architecture search. Experiment results show that Single-DARTS
achieves state-of-the-art performance on mainstream search spaces. For
instance, on NAS-Benchmark-201, the searched architectures are nearly optimal
ones. We also validate that the single-level optimization framework is much
more stable than the bi-level one. We hope that this simple yet effective
method will give some insights on differential architecture search. The code is
available at https://github.com/PencilAndBike/Single-DARTS.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1"&gt;Pengfei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Ying Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yukang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specificity-preserving RGB-D Saliency Detection. (arXiv:2108.08162v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08162</id>
        <link href="http://arxiv.org/abs/2108.08162"/>
        <updated>2021-08-19T01:35:02.912Z</updated>
        <summary type="html"><![CDATA[RGB-D saliency detection has attracted increasing attention, due to its
effectiveness and the fact that depth cues can now be conveniently captured.
Existing works often focus on learning a shared representation through various
fusion strategies, with few methods explicitly considering how to preserve
modality-specific characteristics. In this paper, taking a new perspective, we
propose a specificity-preserving network (SP-Net) for RGB-D saliency detection,
which benefits saliency detection performance by exploring both the shared
information and modality-specific properties (e.g., specificity). Specifically,
two modality-specific networks and a shared learning network are adopted to
generate individual and shared saliency maps. A cross-enhanced integration
module (CIM) is proposed to fuse cross-modal features in the shared learning
network, which are then propagated to the next layer for integrating
cross-level information. Besides, we propose a multi-modal feature aggregation
(MFA) module to integrate the modality-specific features from each individual
decoder into the shared decoder, which can provide rich complementary
multi-modal information to boost the saliency detection performance. Further, a
skip connection is used to combine hierarchical features between the encoder
and decoder layers. Experiments on six benchmark datasets demonstrate that our
SP-Net outperforms other state-of-the-art methods. Code is available at:
https://github.com/taozh2017/SPNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Geng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting. (arXiv:2108.08165v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08165</id>
        <link href="http://arxiv.org/abs/2108.08165"/>
        <updated>2021-08-19T01:35:02.878Z</updated>
        <summary type="html"><![CDATA[Both generalized and incremental few-shot learning have to deal with three
major challenges: learning novel classes from only few samples per class,
preventing catastrophic forgetting of base classes, and classifier calibration
across novel and base classes. In this work we propose a three-stage framework
that allows to explicitly and effectively address these challenges. While the
first phase learns base classes with many samples, the second phase learns a
calibrated classifier for novel classes from few samples while also preventing
catastrophic forgetting. In the final phase, calibration is achieved across all
classes. We evaluate the proposed framework on four challenging benchmark
datasets for image and video few-shot classification and obtain
state-of-the-art results for both generalized and incremental few shot
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1"&gt;Anna Kukleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1"&gt;Bernt Schiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gastric Cancer Detection from X-ray Images Using Effective Data Augmentation and Hard Boundary Box Training. (arXiv:2108.08158v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08158</id>
        <link href="http://arxiv.org/abs/2108.08158"/>
        <updated>2021-08-19T01:35:02.870Z</updated>
        <summary type="html"><![CDATA[X-ray examination is suitable for screening of gastric cancer. Compared to
endoscopy, which can only be performed by doctors, X-ray imaging can also be
performed by radiographers, and thus, can treat more patients. However, the
diagnostic accuracy of gastric radiographs is as low as 85%. To address this
problem, highly accurate and quantitative automated diagnosis using machine
learning needs to be performed. This paper proposes a diagnostic support method
for detecting gastric cancer sites from X-ray images with high accuracy. The
two new technical proposal of the method are (1) stochastic functional gastric
image augmentation (sfGAIA), and (2) hard boundary box training (HBBT). The
former is a probabilistic enhancement of gastric folds in X-ray images based on
medical knowledge, whereas the latter is a recursive retraining technique to
reduce false positives. We use 4,724 gastric radiographs of 145 patients in
clinical practice and evaluate the cancer detection performance of the method
in a patient-based five-group cross-validation. The proposed sfGAIA and HBBT
significantly enhance the performance of the EfficientDet-D7 network by 5.9% in
terms of the F1-score, and our screening method reaches a practical screening
capability for gastric cancer (F1: 57.8%, recall: 90.2%, precision: 42.5%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Okamoto_H/0/1/0/all/0/1"&gt;Hideaki Okamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nomura_T/0/1/0/all/0/1"&gt;Takakiyo Nomura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nabeshima_K/0/1/0/all/0/1"&gt;Kazuhito Nabeshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hashimoto_J/0/1/0/all/0/1"&gt;Jun Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Iyatomi_H/0/1/0/all/0/1"&gt;Hitoshi Iyatomi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision. (arXiv:2108.08119v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08119</id>
        <link href="http://arxiv.org/abs/2108.08119"/>
        <updated>2021-08-19T01:35:02.843Z</updated>
        <summary type="html"><![CDATA[Learning RAW-to-sRGB mapping has drawn increasing attention in recent years,
wherein an input raw image is trained to imitate the target sRGB image captured
by another camera. However, the severe color inconsistency makes it very
challenging to generate well-aligned training pairs of input raw and target
sRGB images. While learning with inaccurately aligned supervision is prone to
causing pixel shift and producing blurry results. In this paper, we circumvent
such issue by presenting a joint learning model for image alignment and
RAW-to-sRGB mapping. To diminish the effect of color inconsistency in image
alignment, we introduce to use a global color mapping (GCM) module to generate
an initial sRGB image given the input raw image, which can keep the spatial
location of the pixels unchanged, and the target sRGB image is utilized to
guide GCM for converting the color towards it. Then a pre-trained optical flow
estimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to
align with the GCM output. To alleviate the effect of inaccurately aligned
supervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB
mapping. When training is done, the GCM module and optical flow network can be
detached, thereby bringing no extra computation cost for inference. Experiments
show that our method performs favorably against state-of-the-arts on ZRR and
SR-RAW datasets. With our joint learning model, a light-weight backbone can
achieve better quantitative and qualitative performance on ZRR dataset. Codes
are available at https://github.com/cszhilu1998/RAW-to-sRGB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haolin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruohao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wangmeng Zuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Variational Capsule Network for Open Set Recognition. (arXiv:2104.09159v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2104.09159</id>
        <link href="http://arxiv.org/abs/2104.09159"/>
        <updated>2021-08-19T01:35:02.807Z</updated>
        <summary type="html"><![CDATA[In open set recognition, a classifier has to detect unknown classes that are
not known at training time. In order to recognize new categories, the
classifier has to project the input samples of known classes in very compact
and separated regions of the features space for discriminating samples of
unknown classes. Recently proposed Capsule Networks have shown to outperform
alternatives in many fields, particularly in image recognition, however they
have not been fully applied yet to open-set recognition. In capsule networks,
scalar neurons are replaced by capsule vectors or matrices, whose entries
represent different properties of objects. In our proposal, during training,
capsules features of the same known class are encouraged to match a pre-defined
gaussian, one for each class. To this end, we use the variational autoencoder
framework, with a set of gaussian priors as the approximation for the posterior
distribution. In this way, we are able to control the compactness of the
features of the same class around the center of the gaussians, thus controlling
the ability of the classifier in detecting samples from unknown classes. We
conducted several experiments and ablation of our model, obtaining state of the
art results on different datasets in the open set recognition and unknown
detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yunrui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1"&gt;Guglielmo Camporese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenjing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1"&gt;Alessandro Sperduti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1"&gt;Lamberto Ballan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Observer Visual Problem-Solving Methods are Dynamically Hypothesized, Deployed and Tested. (arXiv:2108.08145v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08145</id>
        <link href="http://arxiv.org/abs/2108.08145"/>
        <updated>2021-08-19T01:35:02.800Z</updated>
        <summary type="html"><![CDATA[The STAR architecture was designed to test the value of the full Selective
Tuning model of visual attention for complex real-world visuospatial tasks and
behaviors. However, knowledge of how humans solve such tasks in 3D as active
observers is lean. We thus devised a novel experimental setup and examined such
behavior. We discovered that humans exhibit a variety of problem-solving
strategies whose breadth and complexity are surprising and not easily handled
by current methodologies. It is apparent that solution methods are dynamically
composed by hypothesizing sequences of actions, testing them, and if they fail,
trying different ones. The importance of active observation is striking as is
the lack of any learning effect. These results inform our Cognitive Program
representation of STAR extending its relevance to real-world tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solbach_M/0/1/0/all/0/1"&gt;Markus D. Solbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1"&gt;John K. Tsotsos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Anchor Active Domain Adaptation for Semantic Segmentation. (arXiv:2108.08012v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08012</id>
        <link href="http://arxiv.org/abs/2108.08012"/>
        <updated>2021-08-19T01:35:02.794Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaption has proven to be an effective approach for
alleviating the intensive workload of manual annotation by aligning the
synthetic source-domain data and the real-world target-domain samples.
Unfortunately, mapping the target-domain distribution to the source-domain
unconditionally may distort the essential structural information of the
target-domain data. To this end, we firstly propose to introduce a novel
multi-anchor based active learning strategy to assist domain adaptation
regarding the semantic segmentation task. By innovatively adopting multiple
anchors instead of a single centroid, the source domain can be better
characterized as a multimodal distribution, thus more representative and
complimentary samples are selected from the target domain. With little workload
to manually annotate these active samples, the distortion of the target-domain
distribution can be effectively alleviated, resulting in a large performance
gain. The multi-anchor strategy is additionally employed to model the
target-distribution. By regularizing the latent representation of the target
samples compact around multiple anchors through a novel soft alignment loss,
more precise segmentation can be achieved. Extensive experiments are conducted
on public datasets to demonstrate that the proposed approach outperforms
state-of-the-art methods significantly, along with thorough ablation study to
verify the effectiveness of each component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1"&gt;Munan Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Donghuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1"&gt;Cheng Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chenglang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shuang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM. (arXiv:2108.08095v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08095</id>
        <link href="http://arxiv.org/abs/2108.08095"/>
        <updated>2021-08-19T01:35:02.758Z</updated>
        <summary type="html"><![CDATA[Medical Imaging is one of the growing fields in the world of computer vision.
In this study, we aim to address the Diabetic Retinopathy (DR) problem as one
of the open challenges in medical imaging. In this research, we propose a new
lesion detection architecture, comprising of two sub-modules, which is an
optimal solution to detect and find not only the type of lesions caused by DR,
their corresponding bounding boxes, and their masks; but also the severity
level of the overall case. Aside from traditional accuracy, we also use two
popular evaluation criteria to evaluate the outputs of our models, which are
intersection over union (IOU) and mean average precision (mAP). We hypothesize
that this new solution enables specialists to detect lesions with high
confidence and estimate the severity of the damage with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1"&gt;Farzan Shenavarmasouleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohammadi_F/0/1/0/all/0/1"&gt;Farid Ghareh Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amini_M/0/1/0/all/0/1"&gt;M. Hadi Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Taha_T/0/1/0/all/0/1"&gt;Thiab Taha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rasheed_K/0/1/0/all/0/1"&gt;Khaled Rasheed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arabnia_H/0/1/0/all/0/1"&gt;Hamid R. Arabnia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models. (arXiv:2108.08111v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08111</id>
        <link href="http://arxiv.org/abs/2108.08111"/>
        <updated>2021-08-19T01:35:02.750Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of generating table captions for scholarly
documents, which often require additional information outside the table. To
this end, we propose a method of retrieving relevant sentences from the paper
body, and feeding the table content as well as the retrieved sentences into
pre-trained language models (e.g. T5 and GPT-2) for generating table captions.
The contributions of this paper are: (1) discussion on the challenges in table
captioning for scholarly documents; (2) development of a dataset DocBank-TB,
which is publicly available; and (3) comparison of caption generation methods
for scholarly documents with different strategies to retrieve relevant
sentences from the paper body. Our experimental results showed that T5 is the
better generation model for this task, as it outperformed GPT-2 in BLEU and
METEOR implying that the generated text are clearer and more precise. Moreover,
inputting relevant sentences matching the row header or whole table is
effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junjie H. Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shinden_K/0/1/0/all/0/1"&gt;Kohei Shinden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1"&gt;Makoto P. Kato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand Hygiene Video Classification Based on Deep Learning. (arXiv:2108.08127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08127</id>
        <link href="http://arxiv.org/abs/2108.08127"/>
        <updated>2021-08-19T01:35:02.742Z</updated>
        <summary type="html"><![CDATA[In this work, an extensive review of literature in the field of gesture
recognition carried out along with the implementation of a simple
classification system for hand hygiene stages based on deep learning solutions.
A subset of robust dataset that consist of handwashing gestures with two hands
as well as one-hand gestures such as linear hand movement utilized. A
pretrained neural network model, RES Net 50, with image net weights used for
the classification of 3 categories: Linear hand movement, rub hands palm to
palm and rub hands with fingers interlaced movement. Correct predictions made
for the first two classes with > 60% accuracy. A complete dataset along with
increased number of classes and training steps will be explored as a future
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1"&gt;Rashmi Bakshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Target Adaptive Context Aggregation for Video Scene Graph Generation. (arXiv:2108.08121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08121</id>
        <link href="http://arxiv.org/abs/2108.08121"/>
        <updated>2021-08-19T01:35:02.736Z</updated>
        <summary type="html"><![CDATA[This paper deals with a challenging task of video scene graph generation
(VidSGG), which could serve as a structured video representation for high-level
understanding tasks. We present a new {\em detect-to-track} paradigm for this
task by decoupling the context modeling for relation prediction from the
complicated low-level entity tracking. Specifically, we design an efficient
method for frame-level VidSGG, termed as {\em Target Adaptive Context
Aggregation Network} (TRACE), with a focus on capturing spatio-temporal context
information for relation recognition. Our TRACE framework streamlines the
VidSGG pipeline with a modular design, and presents two unique blocks of
Hierarchical Relation Tree (HRTree) construction and Target-adaptive Context
Aggregation. More specific, our HRTree first provides an adpative structure for
organizing possible relation candidates efficiently, and guides context
aggregation module to effectively capture spatio-temporal structure
information. Then, we obtain a contextualized feature representation for each
relation candidate and build a classification head to recognize its relation
category. Finally, we provide a simple temporal association strategy to track
TRACE detected results to yield the video-level VidSGG. We perform experiments
on two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results
demonstrate that our TRACE achieves the state-of-the-art performance. The code
and models are made available at \url{https://github.com/MCG-NJU/TRACE}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yao Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gangshan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Batch Incremental Road Object Detection via Detector Fusion. (arXiv:2108.08048v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08048</id>
        <link href="http://arxiv.org/abs/2108.08048"/>
        <updated>2021-08-19T01:35:02.729Z</updated>
        <summary type="html"><![CDATA[Incremental few-shot learning has emerged as a new and challenging area in
deep learning, whose objective is to train deep learning models using very few
samples of new class data, and none of the old class data. In this work we
tackle the problem of batch incremental few-shot road object detection using
data from the India Driving Dataset (IDD). Our approach, DualFusion, combines
object detectors in a manner that allows us to learn to detect rare objects
with very limited data, all without severely degrading the performance of the
detector on the abundant classes. In the IDD OpenSet incremental few-shot
detection task, we achieve a mAP50 score of 40.0 on the base classes and an
overall mAP50 score of 38.8, both of which are the highest to date. In the COCO
batch incremental few-shot detection task, we achieve a novel AP score of 9.9,
surpassing the state-of-the-art novel class performance on the same by over 6.6
times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tambwekar_A/0/1/0/all/0/1"&gt;Anuj Tambwekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1"&gt;Kshitij Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majee_A/0/1/0/all/0/1"&gt;Anay Majee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1"&gt;Anbumani Subramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Hybrid Self-Prior for Full 3D Mesh Generation. (arXiv:2108.08017v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08017</id>
        <link href="http://arxiv.org/abs/2108.08017"/>
        <updated>2021-08-19T01:35:02.704Z</updated>
        <summary type="html"><![CDATA[We present a deep learning pipeline that leverages network self-prior to
recover a full 3D model consisting of both a triangular mesh and a texture map
from the colored 3D point cloud. Different from previous methods either
exploiting 2D self-prior for image editing or 3D self-prior for pure surface
reconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep
neural networks to significantly improve the geometry quality and produce a
high-resolution texture map, which is typically missing from the output of
commodity-level 3D scanners. In particular, we first generate an initial mesh
using a 3D convolutional neural network with 3D self-prior, and then encode
both 3D information and color information in the 2D UV atlas, which is further
refined by 2D convolutional neural networks with the self-prior. In this way,
both 2D and 3D self-priors are utilized for the mesh and texture recovery.
Experiments show that, without the need of any additional training data, our
method recovers the 3D textured mesh model of high quality from sparse input,
and outperforms the state-of-the-art methods in terms of both the geometry and
texture quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xingkui Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhaopeng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yinda Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Outdoor Architecture Reconstruction by Exploration and Classification. (arXiv:2108.07990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07990</id>
        <link href="http://arxiv.org/abs/2108.07990"/>
        <updated>2021-08-19T01:35:02.694Z</updated>
        <summary type="html"><![CDATA[This paper presents an explore-and-classify framework for structured
architectural reconstruction from an aerial image. Starting from a potentially
imperfect building reconstruction by an existing algorithm, our approach 1)
explores the space of building models by modifying the reconstruction via
heuristic actions; 2) learns to classify the correctness of building models
while generating classification labels based on the ground-truth, and 3)
repeat. At test time, we iterate exploration and classification, seeking for a
result with the best classification score. We evaluate the approach using
initial reconstructions by two baselines and two state-of-the-art
reconstruction algorithms. Qualitative and quantitative evaluations demonstrate
that our approach consistently improves the reconstruction quality from every
initial reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nauata_N/0/1/0/all/0/1"&gt;Nelson Nauata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1"&gt;Yasutaka Furukawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased IoU for Spherical Image Object Detection. (arXiv:2108.08029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08029</id>
        <link href="http://arxiv.org/abs/2108.08029"/>
        <updated>2021-08-19T01:35:02.684Z</updated>
        <summary type="html"><![CDATA[As one of the most fundamental and challenging problems in computer vision,
object detection tries to locate object instances and find their categories in
natural images. The most important step in the evaluation of object detection
algorithm is calculating the intersection-over-union (IoU) between the
predicted bounding box and the ground truth one. Although this procedure is
well-defined and solved for planar images, it is not easy for spherical image
object detection. Existing methods either compute the IoUs based on biased
bounding box representations or make excessive approximations, thus would give
incorrect results. In this paper, we first identify that spherical rectangles
are unbiased bounding boxes for objects in spherical images, and then propose
an analytical method for IoU calculation without any approximations. Based on
the unbiased representation and calculation, we also present an anchor free
object detection algorithm for spherical images. The experiments on two
spherical object detection datasets show that the proposed method can achieve
better performance than existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yike Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaodong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bailan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_F/0/1/0/all/0/1"&gt;Feng Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting. (arXiv:2108.08023v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08023</id>
        <link href="http://arxiv.org/abs/2108.08023"/>
        <updated>2021-08-19T01:35:02.677Z</updated>
        <summary type="html"><![CDATA[In crowd counting, due to the problem of laborious labelling, it is perceived
intractability of collecting a new large-scale dataset which has plentiful
images with large diversity in density, scene, etc. Thus, for learning a
general model, training with data from multiple different datasets might be a
remedy and be of great value. In this paper, we resort to the multi-domain
joint learning and propose a simple but effective Domain-specific Knowledge
Propagating Network (DKPNet)1 for unbiasedly learning the knowledge from
multiple diverse data domains at the same time. It is mainly achieved by
proposing the novel Variational Attention(VA) technique for explicitly modeling
the attention distributions for different domains. And as an extension to VA,
Intrinsic Variational Attention(InVA) is proposed to handle the problems of
over-lapped domains and sub-domains. Extensive experiments have been conducted
to validate the superiority of our DKPNet over several popular datasets,
including ShanghaiTech A/B, UCF-QNRF and NWPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Binghui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhaoyi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pengyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Biao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Image Generation with Infinite Generative Adversarial Networks. (arXiv:2108.07975v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07975</id>
        <link href="http://arxiv.org/abs/2108.07975"/>
        <updated>2021-08-19T01:35:02.652Z</updated>
        <summary type="html"><![CDATA[Image generation has been heavily investigated in computer vision, where one
core research challenge is to generate images from arbitrarily complex
distributions with little supervision. Generative Adversarial Networks (GANs)
as an implicit approach have achieved great successes in this direction and
therefore been employed widely. However, GANs are known to suffer from issues
such as mode collapse, non-structured latent space, being unable to compute
likelihoods, etc. In this paper, we propose a new unsupervised non-parametric
method named mixture of infinite conditional GANs or MIC-GANs, to tackle
several GAN issues together, aiming for image generation with parsimonious
prior knowledge. Through comprehensive evaluations across different datasets,
we show that MIC-GANs are effective in structuring the latent space and
avoiding mode collapse, and outperform state-of-the-art methods. MICGANs are
adaptive, versatile, and robust. They offer a promising solution to several
well-known GAN issues. Code available: github.com/yinghdb/MICGANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1"&gt;Hui Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1"&gt;Tianjia Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Panoramic Depth Estimation via Supervised and Unsupervised Learning in Indoor Scenes. (arXiv:2108.08076v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08076</id>
        <link href="http://arxiv.org/abs/2108.08076"/>
        <updated>2021-08-19T01:35:02.631Z</updated>
        <summary type="html"><![CDATA[Depth estimation, as a necessary clue to convert 2D images into the 3D space,
has been applied in many machine vision areas. However, to achieve an entire
surrounding 360-degree geometric sensing, traditional stereo matching
algorithms for depth estimation are limited due to large noise, low accuracy,
and strict requirements for multi-camera calibration. In this work, for a
unified surrounding perception, we introduce panoramic images to obtain larger
field of view. We extend PADENet first appeared in our previous conference work
for outdoor scene understanding, to perform panoramic monocular depth
estimation with a focus for indoor scenes. At the same time, we improve the
training process of the neural network adapted to the characteristics of
panoramic images. In addition, we fuse traditional stereo matching algorithm
with deep learning methods and further improve the accuracy of depth
predictions. With a comprehensive variety of experiments, this research
demonstrates the effectiveness of our schemes aiming for indoor scene
perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Keyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaiwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel-Temporal Attention for First-Person Video Domain Adaptation. (arXiv:2108.07846v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07846</id>
        <link href="http://arxiv.org/abs/2108.07846"/>
        <updated>2021-08-19T01:35:02.625Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) can transfer knowledge from labeled
source data to unlabeled target data of the same categories. However, UDA for
first-person action recognition is an under-explored problem, with lack of
datasets and limited consideration of first-person video characteristics. This
paper focuses on addressing this problem. Firstly, we propose two small-scale
first-person video domain adaptation datasets: ADL$_{small}$ and GTEA-KITCHEN.
Secondly, we introduce channel-temporal attention blocks to capture the
channel-wise and temporal-wise relationships and model their inter-dependencies
important to first-person vision. Finally, we propose a Channel-Temporal
Attention Network (CTAN) to integrate these blocks into existing architectures.
CTAN outperforms baselines on the two proposed datasets and one existing
dataset EPIC$_{cvpr20}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1"&gt;Tao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05045</id>
        <link href="http://arxiv.org/abs/2107.05045"/>
        <updated>2021-08-19T01:35:02.609Z</updated>
        <summary type="html"><![CDATA[Learning from positive and unlabeled (PU) data is an important problem in
various applications. Most of the recent approaches for PU classification
assume that the class-prior (the ratio of positive samples) in the training
unlabeled dataset is identical to that of the test data, which does not hold in
many practical cases. In addition, we usually do not know the class-priors of
the training and test data, thus we have no clue on how to train a classifier
without them. To address these problems, we propose a novel PU classification
method based on density ratio estimation. A notable advantage of our proposed
method is that it does not require the class-priors in the training phase;
class-prior shift is incorporated only in the test phase. We theoretically
justify our proposed method and experimentally demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shota Nakajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynFace: Face Recognition with Synthetic Data. (arXiv:2108.07960v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07960</id>
        <link href="http://arxiv.org/abs/2108.07960"/>
        <updated>2021-08-19T01:35:02.601Z</updated>
        <summary type="html"><![CDATA[With the recent success of deep neural networks, remarkable progress has been
achieved on face recognition. However, collecting large-scale real-world
training data for face recognition has turned out to be challenging, especially
due to the label noise and privacy issues. Meanwhile, existing face recognition
datasets are usually collected from web images, lacking detailed annotations on
attributes (e.g., pose and expression), so the influences of different
attributes on face recognition have been poorly investigated. In this paper, we
address the above-mentioned issues in face recognition using synthetic face
images, i.e., SynFace. Specifically, we first explore the performance gap
between recent state-of-the-art face recognition models trained with synthetic
and real face images. We then analyze the underlying causes behind the
performance gap, e.g., the poor intra-class variations and the domain gap
between synthetic and real face images. Inspired by this, we devise the SynFace
with identity mixup (IM) and domain mixup (DM) to mitigate the above
performance gap, demonstrating the great potentials of synthetic data for face
recognition. Furthermore, with the controllable face synthesis model, we can
easily manage different factors of synthetic face generation, including pose,
expression, illumination, the number of identities, and samples per identity.
Therefore, we also perform a systematically empirical analysis on synthetic
face images to provide some insights on how to effectively utilize synthetic
data for face recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Haibo Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1"&gt;Dihong Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WRICNet:A Weighted Rich-scale Inception Coder Network for Multi-Resolution Remote Sensing Image Change Detection. (arXiv:2108.07955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07955</id>
        <link href="http://arxiv.org/abs/2108.07955"/>
        <updated>2021-08-19T01:35:02.589Z</updated>
        <summary type="html"><![CDATA[Majority models of remote sensing image changing detection can only get great
effect in a specific resolution data set. With the purpose of improving change
detection effectiveness of the model in the multi-resolution data set, a
weighted rich-scale inception coder network (WRICNet) is proposed in this
article, which can make a great fusion of shallow multi-scale features, and
deep multi-scale features. The weighted rich-scale inception module of the
proposed can obtain shallow multi-scale features, the weighted rich-scale coder
module can obtain deep multi-scale features. The weighted scale block assigns
appropriate weights to features of different scales, which can strengthen
expressive ability of the edge of the changing area. The performance
experiments on the multi-resolution data set demonstrate that, compared to the
comparative methods, the proposed can further reduce the false alarm outside
the change area, and the missed alarm in the change area, besides, the edge of
the change area is more accurate. The ablation study of the proposed shows that
the training strategy, and improvements of this article can improve the
effectiveness of change detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongmei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Convolutions with Per-pixel Dynamic Filter Atom. (arXiv:2108.07895v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07895</id>
        <link href="http://arxiv.org/abs/2108.07895"/>
        <updated>2021-08-19T01:35:02.579Z</updated>
        <summary type="html"><![CDATA[Applying feature dependent network weights have been proved to be effective
in many fields. However, in practice, restricted by the enormous size of model
parameters and memory footprints, scalable and versatile dynamic convolutions
with per-pixel adapted filters are yet to be fully explored. In this paper, we
address this challenge by decomposing filters, adapted to each spatial
position, over dynamic filter atoms generated by a light-weight network from
local features. Adaptive receptive fields can be supported by further
representing each filter atom over sets of pre-fixed multi-scale bases. As
plug-and-play replacements to convolutional layers, the introduced adaptive
convolutions with per-pixel dynamic atoms enable explicit modeling of
intra-image variance, while avoiding heavy computation, parameters, and memory
cost. Our method preserves the appealing properties of conventional
convolutions as being translation-equivariant and parametrically efficient. We
present experiments to show that, the proposed method delivers comparable or
even better performance across tasks, and are particularly effective on
handling tasks with significant intra-image variance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1"&gt;Zichen Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1"&gt;Qiang Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Visual Representations Learning by Contrastive Mask Prediction. (arXiv:2108.07954v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07954</id>
        <link href="http://arxiv.org/abs/2108.07954"/>
        <updated>2021-08-19T01:35:02.549Z</updated>
        <summary type="html"><![CDATA[Advanced self-supervised visual representation learning methods rely on the
instance discrimination (ID) pretext task. We point out that the ID task has an
implicit semantic consistency (SC) assumption, which may not hold in
unconstrained datasets. In this paper, we propose a novel contrastive mask
prediction (CMP) task for visual representation learning and design a mask
contrast (MaskCo) framework to implement the idea. MaskCo contrasts
region-level features instead of view-level features, which makes it possible
to identify the positive sample without any assumptions. To solve the domain
gap between masked and unmasked features, we design a dedicated mask prediction
head in MaskCo. This module is shown to be the key to the success of the CMP.
We evaluated MaskCo on training datasets beyond ImageNet and compare its
performance with MoCo V2. Results show that MaskCo achieves comparable
performance with MoCo V2 using ImageNet training dataset, but demonstrates a
stronger performance across a range of downstream tasks when COCO or Conceptual
Captions are used for training. MaskCo provides a promising alternative to the
ID-based methods for self-supervised learning in the wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yucheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangting Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Chong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15176</id>
        <link href="http://arxiv.org/abs/2105.15176"/>
        <updated>2021-08-19T01:35:02.518Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence models provide a viable new approach to generative
summarization, allowing models that are no longer limited to simply selecting
and recombining sentences from the original text. However, these models have
three drawbacks: their grasp of the details of the original text is often
inaccurate, and the text generated by such models often has repetitions, while
it is difficult to handle words that are beyond the word list. In this paper,
we propose a new architecture that combines reinforcement learning and
adversarial generative networks to enhance the sequence-to-sequence attention
model. First, we use a hybrid pointer-generator network that copies words
directly from the source text, contributing to accurate reproduction of
information without sacrificing the ability of generators to generate new
words. Second, we use both intra-temporal and intra-decoder attention to
penalize summarized content and thus discourage repetition. We apply our model
to our own proposed COVID-19 paper title summarization task and achieve close
approximations to the current model on ROUEG, while bringing better
readability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tianyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunyun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VariTex: Variational Neural Face Textures. (arXiv:2104.05988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05988</id>
        <link href="http://arxiv.org/abs/2104.05988"/>
        <updated>2021-08-19T01:35:02.499Z</updated>
        <summary type="html"><![CDATA[Deep generative models can synthesize photorealistic images of human faces
with novel identities. However, a key challenge to the wide applicability of
such techniques is to provide independent control over semantically meaningful
parameters: appearance, head pose, face shape, and facial expressions. In this
paper, we propose VariTex - to the best of our knowledge the first method that
learns a variational latent feature space of neural face textures, which allows
sampling of novel identities. We combine this generative model with a
parametric face model and gain explicit control over head pose and facial
expressions. To generate complete images of human heads, we propose an additive
decoder that adds plausible details such as hair. A novel training scheme
enforces a pose-independent latent space and in consequence, allows learning a
one-to-many mapping between latent codes and pose-conditioned exterior regions.
The resulting method can generate geometrically consistent images of novel
identities under fine-grained control over head pose, face shape, and facial
expressions. This facilitates a broad range of downstream tasks, like sampling
novel identities, changing the head pose, expression transfer, and more. Code
and models are available for research on https://mcbuehler.github.io/VariTex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1"&gt;Marcel C. B&amp;#xfc;hler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gengyan Li&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1"&gt;Thabo Beeler&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt; (1) ((1) ETH Zurich, (2) Google)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Journey from SDRTV to HDRTV. (arXiv:2108.07978v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07978</id>
        <link href="http://arxiv.org/abs/2108.07978"/>
        <updated>2021-08-19T01:35:02.488Z</updated>
        <summary type="html"><![CDATA[Nowadays modern displays are capable to render video content with high
dynamic range (HDR) and wide color gamut (WCG). However, most available
resources are still in standard dynamic range (SDR). Therefore, there is an
urgent demand to transform existing SDR-TV contents into their HDR-TV versions.
In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the
formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step
solution pipeline including adaptive global color mapping, local enhancement
and highlight generation. Moreover, the above analysis inspires us to present a
lightweight network that utilizes global statistics as guidance to conduct
image-adaptive color mapping. In addition, we construct a dataset using HDR
videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate
the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results
achieve state-of-the-art performance in quantitative comparisons and visual
quality. The code and dataset are available at
https://github.com/chxy95/HDRTVNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jimmy S. Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1"&gt;Lynhoo Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task learning for jersey number recognition in Ice Hockey. (arXiv:2108.07848v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07848</id>
        <link href="http://arxiv.org/abs/2108.07848"/>
        <updated>2021-08-19T01:35:02.466Z</updated>
        <summary type="html"><![CDATA[Identifying players in sports videos by recognizing their jersey numbers is a
challenging task in computer vision. We have designed and implemented a
multi-task learning network for jersey number recognition. In order to train a
network to recognize jersey numbers, two output label representations are used
(1) Holistic - considers the entire jersey number as one class, and (2)
Digit-wise - considers the two digits in a jersey number as two separate
classes. The proposed network learns both holistic and digit-wise
representations through a multi-task loss function. We determine the optimal
weights to be assigned to holistic and digit-wise losses through an ablation
study. Experimental results demonstrate that the proposed multi-task learning
network performs better than the constituent holistic and digit-wise
single-task learning networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1"&gt;Kanav Vats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fani_M/0/1/0/all/0/1"&gt;Mehrnaz Fani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1"&gt;David A. Clausi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1"&gt;John Zelek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amplitude Mean of Functional Data on $\mathbb{S}^2$. (arXiv:2107.13721v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13721</id>
        <link href="http://arxiv.org/abs/2107.13721"/>
        <updated>2021-08-19T01:35:02.460Z</updated>
        <summary type="html"><![CDATA[Manifold-valued functional data analysis (FDA) recently becomes an active
area of research motivated by the raising availability of trajectories or
longitudinal data observed on non-linear manifolds. The challenges of analyzing
such data come from many aspects, including infinite dimensionality and
nonlinearity, as well as time-domain or phase variability. In this paper, we
study the amplitude part of manifold-valued functions on $\mathbb{S}^2$, which
is invariant to random time warping or re-parameterization. Utilizing the nice
geometry of $\mathbb{S}^2$, we develop a set of efficient and accurate tools
for temporal alignment of functions, geodesic computing, and sample mean
calculation. At the heart of these tools, they rely on gradient descent
algorithms with carefully derived gradients. We show the advantages of these
newly developed tools over its competitors with extensive simulations and real
data and demonstrate the importance of considering the amplitude part of
functions instead of mixing it with phase variability in manifold-valued FDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengwu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1"&gt;Bayan Saparbayeva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OncoPetNet: A Deep Learning based AI system for mitotic figure counting on H&E stained whole slide digital images in a large veterinary diagnostic lab setting. (arXiv:2108.07856v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07856</id>
        <link href="http://arxiv.org/abs/2108.07856"/>
        <updated>2021-08-19T01:35:02.445Z</updated>
        <summary type="html"><![CDATA[Background: Histopathology is an important modality for the diagnosis and
management of many diseases in modern healthcare, and plays a critical role in
cancer care. Pathology samples can be large and require multi-site sampling,
leading to upwards of 20 slides for a single tumor, and the human-expert tasks
of site selection and and quantitative assessment of mitotic figures are time
consuming and subjective. Automating these tasks in the setting of a digital
pathology service presents significant opportunities to improve workflow
efficiency and augment human experts in practice. Approach: Multiple
state-of-the-art deep learning techniques for histopathology image
classification and mitotic figure detection were used in the development of
OncoPetNet. Additionally, model-free approaches were used to increase speed and
accuracy. The robust and scalable inference engine leverages Pytorch's
performance optimizations as well as specifically developed speed up techniques
in inference. Results: The proposed system, demonstrated significantly improved
mitotic counting performance for 41 cancer cases across 14 cancer types
compared to human expert baselines. In 21.9% of cases use of OncoPetNet led to
change in tumor grading compared to human expert evaluation. In deployment, an
effective 0.27 min/slide inference was achieved in a high throughput veterinary
diagnostic pathology service across 2 centers processing 3,323 digital whole
slide images daily. Conclusion: This work represents the first successful
automated deployment of deep learning systems for real-time expert-level
performance on important histopathology tasks at scale in a high volume
clinical practice. The resulting impact outlines important considerations for
model development, deployment, clinical decision making, and informs best
practices for implementation of deep learning systems in digital histopathology
practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fitzke_M/0/1/0/all/0/1"&gt;Michael Fitzke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Whitley_D/0/1/0/all/0/1"&gt;Derick Whitley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yau_W/0/1/0/all/0/1"&gt;Wilson Yau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1"&gt;Fernando Rodrigues Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fadeev_V/0/1/0/all/0/1"&gt;Vladimir Fadeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bacmeister_C/0/1/0/all/0/1"&gt;Cindy Bacmeister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Carter_C/0/1/0/all/0/1"&gt;Chris Carter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1"&gt;Jeffrey Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parkinson_M/0/1/0/all/0/1"&gt;Mark Parkinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing gradients by exploiting temporal correlation in momentum-SGD. (arXiv:2108.07827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07827</id>
        <link href="http://arxiv.org/abs/2108.07827"/>
        <updated>2021-08-19T01:35:02.434Z</updated>
        <summary type="html"><![CDATA[An increasing bottleneck in decentralized optimization is communication.
Bigger models and growing datasets mean that decentralization of computation is
important and that the amount of information exchanged is quickly growing.
While compression techniques have been introduced to cope with the latter, none
has considered leveraging the temporal correlations that exist in consecutive
vector updates. An important example is distributed momentum-SGD where temporal
correlation is enhanced by the low-pass-filtering effect of applying momentum.
In this paper we design and analyze compression methods that exploit temporal
correlation in systems both with and without error-feedback. Experiments with
the ImageNet dataset demonstrate that our proposed methods offer significant
reduction in the rate of communication at only a negligible increase in
computation complexity. We further analyze the convergence of SGD when
compression is applied with error-feedback. In the literature, convergence
guarantees are developed only for compressors that provide error-bounds
point-wise, i.e., for each input to the compressor. In contrast, many important
codes (e.g. rate-distortion codes) provide error-bounds only in expectation and
thus provide a more general guarantee. In this paper we prove the convergence
of SGD under an expected error assumption by establishing a bound for the
minimum gradient norm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adikari_T/0/1/0/all/0/1"&gt;Tharindu B. Adikari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Draper_S/0/1/0/all/0/1"&gt;Stark C. Draper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schr\"{o}dinger PCA: On the Duality between Principal Component Analysis and Schr\"{o}dinger Equation. (arXiv:2006.04379v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04379</id>
        <link href="http://arxiv.org/abs/2006.04379"/>
        <updated>2021-08-19T01:35:02.420Z</updated>
        <summary type="html"><![CDATA[Principal component analysis (PCA) has achieved great success in unsupervised
learning by identifying covariance correlations among features. If the data
collection fails to capture the covariance information, PCA will not be able to
discover meaningful modes. In particular, PCA will fail the spatial Gaussian
Process (GP) model in the undersampling regime, i.e. the averaged distance of
neighboring anchor points (spatial features) is greater than the correlation
length of GP. Counterintuitively, by drawing the connection between PCA and
Schr\"odinger equation, we can not only attack the undersampling challenge but
also compute in an efficient and decoupled way with the proposed algorithm
called Schr\"odinger PCA. Our algorithm only requires variances of features and
estimated correlation length as input, constructs the corresponding
Schr\"odinger equation, and solves it to obtain the energy eigenstates, which
coincide with principal components. We will also establish the connection of
our algorithm to the model reduction techniques in the partial differential
equation (PDE) community, where the steady-state Schr\"odinger operator is
identified as a second-order approximation to the covariance function.
Numerical experiments are implemented to testify the validity and efficiency of
the proposed algorithm, showing its potential for unsupervised learning tasks
on general graphs and manifolds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Qian_S/0/1/0/all/0/1"&gt;Sitian Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuxuan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianyi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Warp Consistency for Unsupervised Learning of Dense Correspondences. (arXiv:2104.03308v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03308</id>
        <link href="http://arxiv.org/abs/2104.03308"/>
        <updated>2021-08-19T01:35:02.399Z</updated>
        <summary type="html"><![CDATA[The key challenge in learning dense correspondences lies in the lack of
ground-truth matches for real image pairs. While photometric consistency losses
provide unsupervised alternatives, they struggle with large appearance changes,
which are ubiquitous in geometric and semantic matching tasks. Moreover,
methods relying on synthetic training pairs often suffer from poor
generalisation to real data.

We propose Warp Consistency, an unsupervised learning objective for dense
correspondence regression. Our objective is effective even in settings with
large appearance and view-point changes. Given a pair of real images, we first
construct an image triplet by applying a randomly sampled warp to one of the
original images. We derive and analyze all flow-consistency constraints arising
between the triplet. From our observations and empirical results, we design a
general unsupervised objective employing two of the derived constraints. We
validate our warp consistency loss by training three recent dense
correspondence networks for the geometric and semantic matching tasks. Our
approach sets a new state-of-the-art on several challenging benchmarks,
including MegaDepth, RobotCar and TSS. Code and models are at
github.com/PruneTruong/DenseMatching.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1"&gt;Prune Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13467</id>
        <link href="http://arxiv.org/abs/2107.13467"/>
        <updated>2021-08-19T01:35:02.392Z</updated>
        <summary type="html"><![CDATA[The unsupervised domain adaptation (UDA) has been widely adopted to alleviate
the data scalability issue, while the existing works usually focus on
classifying independently discrete labels. However, in many tasks (e.g.,
medical diagnosis), the labels are discrete and successively distributed. The
UDA for ordinal classification requires inducing non-trivial ordinal
distribution prior to the latent space. Target for this, the partially ordered
set (poset) is defined for constraining the latent vector. Instead of the
typically i.i.d. Gaussian latent prior, in this work, a recursively conditional
Gaussian (RCG) set is adapted for ordered constraint modeling, which admits a
tractable joint distribution prior. Furthermore, we are able to control the
density of content vector that violates the poset constraints by a simple
"three-sigma rule". We explicitly disentangle the cross-domain images into a
shared ordinal prior induced ordinal content space and two separate
source/target ordinal-unrelated spaces, and the self-training is worked on the
shared space exclusively for ordinal-aware domain alignment. Extensive
experiments on UDA medical diagnoses and facial age estimation demonstrate its
effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yubin Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1"&gt;Pengyi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Focus for Efficient Video Recognition. (arXiv:2105.03245v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03245</id>
        <link href="http://arxiv.org/abs/2105.03245"/>
        <updated>2021-08-19T01:35:02.383Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the spatial redundancy in video recognition with
the aim to improve the computational efficiency. It is observed that the most
informative region in each frame of a video is usually a small image patch,
which shifts smoothly across frames. Therefore, we model the patch localization
problem as a sequential decision task, and propose a reinforcement learning
based approach for efficient spatially adaptive video recognition (AdaFocus).
In specific, a light-weighted ConvNet is first adopted to quickly process the
full video sequence, whose features are used by a recurrent policy network to
localize the most task-relevant regions. Then the selected patches are inferred
by a high-capacity network for the final prediction. During offline inference,
once the informative patch sequence has been generated, the bulk of computation
can be done in parallel, and is efficient on modern GPU devices. In addition,
we demonstrate that the proposed method can be easily extended by further
considering the temporal redundancy, e.g., dynamically skipping less valuable
frames. Extensive experiments on five benchmark datasets, i.e., ActivityNet,
FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is
significantly more efficient than the competitive baselines. Code is available
at https://github.com/blackfeather-wang/AdaFocus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhaoxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haojun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yizeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving. (arXiv:2108.08019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08019</id>
        <link href="http://arxiv.org/abs/2108.08019"/>
        <updated>2021-08-19T01:35:02.376Z</updated>
        <summary type="html"><![CDATA[Predictor-based algorithms have achieved remarkable performance in the Neural
Architecture Search (NAS) tasks. However, these methods suffer from high
computation costs, as training the performance predictor usually requires
training and evaluating hundreds of architectures from scratch. Previous works
along this line mainly focus on reducing the number of architectures required
to fit the predictor. In this work, we tackle this challenge from a different
perspective - improve search efficiency by cutting down the computation budget
of architecture training. We propose NOn-uniform Successive Halving (NOSH), a
hierarchical scheduling algorithm that terminates the training of
underperforming architectures early to avoid wasting budget. To effectively
leverage the non-uniform supervision signals produced by NOSH, we formulate
predictor-based architecture search as learning to rank with pairwise
comparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x
while achieving competitive or even better performance than previous
state-of-the-art predictor-based methods on various spaces and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruochen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Minhao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deployment of Deep Neural Networks for Object Detection on Edge AI Devices with Runtime Optimization. (arXiv:2108.08166v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08166</id>
        <link href="http://arxiv.org/abs/2108.08166"/>
        <updated>2021-08-19T01:35:02.369Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have proven increasingly important for automotive scene
understanding with new algorithms offering constant improvements of the
detection performance. However, there is little emphasis on experiences and
needs for deployment in embedded environments. We therefore perform a case
study of the deployment of two representative object detection networks on an
edge AI platform. In particular, we consider RetinaNet for image-based 2D
object detection and PointPillars for LiDAR-based 3D object detection. We
describe the modifications necessary to convert the algorithms from a PyTorch
training environment to the deployment environment taking into account the
available tools. We evaluate the runtime of the deployed DNN using two
different libraries, TensorRT and TorchScript. In our experiments, we observe
slight advantages of TensorRT for convolutional layers and TorchScript for
fully connected layers. We also study the trade-off between runtime and
performance, when selecting an optimized setup for deployment, and observe that
quantization significantly reduces the runtime while having only little impact
on the detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stacker_L/0/1/0/all/0/1"&gt;Lukas St&amp;#xe4;cker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1"&gt;Juncong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1"&gt;Philipp Heidenreich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonarens_F/0/1/0/all/0/1"&gt;Frank Bonarens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1"&gt;Jason Rambach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1"&gt;Didier Stricker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1"&gt;Christoph Stiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of Parameter Optimization on Classical and Learning-based Image Matching Methods. (arXiv:2108.08179v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08179</id>
        <link href="http://arxiv.org/abs/2108.08179"/>
        <updated>2021-08-19T01:35:02.364Z</updated>
        <summary type="html"><![CDATA[Deep learning-based image matching methods are improved significantly during
the recent years. Although these methods are reported to outperform the
classical techniques, the performance of the classical methods is not examined
in detail. In this study, we compare classical and learning-based methods by
employing mutual nearest neighbor search with ratio test and optimizing the
ratio test threshold to achieve the best performance on two different
performance metrics. After a fair comparison, the experimental results on
HPatches dataset reveal that the performance gap between classical and
learning-based methods is not that significant. Throughout the experiments, we
demonstrated that SuperGlue is the state-of-the-art technique for the image
matching problem on HPatches dataset. However, if a single parameter, namely
ratio test threshold, is carefully optimized, a well-known traditional method
SIFT performs quite close to SuperGlue and even outperforms in terms of mean
matching accuracy (MMA) under 1 and 2 pixel thresholds. Moreover, a recent
approach, DFM, which only uses pre-trained VGG features as descriptors and
ratio test, is shown to outperform most of the well-trained learning-based
methods. Therefore, we conclude that the parameters of any classical method
should be analyzed carefully before comparing against a learning-based
technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1"&gt;Ufuk Efe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1"&gt;Kutalmis Gokalp Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activity Recognition for Autism Diagnosis. (arXiv:2108.07917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07917</id>
        <link href="http://arxiv.org/abs/2108.07917"/>
        <updated>2021-08-19T01:35:02.339Z</updated>
        <summary type="html"><![CDATA[A formal autism diagnosis is an inefficient and lengthy process. Families
often have to wait years before receiving a diagnosis for their child; some may
not receive one at all due to this delay. One approach to this problem is to
use digital technologies to detect the presence of behaviors related to autism,
which in aggregate may lead to remote and automated diagnostics. One of the
strongest indicators of autism is stimming, which is a set of repetitive,
self-stimulatory behaviors such as hand flapping, headbanging, and spinning.
Using computer vision to detect hand flapping is especially difficult due to
the sparsity of public training data in this space and excessive shakiness and
motion in such data. Our work demonstrates a novel method that overcomes these
issues: we use hand landmark detection over time as a feature representation
which is then fed into a Long Short-Term Memory (LSTM) model. We achieve a
validation accuracy and F1 Score of about 72% on detecting whether videos from
the Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our
best model also predicts accurately on external videos we recorded of ourselves
outside of the dataset it was trained on. This model uses less than 26,000
parameters, providing promise for fast deployment into ubiquitous and wearable
digital settings for a remote autism diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1"&gt;Anish Lakkapragada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1"&gt;Peter Washington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1"&gt;Dennis Wall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-08-19T01:35:02.331Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08195</id>
        <link href="http://arxiv.org/abs/2108.08195"/>
        <updated>2021-08-19T01:35:02.322Z</updated>
        <summary type="html"><![CDATA[Due to morphological similarity at the microscopic level, making an accurate
and time-sensitive distinction between blood cells affected by Acute
Lymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage
of machine learning architectures. However, three of the most common models,
VGG, ResNet, and Inception, each come with their own set of flaws with room for
improvement which demands the need for a superior model. ALLNet, the proposed
hybrid convolutional neural network architecture, consists of a combination of
the VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019
(available here) contains 10,691 images of white blood cells which were used to
train and test the models. 7,272 of the images in the dataset are of cells with
ALL and 3,419 of them are of healthy cells. Of the images, 60% were used to
train the model, 20% were used for the cross-validation set, and 20% were used
for the test set. ALLNet outperformed the VGG, ResNet, and the Inception models
across the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,
a specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803
in the cross-validation set. In the test set, ALLNet achieved an accuracy of
92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of
0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the
clinical workspace can better treat the thousands of people suffering from ALL
across the world, many of whom are children.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1"&gt;Sai Mattapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1"&gt;Rishi Athavale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thermal Image Processing via Physics-Inspired Deep Networks. (arXiv:2108.07973v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07973</id>
        <link href="http://arxiv.org/abs/2108.07973"/>
        <updated>2021-08-19T01:35:02.313Z</updated>
        <summary type="html"><![CDATA[We introduce DeepIR, a new thermal image processing framework that combines
physically accurate sensor modeling with deep network-based image
representation. Our key enabling observations are that the images captured by
thermal sensors can be factored into slowly changing, scene-independent sensor
non-uniformities (that can be accurately modeled using physics) and a
scene-specific radiance flux (that is well-represented using a deep
network-based regularizer). DeepIR requires neither training data nor periodic
ground-truth calibration with a known black body target--making it well suited
for practical computer vision tasks. We demonstrate the power of going DeepIR
by developing new denoising and super-resolution algorithms that exploit
multiple images of the scene captured with camera jitter. Simulated and real
data experiments demonstrate that DeepIR can perform high-quality
non-uniformity correction with as few as three images, achieving a 10dB PSNR
improvement over competing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saragadam_V/0/1/0/all/0/1"&gt;Vishwanath Saragadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dave_A/0/1/0/all/0/1"&gt;Akshat Dave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1"&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ME-PCN: Point Completion Conditioned on Mask Emptiness. (arXiv:2108.08187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08187</id>
        <link href="http://arxiv.org/abs/2108.08187"/>
        <updated>2021-08-19T01:35:02.303Z</updated>
        <summary type="html"><![CDATA[Point completion refers to completing the missing geometries of an object
from incomplete observations. Main-stream methods predict the missing shapes by
decoding a global feature learned from the input point cloud, which often leads
to deficient results in preserving topology consistency and surface details. In
this work, we present ME-PCN, a point completion network that leverages
`emptiness' in 3D shape space. Given a single depth scan, previous methods
often encode the occupied partial shapes while ignoring the empty regions (e.g.
holes) in depth maps. In contrast, we argue that these `emptiness' clues
indicate shape boundaries that can be used to improve topology representation
and detail granularity on surfaces. Specifically, our ME-PCN encodes both the
occupied point cloud and the neighboring `empty points'. It estimates
coarse-grained but complete and reasonable surface points in the first stage,
followed by a refinement stage to produce fine-grained surface details.
Comprehensive experiments verify that our ME-PCN presents better qualitative
and quantitative performance against the state-of-the-art. Besides, we further
prove that our `emptiness' design is lightweight and easy to embed in existing
methods, which shows consistent effectiveness in improving the CD and EMD
scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Bingchen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yinyu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yiqun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning. (arXiv:2108.07938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07938</id>
        <link href="http://arxiv.org/abs/2108.07938"/>
        <updated>2021-08-19T01:35:02.276Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a talking face generation method that takes an
audio signal as input and a short target video clip as reference, and
synthesizes a photo-realistic video of the target face with natural lip
motions, head poses, and eye blinks that are in-sync with the input audio
signal. We note that the synthetic face attributes include not only explicit
ones such as lip motions that have high correlations with speech, but also
implicit ones such as head poses and eye blinks that have only weak correlation
with the input audio. To model such complicated relationships among different
face attributes with input audio, we propose a FACe Implicit Attribute Learning
Generative Adversarial Network (FACIAL-GAN), which integrates the
phonetics-aware, context-aware, and identity-aware information to synthesize
the 3D face animation with realistic motions of lips, head poses, and eye
blinks. Then, our Rendering-to-Video network takes the rendered face images and
the attention map of eye blinks as input to generate the photo-realistic output
video frames. Experimental results and user studies show our method can
generate realistic talking face videos with not only synchronized lip motions,
but also natural head movements and eye blinks, with better qualities than the
results of state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chenxu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yifan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Ming Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1"&gt;Saifeng Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Budagavi_M/0/1/0/all/0/1"&gt;Madhukar Budagavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaohu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective and scalable clustering of SARS-CoV-2 sequences. (arXiv:2108.08143v1 [q-bio.PE])]]></title>
        <id>http://arxiv.org/abs/2108.08143</id>
        <link href="http://arxiv.org/abs/2108.08143"/>
        <updated>2021-08-19T01:35:02.269Z</updated>
        <summary type="html"><![CDATA[SARS-CoV-2, like any other virus, continues to mutate as it spreads,
according to an evolutionary process. Unlike any other virus, the number of
currently available sequences of SARS-CoV-2 in public databases such as GISAID
is already several million. This amount of data has the potential to uncover
the evolutionary dynamics of a virus like never before. However, a million is
already several orders of magnitude beyond what can be processed by the
traditional methods designed to reconstruct a virus's evolutionary history,
such as those that build a phylogenetic tree. Hence, new and scalable methods
will need to be devised in order to make use of the ever increasing number of
viral sequences being collected.

Since identifying variants is an important part of understanding the
evolution of a virus, in this paper, we propose an approach based on clustering
sequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer
based feature vector generation and efficient feature selection methods, our
approach is effective in identifying variants, as well as being efficient and
scalable to millions of sequences. Such a clustering method allows us to show
the relative proportion of each variant over time, giving the rate of spread of
each variant in different locations -- something which is important for vaccine
development and distribution. We also compute the importance of each amino acid
position of the spike protein in identifying a given variant in terms of
information gain. Positions of high variant-specific importance tend to agree
with those reported by the USA's Centers for Disease Control and Prevention
(CDC), further demonstrating our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sarwan Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Tamkanat-E-Ali/0/1/0/all/0/1"&gt;Tamkanat-E-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Khan_M/0/1/0/all/0/1"&gt;Muhammad Asad Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1"&gt;Imdadullah Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1"&gt;Murray Patterson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural Networks: A Performance Benchmark. (arXiv:2103.13933v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13933</id>
        <link href="http://arxiv.org/abs/2103.13933"/>
        <updated>2021-08-19T01:35:02.262Z</updated>
        <summary type="html"><![CDATA[Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due
to both negligent and malicious use. For this reason, the automated detection
and tracking of UAV is a fundamental task in aerial security systems. Common
technologies for UAV detection include visible-band and thermal infrared
imaging, radio frequency and radar. Recent advances in deep neural networks
(DNNs) for image-based object detection open the possibility to use visual
information for this detection and tracking task. Furthermore, these detection
architectures can be implemented as backbones for visual tracking systems,
thereby enabling persistent tracking of UAV incursions. To date, no
comprehensive performance benchmark exists that applies DNNs to visible-band
imagery for UAV detection and tracking. To this end, three datasets with varied
environmental conditions for UAV detection and tracking, comprising a total of
241 videos (331,486 images), are assessed using four detection architectures
and three tracking frameworks. The best performing detector architecture
obtains an mAP of 98.6% and the best performing tracking framework obtains a
MOTA of 96.3%. Cross-modality evaluation is carried out between visible and
infrared spectrums, achieving a maximal 82.8% mAP on visible images when
training in the infrared modality. These results provide the first public
multi-approach benchmark for state-of-the-art deep learning-based methods and
give insight into which detection and tracking architectures are effective in
the UAV domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isaac_Medina_B/0/1/0/all/0/1"&gt;Brian K. S. Isaac-Medina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poyser_M/0/1/0/all/0/1"&gt;Matt Poyser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1"&gt;Daniel Organisciak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1"&gt;Chris G. Willcocks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1"&gt;Toby P. Breckon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v5 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.13216</id>
        <link href="http://arxiv.org/abs/1904.13216"/>
        <updated>2021-08-19T01:35:02.254Z</updated>
        <summary type="html"><![CDATA[Deep learning has revolutionized computer vision utilizing the increased
availability of big data and the power of parallel computational units such as
graphical processing units. The vast majority of deep learning research is
conducted using images as training data, however the biomedical domain is rich
in physiological signals that are used for diagnosis and prediction problems.
It is still an open research question how to best utilize signals to train deep
neural networks.

In this paper we define the term Signal2Image (S2Is) as trainable or
non-trainable prefix modules that convert signals, such as
Electroencephalography (EEG), to image-like representations making them
suitable for training image-based deep neural networks defined as `base
models'. We compare the accuracy and time performance of four S2Is (`signal as
image', spectrogram, one and two layer Convolutional Neural Networks (CNNs))
combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)
along with the depth-wise and 1D variations of the latter. We also provide
empirical evidence that the one layer CNN S2I performs better in eleven out of
fifteen tested models than non-trainable S2Is for classifying EEG signals and
we present visual comparisons of the outputs of the S2Is.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1"&gt;Paschalis Bizopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1"&gt;George I Lambrou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1"&gt;Dimitrios Koutsouris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08264</id>
        <link href="http://arxiv.org/abs/2108.08264"/>
        <updated>2021-08-19T01:35:02.246Z</updated>
        <summary type="html"><![CDATA[Expert systems have been used to enable computers to make recommendations and
decisions. This paper presents the use of a machine learning trained expert
system (MLES) for phishing site detection and fake news detection. Both topics
share a similar goal: to design a rule-fact network that allows a computer to
make explainable decisions like domain experts in each respective area. The
phishing website detection study uses a MLES to detect potential phishing
websites by analyzing site properties (like URL length and expiration time).
The fake news detection study uses a MLES rule-fact network to gauge news story
truthfulness based on factors such as emotion, the speaker's political
affiliation status, and job. The two studies use different MLES network
implementations, which are presented and compared herein. The fake news study
utilized a more linear design while the phishing project utilized a more
complex connection structure. Both networks' inputs are based on commonly
available data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1"&gt;Benjamin Fitzpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xinyu &amp;quot;Sherwin&amp;quot; Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1"&gt;Jeremy Straub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency. (arXiv:2107.11355v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11355</id>
        <link href="http://arxiv.org/abs/2107.11355"/>
        <updated>2021-08-19T01:35:02.167Z</updated>
        <summary type="html"><![CDATA[Deep learning-based 3D object detection has achieved unprecedented success
with the advent of large-scale autonomous driving datasets. However, drastic
performance degradation remains a critical challenge for cross-domain
deployment. In addition, existing 3D domain adaptive detection methods often
assume prior access to the target domain annotations, which is rarely feasible
in the real world. To address this challenge, we study a more realistic
setting, unsupervised 3D domain adaptive detection, which only utilizes source
domain annotations. 1) We first comprehensively investigate the major
underlying factors of the domain gap in 3D detection. Our key insight is that
geometric mismatch is the key factor of domain shift. 2) Then, we propose a
novel and unified framework, Multi-Level Consistency Network (MLC-Net), which
employs a teacher-student paradigm to generate adaptive and reliable
pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level
consistency to facilitate cross-domain transfer. Extensive experiments
demonstrate that MLC-Net outperforms existing state-of-the-art methods
(including those using additional target domain information) on standard
benchmarks. Notably, our approach is detector-agnostic, which achieves
consistent gains on both single- and two-stage 3D detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhongang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Changqing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gongjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07908</id>
        <link href="http://arxiv.org/abs/2108.07908"/>
        <updated>2021-08-19T01:35:02.159Z</updated>
        <summary type="html"><![CDATA[This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent
Component Analysis ('FastICA') method ('m-ar-K-FastICA') for feature
extraction. The kernel trick has enabled dimensionality reduction techniques to
capture a higher extent of non-linearity in the data; however, reproducible,
open-source kernels to aid with feature extraction are still limited and may
not be reliable when projecting features from entropic data. The m-ar-K
function, freely available in Python and compatible with its open-source
library 'scikit-learn', is hereby coupled with FastICA to achieve more reliable
feature extraction in presence of a high extent of randomness in the data,
reducing the need for pre-whitening. Different classification tasks were
considered, as related to five (N = 5) open access datasets of various degrees
of information entropy, available from scikit-learn and the University
California Irvine (UCI) Machine Learning repository. Experimental results
demonstrate improvements in the classification performance brought by the
proposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction
approach is compared to the 'FastICA' gold standard method, supporting its
higher reliability and computational efficiency, regardless of the underlying
uncertainty in the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1"&gt;Luca Parisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Siamese Networks for One-Shot Intrusion Detection Model. (arXiv:2006.15343v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15343</id>
        <link href="http://arxiv.org/abs/2006.15343"/>
        <updated>2021-08-19T01:35:02.152Z</updated>
        <summary type="html"><![CDATA[The use of supervised Machine Learning (ML) to enhance Intrusion Detection
Systems has been the subject of significant research. Supervised ML is based
upon learning by example, demanding significant volumes of representative
instances for effective training and the need to re-train the model for every
unseen cyber-attack class. However, retraining the models in-situ renders the
network susceptible to attacks owing to the time-window required to acquire a
sufficient volume of data. Although anomaly detection systems provide a
coarse-grained defence against unseen attacks, these approaches are
significantly less accurate and suffer from high false-positive rates. Here, a
complementary approach referred to as 'One-Shot Learning', whereby a limited
number of examples of a new attack-class is used to identify a new attack-class
(out of many) is detailed. The model grants a new cyber-attack classification
without retraining. A Siamese Network is trained to differentiate between
classes based on pairs similarities, rather than features, allowing to identify
new and previously unseen attacks. The performance of a pre-trained model to
classify attack-classes based only on one example is evaluated using three
datasets. Results confirm the adaptability of the model in classifying unseen
attacks and the trade-off between performance and the need for distinctive
class representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hindy_H/0/1/0/all/0/1"&gt;Hanan Hindy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tachtatzis_C/0/1/0/all/0/1"&gt;Christos Tachtatzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkinson_R/0/1/0/all/0/1"&gt;Robert Atkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brosset_D/0/1/0/all/0/1"&gt;David Brosset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bures_M/0/1/0/all/0/1"&gt;Miroslav Bures&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andonovic_I/0/1/0/all/0/1"&gt;Ivan Andonovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michie_C/0/1/0/all/0/1"&gt;Craig Michie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellekens_X/0/1/0/all/0/1"&gt;Xavier Bellekens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAM: Temporal Adaptive Module for Video Recognition. (arXiv:2005.06803v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.06803</id>
        <link href="http://arxiv.org/abs/2005.06803"/>
        <updated>2021-08-19T01:35:02.140Z</updated>
        <summary type="html"><![CDATA[Video data is with complex temporal dynamics due to various factors such as
camera motion, speed variation, and different activities. To effectively
capture this diverse motion pattern, this paper presents a new temporal
adaptive module ({\bf TAM}) to generate video-specific temporal kernels based
on its own feature map. TAM proposes a unique two-level adaptive modeling
scheme by decoupling the dynamic kernel into a location sensitive importance
map and a location invariant aggregation weight. The importance map is learned
in a local temporal window to capture short-term information, while the
aggregation weight is generated from a global view with a focus on long-term
structure. TAM is a modular block and could be integrated into 2D CNNs to yield
a powerful video architecture (TANet) with a very small extra computational
cost. The extensive experiments on Kinetics-400 and Something-Something
datasets demonstrate that our TAM outperforms other temporal modeling methods
consistently, and achieves the state-of-the-art performance under the similar
complexity. The code is available at \url{
https://github.com/liu-zhy/temporal-adaptive-module}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhaoyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wayne Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2103.16694v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16694</id>
        <link href="http://arxiv.org/abs/2103.16694"/>
        <updated>2021-08-19T01:35:02.122Z</updated>
        <summary type="html"><![CDATA[Simulators can efficiently generate large amounts of labeled synthetic data
with perfect supervision for hard-to-label tasks like semantic segmentation.
However, they introduce a domain gap that severely hurts real-world
performance. We propose to use self-supervised monocular depth estimation as a
proxy task to bridge this gap and improve sim-to-real unsupervised domain
adaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA)
learns a domain-invariant representation via a multi-task objective combining
synthetic semantic supervision with real-world geometric constraints on videos.
GUDA establishes a new state of the art in UDA for semantic segmentation on
three benchmarks, outperforming methods that use domain adversarial learning,
self-training, or other self-supervised proxy tasks. Furthermore, we show that
our method scales well with the quality and quantity of synthetic data while
also improving depth prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1"&gt;Vitor Guizilini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1"&gt;Rares Ambrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02265</id>
        <link href="http://arxiv.org/abs/2104.02265"/>
        <updated>2021-08-19T01:35:02.115Z</updated>
        <summary type="html"><![CDATA[Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1"&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuzhuo Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1"&gt;Mengyuan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GyroFlow: Gyroscope-Guided Unsupervised Optical Flow Learning. (arXiv:2103.13725v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13725</id>
        <link href="http://arxiv.org/abs/2103.13725"/>
        <updated>2021-08-19T01:35:02.108Z</updated>
        <summary type="html"><![CDATA[Existing optical flow methods are erroneous in challenging scenes, such as
fog, rain, and night because the basic optical flow assumptions such as
brightness and gradient constancy are broken. To address this problem, we
present an unsupervised learning approach that fuses gyroscope into optical
flow learning. Specifically, we first convert gyroscope readings into motion
fields named gyro field. Second, we design a self-guided fusion module to fuse
the background motion extracted from the gyro field with the optical flow and
guide the network to focus on motion details. To the best of our knowledge,
this is the first deep learning-based framework that fuses gyroscope data and
image content for optical flow learning. To validate our method, we propose a
new dataset that covers regular and challenging scenes. Experiments show that
our method outperforms the state-of-art methods in both regular and challenging
scenes. Code and dataset are available at
https://github.com/megvii-research/GyroFlow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haipeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1"&gt;Kunming Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Alignment Constraint for Continuous Sign Language Recognition. (arXiv:2104.02330v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02330</id>
        <link href="http://arxiv.org/abs/2104.02330"/>
        <updated>2021-08-19T01:35:02.101Z</updated>
        <summary type="html"><![CDATA[Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize
unsegmented signs from image streams. Overfitting is one of the most critical
problems in CSLR training, and previous works show that the iterative training
scheme can partially solve this problem while also costing more training time.
In this study, we revisit the iterative training scheme in recent CSLR works
and realize that sufficient training of the feature extractor is critical to
solving the overfitting problem. Therefore, we propose a Visual Alignment
Constraint (VAC) to enhance the feature extractor with alignment supervision.
Specifically, the proposed VAC comprises two auxiliary losses: one focuses on
visual features only, and the other enforces prediction alignment between the
feature extractor and the alignment module. Moreover, we propose two metrics to
reflect overfitting by measuring the prediction inconsistency between the
feature extractor and the alignment module. Experimental results on two
challenging CSLR datasets show that the proposed VAC makes CSLR networks
end-to-end trainable and achieves competitive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1"&gt;Yuecong Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1"&gt;Aiming Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_X/0/1/0/all/0/1"&gt;Xiujuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Unpaired Shape Transforming Method for Image Translation and Cross-Domain Retrieval. (arXiv:1812.02134v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.02134</id>
        <link href="http://arxiv.org/abs/1812.02134"/>
        <updated>2021-08-19T01:35:02.094Z</updated>
        <summary type="html"><![CDATA[We address the problem of unpaired geometric image-to-image translation.
Rather than transferring the style of an image as a whole, our goal is to
translate the geometry of an object as depicted in different domains while
preserving its appearance characteristics. Our model is trained in an unpaired
fashion, i.e. without the need of paired images during training. It performs
all steps of the shape transfer within a single model and without additional
post-processing stages. Extensive experiments on the VITON, CMU-Multi-PIE and
our own FashionStyle datasets show the effectiveness of the method. In
addition, we show that despite their low-dimensionality, the features learned
by our model are useful to the item retrieval task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Liqian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1"&gt;Jose Oramas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1"&gt;Tinne Tuytelaars&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn-to-Race: A Multimodal Control Environment for Autonomous Racing. (arXiv:2103.11575v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11575</id>
        <link href="http://arxiv.org/abs/2103.11575"/>
        <updated>2021-08-19T01:35:02.073Z</updated>
        <summary type="html"><![CDATA[Existing research on autonomous driving primarily focuses on urban driving,
which is insufficient for characterising the complex driving behaviour
underlying high-speed racing. At the same time, existing racing simulation
frameworks struggle in capturing realism, with respect to visual rendering,
vehicular dynamics, and task objectives, inhibiting the transfer of learning
agents to real-world contexts. We introduce a new environment, where agents
Learn-to-Race (L2R) in simulated competition-style racing, using multimodal
information--from virtual cameras to a comprehensive array of inertial
measurement sensors. Our environment, which includes a simulator and an
interfacing training framework, accurately models vehicle dynamics and racing
conditions. In this paper, we release the Arrival simulator for autonomous
racing. Next, we propose the L2R task with challenging metrics, inspired by
learning-to-drive challenges, Formula-style racing, and multimodal trajectory
prediction for autonomous driving. Additionally, we provide the L2R framework
suite, facilitating simulated racing on high-precision models of real-world
tracks. Finally, we provide an official L2R task dataset of expert
demonstrations, as well as a series of baseline experiments and reference
implementations. We make all code available:
https://github.com/learn-to-race/l2r.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herman_J/0/1/0/all/0/1"&gt;James Herman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bingqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skabelkin_A/0/1/0/all/0/1"&gt;Alexey Skabelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhukov_I/0/1/0/all/0/1"&gt;Ivan Zhukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumskoy_M/0/1/0/all/0/1"&gt;Max Kumskoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1"&gt;Eric Nyberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Face Recognition Challenge: The InsightFace Track Report. (arXiv:2108.08191v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08191</id>
        <link href="http://arxiv.org/abs/2108.08191"/>
        <updated>2021-08-19T01:35:02.055Z</updated>
        <summary type="html"><![CDATA[During the COVID-19 coronavirus epidemic, almost everyone wears a facial
mask, which poses a huge challenge to deep face recognition. In this workshop,
we organize Masked Face Recognition (MFR) challenge and focus on bench-marking
deep face recognition methods under the existence of facial masks. In the MFR
challenge, there are two main tracks: the InsightFace track and the WebFace260M
track. For the InsightFace track, we manually collect a large-scale masked face
test set with 7K identities. In addition, we also collect a children test set
including 14K identities and a multi-racial test set containing 242K
identities. By using these three test sets, we build up an online model testing
system, which can give a comprehensive evaluation of face recognition models.
To avoid data privacy problems, no test image is released to the public. As the
challenge is still under-going, we will keep on updating the top-ranked
solutions as well as this report on the arxiv.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiankang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jia Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1"&gt;Xiang An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Scene-Aware Motion Prediction. (arXiv:2108.08284v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08284</id>
        <link href="http://arxiv.org/abs/2108.08284"/>
        <updated>2021-08-19T01:35:02.048Z</updated>
        <summary type="html"><![CDATA[A long-standing goal in computer vision is to capture, model, and
realistically synthesize human behavior. Specifically, by learning from data,
our goal is to enable virtual humans to navigate within cluttered indoor scenes
and naturally interact with objects. Such embodied behavior has applications in
virtual reality, computer games, and robotics, while synthesized behavior can
be used as a source of training data. This is challenging because real human
motion is diverse and adapts to the scene. For example, a person can sit or lie
on a sofa in many places and with varying styles. It is necessary to model this
diversity when synthesizing virtual humans that realistically perform
human-scene interactions. We present a novel data-driven, stochastic motion
synthesis method that models different styles of performing a given action with
a target object. Our method, called SAMP, for Scene-Aware Motion Prediction,
generalizes to target objects of various geometries while enabling the
character to navigate in cluttered scenes. To train our method, we collected
MoCap data covering various sitting, lying down, walking, and running styles.
We demonstrate our method on complex indoor scenes and achieve superior
performance compared to existing solutions. Our code and data are available for
research at https://samp.is.tue.mpg.de.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Mohamed Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1"&gt;Duygu Ceylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1"&gt;Ruben Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1"&gt;Jun Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing MLPs With Dropouts, Batch Normalization, and Skip Connections. (arXiv:2108.08186v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08186</id>
        <link href="http://arxiv.org/abs/2108.08186"/>
        <updated>2021-08-19T01:35:02.040Z</updated>
        <summary type="html"><![CDATA[A multilayer perceptron (MLP) is typically made of multiple fully connected
layers with nonlinear activation functions. There have been several approaches
to make them better (e.g. faster convergence, better convergence limit, etc.).
But the researches lack in more structured ways to test them. We test different
MLP architectures by carrying out the experiments on the age and gender
datasets. We empirically show that by whitening inputs before every linear
layer and adding skip connections, our proposed MLP architecture can result in
better performance. Since the whitening process includes dropouts, it can also
be used to approximate Bayesian inference. We have open sourced our code
released models and docker images at https://github.com/tae898/age-gender/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taewoon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Near-Optimal Hypothesis Selection. (arXiv:2108.07880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07880</id>
        <link href="http://arxiv.org/abs/2108.07880"/>
        <updated>2021-08-19T01:35:02.033Z</updated>
        <summary type="html"><![CDATA[Hypothesis Selection is a fundamental distribution learning problem where
given a comparator-class $Q=\{q_1,\ldots, q_n\}$ of distributions, and a
sampling access to an unknown target distribution $p$, the goal is to output a
distribution $q$ such that $\mathsf{TV}(p,q)$ is close to $opt$, where $opt =
\min_i\{\mathsf{TV}(p,q_i)\}$ and $\mathsf{TV}(\cdot, \cdot)$ denotes the
total-variation distance. Despite the fact that this problem has been studied
since the 19th century, its complexity in terms of basic resources, such as
number of samples and approximation guarantees, remains unsettled (this is
discussed, e.g., in the charming book by Devroye and Lugosi `00). This is in
stark contrast with other (younger) learning settings, such as PAC learning,
for which these complexities are well understood.

We derive an optimal $2$-approximation learning strategy for the Hypothesis
Selection problem, outputting $q$ such that $\mathsf{TV}(p,q) \leq2 \cdot opt +
\eps$, with a (nearly) optimal sample complexity of~$\tilde O(\log
n/\epsilon^2)$. This is the first algorithm that simultaneously achieves the
best approximation factor and sample complexity: previously, Bousquet, Kane,
and Moran (COLT `19) gave a learner achieving the optimal $2$-approximation,
but with an exponentially worse sample complexity of $\tilde
O(\sqrt{n}/\epsilon^{2.5})$, and Yatracos~(Annals of Statistics `85) gave a
learner with optimal sample complexity of $O(\log n /\epsilon^2)$ but with a
sub-optimal approximation factor of $3$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bousquet_O/0/1/0/all/0/1"&gt;Olivier Bousquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1"&gt;Mark Braverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efremenko_K/0/1/0/all/0/1"&gt;Klim Efremenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kol_G/0/1/0/all/0/1"&gt;Gillat Kol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1"&gt;Shay Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection. (arXiv:2103.15346v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15346</id>
        <link href="http://arxiv.org/abs/2103.15346"/>
        <updated>2021-08-19T01:35:02.011Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new framework for unsupervised deep homography
estimation. Our contributions are 3 folds. First, unlike previous methods that
regress 4 offsets for a homography, we propose a homography flow
representation, which can be estimated by a weighted sum of 8 pre-defined
homography flow bases. Second, considering a homography contains 8
Degree-of-Freedoms (DOFs) that is much less than the rank of the network
features, we propose a Low Rank Representation (LRR) block that reduces the
feature rank, so that features corresponding to the dominant motions are
retained while others are rejected. Last, we propose a Feature Identity Loss
(FIL) to enforce the learned image feature warp-equivariant, meaning that the
result should be identical if the order of warp operation and feature
extraction is swapped. With this constraint, the unsupervised optimization is
achieved more effectively and more stable features are learned. Extensive
experiments are conducted to demonstrate the effectiveness of all the newly
proposed components, and results show that our approach outperforms the
state-of-the-art on the homography benchmark datasets both qualitatively and
quantitatively. Code is available at
https://github.com/megvii-research/BasesHomo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nianjin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Disparity. (arXiv:2108.07939v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07939</id>
        <link href="http://arxiv.org/abs/2108.07939"/>
        <updated>2021-08-19T01:35:02.004Z</updated>
        <summary type="html"><![CDATA[Most of stereo vision works are focusing on computing the dense pixel
disparity of a given pair of left and right images. A camera pair usually
required lens undistortion and stereo calibration to provide an undistorted
epipolar line calibrated image pair for accurate dense pixel disparity
computation. Due to noise, object occlusion, repetitive or lack of texture and
limitation of matching algorithms, the pixel disparity accuracy usually suffers
the most at those object boundary areas. Although statistically the total
number of pixel disparity errors might be low (under 2% according to the Kitti
Vision Benchmark of current top ranking algorithms), the percentage of these
disparity errors at object boundaries are very high. This renders the
subsequence 3D object distance detection with much lower accuracy than desired.
This paper proposed a different approach for solving a 3D object distance
detection by detecting object disparity directly without going through a dense
pixel disparity computation. An example squeezenet Object Disparity-SSD
(OD-SSD) was constructed to demonstrate an efficient object disparity detection
with comparable accuracy compared with Kitti dataset pixel disparity ground
truth. Further training and testing results with mixed image dataset captured
by several different stereo systems may suggest that an OD-SSD might be
agnostic to stereo system parameters such as a baseline, FOV, lens distortion,
even left/right camera epipolar line misalignment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ynjiun Paul Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Bidirectional Unsupervised Domain Adaptation Segmentation Framework. (arXiv:2108.07979v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07979</id>
        <link href="http://arxiv.org/abs/2108.07979"/>
        <updated>2021-08-19T01:35:01.997Z</updated>
        <summary type="html"><![CDATA[Domain shift happens in cross-domain scenarios commonly because of the wide
gaps between different domains: when applying a deep learning model
well-trained in one domain to another target domain, the model usually performs
poorly. To tackle this problem, unsupervised domain adaptation (UDA) techniques
are proposed to bridge the gap between different domains, for the purpose of
improving model performance without annotation in the target domain.
Particularly, UDA has a great value for multimodal medical image analysis,
where annotation difficulty is a practical concern. However, most existing UDA
methods can only achieve satisfactory improvements in one adaptation direction
(e.g., MRI to CT), but often perform poorly in the other (CT to MRI), limiting
their practical usage. In this paper, we propose a bidirectional UDA (BiUDA)
framework based on disentangled representation learning for equally competent
two-way UDA performances. This framework employs a unified domain-aware pattern
encoder which not only can adaptively encode images in different domains
through a domain controller, but also improve model efficiency by eliminating
redundant parameters. Furthermore, to avoid distortion of contents and patterns
of input images during the adaptation process, a content-pattern consistency
loss is introduced. Additionally, for better UDA segmentation performance, a
label consistency strategy is proposed to provide extra supervision by
recomposing target-domain-styled images and corresponding source-domain
annotations. Comparison experiments and ablation studies conducted on two
public datasets demonstrate the superiority of our BiUDA framework to current
state-of-the-art UDA methods and the effectiveness of its novel designs. By
successfully addressing two-way adaptations, our BiUDA framework offers a
flexible solution of UDA techniques to the real-world scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1"&gt;Munan Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1"&gt;Cheng Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chenglang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaohua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates. (arXiv:2108.08020v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08020</id>
        <link href="http://arxiv.org/abs/2108.08020"/>
        <updated>2021-08-19T01:35:01.981Z</updated>
        <summary type="html"><![CDATA[Co-speech gesture generation is to synthesize a gesture sequence that not
only looks real but also matches with the input speech audio. Our method
generates the movements of a complete upper body, including arms, hands, and
the head. Although recent data-driven methods achieve great success, challenges
still exist, such as limited variety, poor fidelity, and lack of objective
metrics. Motivated by the fact that the speech cannot fully determine the
gesture, we design a method that learns a set of gesture template vectors to
model the latent conditions, which relieve the ambiguity. For our method, the
template vector determines the general appearance of a generated gesture
sequence, while the speech audio drives subtle movements of the body, both
indispensable for synthesizing a realistic gesture sequence. Due to the
intractability of an objective metric for gesture-speech synchronization, we
adopt the lip-sync error as a proxy metric to tune and evaluate the
synchronization ability of our model. Extensive experiments show the
superiority of our method in both objective and subjective evaluations on
fidelity and synchronization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shenhan Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhi Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1"&gt;YiHao Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shenghua Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Graph Convolution for Point Cloud Analysis. (arXiv:2108.08035v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08035</id>
        <link href="http://arxiv.org/abs/2108.08035"/>
        <updated>2021-08-19T01:35:01.962Z</updated>
        <summary type="html"><![CDATA[Convolution on 3D point clouds that generalized from 2D grid-like domains is
widely researched yet far from perfect. The standard convolution characterises
feature correspondences indistinguishably among 3D points, presenting an
intrinsic limitation of poor distinctive feature learning. In this paper, we
propose Adaptive Graph Convolution (AdaptConv) which generates adaptive kernels
for points according to their dynamically learned features. Compared with using
a fixed/isotropic kernel, AdaptConv improves the flexibility of point cloud
convolutions, effectively and precisely capturing the diverse relations between
points from different semantic parts. Unlike popular attentional weight
schemes, the proposed AdaptConv implements the adaptiveness inside the
convolution operation instead of simply assigning different weights to the
neighboring points. Extensive qualitative and quantitative evaluations show
that our method outperforms state-of-the-art point cloud classification and
segmentation approaches on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/AdaptConv-master.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haoran Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yidan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Mingsheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1"&gt;Mingqiang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs. (arXiv:2108.07884v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07884</id>
        <link href="http://arxiv.org/abs/2108.07884"/>
        <updated>2021-08-19T01:35:01.953Z</updated>
        <summary type="html"><![CDATA[In this paper, we challenge the common assumption that collapsing the spatial
dimensions of a 3D (spatial-channel) tensor in a convolutional neural network
(CNN) into a vector via global pooling removes all spatial information.
Specifically, we demonstrate that positional information is encoded based on
the ordering of the channel dimensions, while semantic information is largely
not. Following this demonstration, we show the real world impact of these
findings by applying them to two applications. First, we propose a simple yet
effective data augmentation strategy and loss function which improves the
translation invariance of a CNN's output. Second, we propose a method to
efficiently determine which channels in the latent representation are
responsible for (i) encoding overall position information or (ii)
region-specific positions. We first show that semantic segmentation has a
significant reliance on the overall position channels to make predictions. We
then show for the first time that it is possible to perform a `region-specific'
attack, and degrade a network's performance in a particular part of the input.
We believe our findings and demonstrated applications will benefit research
areas concerned with understanding the characteristics of CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md Amirul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1"&gt;Matthew Kowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1"&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1"&gt;Neil D. B. Bruce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10437</id>
        <link href="http://arxiv.org/abs/2106.10437"/>
        <updated>2021-08-19T01:35:01.930Z</updated>
        <summary type="html"><![CDATA[Recently, there has been discussions on the ill-posed nature of
super-resolution that multiple possible reconstructions exist for a given
low-resolution image. Using normalizing flows, SRflow[23] achieves
state-of-the-art perceptual quality by learning the distribution of the output
instead of a deterministic output to one estimate. In this paper, we adapt the
concepts of SRFlow to improve GAN-based super-resolution by properly
implementing the one-to-many property. We modify the generator to estimate a
distribution as a mapping from random noise. We improve the content loss that
hampers the perceptual training objectives. We also propose additional training
techniques to further enhance the perceptual quality of generated images. Using
our proposed methods, we were able to improve the performance of ESRGAN[1] in
x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual
extreme SR by applying our methods to RFB-ESRGAN[21].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sieun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibration Method of the Monocular Omnidirectional Stereo Camera. (arXiv:2108.07936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07936</id>
        <link href="http://arxiv.org/abs/2108.07936"/>
        <updated>2021-08-19T01:35:01.893Z</updated>
        <summary type="html"><![CDATA[Compact and low-cost devices are needed for autonomous driving to image and
measure distances to objects 360-degree around. We have been developing an
omnidirectional stereo camera exploiting two hyperbolic mirrors and a single
set of a lens and sensor, which makes this camera compact and cost efficient.
We establish a new calibration method for this camera considering higher-order
radial distortion, detailed tangential distortion, an image sensor tilt, and a
lens-mirror offset. Our method reduces the calibration error by 6.0 and 4.3
times for the upper- and lower-view images, respectively. The random error of
the distance measurement is 4.9% and the systematic error is 5.7% up to objects
14 meters apart, which is improved almost nine times compared to the
conventional method. The remaining distance errors is due to a degraded optical
resolution of the prototype, which we plan to make further improvements as
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kawamata_R/0/1/0/all/0/1"&gt;Ryota Kawamata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Betsui_K/0/1/0/all/0/1"&gt;Keiichi Betsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamazaki_K/0/1/0/all/0/1"&gt;Kazuyoshi Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sakakibara_R/0/1/0/all/0/1"&gt;Rei Sakakibara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shimano_T/0/1/0/all/0/1"&gt;Takeshi Shimano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images. (arXiv:2108.07944v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07944</id>
        <link href="http://arxiv.org/abs/2108.07944"/>
        <updated>2021-08-19T01:35:01.885Z</updated>
        <summary type="html"><![CDATA[Many power line companies are using UAVs to perform their inspection
processes instead of putting their workers at risk by making them climb high
voltage power line towers, for instance. A crucial task for the inspection is
to detect and classify assets in the power transmission lines. However, public
data related to power line assets are scarce, preventing a faster evolution of
this area. This work proposes the Power Line Assets Dataset, containing
high-resolution and real-world images of multiple high-voltage power line
components. It has 2,409 annotated objects divided into five classes:
transmission tower, insulator, spacer, tower plate, and Stockbridge damper,
which vary in size (resolution), orientation, illumination, angulation, and
background. This work also presents an evaluation with popular deep object
detection methods, showing considerable room for improvement. The PLAD dataset
is publicly available at https://github.com/andreluizbvs/PLAD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vieira_e_Silva_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Luiz Buarque Vieira-e-Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felix_H/0/1/0/all/0/1"&gt;Heitor de Castro Felix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaves_T/0/1/0/all/0/1"&gt;Thiago de Menezes Chaves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1"&gt;Francisco Paulo Magalh&amp;#xe3;es Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1"&gt;Veronica Teichrieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Michel Mozinho dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1"&gt;Hemir da Cunha Santiago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sgotti_V/0/1/0/all/0/1"&gt;Virginia Ad&amp;#xe9;lia Cordeiro Sgotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neto_H/0/1/0/all/0/1"&gt;Henrique Baptista Duffles Teixeira Lott Neto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Hyperspherical Uniformity. (arXiv:2103.01649v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01649</id>
        <link href="http://arxiv.org/abs/2103.01649"/>
        <updated>2021-08-19T01:35:01.871Z</updated>
        <summary type="html"><![CDATA[Due to the over-parameterization nature, neural networks are a powerful tool
for nonlinear function approximation. In order to achieve good generalization
on unseen data, a suitable inductive bias is of great importance for neural
networks. One of the most straightforward ways is to regularize the neural
network with some additional objectives. L2 regularization serves as a standard
regularization for neural networks. Despite its popularity, it essentially
regularizes one dimension of the individual neuron, which is not strong enough
to control the capacity of highly over-parameterized neural networks. Motivated
by this, hyperspherical uniformity is proposed as a novel family of relational
regularizations that impact the interaction among neurons. We consider several
geometrically distinct ways to achieve hyperspherical uniformity. The
effectiveness of hyperspherical uniformity is justified by theoretical insights
and empirical evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability. (arXiv:2108.08136v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08136</id>
        <link href="http://arxiv.org/abs/2108.08136"/>
        <updated>2021-08-19T01:35:01.861Z</updated>
        <summary type="html"><![CDATA[This work employs a pre-trained, multi-view Convolutional Neural Network
(CNN) with a spatial attention block to optimise knee injury detection. An
open-source Magnetic Resonance Imaging (MRI) data set with image-level labels
was leveraged for this analysis. As MRI data is acquired from three planes, we
compare our technique using data from a single-plane and multiple planes
(multi-plane). For multi-plane, we investigate various methods of fusing the
planes in the network. This analysis resulted in the novel 'MPFuseNet' network
and state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior
Cruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977
and 0.957 respectively. We then developed an objective metric, Penalised
Localisation Accuracy (PLA), to validate the model's localisation ability. This
metric compares binary masks generated from Grad-Cam output and the
radiologist's annotations on a sample of MRIs. We also extracted explainability
features in a model-agnostic approach that were then verified as clinically
relevant by the radiologist.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1"&gt;Niamh Belton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welaratne_I/0/1/0/all/0/1"&gt;Ivan Welaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahlan_A/0/1/0/all/0/1"&gt;Adil Dahlan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hearne_R/0/1/0/all/0/1"&gt;Ronan T Hearne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1"&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1"&gt;Aonghus Lawlor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1"&gt;Kathleen M. Curran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Opportunities and Risks of Foundation Models. (arXiv:2108.07258v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07258</id>
        <link href="http://arxiv.org/abs/2108.07258"/>
        <updated>2021-08-19T01:35:01.830Z</updated>
        <summary type="html"><![CDATA[AI is undergoing a paradigm shift with the rise of models (e.g., BERT,
DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a
wide range of downstream tasks. We call these models foundation models to
underscore their critically central yet incomplete character. This report
provides a thorough account of the opportunities and risks of foundation
models, ranging from their capabilities (e.g., language, vision, robotics,
reasoning, human interaction) and technical principles(e.g., model
architectures, training procedures, data, systems, security, evaluation,
theory) to their applications (e.g., law, healthcare, education) and societal
impact (e.g., inequity, misuse, economic and environmental impact, legal and
ethical considerations). Though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent
capabilities,and their effectiveness across so many tasks incentivizes
homogenization. Homogenization provides powerful leverage but demands caution,
as the defects of the foundation model are inherited by all the adapted models
downstream. Despite the impending widespread deployment of foundation models,
we currently lack a clear understanding of how they work, when they fail, and
what they are even capable of due to their emergent properties. To tackle these
questions, we believe much of the critical research on foundation models will
require deep interdisciplinary collaboration commensurate with their
fundamentally sociotechnical nature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1"&gt;Rishi Bommasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1"&gt;Drew A. Hudson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altman_R/0/1/0/all/0/1"&gt;Russ Altman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1"&gt;Simran Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arx_S/0/1/0/all/0/1"&gt;Sydney von Arx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1"&gt;Michael S. Bernstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1"&gt;Jeannette Bohg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1"&gt;Antoine Bosselut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1"&gt;Emma Brunskill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brynjolfsson_E/0/1/0/all/0/1"&gt;Erik Brynjolfsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1"&gt;Dallas Card&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1"&gt;Rodrigo Castellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Annie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creel_K/0/1/0/all/0/1"&gt;Kathleen Creel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Quincy Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1"&gt;Dora Demszky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doumbouya_M/0/1/0/all/0/1"&gt;Moussa Doumbouya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1"&gt;Esin Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etchemendy_J/0/1/0/all/0/1"&gt;John Etchemendy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1"&gt;Kawin Ethayarajh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1"&gt;Trevor Gale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1"&gt;Lauren Gillespie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1"&gt;Karan Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grossman_S/0/1/0/all/0/1"&gt;Shelby Grossman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1"&gt;Neel Guha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1"&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1"&gt;Peter Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1"&gt;John Hewitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1"&gt;Daniel E. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Jenny Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_K/0/1/0/all/0/1"&gt;Kyle Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1"&gt;Thomas Icard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1"&gt;Pratyusha Kalluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1"&gt;Siddharth Karamcheti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keeling_G/0/1/0/all/0/1"&gt;Geoff Keeling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khani_F/0/1/0/all/0/1"&gt;Fereshte Khani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1"&gt;Omar Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohd_P/0/1/0/all/0/1"&gt;Pang Wei Kohd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1"&gt;Mark Krass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1"&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuditipudi_R/0/1/0/all/0/1"&gt;Rohith Kuditipudi&lt;/a&gt;, et al. (62 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Techniques for Model Inversion Attacks. (arXiv:2010.04092v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04092</id>
        <link href="http://arxiv.org/abs/2010.04092"/>
        <updated>2021-08-19T01:35:01.813Z</updated>
        <summary type="html"><![CDATA[Model inversion (MI) attacks are aimed at reconstructing training data from
model parameters. Such attacks have triggered increasing concerns about
privacy, especially given a growing number of online model repositories.
However, existing MI attacks against deep neural networks (DNNs) have large
room for performance improvement. We present a novel inversion-specific GAN
that can better distill knowledge useful for performing attacks on private
models from public data. In particular, we train the discriminator to
differentiate not only the real and fake samples but the soft-labels provided
by the target model. Moreover, unlike previous work that directly searches for
a single data point to represent a target class, we propose to model a private
data distribution for each target class. Our experiments show that the
combination of these techniques can significantly boost the success rate of the
state-of-the-art MI attacks by 150%, and generalize better to a variety of
datasets and models. Our code is available at
https://github.com/SCccc21/Knowledge-Enriched-DMI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guo-Jun Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embed Me If You Can: A Geometric Perceptron. (arXiv:2006.06507v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06507</id>
        <link href="http://arxiv.org/abs/2006.06507"/>
        <updated>2021-08-19T01:35:01.794Z</updated>
        <summary type="html"><![CDATA[Solving geometric tasks involving point clouds by using machine learning is a
challenging problem. Standard feed-forward neural networks combine linear or,
if the bias parameter is included, affine layers and activation functions.
Their geometric modeling is limited, which motivated the prior work introducing
the multilayer hypersphere perceptron (MLHP). Its constituent part, i.e., the
hypersphere neuron, is obtained by applying a conformal embedding of Euclidean
space. By virtue of Clifford algebra, it can be implemented as the Cartesian
dot product of inputs and weights. If the embedding is applied in a manner
consistent with the dimensionality of the input space geometry, the decision
surfaces of the model units become combinations of hyperspheres and make the
decision-making process geometrically interpretable for humans. Our extension
of the MLHP model, the multilayer geometric perceptron (MLGP), and its
respective layer units, i.e., geometric neurons, are consistent with the 3D
geometry and provide a geometric handle of the learned coefficients. In
particular, the geometric neuron activations are isometric in 3D, which is
necessary for rotation and translation equivariance. When classifying the 3D
Tetris shapes, we quantitatively show that our model requires no activation
function in the hidden layers other than the embedding to outperform the
vanilla multilayer perceptron. In the presence of noise in the data, our model
is also superior to the MLHP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1"&gt;Pavlo Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1"&gt;M&amp;#xe5;rten Wadenb&amp;#xe4;ck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new semi-supervised inductive transfer learning framework: Co-Transfer. (arXiv:2108.07930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07930</id>
        <link href="http://arxiv.org/abs/2108.07930"/>
        <updated>2021-08-19T01:35:01.778Z</updated>
        <summary type="html"><![CDATA[In many practical data mining scenarios, such as network intrusion detection,
Twitter spam detection, and computer-aided diagnosis, a source domain that is
different from but related to a target domain is very common. In addition, a
large amount of unlabeled data is available in both source and target domains,
but labeling each of them is difficult, expensive, time-consuming, and sometime
unnecessary. Therefore, it is very important and worthwhile to fully explore
the labeled and unlabeled data in source and target domains to settle the task
in target domain. In this paper, a new semi-supervised inductive transfer
learning framework, named \emph{Co-Transfer} is proposed. Co-Transfer first
generates three TrAdaBoost classifiers for transfer learning from the source
domain to the target domain, and meanwhile another three TrAdaBoost classifiers
are generated for transfer learning from the target domain to the source
domain, using bootstraped samples from the original labeled data. In each round
of co-transfer, each group of TrAdaBoost classifiers are refined using the
carefully labeled data. Finally, the group of TrAdaBoost classifiers learned to
transfer from the source domain to the target domain produce the final
hypothesis. Experiments results illustrate Co-Transfer can effectively exploit
and reuse the labeled and unlabeled data in source and target domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Ze Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yimin Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07845</id>
        <link href="http://arxiv.org/abs/2108.07845"/>
        <updated>2021-08-19T01:35:01.772Z</updated>
        <summary type="html"><![CDATA[We present ARCH++, an image-based method to reconstruct 3D avatars with
arbitrary clothing styles. Our reconstructed avatars are animation-ready and
highly realistic, in both the visible regions from input views and the unseen
regions. While prior work shows great promise of reconstructing animatable
clothed humans with various topologies, we observe that there exist fundamental
limitations resulting in sub-optimal reconstruction quality. In this paper, we
revisit the major steps of image-based avatar reconstruction and address the
limitations with ARCH++. First, we introduce an end-to-end point based geometry
encoder to better describe the semantics of the underlying 3D human body, in
replacement of previous hand-crafted features. Second, in order to address the
occupancy ambiguity caused by topological changes of clothed humans in the
canonical pose, we propose a co-supervising framework with cross-space
consistency to jointly estimate the occupancy in both the posed and canonical
spaces. Last, we use image-to-image translation networks to further refine
detailed geometry and texture on the reconstructed surface, which improves the
fidelity and consistency across arbitrary viewpoints. In the experiments, we
demonstrate improvements over the state of the art on both public benchmarks
and user studies in reconstruction quality and realism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuanlu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1"&gt;Shunsuke Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1"&gt;Tony Tung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look Before You Leap! Designing a Human-Centered AI System for Change Risk Assessment. (arXiv:2108.07951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07951</id>
        <link href="http://arxiv.org/abs/2108.07951"/>
        <updated>2021-08-19T01:35:01.766Z</updated>
        <summary type="html"><![CDATA[Reducing the number of failures in a production system is one of the most
challenging problems in technology driven industries, such as, the online
retail industry. To address this challenge, change management has emerged as a
promising sub-field in operations that manages and reviews the changes to be
deployed in production in a systematic manner. However, it is practically
impossible to manually review a large number of changes on a daily basis and
assess the risk associated with them. This warrants the development of an
automated system to assess the risk associated with a large number of changes.
There are a few commercial solutions available to address this problem but
those solutions lack the ability to incorporate domain knowledge and continuous
feedback from domain experts into the risk assessment process. As part of this
work, we aim to bridge the gap between model-driven risk assessment of change
requests and the assessment of domain experts by building a continuous feedback
loop into the risk assessment process. Here we present our work to build an
end-to-end machine learning system along with the discussion of some of
practical challenges we faced related to extreme skewness in class
distribution, concept drift, estimation of the uncertainty associated with the
model's prediction and the overall scalability of the system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_B/0/1/0/all/0/1"&gt;Binay Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1"&gt;Anirban Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matha_H/0/1/0/all/0/1"&gt;Harika Matha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_K/0/1/0/all/0/1"&gt;Kunal Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parsai_L/0/1/0/all/0/1"&gt;Lalitdutt Parsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1"&gt;Vijay Agneeswaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08212</id>
        <link href="http://arxiv.org/abs/2108.08212"/>
        <updated>2021-08-19T01:35:01.748Z</updated>
        <summary type="html"><![CDATA[Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yangdi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1"&gt;Yang Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wenbo He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Collaborate. (arXiv:2108.07926v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07926</id>
        <link href="http://arxiv.org/abs/2108.07926"/>
        <updated>2021-08-19T01:35:01.742Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on effective learning over a collaborative research
network involving multiple clients. Each client has its own sample population
which may not be shared with other clients due to privacy concerns. The goal is
to learn a model for each client, which behaves better than the one learned
from its own data, through secure collaborations with other clients in the
network. Due to the discrepancies of the sample distributions across different
clients, it is not necessarily that collaborating with everyone will lead to
the best local models. We propose a learning to collaborate framework, where
each client can choose to collaborate with certain members in the network to
achieve a "collaboration equilibrium", where smaller collaboration coalitions
are formed within the network so that each client can obtain the model with the
best utility. We propose the concept of benefit graph which describes how each
client can benefit from collaborating with other clients and develop a Pareto
optimization approach to obtain it. Finally the collaboration coalitions can be
derived from it based on graph operations. Our framework provides a new way of
setting up collaborations in a research network. Experiments on both synthetic
and real world data sets are provided to demonstrate the effectiveness of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Sen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weishen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepFake MNIST+: A DeepFake Facial Animation Dataset. (arXiv:2108.07949v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07949</id>
        <link href="http://arxiv.org/abs/2108.07949"/>
        <updated>2021-08-19T01:35:01.734Z</updated>
        <summary type="html"><![CDATA[The DeepFakes, which are the facial manipulation techniques, is the emerging
threat to digital society. Various DeepFake detection methods and datasets are
proposed for detecting such data, especially for face-swapping. However, recent
researches less consider facial animation, which is also important in the
DeepFake attack side. It tries to animate a face image with actions provided by
a driving video, which also leads to a concern about the security of recent
payment systems that reply on liveness detection to authenticate real users via
recognising a sequence of user facial actions. However, our experiments show
that the existed datasets are not sufficient to develop reliable detection
methods. While the current liveness detector cannot defend such videos as the
attack. As a response, we propose a new human face animation dataset, called
DeepFake MNIST+, generated by a SOTA image animation generator. It includes
10,000 facial animation videos in ten different actions, which can spoof the
recent liveness detectors. A baseline detection method and a comprehensive
analysis of the method is also included in this paper. In addition, we analyze
the proposed dataset's properties and reveal the difficulty and importance of
detecting animation datasets under different types of motion and compression
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiajun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xueyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1"&gt;Pei Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09485</id>
        <link href="http://arxiv.org/abs/2106.09485"/>
        <updated>2021-08-19T01:35:01.728Z</updated>
        <summary type="html"><![CDATA[We consider a distributed function computation problem in which parties
observing noisy versions of a remote source facilitate the computation of a
function of their observations at a fusion center through public communication.
The distributed function computation is subject to constraints, including not
only reliability and storage but also privacy and secrecy. Specifically, 1) the
remote source should remain private from an eavesdropper and the fusion center,
measured in terms of the information leaked about the remote source; 2) the
function computed should remain secret from the eavesdropper, measured in terms
of the information leaked about the arguments of the function, to ensure
secrecy regardless of the exact function used. We derive the exact rate regions
for lossless and lossy single-function computation and illustrate the lossy
single-function computation rate region for an information bottleneck example,
in which the optimal auxiliary random variables are characterized for
binary-input symmetric-output channels. We extend the approach to lossless and
lossy asynchronous multiple-function computations with joint secrecy and
privacy constraints, in which case inner and outer bounds for the rate regions
differing only in the Markov chain conditions imposed are characterized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gunlu_O/0/1/0/all/0/1"&gt;Onur G&amp;#xfc;nl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloch_M/0/1/0/all/0/1"&gt;Matthieu Bloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1"&gt;Rafael F. Schaefer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11574</id>
        <link href="http://arxiv.org/abs/2104.11574"/>
        <updated>2021-08-19T01:35:01.721Z</updated>
        <summary type="html"><![CDATA[Capillaries are the smallest vessels in the body responsible for the delivery
of oxygen and nutrients to the surrounding cells. Various diseases have been
shown to alter the density of nutritive capillaries and the flow velocity of
erythrocytes. In previous studies, capillary density and flow velocity have
been assessed manually by trained specialists. Manual analysis of a standard
20-second long microvascular video takes on average 20 minutes and requires
extensive training. Several studies have reported that manual analysis hinders
the application of microvascular microscopy in a clinical setting. In this
paper, we present a fully automated state-of-the-art system, called
CapillaryNet, that can quantify skin nutritive capillary density and red blood
cell velocity from handheld microscopy videos. Moreover, CapillaryNet measures
several novel microvascular parameters that researchers were previously unable
to quantify, i.e. capillary hematocrit and Intra-capillary flow velocity
heterogeneity. Our system has been used to analyze skin microcirculation videos
from various patient groups (COVID-19, pancreatitis, and acute heart diseases).
Our proposed system excels from existing capillary detection systems as it
combines the speed of traditional computer vision algorithms and the accuracy
of convolutional neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1"&gt;Anastasiya Dykyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1"&gt;Paulo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1"&gt;Eric Jul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially and color consistent environment lighting estimation using deep neural networks for mixed reality. (arXiv:2108.07903v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07903</id>
        <link href="http://arxiv.org/abs/2108.07903"/>
        <updated>2021-08-19T01:35:01.702Z</updated>
        <summary type="html"><![CDATA[The representation of consistent mixed reality (XR) environments requires
adequate real and virtual illumination composition in real-time. Estimating the
lighting of a real scenario is still a challenge. Due to the ill-posed nature
of the problem, classical inverse-rendering techniques tackle the problem for
simple lighting setups. However, those assumptions do not satisfy the current
state-of-art in computer graphics and XR applications. While many recent works
solve the problem using machine learning techniques to estimate the environment
light and scene's materials, most of them are limited to geometry or previous
knowledge. This paper presents a CNN-based model to estimate complex lighting
for mixed reality environments with no previous information about the scene. We
model the environment illumination using a set of spherical harmonics (SH)
environment lighting, capable of efficiently represent area lighting. We
propose a new CNN architecture that inputs an RGB image and recognizes, in
real-time, the environment lighting. Unlike previous CNN-based lighting
estimation methods, we propose using a highly optimized deep neural network
architecture, with a reduced number of parameters, that can learn high complex
lighting scenarios from real-world high-dynamic-range (HDR) environment images.
We show in the experiments that the CNN architecture can predict the
environment lighting with an average mean squared error (MSE) of \num{7.85e-04}
when comparing SH lighting coefficients. We validate our model in a variety of
mixed reality scenarios. Furthermore, we present qualitative results comparing
relights of real-world scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marques_B/0/1/0/all/0/1"&gt;Bruno Augusto Dorta Marques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clua_E/0/1/0/all/0/1"&gt;Esteban Walter Gonzalez Clua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montenegro_A/0/1/0/all/0/1"&gt;Anselmo Antunes Montenegro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Nader Vasconcelos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Framework for 3D Lensless Imaging with Programmable Masks. (arXiv:2108.07966v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07966</id>
        <link href="http://arxiv.org/abs/2108.07966"/>
        <updated>2021-08-19T01:35:01.695Z</updated>
        <summary type="html"><![CDATA[Lensless cameras provide a framework to build thin imaging systems by
replacing the lens in a conventional camera with an amplitude or phase mask
near the sensor. Existing methods for lensless imaging can recover the depth
and intensity of the scene, but they require solving computationally-expensive
inverse problems. Furthermore, existing methods struggle to recover dense
scenes with large depth variations. In this paper, we propose a lensless
imaging system that captures a small number of measurements using different
patterns on a programmable mask. In this context, we make three contributions.
First, we present a fast recovery algorithm to recover textures on a fixed
number of depth planes in the scene. Second, we consider the mask design
problem, for programmable lensless cameras, and provide a design template for
optimizing the mask patterns with the goal of improving depth estimation.
Third, we use a refinement network as a post-processing step to identify and
remove artifacts in the reconstruction. These modifications are evaluated
extensively with experimental results on a lensless camera prototype to
showcase the performance benefits of the optimized masks and recovery
algorithms over the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yucheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yi Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin C. Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asif_M/0/1/0/all/0/1"&gt;M. Salman Asif&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Framework for Cross-Domain and Cross-System Recommendations. (arXiv:2108.07976v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07976</id>
        <link href="http://arxiv.org/abs/2108.07976"/>
        <updated>2021-08-19T01:35:01.684Z</updated>
        <summary type="html"><![CDATA[Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have
been proposed to improve the recommendation accuracy in a target dataset
(domain/system) with the help of a source one with relatively richer
information. However, most existing CDR and CSR approaches are single-target,
namely, there is a single target dataset, which can only help the target
dataset and thus cannot benefit the source dataset. In this paper, we focus on
three new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR),
and CDR+CSR, and aim to improve the recommendation accuracy in all datasets
simultaneously for all scenarios. To do this, we propose a unified framework,
called GA (based on Graph embedding and Attention techniques), for all three
scenarios. In GA, we first construct separate heterogeneous graphs to generate
more representative user and item embeddings. Then, we propose an element-wise
attention mechanism to effectively combine the embeddings of common entities
(users/items) learned from different datasets. Moreover, to avoid negative
transfer, we further propose a Personalized training strategy to minimize the
embedding difference of common entities between a richer dataset and a sparser
dataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and
GA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments
conducted on four real-world datasets demonstrate that our proposed GA models
significantly outperform the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaochao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Longfei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guanfeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment. (arXiv:2108.07948v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07948</id>
        <link href="http://arxiv.org/abs/2108.07948"/>
        <updated>2021-08-19T01:35:01.662Z</updated>
        <summary type="html"><![CDATA[An important scenario for image quality assessment (IQA) is to evaluate image
restoration (IR) algorithms. The state-of-the-art approaches adopt a
full-reference paradigm that compares restored images with their corresponding
pristine-quality images. However, pristine-quality images are usually
unavailable in blind image restoration tasks and real-world scenarios. In this
paper, we propose a practical solution named degraded-reference IQA (DR-IQA),
which exploits the inputs of IR models, degraded images, as references.
Specifically, we extract reference information from degraded images by
distilling knowledge from pristine-quality images. The distillation is achieved
through learning a reference space, where various degraded images are
encouraged to share the same feature statistics with pristine-quality images.
And the reference space is optimized to capture deep image priors that are
useful for quality assessment. Note that pristine-quality images are only used
during training. Our work provides a powerful and differentiable metric for
blind IRs, especially for GAN-based methods. Extensive experiments show that
our results can even be close to the performance of full-reference settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Heliang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LightMove: A Lightweight Next-POI Recommendation for Taxicab Rooftop Advertising. (arXiv:2108.04993v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04993</id>
        <link href="http://arxiv.org/abs/2108.04993"/>
        <updated>2021-08-19T01:35:01.647Z</updated>
        <summary type="html"><![CDATA[Mobile digital billboards are an effective way to augment brand-awareness.
Among various such mobile billboards, taxicab rooftop devices are emerging in
the market as a brand new media. Motov is a leading company in South Korea in
the taxicab rooftop advertising market. In this work, we present a lightweight
yet accurate deep learning-based method to predict taxicabs' next locations to
better prepare for targeted advertising based on demographic information of
locations. Considering the fact that next POI recommendation datasets are
frequently sparse, we design our presented model based on neural ordinary
differential equations (NODEs), which are known to be robust to
sparse/incorrect input, with several enhancements. Our model, which we call
LightMove, has a larger prediction accuracy, a smaller number of parameters,
and/or a smaller training/inference time, when evaluating with various
datasets, in comparison with state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1"&gt;Jinsung Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Soyoung Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_M/0/1/0/all/0/1"&gt;Minju Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seunghyeon Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Noseong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seonghoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chiyoung Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural Networks: A Performance Benchmark. (arXiv:2103.13933v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13933</id>
        <link href="http://arxiv.org/abs/2103.13933"/>
        <updated>2021-08-19T01:35:01.628Z</updated>
        <summary type="html"><![CDATA[Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due
to both negligent and malicious use. For this reason, the automated detection
and tracking of UAV is a fundamental task in aerial security systems. Common
technologies for UAV detection include visible-band and thermal infrared
imaging, radio frequency and radar. Recent advances in deep neural networks
(DNNs) for image-based object detection open the possibility to use visual
information for this detection and tracking task. Furthermore, these detection
architectures can be implemented as backbones for visual tracking systems,
thereby enabling persistent tracking of UAV incursions. To date, no
comprehensive performance benchmark exists that applies DNNs to visible-band
imagery for UAV detection and tracking. To this end, three datasets with varied
environmental conditions for UAV detection and tracking, comprising a total of
241 videos (331,486 images), are assessed using four detection architectures
and three tracking frameworks. The best performing detector architecture
obtains an mAP of 98.6% and the best performing tracking framework obtains a
MOTA of 96.3%. Cross-modality evaluation is carried out between visible and
infrared spectrums, achieving a maximal 82.8% mAP on visible images when
training in the infrared modality. These results provide the first public
multi-approach benchmark for state-of-the-art deep learning-based methods and
give insight into which detection and tracking architectures are effective in
the UAV domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isaac_Medina_B/0/1/0/all/0/1"&gt;Brian K. S. Isaac-Medina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poyser_M/0/1/0/all/0/1"&gt;Matt Poyser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1"&gt;Daniel Organisciak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1"&gt;Chris G. Willcocks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1"&gt;Toby P. Breckon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Detection in Cough, Breath and Speech using Deep Transfer Learning and Bottleneck Features. (arXiv:2104.02477v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02477</id>
        <link href="http://arxiv.org/abs/2104.02477"/>
        <updated>2021-08-19T01:35:01.620Z</updated>
        <summary type="html"><![CDATA[We present an experimental investigation into the effectiveness of transfer
learning and bottleneck feature extraction in detecting COVID-19 from audio
recordings of cough, breath and speech.

This type of screening is non-contact, does not require specialist medical
expertise or laboratory facilities and can be deployed on inexpensive consumer
hardware.

We use datasets that contain recordings of coughing, sneezing, speech and
other noises, but do not contain COVID-19 labels, to pre-train three deep
neural networks: a CNN, an LSTM and a Resnet50.

These pre-trained networks are subsequently either fine-tuned using smaller
datasets of coughing with COVID-19 labels in the process of transfer learning,
or are used as bottleneck feature extractors.

Results show that a Resnet50 classifier trained by this transfer learning
process delivers optimal or near-optimal performance across all datasets
achieving areas under the receiver operating characteristic (ROC AUC) of 0.98,
0.94 and 0.92 respectively for all three sound classes (coughs, breaths and
speech).

This indicates that coughs carry the strongest COVID-19 signature, followed
by breath and speech.

Our results also show that applying transfer learning and extracting
bottleneck features using the larger datasets without COVID-19 labels led not
only to improve performance, but also to minimise the standard deviation of the
classifier AUCs among the outer folds of the leave-$p$-out cross-validation,
indicating better generalisation.

We conclude that deep transfer learning and bottleneck feature extraction can
improve COVID-19 cough, breath and speech audio classification, yielding
automatic classifiers with higher accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1"&gt;Madhurananda Pahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klopper_M/0/1/0/all/0/1"&gt;Marisa Klopper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_R/0/1/0/all/0/1"&gt;Robin Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1"&gt;Thomas Niesler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03761</id>
        <link href="http://arxiv.org/abs/2105.03761"/>
        <updated>2021-08-19T01:35:01.609Z</updated>
        <summary type="html"><![CDATA[Recently, there has been an increasing number of efforts to introduce models
capable of generating natural language explanations (NLEs) for their
predictions on vision-language (VL) tasks. Such models are appealing, because
they can provide human-friendly and comprehensive explanations. However, there
is a lack of comparison between existing methods, which is due to a lack of
re-usable evaluation frameworks and a scarcity of datasets. In this work, we
introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable
vision-language tasks that establishes a unified evaluation framework and
provides the first comprehensive comparison of existing approaches that
generate NLEs for VL tasks. It spans four models and three datasets and both
automatic metrics and human evaluation are used to assess model-generated
explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs
(over 430k instances). We also propose a new model that combines UNITER, which
learns joint embeddings of images and text, and GPT-2, a pre-trained language
model that is well-suited for text generation. It surpasses the previous state
of the art by a large margin across all datasets. Code and data are available
here: https://github.com/maximek3/e-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1"&gt;Maxime Kayser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1"&gt;Leonard Salewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1"&gt;Cornelius Emde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1"&gt;Virginie Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Relighting against Face Recognition. (arXiv:2108.07920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07920</id>
        <link href="http://arxiv.org/abs/2108.07920"/>
        <updated>2021-08-19T01:35:01.601Z</updated>
        <summary type="html"><![CDATA[Deep face recognition (FR) has achieved significantly high accuracy on
several challenging datasets and fosters successful real-world applications,
even showing high robustness to the illumination variation that is usually
regarded as a main threat to the FR system. However, in the real world,
illumination variation caused by diverse lighting conditions cannot be fully
covered by the limited face dataset. In this paper, we study the threat of
lighting against FR from a new angle, i.e., adversarial attack, and identify a
new task, i.e., adversarial relighting. Given a face image, adversarial
relighting aims to produce a naturally relighted counterpart while fooling the
state-of-the-art deep FR methods. To this end, we first propose the physical
model-based adversarial relighting attack (ARA) denoted as
albedo-quotient-based adversarial relighting attack (AQ-ARA). It generates
natural adversarial light under the physical lighting model and guidance of FR
systems and synthesizes adversarially relighted face images. Moreover, we
propose the auto-predictive adversarial relighting attack (AP-ARA) by training
an adversarial relighting network (ARNet) to automatically predict the
adversarial light in a one-step manner according to different input faces,
allowing efficiency-sensitive applications. More importantly, we propose to
transfer the above digital attacks to physical ARA (Phy-ARA) through a precise
relighting device, making the estimated adversarial lighting condition
reproducible in the real world. We validate our methods on three
state-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two
public datasets. The extensive and insightful results demonstrate our work can
generate realistic adversarial relighted face images fooling FR easily,
revealing the threat of specific light directions and strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruijun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14956</id>
        <link href="http://arxiv.org/abs/2011.14956"/>
        <updated>2021-08-19T01:35:01.593Z</updated>
        <summary type="html"><![CDATA[Learning from noisy labels is an important concern because of the lack of
accurate ground-truth labels in plenty of real-world scenarios. In practice,
various approaches for this concern first make some corrections corresponding
to potentially noisy-labeled instances, and then update predictive model with
information of the made corrections. However, in specific areas, such as
medical histopathology whole slide image analysis (MHWSIA), it is often
difficult or even impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. In this paper, we
focus on alleviating these two problems. For the problem 1), we present
one-step abductive multi-target learning (OSAMTL) that imposes a one-step
logical reasoning upon machine learning via a multi-target learning procedure
to constrain the predictions of the learning model to be subject to our prior
knowledge about the true target. For the problem 2), we propose a logical
assessment formula (LAF) that evaluates the logical rationality of the outputs
of an approach by estimating the consistencies between the predictions of the
learning model and the logical facts narrated from the results of the one-step
logical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori
(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable
the machine learning model achieving logically more rational predictions, which
is beyond various state-of-the-art approaches in handling complex noisy labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongquan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yiming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jiayi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhongxi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pitfalls in Machine Learning Research: Reexamining the Development Cycle. (arXiv:2011.02832v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02832</id>
        <link href="http://arxiv.org/abs/2011.02832"/>
        <updated>2021-08-19T01:35:01.572Z</updated>
        <summary type="html"><![CDATA[Machine learning has the potential to fuel further advances in data science,
but it is greatly hindered by an ad hoc design process, poor data hygiene, and
a lack of statistical rigor in model evaluation. Recently, these issues have
begun to attract more attention as they have caused public and embarrassing
issues in research and development. Drawing from our experience as machine
learning researchers, we follow the machine learning process from algorithm
design to data collection to model evaluation, drawing attention to common
pitfalls and providing practical recommendations for improvements. At each
step, case studies are introduced to highlight how these pitfalls occur in
practice, and where things could be improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1"&gt;Stella Biderman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1"&gt;Walter J. Scheirer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics. (arXiv:2104.03408v3 [hep-ex] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03408</id>
        <link href="http://arxiv.org/abs/2104.03408"/>
        <updated>2021-08-19T01:35:01.564Z</updated>
        <summary type="html"><![CDATA[We present a novel implementation of classification using the machine
learning / artificial intelligence method called boosted decision trees (BDT)
on field programmable gate arrays (FPGA). The firmware implementation of binary
classification requiring 100 training trees with a maximum depth of 4 using
four input variables gives a latency value of about 10 ns, independent of the
clock speed from 100 to 320 MHz in our setup. The low timing values are
achieved by restructuring the BDT layout and reconfiguring its parameters. The
FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our
setup. A software package called fwXmachina achieves this implementation. Our
intended user is an expert of custom electronics-based trigger systems in high
energy physics experiments or anyone that needs decisions at the lowest latency
values for real-time event classification. Two problems from high energy
physics are considered, in the separation of electrons vs. photons and in the
selection of vector boson fusion-produced Higgs bosons vs. the rejection of the
multijet processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ex/1/au:+Hong_T/0/1/0/all/0/1"&gt;Tae Min Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Carlson_B/0/1/0/all/0/1"&gt;Benjamin Carlson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Eubanks_B/0/1/0/all/0/1"&gt;Brandon Eubanks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Racz_S/0/1/0/all/0/1"&gt;Stephen Racz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Roche_S/0/1/0/all/0/1"&gt;Stephen Roche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Stelzer_J/0/1/0/all/0/1"&gt;Joerg Stelzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Stumpp_D/0/1/0/all/0/1"&gt;Daniel Stumpp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection. (arXiv:2108.07897v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07897</id>
        <link href="http://arxiv.org/abs/2108.07897"/>
        <updated>2021-08-19T01:35:01.554Z</updated>
        <summary type="html"><![CDATA[Automated systems that detect the social behavior of deception can enhance
human well-being across medical, social work, and legal domains. Labeled
datasets to train supervised deception detection models can rarely be collected
for real-world, high-stakes contexts. To address this challenge, we propose the
first unsupervised approach for detecting real-world, high-stakes deception in
videos without requiring labels. This paper presents our novel approach for
affect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative
representations of deceptive and truthful behavior. Drawing on psychology
theories that link affect and deception, we experimented with unimodal and
multimodal DBN-based approaches trained on facial valence, facial arousal,
audio, and visual features. In addition to using facial affect as a feature on
which DBN models are trained, we also introduce a DBN training procedure that
uses facial affect as an aligner of audio-visual representations. We conducted
classification experiments with unsupervised Gaussian Mixture Model clustering
to evaluate our approaches. Our best unsupervised approach (trained on facial
valence and visual features) achieved an AUC of 80%, outperforming human
ability and performing comparably to fully-supervised models. Our results
motivate future work on unsupervised, affect-aware computational approaches for
detecting deception and other social behaviors in the wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1"&gt;Leena Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1"&gt;Maja J Matari&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-preserving Data Analysis through Representation Learning and Transformation. (arXiv:2011.08315v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08315</id>
        <link href="http://arxiv.org/abs/2011.08315"/>
        <updated>2021-08-19T01:35:01.539Z</updated>
        <summary type="html"><![CDATA[The abundance of data collected by sensors in Internet of Things (IoT)
devices, and the success of deep neural networks in uncovering hidden patterns
in time series data have led to mounting privacy concerns. This is because
private and sensitive information can be potentially learned from sensor data
by applications that have access to this data. In this paper, we aim to examine
the tradeoff between utility and privacy loss by learning low-dimensional
representations that are useful for data obfuscation. We propose deterministic
and probabilistic transformations in the latent space of a variational
autoencoder to synthesize time series data such that intrusive inferences are
prevented while desired inferences can still be made with sufficient accuracy.
In the deterministic case, we use a linear transformation to move the
representation of input data in the latent space such that the reconstructed
data is likely to have the same public attribute but a different private
attribute than the original input data. In the probabilistic case, we apply the
linear transformation to the latent representation of input data with some
probability. We compare our technique with autoencoder-based anonymization
techniques and additionally show that it can anonymize data in real time on
resource-constrained edge devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajihassani_O/0/1/0/all/0/1"&gt;Omid Hajihassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardakanian_O/0/1/0/all/0/1"&gt;Omid Ardakanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khazaei_H/0/1/0/all/0/1"&gt;Hamzeh Khazaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying Reinforcement Learning Policies with $\mathcal{L}_1$ Adaptive Control. (arXiv:2106.02249v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02249</id>
        <link href="http://arxiv.org/abs/2106.02249"/>
        <updated>2021-08-19T01:35:01.527Z</updated>
        <summary type="html"><![CDATA[A reinforcement learning (RL) policy trained in a nominal environment could
fail in a new/perturbed environment due to the existence of dynamic variations.
Existing robust methods try to obtain a fixed policy for all envisioned dynamic
variation scenarios through robust or adversarial training. These methods could
lead to conservative performance due to emphasis on the worst case, and often
involve tedious modifications to the training environment. We propose an
approach to robustifying a pre-trained non-robust RL policy with
$\mathcal{L}_1$ adaptive control. Leveraging the capability of an
$\mathcal{L}_1$ control law in the fast estimation of and active compensation
for dynamic variations, our approach can significantly improve the robustness
of an RL policy trained in a standard (i.e., non-robust) way, either in a
simulator or in the real world. Numerical experiments are provided to validate
the efficacy of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yikun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1"&gt;Manan Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1"&gt;Naira Hovakimyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gender Bias in Depression Detection Using Audio Features. (arXiv:2010.15120v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15120</id>
        <link href="http://arxiv.org/abs/2010.15120"/>
        <updated>2021-08-19T01:35:01.507Z</updated>
        <summary type="html"><![CDATA[Depression is a large-scale mental health problem and a challenging area for
machine learning researchers in detection of depression. Datasets such as
Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created
to aid research in this area. However, on top of the challenges inherent in
accurately detecting depression, biases in datasets may result in skewed
classification performance. In this paper we examine gender bias in the
DAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an
overreporting of performance. By different concepts from Fair Machine Learning,
such as data re-distribution, and using raw audio features, we can mitigate
against the harmful effects of bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bailey_A/0/1/0/all/0/1"&gt;Andrew Bailey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1"&gt;Mark D. Plumbley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09854</id>
        <link href="http://arxiv.org/abs/2012.09854"/>
        <updated>2021-08-19T01:35:01.500Z</updated>
        <summary type="html"><![CDATA[We present Worldsheet, a method for novel view synthesis using just a single
RGB image as input. The main insight is that simply shrink-wrapping a planar
mesh sheet onto the input image, consistent with the learned intermediate
depth, captures underlying geometry sufficient to generate photorealistic
unseen views with large viewpoint changes. To operationalize this, we propose a
novel differentiable texture sampler that allows our wrapped mesh sheet to be
textured and rendered differentiably into an image from a target viewpoint. Our
approach is category-agnostic, end-to-end trainable without using any 3D
supervision, and requires a single image at test time. We also explore a simple
extension by stacking multiple layers of Worldsheets to better handle
occlusions. Worldsheet consistently outperforms prior state-of-the-art methods
on single-image view synthesis across several datasets. Furthermore, this
simple idea captures novel views surprisingly well on a wide range of
high-resolution in-the-wild images, converting them into navigable 3D pop-ups.
Video results and code are available at https://worldsheet.github.io.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ronghang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1"&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1"&gt;Alexander C. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08677</id>
        <link href="http://arxiv.org/abs/2012.08677"/>
        <updated>2021-08-19T01:35:01.493Z</updated>
        <summary type="html"><![CDATA[In order to meet the requirements for performance, safety, and latency in
many IoT applications, intelligent decisions must be made right here right now
at the network edge. However, the constrained resources and limited local data
amount pose significant challenges to the development of edge AI. To overcome
these challenges, we explore continual edge learning capable of leveraging the
knowledge transfer from previous tasks. Aiming to achieve fast and continual
edge learning, we propose a platform-aided federated meta-learning architecture
where edge nodes collaboratively learn a meta-model, aided by the knowledge
transfer from prior tasks. The edge learning problem is cast as a regularized
optimization problem, where the valuable knowledge learned from previous tasks
is extracted as regularization. Then, we devise an ADMM based federated
meta-learning algorithm, namely ADMM-FedMeta, where ADMM offers a natural
mechanism to decompose the original problem into many subproblems which can be
solved in parallel across edge nodes and the platform. Further, a variant of
inexact-ADMM method is employed where the subproblems are `solved' via linear
approximation as well as Hessian estimation to reduce the computational cost
per round to $\mathcal{O}(n)$. We provide a comprehensive analysis of
ADMM-FedMeta, in terms of the convergence properties, the rapid adaptation
performance, and the forgetting effect of prior knowledge transfer, for the
general non-convex case. Extensive experimental studies demonstrate the
effectiveness and efficiency of ADMM-FedMeta, and showcase that it
substantially outperforms the existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Sheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Ju Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jiang Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junshan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net. (arXiv:2108.07851v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07851</id>
        <link href="http://arxiv.org/abs/2108.07851"/>
        <updated>2021-08-19T01:35:01.476Z</updated>
        <summary type="html"><![CDATA[Existing salient object detection (SOD) models mainly rely on CNN-based
U-shaped structures with skip connections to combine the global contexts and
local spatial details that are crucial for locating salient objects and
refining object details, respectively. Despite great successes, the ability of
CNN in learning global contexts is limited. Recently, the vision transformer
has achieved revolutionary progress in computer vision owing to its powerful
modeling of global dependencies. However, directly applying the transformer to
SOD is obviously suboptimal because the transformer lacks the ability to learn
local spatial representations. To this end, this paper explores the combination
of transformer and CNN to learn both global and local representations for SOD.
We propose a transformer-based Asymmetric Bilateral U-Net (AbiU-Net). The
asymmetric bilateral encoder has a transformer path and a lightweight CNN path,
where the two paths communicate at each encoder stage to learn complementary
global contexts and local spatial details, respectively. The asymmetric
bilateral decoder also consists of two paths to process features from the
transformer and CNN encoder paths, with communication at each decoder stage for
decoding coarse salient object locations and find-grained object details,
respectively. Such communication between the two encoder/decoder paths enables
AbiU-Net to learn complementary global and local representations, taking
advantage of the natural properties of transformer and CNN, respectively.
Hence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive
experiments demonstrate that ABiU-Net performs favorably against previous
state-of-the-art SOD methods. The code will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yu Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HuMoR: 3D Human Motion Model for Robust Pose Estimation. (arXiv:2105.04668v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04668</id>
        <link href="http://arxiv.org/abs/2105.04668"/>
        <updated>2021-08-19T01:35:01.464Z</updated>
        <summary type="html"><![CDATA[We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal
pose and shape. Though substantial progress has been made in estimating 3D
human motion and shape from dynamic observations, recovering plausible pose
sequences in the presence of noise and occlusions remains a challenge. For this
purpose, we propose an expressive generative model in the form of a conditional
variational autoencoder, which learns a distribution of the change in pose at
each step of a motion sequence. Furthermore, we introduce a flexible
optimization-based approach that leverages HuMoR as a motion prior to robustly
estimate plausible pose and shape from ambiguous observations. Through
extensive evaluations, we demonstrate that our model generalizes to diverse
motions and body shapes after training on a large motion capture dataset, and
enables motion reconstruction from multiple input modalities including 3D
keypoints and RGB(-D) videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1"&gt;Davis Rempe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1"&gt;Tolga Birdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1"&gt;Aaron Hertzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1"&gt;Srinath Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas J. Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listening to the city, attentively: A Spatio-Temporal Attention Boosted Autoencoder for the Short-Term Flow Prediction Problem. (arXiv:2103.00983v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00983</id>
        <link href="http://arxiv.org/abs/2103.00983"/>
        <updated>2021-08-19T01:35:01.444Z</updated>
        <summary type="html"><![CDATA[In recent years, studying and predicting alternative mobility (e.g., sharing
services) patterns in urban environments has become increasingly important as
accurate and timely information on current and future vehicle flows can
successfully increase the quality and availability of transportation services.
This need is aggravated during the current pandemic crisis, which pushes
policymakers and private citizens to seek social-distancing compliant urban
mobility services, such as electric bikes and scooter sharing offerings.
However, predicting the number of incoming and outgoing vehicles for different
city areas is challenging due to the nonlinear spatial and temporal
dependencies typical of urban mobility patterns. In this work, we propose
STREED-Net, a novel deep learning network with a multi-attention (spatial and
temporal) mechanism that effectively captures and exploits complex spatial and
temporal patterns in mobility data. The results of a thorough experimental
analysis using real-life data are reported, indicating that the proposed model
improves the state-of-the-art for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fiorini_S/0/1/0/all/0/1"&gt;Stefano Fiorini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciavotta_M/0/1/0/all/0/1"&gt;Michele Ciavotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurino_A/0/1/0/all/0/1"&gt;Andrea Maurino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02672</id>
        <link href="http://arxiv.org/abs/2101.02672"/>
        <updated>2021-08-19T01:35:01.437Z</updated>
        <summary type="html"><![CDATA[Existing point-cloud based 3D object detectors use convolution-like operators
to process information in a local neighbourhood with fixed-weight kernels and
aggregate global context hierarchically. However, non-local neural networks and
self-attention for 2D vision have shown that explicitly modeling long-range
interactions can lead to more robust and competitive models. In this paper, we
propose two variants of self-attention for contextual modeling in 3D object
detection by augmenting convolutional features with self-attention features. We
first incorporate the pairwise self-attention mechanism into the current
state-of-the-art BEV, voxel and point-based detectors and show consistent
improvement over strong baseline models of up to 1.5 3D AP while simultaneously
reducing their parameter footprint and computational cost by 15-80% and 30-50%,
respectively, on the KITTI validation set. We next propose a self-attention
variant that samples a subset of the most representative features by learning
deformations over randomly sampled locations. This not only allows us to scale
explicit global contextual modeling to larger point-clouds, but also leads to
more discriminative and informative feature descriptors. Our method can be
flexibly applied to most state-of-the-art detectors with increased accuracy and
parameter and compute efficiency. We show our proposed method improves 3D
object detection performance on KITTI, nuScenes and Waymo Open datasets. Code
is available at https://github.com/AutoVision-cloud/SA-Det3D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Prarthana Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chengjie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1"&gt;Krzysztof Czarnecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06606</id>
        <link href="http://arxiv.org/abs/2009.06606"/>
        <updated>2021-08-19T01:35:01.429Z</updated>
        <summary type="html"><![CDATA[In the regret-based formulation of Multi-armed Bandit (MAB) problems, except
in rare instances, much of the literature focuses on arms with i.i.d. rewards.
In this paper, we consider the problem of obtaining regret guarantees for MAB
problems in which the rewards of each arm form a Markov chain which may not
belong to a single parameter exponential family. To achieve logarithmic regret
in such problems is not difficult: a variation of standard Kullback-Leibler
Upper Confidence Bound (KL-UCB) does the job. However, the constants obtained
from such an analysis are poor for the following reason: i.i.d. rewards are a
special case of Markov rewards and it is difficult to design an algorithm that
works well independent of whether the underlying model is truly Markovian or
i.i.d. To overcome this issue, we introduce a novel algorithm that identifies
whether the rewards from each arm are truly Markovian or i.i.d. using a total
variation distance-based test. Our algorithm then switches from using a
standard KL-UCB to a specialized version of KL-UCB when it determines that the
arm reward is Markovian, thus resulting in low regret for both i.i.d. and
Markovian settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Arghyadip Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BIKED: A Dataset for Computational Bicycle Design with Machine Learning Benchmarks. (arXiv:2103.05844v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05844</id>
        <link href="http://arxiv.org/abs/2103.05844"/>
        <updated>2021-08-19T01:35:01.422Z</updated>
        <summary type="html"><![CDATA[In this paper, we present "BIKED," a dataset comprised of 4500 individually
designed bicycle models sourced from hundreds of designers. We expect BIKED to
enable a variety of data-driven design applications for bicycles and support
the development of data-driven design methods. The dataset is comprised of a
variety of design information including assembly images, component images,
numerical design parameters, and class labels. In this paper, we first discuss
the processing of the dataset, then highlight some prominent research questions
that BIKED can help address. Of these questions, we further explore the
following in detail: 1) Are there prominent gaps in the current bicycle market
and design space? We explore the design space using unsupervised dimensionality
reduction methods. 2) How does one identify the class of a bicycle and what
factors play a key role in defining it? We address the bicycle classification
task by training a multitude of classifiers using different forms of design
data and identifying parameters of particular significance through
permutation-based interpretability analysis. 3) How does one synthesize new
bicycles using different representation methods? We consider numerous machine
learning methods to generate new bicycle models as well as interpolate between
and extrapolate from existing models using Variational Autoencoders. The
dataset and code are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regenwetter_L/0/1/0/all/0/1"&gt;Lyle Regenwetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curry_B/0/1/0/all/0/1"&gt;Brent Curry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1"&gt;Faez Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attacks are Easier on Difficult Problems. (arXiv:2102.07762v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07762</id>
        <link href="http://arxiv.org/abs/2102.07762"/>
        <updated>2021-08-19T01:35:01.397Z</updated>
        <summary type="html"><![CDATA[Membership inference attacks (MIA) try to detect if data samples were used to
train a neural network model, e.g. to detect copyright abuses. We show that
models with higher dimensional input and output are more vulnerable to MIA, and
address in more detail models for image translation and semantic segmentation,
including medical image segmentation. We show that reconstruction-errors can
lead to very effective MIA attacks as they are indicative of memorization.
Unfortunately, reconstruction error alone is less effective at discriminating
between non-predictable images used in training and easy to predict images that
were never seen before. To overcome this, we propose using a novel
predictability error that can be computed for each sample, and its computation
does not require a training set. Our membership error, obtained by subtracting
the predictability error from the reconstruction error, is shown to achieve
high MIA accuracy on an extensive number of benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shafran_A/0/1/0/all/0/1"&gt;Avital Shafran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peleg_S/0/1/0/all/0/1"&gt;Shmuel Peleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neuromorphic Computing for Content-based Image Retrieval. (arXiv:2008.01380v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01380</id>
        <link href="http://arxiv.org/abs/2008.01380"/>
        <updated>2021-08-19T01:35:01.375Z</updated>
        <summary type="html"><![CDATA[Neuromorphic computing mimics the neural activity of the brain through
emulating spiking neural networks. In numerous machine learning tasks,
neuromorphic chips are expected to provide superior solutions in terms of cost
and power efficiency. Here, we explore the application of Loihi, a neuromorphic
computing chip developed by Intel, for the computer vision task of image
retrieval. We evaluated the functionalities and the performance metrics that
are critical in content-based visual search and recommender systems using
deep-learning embeddings. Our results show that the neuromorphic solution is
about 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and
12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a
lightweight convolutional neural network without batching while maintaining the
same level of matching accuracy. The study validates the potential of
neuromorphic computing in low-power image retrieval, as a complementary
paradigm to the existing von Neumann architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Te-Yuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1"&gt;Ata Mahjoubfar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prusinski_D/0/1/0/all/0/1"&gt;Daniel Prusinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_L/0/1/0/all/0/1"&gt;Luis Stevens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Graph Memory Networks for Forgetting-Robust Knowledge Tracing. (arXiv:2108.08105v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08105</id>
        <link href="http://arxiv.org/abs/2108.08105"/>
        <updated>2021-08-19T01:35:01.365Z</updated>
        <summary type="html"><![CDATA[Tracing a student's knowledge is vital for tailoring the learning experience.
Recent knowledge tracing methods tend to respond to these challenges by
modelling knowledge state dynamics across learning concepts. However, they
still suffer from several inherent challenges including: modelling forgetting
behaviours and identifying relationships among latent concepts. To address
these challenges, in this paper, we propose a novel knowledge tracing model,
namely \emph{Deep Graph Memory Network} (DGMN). In this model, we incorporate a
forget gating mechanism into an attention memory structure in order to capture
forgetting behaviours dynamically during the knowledge tracing process.
Particularly, this forget gating mechanism is built upon attention forgetting
features over latent concepts considering their mutual dependencies. Further,
this model has the capability of learning relationships between latent concepts
from a dynamic latent concept graph in light of a student's evolving knowledge
states. A comprehensive experimental evaluation has been conducted using four
well-established benchmark datasets. The results show that DGMN consistently
outperforms the state-of-the-art KT models over all the datasets. The
effectiveness of modelling forgetting behaviours and learning latent concept
graphs has also been analyzed in our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelrahman_G/0/1/0/all/0/1"&gt;Ghodai Abdelrahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Regression with Distributed Learning: A Generalization Error Perspective. (arXiv:2101.09001v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09001</id>
        <link href="http://arxiv.org/abs/2101.09001"/>
        <updated>2021-08-19T01:35:01.355Z</updated>
        <summary type="html"><![CDATA[Distributed learning provides an attractive framework for scaling the
learning task by sharing the computational load over multiple nodes in a
network. Here, we investigate the performance of distributed learning for
large-scale linear regression where the model parameters, i.e., the unknowns,
are distributed over the network. We adopt a statistical learning approach. In
contrast to works that focus on the performance on the training data, we focus
on the generalization error, i.e., the performance on unseen data. We provide
high-probability bounds on the generalization error for both isotropic and
correlated Gaussian data as well as sub-gaussian data. These results reveal the
dependence of the generalization performance on the partitioning of the model
over the network. In particular, our results show that the generalization error
of the distributed solution can be substantially higher than that of the
centralized solution even when the error on the training data is at the same
level for both the centralized and distributed approaches. Our numerical
results illustrate the performance with both real-world image data as well as
synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hellkvist_M/0/1/0/all/0/1"&gt;Martin Hellkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozcelikkale_A/0/1/0/all/0/1"&gt;Ay&amp;#xe7;a &amp;#xd6;z&amp;#xe7;elikkale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ahlen_A/0/1/0/all/0/1"&gt;Anders Ahl&amp;#xe9;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-TGAN: Federated Learning Framework for Synthesizing Tabular Data. (arXiv:2108.07927v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07927</id>
        <link href="http://arxiv.org/abs/2108.07927"/>
        <updated>2021-08-19T01:35:01.345Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) are typically trained to synthesize
data, from images and more recently tabular data, under the assumption of
directly accessible training data. Recently, federated learning (FL) is an
emerging paradigm that features decentralized learning on client's local data
with a privacy-preserving capability. And, while learning GANs to synthesize
images on FL systems has just been demonstrated, it is unknown if GANs for
tabular data can be learned from decentralized data sources. Moreover, it
remains unclear which distributed architecture suits them best. Different from
image GANs, state-of-the-art tabular GANs require prior knowledge on the data
distribution of each (discrete and continuous) column to agree on a common
encoding -- risking privacy guarantees. In this paper, we propose Fed-TGAN, the
first Federated learning framework for Tabular GANs. To effectively learn a
complex tabular GAN on non-identical participants, Fed-TGAN designs two novel
features: (i) a privacy-preserving multi-source feature encoding for model
initialization; and (ii) table similarity aware weighting strategies to
aggregate local models for countering data skew. We extensively evaluate the
proposed Fed-TGAN against variants of decentralized learning architectures on
four widely used datasets. Results show that Fed-TGAN accelerates training time
per epoch up to 200% compared to the alternative architectures, for both IID
and Non-IID data. Overall, Fed-TGAN not only stabilizes the training loss, but
also achieves better similarity between generated and original data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zilong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1"&gt;Aditya Kunar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOME/IP Intrusion Detection using Deep Learning-based Sequential Models in Automotive Ethernet Networks. (arXiv:2108.08262v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08262</id>
        <link href="http://arxiv.org/abs/2108.08262"/>
        <updated>2021-08-19T01:35:01.291Z</updated>
        <summary type="html"><![CDATA[Intrusion Detection Systems are widely used to detect cyberattacks,
especially on protocols vulnerable to hacking attacks such as SOME/IP. In this
paper, we present a deep learning-based sequential model for offline intrusion
detection on SOME/IP application layer protocol. To assess our intrusion
detection system, we have generated and labeled a dataset with several classes
representing realistic intrusions, and a normal class - a significant
contribution due to the absence of such publicly available datasets.
Furthermore, we also propose a simple recurrent neural network (RNN), as an
instance of deep learning-based sequential model, that we apply to our
generated dataset. The numerical results show that RNN excel at predicting
in-vehicle intrusions, with F1 Scores and AUC values of 0.99 for each type of
intrusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alkhatib_N/0/1/0/all/0/1"&gt;Natasha Alkhatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1"&gt;Hadi Ghauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danger_J/0/1/0/all/0/1"&gt;Jean-Luc Danger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Pricing in Machine Learning Pipelines. (arXiv:2108.07915v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07915</id>
        <link href="http://arxiv.org/abs/2108.07915"/>
        <updated>2021-08-19T01:35:01.264Z</updated>
        <summary type="html"><![CDATA[Machine learning is disruptive. At the same time, machine learning can only
succeed by collaboration among many parties in multiple steps naturally as
pipelines in an eco-system, such as collecting data for possible machine
learning applications, collaboratively training models by multiple parties and
delivering machine learning services to end users. Data is critical and
penetrating in the whole machine learning pipelines. As machine learning
pipelines involve many parties and, in order to be successful, have to form a
constructive and dynamic eco-system, marketplaces and data pricing are
fundamental in connecting and facilitating those many parties. In this article,
we survey the principles and the latest research development of data pricing in
machine learning pipelines. We start with a brief review of data marketplaces
and pricing desiderata. Then, we focus on pricing in three important steps in
machine learning pipelines. To understand pricing in the step of training data
collection, we review pricing raw data sets and data labels. We also
investigate pricing in the step of collaborative training of machine learning
models, and overview pricing machine learning models for end users in the step
of machine learning deployment. We also discuss a series of possible future
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cong_Z/0/1/0/all/0/1"&gt;Zicun Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1"&gt;Pei Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feida Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining K-means type algorithms with Hill Climbing for Joint Stratification and Sample Allocation Designs. (arXiv:2108.08038v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08038</id>
        <link href="http://arxiv.org/abs/2108.08038"/>
        <updated>2021-08-19T01:35:01.256Z</updated>
        <summary type="html"><![CDATA[In this paper we combine the k-means and/or k-means type algorithms with a
hill climbing algorithm in stages to solve the joint stratification and sample
allocation problem. This is a combinatorial optimisation problem in which we
search for the optimal stratification from the set of all possible
stratifications of basic strata. Each stratification being a solution the
quality of which is measured by its cost. This problem is intractable for
larger sets. Furthermore evaluating the cost of each solution is expensive. A
number of heuristic algorithms have already been developed to solve this
problem with the aim of finding acceptable solutions in reasonable computation
times. However, the heuristics for these algorithms need to be trained in order
to optimise performance in each instance. We compare the above multi-stage
combination of algorithms with three recent algorithms and report the solution
costs, evaluation times and training times. The multi-stage combinations
generally compare well with the recent algorithms both in the case of atomic
and continuous strata and provide the survey designer with a greater choice of
algorithms to choose from.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+OLuing_M/0/1/0/all/0/1"&gt;Mervyn O&amp;#x27;Luing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Prestwich_S/0/1/0/all/0/1"&gt;Steven Prestwich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tarim_S/0/1/0/all/0/1"&gt;S. Armagan Tarim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coverage Hole Detection for mmWave Networks: An Unsupervised Learning Approach. (arXiv:2108.07854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07854</id>
        <link href="http://arxiv.org/abs/2108.07854"/>
        <updated>2021-08-19T01:35:01.249Z</updated>
        <summary type="html"><![CDATA[The utilization of millimeter-wave (mmWave) bands in 5G networks poses new
challenges to network planning. Vulnerability to blockages at mmWave bands can
cause coverage holes (CHs) in the radio environment, leading to radio link
failure when a user enters these CHs. Detection of the CHs carries critical
importance so that necessary remedies can be introduced to improve coverage. In
this letter, we propose a novel approach to identify the CHs in an unsupervised
fashion using a state-of-the-art manifold learning technique: uniform manifold
approximation and projection. The key idea is to preserve the
local-connectedness structure inherent in the collected unlabelled channel
samples, such that the CHs from the service area are detectable. Our results on
the DeepMIMO dataset scenario demonstrate that the proposed method can learn
the structure within the data samples and provide visual holes in the
low-dimensional embedding while preserving the CH boundaries. Once the CH
boundary is determined in the low-dimensional embedding, channel-based
localization techniques can be applied to these samples to obtain the
geographical boundaries of the CHs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anjinappa_C/0/1/0/all/0/1"&gt;Chethan K. Anjinappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guvenc_I/0/1/0/all/0/1"&gt;Ismail Guvenc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Casual Inference using Deep Bayesian Dynamic Survival Model (CDS). (arXiv:2101.10643v8 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10643</id>
        <link href="http://arxiv.org/abs/2101.10643"/>
        <updated>2021-08-19T01:35:01.238Z</updated>
        <summary type="html"><![CDATA[Causal inference in longitudinal observational health data often requires the
accurate estimation of treatment effects on time-to-event outcomes in the
presence of time-varying covariates. To tackle this sequential treatment effect
estimation problem, we have developed a causal dynamic survival (CDS) model
that uses the potential outcomes framework with the recurrent sub-networks with
random seed ensembles to estimate the difference in survival curves of its
confidence interval. Using simulated survival datasets, the CDS model has shown
good causal effect estimation performance across scenarios of sample dimension,
event rate, confounding and overlapping. However, increasing the sample size is
not effective to alleviate the adverse impact from high level of confounding.
In two large clinical cohort studies, our model identified the expected
conditional average treatment effect and detected individual effect
heterogeneity over time and patient subgroups. CDS provides individualised
absolute treatment effect estimations to improve clinical decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gallego_B/0/1/0/all/0/1"&gt;Blanca Gallego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Neural Networks With Benford's Law. (arXiv:2102.03313v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03313</id>
        <link href="http://arxiv.org/abs/2102.03313"/>
        <updated>2021-08-19T01:35:01.228Z</updated>
        <summary type="html"><![CDATA[Benford's Law (BL) or the Significant Digit Law defines the probability
distribution of the first digit of numerical values in a data sample. This Law
is observed in many naturally occurring datasets. It can be seen as a measure
of naturalness of a given distribution and finds its application in areas like
anomaly and fraud detection. In this work, we address the following question:
Is the distribution of the Neural Network parameters related to the network's
generalization capability? To that end, we first define a metric, MLH (Model
Enthalpy),that measures the closeness of a set of numbers to Benford's Law and
we show empirically that it is a strong predictor of Validation Accuracy.
Second, we use MLH as an alternative to Validation Accuracy for Early Stopping,
removing the need for a Validation set. We provide experimental evidence that
even if the optimal size of the validation set is known before-hand, the peak
test accuracy attained is lower than not using a validation set at all.
Finally, we investigate the connection of BL to Free Energy Principle and First
Law of Thermodynamics, showing that MLH is a component of the internal energy
of the learning system and optimization as an analogy to minimizing the total
energy to attain equilibrium.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1"&gt;Surya Kant Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1"&gt;Abhinav Java&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1"&gt;Arshad Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1"&gt;Yannic Kilcher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCVA: Automated Commit-level Vulnerability Assessment with Deep Multi-task Learning. (arXiv:2108.08041v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.08041</id>
        <link href="http://arxiv.org/abs/2108.08041"/>
        <updated>2021-08-19T01:35:01.210Z</updated>
        <summary type="html"><![CDATA[It is increasingly suggested to identify Software Vulnerabilities (SVs) in
code commits to give early warnings about potential security risks. However,
there is a lack of effort to assess vulnerability-contributing commits right
after they are detected to provide timely information about the exploitability,
impact and severity of SVs. Such information is important to plan and
prioritize the mitigation for the identified SVs. We propose a novel Deep
multi-task learning model, DeepCVA, to automate seven Commit-level
Vulnerability Assessment tasks simultaneously based on Common Vulnerability
Scoring System (CVSS) metrics. We conduct large-scale experiments on 1,229
vulnerability-contributing commits containing 542 different SVs in 246
real-world software projects to evaluate the effectiveness and efficiency of
our model. We show that DeepCVA is the best-performing model with 38% to 59.8%
higher Matthews Correlation Coefficient than many supervised and unsupervised
baseline models. DeepCVA also requires 6.3 times less training and validation
time than seven cumulative assessment models, leading to significantly less
model maintenance cost as well. Overall, DeepCVA presents the first effective
and efficient solution to automatically assess SVs early in software systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Triet H. M. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hin_D/0/1/0/all/0/1"&gt;David Hin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croft_R/0/1/0/all/0/1"&gt;Roland Croft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1"&gt;M. Ali Babar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure Parameter Optimized Kernel Based Online Prediction with a Generalized Optimization Strategy for Nonstationary Time Series. (arXiv:2108.08180v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.08180</id>
        <link href="http://arxiv.org/abs/2108.08180"/>
        <updated>2021-08-19T01:35:01.204Z</updated>
        <summary type="html"><![CDATA[In this paper, sparsification techniques aided online prediction algorithms
in a reproducing kernel Hilbert space are studied for nonstationary time
series. The online prediction algorithms as usual consist of the selection of
kernel structure parameters and the kernel weight vector updating. For
structure parameters, the kernel dictionary is selected by some sparsification
techniques with online selective modeling criteria, and moreover the kernel
covariance matrix is intermittently optimized in the light of the covariance
matrix adaptation evolution strategy (CMA-ES). Optimizing the real symmetric
covariance matrix can not only improve the kernel structure's flexibility by
the cross relatedness of the input variables, but also partly alleviate the
prediction uncertainty caused by the kernel dictionary selection for
nonstationary time series. In order to sufficiently capture the underlying
dynamic characteristics in prediction-error time series, a generalized
optimization strategy is designed to construct the kernel dictionary
sequentially in multiple kernel connection modes. The generalized optimization
strategy provides a more self-contained way to construct the entire kernel
connections, which enhances the ability to adaptively track the changing
dynamic characteristics. Numerical simulations have demonstrated that the
proposed approach has superior prediction performance for nonstationary time
series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinhua Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sheng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Interpreting Zoonotic Potential of Betacoronavirus Sequences With Attention. (arXiv:2108.08077v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2108.08077</id>
        <link href="http://arxiv.org/abs/2108.08077"/>
        <updated>2021-08-19T01:35:01.193Z</updated>
        <summary type="html"><![CDATA[Current methods for viral discovery target evolutionarily conserved proteins
that accurately identify virus families but remain unable to distinguish the
zoonotic potential of newly discovered viruses. Here, we apply an
attention-enhanced long-short-term memory (LSTM) deep neural net classifier to
a highly conserved viral protein target to predict zoonotic potential across
betacoronaviruses. The classifier performs with a 94% accuracy. Analysis and
visualization of attention at the sequence and structure-level features
indicate possible association between important protein-protein interactions
governing viral replication in zoonotic betacoronaviruses and zoonotic
transmission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wadhawan_K/0/1/0/all/0/1"&gt;Kahini Wadhawan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Han_B/0/1/0/all/0/1"&gt;Barbara A. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Fischhoff_I/0/1/0/all/0/1"&gt;Ilya R. Fischhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Castellanos_A/0/1/0/all/0/1"&gt;Adrian C. Castellanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Varsani_A/0/1/0/all/0/1"&gt;Arvind Varsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised tensor decomposition with features on multiple modes. (arXiv:1910.09499v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09499</id>
        <link href="http://arxiv.org/abs/1910.09499"/>
        <updated>2021-08-19T01:35:01.183Z</updated>
        <summary type="html"><![CDATA[Higher-order tensors have received increased attention across science and
engineering. While most tensor decomposition methods are developed for a single
tensor observation, scientific studies often collect side information, in the
form of node features and interactions thereof, together with the tensor data.
Such data problems are common in neuroimaging, network analysis, and
spatial-temporal modeling. Identifying the relationship between a
high-dimensional tensor and side information is important yet challenging.
Here, we develop a tensor decomposition method that incorporates multiple
feature matrices as side information. Unlike unsupervised tensor decomposition,
our supervised decomposition captures the effective dimension reduction of the
data tensor confined to feature space of interest. An efficient alternating
optimization algorithm with provable spectral initialization is further
developed. Our proposal handles a broad range of data types, including
continuous, count, and binary observations. We apply the method to diffusion
tensor imaging data from human connectome project and multi-relational
political network data. We identify the key global connectivity pattern and
pinpoint the local regions that are associated with available features. Our
simulation code, R-package tensorregress, and datasets used in the paper are
available at https://CRAN.R-project.org/package=tensorregress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jiaxin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chanwoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1"&gt;Miaoyan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Shift Equivariance Impacts Metric Learning for Instance Segmentation. (arXiv:2101.05846v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05846</id>
        <link href="http://arxiv.org/abs/2101.05846"/>
        <updated>2021-08-19T01:35:01.167Z</updated>
        <summary type="html"><![CDATA[Metric learning has received conflicting assessments concerning its
suitability for solving instance segmentation tasks. It has been dismissed as
theoretically flawed due to the shift equivariance of the employed CNNs and
their respective inability to distinguish same-looking objects. Yet it has been
shown to yield state of the art results for a variety of tasks, and practical
issues have mainly been reported in the context of tile-and-stitch approaches,
where discontinuities at tile boundaries have been observed. To date, neither
of the reported issues have undergone thorough formal analysis. In our work, we
contribute a comprehensive formal analysis of the shift equivariance properties
of encoder-decoder-style CNNs, which yields a clear picture of what can and
cannot be achieved with metric learning in the face of same-looking objects. In
particular, we prove that a standard encoder-decoder network that takes
$d$-dimensional images as input, with $l$ pooling layers and pooling factor
$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and
we show that this upper limit can be reached. Furthermore, we show that to
avoid discontinuities in a tile-and-stitch approach, assuming standard batch
size 1, it is necessary to employ valid convolutions in combination with a
training output window size strictly greater than $f^l$, while at test-time it
is necessary to crop tiles to size $n\cdot f^l$ before stitching, with $n\geq
1$. We complement these theoretical findings by discussing a number of
insightful special cases for which we show empirical results on synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rumberger_J/0/1/0/all/0/1"&gt;Josef Lorenz Rumberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaoyan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirsch_P/0/1/0/all/0/1"&gt;Peter Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohmen_M/0/1/0/all/0/1"&gt;Melanie Dohmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guarino_V/0/1/0/all/0/1"&gt;Vanessa Emanuela Guarino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mokarian_A/0/1/0/all/0/1"&gt;Ashkan Mokarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mais_L/0/1/0/all/0/1"&gt;Lisa Mais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1"&gt;Jan Funke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1"&gt;Dagmar Kainmueller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving. (arXiv:2108.08019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08019</id>
        <link href="http://arxiv.org/abs/2108.08019"/>
        <updated>2021-08-19T01:35:01.156Z</updated>
        <summary type="html"><![CDATA[Predictor-based algorithms have achieved remarkable performance in the Neural
Architecture Search (NAS) tasks. However, these methods suffer from high
computation costs, as training the performance predictor usually requires
training and evaluating hundreds of architectures from scratch. Previous works
along this line mainly focus on reducing the number of architectures required
to fit the predictor. In this work, we tackle this challenge from a different
perspective - improve search efficiency by cutting down the computation budget
of architecture training. We propose NOn-uniform Successive Halving (NOSH), a
hierarchical scheduling algorithm that terminates the training of
underperforming architectures early to avoid wasting budget. To effectively
leverage the non-uniform supervision signals produced by NOSH, we formulate
predictor-based architecture search as learning to rank with pairwise
comparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x
while achieving competitive or even better performance than previous
state-of-the-art predictor-based methods on various spaces and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruochen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Minhao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OncoPetNet: A Deep Learning based AI system for mitotic figure counting on H&E stained whole slide digital images in a large veterinary diagnostic lab setting. (arXiv:2108.07856v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07856</id>
        <link href="http://arxiv.org/abs/2108.07856"/>
        <updated>2021-08-19T01:35:01.148Z</updated>
        <summary type="html"><![CDATA[Background: Histopathology is an important modality for the diagnosis and
management of many diseases in modern healthcare, and plays a critical role in
cancer care. Pathology samples can be large and require multi-site sampling,
leading to upwards of 20 slides for a single tumor, and the human-expert tasks
of site selection and and quantitative assessment of mitotic figures are time
consuming and subjective. Automating these tasks in the setting of a digital
pathology service presents significant opportunities to improve workflow
efficiency and augment human experts in practice. Approach: Multiple
state-of-the-art deep learning techniques for histopathology image
classification and mitotic figure detection were used in the development of
OncoPetNet. Additionally, model-free approaches were used to increase speed and
accuracy. The robust and scalable inference engine leverages Pytorch's
performance optimizations as well as specifically developed speed up techniques
in inference. Results: The proposed system, demonstrated significantly improved
mitotic counting performance for 41 cancer cases across 14 cancer types
compared to human expert baselines. In 21.9% of cases use of OncoPetNet led to
change in tumor grading compared to human expert evaluation. In deployment, an
effective 0.27 min/slide inference was achieved in a high throughput veterinary
diagnostic pathology service across 2 centers processing 3,323 digital whole
slide images daily. Conclusion: This work represents the first successful
automated deployment of deep learning systems for real-time expert-level
performance on important histopathology tasks at scale in a high volume
clinical practice. The resulting impact outlines important considerations for
model development, deployment, clinical decision making, and informs best
practices for implementation of deep learning systems in digital histopathology
practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fitzke_M/0/1/0/all/0/1"&gt;Michael Fitzke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Whitley_D/0/1/0/all/0/1"&gt;Derick Whitley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yau_W/0/1/0/all/0/1"&gt;Wilson Yau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1"&gt;Fernando Rodrigues Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fadeev_V/0/1/0/all/0/1"&gt;Vladimir Fadeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bacmeister_C/0/1/0/all/0/1"&gt;Cindy Bacmeister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Carter_C/0/1/0/all/0/1"&gt;Chris Carter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1"&gt;Jeffrey Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parkinson_M/0/1/0/all/0/1"&gt;Mark Parkinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregated Customer Engagement Model. (arXiv:2108.07872v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.07872</id>
        <link href="http://arxiv.org/abs/2108.07872"/>
        <updated>2021-08-19T01:35:01.137Z</updated>
        <summary type="html"><![CDATA[E-commerce websites use machine learned ranking models to serve shopping
results to customers. Typically, the websites log the customer search events,
which include the query entered and the resulting engagement with the shopping
results, such as clicks and purchases. Each customer search event serves as
input training data for the models, and the individual customer engagement
serves as a signal for customer preference. So a purchased shopping result, for
example, is perceived to be more important than one that is not. However, new
or under-impressed products do not have enough customer engagement signals and
end up at a disadvantage when being ranked alongside popular products. In this
paper, we propose a novel method for data curation that aggregates all customer
engagements within a day for the same query to use as input training data. This
aggregated customer engagement gives the models a complete picture of the
relative importance of shopping results. Training models on this aggregated
data leads to less reliance on behavioral features. This helps mitigate the
cold start problem and boosted relevant new products to top search results. In
this paper, we present the offline and online analysis and results comparing
the individual and aggregated customer engagement models trained on e-commerce
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Priya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Han_C/0/1/0/all/0/1"&gt;Cuize Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XAI Methods for Neural Time Series Classification: A Brief Review. (arXiv:2108.08009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08009</id>
        <link href="http://arxiv.org/abs/2108.08009"/>
        <updated>2021-08-19T01:35:01.116Z</updated>
        <summary type="html"><![CDATA[Deep learning models have recently demonstrated remarkable results in a
variety of tasks, which is why they are being increasingly applied in
high-stake domains, such as industry, medicine, and finance. Considering that
automatic predictions in these domains might have a substantial impact on the
well-being of a person, as well as considerable financial and legal
consequences to an individual or a company, all actions and decisions that
result from applying these models have to be accountable. Given that a
substantial amount of data that is collected in high-stake domains are in the
form of time series, in this paper we examine the current state of eXplainable
AI (XAI) methods with a focus on approaches for opening up deep learning black
boxes for the task of time series classification. Finally, our contribution
also aims at deriving promising directions for future work, to advance XAI for
deep learning on time series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simic_I/0/1/0/all/0/1"&gt;Ilija &amp;#x160;imi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabol_V/0/1/0/all/0/1"&gt;Vedran Sabol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veas_E/0/1/0/all/0/1"&gt;Eduardo Veas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07908</id>
        <link href="http://arxiv.org/abs/2108.07908"/>
        <updated>2021-08-19T01:35:01.071Z</updated>
        <summary type="html"><![CDATA[This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent
Component Analysis ('FastICA') method ('m-ar-K-FastICA') for feature
extraction. The kernel trick has enabled dimensionality reduction techniques to
capture a higher extent of non-linearity in the data; however, reproducible,
open-source kernels to aid with feature extraction are still limited and may
not be reliable when projecting features from entropic data. The m-ar-K
function, freely available in Python and compatible with its open-source
library 'scikit-learn', is hereby coupled with FastICA to achieve more reliable
feature extraction in presence of a high extent of randomness in the data,
reducing the need for pre-whitening. Different classification tasks were
considered, as related to five (N = 5) open access datasets of various degrees
of information entropy, available from scikit-learn and the University
California Irvine (UCI) Machine Learning repository. Experimental results
demonstrate improvements in the classification performance brought by the
proposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction
approach is compared to the 'FastICA' gold standard method, supporting its
higher reliability and computational efficiency, regardless of the underlying
uncertainty in the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1"&gt;Luca Parisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Federated Representations and Recommendations with Limited Negatives. (arXiv:2108.07931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07931</id>
        <link href="http://arxiv.org/abs/2108.07931"/>
        <updated>2021-08-19T01:35:01.046Z</updated>
        <summary type="html"><![CDATA[Deep retrieval models are widely used for learning entity representations and
recommendations. Federated learning provides a privacy-preserving way to train
these models without requiring centralization of user data. However, federated
deep retrieval models usually perform much worse than their centralized
counterparts due to non-IID (independent and identically distributed) training
data on clients, an intrinsic property of federated learning that limits
negatives available for training. We demonstrate that this issue is distinct
from the commonly studied client drift problem. This work proposes
batch-insensitive losses as a way to alleviate the non-IID negatives issue for
federated movie recommendation. We explore a variety of techniques and identify
that batch-insensitive losses can effectively improve the performance of
federated deep retrieval models, increasing the relative recall of the
federated model by up to 93.15% and reducing the relative gap in recall between
it and a centralized model from 27.22% - 43.14% to 0.53% - 2.42%. We
open-source our code framework to accelerate further research and applications
of federated deep retrieval models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ning_L/0/1/0/all/0/1"&gt;Lin Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1"&gt;Karan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Ellie X. Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1"&gt;Sushant Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers predicting the future. Applying attention in next-frame and time series forecasting. (arXiv:2108.08224v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08224</id>
        <link href="http://arxiv.org/abs/2108.08224"/>
        <updated>2021-08-19T01:35:01.037Z</updated>
        <summary type="html"><![CDATA[Recurrent Neural Networks were, until recently, one of the best ways to
capture the timely dependencies in sequences. However, with the introduction of
the Transformer, it has been proven that an architecture with only
attention-mechanisms without any RNN can improve on the results in various
sequence processing tasks (e.g. NLP). Multiple studies since then have shown
that similar approaches can be applied for images, point clouds, video, audio
or time series forecasting. Furthermore, solutions such as the Perceiver or the
Informer have been introduced to expand on the applicability of the
Transformer. Our main objective is testing and evaluating the effectiveness of
applying Transformer-like models on time series data, tackling susceptibility
to anomalies, context awareness and space complexity by fine-tuning the
hyperparameters, preprocessing the data, applying dimensionality reduction or
convolutional encodings, etc. We are also looking at the problem of next-frame
prediction and exploring ways to modify existing solutions in order to achieve
higher performance and learn generalized knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cholakov_R/0/1/0/all/0/1"&gt;Radostin Cholakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolev_T/0/1/0/all/0/1"&gt;Todor Kolev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TB-ICT: A Trustworthy Blockchain-Enabled System for Indoor COVID-19 Contact Tracing. (arXiv:2108.08275v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08275</id>
        <link href="http://arxiv.org/abs/2108.08275"/>
        <updated>2021-08-19T01:35:01.031Z</updated>
        <summary type="html"><![CDATA[Recently, as a consequence of the COVID-19 pandemic, dependence on Contact
Tracing (CT) models has significantly increased to prevent spread of this
highly contagious virus and be prepared for the potential future ones. Since
the spreading probability of the novel coronavirus in indoor environments is
much higher than that of the outdoors, there is an urgent and unmet quest to
develop/design efficient, autonomous, trustworthy, and secure indoor CT
solutions. Despite such an urgency, this field is still in its infancy. The
paper addresses this gap and proposes the Trustworthy Blockchain-enabled system
for Indoor Contact Tracing (TB-ICT) framework. The TB-ICT framework is proposed
to protect privacy and integrity of the underlying CT data from unauthorized
access. More specifically, it is a fully distributed and innovative blockchain
platform exploiting the proposed dynamic Proof of Work (dPoW) credit-based
consensus algorithm coupled with Randomized Hash Window (W-Hash) and dynamic
Proof of Credit (dPoC) mechanisms to differentiate between honest and dishonest
nodes. The TB-ICT not only provides a decentralization in data replication but
also quantifies the node's behavior based on its underlying credit-based
mechanism. For achieving high localization performance, we capitalize on
availability of Internet of Things (IoT) indoor localization infrastructures,
and develop a data driven localization model based on Bluetooth Low Energy
(BLE) sensor measurements. The simulation results show that the proposed TB-ICT
prevents the COVID-19 from spreading by implementation of a highly accurate
contact tracing model while improving the users' privacy and security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salimibeni_M/0/1/0/all/0/1"&gt;Mohammad Salimibeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiakhondi_Meybodi_Z/0/1/0/all/0/1"&gt;Zohreh Hajiakhondi-Meybodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1"&gt;Arash Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingxu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Belief Learning. (arXiv:2103.04000v5 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04000</id>
        <link href="http://arxiv.org/abs/2103.04000"/>
        <updated>2021-08-19T01:35:01.014Z</updated>
        <summary type="html"><![CDATA[The standard problem setting in Dec-POMDPs is self-play, where the goal is to
find a set of policies that play optimally together. Policies learned through
self-play may adopt arbitrary conventions and implicitly rely on multi-step
reasoning based on fragile assumptions about other agents' actions and thus
fail when paired with humans or independently trained agents at test time. To
address this, we present off-belief learning (OBL). At each timestep OBL agents
follow a policy $\pi_1$ that is optimized assuming past actions were taken by a
given, fixed policy ($\pi_0$), but assuming that future actions will be taken
by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy
that does not rely on inferences based on other agents' behavior (an optimal
grounded policy). OBL can be iterated in a hierarchy, where the optimal policy
from one level becomes the input to the next, thereby introducing multi-level
cognitive reasoning in a controlled manner. Unlike existing approaches, which
may converge to any equilibrium policy, OBL converges to a unique policy,
making it suitable for zero-shot coordination (ZSC). OBL can be scaled to
high-dimensional settings with a fictitious transition mechanism and shows
strong performance in both a toy-setting and the benchmark human-AI & ZSC
problem Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1"&gt;Adam Lerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;David Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1"&gt;Noam Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bagging Supervised Autoencoder Classifier for Credit Scoring. (arXiv:2108.07800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07800</id>
        <link href="http://arxiv.org/abs/2108.07800"/>
        <updated>2021-08-19T01:35:01.007Z</updated>
        <summary type="html"><![CDATA[Credit scoring models, which are among the most potent risk management tools
that banks and financial institutes rely on, have been a popular subject for
research in the past few decades. Accordingly, many approaches have been
developed to address the challenges in classifying loan applicants and improve
and facilitate decision-making. The imbalanced nature of credit scoring
datasets, as well as the heterogeneous nature of features in credit scoring
datasets, pose difficulties in developing and implementing effective credit
scoring models, targeting the generalization power of classification models on
unseen data. In this paper, we propose the Bagging Supervised Autoencoder
Classifier (BSAC) that mainly leverages the superior performance of the
Supervised Autoencoder, which learns low-dimensional embeddings of the input
data exclusively with regards to the ultimate classification task of credit
scoring, based on the principles of multi-task learning. BSAC also addresses
the data imbalance problem by employing a variant of the Bagging process based
on the undersampling of the majority class. The obtained results from our
experiments on the benchmark and real-life credit scoring datasets illustrate
the robustness and effectiveness of the Bagging Supervised Autoencoder
Classifier in the classification of loan applicants that can be regarded as a
positive development in credit scoring models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdoli_M/0/1/0/all/0/1"&gt;Mahsan Abdoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1"&gt;Mohammad Akbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahrabi_J/0/1/0/all/0/1"&gt;Jamal Shahrabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsely Activated Networks. (arXiv:1907.06592v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.06592</id>
        <link href="http://arxiv.org/abs/1907.06592"/>
        <updated>2021-08-19T01:35:00.997Z</updated>
        <summary type="html"><![CDATA[Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1"&gt;Paschalis Bizopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1"&gt;Dimitrios Koutsouris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stack Index Prediction Using Time-Series Analysis. (arXiv:2108.08120v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08120</id>
        <link href="http://arxiv.org/abs/2108.08120"/>
        <updated>2021-08-19T01:35:00.989Z</updated>
        <summary type="html"><![CDATA[The Prevalence of Community support and engagement for different domains in
the tech industry has changed and evolved throughout the years. In this study,
we aim to understand, analyze and predict the trends of technology in a
scientific manner, having collected data on numerous topics and their growth
throughout the years in the past decade. We apply machine learning models on
collected data, to understand, analyze and forecast the trends in the
advancement of different fields. We show that certain technical concepts such
as python, machine learning, and Keras have an undisputed uptrend, finally
concluding that the Stackindex model forecasts with high accuracy and can be a
viable tool for forecasting different tech domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raman_R/0/1/0/all/0/1"&gt;Raja CSP Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_R/0/1/0/all/0/1"&gt;Rohith Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perumal_D/0/1/0/all/0/1"&gt;Divya Perumal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_V/0/1/0/all/0/1"&gt;Vedha Sankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Talha Abdur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08236</id>
        <link href="http://arxiv.org/abs/2108.08236"/>
        <updated>2021-08-19T01:35:00.982Z</updated>
        <summary type="html"><![CDATA[Recent advances in trajectory prediction have shown that explicit reasoning
about agents' intent is important to accurately forecast their motion. However,
the current research activities are not directly applicable to intelligent and
safety critical systems. This is mainly because very few public datasets are
available, and they only consider pedestrian-specific intents for a short
temporal horizon from a restricted egocentric view. To this end, we propose
LOKI (LOng term and Key Intentions), a novel large-scale dataset that is
designed to tackle joint trajectory and intention prediction for heterogeneous
traffic agents (pedestrians and vehicles) in an autonomous driving setting. The
LOKI dataset is created to discover several factors that may affect intention,
including i) agent's own will, ii) social interactions, iii) environmental
constraints, and iv) contextual information. We also propose a model that
jointly performs trajectory and intention prediction, showing that recurrently
reasoning about intention can assist with trajectory prediction. We show our
method outperforms state-of-the-art trajectory prediction methods by upto
$27\%$ and also provide a baseline for frame-wise intention estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1"&gt;Harshayu Girase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1"&gt;Haiming Gang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1"&gt;Srikanth Malla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1"&gt;Akira Kanehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1"&gt;Karttikeya Mangalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1"&gt;Chiho Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distinguishing Healthy Ageing from Dementia: a Biomechanical Simulation of Brain Atrophy using Deep Networks. (arXiv:2108.08214v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2108.08214</id>
        <link href="http://arxiv.org/abs/2108.08214"/>
        <updated>2021-08-19T01:35:00.974Z</updated>
        <summary type="html"><![CDATA[Biomechanical modeling of tissue deformation can be used to simulate
different scenarios of longitudinal brain evolution. In this work,we present a
deep learning framework for hyper-elastic strain modelling of brain atrophy,
during healthy ageing and in Alzheimer's Disease. The framework directly models
the effects of age, disease status, and scan interval to regress regional
patterns of atrophy, from which a strain-based model estimates deformations.
This model is trained and validated using 3D structural magnetic resonance
imaging data from the ADNI cohort. Results show that the framework can estimate
realistic deformations, following the known course of Alzheimer's disease, that
clearly differentiate between healthy and demented patterns of ageing. This
suggests the framework has potential to be incorporated into explainable models
of disease, for the exploration of interventions and counterfactual examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mariana Da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sudre_C/0/1/0/all/0/1"&gt;Carole H. Sudre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Garcia_K/0/1/0/all/0/1"&gt;Kara Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bass_C/0/1/0/all/0/1"&gt;Cher Bass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cardoso_M/0/1/0/all/0/1"&gt;M. Jorge Cardoso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1"&gt;Emma C. Robinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Dynamic Stability of Power Grids using Graph Neural Networks. (arXiv:2108.08230v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2108.08230</id>
        <link href="http://arxiv.org/abs/2108.08230"/>
        <updated>2021-08-19T01:35:00.968Z</updated>
        <summary type="html"><![CDATA[The prediction of dynamical stability of power grids becomes more important
and challenging with increasing shares of renewable energy sources due to their
decentralized structure, reduced inertia and volatility. We investigate the
feasibility of applying graph neural networks (GNN) to predict dynamic
stability of synchronisation in complex power grids using the single-node basin
stability (SNBS) as a measure. To do so, we generate two synthetic datasets for
grids with 20 and 100 nodes respectively and estimate SNBS using Monte-Carlo
sampling. Those datasets are used to train and evaluate the performance of
eight different GNN-models. All models use the full graph without
simplifications as input and predict SNBS in a nodal-regression-setup. We show
that SNBS can be predicted in general and the performance significantly changes
using different GNN-models. Furthermore, we observe interesting transfer
capabilities of our approach: GNN-models trained on smaller grids can directly
be applied on larger grids without the need of retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Nauck_C/0/1/0/all/0/1"&gt;Christian Nauck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lindner_M/0/1/0/all/0/1"&gt;Michael Lindner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schurholt_K/0/1/0/all/0/1"&gt;Konstantin Sch&amp;#xfc;rholt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schultz_P/0/1/0/all/0/1"&gt;Paul Schultz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kurths_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Kurths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Isenhardt_I/0/1/0/all/0/1"&gt;Ingrid Isenhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hellmann_F/0/1/0/all/0/1"&gt;Frank Hellmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-DARTS: Towards Stable Architecture Search. (arXiv:2108.08128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08128</id>
        <link href="http://arxiv.org/abs/2108.08128"/>
        <updated>2021-08-19T01:35:00.961Z</updated>
        <summary type="html"><![CDATA[Differentiable architecture search (DARTS) marks a milestone in Neural
Architecture Search (NAS), boasting simplicity and small search costs. However,
DARTS still suffers from frequent performance collapse, which happens when some
operations, such as skip connections, zeroes and poolings, dominate the
architecture. In this paper, we are the first to point out that the phenomenon
is attributed to bi-level optimization. We propose Single-DARTS which merely
uses single-level optimization, updating network weights and architecture
parameters simultaneously with the same data batch. Even single-level
optimization has been previously attempted, no literature provides a systematic
explanation on this essential point. Replacing the bi-level optimization,
Single-DARTS obviously alleviates performance collapse as well as enhances the
stability of architecture search. Experiment results show that Single-DARTS
achieves state-of-the-art performance on mainstream search spaces. For
instance, on NAS-Benchmark-201, the searched architectures are nearly optimal
ones. We also validate that the single-level optimization framework is much
more stable than the bi-level one. We hope that this simple yet effective
method will give some insights on differential architecture search. The code is
available at https://github.com/PencilAndBike/Single-DARTS.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1"&gt;Pengfei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Ying Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yukang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08195</id>
        <link href="http://arxiv.org/abs/2108.08195"/>
        <updated>2021-08-19T01:35:00.911Z</updated>
        <summary type="html"><![CDATA[Due to morphological similarity at the microscopic level, making an accurate
and time-sensitive distinction between blood cells affected by Acute
Lymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage
of machine learning architectures. However, three of the most common models,
VGG, ResNet, and Inception, each come with their own set of flaws with room for
improvement which demands the need for a superior model. ALLNet, the proposed
hybrid convolutional neural network architecture, consists of a combination of
the VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019
(available here) contains 10,691 images of white blood cells which were used to
train and test the models. 7,272 of the images in the dataset are of cells with
ALL and 3,419 of them are of healthy cells. Of the images, 60% were used to
train the model, 20% were used for the cross-validation set, and 20% were used
for the test set. ALLNet outperformed the VGG, ResNet, and the Inception models
across the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,
a specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803
in the cross-validation set. In the test set, ALLNet achieved an accuracy of
92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of
0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the
clinical workspace can better treat the thousands of people suffering from ALL
across the world, many of whom are children.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1"&gt;Sai Mattapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1"&gt;Rishi Athavale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08217</id>
        <link href="http://arxiv.org/abs/2108.08217"/>
        <updated>2021-08-19T01:35:00.905Z</updated>
        <summary type="html"><![CDATA[With the rise and development of deep learning over the past decade, there
has been a steady momentum of innovation and breakthroughs that convincingly
push the state-of-the-art of cross-modal analytics between vision and language
in multimedia field. Nevertheless, there has not been an open-source codebase
in support of training and deploying numerous neural network models for
cross-modal analytics in a unified and modular fashion. In this work, we
propose X-modaler -- a versatile and high-performance codebase that
encapsulates the state-of-the-art cross-modal analytics into several
general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,
decoder, and decode strategy). Each stage is empowered with the functionality
that covers a series of modules widely adopted in state-of-the-arts and allows
seamless switching in between. This way naturally enables a flexible
implementation of state-of-the-art algorithms for image captioning, video
captioning, and vision-language pre-training, aiming to facilitate the rapid
development of research community. Meanwhile, since the effective modular
designs in several stages (e.g., cross-modal interaction) are shared across
different vision-language tasks, X-modaler can be simply extended to power
startup prototypes for other tasks in cross-modal analytics, including visual
question answering, visual commonsense reasoning, and cross-modal retrieval.
X-modaler is an Apache-licensed codebase, and its source codes, sample projects
and pre-trained models are available on-line:
https://github.com/YehLi/xmodaler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yehao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepExpress: Heterogeneous and Coupled Sequence Modeling for Express Delivery Prediction. (arXiv:2108.08170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08170</id>
        <link href="http://arxiv.org/abs/2108.08170"/>
        <updated>2021-08-19T01:35:00.897Z</updated>
        <summary type="html"><![CDATA[The prediction of express delivery sequence, i.e., modeling and estimating
the volumes of daily incoming and outgoing parcels for delivery, is critical
for online business, logistics, and positive customer experience, and
specifically for resource allocation optimization and promotional activity
arrangement. A precise estimate of consumer delivery requests has to involve
sequential factors such as shopping behaviors, weather conditions, events,
business campaigns, and their couplings. Besides, conventional sequence
prediction assumes a stable sequence evolution, failing to address complex
nonlinear sequences and various feature effects in the above multi-source data.
Although deep networks and attention mechanisms demonstrate the potential of
complex sequence modeling, extant networks ignore the heterogeneous and
coupling situation between features and sequences, resulting in weak prediction
accuracy. To address these issues, we propose DeepExpress - a deep-learning
based express delivery sequence prediction model, which extends the classic
seq2seq framework to learning complex coupling between sequence and features.
DeepExpress leverages an express delivery seq2seq learning, a
carefully-designed heterogeneous feature representation, and a novel joint
training attention mechanism to adaptively map heterogeneous data, and capture
sequence-feature coupling for precise estimation. Experimental results on
real-world data demonstrate that the proposed method outperforms both shallow
and deep baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Siyuan Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Bin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiwen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07971</id>
        <link href="http://arxiv.org/abs/2108.07971"/>
        <updated>2021-08-19T01:35:00.890Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel problem formulation for de-identification of
unstructured clinical text. We formulate the de-identification problem as a
sequence to sequence learning problem instead of a token classification
problem. Our approach is inspired by the recent state-of -the-art performance
of sequence to sequence learning models for named entity recognition. Early
experimentation of our proposed approach achieved 98.91% recall rate on i2b2
dataset. This performance is comparable to current state-of-the-art models for
unstructured clinical text de-identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1"&gt;Md Monowar Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1"&gt;Noman Mohammed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoqian Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations. (arXiv:2108.08197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08197</id>
        <link href="http://arxiv.org/abs/2108.08197"/>
        <updated>2021-08-19T01:35:00.883Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanation methods interpret the outputs of a machine
learning model in the form of "what-if scenarios" without compromising the
fidelity-interpretability trade-off. They explain how to obtain a desired
prediction from the model by recommending small changes to the input features,
aka recourse. We believe an actionable recourse should be created based on
sound counterfactual explanations originating from the distribution of the
ground-truth data and linked to the domain knowledge. Moreover, it needs to
preserve the coherency between changed/unchanged features while satisfying
user/domain-specified constraints. This paper introduces CARE, a modular
explanation framework that addresses the model- and user-level desiderata in a
consecutive and structured manner. We tackle the existing requirements by
proposing novel and efficient solutions that are formulated in a
multi-objective optimization framework. The designed framework enables
including arbitrary requirements and generating counterfactual explanations and
actionable recourse by choice. As a model-agnostic approach, CARE generates
multiple, diverse explanations for any black-box model in tabular
classification and regression settings. Several experiments on standard data
sets and black-box models demonstrate the effectiveness of our modular
framework and its superior performance compared to the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rasouli_P/0/1/0/all/0/1"&gt;Peyman Rasouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1"&gt;Ingrid Chieh Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Resize Images for Computer Vision Tasks. (arXiv:2103.09950v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09950</id>
        <link href="http://arxiv.org/abs/2103.09950"/>
        <updated>2021-08-19T01:35:00.863Z</updated>
        <summary type="html"><![CDATA[For all the ways convolutional neural nets have revolutionized computer
vision in recent years, one important aspect has received surprisingly little
attention: the effect of image size on the accuracy of tasks being trained for.
Typically, to be efficient, the input images are resized to a relatively small
spatial resolution (e.g. 224x224), and both training and inference are carried
out at this resolution. The actual mechanism for this re-scaling has been an
afterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic
are commonly used in most machine learning software frameworks. But do these
resizers limit the on task performance of the trained networks? The answer is
yes. Indeed, we show that the typical linear resizer can be replaced with
learned resizers that can substantially improve performance. Importantly, while
the classical resizers typically result in better perceptual quality of the
downscaled images, our proposed learned resizers do not necessarily give better
visual quality, but instead improve task performance. Our learned image resizer
is jointly trained with a baseline vision model. This learned CNN-based resizer
creates machine friendly visual manipulations that lead to a consistent
improvement of the end task metric over the baseline model. Specifically, here
we focus on the classification task with the ImageNet dataset, and experiment
with four different models to learn resizers adapted to each model. Moreover,
we show that the proposed resizer can also be useful for fine-tuning the
classification baselines for other vision tasks. To this end, we experiment
with three different baselines to develop image quality assessment (IQA) models
on the AVA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1"&gt;Hossein Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1"&gt;Peyman Milanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07886</id>
        <link href="http://arxiv.org/abs/2108.07886"/>
        <updated>2021-08-19T01:35:00.855Z</updated>
        <summary type="html"><![CDATA[Generating context-aware language that embodies diverse emotions is an
important step towards building empathetic NLP systems. In this paper, we
propose a formulation of modulated layer normalization -- a technique inspired
by computer vision -- that allows us to use large-scale language models for
emotional response generation. In automatic and human evaluation on the
MojiTalk dataset, our proposed modulated layer normalization method outperforms
prior baseline methods while maintaining diversity, fluency, and coherence. Our
method also obtains competitive performance even when using only 10% of the
available training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chenyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsely Activated Networks: A new method for decomposing and compressing data. (arXiv:1911.00400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.00400</id>
        <link href="http://arxiv.org/abs/1911.00400"/>
        <updated>2021-08-19T01:35:00.846Z</updated>
        <summary type="html"><![CDATA[Recent literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features, but without considering
the description length of the representations. In this thesis, first we
introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined metric $\varphi$. We lastly present Sparsely Activated
Networks (SANs) that consist of kernels with shared weights that, during
encoding, are convolved with the input and then passed through a sparse
activation function. During decoding, the same weights are convolved with the
sparse activation map and subsequently the partial reconstructions from each
weight are summed to reconstruct the input. We compare SANs using the five
previously defined activation functions on a variety of datasets (Physionet,
UCI-epilepsy, MNIST, FMNIST) and show that models that are selected using
$\varphi$ have small description representation length and consist of
interpretable kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1"&gt;Paschalis Bizopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Graph Normalized Auto-Encoders. (arXiv:2108.08046v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08046</id>
        <link href="http://arxiv.org/abs/2108.08046"/>
        <updated>2021-08-19T01:35:00.836Z</updated>
        <summary type="html"><![CDATA[Link prediction is one of the key problems for graph-structured data. With
the advancement of graph neural networks, graph autoencoders (GAEs) and
variational graph autoencoders (VGAEs) have been proposed to learn graph
embeddings in an unsupervised way. It has been shown that these methods are
effective for link prediction tasks. However, they do not work well in link
predictions when a node whose degree is zero (i.g., isolated node) is involved.
We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero
regardless of their content features. In this paper, we propose a novel
Variational Graph Normalized AutoEncoder (VGNAE) that utilize
$L_2$-normalization to derive better embeddings for isolated nodes. We show
that our VGNAEs outperform the existing state-of-the-art models for link
prediction tasks. The code is available at
https://github.com/SeongJinAhn/VGNAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Seong Jin Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Myoung Ho Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned holographic light transport. (arXiv:2108.08253v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2108.08253</id>
        <link href="http://arxiv.org/abs/2108.08253"/>
        <updated>2021-08-19T01:35:00.823Z</updated>
        <summary type="html"><![CDATA[Computer-Generated Holography (CGH) algorithms often fall short in matching
simulations with results from a physical holographic display. Our work
addresses this mismatch by learning the holographic light transport in
holographic displays. Using a camera and a holographic display, we capture the
image reconstructions of optimized holograms that rely on ideal simulations to
generate a dataset. Inspired by the ideal simulations, we learn a
complex-valued convolution kernel that can propagate given holograms to
captured photographs in our dataset. Our method can dramatically improve
simulation accuracy and image quality in holographic displays while paving the
way for physically informed learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kavakli_K/0/1/0/all/0/1"&gt;Koray Kavakl&amp;#x131;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Urey_H/0/1/0/all/0/1"&gt;Hakan Urey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Aksit_K/0/1/0/all/0/1"&gt;Kaan Ak&amp;#x15f;it&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperSF: Spectral Hypergraph Coarsening via Flow-based Local Clustering. (arXiv:2108.07901v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07901</id>
        <link href="http://arxiv.org/abs/2108.07901"/>
        <updated>2021-08-19T01:35:00.797Z</updated>
        <summary type="html"><![CDATA[Hypergraphs allow modeling problems with multi-way high-order relationships.
However, the computational cost of most existing hypergraph-based algorithms
can be heavily dependent upon the input hypergraph sizes. To address the
ever-increasing computational challenges, graph coarsening can be potentially
applied for preprocessing a given hypergraph by aggressively aggregating its
vertices (nodes). However, state-of-the-art hypergraph partitioning
(clustering) methods that incorporate heuristic graph coarsening techniques are
not optimized for preserving the structural (global) properties of hypergraphs.
In this work, we propose an efficient spectral hypergraph coarsening scheme
(HyperSF) for well preserving the original spectral (structural) properties of
hypergraphs. Our approach leverages a recent strongly-local max-flow-based
clustering algorithm for detecting the sets of hypergraph vertices that
minimize ratio cut. To further improve the algorithm efficiency, we propose a
divide-and-conquer scheme by leveraging spectral clustering of the bipartite
graphs corresponding to the original hypergraphs. Our experimental results for
a variety of hypergraphs extracted from real-world VLSI design benchmarks show
that the proposed hypergraph coarsening algorithm can significantly improve the
multi-way conductance of hypergraph clustering as well as runtime efficiency
when compared with existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaei_A/0/1/0/all/0/1"&gt;Ali Aghdaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhiqiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhuo Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FOX-NAS: Fast, On-device and Explainable Neural Architecture Search. (arXiv:2108.08189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08189</id>
        <link href="http://arxiv.org/abs/2108.08189"/>
        <updated>2021-08-19T01:35:00.789Z</updated>
        <summary type="html"><![CDATA[Neural architecture search can discover neural networks with good
performance, and One-Shot approaches are prevalent. One-Shot approaches
typically require a supernet with weight sharing and predictors that predict
the performance of architecture. However, the previous methods take much time
to generate performance predictors thus are inefficient. To this end, we
propose FOX-NAS that consists of fast and explainable predictors based on
simulated annealing and multivariate regression. Our method is
quantization-friendly and can be efficiently deployed to the edge. The
experiments on different hardware show that FOX-NAS models outperform some
other popular neural network architectures. For example, FOX-NAS matches
MobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on
the edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer
Vision Challenge (LPCVC), DSP classification track. See all evaluation results
at https://lpcv.ai/competitions/2020. Search code and pre-trained models are
released at https://github.com/great8nctu/FOX-NAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chia-Hsiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yu-Shin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Yuan-Yao Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hung-Yueh Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kai-Chiang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Detection using Outlier Detection Methods. (arXiv:2108.08218v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08218</id>
        <link href="http://arxiv.org/abs/2108.08218"/>
        <updated>2021-08-19T01:35:00.781Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution detection (OOD) deals with anomalous input to neural
networks. In the past, specialized methods have been proposed to reject
predictions on anomalous input. We use outlier detection algorithms to detect
anomalous input as reliable as specialized methods from the field of OOD. No
neural network adaptation is required; detection is based on the model's
softmax score. Our approach works unsupervised with an Isolation Forest or with
supervised classifiers such as a Gradient Boosting machine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diers_J/0/1/0/all/0/1"&gt;Jan Diers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pigorsch_C/0/1/0/all/0/1"&gt;Christian Pigorsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Verified Neural Networks via Floating Point Numerical Error. (arXiv:2003.03021v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03021</id>
        <link href="http://arxiv.org/abs/2003.03021"/>
        <updated>2021-08-19T01:35:00.761Z</updated>
        <summary type="html"><![CDATA[Researchers have developed neural network verification algorithms motivated
by the need to characterize the robustness of deep neural networks. The
verifiers aspire to answer whether a neural network guarantees certain
properties with respect to all inputs in a space. However, many verifiers
inaccurately model floating point arithmetic but do not thoroughly discuss the
consequences.

We show that the negligence of floating point error leads to unsound
verification that can be systematically exploited in practice. For a pretrained
neural network, we present a method that efficiently searches inputs as
witnesses for the incorrectness of robustness claims made by a complete
verifier. We also present a method to construct neural network architectures
and weights that induce wrong results of an incomplete verifier. Our results
highlight that, to achieve practically reliable verification of neural
networks, any verification system must accurately (or conservatively) model the
effects of any floating point computations in the network inference or
verification system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kai Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1"&gt;Martin Rinard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moser Flow: Divergence-based Generative Modeling on Manifolds. (arXiv:2108.08052v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08052</id>
        <link href="http://arxiv.org/abs/2108.08052"/>
        <updated>2021-08-19T01:35:00.707Z</updated>
        <summary type="html"><![CDATA[We are interested in learning generative models for complex geometries
described via manifolds, such as spheres, tori, and other implicit surfaces.
Current extensions of existing (Euclidean) generative models are restricted to
specific geometries and typically suffer from high computational costs. We
introduce Moser Flow (MF), a new class of generative models within the family
of continuous normalizing flows (CNF). MF also produces a CNF via a solution to
the change-of-variable formula, however differently from other CNF methods, its
model (learned) density is parameterized as the source (prior) density minus
the divergence of a neural network (NN). The divergence is a local, linear
differential operator, easy to approximate and calculate on manifolds.
Therefore, unlike other CNFs, MF does not require invoking or backpropagating
through an ODE solver during training. Furthermore, representing the model
density explicitly as the divergence of a NN rather than as a solution of an
ODE facilitates learning high fidelity densities. Theoretically, we prove that
MF constitutes a universal density approximator under suitable assumptions.
Empirically, we demonstrate for the first time the use of flow models for
sampling from general curved surfaces and achieve significant improvements in
density estimation, sample quality, and training complexity over existing CNFs
on challenging synthetic geometries and real-world benchmarks from the earth
and climate sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rozen_N/0/1/0/all/0/1"&gt;Noam Rozen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nickel_M/0/1/0/all/0/1"&gt;Maximilian Nickel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection. (arXiv:2108.07897v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07897</id>
        <link href="http://arxiv.org/abs/2108.07897"/>
        <updated>2021-08-19T01:35:00.683Z</updated>
        <summary type="html"><![CDATA[Automated systems that detect the social behavior of deception can enhance
human well-being across medical, social work, and legal domains. Labeled
datasets to train supervised deception detection models can rarely be collected
for real-world, high-stakes contexts. To address this challenge, we propose the
first unsupervised approach for detecting real-world, high-stakes deception in
videos without requiring labels. This paper presents our novel approach for
affect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative
representations of deceptive and truthful behavior. Drawing on psychology
theories that link affect and deception, we experimented with unimodal and
multimodal DBN-based approaches trained on facial valence, facial arousal,
audio, and visual features. In addition to using facial affect as a feature on
which DBN models are trained, we also introduce a DBN training procedure that
uses facial affect as an aligner of audio-visual representations. We conducted
classification experiments with unsupervised Gaussian Mixture Model clustering
to evaluate our approaches. Our best unsupervised approach (trained on facial
valence and visual features) achieved an AUC of 80%, outperforming human
ability and performing comparably to fully-supervised models. Our results
motivate future work on unsupervised, affect-aware computational approaches for
detecting deception and other social behaviors in the wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1"&gt;Leena Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1"&gt;Maja J Matari&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversity-based Trajectory and Goal Selection with Hindsight Experience Replay. (arXiv:2108.07887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07887</id>
        <link href="http://arxiv.org/abs/2108.07887"/>
        <updated>2021-08-19T01:35:00.640Z</updated>
        <summary type="html"><![CDATA[Hindsight experience replay (HER) is a goal relabelling technique typically
used with off-policy deep reinforcement learning algorithms to solve
goal-oriented tasks; it is well suited to robotic manipulation tasks that
deliver only sparse rewards. In HER, both trajectories and transitions are
sampled uniformly for training. However, not all of the agent's experiences
contribute equally to training, and so naive uniform sampling may lead to
inefficient learning. In this paper, we propose diversity-based trajectory and
goal selection with HER (DTGSH). Firstly, trajectories are sampled according to
the diversity of the goal states as modelled by determinantal point processes
(DPPs). Secondly, transitions with diverse goal states are selected from the
trajectories by using k-DPPs. We evaluate DTGSH on five challenging robotic
manipulation tasks in simulated robot environments, where we show that our
method can learn more quickly and reach higher performance than other
state-of-the-art approaches on all tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1"&gt;Tianhong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hengyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1"&gt;Kai Arulkumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1"&gt;Guangyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1"&gt;Anil Anthony Bharath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06412</id>
        <link href="http://arxiv.org/abs/2009.06412"/>
        <updated>2021-08-19T01:35:00.632Z</updated>
        <summary type="html"><![CDATA[Recently there has been an explosion in the use of Deep Learning (DL) methods
for medical image segmentation. However the field's reliability is hindered by
the lack of a common base of reference for accuracy/performance evaluation and
the fact that previous research uses different datasets for evaluation. In this
paper, an extensive comparison of DL models for lung and COVID-19 lesion
segmentation in Computerized Tomography (CT) scans is presented, which can also
be used as a benchmark for testing medical image segmentation models. Four DL
architectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomly
initialized and pretrained encoders (variations of VGG, DenseNet, ResNet,
ResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct
200 tested models. Three experimental setups are conducted for lung
segmentation, lesion segmentation and lesion segmentation using the original
lung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20
for validation) is used for training/validation and a different public dataset
consisting of 829 images from 9 CT scan volumes for testing. Multiple findings
are provided including the best architecture-encoder models for each experiment
as well as mean Dice results for each experiment, architecture and encoder
independently. Finally, the upper bounds improvements when using lung masks as
a preprocessing step or when using pretrained models are quantified. The source
code and 600 pretrained models for the three experiments are provided, suitable
for fine-tuning in experimental setups without GPU capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1"&gt;Paschalis Bizopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vretos_N/0/1/0/all/0/1"&gt;Nicholas Vretos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Daras_P/0/1/0/all/0/1"&gt;Petros Daras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08689</id>
        <link href="http://arxiv.org/abs/1911.08689"/>
        <updated>2021-08-19T01:35:00.625Z</updated>
        <summary type="html"><![CDATA[We initiate the study of multi-stage episodic reinforcement learning under
adversarial corruptions in both the rewards and the transition probabilities of
the underlying system extending recent results for the special case of
stochastic bandits. We provide a framework which modifies the aggressive
exploration enjoyed by existing reinforcement learning approaches based on
"optimism in the face of uncertainty", by complementing them with principles
from "action elimination". Importantly, our framework circumvents the major
challenges posed by naively applying action elimination in the RL setting, as
formalized by a lower bound we demonstrate. Our framework yields efficient
algorithms which (a) attain near-optimal regret in the absence of corruptions
and (b) adapt to unknown levels corruption, enjoying regret guarantees which
degrade gracefully in the total corruption encountered. To showcase the
generality of our approach, we derive results for both tabular settings (where
states and actions are finite) as well as linear-function-approximation
settings (where the dynamics and rewards admit a linear underlying
representation). Notably, our work provides the first sublinear regret
guarantee which accommodates any deviation from purely i.i.d. transitions in
the bandit-feedback model for episodic reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1"&gt;Thodoris Lykouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Verifying Low-dimensional Input Neural Networks via Input Quantization. (arXiv:2108.07961v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07961</id>
        <link href="http://arxiv.org/abs/2108.07961"/>
        <updated>2021-08-19T01:35:00.574Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are an attractive tool for compressing the control
policy lookup tables in systems such as the Airborne Collision Avoidance System
(ACAS). It is vital to ensure the safety of such neural controllers via
verification techniques. The problem of analyzing ACAS Xu networks has
motivated many successful neural network verifiers. These verifiers typically
analyze the internal computation of neural networks to decide whether a
property regarding the input/output holds. The intrinsic complexity of neural
network computation renders such verifiers slow to run and vulnerable to
floating-point error.

This paper revisits the original problem of verifying ACAS Xu networks. The
networks take low-dimensional sensory inputs with training data provided by a
precomputed lookup table. We propose to prepend an input quantization layer to
the network. Quantization allows efficient verification via input state
enumeration, whose complexity is bounded by the size of the quantization space.
Quantization is equivalent to nearest-neighbor interpolation at run time, which
has been shown to provide acceptable accuracy for ACAS in simulation. Moreover,
our technique can deliver exact verification results immune to floating-point
error if we directly enumerate the network outputs on the target inference
implementation or on an accurate simulation of the target implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kai Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1"&gt;Martin Rinard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning distant cause and effect using only local and immediate credit assignment. (arXiv:1905.11589v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11589</id>
        <link href="http://arxiv.org/abs/1905.11589"/>
        <updated>2021-08-19T01:35:00.470Z</updated>
        <summary type="html"><![CDATA[We present a recurrent neural network memory that uses sparse coding to
create a combinatoric encoding of sequential inputs. Using several examples, we
show that the network can associate distant causes and effects in a discrete
stochastic process, predict partially-observable higher-order sequences, and
enable a DQN agent to navigate a maze by giving it memory. The network uses
only biologically-plausible, local and immediate credit assignment. Memory
requirements are typically one order of magnitude less than existing LSTM, GRU
and autoregressive feed-forward sequence learning models. The most significant
limitation of the memory is generalization to unseen input sequences. We
explore this limitation by measuring next-word prediction perplexity on the
Penn Treebank dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rawlinson_D/0/1/0/all/0/1"&gt;David Rawlinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Abdelrahman Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kowadlo_G/0/1/0/all/0/1"&gt;Gideon Kowadlo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OACAL: Finding Module-Consistent Solutions to Weaken User Obligations. (arXiv:2108.08282v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08282</id>
        <link href="http://arxiv.org/abs/2108.08282"/>
        <updated>2021-08-19T01:35:00.454Z</updated>
        <summary type="html"><![CDATA[Users interacting with a UI-embedded machine or system are typically obliged
to perform their actions in a pre-determined order, to successfully achieve
certain functional goals. However, such obligations are often not followed
strictly by users, which may lead to the violation to security properties,
especially in security-critical systems. In order to improve the security with
the awareness of unexpected user behaviors, a system can be redesigned to a
more robust one by changing the order of actions in its specification.
Meanwhile, we anticipate that the functionalities would remain consistent
following the modifications. In this paper, we propose an efficient algorithm
to automatically produce specification revisions tackling with attack scenarios
caused by the weakened user obligations. By our algorithm, all the revisions
maintain the integrity of the functionalities as the original specification,
which are generated using a novel recomposition approach. Then, the qualified
revisions that can satisfy the security requirements would be efficiently
spotted by a hybrid approach combining model checking and machine learning
techniques. We evaluate our algorithm by comparing its performance with a
state-of-the-art approach regarding their coverage and searching speed of the
desirable revisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Pengcheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tei_K/0/1/0/all/0/1"&gt;Kenji Tei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory. (arXiv:2108.07879v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.07879</id>
        <link href="http://arxiv.org/abs/2108.07879"/>
        <updated>2021-08-19T01:35:00.445Z</updated>
        <summary type="html"><![CDATA[Realizing today's cloud-level artificial intelligence functionalities
directly on devices distributed at the edge of the internet calls for edge
hardware capable of processing multiple modalities of sensory data (e.g. video,
audio) at unprecedented energy-efficiency. AI hardware architectures today
cannot meet the demand due to a fundamental "memory wall": data movement
between separate compute and memory units consumes large energy and incurs long
latency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)
architectures promise to bring orders of magnitude energy-efficiency
improvement by performing computation directly within memory. However,
conventional approaches to CIM hardware design limit its functional flexibility
necessary for processing diverse AI workloads, and must overcome hardware
imperfections that degrade inference accuracy. Such trade-offs between
efficiency, versatility and accuracy cannot be addressed by isolated
improvements on any single level of the design. By co-optimizing across all
hierarchies of the design from algorithms and architecture to circuits and
devices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM
to simultaneously deliver a high degree of versatility for diverse model
architectures, record energy-efficiency $5\times$ - $8\times$ better than prior
art across various computational bit-precisions, and inference accuracy
comparable to software models with 4-bit weights on all measured standard AI
benchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image
classification, 84.7% accuracy on Google speech command recognition, and a 70%
reduction in image reconstruction error on a Bayesian image recovery task. This
work paves a way towards building highly efficient and reconfigurable edge AI
hardware platforms for the more demanding and heterogeneous AI applications of
the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1"&gt;Weier Wan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Kubendran_R/0/1/0/all/0/1"&gt;Rajkumar Kubendran&lt;/a&gt; (2 and 5), &lt;a href="http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1"&gt;Clemens Schaefer&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Eryilmaz_S/0/1/0/all/0/1"&gt;S. Burc Eryilmaz&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenqiang Zhang&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dabin Wu&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Deiss_S/0/1/0/all/0/1"&gt;Stephen Deiss&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_P/0/1/0/all/0/1"&gt;Priyanka Raina&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;He Qian&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Bin Gao&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Siddharth Joshi&lt;/a&gt; (4 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huaqiang Wu&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_H/0/1/0/all/0/1"&gt;H.-S. Philip Wong&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Cauwenberghs_G/0/1/0/all/0/1"&gt;Gert Cauwenberghs&lt;/a&gt; (2) ((1) Stanford University, (2) University of California San Diego, (3) Tsinghua University, (4) University of Notre Dame, (5) University of Pittsburgh)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.08264</id>
        <link href="http://arxiv.org/abs/2108.08264"/>
        <updated>2021-08-19T01:35:00.438Z</updated>
        <summary type="html"><![CDATA[Expert systems have been used to enable computers to make recommendations and
decisions. This paper presents the use of a machine learning trained expert
system (MLES) for phishing site detection and fake news detection. Both topics
share a similar goal: to design a rule-fact network that allows a computer to
make explainable decisions like domain experts in each respective area. The
phishing website detection study uses a MLES to detect potential phishing
websites by analyzing site properties (like URL length and expiration time).
The fake news detection study uses a MLES rule-fact network to gauge news story
truthfulness based on factors such as emotion, the speaker's political
affiliation status, and job. The two studies use different MLES network
implementations, which are presented and compared herein. The fake news study
utilized a more linear design while the phishing project utilized a more
complex connection structure. Both networks' inputs are based on commonly
available data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1"&gt;Benjamin Fitzpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xinyu &amp;quot;Sherwin&amp;quot; Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1"&gt;Jeremy Straub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Extensible Benchmark Suite for Learning to Simulate Physical Systems. (arXiv:2108.07799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07799</id>
        <link href="http://arxiv.org/abs/2108.07799"/>
        <updated>2021-08-19T01:35:00.423Z</updated>
        <summary type="html"><![CDATA[Simulating physical systems is a core component of scientific computing,
encompassing a wide range of physical domains and applications. Recently, there
has been a surge in data-driven methods to complement traditional numerical
simulations methods, motivated by the opportunity to reduce computational costs
and/or learn new physical models leveraging access to large collections of
data. However, the diversity of problem settings and applications has led to a
plethora of approaches, each one evaluated on a different setup and with
different evaluation metrics. We introduce a set of benchmark problems to take
a step towards unified benchmarks and evaluation protocols. We propose four
representative physical systems, as well as a collection of both widely used
classical time integrators and representative data-driven methods
(kernel-based, MLP, CNN, nearest neighbors). Our framework allows evaluating
objectively and systematically the stability, accuracy, and computational
efficiency of data-driven methods. Additionally, it is configurable to permit
adjustments for accommodating other learning tasks and for establishing a
foundation for future developments in machine learning for scientific
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Otness_K/0/1/0/all/0/1"&gt;Karl Otness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gjoka_A/0/1/0/all/0/1"&gt;Arvi Gjoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1"&gt;Daniele Panozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1"&gt;Benjamin Peherstorfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_T/0/1/0/all/0/1"&gt;Teseo Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1"&gt;Denis Zorin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13736</id>
        <link href="http://arxiv.org/abs/2106.13736"/>
        <updated>2021-08-19T01:35:00.347Z</updated>
        <summary type="html"><![CDATA[While pretrained encoders have achieved success in various natural language
understanding (NLU) tasks, there is a gap between these pretrained encoders and
natural language generation (NLG). NLG tasks are often based on the
encoder-decoder framework, where the pretrained encoders can only benefit part
of it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual
encoder-decoder model that regards the decoder as the task layer of
off-the-shelf pretrained encoders. Specifically, we augment the pretrained
multilingual encoder with a decoder and pre-train it in a self-supervised way.
To take advantage of both the large-scale monolingual data and bilingual data,
we adopt the span corruption and translation span corruption as the
pre-training tasks. Experiments show that DeltaLM outperforms various strong
baselines on both natural language generation and translation tasks, including
machine translation, abstractive text summarization, data-to-text, and question
generation. The code and pretrained models are available at
\url{https://aka.ms/deltalm}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1"&gt;Alexandre Muzio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1"&gt;Saksham Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1"&gt;Hany Hassan Awadalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CUSTOM: Aspect-Oriented Product Summarization for E-Commerce. (arXiv:2108.08010v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08010</id>
        <link href="http://arxiv.org/abs/2108.08010"/>
        <updated>2021-08-19T01:35:00.336Z</updated>
        <summary type="html"><![CDATA[Product summarization aims to automatically generate product descriptions,
which is of great commercial potential. Considering the customer preferences on
different product aspects, it would benefit from generating aspect-oriented
customized summaries. However, conventional systems typically focus on
providing general product summaries, which may miss the opportunity to match
products with customer interests. To address the problem, we propose CUSTOM,
aspect-oriented product summarization for e-commerce, which generates diverse
and controllable summaries towards different product aspects. To support the
study of CUSTOM and further this line of research, we construct two Chinese
datasets, i.e., SMARTPHONE and COMPUTER, including 76,279 / 49,280 short
summaries for 12,118 / 11,497 real-world commercial products, respectively.
Furthermore, we introduce EXT, an extraction-enhanced generation framework for
CUSTOM, where two famous sequence-to-sequence models are implemented in this
paper. We conduct extensive experiments on the two proposed datasets for CUSTOM
and show results of two famous baseline models and EXT, which indicates that
EXT can generate diverse, high-quality, and consistent summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiahui Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Junwei Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Youzheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaodong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10772</id>
        <link href="http://arxiv.org/abs/2102.10772"/>
        <updated>2021-08-19T01:35:00.258Z</updated>
        <summary type="html"><![CDATA[We propose UniT, a Unified Transformer model to simultaneously learn the most
prominent tasks across different domains, ranging from object detection to
natural language understanding and multimodal reasoning. Based on the
transformer encoder-decoder architecture, our UniT model encodes each input
modality with an encoder and makes predictions on each task with a shared
decoder over the encoded input representations, followed by task-specific
output heads. The entire model is jointly trained end-to-end with losses from
each task. Compared to previous efforts on multi-task learning with
transformers, we share the same model parameters across all tasks instead of
separately fine-tuning task-specific models and handle a much higher variety of
tasks across different domains. In our experiments, we learn 7 tasks jointly
over 8 datasets, achieving strong performance on each task with significantly
fewer parameters. Our code is available in MMF at https://mmf.sh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ronghang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images. (arXiv:2108.08157v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08157</id>
        <link href="http://arxiv.org/abs/2108.08157"/>
        <updated>2021-08-19T01:35:00.250Z</updated>
        <summary type="html"><![CDATA[Recently, FCNs have attracted widespread attention in the CD field. In
pursuit of better CD performance, it has become a tendency to design deeper and
more complicated FCNs, which inevitably brings about huge numbers of parameters
and an unbearable computational burden. With the goal of designing a quite deep
architecture to obtain more precise CD results while simultaneously decreasing
parameter numbers to improve efficiency, in this work, we present a very deep
and efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the
numerous parameters associated with deep architecture, an efficient convolution
consisting of depth-wise convolution and group convolution with a channel
shuffle mechanism is introduced to replace standard convolutional layers. In
terms of the specific network architecture, EffCDNet does not use mainstream
UNet-like architecture, but rather adopts the architecture with a very deep
encoder and a lightweight decoder. In the very deep encoder, two very deep
siamese streams stacked by efficient convolution first extract two highly
representative and informative feature maps from input image-pairs.
Subsequently, an efficient ASPP module is designed to capture multi-scale
change information. In the lightweight decoder, a recurrent criss-cross
self-attention (RCCA) module is applied to efficiently utilize non-local
similar feature representations to enhance discriminability for each pixel,
thus effectively separating the changed and unchanged regions. Moreover, to
tackle the optimization problem in confused pixels, two novel loss functions
based on information entropy are presented. On two challenging CD datasets, our
approach outperforms other SOTA FCN-based methods, with only benchmark-level
parameter numbers and quite low computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongruixuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08103</id>
        <link href="http://arxiv.org/abs/2108.08103"/>
        <updated>2021-08-19T01:35:00.243Z</updated>
        <summary type="html"><![CDATA[The open-access dissemination of pretrained language models through online
repositories has led to a democratization of state-of-the-art natural language
processing (NLP) research. This also allows people outside of NLP to use such
models and adapt them to specific use-cases. However, a certain amount of
technical proficiency is still required which is an entry barrier for users who
want to apply these models to a certain task but lack the necessary knowledge
or resources. In this work, we aim to overcome this gap by providing a tool
which allows researchers to leverage pretrained models without writing a single
line of code. Built upon the parameter-efficient adapter modules for transfer
learning, our AdapterHub Playground provides an intuitive interface, allowing
the usage of adapters for prediction, training and analysis of textual data for
a variety of NLP tasks. We present the tool's architecture and demonstrate its
advantages with prototypical use-cases, where we show that predictive
performance can easily be increased in a few-shot learning scenario. Finally,
we evaluate its usability in a user study. We provide the code and a live
interface at https://adapter-hub.github.io/playground.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1"&gt;Tilman Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohlender_B/0/1/0/all/0/1"&gt;Bela Bohlender&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1"&gt;Christina Viehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hane_V/0/1/0/all/0/1"&gt;Vincent Hane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adamson_Y/0/1/0/all/0/1"&gt;Yanik Adamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khuri_J/0/1/0/all/0/1"&gt;Jaber Khuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossmann_J/0/1/0/all/0/1"&gt;Jonas Brossmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1"&gt;Jonas Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better. (arXiv:2108.07969v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.07969</id>
        <link href="http://arxiv.org/abs/2108.07969"/>
        <updated>2021-08-19T01:35:00.225Z</updated>
        <summary type="html"><![CDATA[Adversarial training is one effective approach for training robust deep
neural networks against adversarial attacks. While being able to bring reliable
robustness, adversarial training (AT) methods in general favor high capacity
models, i.e., the larger the model the better the robustness. This tends to
limit their effectiveness on small models, which are more preferable in
scenarios where storage or computing resources are very limited (e.g., mobile
devices). In this paper, we leverage the concept of knowledge distillation to
improve the robustness of small models by distilling from adversarially trained
large models. We first revisit several state-of-the-art AT methods from a
distillation perspective and identify one common technique that can lead to
improved robustness: the use of robust soft labels -- predictions of a robust
model. Following this observation, we propose a novel adversarial robustness
distillation method called Robust Soft Label Adversarial Distillation (RSLAD)
to train robust small student models. RSLAD fully exploits the robust soft
labels produced by a robust (adversarially-trained) large teacher model to
guide the student's learning on both natural and adversarial examples in all
loss terms. We empirically demonstrate the effectiveness of our RSLAD approach
over existing adversarial training and distillation methods in improving the
robustness of small models against state-of-the-art attacks including the
AutoAttack. We also provide a set of understandings on our RSLAD and the
importance of robust soft labels for adversarial robustness distillation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1"&gt;Bojia Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shihao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xingjun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu-Gang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03761</id>
        <link href="http://arxiv.org/abs/2105.03761"/>
        <updated>2021-08-19T01:35:00.217Z</updated>
        <summary type="html"><![CDATA[Recently, there has been an increasing number of efforts to introduce models
capable of generating natural language explanations (NLEs) for their
predictions on vision-language (VL) tasks. Such models are appealing, because
they can provide human-friendly and comprehensive explanations. However, there
is a lack of comparison between existing methods, which is due to a lack of
re-usable evaluation frameworks and a scarcity of datasets. In this work, we
introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable
vision-language tasks that establishes a unified evaluation framework and
provides the first comprehensive comparison of existing approaches that
generate NLEs for VL tasks. It spans four models and three datasets and both
automatic metrics and human evaluation are used to assess model-generated
explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs
(over 430k instances). We also propose a new model that combines UNITER, which
learns joint embeddings of images and text, and GPT-2, a pre-trained language
model that is well-suited for text generation. It surpasses the previous state
of the art by a large margin across all datasets. Code and data are available
here: https://github.com/maximek3/e-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1"&gt;Maxime Kayser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1"&gt;Leonard Salewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1"&gt;Cornelius Emde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1"&gt;Virginie Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08102</id>
        <link href="http://arxiv.org/abs/2108.08102"/>
        <updated>2021-08-19T01:35:00.209Z</updated>
        <summary type="html"><![CDATA[Understanding speaker's feelings and producing appropriate responses with
emotion connection is a key communicative skill for empathetic dialogue
systems. In this paper, we propose a simple technique called Affective Decoding
for empathetic response generation. Our method can effectively incorporate
emotion signals during each decoding step, and can additionally be augmented
with an auxiliary dual emotion encoder, which learns separate embeddings for
the speaker and listener given the emotion base of the dialogue. Extensive
empirical studies show that our models are perceived to be more empathetic by
human evaluations, in comparison to several strong mainstream methods for
empathetic responding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chengkun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guanyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chenghua Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruizhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhigang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Do Your Biomedical Named Entity Models Generalize to Novel Entities?. (arXiv:2101.00160v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00160</id>
        <link href="http://arxiv.org/abs/2101.00160"/>
        <updated>2021-08-19T01:35:00.202Z</updated>
        <summary type="html"><![CDATA[The number of biomedical literature on new biomedical concepts is rapidly
increasing, which necessitates a reliable biomedical named entity recognition
(BioNER) model for identifying new and unseen entity mentions. However, it is
questionable whether existing BioNER models can effectively handle them. In
this work, we systematically analyze the three types of recognition abilities
of BioNER models: memorization, synonym generalization, and concept
generalization. We find that although BioNER models achieve state-of-the-art
performance on BioNER benchmarks based on overall performance, they have
limitations in identifying synonyms and new biomedical concepts such as
COVID-19. From this observation, we conclude that existing BioNER models are
overestimated in terms of their generalization abilities. Also, we identify
several difficulties in recognizing unseen mentions in BioNER and make the
following conclusions: (1) BioNER models tend to exploit dataset biases, which
hinders the models' abilities to generalize, and (2) several biomedical names
have novel morphological patterns with little name regularity such as COVID-19,
and models fail to recognize them. We apply a current statistics-based
debiasing method to our problem as a simple remedy and show the improvement in
generalization to unseen mentions. We hope that our analyses and findings would
be able to facilitate further research into the generalization capabilities of
NER models in a domain where their reliability is of utmost importance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunjae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jaewoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability. (arXiv:2108.08136v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08136</id>
        <link href="http://arxiv.org/abs/2108.08136"/>
        <updated>2021-08-19T01:35:00.194Z</updated>
        <summary type="html"><![CDATA[This work employs a pre-trained, multi-view Convolutional Neural Network
(CNN) with a spatial attention block to optimise knee injury detection. An
open-source Magnetic Resonance Imaging (MRI) data set with image-level labels
was leveraged for this analysis. As MRI data is acquired from three planes, we
compare our technique using data from a single-plane and multiple planes
(multi-plane). For multi-plane, we investigate various methods of fusing the
planes in the network. This analysis resulted in the novel 'MPFuseNet' network
and state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior
Cruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977
and 0.957 respectively. We then developed an objective metric, Penalised
Localisation Accuracy (PLA), to validate the model's localisation ability. This
metric compares binary masks generated from Grad-Cam output and the
radiologist's annotations on a sample of MRIs. We also extracted explainability
features in a model-agnostic approach that were then verified as clinically
relevant by the radiologist.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1"&gt;Niamh Belton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welaratne_I/0/1/0/all/0/1"&gt;Ivan Welaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahlan_A/0/1/0/all/0/1"&gt;Adil Dahlan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hearne_R/0/1/0/all/0/1"&gt;Ronan T Hearne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1"&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1"&gt;Aonghus Lawlor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1"&gt;Kathleen M. Curran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Identification of Covariate Shift in Image Data. (arXiv:2108.08000v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08000</id>
        <link href="http://arxiv.org/abs/2108.08000"/>
        <updated>2021-08-19T01:35:00.187Z</updated>
        <summary type="html"><![CDATA[Identifying covariate shift is crucial for making machine learning systems
robust in the real world and for detecting training data biases that are not
reflected in test data. However, detecting covariate shift is challenging,
especially when the data consists of high-dimensional images, and when multiple
types of localized covariate shift affect different subspaces of the data.
Although automated techniques can be used to detect the existence of covariate
shift, our goal is to help human users characterize the extent of covariate
shift in large image datasets with interfaces that seamlessly integrate
information obtained from the detection algorithms. In this paper, we design
and evaluate a new visual interface that facilitates the comparison of the
local distributions of training and test data. We conduct a quantitative user
study on multi-attribute facial data to compare two different learned
low-dimensional latent representations (pretrained ImageNet CNN vs. density
ratio) and two user analytic workflows (nearest-neighbor vs.
cluster-to-cluster). Our results indicate that the latent representation of our
density ratio model, combined with a nearest-neighbor comparison, is the most
effective at helping humans identify covariate shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olson_M/0/1/0/all/0/1"&gt;Matthew L. Olson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuy-Vy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_G/0/1/0/all/0/1"&gt;Gaurav Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratzlaff_N/0/1/0/all/0/1"&gt;Neale Ratzlaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1"&gt;Weng-Keen Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1"&gt;Minsuk Kahng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07971</id>
        <link href="http://arxiv.org/abs/2108.07971"/>
        <updated>2021-08-19T01:35:00.180Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel problem formulation for de-identification of
unstructured clinical text. We formulate the de-identification problem as a
sequence to sequence learning problem instead of a token classification
problem. Our approach is inspired by the recent state-of -the-art performance
of sequence to sequence learning models for named entity recognition. Early
experimentation of our proposed approach achieved 98.91% recall rate on i2b2
dataset. This performance is comparable to current state-of-the-art models for
unstructured clinical text de-identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1"&gt;Md Monowar Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1"&gt;Noman Mohammed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoqian Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.05067</id>
        <link href="http://arxiv.org/abs/2108.05067"/>
        <updated>2021-08-19T01:35:00.171Z</updated>
        <summary type="html"><![CDATA[Medical imaging technologies, including computed tomography (CT) or chest
X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.
Since manual report writing is usually too time-consuming, a more intelligent
auxiliary medical system that could generate medical reports automatically and
immediately is urgently needed. In this article, we propose to use the medical
visual language BERT (Medical-VLBERT) model to identify the abnormality on the
COVID-19 scans and generate the medical report automatically based on the
detected lesion regions. To produce more accurate medical reports and minimize
the visual-and-linguistic differences, this model adopts an alternate learning
strategy with two procedures that are knowledge pretraining and transferring.
To be more precise, the knowledge pretraining procedure is to memorize the
knowledge from medical texts, while the transferring procedure is to utilize
the acquired knowledge for professional medical sentences generations through
observations of medical images. In practice, for automatic medical report
generation on the COVID-19 cases, we constructed a dataset of 368 medical
findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital
of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun
Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of
the COVID-19 training samples, our model was first trained on the large-scale
Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for
further fine-tuning. The experimental results showed that Medical-VLBERT
achieved state-of-the-art performances on terminology prediction and report
generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The
Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yinghong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fuyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuixing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine Reading Comprehension. (arXiv:2108.07994v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07994</id>
        <link href="http://arxiv.org/abs/2108.07994"/>
        <updated>2021-08-19T01:35:00.146Z</updated>
        <summary type="html"><![CDATA[Reasoning machine reading comprehension (R-MRC) aims to answer complex
questions that require discrete reasoning based on text. To support discrete
reasoning, evidence, typically the concise textual fragments that describe
question-related facts, including topic entities and attribute values, are
crucial clues from question to answer. However, previous end-to-end methods
that achieve state-of-the-art performance rarely solve the problem by paying
enough emphasis on the modeling of evidence, missing the opportunity to further
improve the model's reasoning ability for R-MRC. To alleviate the above issue,
in this paper, we propose an evidence-emphasized discrete reasoning approach
(EviDR), in which sentence and clause level evidence is first detected based on
distant supervision, and then used to drive a reasoning module implemented with
a relational heterogeneous graph convolutional network to derive answers.
Extensive experiments are conducted on DROP (discrete reasoning over
paragraphs) dataset, and the results demonstrate the effectiveness of our
proposed approach. In addition, qualitative analysis verifies the capability of
the proposed evidence-emphasized discrete reasoning for R-MRC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yongwei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Junwei Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haipeng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiahui Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Youzheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaodong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiejun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation. (arXiv:2108.07998v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07998</id>
        <link href="http://arxiv.org/abs/2108.07998"/>
        <updated>2021-08-19T01:35:00.095Z</updated>
        <summary type="html"><![CDATA[Existing data-driven methods can well handle short text generation. However,
when applied to the long-text generation scenarios such as story generation or
advertising text generation in the commercial scenario, these methods may
generate illogical and uncontrollable texts. To address these aforementioned
issues, we propose a graph-based grouping planner(GGP) following the idea of
first-plan-then-generate. Specifically, given a collection of key phrases, GGP
firstly encodes these phrases into an instance-level sequential representation
and a corpus-level graph-based representation separately. With these two
synergic representations, we then regroup these phrases into a fine-grained
plan, based on which we generate the final long text. We conduct our
experiments on three long text generation datasets and the experimental results
reveal that GGP significantly outperforms baselines, which proves that GGP can
control the long text generation by knowing how to say and in what order.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shaobo Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhongzhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haiqing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative Uniform Stability of the Iterative Proportional Fitting Procedure. (arXiv:2108.08129v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.08129</id>
        <link href="http://arxiv.org/abs/2108.08129"/>
        <updated>2021-08-19T01:35:00.075Z</updated>
        <summary type="html"><![CDATA[We establish the uniform in time stability, w.r.t. the marginals, of the
Iterative Proportional Fitting Procedure, also known as Sinkhorn algorithm,
used to solve entropy-regularised Optimal Transport problems. Our result is
quantitative and stated in terms of the 1-Wasserstein metric. As a corollary we
establish a quantitative stability result for Schr\"odinger bridges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1"&gt;Valentin De Bortoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Cluster Embedding. (arXiv:2108.08003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08003</id>
        <link href="http://arxiv.org/abs/2108.08003"/>
        <updated>2021-08-19T01:35:00.028Z</updated>
        <summary type="html"><![CDATA[Neighbor Embedding (NE) that aims to preserve pairwise similarities between
data items has been shown to yield an effective principle for data
visualization. However, even the currently best NE methods such as Stochastic
Neighbor Embedding (SNE) may leave large-scale patterns such as clusters hidden
despite of strong signals being present in the data. To address this, we
propose a new cluster visualization method based on Neighbor Embedding. We
first present a family of Neighbor Embedding methods which generalizes SNE by
using non-normalized Kullback-Leibler divergence with a scale parameter. In
this family, much better cluster visualizations often appear with a parameter
value different from the one corresponding to SNE. We also develop an efficient
software which employs asynchronous stochastic block coordinate descent to
optimize the new family of objective functions. The experimental results
demonstrate that our method consistently and substantially improves
visualization of data clusters compared with the state-of-the-art NE
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhirong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedov_D/0/1/0/all/0/1"&gt;Denis Sedov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corander_J/0/1/0/all/0/1"&gt;Jukka Corander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with ReLU activation. (arXiv:2108.08106v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.08106</id>
        <link href="http://arxiv.org/abs/2108.08106"/>
        <updated>2021-08-19T01:34:59.912Z</updated>
        <summary type="html"><![CDATA[The training of artificial neural networks (ANNs) with rectified linear unit
(ReLU) activation via gradient descent (GD) type optimization schemes is
nowadays a common industrially relevant procedure. Till this day in the
scientific literature there is in general no mathematical convergence analysis
which explains the numerical success of GD type optimization schemes in the
training of ANNs with ReLU activation. GD type optimization schemes can be
regarded as temporal discretization methods for the gradient flow (GF)
differential equations associated to the considered optimization problem and,
in view of this, it seems to be a natural direction of research to first aim to
develop a mathematical convergence theory for time-continuous GF differential
equations and, thereafter, to aim to extend such a time-continuous convergence
theory to implementable time-discrete GD type optimization methods. In this
article we establish two basic results for GF differential equations in the
training of fully-connected feedforward ANNs with one hidden layer and ReLU
activation. In the first main result of this article we establish in the
training of such ANNs under the assumption that the probability distribution of
the input data of the considered supervised learning problem is absolutely
continuous with a bounded density function that every GF differential equation
admits for every initial value a solution which is also unique among a suitable
class of solutions. In the second main result of this article we prove in the
training of such ANNs under the assumption that the target function and the
density function of the probability distribution of the input data are
piecewise polynomial that every non-divergent GF trajectory converges with an
appropriate rate of convergence to a critical point and that the risk of the
non-divergent GF trajectory converges with rate 1 to the risk of the critical
point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eberle_S/0/1/0/all/0/1"&gt;Simon Eberle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riekert_A/0/1/0/all/0/1"&gt;Adrian Riekert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Georg S. Weiss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparative study of universal quantum computing models: towards a physical unification. (arXiv:2108.07909v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.07909</id>
        <link href="http://arxiv.org/abs/2108.07909"/>
        <updated>2021-08-19T01:34:59.845Z</updated>
        <summary type="html"><![CDATA[Quantum computing has been a fascinating research field in quantum physics.
Recent progresses motivate us to study in depth the universal quantum computing
models (UQCM), which lie at the foundation of quantum computing and have tight
connections with fundamental physics. Although being developed decades ago, a
physically concise principle or picture to formalize and understand UQCM is
still lacking. This is challenging given the diversity of still-emerging
models, but important to understand the difference between classical and
quantum computing. In this work, we carried out a primary attempt to unify UQCM
by classifying a few of them as two categories, hence making a table of models.
With such a table, some known models or schemes appear as hybridization or
combination of models, and more importantly, it leads to new schemes that have
not been explored yet. Our study of UQCM also leads to some insights into
quantum algorithms. This work reveals the importance and feasibility of
systematic study of computing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_D/0/1/0/all/0/1"&gt;D.-S. Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15176</id>
        <link href="http://arxiv.org/abs/2105.15176"/>
        <updated>2021-08-19T01:34:59.788Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence models provide a viable new approach to generative
summarization, allowing models that are no longer limited to simply selecting
and recombining sentences from the original text. However, these models have
three drawbacks: their grasp of the details of the original text is often
inaccurate, and the text generated by such models often has repetitions, while
it is difficult to handle words that are beyond the word list. In this paper,
we propose a new architecture that combines reinforcement learning and
adversarial generative networks to enhance the sequence-to-sequence attention
model. First, we use a hybrid pointer-generator network that copies words
directly from the source text, contributing to accurate reproduction of
information without sacrificing the ability of generators to generate new
words. Second, we use both intra-temporal and intra-decoder attention to
penalize summarized content and thus discourage repetition. We apply our model
to our own proposed COVID-19 paper title summarization task and achieve close
approximations to the current model on ROUEG, while bringing better
readability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tianyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunyun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08226</id>
        <link href="http://arxiv.org/abs/2108.08226"/>
        <updated>2021-08-19T01:34:59.714Z</updated>
        <summary type="html"><![CDATA[Coming up with effective ad text is a time consuming process, and
particularly challenging for small businesses with limited advertising
experience. When an inexperienced advertiser onboards with a poorly written ad
text, the ad platform has the opportunity to detect low performing ad text, and
provide improvement suggestions. To realize this opportunity, we propose an ad
text strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)
for an input ad text, (ii) fetches similar existing ads to create a
neighborhood around the input ad, (iii) and compares the predicted CTRs in the
neighborhood to declare whether the input ad is strong or weak. In addition, as
suggestions for ad text improvement, TSI shows anonymized versions of superior
ads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT
based text-to-CTR model trained on impressions and clicks associated with an ad
text. For (ii), we propose a sentence-BERT based semantic-ad-similarity model
trained using weak labels from ad campaign setup data. Offline experiments
demonstrate that our BERT based text-to-CTR model achieves a significant lift
in CTR prediction AUC for cold start (new) advertisers compared to bag-of-words
based baselines. In addition, our semantic-textual-similarity model for similar
ads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same
product category); this is significantly higher compared to unsupervised
TF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising
online results from advertisers in the Yahoo (Verizon Media) ad platform where
a variant of TSI was implemented with sub-second end-to-end latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Changwei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1"&gt;Manisha Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1"&gt;Kevin Yen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yifan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1"&gt;Maxim Sviridenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Natural Language Processing for LinkedIn Search Systems. (arXiv:2108.08252v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08252</id>
        <link href="http://arxiv.org/abs/2108.08252"/>
        <updated>2021-08-19T01:34:59.696Z</updated>
        <summary type="html"><![CDATA[Many search systems work with large amounts of natural language data, e.g.,
search queries, user profiles and documents, where deep learning based natural
language processing techniques (deep NLP) can be of great help. In this paper,
we introduce a comprehensive study of applying deep NLP techniques to five
representative tasks in search engines. Through the model design and
experiments of the five tasks, readers can find answers to three important
questions: (1) When is deep NLP helpful/not helpful in search systems? (2) How
to address latency challenges? (3) How to ensure model robustness? This work
builds on existing efforts of LinkedIn search, and is tested at scale on a
commercial search engine. We believe our experiences can provide useful
insights for the industry and research communities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Weiwei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaowei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sida Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazi_M/0/1/0/all/0/1"&gt;Michaeel Kazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhoutong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1"&gt;Huiji Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jun Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Bo Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHAQ: Single Headed Attention with Quasi-Recurrence. (arXiv:2108.08207v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08207</id>
        <link href="http://arxiv.org/abs/2108.08207"/>
        <updated>2021-08-19T01:34:59.675Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing research has recently been dominated by large
scale transformer models. Although they achieve state of the art on many
important language tasks, transformers often require expensive compute
resources, and days spanning to weeks to train. This is feasible for
researchers at big tech companies and leading research universities, but not
for scrappy start-up founders, students, and independent researchers. Stephen
Merity's SHA-RNN, a compact, hybrid attention-RNN model, is designed for
consumer-grade modeling as it requires significantly fewer parameters and less
training time to reach near state of the art results. We analyze Merity's model
here through an exploratory model analysis over several units of the
architecture considering both training time and overall quality in our
assessment. Ultimately, we combine these findings into a new architecture which
we call SHAQ: Single Headed Attention Quasi-recurrent Neural Network. With our
new architecture we achieved similar accuracy results as the SHA-RNN while
accomplishing a 4x speed boost in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bharwani_N/0/1/0/all/0/1"&gt;Nashwin Bharwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kushner_W/0/1/0/all/0/1"&gt;Warren Kushner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dandona_S/0/1/0/all/0/1"&gt;Sangeet Dandona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreiber_B/0/1/0/all/0/1"&gt;Ben Schreiber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM. (arXiv:2108.08095v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.08095</id>
        <link href="http://arxiv.org/abs/2108.08095"/>
        <updated>2021-08-19T01:34:59.662Z</updated>
        <summary type="html"><![CDATA[Medical Imaging is one of the growing fields in the world of computer vision.
In this study, we aim to address the Diabetic Retinopathy (DR) problem as one
of the open challenges in medical imaging. In this research, we propose a new
lesion detection architecture, comprising of two sub-modules, which is an
optimal solution to detect and find not only the type of lesions caused by DR,
their corresponding bounding boxes, and their masks; but also the severity
level of the overall case. Aside from traditional accuracy, we also use two
popular evaluation criteria to evaluate the outputs of our models, which are
intersection over union (IOU) and mean average precision (mAP). We hypothesize
that this new solution enables specialists to detect lesions with high
confidence and estimate the severity of the damage with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1"&gt;Farzan Shenavarmasouleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohammadi_F/0/1/0/all/0/1"&gt;Farid Ghareh Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amini_M/0/1/0/all/0/1"&gt;M. Hadi Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Taha_T/0/1/0/all/0/1"&gt;Thiab Taha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rasheed_K/0/1/0/all/0/1"&gt;Khaled Rasheed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arabnia_H/0/1/0/all/0/1"&gt;Hamid R. Arabnia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07886</id>
        <link href="http://arxiv.org/abs/2108.07886"/>
        <updated>2021-08-19T01:34:59.642Z</updated>
        <summary type="html"><![CDATA[Generating context-aware language that embodies diverse emotions is an
important step towards building empathetic NLP systems. In this paper, we
propose a formulation of modulated layer normalization -- a technique inspired
by computer vision -- that allows us to use large-scale language models for
emotional response generation. In automatic and human evaluation on the
MojiTalk dataset, our proposed modulated layer normalization method outperforms
prior baseline methods while maintaining diversity, fluency, and coherence. Our
method also obtains competitive performance even when using only 10% of the
available training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chenyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MeDiaQA: A Question Answering Dataset on Medical Dialogues. (arXiv:2108.08074v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08074</id>
        <link href="http://arxiv.org/abs/2108.08074"/>
        <updated>2021-08-19T01:34:59.635Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce MeDiaQA, a novel question answering(QA) dataset,
which constructed on real online Medical Dialogues. It contains 22k
multiple-choice questions annotated by human for over 11k dialogues with 120k
utterances between patients and doctors, covering 150 specialties of diseases,
which are collected from haodf.com and dxy.com. MeDiaQA is the first QA dataset
where reasoning over medical dialogues, especially their quantitative contents.
The dataset has the potential to test the computing, reasoning and
understanding ability of models across multi-turn dialogues, which is
challenging compared with the existing datasets. To address the challenges, we
design MeDia-BERT, and it achieves 64.3% accuracy, while human performance of
93% accuracy, which indicates that there still remains a large room for
improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suri_H/0/1/0/all/0/1"&gt;Huqun Suri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_W/0/1/0/all/0/1"&gt;Wenhua Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Chunsheng Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Multiple Intent Detection and Slot Filling via Self-distillation. (arXiv:2108.08042v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08042</id>
        <link href="http://arxiv.org/abs/2108.08042"/>
        <updated>2021-08-19T01:34:59.627Z</updated>
        <summary type="html"><![CDATA[Intent detection and slot filling are two main tasks in natural language
understanding (NLU) for identifying users' needs from their utterances. These
two tasks are highly related and often trained jointly. However, most previous
works assume that each utterance only corresponds to one intent, ignoring the
fact that a user utterance in many cases could include multiple intents. In
this paper, we propose a novel Self-Distillation Joint NLU model (SDJN) for
multi-intent NLU. First, we formulate multiple intent detection as a weakly
supervised problem and approach with multiple instance learning (MIL). Then, we
design an auxiliary loop via self-distillation with three orderly arranged
decoders: Initial Slot Decoder, MIL Intent Decoder, and Final Slot Decoder. The
output of each decoder will serve as auxiliary information for the next
decoder. With the auxiliary knowledge provided by the MIL Intent Decoder, we
set Final Slot Decoder as the teacher model that imparts knowledge back to
Initial Slot Decoder to complete the loop. The auxiliary loop enables intents
and slots to guide mutually in-depth and further boost the overall NLU
performance. Experimental results on two public multi-intent datasets indicate
that our model achieves strong performance compared to others.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lisong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Peilin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RTE: A Tool for Annotating Relation Triplets from Text. (arXiv:2108.08184v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08184</id>
        <link href="http://arxiv.org/abs/2108.08184"/>
        <updated>2021-08-19T01:34:59.618Z</updated>
        <summary type="html"><![CDATA[In this work, we present a Web-based annotation tool `Relation Triplets
Extractor' \footnote{https://abera87.github.io/annotate/} (RTE) for annotating
relation triplets from the text. Relation extraction is an important task for
extracting structured information about real-world entities from the
unstructured text available on the Web. In relation extraction, we focus on
binary relation that refers to relations between two entities. Recently, many
supervised models are proposed to solve this task, but they mostly use noisy
training data obtained using the distant supervision method. In many cases,
evaluation of the models is also done based on a noisy test dataset. The lack
of annotated clean dataset is a key challenge in this area of research. In this
work, we built a web-based tool where researchers can annotate datasets for
relation extraction on their own very easily. We use a server-less architecture
for this tool, and the entire annotation operation is processed using
client-side code. Thus it does not suffer from any network latency, and the
privacy of the user's data is also maintained. We hope that this tool will be
beneficial for the researchers to advance the field of relation extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1"&gt;Ankan Mullick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Animesh Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1"&gt;Tapas Nayak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity. (arXiv:2108.07992v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.07992</id>
        <link href="http://arxiv.org/abs/2108.07992"/>
        <updated>2021-08-19T01:34:59.592Z</updated>
        <summary type="html"><![CDATA[We study the multi-marginal partial optimal transport (POT) problem between
$m$ discrete (unbalanced) measures with at most $n$ supports. We first prove
that we can obtain two equivalence forms of the multimarginal POT problem in
terms of the multimarginal optimal transport problem via novel extensions of
cost tensor. The first equivalence form is derived under the assumptions that
the total masses of each measure are sufficiently close while the second
equivalence form does not require any conditions on these masses but at the
price of more sophisticated extended cost tensor. Our proof techniques for
obtaining these equivalence forms rely on novel procedures of moving mass in
graph theory to push transportation plan into appropriate regions. Finally,
based on the equivalence forms, we develop optimization algorithm, named
ApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the
entropic regularized multimarginal optimal transport. We demonstrate that the
ApproxMPOT algorithm can approximate the optimal value of multimarginal POT
problem with a computational complexity upper bound of the order
$\tilde{\mathcal{O}}(m^3(n+1)^{m}/ \varepsilon^2)$ where $\varepsilon > 0$
stands for the desired tolerance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Le_K/0/1/0/all/0/1"&gt;Khang Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Huy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pham_T/0/1/0/all/0/1"&gt;Tung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08217</id>
        <link href="http://arxiv.org/abs/2108.08217"/>
        <updated>2021-08-19T01:34:59.554Z</updated>
        <summary type="html"><![CDATA[With the rise and development of deep learning over the past decade, there
has been a steady momentum of innovation and breakthroughs that convincingly
push the state-of-the-art of cross-modal analytics between vision and language
in multimedia field. Nevertheless, there has not been an open-source codebase
in support of training and deploying numerous neural network models for
cross-modal analytics in a unified and modular fashion. In this work, we
propose X-modaler -- a versatile and high-performance codebase that
encapsulates the state-of-the-art cross-modal analytics into several
general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,
decoder, and decode strategy). Each stage is empowered with the functionality
that covers a series of modules widely adopted in state-of-the-arts and allows
seamless switching in between. This way naturally enables a flexible
implementation of state-of-the-art algorithms for image captioning, video
captioning, and vision-language pre-training, aiming to facilitate the rapid
development of research community. Meanwhile, since the effective modular
designs in several stages (e.g., cross-modal interaction) are shared across
different vision-language tasks, X-modaler can be simply extended to power
startup prototypes for other tasks in cross-modal analytics, including visual
question answering, visual commonsense reasoning, and cross-modal retrieval.
X-modaler is an Apache-licensed codebase, and its source codes, sample projects
and pre-trained models are available on-line:
https://github.com/YehLi/xmodaler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yehao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07935</id>
        <link href="http://arxiv.org/abs/2108.07935"/>
        <updated>2021-08-19T01:34:59.499Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the problem of developing personalized chatbots. A
personalized chatbot is designed as a digital chatting assistant for a user.
The key characteristic of a personalized chatbot is that it should have a
consistent personality with the corresponding user. It can talk the same way as
the user when it is delegated to respond to others' messages. We present a
retrieval-based personalized chatbot model, namely IMPChat, to learn an
implicit user profile from the user's dialogue history. We argue that the
implicit user profile is superior to the explicit user profile regarding
accessibility and flexibility. IMPChat aims to learn an implicit user profile
through modeling user's personalized language style and personalized
preferences separately. To learn a user's personalized language style, we
elaborately build language models from shallow to deep using the user's
historical responses; To model a user's personalized preferences, we explore
the conditional relations underneath each post-response pair of the user. The
personalized preferences are dynamic and context-aware: we assign higher
weights to those historical pairs that are topically related to the current
query when aggregating the personalized preferences. We match each response
candidate with the personalized language style and personalized preference,
respectively, and fuse the two matching signals to determine the final ranking
score. Comprehensive experiments on two large datasets show that our method
outperforms all baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Hongjin Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yutao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yueyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03788</id>
        <link href="http://arxiv.org/abs/2108.03788"/>
        <updated>2021-08-19T01:34:59.170Z</updated>
        <summary type="html"><![CDATA[This paper presents a three-tier modality alignment approach to learning
text-image joint embedding, coined as JEMA, for cross-modal retrieval of
cooking recipes and food images. The first tier improves recipe text embedding
by optimizing the LSTM networks with term extraction and ranking enhanced
sequence patterns, and optimizes the image embedding by combining the
ResNeXt-101 image encoder with the category embedding using wideResNet-50 with
word2vec. The second tier modality alignment optimizes the textual-visual joint
embedding loss function using a double batch-hard triplet loss with soft-margin
optimization. The third modality alignment incorporates two types of
cross-modality alignments as the auxiliary loss regularizations to further
reduce the alignment errors in the joint learning of the two modality-specific
embedding functions. The category-based cross-modal alignment aims to align the
image category with the recipe category as a loss regularization to the joint
embedding. The cross-modal discriminator-based alignment aims to add the
visual-textual embedding distribution alignment to further regularize the joint
embedding loss. Extensive experiments with the one-million recipes benchmark
dataset Recipe1M demonstrate that the proposed JEMA approach outperforms the
state-of-the-art cross-modal embedding methods for both image-to-recipe and
recipe-to-image retrievals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report. (arXiv:2108.07865v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07865</id>
        <link href="http://arxiv.org/abs/2108.07865"/>
        <updated>2021-08-19T01:34:59.161Z</updated>
        <summary type="html"><![CDATA[This workshop is the fourth issue of a series of workshops on automatic
extraction of socio-political events from news, organized by the Emerging
Market Welfare Project, with the support of the Joint Research Centre of the
European Commission and with contributions from many other prominent scholars
in this field. The purpose of this series of workshops is to foster research
and development of reliable, valid, robust, and practical solutions for
automatically detecting descriptions of socio-political events, such as
protests, riots, wars and armed conflicts, in text streams. This year workshop
contributors make use of the state-of-the-art NLP technologies, such as Deep
Learning, Word Embeddings and Transformers and cover a wide range of topics
from text classification to news bias detection. Around 40 teams have
registered and 15 teams contributed to three tasks that are i) multilingual
protest news detection, ii) fine-grained classification of socio-political
events, and iii) discovering Black Lives Matter protest events. The workshop
also highlights two keynote and four invited talks about various aspects of
creating event data sets and multi- and cross-lingual machine learning in few-
and zero-shot settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hurriyetoglu_A/0/1/0/all/0/1"&gt;Ali H&amp;#xfc;rriyeto&amp;#x11f;lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanev_H/0/1/0/all/0/1"&gt;Hristo Tanev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zavarella_V/0/1/0/all/0/1"&gt;Vanni Zavarella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piskorski_J/0/1/0/all/0/1"&gt;Jakub Piskorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1"&gt;Reyyan Yeniterzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoruk_E/0/1/0/all/0/1"&gt;Erdem Y&amp;#xf6;r&amp;#xfc;k&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07908</id>
        <link href="http://arxiv.org/abs/2108.07908"/>
        <updated>2021-08-19T01:34:59.148Z</updated>
        <summary type="html"><![CDATA[This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent
Component Analysis ('FastICA') method ('m-ar-K-FastICA') for feature
extraction. The kernel trick has enabled dimensionality reduction techniques to
capture a higher extent of non-linearity in the data; however, reproducible,
open-source kernels to aid with feature extraction are still limited and may
not be reliable when projecting features from entropic data. The m-ar-K
function, freely available in Python and compatible with its open-source
library 'scikit-learn', is hereby coupled with FastICA to achieve more reliable
feature extraction in presence of a high extent of randomness in the data,
reducing the need for pre-whitening. Different classification tasks were
considered, as related to five (N = 5) open access datasets of various degrees
of information entropy, available from scikit-learn and the University
California Irvine (UCI) Machine Learning repository. Experimental results
demonstrate improvements in the classification performance brought by the
proposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction
approach is compared to the 'FastICA' gold standard method, supporting its
higher reliability and computational efficiency, regardless of the underlying
uncertainty in the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1"&gt;Luca Parisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualizing Variation in Text Style Transfer Datasets. (arXiv:2108.07871v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07871</id>
        <link href="http://arxiv.org/abs/2108.07871"/>
        <updated>2021-08-19T01:34:59.124Z</updated>
        <summary type="html"><![CDATA[Text style transfer involves rewriting the content of a source sentence in a
target style. Despite there being a number of style tasks with available data,
there has been limited systematic discussion of how text style datasets relate
to each other. This understanding, however, is likely to have implications for
selecting multiple data sources for model training. While it is prudent to
consider inherent stylistic properties when determining these relationships, we
also must consider how a style is realized in a particular dataset. In this
paper, we conduct several empirical analyses of existing text style datasets.
Based on our results, we propose a categorization of stylistic and dataset
properties to consider when utilizing or comparing text style datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1"&gt;Stephanie Schoch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wanyu Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yangfeng Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.08226</id>
        <link href="http://arxiv.org/abs/2108.08226"/>
        <updated>2021-08-19T01:34:59.096Z</updated>
        <summary type="html"><![CDATA[Coming up with effective ad text is a time consuming process, and
particularly challenging for small businesses with limited advertising
experience. When an inexperienced advertiser onboards with a poorly written ad
text, the ad platform has the opportunity to detect low performing ad text, and
provide improvement suggestions. To realize this opportunity, we propose an ad
text strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)
for an input ad text, (ii) fetches similar existing ads to create a
neighborhood around the input ad, (iii) and compares the predicted CTRs in the
neighborhood to declare whether the input ad is strong or weak. In addition, as
suggestions for ad text improvement, TSI shows anonymized versions of superior
ads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT
based text-to-CTR model trained on impressions and clicks associated with an ad
text. For (ii), we propose a sentence-BERT based semantic-ad-similarity model
trained using weak labels from ad campaign setup data. Offline experiments
demonstrate that our BERT based text-to-CTR model achieves a significant lift
in CTR prediction AUC for cold start (new) advertisers compared to bag-of-words
based baselines. In addition, our semantic-textual-similarity model for similar
ads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same
product category); this is significantly higher compared to unsupervised
TF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising
online results from advertisers in the Yahoo (Verizon Media) ad platform where
a variant of TSI was implemented with sub-second end-to-end latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Changwei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1"&gt;Manisha Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1"&gt;Kevin Yen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yifan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1"&gt;Maxim Sviridenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2108.07805</id>
        <link href="http://arxiv.org/abs/2108.07805"/>
        <updated>2021-08-19T01:34:59.072Z</updated>
        <summary type="html"><![CDATA[Programming microcontrollers involves low-level interfacing with hardware and
peripherals that are concurrent and reactive. Such programs are typically
written in a mixture of C and assembly using concurrent language extensions
(like $\texttt{FreeRTOS tasks}$ and $\texttt{semaphores}$), resulting in
unsafe, callback-driven, error-prone and difficult-to-maintain code.

We address this challenge by introducing $\texttt{SenseVM}$ - a
bytecode-interpreted virtual machine that provides a message-passing based
$\textit{higher-order concurrency}$ model, originally introduced by Reppy, for
microcontroller programming. This model treats synchronous operations as
first-class values (called $\texttt{Events}$) akin to the treatment of
first-class functions in functional languages. This primarily allows the
programmer to compose and tailor their own concurrency abstractions and,
additionally, abstracts away unsafe memory operations, common in shared-memory
concurrency models, thereby making microcontroller programs safer, composable
and easier-to-maintain.

Our VM is made portable via a low-level $\textit{bridge}$ interface, built
atop the embedded OS - Zephyr. The bridge is implemented by all drivers and
designed such that programming in response to a software message or a hardware
interrupt remains uniform and indistinguishable. In this paper we demonstrate
the features of our VM through an example, written in a Caml-like functional
language, running on the $\texttt{nRF52840}$ and $\texttt{STM32F4}$
microcontrollers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Abhiroop Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krook_R/0/1/0/all/0/1"&gt;Robert Krook&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svensson_B/0/1/0/all/0/1"&gt;Bo Joel Svensson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheeran_M/0/1/0/all/0/1"&gt;Mary Sheeran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Attitudinal Network Graphs through Frustration Cloud. (arXiv:2009.07776v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07776</id>
        <link href="http://arxiv.org/abs/2009.07776"/>
        <updated>2021-08-19T01:34:59.058Z</updated>
        <summary type="html"><![CDATA[Attitudinal Network Graphs are signed graphs where edges capture an expressed
opinion; two vertices connected by an edge can be agreeable (positive) or
antagonistic (negative). A signed graph is called balanced if each of its
cycles includes an even number of negative edges. Balance is often
characterized by the frustration index or by finding a single convergent
balanced state of network consensus. In this paper, we propose to expand the
measures of consensus from a single balanced state associated with the
frustration index to the set of nearest balanced states. We introduce the
frustration cloud as a set of all nearest balanced states and use a
graph-balancing algorithm to find all nearest balanced states in a
deterministic way. Computational concerns are addressed by measuring consensus
probabilistically, and we introduce new vertex and edge metrics to quantify
status, agreement, and influence. We also introduce a new global measure of
controversy for a given signed graph and show that vertex status is a zero-sum
game in the signed network. We propose an efficient scalable algorithm for
calculating frustration cloud-based measures in social network and survey data
of up to 80,000 vertices and half-a-million edges. We also demonstrate the
power of the proposed approach to provide discriminant features for community
discovery when compared to spectral clustering and to automatically identify
dominant vertices and anomalous decisions in the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rusnak_L/0/1/0/all/0/1"&gt;Lucas Rusnak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tesic_J/0/1/0/all/0/1"&gt;Jelena Te&amp;#x161;i&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Performance and Energy trade-offs in Online Data-Intensive Applications. (arXiv:2108.08199v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.08199</id>
        <link href="http://arxiv.org/abs/2108.08199"/>
        <updated>2021-08-19T01:34:59.025Z</updated>
        <summary type="html"><![CDATA[We consider energy minimization for data-intensive applications run on large
number of servers, for given performance guarantees. We consider a system,
where each incoming application is sent to a set of servers, and is considered
to be completed if a subset of them finish serving it. We consider a simple
case when each server core has two speed levels, where the higher speed can be
achieved by higher power for each core independently. The core selects one of
the two speeds probabilistically for each incoming application request. We
model arrival of application requests by a Poisson process, and random service
time at the server with independent exponential random variables. Our model and
analysis generalizes to today's state-of-the-art in CPU energy management where
each core can independently select a speed level from a set of supported speeds
and corresponding voltages. The performance metrics under consideration are the
mean number of applications in the system and the average energy expenditure.
We first provide a tight approximation to study this previously intractable
problem and derive closed form approximate expressions for the performance
metrics when service times are exponentially distributed. Next, we study the
trade-off between the approximate mean number of applications and energy
expenditure in terms of the switching probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Badita_A/0/1/0/all/0/1"&gt;Ajay Badita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jinan_R/0/1/0/all/0/1"&gt;Rooji Jinan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vamanan_B/0/1/0/all/0/1"&gt;Balajee Vamanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parag_P/0/1/0/all/0/1"&gt;Parimal Parag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.08217</id>
        <link href="http://arxiv.org/abs/2108.08217"/>
        <updated>2021-08-19T01:34:58.952Z</updated>
        <summary type="html"><![CDATA[With the rise and development of deep learning over the past decade, there
has been a steady momentum of innovation and breakthroughs that convincingly
push the state-of-the-art of cross-modal analytics between vision and language
in multimedia field. Nevertheless, there has not been an open-source codebase
in support of training and deploying numerous neural network models for
cross-modal analytics in a unified and modular fashion. In this work, we
propose X-modaler -- a versatile and high-performance codebase that
encapsulates the state-of-the-art cross-modal analytics into several
general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,
decoder, and decode strategy). Each stage is empowered with the functionality
that covers a series of modules widely adopted in state-of-the-arts and allows
seamless switching in between. This way naturally enables a flexible
implementation of state-of-the-art algorithms for image captioning, video
captioning, and vision-language pre-training, aiming to facilitate the rapid
development of research community. Meanwhile, since the effective modular
designs in several stages (e.g., cross-modal interaction) are shared across
different vision-language tasks, X-modaler can be simply extended to power
startup prototypes for other tasks in cross-modal analytics, including visual
question answering, visual commonsense reasoning, and cross-modal retrieval.
X-modaler is an Apache-licensed codebase, and its source codes, sample projects
and pre-trained models are available on-line:
https://github.com/YehLi/xmodaler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yehao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07935</id>
        <link href="http://arxiv.org/abs/2108.07935"/>
        <updated>2021-08-19T01:34:58.680Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the problem of developing personalized chatbots. A
personalized chatbot is designed as a digital chatting assistant for a user.
The key characteristic of a personalized chatbot is that it should have a
consistent personality with the corresponding user. It can talk the same way as
the user when it is delegated to respond to others' messages. We present a
retrieval-based personalized chatbot model, namely IMPChat, to learn an
implicit user profile from the user's dialogue history. We argue that the
implicit user profile is superior to the explicit user profile regarding
accessibility and flexibility. IMPChat aims to learn an implicit user profile
through modeling user's personalized language style and personalized
preferences separately. To learn a user's personalized language style, we
elaborately build language models from shallow to deep using the user's
historical responses; To model a user's personalized preferences, we explore
the conditional relations underneath each post-response pair of the user. The
personalized preferences are dynamic and context-aware: we assign higher
weights to those historical pairs that are topically related to the current
query when aggregating the personalized preferences. We match each response
candidate with the personalized language style and personalized preference,
respectively, and fuse the two matching signals to determine the final ranking
score. Comprehensive experiments on two large datasets show that our method
outperforms all baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Hongjin Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yutao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yueyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing Highlight Cues. (arXiv:2108.08112v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.08112</id>
        <link href="http://arxiv.org/abs/2108.08112"/>
        <updated>2021-08-19T01:34:58.636Z</updated>
        <summary type="html"><![CDATA[This paper presents a commentator for providing real-time game commentary in
a fighting game. The commentary takes into account highlight cues, obtained by
analyzing scenes during gameplay, as input to adjust the pitch and loudness of
commentary to be spoken by using a Text-to-Speech (TTS) technology. We
investigate different designs for pitch and loudness adjustment. The proposed
AI consists of two parts: a dynamic adjuster for controlling pitch and loudness
of the TTS and a real-time game commentary generator. We conduct a pilot study
on a fighting game, and our result shows that by adjusting the loudness
significantly according to the level of game highlight, the entertainment of
the gameplay can be enhanced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junjie H. Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhou Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qihang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohno_S/0/1/0/all/0/1"&gt;Satoru Ohno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1"&gt;Pujana Paliyawan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Framework for Cross-Domain and Cross-System Recommendations. (arXiv:2108.07976v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07976</id>
        <link href="http://arxiv.org/abs/2108.07976"/>
        <updated>2021-08-19T01:34:58.612Z</updated>
        <summary type="html"><![CDATA[Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have
been proposed to improve the recommendation accuracy in a target dataset
(domain/system) with the help of a source one with relatively richer
information. However, most existing CDR and CSR approaches are single-target,
namely, there is a single target dataset, which can only help the target
dataset and thus cannot benefit the source dataset. In this paper, we focus on
three new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR),
and CDR+CSR, and aim to improve the recommendation accuracy in all datasets
simultaneously for all scenarios. To do this, we propose a unified framework,
called GA (based on Graph embedding and Attention techniques), for all three
scenarios. In GA, we first construct separate heterogeneous graphs to generate
more representative user and item embeddings. Then, we propose an element-wise
attention mechanism to effectively combine the embeddings of common entities
(users/items) learned from different datasets. Moreover, to avoid negative
transfer, we further propose a Personalized training strategy to minimize the
embedding difference of common entities between a richer dataset and a sparser
dataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and
GA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments
conducted on four real-world datasets demonstrate that our proposed GA models
significantly outperform the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaochao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Longfei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guanfeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIFN: A Sentiment-aware Interactive Fusion Network for Review-based Item Recommendation. (arXiv:2108.08022v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.08022</id>
        <link href="http://arxiv.org/abs/2108.08022"/>
        <updated>2021-08-19T01:34:58.596Z</updated>
        <summary type="html"><![CDATA[Recent studies in recommender systems have managed to achieve significantly
improved performance by leveraging reviews for rating prediction. However,
despite being extensively studied, these methods still suffer from some
limitations. First, previous studies either encode the document or extract
latent sentiment via neural networks, which are difficult to interpret the
sentiment of reviewers intuitively. Second, they neglect the personalized
interaction of reviews with user/item, i.e., each review has different
contributions when modeling the sentiment preference of user/item. To remedy
these issues, we propose a Sentiment-aware Interactive Fusion Network (SIFN)
for review-based item recommendation. Specifically, we first encode user/item
reviews via BERT and propose a light-weighted sentiment learner to extract
semantic features of each review. Then, we propose a sentiment prediction task
that guides the sentiment learner to extract sentiment-aware features via
explicit sentiment labels. Finally, we design a rating prediction task that
contains a rating learner with an interactive and fusion module to fuse the
identity (i.e., user and item ID) and each review representation so that
various interactive features can synergistically influence the final rating
score. Experimental results on five real-world datasets demonstrate that the
proposed model is superior to state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Hao Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianhui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Promoting Mental Well-Being for Audiences in a Live-Streaming Game by Highlight-Based Bullet Comments. (arXiv:2108.08083v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.08083</id>
        <link href="http://arxiv.org/abs/2108.08083"/>
        <updated>2021-08-19T01:34:58.562Z</updated>
        <summary type="html"><![CDATA[This paper proposes a method for generating bullet comments for
live-streaming games based on highlights (i.e., the exciting parts of video
clips) extracted from the game content and evaluate the effect of mental health
promotion. Game live streaming is becoming a popular theme for academic
research. Compared to traditional online video sharing platforms, such as
Youtube and Vimeo, video live streaming platform has the benefits of
communicating with other viewers in real-time. In sports broadcasting, the
commentator plays an essential role as mood maker by making matches more
exciting. The enjoyment emerged while watching game live streaming also
benefits the audience's mental health. However, many e-sports live streaming
channels do not have a commentator for entertaining viewers. Therefore, this
paper presents a design of an AI commentator that can be embedded in live
streaming games. To generate bullet comments for real-time game live streaming,
the system employs highlight evaluation to detect the highlights, and generate
the bullet comments. An experiment is conducted and the effectiveness of
generated bullet comments in a live-streaming fighting game channel is
evaluated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junjie H. Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yulin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhou Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1"&gt;Pujana Paliyawan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photon-Starved Scene Inference using Single Photon Cameras. (arXiv:2107.11001v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11001</id>
        <link href="http://arxiv.org/abs/2107.11001"/>
        <updated>2021-08-18T01:55:02.599Z</updated>
        <summary type="html"><![CDATA[Scene understanding under low-light conditions is a challenging problem. This
is due to the small number of photons captured by the camera and the resulting
low signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging
sensing modality that are capable of capturing images with high sensitivity.
Despite having minimal read-noise, images captured by SPCs in photon-starved
conditions still suffer from strong shot noise, preventing reliable scene
inference. We propose photon scale-space a collection of high-SNR images
spanning a wide range of photons-per-pixel (PPP) levels (but same scene
content) as guides to train inference model on low photon flux images. We
develop training techniques that push images with different illumination levels
closer to each other in feature representation space. The key idea is that
having a spectrum of different brightness levels during training enables
effective guidance, and increases robustness to shot noise even in extreme
noise cases. Based on the proposed approach, we demonstrate, via simulations
and real experiments with a SPAD camera, high-performance on various inference
tasks such as image classification and monocular depth estimation under ultra
low-light, down to < 1 PPP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Goyal_B/0/1/0/all/0/1"&gt;Bhavya Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Mohit Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VideoLT: Large-scale Long-tailed Video Recognition. (arXiv:2105.02668v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02668</id>
        <link href="http://arxiv.org/abs/2105.02668"/>
        <updated>2021-08-18T01:55:02.592Z</updated>
        <summary type="html"><![CDATA[Label distributions in real-world are oftentimes long-tailed and imbalanced,
resulting in biased models towards dominant labels. While long-tailed
recognition has been extensively studied for image classification tasks,
limited effort has been made for video domain. In this paper, we introduce
VideoLT, a large-scale long-tailed video recognition dataset, as a step toward
real-world video recognition. Our VideoLT contains 256,218 untrimmed videos,
annotated into 1,004 classes with a long-tailed distribution. Through extensive
studies, we demonstrate that state-of-the-art methods used for long-tailed
image recognition do not perform well in the video domain due to the additional
temporal dimension in video data. This motivates us to propose FrameStack, a
simple yet effective method for long-tailed video recognition task. In
particular, FrameStack performs sampling at the frame-level in order to balance
class distributions, and the sampling ratio is dynamically determined using
knowledge derived from the network during training. Experimental results
demonstrate that FrameStack can improve classification performance without
sacrificing overall accuracy. Code and dataset are available at:
https://github.com/17Skye17/VideoLT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1"&gt;Zejia Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingjing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12085</id>
        <link href="http://arxiv.org/abs/2105.12085"/>
        <updated>2021-08-18T01:55:02.586Z</updated>
        <summary type="html"><![CDATA[Long-range and short-range temporal modeling are two complementary and
crucial aspects of video recognition. Most of the state-of-the-arts focus on
short-range spatio-temporal modeling and then average multiple snippet-level
predictions to yield the final video-level prediction. Thus, their video-level
prediction does not consider spatio-temporal features of how video evolves
along the temporal dimension. In this paper, we introduce a novel Dynamic
Segment Aggregation (DSA) module to capture relationship among snippets. To be
more specific, we attempt to generate a dynamic kernel for a convolutional
operation to aggregate long-range temporal information among adjacent snippets
adaptively. The DSA module is an efficient plug-and-play module and can be
combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform
powerful long-range modeling with minimal overhead. The final video
architecture, coined as DSANet. We conduct extensive experiments on several
video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,
Something-Something V1 and ActivityNet) to show its superiority. Our proposed
DSA module is shown to benefit various video recognition models significantly.
For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is
improved from 74.9% to 78.2% on Kinetics-400. Codes are available at
https://github.com/whwu95/DSANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Mingde Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zichao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yifeng Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-wise Inhibition based Feature Regularization for Robust Classification. (arXiv:2103.02152v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02152</id>
        <link href="http://arxiv.org/abs/2103.02152"/>
        <updated>2021-08-18T01:55:02.577Z</updated>
        <summary type="html"><![CDATA[The convolutional neural network (CNN) is vulnerable to degraded images with
even very small variations (e.g. corrupted and adversarial samples). One of the
possible reasons is that CNN pays more attention to the most discriminative
regions, but ignores the auxiliary features when learning, leading to the lack
of feature diversity for final judgment. In our method, we propose to
dynamically suppress significant activation values of CNN by group-wise
inhibition, but not fixedly or randomly handle them when training. The feature
maps with different activation distribution are then processed separately to
take the feature independence into account. CNN is finally guided to learn
richer discriminative features hierarchically for robust classification
according to the proposed regularization. Our method is comprehensively
evaluated under multiple settings, including classification against
corruptions, adversarial attacks and low data regime. Extensive experimental
results show that the proposed method can achieve significant improvements in
terms of both robustness and generalization performances, when compared with
the state-of-the-art methods. Code is available at
https://github.com/LinusWu/TENET_Training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haozhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haoqian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weicheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07761</id>
        <link href="http://arxiv.org/abs/2108.07761"/>
        <updated>2021-08-18T01:55:02.555Z</updated>
        <summary type="html"><![CDATA[Vision plays a crucial role to comprehend the world around us as more than
85% of the external information is obtained through the vision system. It
largely influences our mobility, cognition, information access, and interaction
with the environment as well as with other people. Blindness prevents a person
from gaining knowledge of the surrounding environment and makes unassisted
navigation, object recognition, obstacle avoidance, and reading tasks major
challenges. Many existing systems are often limited by cost and complexity. To
help the visually challenged overcome these difficulties faced in everyday
life, we propose the idea of VisBuddy, a smart assistant which will help the
visually challenged with their day-to-day activities. VisBuddy is a voice-based
assistant, where the user can give voice commands to perform specific tasks.
VisBuddy uses the techniques of image captioning for describing the user's
surroundings, optical character recognition (OCR) for reading the text in the
user's view, object detection to search and find the objects in a room and web
scraping to give the user the latest news. VisBuddy has been built by combining
the concepts from Deep Learning and the Internet of Things. Thus, VisBuddy
serves as a cost-efficient, powerful and all-in-one assistant for the visually
challenged by helping them with their day-to-day activities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sivakumar_I/0/1/0/all/0/1"&gt;Ishwarya Sivakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meenakshisundaram_N/0/1/0/all/0/1"&gt;Nishaali Meenakshisundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_I/0/1/0/all/0/1"&gt;Ishwarya Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1"&gt;Shiloah Elizabeth D&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1"&gt;Sunil Retmin Raj C&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially Conditioned Graphs for Detecting Human-Object Interactions. (arXiv:2012.06060v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06060</id>
        <link href="http://arxiv.org/abs/2012.06060"/>
        <updated>2021-08-18T01:55:02.548Z</updated>
        <summary type="html"><![CDATA[We address the problem of detecting human-object interactions in images using
graphical neural networks. Unlike conventional methods, where nodes send scaled
but otherwise identical messages to each of their neighbours, we propose to
condition messages between pairs of nodes on their spatial relationships,
resulting in different messages going to neighbours of the same node. To this
end, we explore various ways of applying spatial conditioning under a
multi-branch structure. Through extensive experimentation we demonstrate the
advantages of spatial conditioning for the computation of the adjacency
structure, messages and the refined graph features. In particular, we
empirically show that as the quality of the bounding boxes increases, their
coarse appearance features contribute relatively less to the disambiguation of
interactions compared to the spatial information. Our method achieves an mAP of
31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming
state-of-the-art on fine-tuned detections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frederic Z. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1"&gt;Dylan Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. (arXiv:2107.10833v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10833</id>
        <link href="http://arxiv.org/abs/2107.10833"/>
        <updated>2021-08-18T01:55:02.542Z</updated>
        <summary type="html"><![CDATA[Though many attempts have been made in blind super-resolution to restore
low-resolution images with unknown and complex degradations, they are still far
from addressing general real-world degraded images. In this work, we extend the
powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),
which is trained with pure synthetic data. Specifically, a high-order
degradation modeling process is introduced to better simulate complex
real-world degradations. We also consider the common ringing and overshoot
artifacts in the synthesis process. In addition, we employ a U-Net
discriminator with spectral normalization to increase discriminator capability
and stabilize the training dynamics. Extensive comparisons have shown its
superior visual performance than prior works on various real datasets. We also
provide efficient implementations to synthesize training pairs on the fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1"&gt;Liangbin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar. (arXiv:2103.08863v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08863</id>
        <link href="http://arxiv.org/abs/2103.08863"/>
        <updated>2021-08-18T01:55:02.534Z</updated>
        <summary type="html"><![CDATA[Conventional face super-resolution methods usually assume testing
low-resolution (LR) images lie in the same domain as the training ones. Due to
different lighting conditions and imaging hardware, domain gaps between
training and testing images inevitably occur in many real-world scenarios.
Neglecting those domain gaps would lead to inferior face super-resolution (FSR)
performance. However, how to transfer a trained FSR model to a target domain
efficiently and effectively has not been investigated. To tackle this problem,
we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named
DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces
from a target domain by exploiting only a pair of high-resolution (HR) and LR
exemplar in the target domain. To be specific, our DAP-FSR firstly employs its
encoder to extract the multi-scale latent representations of the input LR face.
Considering only one target domain example is available, we propose to augment
the target domain data by mixing the latent representations of the target
domain face and source domain ones, and then feed the mixed representations to
the decoder of our DAP-FSR. The decoder will generate new face images
resembling the target domain image style. The generated HR faces in turn are
used to optimize our decoder to reduce the domain gap. By iteratively updating
the latent representations and our decoder, our DAP-FSR will be adapted to the
target domain, thus achieving authentic and high-quality upsampled HR faces.
Extensive experiments on three newly constructed benchmarks validate the
effectiveness and superior performance of our DAP-FSR compared to the
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peike Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06069</id>
        <link href="http://arxiv.org/abs/2101.06069"/>
        <updated>2021-08-18T01:55:02.516Z</updated>
        <summary type="html"><![CDATA[Pretrained deep models hold their learnt knowledge in the form of model
parameters. These parameters act as "memory" for the trained models and help
them generalize well on unseen data. However, in absence of training data, the
utility of a trained model is merely limited to either inference or better
initialization towards a target task. In this paper, we go further and extract
synthetic data by leveraging the learnt model parameters. We dub them "Data
Impressions", which act as proxy to the training data and can be used to
realize a variety of tasks. These are useful in scenarios where only the
pretrained models are available and the training data is not shared (e.g., due
to privacy or sensitivity concerns). We show the applicability of data
impressions in solving several computer vision tasks such as unsupervised
domain adaptation, continual learning as well as knowledge distillation. We
also study the adversarial robustness of lightweight models trained via
knowledge distillation using these data impressions. Further, we demonstrate
the efficacy of data impressions in generating data-free Universal Adversarial
Perturbations (UAPs) with better fooling rates. Extensive experiments performed
on benchmark datasets demonstrate competitive performance achieved using data
impressions in absence of original training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1"&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1"&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saksham Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Anirban Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13459</id>
        <link href="http://arxiv.org/abs/2107.13459"/>
        <updated>2021-08-18T01:55:02.510Z</updated>
        <summary type="html"><![CDATA[In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose a point cloud-applicable explainability approach
based on local surrogate model-based method to show which components contribute
to the classification. Moreover, we propose quantitative fidelity validations
for generated explanations that enhance the persuasive power of explainability
and compare the plausibility of different existing point cloud-applicable
explainability methods. Our new explainability approach provides a fairly
accurate, more semantically coherent and widely applicable explanation for
point cloud classification tasks. Our code is available at
https://github.com/Explain3D/LIME-3D]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hanxiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generative Models of Textured 3D Meshes from Real-World Images. (arXiv:2103.15627v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15627</id>
        <link href="http://arxiv.org/abs/2103.15627"/>
        <updated>2021-08-18T01:55:02.503Z</updated>
        <summary type="html"><![CDATA[Recent advances in differentiable rendering have sparked an interest in
learning generative models of textured 3D meshes from image collections. These
models natively disentangle pose and appearance, enable downstream applications
in computer graphics, and improve the ability of generative models to
understand the concept of image formation. Although there has been prior work
on learning such models from collections of 2D images, these approaches require
a delicate pose estimation step that exploits annotated keypoints, thereby
restricting their applicability to a few specific datasets. In this work, we
propose a GAN framework for generating textured triangle meshes without relying
on such annotations. We show that the performance of our approach is on par
with prior work that relies on ground-truth keypoints, and more importantly, we
demonstrate the generality of our method by setting new baselines on a larger
set of categories from ImageNet - for which keypoints are not available -
without any class-specific hyperparameter tuning. We release our code at
https://github.com/dariopavllo/textured-3d-gan]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1"&gt;Dario Pavllo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas Kohler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1"&gt;Aurelien Lucchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack. (arXiv:2011.14585v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14585</id>
        <link href="http://arxiv.org/abs/2011.14585"/>
        <updated>2021-08-18T01:55:02.497Z</updated>
        <summary type="html"><![CDATA[The video-based action recognition task has been extensively studied in
recent years. In this paper, we study the structural vulnerability of deep
learning-based action recognition models against the adversarial attack using
the one frame attack that adds an inconspicuous perturbation to only a single
frame of a given video clip. Our analysis shows that the models are highly
vulnerable against the one frame attack due to their structural properties.
Experiments demonstrate high fooling rates and inconspicuous characteristics of
the attack. Furthermore, we show that strong universal one frame perturbations
can be obtained under various scenarios. Our work raises the serious issue of
adversarial vulnerability of the state-of-the-art action recognition models in
various perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jaehui Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jun-Hyuk Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Seok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parametric Contrastive Learning. (arXiv:2107.12028v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12028</id>
        <link href="http://arxiv.org/abs/2107.12028"/>
        <updated>2021-08-18T01:55:02.491Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle
long-tailed recognition. Based on theoretical analysis, we observe supervised
contrastive loss tends to bias on high-frequency classes and thus increases the
difficulty of imbalanced learning. We introduce a set of parametric class-wise
learnable centers to rebalance from an optimization perspective. Further, we
analyze our PaCo loss under a balanced setting. Our analysis demonstrates that
PaCo can adaptively enhance the intensity of pushing samples of the same class
close as more samples are pulled together with their corresponding centers and
benefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,
Places, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed
recognition. On full ImageNet, models trained with PaCo loss surpass supervised
contrastive learning across various ResNet backbones, e.g., our ResNet-200
achieves 81.8% top-1 accuracy. Our code is available at
https://github.com/dvlab-research/Parametric-Contrastive-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jiequan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhisheng Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion. (arXiv:2107.01175v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01175</id>
        <link href="http://arxiv.org/abs/2107.01175"/>
        <updated>2021-08-18T01:55:02.485Z</updated>
        <summary type="html"><![CDATA[We propose an audio-visual spatial-temporal deep neural network with: (1) a
visual block containing a pretrained 2D-CNN followed by a temporal
convolutional network (TCN); (2) an aural block containing several parallel
TCNs; and (3) a leader-follower attentive fusion block combining the
audio-visual information. The TCN with large history coverage enables our model
to exploit spatial-temporal information within a much larger window length
(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36
or 48). The fusion block emphasizes the visual modality while exploits the
noisy aural modality using the inter-modality attention mechanism. To make full
use of the data and alleviate over-fitting, cross-validation is carried out on
the training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. On the test (validation)
set of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence
and 0.492 (0.649) for arousal, which significantly outperforms the baseline
method with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for
valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03035</id>
        <link href="http://arxiv.org/abs/2107.03035"/>
        <updated>2021-08-18T01:55:02.478Z</updated>
        <summary type="html"><![CDATA[Coronary artery disease (CAD) has posed a leading threat to the lives of
cardiovascular disease patients worldwide for a long time. Therefore, automated
diagnosis of CAD has indispensable significance in clinical medicine. However,
the complexity of coronary artery plaques that cause CAD makes the automatic
detection of coronary artery stenosis in Coronary CT angiography (CCTA) a
difficult task. In this paper, we propose a Transformer network (TR-Net) for
the automatic detection of significant stenosis (i.e. luminal narrowing > 50%)
while practically completing the computer-assisted diagnosis of CAD. The
proposed TR-Net introduces a novel Transformer, and tightly combines
convolutional layers and Transformer encoders, allowing their advantages to be
demonstrated in the task. By analyzing semantic information sequences, TR-Net
can fully understand the relationship between image information in each
position of a multiplanar reformatted (MPR) image, and accurately detect
significant stenosis based on both local and global information. We evaluate
our TR-Net on a dataset of 76 patients from different patients annotated by
experienced radiologists. Experimental results illustrate that our TR-Net has
achieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and
MCC (0.74) indicators compared with the state-of-the-art methods. The source
code is publicly available from the link (https://github.com/XinghuaMa/TR-Net).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinghua Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1"&gt;Gongning Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kuanquan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12230</id>
        <link href="http://arxiv.org/abs/2010.12230"/>
        <updated>2021-08-18T01:55:02.472Z</updated>
        <summary type="html"><![CDATA[The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1"&gt;Suvrit Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised 3D Human Pose Estimation with Multiple-View Geometry. (arXiv:2108.07777v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07777</id>
        <link href="http://arxiv.org/abs/2108.07777"/>
        <updated>2021-08-18T01:55:02.452Z</updated>
        <summary type="html"><![CDATA[We present a self-supervised learning algorithm for 3D human pose estimation
of a single person based on a multiple-view camera system and 2D body pose
estimates for each view. To train our model, represented by a deep neural
network, we propose a four-loss function learning algorithm, which does not
require any 2D or 3D body pose ground-truth. The proposed loss functions make
use of the multiple-view geometry to reconstruct 3D body pose estimates and
impose body pose constraints across the camera views. Our approach utilizes all
available camera views during training, while the inference is single-view. In
our evaluations, we show promising performance on Human3.6M and HumanEva
benchmarks, while we also present a generalization study on MPI-INF-3DHP
dataset, as well as several ablation results. Overall, we outperform all
self-supervised learning methods and reach comparable results to supervised and
weakly-supervised learning approaches. Our code and models are publicly
available]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1"&gt;Arij Bouazizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiederer_J/0/1/0/all/0/1"&gt;Julian Wiederer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1"&gt;Ulrich Kressel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Autoregressive Transformer for Image Captioning. (arXiv:2106.09436v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09436</id>
        <link href="http://arxiv.org/abs/2106.09436"/>
        <updated>2021-08-18T01:55:02.427Z</updated>
        <summary type="html"><![CDATA[Current state-of-the-art image captioning models adopt autoregressive
decoders, \ie they generate each word by conditioning on previously generated
words, which leads to heavy latency during inference. To tackle this issue,
non-autoregressive image captioning models have recently been proposed to
significantly accelerate the speed of inference by generating all words in
parallel. However, these non-autoregressive models inevitably suffer from large
generation quality degradation since they remove words dependence excessively.
To make a better trade-off between speed and quality, we introduce a
semi-autoregressive model for image captioning~(dubbed as SATIC), which keeps
the autoregressive property in global but generates words parallelly in local .
Based on Transformer, there are only a few modifications needed to implement
SATIC. Experimental results on the MSCOCO image captioning benchmark show that
SATIC can achieve a good trade-off without bells and whistles. Code is
available at {\color{magenta}\url{https://github.com/YuanEZhou/satic}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuanen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhenzhen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02960</id>
        <link href="http://arxiv.org/abs/2107.02960"/>
        <updated>2021-08-18T01:55:02.407Z</updated>
        <summary type="html"><![CDATA[We introduce the first Neural Architecture Search (NAS) method to find a
better transformer architecture for image recognition. Recently, transformers
without CNN-based backbones are found to achieve impressive performance for
image recognition. However, the transformer is designed for NLP tasks and thus
could be sub-optimal when directly used for image recognition. In order to
improve the visual representation ability for transformers, we propose a new
search space and searching algorithm. Specifically, we introduce a locality
module that models the local correlations in images explicitly with fewer
computational cost. With the locality module, our search space is defined to
let the search algorithm freely trade off between global and local information
as well as optimizing the low-level design choice in each module. To tackle the
problem caused by huge search space, a hierarchical neural architecture search
method is proposed to search the optimal vision transformer from two levels
separately with the evolutionary algorithm. Extensive experiments on the
ImageNet dataset demonstrate that our method can find more discriminative and
efficient transformer variants than the ResNet family (e.g., ResNet101) and the
baseline ViT for image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peixia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baopu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1"&gt;Lei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Ming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1"&gt;Junjie yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Aware Universal Style Transfer. (arXiv:2108.04441v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04441</id>
        <link href="http://arxiv.org/abs/2108.04441"/>
        <updated>2021-08-18T01:55:02.390Z</updated>
        <summary type="html"><![CDATA[Style transfer aims to reproduce content images with the styles from
reference images. Existing universal style transfer methods successfully
deliver arbitrary styles to original images either in an artistic or a
photo-realistic way. However, the range of 'arbitrary style' defined by
existing works is bounded in the particular domain due to their structural
limitation. Specifically, the degrees of content preservation and stylization
are established according to a predefined target domain. As a result, both
photo-realistic and artistic models have difficulty in performing the desired
style transfer for the other domain. To overcome this limitation, we propose a
unified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer
not only the style but also the property of domain (i.e., domainness) from a
given reference image. To this end, we design a novel domainness indicator that
captures the domainness value from the texture and structural features of
reference images. Moreover, we introduce a unified framework with domain-aware
skip connection to adaptively transfer the stroke and palette to the input
contents guided by the domainness indicator. Our extensive experiments validate
that our model produces better qualitative results and outperforms previous
methods in terms of proxy metrics on both artistic and photo-realistic
stylizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1"&gt;Kibeom Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1"&gt;Seogkyu Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1"&gt;Hyeran Byun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks. (arXiv:2011.11479v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11479</id>
        <link href="http://arxiv.org/abs/2011.11479"/>
        <updated>2021-08-18T01:55:02.384Z</updated>
        <summary type="html"><![CDATA[Due to the large memory footprint of untrimmed videos, current
state-of-the-art video localization methods operate atop precomputed video clip
features. These features are extracted from video encoders typically trained
for trimmed action classification tasks, making such features not necessarily
suitable for temporal localization. In this work, we propose a novel supervised
pretraining paradigm for clip features that not only trains to classify
activities but also considers background clips and global video information to
improve temporal sensitivity. Extensive experiments show that using features
trained with our novel pretraining strategy significantly improves the
performance of recent state-of-the-art methods on three tasks: Temporal Action
Localization, Action Proposal Generation, and Dense Video Captioning. We also
show that our pretraining approach is effective across three encoder
architectures and two pretraining datasets. We believe video feature encoding
is an important building block for localization algorithms, and extracting
temporally-sensitive features should be of paramount importance in building
more accurate models. The code and pretrained models are available on our
project website.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alwassel_H/0/1/0/all/0/1"&gt;Humam Alwassel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1"&gt;Silvio Giancola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction. (arXiv:2108.07152v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07152</id>
        <link href="http://arxiv.org/abs/2108.07152"/>
        <updated>2021-08-18T01:55:02.377Z</updated>
        <summary type="html"><![CDATA[Human motion prediction is a challenging task due to the stochasticity and
aperiodicity of future poses. Recently, graph convolutional network has been
proven to be very effective to learn dynamic relations among pose joints, which
is helpful for pose prediction. On the other hand, one can abstract a human
pose recursively to obtain a set of poses at multiple scales. With the increase
of the abstraction level, the motion of the pose becomes more stable, which
benefits pose prediction too. In this paper, we propose a novel Multi-Scale
Residual Graph Convolution Network (MSR-GCN) for human pose prediction task in
the manner of end-to-end. The GCNs are used to extract features from fine to
coarse scale and then from coarse to fine scale. The extracted features at each
scale are then combined and decoded to obtain the residuals between the input
and target poses. Intermediate supervisions are imposed on all the predicted
poses, which enforces the network to learn more representative features. Our
proposed approach is evaluated on two standard benchmark datasets, i.e., the
Human3.6M dataset and the CMU Mocap dataset. Experimental results demonstrate
that our method outperforms the state-of-the-art approaches. Code and
pre-trained models are available at https://github.com/Droliven/MSRGCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1"&gt;Lingwei Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yongwei Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guiqing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07253</id>
        <link href="http://arxiv.org/abs/2108.07253"/>
        <updated>2021-08-18T01:55:02.356Z</updated>
        <summary type="html"><![CDATA[We present a task and benchmark dataset for person-centric visual grounding,
the problem of linking between people named in a caption and people pictured in
an image. In contrast to prior work in visual grounding, which is predominantly
object-based, our new task masks out the names of people in captions in order
to encourage methods trained on such image-caption pairs to focus on contextual
cues (such as rich interactions between multiple people), rather than learning
associations between names and appearances. To facilitate this task, we
introduce a new dataset, Who's Waldo, mined automatically from image-caption
data on Wikimedia Commons. We propose a Transformer-based method that
outperforms several strong baselines on this task, and are releasing our data
to the research community to spur work on contextual models that consider both
vision and language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Claire Yuqing Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"&gt;Apoorv Khandelwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1"&gt;Yoav Artzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1"&gt;Noah Snavely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1"&gt;Hadar Averbuch-Elor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (arXiv:2103.14030v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14030</id>
        <link href="http://arxiv.org/abs/2103.14030"/>
        <updated>2021-08-18T01:55:02.345Z</updated>
        <summary type="html"><![CDATA[This paper presents a new vision Transformer, called Swin Transformer, that
capably serves as a general-purpose backbone for computer vision. Challenges in
adapting Transformer from language to vision arise from differences between the
two domains, such as large variations in the scale of visual entities and the
high resolution of pixels in images compared to words in text. To address these
differences, we propose a hierarchical Transformer whose representation is
computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme
brings greater efficiency by limiting self-attention computation to
non-overlapping local windows while also allowing for cross-window connection.
This hierarchical architecture has the flexibility to model at various scales
and has linear computational complexity with respect to image size. These
qualities of Swin Transformer make it compatible with a broad range of vision
tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and
dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP
on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its
performance surpasses the previous state-of-the-art by a large margin of +2.7
box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the
potential of Transformer-based models as vision backbones. The hierarchical
design and the shifted window approach also prove beneficial for all-MLP
architectures. The code and models are publicly available
at~\url{https://github.com/microsoft/Swin-Transformer}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yutong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Baining Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Multi-Target Domain Adaptation. (arXiv:2108.07792v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07792</id>
        <link href="http://arxiv.org/abs/2108.07792"/>
        <updated>2021-08-18T01:55:02.330Z</updated>
        <summary type="html"><![CDATA[Federated learning methods enable us to train machine learning models on
distributed user data while preserving its privacy. However, it is not always
feasible to obtain high-quality supervisory signals from users, especially for
vision tasks. Unlike typical federated settings with labeled client data, we
consider a more practical scenario where the distributed client data is
unlabeled, and a centralized labeled dataset is available on the server. We
further take the server-client and inter-client domain shifts into account and
pose a domain adaptation problem with one source (centralized server data) and
multiple targets (distributed client data). Within this new Federated
Multi-Target Domain Adaptation (FMTDA) task, we analyze the model performance
of exiting domain adaptation methods and propose an effective DualAdapt method
to address the new challenges. Extensive experimental results on image
classification and semantic segmentation tasks demonstrate that our method
achieves high accuracy, incurs minimal communication cost, and requires low
computational resources on client devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1"&gt;Chun-Han Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yukun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Enhanced 3D Point Cloud Reconstruction from A Single Image. (arXiv:2108.07685v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07685</id>
        <link href="http://arxiv.org/abs/2108.07685"/>
        <updated>2021-08-18T01:55:02.322Z</updated>
        <summary type="html"><![CDATA[Solving the challenging problem of 3D object reconstruction from a single
image appropriately gives existing technologies the ability to perform with a
single monocular camera rather than requiring depth sensors. In recent years,
thanks to the development of deep learning, 3D reconstruction of a single image
has demonstrated impressive progress. Existing researches use Chamfer distance
as a loss function to guide the training of the neural network. However, the
Chamfer loss will give equal weights to all points inside the 3D point clouds.
It tends to sacrifice fine-grained and thin structures to avoid incurring a
high loss, which will lead to visually unsatisfactory results. This paper
proposes a framework that can recover a detailed three-dimensional point cloud
from a single image by focusing more on boundaries (edge and corner points).
Experimental results demonstrate that the proposed method outperforms existing
techniques significantly, both qualitatively and quantitatively, and has fewer
training parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ping_G/0/1/0/all/0/1"&gt;Guiju Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1"&gt;Mahdi Abolfazli Esfahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Han Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture-based Feature Space Learning for Few-shot Image Classification. (arXiv:2011.11872v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11872</id>
        <link href="http://arxiv.org/abs/2011.11872"/>
        <updated>2021-08-18T01:55:02.315Z</updated>
        <summary type="html"><![CDATA[We introduce Mixture-based Feature Space Learning (MixtFSL) for obtaining a
rich and robust feature representation in the context of few-shot image
classification. Previous works have proposed to model each base class either
with a single point or with a mixture model by relying on offline clustering
algorithms. In contrast, we propose to model base classes with mixture models
by simultaneously training the feature extractor and learning the mixture model
parameters in an online manner. This results in a richer and more
discriminative feature space which can be employed to classify novel examples
from very few samples. Two main stages are proposed to train the MixtFSL model.
First, the multimodal mixtures for each base class and the feature extractor
parameters are learned using a combination of two loss functions. Second, the
resulting network and mixture models are progressively refined through a
leader-follower learning procedure, which uses the current estimate as a
"target" network. This target network is used to make a consistent assignment
of instances to mixture components, which increases performance and stabilizes
training. The effectiveness of our end-to-end feature space learning approach
is demonstrated with extensive experiments on four standard datasets and four
backbones. Notably, we demonstrate that when we combine our robust
representation with recent alignment-based approaches, we achieve new
state-of-the-art results in the inductive setting, with an absolute accuracy
for 5-shot classification of 82.45 on miniImageNet, 88.20 with tieredImageNet,
and 60.70 in FC100 using the ResNet-12 backbone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afrasiyabi_A/0/1/0/all/0/1"&gt;Arman Afrasiyabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1"&gt;Jean-Fran&amp;#xe7;ois Lalonde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1"&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Dense Video Captioning with Parallel Decoding. (arXiv:2108.07781v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07781</id>
        <link href="http://arxiv.org/abs/2108.07781"/>
        <updated>2021-08-18T01:55:02.308Z</updated>
        <summary type="html"><![CDATA[Dense video captioning aims to generate multiple associated captions with
their temporal locations from the video. Previous methods follow a
sophisticated "localize-then-describe" scheme, which heavily relies on numerous
hand-crafted components. In this paper, we proposed a simple yet effective
framework for end-to-end dense video captioning with parallel decoding (PDVC),
by formulating the dense caption generation as a set prediction task. In
practice, through stacking a newly proposed event counter on the top of a
transformer decoder, the PDVC precisely segments the video into a number of
event pieces under the holistic understanding of the video content, which
effectively increases the coherence and readability of predicted captions.
Compared with prior arts, the PDVC has several appealing advantages: (1)
Without relying on heuristic non-maximum suppression or a recurrent event
sequence selection network to remove redundancy, PDVC directly produces an
event set with an appropriate size; (2) In contrast to adopting the two-stage
scheme, we feed the enhanced representations of event queries into the
localization head and caption head in parallel, making these two sub-tasks
deeply interrelated and mutually promoted through the optimization; (3) Without
bells and whistles, extensive experiments on ActivityNet Captions and YouCook2
show that PDVC is capable of producing high-quality captioning results,
surpassing the state-of-the-art two-stage methods when its localization
accuracy is on par with them. Code is available at
https://github.com/ttengwang/PDVC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Teng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhichao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1"&gt;Ran Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data. (arXiv:2012.03255v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03255</id>
        <link href="http://arxiv.org/abs/2012.03255"/>
        <updated>2021-08-18T01:55:02.289Z</updated>
        <summary type="html"><![CDATA[Recent work has shown impressive results on data-driven defocus deblurring
using the two-image views available on modern dual-pixel (DP) sensors. One
significant challenge in this line of research is access to DP data. Despite
many cameras having DP sensors, only a limited number provide access to the
low-level DP sensor images. In addition, capturing training data for defocus
deblurring involves a time-consuming and tedious setup requiring the camera's
aperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do
not have adjustable apertures, further limiting the ability to produce the
necessary training data. We address the data capture bottleneck by proposing a
procedure to generate realistic DP data synthetically. Our synthesis approach
mimics the optical image formation found on DP sensors and can be applied to
virtual scenes rendered with standard computer software. Leveraging these
realistic synthetic DP images, we introduce a recurrent convolutional network
(RCN) architecture that improves deblurring results and is suitable for use
with single-frame and multi-frame data (e.g., video) captured by DP sensors.
Finally, we show that our synthetic DP data is useful for training DNN models
targeting video deblurring applications where access to DP data remains
challenging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abuolaim_A/0/1/0/all/0/1"&gt;Abdullah Abuolaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Delbracio_M/0/1/0/all/0/1"&gt;Mauricio Delbracio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1"&gt;Damien Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brown_M/0/1/0/all/0/1"&gt;Michael S. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1"&gt;Peyman Milanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01912</id>
        <link href="http://arxiv.org/abs/2108.01912"/>
        <updated>2021-08-18T01:55:02.283Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for video inpainting by adopting an internal
learning strategy. Unlike previous methods that use optical flow for
cross-frame context propagation to inpaint unknown regions, we show that this
can be achieved implicitly by fitting a convolutional neural network to known
regions. Moreover, to handle challenging sequences with ambiguous backgrounds
or long-term occlusion, we design two regularization terms to preserve
high-frequency details and long-term temporal consistency. Extensive
experiments on the DAVIS dataset demonstrate that the proposed method achieves
state-of-the-art inpainting quality quantitatively and qualitatively. We
further extend the proposed method to another challenging task: learning to
remove an object from a video giving a single object mask in only one frame in
a 4K video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1"&gt;Hao Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tengfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Appearance Based Deep Domain Adaptation for the Classification of Aerial Images. (arXiv:2108.07779v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07779</id>
        <link href="http://arxiv.org/abs/2108.07779"/>
        <updated>2021-08-18T01:55:02.276Z</updated>
        <summary type="html"><![CDATA[This paper addresses domain adaptation for the pixel-wise classification of
remotely sensed data using deep neural networks (DNN) as a strategy to reduce
the requirements of DNN with respect to the availability of training data. We
focus on the setting in which labelled data are only available in a source
domain DS, but not in a target domain DT. Our method is based on adversarial
training of an appearance adaptation network (AAN) that transforms images from
DS such that they look like images from DT. Together with the original label
maps from DS, the transformed images are used to adapt a DNN to DT. We propose
a joint training strategy of the AAN and the classifier, which constrains the
AAN to transform the images such that they are correctly classified. In this
way, objects of a certain class are changed such that they resemble objects of
the same class in DT. To further improve the adaptation performance, we propose
a new regularization loss for the discriminator network used in domain
adversarial training. We also address the problem of finding the optimal values
of the trained network parameters, proposing an unsupervised entropy based
parameter selection criterion which compensates for the fact that there is no
validation set in DT that could be monitored. As a minor contribution, we
present a new weighting strategy for the cross-entropy loss, addressing the
problem of imbalanced class distributions. Our method is evaluated in 42
adaptation scenarios using datasets from 7 cities, all consisting of
high-resolution digital orthophotos and height data. It achieves a positive
transfer in all cases, and on average it improves the performance in the target
domain by 4.3% in overall accuracy. In adaptation scenarios between datasets
from the ISPRS semantic labelling benchmark our method outperforms those from
recent publications by 10-20% with respect to the mean intersection over union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wittich_D/0/1/0/all/0/1"&gt;Dennis Wittich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rottensteiner_F/0/1/0/all/0/1"&gt;Franz Rottensteiner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for Depth Completion. (arXiv:2012.08270v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08270</id>
        <link href="http://arxiv.org/abs/2012.08270"/>
        <updated>2021-08-18T01:55:02.269Z</updated>
        <summary type="html"><![CDATA[Depth completion aims to recover a dense depth map from a sparse depth map
with the corresponding color image as input. Recent approaches mainly formulate
depth completion as a one-stage end-to-end learning task, which outputs dense
depth maps directly. However, the feature extraction and supervision in
one-stage frameworks are insufficient, limiting the performance of these
approaches. To address this problem, we propose a novel end-to-end residual
learning framework, which formulates the depth completion as a two-stage
learning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage.
First, a coarse dense depth map is obtained by a simple CNN framework. Then, a
refined depth map is further obtained using a residual learning strategy in the
coarse-to-fine stage with a coarse depth map and color image as input.
Specially, in the coarse-to-fine stage, a channel shuffle extraction operation
is utilized to extract more representative features from the color image and
coarse depth map, and an energy based fusion operation is exploited to
effectively fuse these features obtained by channel shuffle operation, thus
leading to more accurate and refined depth maps. We achieve SoTA performance in
RMSE on KITTI benchmark. Extensive experiments on other datasets future
demonstrate the superiority of our approach over current state-of-the-art depth
completion approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lina Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xibin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1"&gt;Xiaoyang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1"&gt;Junwei Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangjun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization. (arXiv:2011.08517v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08517</id>
        <link href="http://arxiv.org/abs/2011.08517"/>
        <updated>2021-08-18T01:55:02.262Z</updated>
        <summary type="html"><![CDATA[Since the introduction of modern deep learning methods for object pose
estimation, test accuracy and efficiency has increased significantly. For
training, however, large amounts of annotated training data are required for
good performance. While the use of synthetic training data prevents the need
for manual annotation, there is currently a large performance gap between
methods trained on real and synthetic data. This paper introduces a new method,
which bridges this gap.

Most methods trained on synthetic data use 2D images, as domain randomization
in 2D is more developed. To obtain precise poses, many of these methods perform
a final refinement using 3D data. Our method integrates the 3D data into the
network to increase the accuracy of the pose estimation. To allow for domain
randomization in 3D, a sensor-based data augmentation has been developed.
Additionally, we introduce the SparseEdge feature, which uses a wider search
space during point cloud propagation to avoid relying on specific features
without increasing run-time.

Experiments on three large pose estimation benchmarks show that the presented
method outperforms previous methods trained on synthetic data and achieves
comparable results to existing methods trained on real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1"&gt;Frederik Hagelskjaer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_A/0/1/0/all/0/1"&gt;Anders Glent Buch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks. (arXiv:2007.05785v5 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05785</id>
        <link href="http://arxiv.org/abs/2007.05785"/>
        <updated>2021-08-18T01:55:02.243Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have attracted enormous research interest due
to temporal information processing capability, low power consumption, and high
biological plausibility. However, the formulation of efficient and
high-performance learning algorithms for SNNs is still challenging. Most
existing learning methods learn weights only, and require manual tuning of the
membrane-related parameters that determine the dynamics of a single spiking
neuron. These parameters are typically chosen to be the same for all neurons,
which limits the diversity of neurons and thus the expressiveness of the
resulting SNNs. In this paper, we take inspiration from the observation that
membrane-related parameters are different across brain regions, and propose a
training algorithm that is capable of learning not only the synaptic weights
but also the membrane time constants of SNNs. We show that incorporating
learnable membrane time constants can make the network less sensitive to
initial values and can speed up learning. In addition, we reevaluate the
pooling methods in SNNs and find that max-pooling will not lead to significant
information loss and have the advantage of low computation cost and binary
compatibility. We evaluate the proposed method for image classification tasks
on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and
neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment
results show that the proposed method outperforms the state-of-the-art accuracy
on nearly all datasets, using fewer time-steps. Our codes are available at
https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1"&gt;Timothee Masquelier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision. (arXiv:2108.07682v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07682</id>
        <link href="http://arxiv.org/abs/2108.07682"/>
        <updated>2021-08-18T01:55:02.236Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a conceptually simple, strong, and efficient
framework for fully- and weakly-supervised panoptic segmentation, called
Panoptic FCN. Our approach aims to represent and predict foreground things and
background stuff in a unified fully convolutional pipeline, which can be
optimized with point-based fully or weak supervision. In particular, Panoptic
FCN encodes each object instance or stuff category with the proposed kernel
generator and produces the prediction by convolving the high-resolution feature
directly. With this approach, instance-aware and semantically consistent
properties for things and stuff can be respectively satisfied in a simple
generate-kernel-then-segment workflow. Without extra boxes for localization or
instance separation, the proposed approach outperforms the previous box-based
and -free models with high efficiency. Furthermore, we propose a new form of
point-based annotation for weakly-supervised panoptic segmentation. It only
needs several random points for both things and stuff, which dramatically
reduces the annotation cost of human. The proposed Panoptic FCN is also proved
to have much superior performance in this weakly-supervised setting, which
achieves 82% of the fully-supervised performance with only 20 randomly
annotated points per instance. Extensive experiments demonstrate the
effectiveness and efficiency of Panoptic FCN on COCO, VOC 2012, Cityscapes, and
Mapillary Vistas datasets. And it sets up a new leading benchmark for both
fully- and weakly-supervised panoptic segmentation. Our code and models are
made publicly available at https://github.com/dvlab-research/PanopticFCN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiaojuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yukang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepACC:Automate Chromosome Classification based on Metaphase Images using Deep Learning Framework Fused with Prior Knowledge. (arXiv:2006.15528v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15528</id>
        <link href="http://arxiv.org/abs/2006.15528"/>
        <updated>2021-08-18T01:55:02.227Z</updated>
        <summary type="html"><![CDATA[Chromosome classification is an important but difficult and tedious task in
karyotyping. Previous methods only classify manually segmented single
chromosome, which is far from clinical practice. In this work, we propose a
detection based method, DeepACC, to locate and fine classify chromosomes
simultaneously based on the whole metaphase image. We firstly introduce the
Additive Angular Margin Loss to enhance the discriminative power of model. To
alleviate batch effects, we transform decision boundary of each class
case-by-case through a siamese network which make full use of prior knowledges
that chromosomes usually appear in pairs. Furthermore, we take the clinically
seven group criterion as a prior knowledge and design an additional Group
Inner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase
images from clinical laboratory are collected and labelled to evaluate the
performance. Results show that the new design brings encouraging performance
gains comparing to the state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Chunlong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tianqi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yufan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Manqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fuhai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yinhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1"&gt;Jie Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1"&gt;Li Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACAV100M: Automatic Curation of Large-Scale Datasets for Audio-Visual Video Representation Learning. (arXiv:2101.10803v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10803</id>
        <link href="http://arxiv.org/abs/2101.10803"/>
        <updated>2021-08-18T01:55:02.220Z</updated>
        <summary type="html"><![CDATA[The natural association between visual observations and their corresponding
sound provides powerful self-supervisory signals for learning video
representations, which makes the ever-growing amount of online videos an
attractive source of training data. However, large portions of online videos
contain irrelevant audio-visual signals because of edited/overdubbed audio, and
models trained on such uncurated videos have shown to learn suboptimal
representations. Therefore, existing approaches rely almost exclusively on
datasets with predetermined taxonomies of semantic concepts, where there is a
high chance of audio-visual correspondence. Unfortunately, constructing such
datasets require labor intensive manual annotation and/or verification, which
severely limits the utility of online videos for large-scale learning. In this
work, we present an automatic dataset curation approach based on subset
optimization where the objective is to maximize the mutual information between
audio and visual channels in videos. We demonstrate that our approach finds
videos with high audio-visual correspondence and show that self-supervised
models trained on our data achieve competitive performances compared to models
trained on existing manually curated datasets. The most significant benefit of
our approach is scalability: We release ACAV100M that contains 100 million
videos with high audio-visual correspondence, ideal for self-supervised video
representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1"&gt;Jiwan Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gunhee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1"&gt;Thomas Breuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yale Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TOOD: Task-aligned One-stage Object Detection. (arXiv:2108.07755v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07755</id>
        <link href="http://arxiv.org/abs/2108.07755"/>
        <updated>2021-08-18T01:55:02.212Z</updated>
        <summary type="html"><![CDATA[One-stage object detection is commonly implemented by optimizing two
sub-tasks: object classification and localization, using heads with two
parallel branches, which might lead to a certain level of spatial misalignment
in predictions between the two tasks. In this work, we propose a Task-aligned
One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a
learning-based manner. First, we design a novel Task-aligned Head (T-Head)
which offers a better balance between learning task-interactive and
task-specific features, as well as a greater flexibility to learn the alignment
via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL)
to explicitly pull closer (or even unify) the optimal anchors for the two tasks
during training via a designed sample assignment scheme and a task-aligned
loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a
51.1 AP at single-model single-scale testing. This surpasses the recent
one-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP),
and PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also
demonstrate the effectiveness of TOOD for better aligning the tasks of object
classification and localization. Code is available at
https://github.com/fcjian/TOOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chengjian Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yujie Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_M/0/1/0/all/0/1"&gt;Matthew R. Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weilin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Interaction Learning for Video-based Person Re-identification. (arXiv:2103.09013v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09013</id>
        <link href="http://arxiv.org/abs/2103.09013"/>
        <updated>2021-08-18T01:55:02.193Z</updated>
        <summary type="html"><![CDATA[Video-based person re-identification (re-ID) aims at matching the same person
across video clips. Efficiently exploiting multi-scale fine-grained features
while building the structural interaction among them is pivotal for its
success. In this paper, we propose a hybrid framework, Dense Interaction
Learning (DenseIL), that takes the principal advantages of both CNN-based and
Attention-based architectures to tackle video-based person re-ID difficulties.
DenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN
encoder is responsible for efficiently extracting discriminative spatial
features while the DI decoder is designed to densely model spatial-temporal
inherent interaction across frames. Different from previous works, we
additionally let the DI decoder densely attends to intermediate fine-grained
CNN features and that naturally yields multi-grained spatial-temporal
representation for each video clip. Moreover, we introduce Spatio-TEmporal
Positional Embedding (STEP-Emb) into the DI decoder to investigate the
positional relation among the spatial-temporal inputs. Our experiments
consistently and significantly outperform all the state-of-the-art methods on
multiple standard video-based person re-ID datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tianyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13244</id>
        <link href="http://arxiv.org/abs/2011.13244"/>
        <updated>2021-08-18T01:55:02.186Z</updated>
        <summary type="html"><![CDATA[Multi-view projection methods have demonstrated their ability to reach
state-of-the-art performance on 3D shape recognition. Those methods learn
different ways to aggregate information from multiple views. However, the
camera view-points for those views tend to be heuristically set and fixed for
all shapes. To circumvent the lack of dynamism of current multi-view methods,
we propose to learn those view-points. In particular, we introduce the
Multi-View Transformation Network (MVTN) that regresses optimal view-points for
3D shape recognition, building upon advances in differentiable rendering. As a
result, MVTN can be trained end-to-end along with any multi-view network for 3D
shape classification. We integrate MVTN in a novel adaptive multi-view pipeline
that can render either 3D meshes or point clouds. MVTN exhibits clear
performance gains in the tasks of 3D shape classification and 3D shape
retrieval without the need for extra training supervision. In these tasks, MVTN
achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the
most recent and realistic ScanObjectNN dataset (up to 6% improvement).
Interestingly, we also show that MVTN can provide network robustness against
rotation and occlusion in the 3D domain. The code is available at
https://github.com/ajhamdi/MVTN .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1"&gt;Abdullah Hamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1"&gt;Silvio Giancola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-aware Contrastive Regression for Action Quality Assessment. (arXiv:2108.07797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07797</id>
        <link href="http://arxiv.org/abs/2108.07797"/>
        <updated>2021-08-18T01:55:02.178Z</updated>
        <summary type="html"><![CDATA[Assessing action quality is challenging due to the subtle differences between
videos and large variations in scores. Most existing approaches tackle this
problem by regressing a quality score from a single video, suffering a lot from
the large inter-video score variations. In this paper, we show that the
relations among videos can provide important clues for more accurate action
quality assessment during both training and inference. Specifically, we
reformulate the problem of action quality assessment as regressing the relative
scores with reference to another video that has shared attributes (e.g.,
category and difficulty), instead of learning unreferenced scores. Following
this formulation, we propose a new Contrastive Regression (CoRe) framework to
learn the relative scores by pair-wise comparison, which highlights the
differences between videos and guides the models to learn the key hints for
assessment. In order to further exploit the relative information between two
videos, we devise a group-aware regression tree to convert the conventional
score regression into two easier sub-problems: coarse-to-fine classification
and regression in small intervals. To demonstrate the effectiveness of CoRe, we
conduct extensive experiments on three mainstream AQA datasets including AQA-7,
MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large
margin and establishes new state-of-the-art on all three benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xumin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation. (arXiv:2012.03769v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03769</id>
        <link href="http://arxiv.org/abs/2012.03769"/>
        <updated>2021-08-18T01:55:02.171Z</updated>
        <summary type="html"><![CDATA[Privacy concerns around sharing personally identifiable information are a
major practical barrier to data sharing in medical research. However, in many
cases, researchers have no interest in a particular individual's information
but rather aim to derive insights at the level of cohorts. Here, we utilize
Generative Adversarial Networks (GANs) to create derived medical imaging
datasets consisting entirely of synthetic patient data. The synthetic images
ideally have, in aggregate, similar statistical properties to those of a source
dataset but do not contain sensitive personal information. We assess the
quality of synthetic data generated by two GAN models for chest radiographs
with 14 different radiology findings and brain computed tomography (CT) scans
with six types of intracranial hemorrhages. We measure the synthetic image
quality by the performance difference of predictive models trained on either
the synthetic or the real dataset. We find that synthetic data performance
disproportionately benefits from a reduced number of unique label combinations.
Our open-source benchmark also indicates that at low number of samples per
class, label overfitting effects start to dominate GAN training. We
additionally conducted a reader study in which trained radiologists do not
perform better than random on discriminating between synthetic and real medical
images for intermediate levels of resolutions. In accordance with our benchmark
results, the classification accuracy of radiologists increases at higher
spatial resolution levels. Our study offers valuable guidelines and outlines
practical conditions under which insights derived from synthetic medical images
are similar to those that would have been derived from real imaging data. Our
results indicate that synthetic data sharing may be an attractive and
privacy-preserving alternative to sharing real patient-level data in the right
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schutte_A/0/1/0/all/0/1"&gt;August DuMont Sch&amp;#xfc;tte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hetzel_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Hetzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1"&gt;Tobias Hepp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dietz_B/0/1/0/all/0/1"&gt;Benedikt Dietz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schwab_P/0/1/0/all/0/1"&gt;Patrick Schwab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection. (arXiv:2108.07794v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07794</id>
        <link href="http://arxiv.org/abs/2108.07794"/>
        <updated>2021-08-18T01:55:02.163Z</updated>
        <summary type="html"><![CDATA[3D point cloud understanding has made great progress in recent years.
However, one major bottleneck is the scarcity of annotated real datasets,
especially compared to 2D object detection tasks, since a large amount of labor
is involved in annotating the real scans of a scene. A promising solution to
this problem is to make better use of the synthetic dataset, which consists of
CAD object models, to boost the learning on real datasets. This can be achieved
by the pre-training and fine-tuning procedure. However, recent work on 3D
pre-training exhibits failure when transfer features learned on synthetic
objects to other real-world applications. In this work, we put forward a new
method called RandomRooms to accomplish this objective. In particular, we
propose to generate random layouts of a scene by making use of the objects in
the synthetic CAD dataset and learn the 3D scene representation by applying
object-level contrastive learning on two random scenes generated from the same
set of synthetic objects. The model pre-trained in this way can serve as a
better initialization when later fine-tuning on the 3D object detection task.
Empirically, we show consistent improvement in downstream 3D detection tasks on
several base models, especially when less training data are used, which
strongly demonstrates the effectiveness and generalization of our method.
Benefiting from the rich semantic knowledge and diverse objects from synthetic
data, our method establishes the new state-of-the-art on widely-used 3D
detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide
a new perspective for bridging object and scene-level 3D understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving. (arXiv:2108.07736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07736</id>
        <link href="http://arxiv.org/abs/2108.07736"/>
        <updated>2021-08-18T01:55:02.142Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a system for incrementally reconstructing a dense
3D model of the geometry of an outdoor environment using a single monocular
camera attached to a moving vehicle. Dense models provide a rich representation
of the environment facilitating higher-level scene understanding, perception,
and planning. Our system employs dense depth prediction with a hybrid mapping
architecture combining state-of-the-art sparse features and dense fusion-based
visual SLAM algorithms within an integrated framework. Our novel contributions
include design of hybrid sparse-dense camera tracking and loop closure, and
scale estimation improvements in dense depth prediction. We use the motion
estimates from the sparse method to overcome the large and variable inter-frame
displacement typical of outdoor vehicle scenarios. Our system then registers
the live image with the dense model using whole-image alignment. This enables
the fusion of the live frame and dense depth prediction into the model. Global
consistency and alignment between the sparse and dense models are achieved by
applying pose constraints from the sparse method directly within the
deformation of the dense model. We provide qualitative and quantitative results
for both trajectory estimation and surface reconstruction accuracy,
demonstrating competitive performance on the KITTI dataset. Qualitative results
of the proposed approach are illustrated in https://youtu.be/Pn2uaVqjskY.
Source code for the project is publicly available at the following repository
https://github.com/robotvisionmu/DenseMonoSLAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_L/0/1/0/all/0/1"&gt;Louis Gallagher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Varun Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1"&gt;John B. McDonald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation. (arXiv:2108.07628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07628</id>
        <link href="http://arxiv.org/abs/2108.07628"/>
        <updated>2021-08-18T01:55:02.021Z</updated>
        <summary type="html"><![CDATA[Remarkable results have been achieved by DCNN based self-supervised depth
estimation approaches. However, most of these approaches can only handle either
day-time or night-time images, while their performance degrades for all-day
images due to large domain shift and the variation of illumination between day
and night images. To relieve these limitations, we propose a domain-separated
network for self-supervised depth estimation of all-day images. Specifically,
to relieve the negative influence of disturbing terms (illumination, etc.), we
partition the information of day and night image pairs into two complementary
sub-spaces: private and invariant domains, where the former contains the unique
information (illumination, etc.) of day and night images and the latter
contains essential shared information (texture, etc.). Meanwhile, to guarantee
that the day and night images contain the same information, the
domain-separated network takes the day-time images and corresponding night-time
images (generated by GAN) as input, and the private and invariant feature
extractors are learned by orthogonality and similarity loss, where the domain
gap can be alleviated, thus better depth maps can be expected. Meanwhile, the
reconstruction and photometric losses are utilized to estimate complementary
information and depth maps effectively. Experimental results demonstrate that
our approach achieves state-of-the-art depth estimation results for all-day
images on the challenging Oxford RobotCar dataset, proving the superiority of
our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lina Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xibin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangjun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation. (arXiv:2108.07668v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07668</id>
        <link href="http://arxiv.org/abs/2108.07668"/>
        <updated>2021-08-18T01:55:02.014Z</updated>
        <summary type="html"><![CDATA[Unsupervised disentanglement learning is a crucial issue for understanding
and exploiting deep generative models. Recently, SeFa tries to find latent
disentangled directions by performing SVD on the first projection of a
pre-trained GAN. However, it is only applied to the first layer and works in a
post-processing way. Hessian Penalty minimizes the off-diagonal entries of the
output's Hessian matrix to facilitate disentanglement, and can be applied to
multi-layers.However, it constrains each entry of output independently, making
it not sufficient in disentangling the latent directions (e.g., shape, size,
rotation, etc.) of spatially correlated variations. In this paper, we propose a
simple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative
model to learn disentangled representations. It simply encourages the variation
of output caused by perturbations on different latent dimensions to be
orthogonal, and the Jacobian with respect to the input is calculated to
represent this variation. We show that our OroJaR also encourages the output's
Hessian matrix to be diagonal in an indirect manner. In contrast to the Hessian
Penalty, our OroJaR constrains the output in a holistic way, making it very
effective in disentangling latent dimensions corresponding to spatially
correlated variations. Quantitative and qualitative experimental results show
that our method is effective in disentangled and controllable image generation,
and performs favorably against the state-of-the-art methods. Our code is
available at https://github.com/csyxwei/OroJaR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yuxiang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yupeng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Zhilong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wangmeng Zuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07739</id>
        <link href="http://arxiv.org/abs/2108.07739"/>
        <updated>2021-08-18T01:55:01.920Z</updated>
        <summary type="html"><![CDATA[The study of 3D hyperspectral image (HSI) reconstruction refers to the
inverse process of snapshot compressive imaging, during which the optical
system, e.g., the coded aperture snapshot spectral imaging (CASSI) system,
captures the 3D spatial-spectral signal and encodes it to a 2D measurement.
While numerous sophisticated neural networks have been elaborated for
end-to-end reconstruction, trade-offs still need to be made among performance,
efficiency (training and inference time), and feasibility (the ability of
restoring high resolution HSI on limited GPU memory). This raises a challenge
to design a new baseline to conjointly meet the above requirements. In this
paper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual
U-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)
scale/spectral-invariant learning, 2) nested residual learning, and 3)
computational efficiency. Benefiting from these three modules, the proposed
SSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3
dB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To
the greatest extent, SSI-ResU-Net achieves competing performance with over
77.3% reduction in terms of floating-point operations (FLOPs), which for the
first time, makes high-resolution HSI reconstruction feasible under practical
application scenarios. Code and pre-trained models are made available at
https://github.com/Jiamian-Wang/HSI_baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yulun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhiqiang Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look Who's Talking: Active Speaker Detection in the Wild. (arXiv:2108.07640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07640</id>
        <link href="http://arxiv.org/abs/2108.07640"/>
        <updated>2021-08-18T01:55:01.829Z</updated>
        <summary type="html"><![CDATA[In this work, we present a novel audio-visual dataset for active speaker
detection in the wild. A speaker is considered active when his or her face is
visible and the voice is audible simultaneously. Although active speaker
detection is a crucial pre-processing step for many audio-visual tasks, there
is no existing dataset of natural human speech to evaluate the performance of
active speaker detection. We therefore curate the Active Speakers in the Wild
(ASW) dataset which contains videos and co-occurring speech segments with dense
speech activity labels. Videos and timestamps of audible segments are parsed
and adopted from VoxConverse, an existing speaker diarisation dataset that
consists of videos in the wild. Face tracks are extracted from the videos and
active segments are annotated based on the timestamps of VoxConverse in a
semi-automatic way. Two reference systems, a self-supervised system and a fully
supervised one, are evaluated on the dataset to provide the baseline
performances of ASW. Cross-domain evaluation is conducted in order to show the
negative effect of dubbed videos in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;You Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1"&gt;Hee-Soo Heo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_S/0/1/0/all/0/1"&gt;Soyeon Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soo-Whan Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1"&gt;Yoohwan Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1"&gt;Bong-Jin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1"&gt;Youngki Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1"&gt;Joon Son Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency. (arXiv:2106.02342v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02342</id>
        <link href="http://arxiv.org/abs/2106.02342"/>
        <updated>2021-08-18T01:55:01.729Z</updated>
        <summary type="html"><![CDATA[We study self-supervised video representation learning, which is a
challenging task due to 1) lack of labels for explicit supervision; 2)
unstructured and noisy visual information. Existing methods mainly use
contrastive loss with video clips as the instances and learn visual
representation by discriminating instances from each other, but they need a
careful treatment of negative pairs by either relying on large batch sizes,
memory banks, extra modalities or customized mining strategies, which
inevitably includes noisy data. In this paper, we observe that the consistency
between positive samples is the key to learn robust video representation.
Specifically, we propose two tasks to learn the appearance and speed
consistency, respectively. The appearance consistency task aims to maximize the
similarity between two clips of the same video with different playback speeds.
The speed consistency task aims to maximize the similarity between two clips
with the same playback speed but different appearance information. We show that
optimizing the two tasks jointly consistently improves the performance on
downstream tasks, e.g., action recognition and video retrieval. Remarkably, for
action recognition on the UCF-101 dataset, we achieve 90.8\% accuracy without
using any extra modalities or negative pairs for unsupervised pretraining,
which outperforms the ImageNet supervised pretrained model. Codes and models
will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Deng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhihua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiangmiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Neural Architecture Search with Operation Embeddings. (arXiv:2105.04885v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04885</id>
        <link href="http://arxiv.org/abs/2105.04885"/>
        <updated>2021-08-18T01:55:01.722Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) has recently gained increased attention, as
a class of approaches that automatically searches in an input space of network
architectures. A crucial part of the NAS pipeline is the encoding of the
architecture that consists of the applied computational blocks, namely the
operations and the links between them. Most of the existing approaches either
fail to capture the structural properties of the architectures or use
hand-engineered vector to encode the operator information. In this paper, we
propose the replacement of fixed operator encoding with learnable
representations in the optimization process. This approach, which effectively
captures the relations of different operations, leads to smoother and more
accurate representations of the architectures and consequently to improved
performance of the end task. Our extensive evaluation in ENAS benchmark
demonstrates the effectiveness of the proposed operation embeddings to the
generation of highly accurate models, achieving state-of-the-art performance.
Finally, our method produces top-performing architectures that share similar
operation and graph patterns, highlighting a strong correlation between the
structural properties of the architecture and its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1"&gt;Michail Chatzianastasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1"&gt;George Dasoulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1"&gt;Georgios Siolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1"&gt;Michalis Vazirgiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Evaluation of RGB and LiDAR Fusion for Semantic Segmentation. (arXiv:2108.07661v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07661</id>
        <link href="http://arxiv.org/abs/2108.07661"/>
        <updated>2021-08-18T01:55:01.716Z</updated>
        <summary type="html"><![CDATA[LiDARs and cameras are the two main sensors that are planned to be included
in many announced autonomous vehicles prototypes. Each of the two provides a
unique form of data from a different perspective to the surrounding
environment. In this paper, we explore and attempt to answer the question: is
there an added benefit by fusing those two forms of data for the purpose of
semantic segmentation within the context of autonomous driving? We also attempt
to show at which level does said fusion prove to be the most useful. We
evaluated our algorithms on the publicly available SemanticKITTI dataset. All
fusion models show improvements over the base model, with the mid-level fusion
showing the highest improvement of 2.7% in terms of mean Intersection over
Union (mIoU) metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr S. Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1"&gt;Ali Abdelkader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anany_M/0/1/0/all/0/1"&gt;Mohamed Anany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Behady_O/0/1/0/all/0/1"&gt;Omar El-Behady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faisal_M/0/1/0/all/0/1"&gt;Muhammad Faisal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hangal_A/0/1/0/all/0/1"&gt;Asser Hangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1"&gt;Hesham M. Eraqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1"&gt;Mohamed N. Moustafa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[spectrai: A deep learning framework for spectral data. (arXiv:2108.07595v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07595</id>
        <link href="http://arxiv.org/abs/2108.07595"/>
        <updated>2021-08-18T01:55:01.710Z</updated>
        <summary type="html"><![CDATA[Deep learning computer vision techniques have achieved many successes in
recent years across numerous imaging domains. However, the application of deep
learning to spectral data remains a complex task due to the need for
augmentation routines, specific architectures for spectral data, and
significant memory requirements. Here we present spectrai, an open-source deep
learning framework designed to facilitate the training of neural networks on
spectral data and enable comparison between different methods. Spectrai
provides numerous built-in spectral data pre-processing and augmentation
methods, neural networks for spectral data including spectral (image)
denoising, spectral (image) classification, spectral image segmentation, and
spectral image super-resolution. Spectrai includes both command line and
graphical user interfaces (GUI) designed to guide users through model and
hyperparameter decisions for a wide range of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Horgan_C/0/1/0/all/0/1"&gt;Conor C. Horgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bergholt_M/0/1/0/all/0/1"&gt;Mads S. Bergholt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight. (arXiv:2106.04263v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04263</id>
        <link href="http://arxiv.org/abs/2106.04263"/>
        <updated>2021-08-18T01:55:01.704Z</updated>
        <summary type="html"><![CDATA[Vision Transformer (ViT) attains state-of-the-art performance in visual
recognition, and the variant, Local Vision Transformer, makes further
improvements. The major component in Local Vision Transformer, local attention,
performs the attention separately over small local windows. We rephrase local
attention as a channel-wise locally-connected layer and analyze it from two
network regularization manners, sparse connectivity and weight sharing, as well
as weight computation. Sparse connectivity: there is no connection across
channels, and each position is connected to the positions within a small local
window. Weight sharing: the connection weights for one position are shared
across channels or within each group of channels. Dynamic weight: the
connection weights are dynamically predicted according to each image instance.
We point out that local attention resembles depth-wise convolution and its
dynamic version in sparse connectivity. The main difference lies in weight
sharing - depth-wise convolution shares connection weights (kernel weights)
across spatial positions. We empirically observe that the models based on
depth-wise convolution and the dynamic variant with lower computation
complexity perform on-par with or sometimes slightly better than Swin
Transformer, an instance of Local Vision Transformer, for ImageNet
classification, COCO object detection and ADE semantic segmentation. These
observations suggest that Local Vision Transformer takes advantage of two
regularization forms and dynamic weight to increase the network capacity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zejia Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Classification Equilibrium in Long-Tailed Object Detection. (arXiv:2108.07507v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07507</id>
        <link href="http://arxiv.org/abs/2108.07507"/>
        <updated>2021-08-18T01:55:01.685Z</updated>
        <summary type="html"><![CDATA[The conventional detectors tend to make imbalanced classification and suffer
performance drop, when the distribution of the training data is severely
skewed. In this paper, we propose to use the mean classification score to
indicate the classification accuracy for each category during training. Based
on this indicator, we balance the classification via an Equilibrium Loss (EBL)
and a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL
increases the intensity of the adjustment of the decision boundary for the weak
classes by a designed score-guided loss margin between any two classes. On the
other hand, MFS improves the frequency and accuracy of the adjustment of the
decision boundary for the weak classes through over-sampling the instance
features of those classes. Therefore, EBL and MFS work collaboratively for
finding the classification equilibrium in long-tailed detection, and
dramatically improve the performance of tail classes while maintaining or even
improving the performance of head classes. We conduct experiments on LVIS using
Mask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to
show the superiority of the proposed method. It improves the detection
performance of tail classes by 15.6 AP, and outperforms the most recent
long-tailed object detectors by more than 1 AP. Code is available at
https://github.com/fcjian/LOCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chengjian Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yujie Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weilin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Latent Transformer for Disentangled Face Editing in Images and Videos. (arXiv:2106.11895v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11895</id>
        <link href="http://arxiv.org/abs/2106.11895"/>
        <updated>2021-08-18T01:55:01.678Z</updated>
        <summary type="html"><![CDATA[High quality facial image editing is a challenging problem in the movie
post-production industry, requiring a high degree of control and identity
preservation. Previous works that attempt to tackle this problem may suffer
from the entanglement of facial attributes and the loss of the person's
identity. Furthermore, many algorithms are limited to a certain task. To tackle
these limitations, we propose to edit facial attributes via the latent space of
a StyleGAN generator, by training a dedicated latent transformation network and
incorporating explicit disentanglement and identity preservation terms in the
loss function. We further introduce a pipeline to generalize our face editing
to videos. Our model achieves a disentangled, controllable, and
identity-preserving facial attribute editing, even in the challenging case of
real (i.e., non-synthetic) images and videos. We conduct extensive experiments
on image and video datasets and show that our model outperforms other
state-of-the-art methods in visual quality and quantitative evaluation. Source
codes are available at https://github.com/InterDigitalInc/latent-transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1"&gt;Alasdair Newson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1"&gt;Yann Gousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1"&gt;Pierre Hellier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation. (arXiv:2108.07181v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07181</id>
        <link href="http://arxiv.org/abs/2108.07181"/>
        <updated>2021-08-18T01:55:01.672Z</updated>
        <summary type="html"><![CDATA[Various deep learning techniques have been proposed to solve the single-view
2D-to-3D pose estimation problem. While the average prediction accuracy has
been improved significantly over the years, the performance on hard poses with
depth ambiguity, self-occlusion, and complex or rare poses is still far from
satisfactory. In this work, we target these hard poses and present a novel
skeletal GNN learning solution. To be specific, we propose a hop-aware
hierarchical channel-squeezing fusion layer to effectively extract relevant
information from neighboring nodes while suppressing undesired noises in GNN
learning. In addition, we propose a temporal-aware dynamic graph construction
procedure that is robust and effective for 3D pose estimation. Experimental
results on the Human3.6M dataset show that our solution achieves 10.3\% average
prediction accuracy improvement and greatly improves on hard poses over
state-of-the-art techniques. We further apply the proposed technique on the
skeleton-based action recognition task and also achieve state-of-the-art
performance. Our code is available at
https://github.com/ailingzengzzz/Skeletal-GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1"&gt;Nanxuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PlenOctrees for Real-time Rendering of Neural Radiance Fields. (arXiv:2103.14024v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14024</id>
        <link href="http://arxiv.org/abs/2103.14024"/>
        <updated>2021-08-18T01:55:01.666Z</updated>
        <summary type="html"><![CDATA[We introduce a method to render Neural Radiance Fields (NeRFs) in real time
using PlenOctrees, an octree-based 3D representation which supports
view-dependent effects. Our method can render 800x800 images at more than 150
FPS, which is over 3000 times faster than conventional NeRFs. We do so without
sacrificing quality while preserving the ability of NeRFs to perform
free-viewpoint rendering of scenes with arbitrary geometry and view-dependent
effects. Real-time performance is achieved by pre-tabulating the NeRF into a
PlenOctree. In order to preserve view-dependent effects such as specularities,
we factorize the appearance via closed-form spherical basis functions.
Specifically, we show that it is possible to train NeRFs to predict a spherical
harmonic representation of radiance, removing the viewing direction as an input
to the neural network. Furthermore, we show that PlenOctrees can be directly
optimized to further minimize the reconstruction loss, which leads to equal or
better quality compared to competing methods. Moreover, this octree
optimization step can be used to reduce the training time, as we no longer need
to wait for the NeRF training to converge fully. Our real-time neural rendering
approach may potentially enable new applications such as 6-DOF industrial and
product visualizations, as well as next generation AR/VR systems. PlenOctrees
are amenable to in-browser rendering as well; please visit the project page for
the interactive online demo, as well as video and code:
https://alexyu.net/plenoctrees]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1"&gt;Alex Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1"&gt;Matthew Tancik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1"&gt;Ren Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1"&gt;Angjoo Kanazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Transformer Network. (arXiv:2102.00719v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00719</id>
        <link href="http://arxiv.org/abs/2102.00719"/>
        <updated>2021-08-18T01:55:01.658Z</updated>
        <summary type="html"><![CDATA[This paper presents VTN, a transformer-based framework for video recognition.
Inspired by recent developments in vision transformers, we ditch the standard
approach in video action recognition that relies on 3D ConvNets and introduce a
method that classifies actions by attending to the entire video sequence
information. Our approach is generic and builds on top of any given 2D spatial
network. In terms of wall runtime, it trains $16.1\times$ faster and runs
$5.1\times$ faster during inference while maintaining competitive accuracy
compared to other state-of-the-art methods. It enables whole video analysis,
via a single end-to-end pass, while requiring $1.5\times$ fewer GFLOPs. We
report competitive results on Kinetics-400 and present an ablation study of VTN
properties and the trade-off between accuracy and inference speed. We hope our
approach will serve as a new baseline and start a fresh line of research in the
video recognition domain. Code and models are available at:
https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neimark_D/0/1/0/all/0/1"&gt;Daniel Neimark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bar_O/0/1/0/all/0/1"&gt;Omri Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohar_M/0/1/0/all/0/1"&gt;Maya Zohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asselmann_D/0/1/0/all/0/1"&gt;Dotan Asselmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KCNet: An Insect-Inspired Single-Hidden-Layer Neural Network with Randomized Binary Weights for Prediction and Classification Tasks. (arXiv:2108.07554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07554</id>
        <link href="http://arxiv.org/abs/2108.07554"/>
        <updated>2021-08-18T01:55:01.632Z</updated>
        <summary type="html"><![CDATA[Fruit flies are established model systems for studying olfactory learning as
they will readily learn to associate odors with both electric shock or sugar
rewards. The mechanisms of the insect brain apparently responsible for odor
learning form a relatively shallow neuronal architecture. Olfactory inputs are
received by the antennal lobe (AL) of the brain, which produces an encoding of
each odor mixture across ~50 sub-units known as glomeruli. Each of these
glomeruli then project its component of this feature vector to several of ~2000
so-called Kenyon Cells (KCs) in a region of the brain known as the mushroom
body (MB). Fly responses to odors are generated by small downstream neuropils
that decode the higher-order representation from the MB. Research has shown
that there is no recognizable pattern in the glomeruli--KC connections (and
thus the particular higher-order representations); they are akin to
fingerprints~-- even isogenic flies have different projections. Leveraging
insights from this architecture, we propose KCNet, a single-hidden-layer neural
network that contains sparse, randomized, binary weights between the input
layer and the hidden layer and analytically learned weights between the hidden
layer and the output layer. Furthermore, we also propose a dynamic optimization
algorithm that enables the KCNet to increase performance beyond its structural
limits by searching a more efficient set of inputs. For odorant-perception
tasks that predict perceptual properties of an odorant, we show that KCNet
outperforms existing data-driven approaches, such as XGBoost. For
image-classification tasks, KCNet achieves reasonable performance on benchmark
datasets (MNIST, Fashion-MNIST, and EMNIST) without any data-augmentation
methods or convolutional layers and shows particularly fast running time. Thus,
neural networks inspired by the insect brain can be both economical and perform
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Jinyung Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1"&gt;Theodore P. Pavlic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Semantic Relationships for Unpaired Image Captioning. (arXiv:2106.10658v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10658</id>
        <link href="http://arxiv.org/abs/2106.10658"/>
        <updated>2021-08-18T01:55:01.625Z</updated>
        <summary type="html"><![CDATA[Recently, image captioning has aroused great interest in both academic and
industrial worlds. Most existing systems are built upon large-scale datasets
consisting of image-sentence pairs, which, however, are time-consuming to
construct. In addition, even for the most advanced image captioning systems, it
is still difficult to realize deep image understanding. In this work, we
achieve unpaired image captioning by bridging the vision and the language
domains with high-level semantic information. The motivation stems from the
fact that the semantic concepts with the same modality can be extracted from
both images and descriptions. To further improve the quality of captions
generated by the model, we propose the Semantic Relationship Explorer, which
explores the relationships between semantic concepts for better understanding
of the image. Extensive experiments on MSCOCO dataset show that we can generate
desirable captions without paired datasets. Furthermore, the proposed approach
boosts five strong baselines under the paired setting, where the most
significant improvement in CIDEr score reaches 8%, demonstrating that it is
effective and generalizes well to a wide range of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Meng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Variational Capsule Network for Open Set Recognition. (arXiv:2104.09159v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09159</id>
        <link href="http://arxiv.org/abs/2104.09159"/>
        <updated>2021-08-18T01:55:01.619Z</updated>
        <summary type="html"><![CDATA[In open set recognition, a classifier has to detect unknown classes that are
not known at training time. In order to recognize new categories, the
classifier has to project the input samples of known classes in very compact
and separated regions of the features space for discriminating samples of
unknown classes. Recently proposed Capsule Networks have shown to outperform
alternatives in many fields, particularly in image recognition, however they
have not been fully applied yet to open-set recognition. In capsule networks,
scalar neurons are replaced by capsule vectors or matrices, whose entries
represent different properties of objects. In our proposal, during training,
capsules features of the same known class are encouraged to match a pre-defined
gaussian, one for each class. To this end, we use the variational autoencoder
framework, with a set of gaussian priors as the approximation for the posterior
distribution. In this way, we are able to control the compactness of the
features of the same class around the center of the gaussians, thus controlling
the ability of the classifier in detecting samples from unknown classes. We
conducted several experiments and ablation of our model, obtaining state of the
art results on different datasets in the open set recognition and unknown
detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yunrui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1"&gt;Guglielmo Camporese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenjing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1"&gt;Alessandro Sperduti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1"&gt;Lamberto Ballan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathdreamer: A World Model for Indoor Navigation. (arXiv:2105.08756v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08756</id>
        <link href="http://arxiv.org/abs/2105.08756"/>
        <updated>2021-08-18T01:55:01.612Z</updated>
        <summary type="html"><![CDATA[People navigating in unfamiliar buildings take advantage of myriad visual,
spatial and semantic cues to efficiently achieve their navigation goals.
Towards equipping computational agents with similar capabilities, we introduce
Pathdreamer, a visual world model for agents navigating in novel indoor
environments. Given one or more previous visual observations, Pathdreamer
generates plausible high-resolution 360 visual observations (RGB, semantic
segmentation and depth) for viewpoints that have not been visited, in buildings
not seen during training. In regions of high uncertainty (e.g. predicting
around corners, imagining the contents of an unseen room), Pathdreamer can
predict diverse scenes, allowing an agent to sample multiple realistic outcomes
for a given trajectory. We demonstrate that Pathdreamer encodes useful and
accessible visual, spatial and semantic knowledge about human environments by
using it in the downstream task of Vision-and-Language Navigation (VLN).
Specifically, we show that planning ahead with Pathdreamer brings about half
the benefit of looking ahead at actual observations from unobserved parts of
the environment. We hope that Pathdreamer will help unlock model-based
approaches to challenging embodied navigation tasks such as navigating to
specified objects and VLN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1"&gt;Jing Yu Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1"&gt;Peter Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3D CT Lesions. (arXiv:2108.07662v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07662</id>
        <link href="http://arxiv.org/abs/2108.07662"/>
        <updated>2021-08-18T01:55:01.606Z</updated>
        <summary type="html"><![CDATA[With the renaissance of deep learning, automatic diagnostic systems for
computed tomography (CT) have achieved many successful applications. However,
they are mostly attributed to careful expert annotations, which are often
scarce in practice. This drives our interest to the unsupervised representation
learning. Recent studies have shown that self-supervised learning is an
effective approach for learning representations, but most of them rely on the
empirical design of transformations and pretext tasks. To avoid the
subjectivity associated with these methods, we propose the MVCNet, a novel
unsupervised three dimensional (3D) representation learning method working in a
transformation-free manner. We view each 3D lesion from different orientations
to collect multiple two dimensional (2D) views. Then, an embedding function is
learned by minimizing a contrastive loss so that the 2D views of the same 3D
lesion are aggregated, and the 2D views of different lesions are separated. We
evaluate the representations by training a simple classification head upon the
embedding layer. Experimental results show that MVCNet achieves
state-of-the-art accuracies on the LIDC-IDRI (89.55%), LNDb (77.69%) and
TianChi (79.96%) datasets for unsupervised representation learning. When
fine-tuned on 10% of the labeled data, the accuracies are comparable to the
supervised learning model (89.46% vs. 85.03%, 73.85% vs. 73.44%, 83.56% vs.
83.34% on the three datasets, respectively), indicating the superiority of
MVCNet in learning representations with limited annotations. Code is released
at: https://github.com/penghuazhai/MVCNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1"&gt;Penghua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1"&gt;Huaiwei Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1"&gt;Chaowei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Person Re-Identification with Attribute-guided Metric Distillation. (arXiv:2103.01451v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01451</id>
        <link href="http://arxiv.org/abs/2103.01451"/>
        <updated>2021-08-18T01:55:01.588Z</updated>
        <summary type="html"><![CDATA[Despite the great progress of person re-identification (ReID) with the
adoption of Convolutional Neural Networks, current ReID models are opaque and
only outputs a scalar distance between two persons. There are few methods
providing users semantically understandable explanations for why two persons
are the same one or not. In this paper, we propose a post-hoc method, named
Attribute-guided Metric Distillation (AMD), to explain existing ReID models.
This is the first method to explore attributes to answer: 1) what and where the
attributes make two persons different, and 2) how much each attribute
contributes to the difference. In AMD, we design a pluggable interpreter
network for target models to generate quantitative contributions of attributes
and visualize accurate attention maps of the most discriminative attributes. To
achieve this goal, we propose a metric distillation loss by which the
interpreter learns to decompose the distance of two persons into components of
attributes with knowledge distilled from the target model. Moreover, we propose
an attribute prior loss to make the interpreter generate attribute-guided
attention maps and to eliminate biases caused by the imbalanced distribution of
attributes. This loss can guide the interpreter to focus on the exclusive and
discriminative attributes rather than the large-area but common attributes of
two persons. Comprehensive experiments show that the interpreter can generate
effective and intuitive explanations for varied models and generalize well
under cross-domain settings. As a by-product, the accuracy of target models can
be further improved with our interpreter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaodong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation. (arXiv:2108.07482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07482</id>
        <link href="http://arxiv.org/abs/2108.07482"/>
        <updated>2021-08-18T01:55:01.581Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the knowledge distillation (KD) strategy for
object detection and propose an effective framework applicable to both
homogeneous and heterogeneous student-teacher pairs. The conventional feature
imitation paradigm introduces imitation masks to focus on informative
foreground areas while excluding the background noises. However, we find that
those methods fail to fully utilize the semantic information in all feature
pyramid levels, which leads to inefficiency for knowledge distillation between
FPN-based detectors. To this end, we propose a novel semantic-guided feature
imitation technique, which automatically performs soft matching between feature
pairs across all pyramid levels to provide the optimal guidance to the student.
To push the envelop even further, we introduce contrastive distillation to
effectively capture the information encoded in the relationship between
different feature regions. Finally, we propose a generalized detection KD
pipeline, which is capable of distilling both homogeneous and heterogeneous
detector pairs. Our method consistently outperforms the existing detection KD
techniques, and works when (1) components in the framework are used separately
and in conjunction; (2) for both homogeneous and heterogenous student-teacher
pairs and (3) on multiple detection benchmarks. With a powerful
X101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches
44.0% AP, R50-RetinaNet reaches 43.3% AP and R50-FCOS reaches 43.1% AP on COCO
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1"&gt;Renjie Pi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10598</id>
        <link href="http://arxiv.org/abs/2106.10598"/>
        <updated>2021-08-18T01:55:01.575Z</updated>
        <summary type="html"><![CDATA[A table arranging data in rows and columns is a very effective data
structure, which has been widely used in business and scientific research.
Considering large-scale tabular data in online and offline documents, automatic
table recognition has attracted increasing attention from the document analysis
community. Though human can easily understand the structure of tables, it
remains a challenge for machines to understand that, especially due to a
variety of different table layouts and styles. Existing methods usually model a
table as either the markup sequence or the adjacency matrix between different
table cells, failing to address the importance of the logical location of table
cells, e.g., a cell is located in the first row and the second column of the
table. In this paper, we reformulate the problem of table structure recognition
as the table graph reconstruction, and propose an end-to-end trainable table
graph reconstruction network (TGRNet) for table structure recognition.
Specifically, the proposed method has two main branches, a cell detection
branch and a cell logical location branch, to jointly predict the spatial
location and the logical location of different cells. Experimental results on
three popular table recognition datasets and a new dataset with table graph
annotations (TableGraph-350K) demonstrate the effectiveness of the proposed
TGRNet for table structure recognition. Code and annotations will be made
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wenyuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingyong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dense Siamese U-Net trained with Edge Enhanced 3D IOU Loss for Image Co-segmentation. (arXiv:2108.07491v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07491</id>
        <link href="http://arxiv.org/abs/2108.07491"/>
        <updated>2021-08-18T01:55:01.567Z</updated>
        <summary type="html"><![CDATA[Image co-segmentation has attracted a lot of attentions in computer vision
community. In this paper, we propose a new approach to image co-segmentation
through introducing the dense connections into the decoder path of Siamese
U-net and presenting a new edge enhanced 3D IOU loss measured over distance
maps. Considering the rigorous mapping between the signed normalized distance
map (SNDM) and the binary segmentation mask, we estimate the SNDMs directly
from original images and use them to determine the segmentation results. We
apply the Siamese U-net for solving this problem and improve its effectiveness
by densely connecting each layer with subsequent layers in the decoder path.
Furthermore, a new learning loss is designed to measure the 3D intersection
over union (IOU) between the generated SNDMs and the labeled SNDMs. The
experimental results on commonly used datasets for image co-segmentation
demonstrate the effectiveness of our presented dense structure and edge
enhanced 3D IOU loss of SNDM. To our best knowledge, they lead to the
state-of-the-art performance on the Internet and iCoseg datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiabi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xiaopeng Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DR{\AE}M -- A discriminatively trained reconstruction embedding for surface anomaly detection. (arXiv:2108.07610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07610</id>
        <link href="http://arxiv.org/abs/2108.07610"/>
        <updated>2021-08-18T01:55:01.561Z</updated>
        <summary type="html"><![CDATA[Visual surface anomaly detection aims to detect local image regions that
significantly deviate from normal appearance. Recent surface anomaly detection
methods rely on generative models to accurately reconstruct the normal areas
and to fail on anomalies. These methods are trained only on anomaly-free
images, and often require hand-crafted post-processing steps to localize the
anomalies, which prohibits optimizing the feature extraction for maximal
detection capability. In addition to reconstructive approach, we cast surface
anomaly detection primarily as a discriminative problem and propose a
discriminatively trained reconstruction anomaly embedding model (DRAEM). The
proposed method learns a joint representation of an anomalous image and its
anomaly-free reconstruction, while simultaneously learning a decision boundary
between normal and anomalous examples. The method enables direct anomaly
localization without the need for additional complicated post-processing of the
network output and can be trained using simple and general anomaly simulations.
On the challenging MVTec anomaly detection dataset, DRAEM outperforms the
current state-of-the-art unsupervised methods by a large margin and even
delivers detection performance close to the fully-supervised methods on the
widely used DAGM surface-defect detection dataset, while substantially
outperforming them in localization accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1"&gt;Vitjan Zavrtanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1"&gt;Matej Kristan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skocaj_D/0/1/0/all/0/1"&gt;Danijel Sko&amp;#x10d;aj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Image Super-Resolution with Transformers. (arXiv:2108.07597v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07597</id>
        <link href="http://arxiv.org/abs/2108.07597"/>
        <updated>2021-08-18T01:55:01.554Z</updated>
        <summary type="html"><![CDATA[Light field (LF) image super-resolution (SR) aims at reconstructing
high-resolution LF images from their low-resolution counterparts. Although
CNN-based methods have achieved remarkable performance in LF image SR, these
methods cannot fully model the non-local properties of the 4D LF data. In this
paper, we propose a simple but effective Transformer-based method for LF image
SR. In our method, an angular Transformer is designed to incorporate
complementary information among different views, and a spatial Transformer is
developed to capture both local and long-range dependencies within each
sub-aperture image. With the proposed angular and spatial Transformers, the
beneficial information in an LF can be fully exploited and the SR performance
is boosted. We validate the effectiveness of our angular and spatial
Transformers through extensive ablation studies, and compare our method to
recent state-of-the-art methods on five public LF datasets. Our method achieves
superior SR performance with a small model size and low computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhengyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jungang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shilin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness under Long-Tailed Distribution. (arXiv:2104.02703v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02703</id>
        <link href="http://arxiv.org/abs/2104.02703"/>
        <updated>2021-08-18T01:55:01.536Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness has attracted extensive studies recently by revealing
the vulnerability and intrinsic characteristics of deep networks. However,
existing works on adversarial robustness mainly focus on balanced datasets,
while real-world data usually exhibits a long-tailed distribution. To push
adversarial robustness towards more realistic scenarios, in this work we
investigate the adversarial vulnerability as well as defense under long-tailed
distributions. In particular, we first reveal the negative impacts induced by
imbalanced data on both recognition performance and adversarial robustness,
uncovering the intrinsic challenges of this problem. We then perform a
systematic study on existing long-tailed recognition methods in conjunction
with the adversarial training framework. Several valuable observations are
obtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of
robust accuracy exists under unreliable evaluation, and 3) boundary error
limits the promotion of robustness. Inspired by these observations, we propose
a clean yet effective framework, RoBal, which consists of two dedicated
modules, a scale-invariant classifier and data re-balancing via both margin
engineering at training stage and boundary adjustment during inference.
Extensive experiments demonstrate the superiority of our approach over other
state-of-the-art defense methods. To our best knowledge, we are the first to
tackle adversarial robustness under long-tailed distributions, which we believe
would be a significant step towards real-world robustness. Our code is
available at: https://github.com/wutong16/Adversarial_Long-Tail .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingqiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaccadeCam: Adaptive Visual Attention for Monocular Depth Sensing. (arXiv:2103.12981v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12981</id>
        <link href="http://arxiv.org/abs/2103.12981"/>
        <updated>2021-08-18T01:55:01.529Z</updated>
        <summary type="html"><![CDATA[Most monocular depth sensing methods use conventionally captured images that
are created without considering scene content. In contrast, animal eyes have
fast mechanical motions, called saccades, that control how the scene is imaged
by the fovea, where resolution is highest. In this paper, we present the
SaccadeCam framework for adaptively distributing resolution onto regions of
interest in the scene. Our algorithm for adaptive resolution is a
self-supervised network and we demonstrate results for end-to-end learning for
monocular depth estimation. We also show preliminary results with a real
SaccadeCam hardware prototype.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tilmon_B/0/1/0/all/0/1"&gt;Brevin Tilmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppal_S/0/1/0/all/0/1"&gt;Sanjeev J. Koppal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Experimental Urban Case Study with Various Data Sources and a Model for Traffic Estimation. (arXiv:2108.07698v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07698</id>
        <link href="http://arxiv.org/abs/2108.07698"/>
        <updated>2021-08-18T01:55:01.523Z</updated>
        <summary type="html"><![CDATA[Accurate estimation of the traffic state over a network is essential since it
is the starting point for designing and implementing any traffic management
strategy. Hence, traffic operators and users of a transportation network can
make reliable decisions such as influence/change route or mode choice. However,
the problem of traffic state estimation from various sensors within an urban
environment is very complex for several different reasons, such as availability
of sensors, different noise levels, different output quantities, sensor
accuracy, heterogeneous data fusion, and many more. To provide a better
understanding of this problem, we organized an experimental campaign with video
measurement in an area within the urban network of Zurich, Switzerland. We
focus on capturing the traffic state in terms of traffic flow and travel times
by ensuring measurements from established thermal cameras by the city's
authorities, processed video data, and the Google Distance Matrix. We assess
the different data sources, and we propose a simple yet efficient Multiple
Linear Regression (MLR) model to estimate travel times with fusion of various
data sources. Comparative results with ground-truth data (derived from video
measurements) show the efficiency and robustness of the proposed methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Genser_A/0/1/0/all/0/1"&gt;Alexander Genser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hautle_N/0/1/0/all/0/1"&gt;Noel Hautle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makridis_M/0/1/0/all/0/1"&gt;Michail Makridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouvelas_A/0/1/0/all/0/1"&gt;Anastasios Kouvelas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COTR: Correspondence Transformer for Matching Across Images. (arXiv:2103.14167v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14167</id>
        <link href="http://arxiv.org/abs/2103.14167"/>
        <updated>2021-08-18T01:55:01.516Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for finding correspondences in images based on a
deep neural network that, given two images and a query point in one of them,
finds its correspondence in the other. By doing so, one has the option to query
only the points of interest and retrieve sparse correspondences, or to query
all points in an image and obtain dense mappings. Importantly, in order to
capture both local and global priors, and to let our model relate between image
regions using the most relevant among said priors, we realize our network using
a transformer. At inference time, we apply our correspondence network by
recursively zooming in around the estimates, yielding a multiscale pipeline
able to provide highly-accurate correspondences. Our method significantly
outperforms the state of the art on both sparse and dense correspondence
problems on multiple datasets and tasks, ranging from wide-baseline stereo to
optical flow, without any retraining for a specific dataset. We commit to
releasing data, code, and all the tools necessary to train from scratch and
ensure reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trulls_E/0/1/0/all/0/1"&gt;Eduard Trulls&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1"&gt;Jan Hosang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1"&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1"&gt;Kwang Moo Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indoor Semantic Scene Understanding using Multi-modality Fusion. (arXiv:2108.07616v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07616</id>
        <link href="http://arxiv.org/abs/2108.07616"/>
        <updated>2021-08-18T01:55:01.493Z</updated>
        <summary type="html"><![CDATA[Seamless Human-Robot Interaction is the ultimate goal of developing service
robotic systems. For this, the robotic agents have to understand their
surroundings to better complete a given task. Semantic scene understanding
allows a robotic agent to extract semantic knowledge about the objects in the
environment. In this work, we present a semantic scene understanding pipeline
that fuses 2D and 3D detection branches to generate a semantic map of the
environment. The 2D mask proposals from state-of-the-art 2D detectors are
inverse-projected to the 3D space and combined with 3D detections from point
segmentation networks. Unlike previous works that were evaluated on collected
datasets, we test our pipeline on an active photo-realistic robotic environment
- BenchBot. Our novelty includes rectification of 3D proposals using projected
2D detections and modality fusion based on object size. This work is done as
part of the Robotic Vision Scene Understanding Challenge (RVSU). The
performance evaluation demonstrates that our pipeline has improved on baseline
methods without significant computational bottleneck.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopinathan_M/0/1/0/all/0/1"&gt;Muraleekrishna Gopinathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_G/0/1/0/all/0/1"&gt;Giang Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abu_Khalaf_J/0/1/0/all/0/1"&gt;Jumana Abu-Khalaf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct domain adaptation through reciprocal linear transformations. (arXiv:2108.07600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07600</id>
        <link href="http://arxiv.org/abs/2108.07600"/>
        <updated>2021-08-18T01:55:01.485Z</updated>
        <summary type="html"><![CDATA[We propose a direct domain adaptation (DDA) approach to enrich the training
of supervised neural networks on synthetic data by features from real-world
data. The process involves a series of linear operations on the input features
to the NN model, whether they are from the source or target domains, as
follows: 1) A cross-correlation of the input data (i.e. images) with a randomly
picked sample pixel (or pixels) of all images from that domain or the mean of
all randomly picked sample pixel (or pixels) of all images. 2) The convolution
of the resulting data with the mean of the autocorrelated input images from the
other domain. In the training stage, as expected, the input images are from the
source domain, and the mean of auto-correlated images are evaluated from the
target domain. In the inference/application stage, the input images are from
the target domain, and the mean of auto-correlated images are evaluated from
the source domain. The proposed method only manipulates the data from the
source and target domains and does not explicitly interfere with the training
workflow and network architecture. An application that includes training a
convolutional neural network on the MNIST dataset and testing the network on
the MNIST-M dataset achieves a 70% accuracy on the test data. A principal
component analysis (PCA), as well as t-SNE, show that the input features from
the source and target domains, after the proposed direct transformations, share
similar properties along with the principal components as compared to the
original MNIST and MNIST-M input features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alkhalifah_T/0/1/0/all/0/1"&gt;Tariq Alkhalifah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ovcharenko_O/0/1/0/all/0/1"&gt;Oleg Ovcharenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CaT: Weakly Supervised Object Detection with Category Transfer. (arXiv:2108.07487v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07487</id>
        <link href="http://arxiv.org/abs/2108.07487"/>
        <updated>2021-08-18T01:55:01.479Z</updated>
        <summary type="html"><![CDATA[A large gap exists between fully-supervised object detection and
weakly-supervised object detection. To narrow this gap, some methods consider
knowledge transfer from additional fully-supervised dataset. But these methods
do not fully exploit discriminative category information in the
fully-supervised dataset, thus causing low mAP. To solve this issue, we propose
a novel category transfer framework for weakly supervised object detection. The
intuition is to fully leverage both visually-discriminative and
semantically-correlated category information in the fully-supervised dataset to
enhance the object-classification ability of a weakly-supervised detector. To
handle overlapping category transfer, we propose a double-supervision mean
teacher to gather common category information and bridge the domain gap between
two datasets. To handle non-overlapping category transfer, we propose a
semantic graph convolutional network to promote the aggregation of semantic
features between correlated categories. Experiments are conducted with Pascal
VOC 2007 as the target weakly-supervised dataset and COCO as the source
fully-supervised dataset. Our category transfer framework achieves 63.5% mAP
and 80.3% CorLoc with 5 overlapping categories between two datasets, which
outperforms the state-of-the-art methods. Codes are avaliable at
https://github.com/MediaBrain-SJTU/CaT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lianyu Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan-Feng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating transformers in the decomposition of polygonal shapes as point collections. (arXiv:2108.07533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07533</id>
        <link href="http://arxiv.org/abs/2108.07533"/>
        <updated>2021-08-18T01:55:01.472Z</updated>
        <summary type="html"><![CDATA[Transformers can generate predictions in two approaches: 1. auto-regressively
by conditioning each sequence element on the previous ones, or 2. directly
produce an output sequences in parallel. While research has mostly explored
upon this difference on sequential tasks in NLP, we study the difference
between auto-regressive and parallel prediction on visual set prediction tasks,
and in particular on polygonal shapes in images because polygons are
representative of numerous types of objects, such as buildings or obstacles for
aerial vehicles. This is challenging for deep learning architectures as a
polygon can consist of a varying carnality of points. We provide evidence on
the importance of natural orders for Transformers, and show the benefit of
decomposing complex polygons into collections of points in an auto-regressive
manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alfieri_A/0/1/0/all/0/1"&gt;Andrea Alfieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yancong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep MRI Reconstruction with Radial Subsampling. (arXiv:2108.07619v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.07619</id>
        <link href="http://arxiv.org/abs/2108.07619"/>
        <updated>2021-08-18T01:55:01.465Z</updated>
        <summary type="html"><![CDATA[In spite of its extensive adaptation in almost every medical diagnostic and
examinatorial application, Magnetic Resonance Imaging (MRI) is still a slow
imaging modality which limits its use for dynamic imaging. In recent years,
Parallel Imaging (PI) and Compressed Sensing (CS) have been utilised to
accelerate the MRI acquisition. In clinical settings, subsampling the k-space
measurements during scanning time using Cartesian trajectories, such as
rectilinear sampling, is currently the most conventional CS approach applied
which, however, is prone to producing aliased reconstructions. With the advent
of the involvement of Deep Learning (DL) in accelerating the MRI,
reconstructing faithful images from subsampled data became increasingly
promising. Retrospectively applying a subsampling mask onto the k-space data is
a way of simulating the accelerated acquisition of k-space data in real
clinical setting. In this paper we compare and provide a review for the effect
of applying either rectilinear or radial retrospective subsampling on the
quality of the reconstructions outputted by trained deep neural networks. With
the same choice of hyper-parameters, we train and evaluate two distinct
Recurrent Inference Machines (RIMs), one for each type of subsampling. The
qualitative and quantitative results of our experiments indicate that the model
trained on data with radial subsampling attains higher performance and learns
to estimate reconstructions with higher fidelity paving the way for other DL
approaches to involve radial subsampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yiasemis_G/0/1/0/all/0/1"&gt;George Yiasemis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1"&gt;Clara I. S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sonke_J/0/1/0/all/0/1"&gt;Jan-Jakob Sonke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1"&gt;Jonas Teuwen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Flexible Three-Dimensional Hetero-phase Computed Tomography Hepatocellular Carcinoma (HCC) Detection Algorithm for Generalizable and Practical HCC Screening. (arXiv:2108.07492v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07492</id>
        <link href="http://arxiv.org/abs/2108.07492"/>
        <updated>2021-08-18T01:55:01.459Z</updated>
        <summary type="html"><![CDATA[Hepatocellular carcinoma (HCC) can be potentially discovered from abdominal
computed tomography (CT) studies under varied clinical scenarios, e.g., fully
dynamic contrast enhanced (DCE) studies, non-contrast (NC) plus venous phase
(VP) abdominal studies, or NC-only studies. We develop a flexible
three-dimensional deep algorithm, called hetero-phase volumetric detection
(HPVD), that can accept any combination of contrast-phase inputs and with
adjustable sensitivity depending on the clinical purpose. We trained HPVD on
771 DCE CT scans to detect HCCs and tested on external 164 positives and 206
controls, respectively. We compare performance against six clinical readers,
including two radiologists, two hepato-pancreatico-biliary (HPB) surgeons, and
two hepatologists. The area under curve (AUC) of the localization receiver
operating characteristic (LROC) for NC-only, NC plus VP, and full DCE CT
yielded 0.71, 0.81, 0.89 respectively. At a high sensitivity operating point of
80% on DCE CT, HPVD achieved 97% specificity, which is comparable to measured
physician performance. We also demonstrate performance improvements over more
typical and less flexible non hetero-phase detectors. Thus, we demonstrate that
a single deep learning algorithm can be effectively applied to diverse HCC
detection clinical scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chi-Tung Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jinzheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_W/0/1/0/all/0/1"&gt;Wei Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Youjing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;YuTing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Chao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chien-Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Youbao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;Wei-Chen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_T/0/1/0/all/0/1"&gt;Ta-Sen Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Le Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1"&gt;Chien-Hung Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1"&gt;Adam P. Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Colorization Using Mono-Color Image Pairs. (arXiv:2108.07471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07471</id>
        <link href="http://arxiv.org/abs/2108.07471"/>
        <updated>2021-08-18T01:55:01.452Z</updated>
        <summary type="html"><![CDATA[Compared to color images captured by conventional RGB cameras, monochrome
images usually have better signal-to-noise ratio (SNR) and richer textures due
to its higher quantum efficiency. It is thus natural to apply a mono-color
dual-camera system to restore color images with higher visual quality. In this
paper, we propose a mono-color image enhancement algorithm that colorizes the
monochrome image with the color one. Based on the assumption that adjacent
structures with similar luminance values are likely to have similar colors, we
first perform dense scribbling to assign colors to the monochrome pixels
through block matching. Two types of outliers, including occlusion and color
ambiguity, are detected and removed from the initial scribbles. We also
introduce a sampling strategy to accelerate the scribbling process. Then, the
dense scribbles are propagated to the entire image. To alleviate incorrect
color propagation in the regions that have no color hints at all, we generate
extra color seeds based on the existed scribbles to guide the propagation
process. Experimental results show that, our algorithm can efficiently restore
color images with higher SNR and richer details from the mono-color image
pairs, and achieves good performance in solving the color bleeding problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1"&gt;Ze-Hua Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Hui-Liang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1"&gt;Bo-Wen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huaqi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks. (arXiv:2108.07478v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07478</id>
        <link href="http://arxiv.org/abs/2108.07478"/>
        <updated>2021-08-18T01:55:01.446Z</updated>
        <summary type="html"><![CDATA[Instance segmentation in 3D scenes is fundamental in many applications of
scene understanding. It is yet challenging due to the compound factors of data
irregularity and uncertainty in the numbers of instances. State-of-the-art
methods largely rely on a general pipeline that first learns point-wise
features discriminative at semantic and instance levels, followed by a separate
step of point grouping for proposing object instances. While promising, they
have the shortcomings that (1) the second step is not supervised by the main
objective of instance segmentation, and (2) their point-wise feature learning
and grouping are less effective to deal with data irregularities, possibly
resulting in fragmented segmentations. To address these issues, we propose in
this work an end-to-end solution of Semantic Superpoint Tree Network (SSTNet)
for proposing object instances from scene points. Key in SSTNet is an
intermediate, semantic superpoint tree (SST), which is constructed based on the
learned semantic features of superpoints, and which will be traversed and split
at intermediate tree nodes for proposals of object instances. We also design in
SSTNet a refinement module, termed CliqueNet, to prune superpoints that may be
wrongly grouped into instance proposals. Experiments on the benchmarks of
ScanNet and S3DIS show the efficacy of our proposed method. At the time of
submission, SSTNet ranks top on the ScanNet (V2) leaderboard, with 2% higher of
mAP than the second best method. The source code in PyTorch is available at
https://github.com/Gorilla-Lab-SCUT/SSTNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhihao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhihao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Songcen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MV-TON: Memory-based Video Virtual Try-on network. (arXiv:2108.07502v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07502</id>
        <link href="http://arxiv.org/abs/2108.07502"/>
        <updated>2021-08-18T01:55:01.431Z</updated>
        <summary type="html"><![CDATA[With the development of Generative Adversarial Network, image-based virtual
try-on methods have made great progress. However, limited work has explored the
task of video-based virtual try-on while it is important in real-world
applications. Most existing video-based virtual try-on methods usually require
clothing templates and they can only generate blurred and low-resolution
results. To address these challenges, we propose a Memory-based Video virtual
Try-On Network (MV-TON), which seamlessly transfers desired clothes to a target
person without using any clothing templates and generates high-resolution
realistic videos. Specifically, MV-TON consists of two modules: 1) a try-on
module that transfers the desired clothes from model images to frame images by
pose alignment and region-wise replacing of pixels; 2) a memory refinement
module that learns to embed the existing generated frames into the latent space
as external memory for the following frame generation. Experimental results
show the effectiveness of our method in the video virtual try-on task and its
superiority over other existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xiaojing Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Taizhe Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyao Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images. (arXiv:2108.07582v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07582</id>
        <link href="http://arxiv.org/abs/2108.07582"/>
        <updated>2021-08-18T01:55:01.418Z</updated>
        <summary type="html"><![CDATA[Automated animal censuses with aerial imagery are a vital ingredient towards
wildlife conservation. Recent models are generally based on deep learning and
thus require vast amounts of training data. Due to their scarcity and minuscule
size, annotating animals in aerial imagery is a highly tedious process. In this
project, we present a methodology to reduce the amount of required training
data by resorting to self-supervised pretraining. In detail, we examine a
combination of recent contrastive learning methodologies like Momentum Contrast
(MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our
model on the aerial images without the requirement for labels. We show that a
combination of MoCo, CLD, and geometric augmentations outperforms conventional
models pre-trained on ImageNet by a large margin. Crucially, our method still
yields favorable results even if we reduce the number of training animals to
just 10%, at which point our best model scores double the recall of the
baseline at similar precision. This effectively allows reducing the number of
required annotations to a fraction while still being able to train
high-accuracy models in such highly challenging settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaochen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1"&gt;Benjamin Kellenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Rui Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajnsek_I/0/1/0/all/0/1"&gt;Irena Hajnsek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1"&gt;Devis Tuia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion. (arXiv:2108.07506v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07506</id>
        <link href="http://arxiv.org/abs/2108.07506"/>
        <updated>2021-08-18T01:55:01.399Z</updated>
        <summary type="html"><![CDATA[We propose PR-RRN, a novel neural-network based method for Non-rigid
Structure-from-Motion (NRSfM). PR-RRN consists of Residual-Recursive Networks
(RRN) and two extra regularization losses. RRN is designed to effectively
recover 3D shape and camera from 2D keypoints with novel residual-recursive
structure. As NRSfM is a highly under-constrained problem, we propose two new
pairwise regularization to further regularize the reconstruction. The
Rigidity-based Pairwise Contrastive Loss regularizes the shape representation
by encouraging higher similarity between the representations of high-rigidity
pairs of frames than low-rigidity pairs. We propose minimum singular-value
ratio to measure the pairwise rigidity. The Pairwise Consistency Loss enforces
the reconstruction to be consistent when the estimated shapes and cameras are
exchanged between pairs. Our approach achieves state-of-the-art performance on
CMU MOCAP and PASCAL3D+ dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Haitian Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuchao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer. (arXiv:2108.07520v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07520</id>
        <link href="http://arxiv.org/abs/2108.07520"/>
        <updated>2021-08-18T01:55:01.384Z</updated>
        <summary type="html"><![CDATA[With the strength of deep generative models, 3D pose transfer regains
intensive research interests in recent years. Existing methods mainly rely on a
variety of constraints to achieve the pose transfer over 3D meshes, e.g., the
need for the manually encoding for shape and pose disentanglement. In this
paper, we present an unsupervised approach to conduct the pose transfer between
any arbitrate given 3D meshes. Specifically, a novel Intrinsic-Extrinsic
Preserved Generative Adversarial Network (IEP-GAN) is presented for both
intrinsic (i.e., shape) and extrinsic (i.e., pose) information preservation.
Extrinsically, we propose a co-occurrence discriminator to capture the
structural/pose invariance from distinct Laplacians of the mesh. Meanwhile,
intrinsically, a local intrinsic-preserved loss is introduced to preserve the
geodesic priors while avoiding the heavy computations. At last, we show the
possibility of using IEP-GAN to manipulate 3D human meshes in various ways,
including pose transfer, identity swapping and pose interpolation with latent
code vector arithmetic. The extensive experiments on various 3D datasets of
humans, animals and hands qualitatively and quantitatively demonstrate the
generality of our approach. Our proposed model produces better results and is
substantially more efficient compared to recent state-of-the-art methods. Code
is available: https://github.com/mikecheninoulu/Unsupervised_IEPGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Henglin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guoying Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Photofit: Gaze-based Mental Image Reconstruction. (arXiv:2108.07524v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07524</id>
        <link href="http://arxiv.org/abs/2108.07524"/>
        <updated>2021-08-18T01:55:01.365Z</updated>
        <summary type="html"><![CDATA[We propose a novel method that leverages human fixations to visually decode
the image a person has in mind into a photofit (facial composite). Our method
combines three neural networks: An encoder, a scoring network, and a decoder.
The encoder extracts image features and predicts a neural activation map for
each face looked at by a human observer. A neural scoring network compares the
human and neural attention and predicts a relevance score for each extracted
image feature. Finally, image features are aggregated into a single feature
vector as a linear combination of all features weighted by relevance which a
decoder decodes into the final photofit. We train the neural scoring network on
a novel dataset containing gaze data of 19 participants looking at collages of
synthetic faces. We show that our method significantly outperforms a mean
baseline predictor and report on a human study that shows that we can decode
photofits that are visually plausible and close to the observer's mental image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Strohm_F/0/1/0/all/0/1"&gt;Florian Strohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sood_E/0/1/0/all/0/1"&gt;Ekta Sood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_S/0/1/0/all/0/1"&gt;Sven Mayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1"&gt;Philipp M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bace_M/0/1/0/all/0/1"&gt;Mihai B&amp;#xe2;ce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulling_A/0/1/0/all/0/1"&gt;Andreas Bulling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2108.07511v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07511</id>
        <link href="http://arxiv.org/abs/2108.07511"/>
        <updated>2021-08-18T01:55:01.328Z</updated>
        <summary type="html"><![CDATA[Camera and 3D LiDAR sensors have become indispensable devices in modern
autonomous driving vehicles, where the camera provides the fine-grained
texture, color information in 2D space and LiDAR captures more precise and
farther-away distance measurements of the surrounding environments. The
complementary information from these two sensors makes the two-modality fusion
be a desired option. However, two major issues of the fusion between camera and
LiDAR hinder its performance, \ie, how to effectively fuse these two modalities
and how to precisely align them (suffering from the weak spatiotemporal
synchronization problem). In this paper, we propose a coarse-to-fine LiDAR and
camera fusion-based network (termed as LIF-Seg) for LiDAR segmentation. For the
first issue, unlike these previous works fusing the point cloud and image
information in a one-to-one manner, the proposed method fully utilizes the
contextual information of images and introduces a simple but effective
early-fusion strategy. Second, due to the weak spatiotemporal synchronization
problem, an offset rectification approach is designed to align these
two-modality features. The cooperation of these two components leads to the
success of the effective camera-LiDAR fusion. Experimental results on the
nuScenes dataset show the superiority of the proposed LIF-Seg over existing
methods with a large margin. Ablation studies and analyses demonstrate that our
proposed LIF-Seg can effectively tackle the weak spatiotemporal synchronization
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinge Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1"&gt;Wenbing Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferring Knowledge with Attention Distillation for Multi-Domain Image-to-Image Translation. (arXiv:2108.07466v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07466</id>
        <link href="http://arxiv.org/abs/2108.07466"/>
        <updated>2021-08-18T01:55:01.272Z</updated>
        <summary type="html"><![CDATA[Gradient-based attention modeling has been used widely as a way to visualize
and understand convolutional neural networks. However, exploiting these visual
explanations during the training of generative adversarial networks (GANs) is
an unexplored area in computer vision research. Indeed, we argue that this kind
of information can be used to influence GANs training in a positive way. For
this reason, in this paper, it is shown how gradient based attentions can be
used as knowledge to be conveyed in a teacher-student paradigm for multi-domain
image-to-image translation tasks in order to improve the results of the student
architecture. Further, it is demonstrated how "pseudo"-attentions can also be
employed during training when teacher and student networks are trained on
different domains which share some similarities. The approach is validated on
multi-domain facial attributes transfer and human expression synthesis showing
both qualitative and quantitative results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Runze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontanini_T/0/1/0/all/0/1"&gt;Tomaso Fontanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donati_L/0/1/0/all/0/1"&gt;Luca Donati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1"&gt;Andrea Prati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1"&gt;Bir Bhanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation. (arXiv:2108.07413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07413</id>
        <link href="http://arxiv.org/abs/2108.07413"/>
        <updated>2021-08-18T01:55:01.259Z</updated>
        <summary type="html"><![CDATA[Weakly supervised image segmentation trained with image-level labels usually
suffers from inaccurate coverage of object areas during the generation of the
pseudo groundtruth. This is because the object activation maps are trained with
the classification objective and lack the ability to generalize. To improve the
generality of the objective activation maps, we propose a region prototypical
network RPNet to explore the cross-image object diversity of the training set.
Similar object parts across images are identified via region feature
comparison. Object confidence is propagated between regions to discover new
object areas while background regions are suppressed. Experiments show that the
proposed method generates more complete and accurate pseudo object masks, while
achieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In
addition, we investigate the robustness of the proposed method on reduced
training sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weide Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiangfei Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1"&gt;Tzu-Yi Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffeomorphic Particle Image Velocimetry. (arXiv:2108.07438v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07438</id>
        <link href="http://arxiv.org/abs/2108.07438"/>
        <updated>2021-08-18T01:55:01.241Z</updated>
        <summary type="html"><![CDATA[The existing particle image velocimetry (PIV) do not consider the curvature
effect of the non-straight particle trajectory, because it seems to be
impossible to obtain the curvature information from a pair of particle images.
As a result, the computed vector underestimates the real velocity due to the
straight-line approximation, that further causes a systematic error for the PIV
instrument. In this work, the particle curved trajectory between two recordings
is firstly explained with the streamline segment of a steady flow
(diffeomorphic transformation) instead of a single vector, and this idea is
termed as diffeomorphic PIV. Specifically, a deformation field is introduced to
describe the particle displacement, i.e., we try to find the optimal velocity
field, of which the corresponding deformation vector field agrees with the
particle displacement. Because the variation of the deformation function can be
approximated with the variation of the velocity function, the diffeomorphic PIV
can be implemented as iterative PIV. That says, the diffeomorphic PIV warps the
images with deformation vector field instead of the velocity, and keeps the
rest as same as iterative PIVs. Two diffeomorphic deformation schemes --
forward diffeomorphic deformation interrogation (FDDI) and central
diffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on
synthetic images, the FDDI achieves significant accuracy improvement across
different one-pass displacement estimators (cross-correlation, optical flow,
deep learning flow). Besides, the results on three real PIV image pairs
demonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI
provides larger velocity estimation (more accurate) in the fast curvy
streamline areas. The accuracy improvement of the combination of FDDI and
accurate dense estimator means that our diffeomorphic PIV paves a new way for
complex flow measurement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1"&gt;Shuang Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences. (arXiv:2108.07422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07422</id>
        <link href="http://arxiv.org/abs/2108.07422"/>
        <updated>2021-08-18T01:55:01.234Z</updated>
        <summary type="html"><![CDATA[We address the problem of visible-infrared person re-identification
(VI-reID), that is, retrieving a set of person images, captured by visible or
infrared cameras, in a cross-modal setting. Two main challenges in VI-reID are
intra-class variations across person images, and cross-modal discrepancies
between visible and infrared images. Assuming that the person images are
roughly aligned, previous approaches attempt to learn coarse image- or rigid
part-level person representations that are discriminative and generalizable
across different modalities. However, the person images, typically cropped by
off-the-shelf object detectors, are not necessarily well-aligned, which
distract discriminative person representation learning. In this paper, we
introduce a novel feature learning framework that addresses these problems in a
unified way. To this end, we propose to exploit dense correspondences between
cross-modal person images. This allows to address the cross-modal discrepancies
in a pixel-level, suppressing modality-related features from person
representations more effectively. This also encourages pixel-wise associations
between cross-modal local features, further facilitating discriminative feature
learning for VI-reID. Extensive experiments and analyses on standard VI-reID
benchmarks demonstrate the effectiveness of our approach, which significantly
outperforms the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunjong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sanghoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junghyup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1"&gt;Bumsub Ham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification. (arXiv:2108.07464v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07464</id>
        <link href="http://arxiv.org/abs/2108.07464"/>
        <updated>2021-08-18T01:55:01.207Z</updated>
        <summary type="html"><![CDATA[Data labeling in supervised learning is considered an expensive and
infeasible tool in some conditions. The self-supervised learning method is
proposed to tackle the learning effectiveness with fewer labeled data, however,
there is a lack of confidence in the size of labeled data needed to achieve
adequate results. This study aims to draw a baseline on the proportion of the
labeled data that models can appreciate to yield competent accuracy when
compared to training with additional labels. The study implements the
kaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the
self-supervised learning task by implementing random rotations augmentation on
the original datasets. To reveal the true effectiveness of the pretext process
in self-supervised learning, the original dataset is divided into smaller
batches, and learning is repeated on each batch with and without the pretext
pre-training. Results show that the pretext process in the self-supervised
learning improves the accuracy around 15% in the downstream classification task
when compared to the plain supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+AlQuabeh_H/0/1/0/all/0/1"&gt;Hilal AlQuabeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bawazeer_A/0/1/0/all/0/1"&gt;Ameera Bawazeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhashmi_A/0/1/0/all/0/1"&gt;Abdulateef Alhashmi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization. (arXiv:2108.02183v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02183</id>
        <link href="http://arxiv.org/abs/2108.02183"/>
        <updated>2021-08-18T01:55:01.057Z</updated>
        <summary type="html"><![CDATA[The crux of self-supervised video representation learning is to build general
features from unlabeled videos. However, most recent works have mainly focused
on high-level semantics and neglected lower-level representations and their
temporal relationship which are crucial for general video understanding. To
address these challenges, this paper proposes a multi-level feature
optimization framework to improve the generalization and temporal modeling
ability of learned video representations. Concretely, high-level features
obtained from naive and prototypical contrastive learning are utilized to build
distribution graphs, guiding the process of low-level and mid-level feature
learning. We also devise a simple temporal modeling module from multi-level
features to enhance motion pattern learning. Experiments demonstrate that
multi-level feature optimization with the graph constraint and temporal
modeling can greatly improve the representation ability in video understanding.
Code is available at
https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huabin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shuangrui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. (arXiv:2103.10158v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10158</id>
        <link href="http://arxiv.org/abs/2103.10158"/>
        <updated>2021-08-18T01:55:01.026Z</updated>
        <summary type="html"><![CDATA[Automatic augmentation methods have recently become a crucial pillar for
strong model performance in vision tasks. While existing automatic augmentation
methods need to trade off simplicity, cost and performance, we present a most
simple baseline, TrivialAugment, that outperforms previous methods for almost
free. TrivialAugment is parameter-free and only applies a single augmentation
to each image. Thus, TrivialAugment's effectiveness is very unexpected to us
and we performed very thorough experiments to study its performance. First, we
compare TrivialAugment to previous state-of-the-art methods in a variety of
image classification scenarios. Then, we perform multiple ablation studies with
different augmentation spaces, augmentation methods and setups to understand
the crucial requirements for its performance. Additionally, we provide a simple
interface to facilitate the widespread adoption of automatic augmentation
methods, as well as our full code base for reproducibility. Since our work
reveals a stagnation in many parts of automatic augmentation research, we end
with a short proposal of best practices for sustained future progress in
automatic augmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Samuel G. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation and Simulation of Yeast Microscopy Imagery with Deep Learning. (arXiv:2103.11834v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11834</id>
        <link href="http://arxiv.org/abs/2103.11834"/>
        <updated>2021-08-18T01:55:00.994Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescence microscopy (TLFM) is an important and powerful tool
in synthetic biological research. Modeling TLFM experiments based on real data
may enable researchers to repeat certain experiments with minor effort. This
thesis is a study towards deep learning-based modeling of TLFM experiments on
the image level. The modeling of TLFM experiments, by way of the example of
trapped yeast cells, is split into two tasks. The first task is to generate
synthetic image data based on real image data. To approach this problem, a
novel generative adversarial network, for conditionalized and unconditionalized
image generation, is proposed. The second task is the simulation of brightfield
microscopy images over multiple discrete time-steps. To tackle this simulation
task an advanced future frame prediction model is introduced. The proposed
models are trained and tested on a novel dataset that is presented in this
thesis. The obtained results showed that the modeling of TLFM experiments, with
deep learning, is a proper approach, but requires future research to
effectively model real-world experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FMODetect: Robust Detection of Fast Moving Objects. (arXiv:2012.08216v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08216</id>
        <link href="http://arxiv.org/abs/2012.08216"/>
        <updated>2021-08-18T01:55:00.962Z</updated>
        <summary type="html"><![CDATA[We propose the first learning-based approach for fast moving objects
detection. Such objects are highly blurred and move over large distances within
one video frame. Fast moving objects are associated with a deblurring and
matting problem, also called deblatting. We show that the separation of
deblatting into consecutive matting and deblurring allows achieving real-time
performance, i.e. an order of magnitude speed-up, and thus enabling new classes
of application. The proposed method detects fast moving objects as a truncated
distance function to the trajectory by learning from synthetic data. For the
sharp appearance estimation and accurate trajectory estimation, we propose a
matting and fitting network that estimates the blurred appearance without
background, followed by an energy minimization based deblurring. The
state-of-the-art methods are outperformed in terms of recall, precision,
trajectory estimation, and sharp appearance reconstruction. Compared to other
methods, such as deblatting, the inference is of several orders of magnitude
faster and allows applications such as real-time fast moving object detection
and retrieval in large video collections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1"&gt;Denys Rozumnyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sroubek_F/0/1/0/all/0/1"&gt;Filip Sroubek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1"&gt;Martin R. Oswald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching. (arXiv:2012.08950v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08950</id>
        <link href="http://arxiv.org/abs/2012.08950"/>
        <updated>2021-08-18T01:55:00.956Z</updated>
        <summary type="html"><![CDATA[Graph matching (GM) has been a building block in many areas including
computer vision and pattern recognition. Despite the recent impressive
progress, existing deep GM methods often have difficulty in handling outliers
in both graphs, which are ubiquitous in practice. We propose a deep
reinforcement learning (RL) based approach RGM for weighted graph matching,
whose sequential node matching scheme naturally fits with the strategy for
selective inlier matching against outliers, and supports seed graph matching. A
revocable action scheme is devised to improve the agent's flexibility against
the complex constrained matching task. Moreover, we propose a quadratic
approximation technique to regularize the affinity matrix, in the presence of
outliers. As such, the RL agent can finish inlier matching timely when the
objective score stop growing, for which otherwise an additional hyperparameter
i.e. the number of common inliers is needed to avoid matching outliers. In this
paper, we are focused on learning the back-end solver for the most general form
of GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can
also boost other solvers using the affinity input. Experimental results on both
synthetic and real-world datasets showcase its superior performance regarding
both matching accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zetian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lingxiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pinyan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning with Echo State Networks. (arXiv:2105.07674v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07674</id>
        <link href="http://arxiv.org/abs/2105.07674"/>
        <updated>2021-08-18T01:55:00.944Z</updated>
        <summary type="html"><![CDATA[Continual Learning (CL) refers to a learning setup where data is non
stationary and the model has to learn without forgetting existing knowledge.
The study of CL for sequential patterns revolves around trained recurrent
networks. In this work, instead, we introduce CL in the context of Echo State
Networks (ESNs), where the recurrent component is kept fixed. We provide the
first evaluation of catastrophic forgetting in ESNs and we highlight the
benefits in using CL strategies which are not applicable to trained recurrent
models. Our results confirm the ESN as a promising model for CL and open to its
use in streaming scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1"&gt;Andrea Cossu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1"&gt;Claudio Gallicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07253</id>
        <link href="http://arxiv.org/abs/2108.07253"/>
        <updated>2021-08-18T01:55:00.937Z</updated>
        <summary type="html"><![CDATA[We present a task and benchmark dataset for person-centric visual grounding,
the problem of linking between people named in a caption and people pictured in
an image. In contrast to prior work in visual grounding, which is predominantly
object-based, our new task masks out the names of people in captions in order
to encourage methods trained on such image-caption pairs to focus on contextual
cues (such as rich interactions between multiple people), rather than learning
associations between names and appearances. To facilitate this task, we
introduce a new dataset, Who's Waldo, mined automatically from image-caption
data on Wikimedia Commons. We propose a Transformer-based method that
outperforms several strong baselines on this task, and are releasing our data
to the research community to spur work on contextual models that consider both
vision and language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Claire Yuqing Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"&gt;Apoorv Khandelwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1"&gt;Yoav Artzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1"&gt;Noah Snavely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1"&gt;Hadar Averbuch-Elor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUM-VIE: The TUM Stereo Visual-Inertial Event Dataset. (arXiv:2108.07329v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07329</id>
        <link href="http://arxiv.org/abs/2108.07329"/>
        <updated>2021-08-18T01:55:00.917Z</updated>
        <summary type="html"><![CDATA[Event cameras are bio-inspired vision sensors which measure per pixel
brightness changes. They offer numerous benefits over traditional, frame-based
cameras, including low latency, high dynamic range, high temporal resolution
and low power consumption. Thus, these sensors are suited for robotics and
virtual reality applications. To foster the development of 3D perception and
navigation algorithms with event cameras, we present the TUM-VIE dataset. It
consists of a large variety of handheld and head-mounted sequences in indoor
and outdoor environments, including rapid motion during sports and high dynamic
range scenarios. The dataset contains stereo event data, stereo grayscale
frames at 20Hz as well as IMU data at 200Hz. Timestamps between all sensors are
synchronized in hardware. The event cameras contain a large sensor of 1280x720
pixels, which is significantly larger than the sensors used in existing stereo
event datasets (at least by a factor of ten). We provide ground truth poses
from a motion capture system at 120Hz during the beginning and end of each
sequence, which can be used for trajectory evaluation. TUM-VIE includes
challenging sequences where state-of-the art visual SLAM algorithms either fail
or result in large drift. Hence, our dataset can help to push the boundary of
future research on event-based visual-inertial perception algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1"&gt;Simon Klenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chui_J/0/1/0/all/0/1"&gt;Jason Chui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1"&gt;Nikolaus Demmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects. (arXiv:2108.07368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07368</id>
        <link href="http://arxiv.org/abs/2108.07368"/>
        <updated>2021-08-18T01:55:00.909Z</updated>
        <summary type="html"><![CDATA[Segmenting medical images accurately and reliably is important for disease
diagnosis and treatment. It is a challenging task because of the wide variety
of objects' sizes, shapes, and scanning modalities. Recently, many
convolutional neural networks (CNN) have been designed for segmentation tasks
and achieved great success. Few studies, however, have fully considered the
sizes of objects and thus most demonstrate poor performance on segmentation of
small objects segmentation. This can have significant impact on early detection
of disease. This paper proposes a Context Axial Reserve Attention Network
(CaraNet) to improve the segmentation performance on small objects compared
with recent state-of-the-art models. We test our CaraNet on brain tumor (BraTS
2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300 and
ETIS-LaribPolypDB) segmentation. Our CaraNet not only achieves the top-rank
mean Dice segmentation accuracy, but also shows a distinct advantage in
segmentation of small medical objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1"&gt;Ange Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1"&gt;Shuyue Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Human-in-the-Loop Approach based on Explainability to Improve NTL Detection. (arXiv:2009.13437v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13437</id>
        <link href="http://arxiv.org/abs/2009.13437"/>
        <updated>2021-08-18T01:55:00.903Z</updated>
        <summary type="html"><![CDATA[Implementing systems based on Machine Learning to detect fraud and other
Non-Technical Losses (NTL) is challenging: the data available is biased, and
the algorithms currently used are black-boxes that cannot be either easily
trusted or understood by stakeholders. This work explains our human-in-the-loop
approach to mitigate these problems in a real system that uses a supervised
model to detect Non-Technical Losses (NTL) for an international utility company
from Spain. This approach exploits human knowledge (e.g. from the data
scientists or the company's stakeholders) and the information provided by
explanatory methods to guide the system during the training process. This
simple, efficient method that can be easily implemented in other industrial
projects is tested in a real dataset and the results show that the derived
prediction model is better in terms of accuracy, interpretability, robustness
and flexibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coma_Puig_B/0/1/0/all/0/1"&gt;Bernat Coma-Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carmona_J/0/1/0/all/0/1"&gt;Josep Carmona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06676</id>
        <link href="http://arxiv.org/abs/2107.06676"/>
        <updated>2021-08-18T01:55:00.895Z</updated>
        <summary type="html"><![CDATA[One of the most promising approaches for data analysis and exploration of
large data sets is Machine Learning techniques that are inspired by brain
models. Such methods use alternative learning rules potentially more
efficiently than established learning rules. In this work, we focus on the
potential of brain-inspired ML for exploiting High-Performance Computing (HPC)
resources to solve ML problems: we discuss the BCPNN and an HPC implementation,
called StreamBrain, its computational cost, suitability to HPC systems. As an
example, we use StreamBrain to analyze the Higgs Boson dataset from High Energy
Physics and discriminate between background and signal classes in collisions of
high-energy particle colliders. Overall, we reach up to 69.15% accuracy and
76.4% Area Under the Curve (AUC) performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1"&gt;Martin Svedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1"&gt;Artur Podobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Steven W. D. Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1"&gt;Stefano Markidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BN-NAS: Neural Architecture Search with Batch Normalization. (arXiv:2108.07375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07375</id>
        <link href="http://arxiv.org/abs/2108.07375"/>
        <updated>2021-08-18T01:55:00.886Z</updated>
        <summary type="html"><![CDATA[We present BN-NAS, neural architecture search with Batch Normalization
(BN-NAS), to accelerate neural architecture search (NAS). BN-NAS can
significantly reduce the time required by model training and evaluation in NAS.
Specifically, for fast evaluation, we propose a BN-based indicator for
predicting subnet performance at a very early training stage. The BN-based
indicator further facilitates us to improve the training efficiency by only
training the BN parameters during the supernet training. This is based on our
observation that training the whole supernet is not necessary while training
only BN parameters accelerates network convergence for network architecture
search. Extensive experiments show that our method can significantly shorten
the time of training supernet by more than 10 times and shorten the time of
evaluating subnets by more than 600,000 times without losing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peixia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baopu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Ming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRB-GAN: A Dynamic ResBlock Generative Adversarial Network for Artistic Style Transfer. (arXiv:2108.07379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07379</id>
        <link href="http://arxiv.org/abs/2108.07379"/>
        <updated>2021-08-18T01:55:00.880Z</updated>
        <summary type="html"><![CDATA[The paper proposes a Dynamic ResBlock Generative Adversarial Network
(DRB-GAN) for artistic style transfer. The style code is modeled as the shared
parameters for Dynamic ResBlocks connecting both the style encoding network and
the style transfer network. In the style encoding network, a style class-aware
attention mechanism is used to attend the style feature representation for
generating the style codes. In the style transfer network, multiple Dynamic
ResBlocks are designed to integrate the style code and the extracted CNN
semantic feature and then feed into the spatial window Layer-Instance
Normalization (SW-LIN) decoder, which enables high-quality synthetic images
with artistic style transfer. Moreover, the style collection conditional
discriminator is designed to equip our DRB-GAN model with abilities for both
arbitrary style transfer and collection style transfer during the training
stage. No matter for arbitrary style transfer or collection style transfer,
extensive experiments strongly demonstrate that our proposed DRB-GAN
outperforms state-of-the-art methods and exhibits its superior performance in
terms of visual quality and efficiency. Our source code is available at
\color{magenta}{\url{https://github.com/xuwenju123/DRB-GAN}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenju Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruisheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Reinforcement Learning over MDPs. (arXiv:2108.02323v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02323</id>
        <link href="http://arxiv.org/abs/2108.02323"/>
        <updated>2021-08-18T01:55:00.860Z</updated>
        <summary type="html"><![CDATA[The past decade has seen the rapid development of Reinforcement Learning,
which acquires impressive performance with numerous training resources.
However, one of the greatest challenges in RL is generalization efficiency
(i.e., generalization performance in a unit time). This paper proposes a
framework of Active Reinforcement Learning (ARL) over MDPs to improve
generalization efficiency in a limited resource by instance selection. Given a
number of instances, the algorithm chooses out valuable instances as training
sets while training the policy, thereby costing fewer resources. Unlike
existing approaches, we attempt to actively select and use training data rather
than train on all the given data, thereby costing fewer resources. Furthermore,
we introduce a general instance evaluation metrics and selection mechanism into
the framework. Experiments results reveal that the proposed framework with
Proximal Policy Optimization as policy optimizer can effectively improve
generalization efficiency than unselect-ed and unbiased selected methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Peng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Navigation by Continuous-time Neural Networks. (arXiv:2106.08314v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08314</id>
        <link href="http://arxiv.org/abs/2106.08314"/>
        <updated>2021-08-18T01:55:00.852Z</updated>
        <summary type="html"><![CDATA[Imitation learning enables high-fidelity, vision-based learning of policies
within rich, photorealistic environments. However, such techniques often rely
on traditional discrete-time neural models and face difficulties in
generalizing to domain shifts by failing to account for the causal
relationships between the agent and the environment. In this paper, we propose
a theoretical and experimental framework for learning causal representations
using continuous-time neural networks, specifically over their discrete-time
counterparts. We evaluate our method in the context of visual-control learning
of drones over a series of complex tasks, ranging from short- and long-term
navigation, to chasing static and dynamic objects through photorealistic
environments. Our results demonstrate that causal continuous-time deep models
can perform robust navigation tasks, where advanced recurrent models fail.
These models learn complex causal control representations directly from raw
visual inputs and scale to solve a variety of tasks using imitation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vorbach_C/0/1/0/all/0/1"&gt;Charles Vorbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1"&gt;Mathias Lechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Convolutional Neural Networks. (arXiv:2108.07387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07387</id>
        <link href="http://arxiv.org/abs/2108.07387"/>
        <updated>2021-08-18T01:55:00.846Z</updated>
        <summary type="html"><![CDATA[We propose contextual convolution (CoConv) for visual recognition. CoConv is
a direct replacement of the standard convolution, which is the core component
of convolutional neural networks. CoConv is implicitly equipped with the
capability of incorporating contextual information while maintaining a similar
number of parameters and computational cost compared to the standard
convolution. CoConv is inspired by neuroscience studies indicating that (i)
neurons, even from the primary visual cortex (V1 area), are involved in
detection of contextual cues and that (ii) the activity of a visual neuron can
be influenced by the stimuli placed entirely outside of its theoretical
receptive field. On the one hand, we integrate CoConv in the widely-used
residual networks and show improved recognition performance over baselines on
the core tasks and benchmarks for visual recognition, namely image
classification on the ImageNet data set and object detection on the MS COCO
data set. On the other hand, we introduce CoConv in the generator of a
state-of-the-art Generative Adversarial Network, showing improved generative
results on CIFAR-10 and CelebA. Our code is available at
https://github.com/iduta/coconv.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duta_I/0/1/0/all/0/1"&gt;Ionut Cosmin Duta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1"&gt;Mariana Iuliana Georgescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering augmented Self-Supervised Learning: Anapplication to Land Cover Mapping. (arXiv:2108.07323v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07323</id>
        <link href="http://arxiv.org/abs/2108.07323"/>
        <updated>2021-08-18T01:55:00.840Z</updated>
        <summary type="html"><![CDATA[Collecting large annotated datasets in Remote Sensing is often expensive and
thus can become a major obstacle for training advanced machine learning models.
Common techniques of addressing this issue, based on the underlying idea of
pre-training the Deep Neural Networks (DNN) on freely available large datasets,
cannot be used for Remote Sensing due to the unavailability of such large-scale
labeled datasets and the heterogeneity of data sources caused by the varying
spatial and spectral resolution of different sensors. Self-supervised learning
is an alternative approach that learns feature representation from unlabeled
images without using any human annotations. In this paper, we introduce a new
method for land cover mapping by using a clustering based pretext task for
self-supervised learning. We demonstrate the effectiveness of the method on two
societally relevant applications from the aspect of segmentation performance,
discriminative feature representation learning and the underlying cluster
structure. We also show the effectiveness of the active sampling using the
clusters obtained from our method in improving the mapping accuracy given a
limited budget of annotating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Rahul Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chenxi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhenong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Designer: a Unified Model for Scene Search and Synthesis from Sketch. (arXiv:2108.07353v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07353</id>
        <link href="http://arxiv.org/abs/2108.07353"/>
        <updated>2021-08-18T01:55:00.832Z</updated>
        <summary type="html"><![CDATA[Scene Designer is a novel method for searching and generating images using
free-hand sketches of scene compositions; i.e. drawings that describe both the
appearance and relative positions of objects. Our core contribution is a single
unified model to learn both a cross-modal search embedding for matching
sketched compositions to images, and an object embedding for layout synthesis.
We show that a graph neural network (GNN) followed by Transformer under our
novel contrastive learning setting is required to allow learning correlations
between object type, appearance and arrangement, driving a mask generation
module that synthesises coherent scene layouts, whilst also delivering state of
the art sketch based visual search of scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1"&gt;Leo Sampaio Ferraz Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tu Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1"&gt;John Collomosse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1"&gt;Moacir Ponti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PnP-3D: A Plug-and-Play for 3D Point Clouds. (arXiv:2108.07378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07378</id>
        <link href="http://arxiv.org/abs/2108.07378"/>
        <updated>2021-08-18T01:55:00.812Z</updated>
        <summary type="html"><![CDATA[With the help of the deep learning paradigm, many point cloud networks have
been invented for visual analysis. However, there is great potential for
development of these networks since the given information of point cloud data
has not been fully exploited. To improve the effectiveness of existing networks
in analyzing point cloud data, we propose a plug-and-play module, PnP-3D,
aiming to refine the fundamental point cloud feature representations by
involving more local context and global bilinear response from explicit 3D
space and implicit feature space. To thoroughly evaluate our approach, we
conduct experiments on three standard point cloud analysis tasks, including
classification, semantic segmentation, and object detection, where we select
three state-of-the-art networks from each task for evaluation. Serving as a
plug-and-play module, PnP-3D can significantly boost the performances of
established networks. In addition to achieving state-of-the-art results on four
widely used point cloud benchmarks, we present comprehensive ablation studies
and visualizations to demonstrate our approach's advantages. The code will be
available at https://github.com/ShiQiu0419/pnp-3d.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1"&gt;Shi Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1"&gt;Saeed Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Generalization Prediction for Safety Critical Tasks in Novel Operating Domains. (arXiv:2108.07399v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07399</id>
        <link href="http://arxiv.org/abs/2108.07399"/>
        <updated>2021-08-18T01:55:00.806Z</updated>
        <summary type="html"><![CDATA[It is well known that Neural Network (network) performance often degrades
when a network is used in novel operating domains that differ from its training
and testing domains. This is a major limitation, as networks are being
integrated into safety critical, cyber-physical systems that must work in
unconstrained environments, e.g., perception for autonomous vehicles. Training
networks that generalize to novel operating domains and that extract robust
features is an active area of research, but previous work fails to predict what
the network performance will be in novel operating domains. We propose the task
Network Generalization Prediction: predicting the expected network performance
in novel operating domains. We describe the network performance in terms of an
interpretable Context Subspace, and we propose a methodology for selecting the
features of the Context Subspace that provide the most information about the
network performance. We identify the Context Subspace for a pretrained Faster
RCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD)
Dataset, and demonstrate Network Generalization Prediction accuracy within 5%
or less of observed performance. We also demonstrate that the Context Subspace
from the BDD Dataset is informative for completely unseen datasets, JAAD and
Cityscapes, where predictions have a bias of 10% or less.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OBrien_M/0/1/0/all/0/1"&gt;Molly O&amp;#x27;Brien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medoff_M/0/1/0/all/0/1"&gt;Mike Medoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bukowski_J/0/1/0/all/0/1"&gt;Julia Bukowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Greg Hager&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathdreamer: A World Model for Indoor Navigation. (arXiv:2105.08756v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08756</id>
        <link href="http://arxiv.org/abs/2105.08756"/>
        <updated>2021-08-18T01:55:00.800Z</updated>
        <summary type="html"><![CDATA[People navigating in unfamiliar buildings take advantage of myriad visual,
spatial and semantic cues to efficiently achieve their navigation goals.
Towards equipping computational agents with similar capabilities, we introduce
Pathdreamer, a visual world model for agents navigating in novel indoor
environments. Given one or more previous visual observations, Pathdreamer
generates plausible high-resolution 360 visual observations (RGB, semantic
segmentation and depth) for viewpoints that have not been visited, in buildings
not seen during training. In regions of high uncertainty (e.g. predicting
around corners, imagining the contents of an unseen room), Pathdreamer can
predict diverse scenes, allowing an agent to sample multiple realistic outcomes
for a given trajectory. We demonstrate that Pathdreamer encodes useful and
accessible visual, spatial and semantic knowledge about human environments by
using it in the downstream task of Vision-and-Language Navigation (VLN).
Specifically, we show that planning ahead with Pathdreamer brings about half
the benefit of looking ahead at actual observations from unobserved parts of
the environment. We hope that Pathdreamer will help unlock model-based
approaches to challenging embodied navigation tasks such as navigating to
specified objects and VLN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1"&gt;Jing Yu Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1"&gt;Peter Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (arXiv:2103.14030v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14030</id>
        <link href="http://arxiv.org/abs/2103.14030"/>
        <updated>2021-08-18T01:55:00.791Z</updated>
        <summary type="html"><![CDATA[This paper presents a new vision Transformer, called Swin Transformer, that
capably serves as a general-purpose backbone for computer vision. Challenges in
adapting Transformer from language to vision arise from differences between the
two domains, such as large variations in the scale of visual entities and the
high resolution of pixels in images compared to words in text. To address these
differences, we propose a hierarchical Transformer whose representation is
computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme
brings greater efficiency by limiting self-attention computation to
non-overlapping local windows while also allowing for cross-window connection.
This hierarchical architecture has the flexibility to model at various scales
and has linear computational complexity with respect to image size. These
qualities of Swin Transformer make it compatible with a broad range of vision
tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and
dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP
on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its
performance surpasses the previous state-of-the-art by a large margin of +2.7
box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the
potential of Transformer-based models as vision backbones. The hierarchical
design and the shifted window approach also prove beneficial for all-MLP
architectures. The code and models are publicly available
at~\url{https://github.com/microsoft/Swin-Transformer}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yutong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Baining Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuantumFed: A Federated Learning Framework for Collaborative Quantum Training. (arXiv:2106.09109v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09109</id>
        <link href="http://arxiv.org/abs/2106.09109"/>
        <updated>2021-08-18T01:55:00.785Z</updated>
        <summary type="html"><![CDATA[With the fast development of quantum computing and deep learning, quantum
neural networks have attracted great attention recently. By leveraging the
power of quantum computing, deep neural networks can potentially overcome
computational power limitations in classic machine learning. However, when
multiple quantum machines wish to train a global model using the local data on
each machine, it may be very difficult to copy the data into one machine and
train the model. Therefore, a collaborative quantum neural network framework is
necessary. In this article, we borrow the core idea of federated learning to
propose QuantumFed, a quantum federated learning framework to have multiple
quantum nodes with local quantum data train a mode together. Our experiments
show the feasibility and robustness of our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1"&gt;Qi Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choice Set Confounding in Discrete Choice. (arXiv:2105.07959v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07959</id>
        <link href="http://arxiv.org/abs/2105.07959"/>
        <updated>2021-08-18T01:55:00.778Z</updated>
        <summary type="html"><![CDATA[Standard methods in preference learning involve estimating the parameters of
discrete choice models from data of selections (choices) made by individuals
from a discrete set of alternatives (the choice set). While there are many
models for individual preferences, existing learning methods overlook how
choice set assignment affects the data. Often, the choice set itself is
influenced by an individual's preferences; for instance, a consumer choosing a
product from an online retailer is often presented with options from a
recommender system that depend on information about the consumer's preferences.
Ignoring these assignment mechanisms can mislead choice models into making
biased estimates of preferences, a phenomenon that we call choice set
confounding; we demonstrate the presence of such confounding in widely-used
choice datasets.

To address this issue, we adapt methods from causal inference to the discrete
choice setting. We use covariates of the chooser for inverse probability
weighting and/or regression controls, accurately recovering individual
preferences in the presence of choice set confounding under certain
assumptions. When such covariates are unavailable or inadequate, we develop
methods that take advantage of structured choice set assignment to improve
prediction. We demonstrate the effectiveness of our methods on real-world
choice data, showing, for example, that accounting for choice set confounding
makes choices observed in hotel booking and commute transportation more
consistent with rational utility-maximization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomlinson_K/0/1/0/all/0/1"&gt;Kiran Tomlinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ugander_J/0/1/0/all/0/1"&gt;Johan Ugander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1"&gt;Austin R. Benson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13459</id>
        <link href="http://arxiv.org/abs/2107.13459"/>
        <updated>2021-08-18T01:55:00.760Z</updated>
        <summary type="html"><![CDATA[In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose a point cloud-applicable explainability approach
based on local surrogate model-based method to show which components contribute
to the classification. Moreover, we propose quantitative fidelity validations
for generated explanations that enhance the persuasive power of explainability
and compare the plausibility of different existing point cloud-applicable
explainability methods. Our new explainability approach provides a fairly
accurate, more semantically coherent and widely applicable explanation for
point cloud classification tasks. Our code is available at
https://github.com/Explain3D/LIME-3D]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hanxiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convergent and Efficient Deep Q Network Algorithm. (arXiv:2106.15419v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15419</id>
        <link href="http://arxiv.org/abs/2106.15419"/>
        <updated>2021-08-18T01:55:00.753Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of the deep Q network (DQN) reinforcement
learning algorithm and its variants, DQN is still not well understood and it
does not guarantee convergence. In this work, we show that DQN can diverge and
cease to operate in realistic settings. Although there exist gradient-based
convergent methods, we show that they actually have inherent problems in
learning behaviour and elucidate why they often fail in practice. To overcome
these problems, we propose a convergent DQN algorithm (C-DQN) by carefully
modifying DQN, and we show that the algorithm is convergent and can work with
large discount factors (0.9998). It learns robustly in difficult settings and
can learn several difficult games in the Atari 2600 benchmark where DQN fail,
within a moderate computational budget. Our codes have been publicly released
and can be used to reproduce our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhikang T. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity analysis in differentially private machine learning using hybrid automatic differentiation. (arXiv:2107.04265v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04265</id>
        <link href="http://arxiv.org/abs/2107.04265"/>
        <updated>2021-08-18T01:55:00.747Z</updated>
        <summary type="html"><![CDATA[In recent years, formal methods of privacy protection such as differential
privacy (DP), capable of deployment to data-driven tasks such as machine
learning (ML), have emerged. Reconciling large-scale ML with the closed-form
reasoning required for the principled analysis of individual privacy loss
requires the introduction of new tools for automatic sensitivity analysis and
for tracking an individual's data and their features through the flow of
computation. For this purpose, we introduce a novel \textit{hybrid} automatic
differentiation (AD) system which combines the efficiency of reverse-mode AD
with an ability to obtain a closed-form expression for any given quantity in
the computational graph. This enables modelling the sensitivity of arbitrary
differentiable function compositions, such as the training of neural networks
on private data. We demonstrate our approach by analysing the individual DP
guarantees of statistical database queries. Moreover, we investigate the
application of our technique to the training of DP neural networks. Our
approach can enable the principled reasoning about privacy loss in the setting
of data processing, and further the development of automatic sensitivity
analysis and privacy budgeting systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_K/0/1/0/all/0/1"&gt;Kritika Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1"&gt;Andrew Trask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dynamic Interpolation for Extremely Sparse Light Fields with Wide Baselines. (arXiv:2108.07408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07408</id>
        <link href="http://arxiv.org/abs/2108.07408"/>
        <updated>2021-08-18T01:55:00.740Z</updated>
        <summary type="html"><![CDATA[In this paper, we tackle the problem of dense light field (LF) reconstruction
from sparsely-sampled ones with wide baselines and propose a learnable model,
namely dynamic interpolation, to replace the commonly-used geometry warping
operation. Specifically, with the estimated geometric relation between input
views, we first construct a lightweight neural network to dynamically learn
weights for interpolating neighbouring pixels from input views to synthesize
each pixel of novel views independently. In contrast to the fixed and
content-independent weights employed in the geometry warping operation, the
learned interpolation weights implicitly incorporate the correspondences
between the source and novel views and adapt to different image content
information. Then, we recover the spatial correlation between the independently
synthesized pixels of each novel view by referring to that of input views using
a geometry-based spatial refinement module. We also constrain the angular
correlation between the novel views through a disparity-oriented LF structure
loss. Experimental results on LF datasets with wide baselines show that the
reconstructed LFs achieve much higher PSNR/SSIM and preserve the LF parallax
structure better than state-of-the-art methods. The source code is publicly
available at https://github.com/MantangGuo/DI4SLF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mantang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural ODE Processes. (arXiv:2103.12413v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12413</id>
        <link href="http://arxiv.org/abs/2103.12413"/>
        <updated>2021-08-18T01:55:00.716Z</updated>
        <summary type="html"><![CDATA[Neural Ordinary Differential Equations (NODEs) use a neural network to model
the instantaneous rate of change in the state of a system. However, despite
their apparent suitability for dynamics-governed time-series, NODEs present a
few disadvantages. First, they are unable to adapt to incoming data points, a
fundamental requirement for real-time applications imposed by the natural
direction of time. Second, time series are often composed of a sparse set of
measurements that could be explained by many possible underlying dynamics.
NODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are
a family of models providing uncertainty estimation and fast data adaptation
but lack an explicit treatment of the flow of time. To address these problems,
we introduce Neural ODE Processes (NDPs), a new class of stochastic processes
determined by a distribution over Neural ODEs. By maintaining an adaptive
data-dependent distribution over the underlying ODE, we show that our model can
successfully capture the dynamics of low-dimensional systems from just a few
data points. At the same time, we demonstrate that NDPs scale up to challenging
high-dimensional time-series with unknown latent dynamics such as rotating
MNIST digits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Norcliffe_A/0/1/0/all/0/1"&gt;Alexander Norcliffe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1"&gt;Cristian Bodnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Day_B/0/1/0/all/0/1"&gt;Ben Day&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moss_J/0/1/0/all/0/1"&gt;Jacob Moss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06069</id>
        <link href="http://arxiv.org/abs/2101.06069"/>
        <updated>2021-08-18T01:55:00.708Z</updated>
        <summary type="html"><![CDATA[Pretrained deep models hold their learnt knowledge in the form of model
parameters. These parameters act as "memory" for the trained models and help
them generalize well on unseen data. However, in absence of training data, the
utility of a trained model is merely limited to either inference or better
initialization towards a target task. In this paper, we go further and extract
synthetic data by leveraging the learnt model parameters. We dub them "Data
Impressions", which act as proxy to the training data and can be used to
realize a variety of tasks. These are useful in scenarios where only the
pretrained models are available and the training data is not shared (e.g., due
to privacy or sensitivity concerns). We show the applicability of data
impressions in solving several computer vision tasks such as unsupervised
domain adaptation, continual learning as well as knowledge distillation. We
also study the adversarial robustness of lightweight models trained via
knowledge distillation using these data impressions. Further, we demonstrate
the efficacy of data impressions in generating data-free Universal Adversarial
Perturbations (UAPs) with better fooling rates. Extensive experiments performed
on benchmark datasets demonstrate competitive performance achieved using data
impressions in absence of original training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1"&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1"&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saksham Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Anirban Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Causal Graphical Prior Knowledge into Predictive Modeling via Simple Data Augmentation. (arXiv:2103.00136v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00136</id>
        <link href="http://arxiv.org/abs/2103.00136"/>
        <updated>2021-08-18T01:55:00.699Z</updated>
        <summary type="html"><![CDATA[Causal graphs (CGs) are compact representations of the knowledge of the data
generating processes behind the data distributions. When a CG is available,
e.g., from the domain knowledge, we can infer the conditional independence (CI)
relations that should hold in the data distribution. However, it is not
straightforward how to incorporate this knowledge into predictive modeling. In
this work, we propose a model-agnostic data augmentation method that allows us
to exploit the prior knowledge of the CI encoded in a CG for supervised machine
learning. We theoretically justify the proposed method by providing an excess
risk bound indicating that the proposed method suppresses overfitting by
reducing the apparent complexity of the predictor hypothesis class. Using
real-world data with CGs provided by domain experts, we experimentally show
that the proposed method is effective in improving the prediction accuracy,
especially in the small-data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1"&gt;Takeshi Teshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08687</id>
        <link href="http://arxiv.org/abs/2107.08687"/>
        <updated>2021-08-18T01:55:00.690Z</updated>
        <summary type="html"><![CDATA[Various modifications of TRANSFORMER were recently used to solve time-series
forecasting problem. We propose Query Selector - an efficient, deterministic
algorithm for sparse attention matrix. Experiments show it achieves
state-of-the art results on ETT, Helpdesk and BPI'12 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1"&gt;Jacek Klimek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1"&gt;Jakub Klimek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1"&gt;Witold Kraskiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1"&gt;Mateusz Topolewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive analysis for scatterplot-based representations of dimensionality reduction. (arXiv:2101.12044v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12044</id>
        <link href="http://arxiv.org/abs/2101.12044"/>
        <updated>2021-08-18T01:55:00.684Z</updated>
        <summary type="html"><![CDATA[Cluster interpretation after dimensionality reduction (DR) is a ubiquitous
part of exploring multidimensional datasets. DR results are frequently
represented by scatterplots, where spatial proximity encodes similarity among
data samples. In the literature, techniques support the understanding of
scatterplots' organization by visualizing the importance of the features for
cluster definition with layout enrichment strategies. However, current
approaches usually focus on global information, hampering the analysis whenever
the focus is to understand the differences among clusters. Thus, this paper
introduces a methodology to visually explore DR results and interpret clusters'
formation based on contrastive analysis. We also introduce a bipartite graph to
visually interpret and explore the relationship between the statistical
variables employed to understand how the data features influence cluster
formation. Our approach is demonstrated through case studies, in which we
explore two document collections related to news articles and tweets about
COVID-19 symptoms. Finally, we evaluate our approach through quantitative
results to demonstrate its robustness to support multidimensional analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+E%2E_W/0/1/0/all/0/1"&gt;Wilson E. Marc&amp;#xed;lio-Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eler_D/0/1/0/all/0/1"&gt;Danilo M. Eler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1"&gt;Rog&amp;#xe9;rio E. Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. (arXiv:2103.10158v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10158</id>
        <link href="http://arxiv.org/abs/2103.10158"/>
        <updated>2021-08-18T01:55:00.664Z</updated>
        <summary type="html"><![CDATA[Automatic augmentation methods have recently become a crucial pillar for
strong model performance in vision tasks. While existing automatic augmentation
methods need to trade off simplicity, cost and performance, we present a most
simple baseline, TrivialAugment, that outperforms previous methods for almost
free. TrivialAugment is parameter-free and only applies a single augmentation
to each image. Thus, TrivialAugment's effectiveness is very unexpected to us
and we performed very thorough experiments to study its performance. First, we
compare TrivialAugment to previous state-of-the-art methods in a variety of
image classification scenarios. Then, we perform multiple ablation studies with
different augmentation spaces, augmentation methods and setups to understand
the crucial requirements for its performance. Additionally, we provide a simple
interface to facilitate the widespread adoption of automatic augmentation
methods, as well as our full code base for reproducibility. Since our work
reveals a stagnation in many parts of automatic augmentation research, we end
with a short proposal of best practices for sustained future progress in
automatic augmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Samuel G. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lossy Compression for Lossless Prediction. (arXiv:2106.10800v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10800</id>
        <link href="http://arxiv.org/abs/2106.10800"/>
        <updated>2021-08-18T01:55:00.657Z</updated>
        <summary type="html"><![CDATA[Most data is automatically collected and only ever "seen" by algorithms. Yet,
data compressors preserve perceptual fidelity rather than just the information
needed by algorithms performing downstream tasks. In this paper, we
characterize the bit-rate required to ensure high performance on all predictive
tasks that are invariant under a set of transformations, such as data
augmentations. Based on our theory, we design unsupervised objectives for
training neural compressors. Using these objectives, we train a generic image
compressor that achieves substantial rate savings (more than $1000\times$ on
ImageNet) compared to JPEG on 8 datasets, without decreasing downstream
classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1"&gt;Yann Dubois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1"&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generative Models of Textured 3D Meshes from Real-World Images. (arXiv:2103.15627v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15627</id>
        <link href="http://arxiv.org/abs/2103.15627"/>
        <updated>2021-08-18T01:55:00.648Z</updated>
        <summary type="html"><![CDATA[Recent advances in differentiable rendering have sparked an interest in
learning generative models of textured 3D meshes from image collections. These
models natively disentangle pose and appearance, enable downstream applications
in computer graphics, and improve the ability of generative models to
understand the concept of image formation. Although there has been prior work
on learning such models from collections of 2D images, these approaches require
a delicate pose estimation step that exploits annotated keypoints, thereby
restricting their applicability to a few specific datasets. In this work, we
propose a GAN framework for generating textured triangle meshes without relying
on such annotations. We show that the performance of our approach is on par
with prior work that relies on ground-truth keypoints, and more importantly, we
demonstrate the generality of our method by setting new baselines on a larger
set of categories from ImageNet - for which keypoints are not available -
without any class-specific hyperparameter tuning. We release our code at
https://github.com/dariopavllo/textured-3d-gan]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1"&gt;Dario Pavllo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas Kohler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1"&gt;Aurelien Lucchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Inductive Matrix Completion. (arXiv:2004.01653v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01653</id>
        <link href="http://arxiv.org/abs/2004.01653"/>
        <updated>2021-08-18T01:55:00.640Z</updated>
        <summary type="html"><![CDATA[We propose orthogonal inductive matrix completion (OMIC), an interpretable
approach to matrix completion based on a sum of multiple orthonormal side
information terms, together with nuclear-norm regularization. The approach
allows us to inject prior knowledge about the singular vectors of the ground
truth matrix. We optimize the approach by a provably converging algorithm,
which optimizes all components of the model simultaneously. We study the
generalization capabilities of our method in both the distribution-free setting
and in the case where the sampling distribution admits uniform marginals,
yielding learning guarantees that improve with the quality of the injected
knowledge in both cases. As particular cases of our framework, we present
models which can incorporate user and item biases or community information in a
joint and additive fashion. We analyse the performance of OMIC on several
synthetic and real datasets. On synthetic datasets with a sliding scale of user
bias relevance, we show that OMIC better adapts to different regimes than other
methods. On real-life datasets containing user/items recommendations and
relevant side information, we find that OMIC surpasses the state-of-the-art,
with the added benefit of greater interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ledent_A/0/1/0/all/0/1"&gt;Antoine Ledent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1"&gt;Rodrigo Alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1"&gt;Marius Kloft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Midwifery Learning and Forecasting: Predicting Content Demand with User-Generated Logs. (arXiv:2107.02480v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02480</id>
        <link href="http://arxiv.org/abs/2107.02480"/>
        <updated>2021-08-18T01:55:00.630Z</updated>
        <summary type="html"><![CDATA[Every day, 800 women and 6,700 newborns die from complications related to
pregnancy or childbirth. A well-trained midwife can prevent most of these
maternal and newborn deaths. Data science models together with logs generated
by users of online learning applications for midwives can help to improve their
learning competencies. The goal is to use these rich behavioral data to push
digital learning towards personalized content and to provide an adaptive
learning journey. In this work, we evaluate various forecasting methods to
determine the interest of future users on the different kind of contents
available in the app, broken down by profession and region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guitart_A/0/1/0/all/0/1"&gt;Anna Guitart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rio_A/0/1/0/all/0/1"&gt;Ana Fern&amp;#xe1;ndez del R&amp;#xed;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perianez_A/0/1/0/all/0/1"&gt;&amp;#xc1;frica Peri&amp;#xe1;&amp;#xf1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bellhouse_L/0/1/0/all/0/1"&gt;Lauren Bellhouse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analogical and Relational Reasoning with Spiking Neural Networks. (arXiv:2010.06746v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06746</id>
        <link href="http://arxiv.org/abs/2010.06746"/>
        <updated>2021-08-18T01:55:00.622Z</updated>
        <summary type="html"><![CDATA[Raven's Progressive Matrices have been widely used for measuring abstract
reasoning and intelligence in humans. However for artificial learning systems,
abstract reasoning remains a challenging problem. In this paper we investigate
how neural networks augmented with biologically inspired spiking modules gain a
significant advantage in solving this problem. To illustrate this, we first
investigate the performance of our networks with supervised learning, then with
unsupervised learning. Experiments on the RAVEN dataset show that the overall
accuracy of our supervised networks surpass human-level performance, while our
unsupervised networks significantly outperform existing unsupervised methods.
Finally, our results from both supervised and unsupervised learning illustrate
that, unlike their non-augmented counterparts, networks with spiking modules
are able to extract and encode temporal features without any explicit
instruction, do not heavily rely on training data, and generalise more readily
to new problems. In summary, the results reported here indicate that artificial
neural networks with spiking modules are well suited to solving abstract
reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Omari_R/0/1/0/all/0/1"&gt;Rollin Omari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKay_R/0/1/0/all/0/1"&gt;R. I. McKay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1"&gt;Tom Gedeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Through Hand-Eye Coordination: An Action Space for Learning Spatially-Invariant Visuomotor Control. (arXiv:2103.00375v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00375</id>
        <link href="http://arxiv.org/abs/2103.00375"/>
        <updated>2021-08-18T01:55:00.604Z</updated>
        <summary type="html"><![CDATA[Imitation Learning (IL) is an effective framework to learn visuomotor skills
from offline demonstration data. However, IL methods often fail to generalize
to new scene configurations not covered by training data. On the other hand,
humans can manipulate objects in varying conditions. Key to such capability is
hand-eye coordination, a cognitive ability that enables humans to adaptively
direct their movements at task-relevant objects and be invariant to the
objects' absolute spatial location. In this work, we present a learnable action
space, Hand-eye Action Networks (HAN), that can approximate human's hand-eye
coordination behaviors by learning from human teleoperated demonstrations.
Through a set of challenging multi-stage manipulation tasks, we show that a
visuomotor policy equipped with HAN is able to inherit the key spatial
invariance property of hand-eye coordination and achieve zero-shot
generalization to new scene configurations. Additional materials available at
https://sites.google.com/stanford.edu/han]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1"&gt;Ajay Mandlekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Danfei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks. (arXiv:2007.05785v5 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05785</id>
        <link href="http://arxiv.org/abs/2007.05785"/>
        <updated>2021-08-18T01:55:00.595Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have attracted enormous research interest due
to temporal information processing capability, low power consumption, and high
biological plausibility. However, the formulation of efficient and
high-performance learning algorithms for SNNs is still challenging. Most
existing learning methods learn weights only, and require manual tuning of the
membrane-related parameters that determine the dynamics of a single spiking
neuron. These parameters are typically chosen to be the same for all neurons,
which limits the diversity of neurons and thus the expressiveness of the
resulting SNNs. In this paper, we take inspiration from the observation that
membrane-related parameters are different across brain regions, and propose a
training algorithm that is capable of learning not only the synaptic weights
but also the membrane time constants of SNNs. We show that incorporating
learnable membrane time constants can make the network less sensitive to
initial values and can speed up learning. In addition, we reevaluate the
pooling methods in SNNs and find that max-pooling will not lead to significant
information loss and have the advantage of low computation cost and binary
compatibility. We evaluate the proposed method for image classification tasks
on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and
neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment
results show that the proposed method outperforms the state-of-the-art accuracy
on nearly all datasets, using fewer time-steps. Our codes are available at
https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1"&gt;Timothee Masquelier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12230</id>
        <link href="http://arxiv.org/abs/2010.12230"/>
        <updated>2021-08-18T01:55:00.587Z</updated>
        <summary type="html"><![CDATA[The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1"&gt;Suvrit Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models. (arXiv:2007.04131v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04131</id>
        <link href="http://arxiv.org/abs/2007.04131"/>
        <updated>2021-08-18T01:55:00.580Z</updated>
        <summary type="html"><![CDATA[An increasing number of model-agnostic interpretation techniques for machine
learning (ML) models such as partial dependence plots (PDP), permutation
feature importance (PFI) and Shapley values provide insightful model
interpretations, but can lead to wrong conclusions if applied incorrectly. We
highlight many general pitfalls of ML model interpretation, such as using
interpretation techniques in the wrong context, interpreting models that do not
generalize well, ignoring feature dependencies, interactions, uncertainty
estimates and issues in high-dimensional settings, or making unjustified causal
interpretations, and illustrate them with examples. We focus on pitfalls for
global methods that describe the average model behavior, but many pitfalls also
apply to local methods that explain individual predictions. Our paper addresses
ML practitioners by raising awareness of pitfalls and identifying solutions for
correct model interpretation, but also addresses ML researchers by discussing
open issues for further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1"&gt;Christoph Molnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1"&gt;Gunnar K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Herbinger_J/0/1/0/all/0/1"&gt;Julia Herbinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1"&gt;Timo Freiesleben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dandl_S/0/1/0/all/0/1"&gt;Susanne Dandl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholbeck_C/0/1/0/all/0/1"&gt;Christian A. Scholbeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1"&gt;Giuseppe Casalicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1"&gt;Moritz Grosse-Wentrup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching. (arXiv:2012.08950v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08950</id>
        <link href="http://arxiv.org/abs/2012.08950"/>
        <updated>2021-08-18T01:55:00.520Z</updated>
        <summary type="html"><![CDATA[Graph matching (GM) has been a building block in many areas including
computer vision and pattern recognition. Despite the recent impressive
progress, existing deep GM methods often have difficulty in handling outliers
in both graphs, which are ubiquitous in practice. We propose a deep
reinforcement learning (RL) based approach RGM for weighted graph matching,
whose sequential node matching scheme naturally fits with the strategy for
selective inlier matching against outliers, and supports seed graph matching. A
revocable action scheme is devised to improve the agent's flexibility against
the complex constrained matching task. Moreover, we propose a quadratic
approximation technique to regularize the affinity matrix, in the presence of
outliers. As such, the RL agent can finish inlier matching timely when the
objective score stop growing, for which otherwise an additional hyperparameter
i.e. the number of common inliers is needed to avoid matching outliers. In this
paper, we are focused on learning the back-end solver for the most general form
of GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can
also boost other solvers using the affinity input. Experimental results on both
synthetic and real-world datasets showcase its superior performance regarding
both matching accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zetian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lingxiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pinyan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Choices and the Selection Monad. (arXiv:2007.08926v6 [cs.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08926</id>
        <link href="http://arxiv.org/abs/2007.08926"/>
        <updated>2021-08-18T01:55:00.513Z</updated>
        <summary type="html"><![CDATA[Describing systems in terms of choices and their resulting costs and rewards
offers the promise of freeing algorithm designers and programmers from
specifying how those choices should be made; in implementations, the choices
can be realized by optimization techniques and, increasingly, by
machine-learning methods. We study this approach from a programming-language
perspective. We define two small languages that support decision-making
abstractions: one with choices and rewards, and the other additionally with
probabilities. We give both operational and denotational semantics.

In the case of the second language we consider three denotational semantics,
with varying degrees of correlation between possible program values and
expected rewards. The operational semantics combine the usual semantics of
standard constructs with optimization over spaces of possible execution
strategies. The denotational semantics, which are compositional, rely on the
selection monad, to handle choice, augmented with an auxiliary monad to handle
other effects, such as rewards or probability.

We establish adequacy theorems that the two semantics coincide in all cases.
We also prove full abstraction at base types, with varying notions of
observation in the probabilistic case corresponding to the various degrees of
correlation. We present axioms for choice combined with rewards and
probability, establishing completeness at base types for the case of rewards
without probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1"&gt;Martin Abadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1"&gt;Gordon Plotkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially Conditioned Graphs for Detecting Human-Object Interactions. (arXiv:2012.06060v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06060</id>
        <link href="http://arxiv.org/abs/2012.06060"/>
        <updated>2021-08-18T01:55:00.506Z</updated>
        <summary type="html"><![CDATA[We address the problem of detecting human-object interactions in images using
graphical neural networks. Unlike conventional methods, where nodes send scaled
but otherwise identical messages to each of their neighbours, we propose to
condition messages between pairs of nodes on their spatial relationships,
resulting in different messages going to neighbours of the same node. To this
end, we explore various ways of applying spatial conditioning under a
multi-branch structure. Through extensive experimentation we demonstrate the
advantages of spatial conditioning for the computation of the adjacency
structure, messages and the refined graph features. In particular, we
empirically show that as the quality of the bounding boxes increases, their
coarse appearance features contribute relatively less to the disambiguation of
interactions compared to the spatial information. Our method achieves an mAP of
31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming
state-of-the-art on fine-tuned detections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frederic Z. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1"&gt;Dylan Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Neural Architecture Search with Operation Embeddings. (arXiv:2105.04885v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04885</id>
        <link href="http://arxiv.org/abs/2105.04885"/>
        <updated>2021-08-18T01:55:00.497Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) has recently gained increased attention, as
a class of approaches that automatically searches in an input space of network
architectures. A crucial part of the NAS pipeline is the encoding of the
architecture that consists of the applied computational blocks, namely the
operations and the links between them. Most of the existing approaches either
fail to capture the structural properties of the architectures or use
hand-engineered vector to encode the operator information. In this paper, we
propose the replacement of fixed operator encoding with learnable
representations in the optimization process. This approach, which effectively
captures the relations of different operations, leads to smoother and more
accurate representations of the architectures and consequently to improved
performance of the end task. Our extensive evaluation in ENAS benchmark
demonstrates the effectiveness of the proposed operation embeddings to the
generation of highly accurate models, achieving state-of-the-art performance.
Finally, our method produces top-performing architectures that share similar
operation and graph patterns, highlighting a strong correlation between the
structural properties of the architecture and its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1"&gt;Michail Chatzianastasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1"&gt;George Dasoulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1"&gt;Georgios Siolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1"&gt;Michalis Vazirgiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Speaker Verification Backend with Robust Performance across Conditions. (arXiv:2102.01760v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01760</id>
        <link href="http://arxiv.org/abs/2102.01760"/>
        <updated>2021-08-18T01:55:00.490Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of speaker verification in conditions
unseen or unknown during development. A standard method for speaker
verification consists of extracting speaker embeddings with a deep neural
network and processing them through a backend composed of probabilistic linear
discriminant analysis (PLDA) and global logistic regression score calibration.
This method is known to result in systems that work poorly on conditions
different from those used to train the calibration model. We propose to modify
the standard backend, introducing an adaptive calibrator that uses duration and
other automatically extracted side-information to adapt to the conditions of
the inputs. The backend is trained discriminatively to optimize binary
cross-entropy. When trained on a number of diverse datasets that are labeled
only with respect to speaker, the proposed backend consistently and, in some
cases, dramatically improves calibration, compared to the standard PLDA
approach, on a number of held-out datasets, some of which are markedly
different from the training data. Discrimination performance is also
consistently improved. We show that joint training of the PLDA and the adaptive
calibrator is essential -- the same benefits cannot be achieved when freezing
PLDA and fine-tuning the calibrator. To our knowledge, the results in this
paper are the first evidence in the literature that it is possible to develop a
speaker verification system with robust out-of-the-box performance on a large
variety of conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1"&gt;Luciana Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLaren_M/0/1/0/all/0/1"&gt;Mitchell McLaren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brummer_N/0/1/0/all/0/1"&gt;Niko Brummer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13244</id>
        <link href="http://arxiv.org/abs/2011.13244"/>
        <updated>2021-08-18T01:55:00.482Z</updated>
        <summary type="html"><![CDATA[Multi-view projection methods have demonstrated their ability to reach
state-of-the-art performance on 3D shape recognition. Those methods learn
different ways to aggregate information from multiple views. However, the
camera view-points for those views tend to be heuristically set and fixed for
all shapes. To circumvent the lack of dynamism of current multi-view methods,
we propose to learn those view-points. In particular, we introduce the
Multi-View Transformation Network (MVTN) that regresses optimal view-points for
3D shape recognition, building upon advances in differentiable rendering. As a
result, MVTN can be trained end-to-end along with any multi-view network for 3D
shape classification. We integrate MVTN in a novel adaptive multi-view pipeline
that can render either 3D meshes or point clouds. MVTN exhibits clear
performance gains in the tasks of 3D shape classification and 3D shape
retrieval without the need for extra training supervision. In these tasks, MVTN
achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the
most recent and realistic ScanObjectNN dataset (up to 6% improvement).
Interestingly, we also show that MVTN can provide network robustness against
rotation and occlusion in the 3D domain. The code is available at
https://github.com/ajhamdi/MVTN .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1"&gt;Abdullah Hamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1"&gt;Silvio Giancola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack. (arXiv:2011.14585v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14585</id>
        <link href="http://arxiv.org/abs/2011.14585"/>
        <updated>2021-08-18T01:55:00.371Z</updated>
        <summary type="html"><![CDATA[The video-based action recognition task has been extensively studied in
recent years. In this paper, we study the structural vulnerability of deep
learning-based action recognition models against the adversarial attack using
the one frame attack that adds an inconspicuous perturbation to only a single
frame of a given video clip. Our analysis shows that the models are highly
vulnerable against the one frame attack due to their structural properties.
Experiments demonstrate high fooling rates and inconspicuous characteristics of
the attack. Furthermore, we show that strong universal one frame perturbations
can be obtained under various scenarios. Our work raises the serious issue of
adversarial vulnerability of the state-of-the-art action recognition models in
various perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jaehui Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jun-Hyuk Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Seok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01377</id>
        <link href="http://arxiv.org/abs/2008.01377"/>
        <updated>2021-08-18T01:55:00.345Z</updated>
        <summary type="html"><![CDATA[Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a
key requirement for both linguistic research and subsequent automated natural
language processing (NLP) tasks. This problem is commonly tackled using machine
learning methods, i.e., by training a POS tagger on a sufficiently large corpus
of labeled data. While the problem of POS tagging can essentially be considered
as solved for modern languages, historical corpora turn out to be much more
difficult, especially due to the lack of native speakers and sparsity of
training data. Moreover, most texts have no sentences as we know them today,
nor a common orthography. These irregularities render the task of automated POS
tagging more difficult and error-prone. Under these circumstances, instead of
forcing the POS tagger to predict and commit to a single tag, it should be
enabled to express its uncertainty. In this paper, we consider POS tagging
within the framework of set-valued prediction, which allows the POS tagger to
express its uncertainty via predicting a set of candidate POS tags instead of
guessing a single one. The goal is to guarantee a high confidence that the
correct POS tag is included while keeping the number of candidates small. In
our experimental study, we find that extending state-of-the-art POS taggers to
set-valued prediction yields more precise and robust taggings, especially for
unknown words, i.e., words not occurring in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1"&gt;Stefan Heid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1"&gt;Marcel Wever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Learning Guarantees for Compressive Clustering and Compressive Mixture Modeling. (arXiv:2004.08085v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08085</id>
        <link href="http://arxiv.org/abs/2004.08085"/>
        <updated>2021-08-18T01:55:00.326Z</updated>
        <summary type="html"><![CDATA[We provide statistical learning guarantees for two unsupervised learning
tasks in the context of compressive statistical learning, a general framework
for resource-efficient large-scale learning that we introduced in a companion
paper.The principle of compressive statistical learning is to compress a
training collection, in one pass, into a low-dimensional sketch (a vector of
random empirical generalized moments) that captures the information relevant to
the considered learning task. We explicitly describe and analyze random feature
functions which empirical averages preserve the needed information for
compressive clustering and compressive Gaussian mixture modeling with fixed
known variance, and establish sufficient sketch sizes given the problem
dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt; (PANAMA, DANTE), &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchard_G/0/1/0/all/0/1"&gt;Gilles Blanchard&lt;/a&gt; (LMO), &lt;a href="http://arxiv.org/find/cs/1/au:+Keriven_N/0/1/0/all/0/1"&gt;Nicolas Keriven&lt;/a&gt; (GIPSA-GAIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Traonmilin_Y/0/1/0/all/0/1"&gt;Yann Traonmilin&lt;/a&gt; (IMB)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. (arXiv:1903.08543v5 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.08543</id>
        <link href="http://arxiv.org/abs/1903.08543"/>
        <updated>2021-08-18T01:55:00.306Z</updated>
        <summary type="html"><![CDATA[Using a model heat engine, we show that neural network-based reinforcement
learning can identify thermodynamic trajectories of maximal efficiency. We
consider both gradient and gradient-free reinforcement learning. We use an
evolutionary learning algorithm to evolve a population of neural networks,
subject to a directive to maximize the efficiency of a trajectory composed of a
set of elementary thermodynamic processes; the resulting networks learn to
carry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given
an additional irreversible process, this evolutionary scheme learns a
previously unknown thermodynamic cycle. Gradient-based reinforcement learning
is able to learn the Stirling cycle, whereas an evolutionary approach achieves
the optimal Carnot cycle. Our results show how the reinforcement learning
strategies developed for game playing can be applied to solve physical problems
conditioned upon path-extensive order parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_C/0/1/0/all/0/1"&gt;Chris Beeler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahorau_U/0/1/0/all/0/1"&gt;Uladzimir Yahorau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coles_R/0/1/0/all/0/1"&gt;Rory Coles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Kyle Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1"&gt;Stephen Whitelam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1"&gt;Isaac Tamblyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Content-Based Deep Intrusion Detection System. (arXiv:2001.05009v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05009</id>
        <link href="http://arxiv.org/abs/2001.05009"/>
        <updated>2021-08-18T01:55:00.238Z</updated>
        <summary type="html"><![CDATA[The growing number of Internet users and the prevalence of web applications
make it necessary to deal with very complex software and applications in the
network. This results in an increasing number of new vulnerabilities in the
systems, and leading to an increase in cyber threats and, in particular,
zero-day attacks. The cost of generating appropriate signatures for these
attacks is a potential motive for using machine learning-based methodologies.
Although there are many studies on using learning-based methods for attack
detection, they generally use extracted features and overlook raw contents.
This approach can lessen the performance of detection systems against
content-based attacks like SQL injection, Cross-site Scripting (XSS), and
various viruses.

In this work, we propose a framework, called deep intrusion detection (DID)
system, that uses the pure content of traffic flows in addition to traffic
metadata in the learning and detection phases of a passive DNN IDS. To this
end, we deploy and evaluate an offline IDS following the framework using LSTM
as a deep learning technique. Due to the inherent nature of deep learning, it
can process high dimensional data content and, accordingly, discover the
sophisticated relations between the auto extracted features of the traffic. To
evaluate the proposed DID system, we use the CIC-IDS2017 and CSE-CIC-IDS2018
datasets. The evaluation metrics, such as precision and recall, reach $0.992$
and $0.998$ on CIC-IDS2017, and $0.933$ and $0.923$ on CSE-CIC-IDS2018
respectively, which show the high performance of the proposed DID method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1"&gt;Mahdi Soltani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siavoshani_M/0/1/0/all/0/1"&gt;Mahdi Jafari Siavoshani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahangir_A/0/1/0/all/0/1"&gt;Amir Hossein Jahangir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection. (arXiv:2108.07794v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07794</id>
        <link href="http://arxiv.org/abs/2108.07794"/>
        <updated>2021-08-18T01:55:00.217Z</updated>
        <summary type="html"><![CDATA[3D point cloud understanding has made great progress in recent years.
However, one major bottleneck is the scarcity of annotated real datasets,
especially compared to 2D object detection tasks, since a large amount of labor
is involved in annotating the real scans of a scene. A promising solution to
this problem is to make better use of the synthetic dataset, which consists of
CAD object models, to boost the learning on real datasets. This can be achieved
by the pre-training and fine-tuning procedure. However, recent work on 3D
pre-training exhibits failure when transfer features learned on synthetic
objects to other real-world applications. In this work, we put forward a new
method called RandomRooms to accomplish this objective. In particular, we
propose to generate random layouts of a scene by making use of the objects in
the synthetic CAD dataset and learn the 3D scene representation by applying
object-level contrastive learning on two random scenes generated from the same
set of synthetic objects. The model pre-trained in this way can serve as a
better initialization when later fine-tuning on the 3D object detection task.
Empirically, we show consistent improvement in downstream 3D detection tasks on
several base models, especially when less training data are used, which
strongly demonstrates the effectiveness and generalization of our method.
Benefiting from the rich semantic knowledge and diverse objects from synthetic
data, our method establishes the new state-of-the-art on widely-used 3D
detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide
a new perspective for bridging object and scene-level 3D understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Recommendation for Structural Equation Model Discovery in Process Mining. (arXiv:2108.07795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07795</id>
        <link href="http://arxiv.org/abs/2108.07795"/>
        <updated>2021-08-18T01:55:00.210Z</updated>
        <summary type="html"><![CDATA[Process mining techniques can help organizations to improve their operational
processes. Organizations can benefit from process mining techniques in finding
and amending the root causes of performance or compliance problems. Considering
the volume of the data and the number of features captured by the information
system of today's companies, the task of discovering the set of features that
should be considered in root cause analysis can be quite involving. In this
paper, we propose a method for finding the set of (aggregated) features with a
possible effect on the problem.

The root cause analysis task is usually done by applying a machine learning
technique to the data gathered from the information system supporting the
processes. To prevent mixing up correlation and causation, which may happen
because of interpreting the findings of machine learning techniques as causal,
we propose a method for discovering the structural equation model of the
process that can be used for root cause analysis. We have implemented the
proposed method as a plugin in ProM and we have evaluated it using two real and
synthetic event logs. These experiments show the validity and effectiveness of
the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qafari_M/0/1/0/all/0/1"&gt;Mahnaz Sadat Qafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1"&gt;Wil van der Aalst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-aware Contrastive Regression for Action Quality Assessment. (arXiv:2108.07797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07797</id>
        <link href="http://arxiv.org/abs/2108.07797"/>
        <updated>2021-08-18T01:55:00.203Z</updated>
        <summary type="html"><![CDATA[Assessing action quality is challenging due to the subtle differences between
videos and large variations in scores. Most existing approaches tackle this
problem by regressing a quality score from a single video, suffering a lot from
the large inter-video score variations. In this paper, we show that the
relations among videos can provide important clues for more accurate action
quality assessment during both training and inference. Specifically, we
reformulate the problem of action quality assessment as regressing the relative
scores with reference to another video that has shared attributes (e.g.,
category and difficulty), instead of learning unreferenced scores. Following
this formulation, we propose a new Contrastive Regression (CoRe) framework to
learn the relative scores by pair-wise comparison, which highlights the
differences between videos and guides the models to learn the key hints for
assessment. In order to further exploit the relative information between two
videos, we devise a group-aware regression tree to convert the conventional
score regression into two easier sub-problems: coarse-to-fine classification
and regression in small intervals. To demonstrate the effectiveness of CoRe, we
conduct extensive experiments on three mainstream AQA datasets including AQA-7,
MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large
margin and establishes new state-of-the-art on all three benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xumin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtracTree: a Simple and Effective Method for Regulator Validation of Bagging Methods Used in Retail Banking. (arXiv:2004.02326v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02326</id>
        <link href="http://arxiv.org/abs/2004.02326"/>
        <updated>2021-08-18T01:55:00.191Z</updated>
        <summary type="html"><![CDATA[Bootstrap aggregation, known as bagging, is one of the most popular ensemble
methods used in machine learning (ML). An ensemble method is a ML method that
combines multiple hypotheses to form a single hypothesis used for prediction. A
bagging algorithm combines multiple classifiers modeled on different
sub-samples of the same data set to build one large classifier. Banks, and
their retail banking activities, are nowadays using the power of ML algorithms,
including decision trees and random forests, to optimize their processes.
However, banks have to comply with regulators and governance and, hence,
delivering effective ML solutions is a challenging task. It starts with the
bank's validation and governance department, followed by the deployment of the
solution in a production environment up to the external validation of the
national financial regulator. Each proposed ML model has to be validated and
clear rules for every algorithm-based decision must be justified. In this
context, we propose XtracTree, an algorithm capable of efficiently converting
an ML bagging classifier, such as a random forest, into simple "if-then" rules
satisfying the requirements of model validation. We use a public loan data set
from Kaggle to illustrate the usefulness of our approach. Our experiments
demonstrate that using XtracTree, one can convert an ML model into a rule-based
algorithm, leading to easier model validation by national financial regulators
and the bank's validation department. The proposed approach allowed our banking
institution to reduce up to 50% the time of delivery of our AI solutions to the
end-user.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Charlier_J/0/1/0/all/0/1"&gt;Jeremy Charlier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarenkov_V/0/1/0/all/0/1"&gt;Vladimir Makarenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The application of predictive analytics to identify at-risk students in health professions education. (arXiv:2108.07709v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07709</id>
        <link href="http://arxiv.org/abs/2108.07709"/>
        <updated>2021-08-18T01:55:00.180Z</updated>
        <summary type="html"><![CDATA[Introduction: When a learner fails to reach a milestone, educators often
wonder if there had been any warning signs that could have allowed them to
intervene sooner. Machine learning is used to predict which students are at
risk of failing a national certifying exam. Predictions are made well in
advance of the exam, such that educators can meaningfully intervene before
students take the exam.

Methods: Using already-collected, first-year student assessment data from
four cohorts in a Master of Physician Assistant Studies program, the authors
implement an "adaptive minimum match" version of the k-nearest neighbors
algorithm (AMMKNN), using changing numbers of neighbors to predict each
student's future exam scores on the Physician Assistant National Certifying
Examination (PANCE). Leave-one-out cross validation (LOOCV) was used to
evaluate the practical capabilities of this model, before making predictions
for new students.

Results: The best predictive model has an accuracy of 93%, sensitivity of
69%, and specificity of 94%. It generates a predicted PANCE score for each
student, one year before they are scheduled to take the exam. Students can then
be prospectively categorized into groups that need extra support, optional
extra support, or no extra support. The educator then has one year to provide
the appropriate customized support to each type of student.

Conclusions: Predictive analytics can help health professions educators
allocate scarce time and resources across their students. Interprofessional
educators can use the included methods and code to generate predicted test
outcomes for students. The authors recommend that educators using this or
similar predictive methods act responsibly and transparently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Anshul Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_R/0/1/0/all/0/1"&gt;Roger Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_L/0/1/0/all/0/1"&gt;Lisa Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImitAL: Learning Active Learning Strategies from Synthetic Data. (arXiv:2108.07670v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07670</id>
        <link href="http://arxiv.org/abs/2108.07670"/>
        <updated>2021-08-18T01:55:00.173Z</updated>
        <summary type="html"><![CDATA[One of the biggest challenges that complicates applied supervised machine
learning is the need for huge amounts of labeled data. Active Learning (AL) is
a well-known standard method for efficiently obtaining labeled data by first
labeling the samples that contain the most information based on a query
strategy. Although many methods for query strategies have been proposed in the
past, no clear superior method that works well in general for all domains has
been found yet. Additionally, many strategies are computationally expensive
which further hinders the widespread use of AL for large-scale annotation
projects.

We, therefore, propose ImitAL, a novel query strategy, which encodes AL as a
learning-to-rank problem. For training the underlying neural network we chose
Imitation Learning. The required demonstrative expert experience for training
is generated from purely synthetic data.

To show the general and superior applicability of \ImitAL{}, we perform an
extensive evaluation comparing our strategy on 15 different datasets, from a
wide range of domains, with 10 different state-of-the-art query strategies. We
also show that our approach is more runtime performant than most other
strategies, especially on very large datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonsior_J/0/1/0/all/0/1"&gt;Julius Gonsior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiele_M/0/1/0/all/0/1"&gt;Maik Thiele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehner_W/0/1/0/all/0/1"&gt;Wolfgang Lehner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Should You Defend Your Classifier -- A Game-theoretical Analysis of Countermeasures against Adversarial Examples. (arXiv:2108.07602v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07602</id>
        <link href="http://arxiv.org/abs/2108.07602"/>
        <updated>2021-08-18T01:55:00.143Z</updated>
        <summary type="html"><![CDATA[Adversarial machine learning, i.e., increasing the robustness of machine
learning algorithms against so-called adversarial examples, is now an
established field. Yet, newly proposed methods are evaluated and compared under
unrealistic scenarios where costs for adversary and defender are not considered
and either all samples are attacked or no sample is attacked. We scrutinize
these assumptions and propose the advanced adversarial classification game,
which incorporates all relevant parameters of an adversary and a defender in
adversarial classification. Especially, we take into account economic factors
on both sides and the fact that all so far proposed countermeasures against
adversarial examples reduce accuracy on benign samples. Analyzing the scenario
in detail, where both players have two pure strategies, we identify all best
responses and conclude that in practical settings, the most influential factor
might be the maximum amount of adversarial examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samsinger_M/0/1/0/all/0/1"&gt;Maximilian Samsinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merkle_F/0/1/0/all/0/1"&gt;Florian Merkle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schottle_P/0/1/0/all/0/1"&gt;Pascal Sch&amp;#xf6;ttle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1"&gt;Tomas Pevny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3D CT Lesions. (arXiv:2108.07662v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07662</id>
        <link href="http://arxiv.org/abs/2108.07662"/>
        <updated>2021-08-18T01:55:00.116Z</updated>
        <summary type="html"><![CDATA[With the renaissance of deep learning, automatic diagnostic systems for
computed tomography (CT) have achieved many successful applications. However,
they are mostly attributed to careful expert annotations, which are often
scarce in practice. This drives our interest to the unsupervised representation
learning. Recent studies have shown that self-supervised learning is an
effective approach for learning representations, but most of them rely on the
empirical design of transformations and pretext tasks. To avoid the
subjectivity associated with these methods, we propose the MVCNet, a novel
unsupervised three dimensional (3D) representation learning method working in a
transformation-free manner. We view each 3D lesion from different orientations
to collect multiple two dimensional (2D) views. Then, an embedding function is
learned by minimizing a contrastive loss so that the 2D views of the same 3D
lesion are aggregated, and the 2D views of different lesions are separated. We
evaluate the representations by training a simple classification head upon the
embedding layer. Experimental results show that MVCNet achieves
state-of-the-art accuracies on the LIDC-IDRI (89.55%), LNDb (77.69%) and
TianChi (79.96%) datasets for unsupervised representation learning. When
fine-tuned on 10% of the labeled data, the accuracies are comparable to the
supervised learning model (89.46% vs. 85.03%, 73.85% vs. 73.44%, 83.56% vs.
83.34% on the three datasets, respectively), indicating the superiority of
MVCNet in learning representations with limited annotations. Code is released
at: https://github.com/penghuazhai/MVCNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1"&gt;Penghua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1"&gt;Huaiwei Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1"&gt;Chaowei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing value from data science in business: ensuring explainability and fairness of solutions. (arXiv:2108.07714v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07714</id>
        <link href="http://arxiv.org/abs/2108.07714"/>
        <updated>2021-08-18T01:55:00.094Z</updated>
        <summary type="html"><![CDATA[The paper introduces concepts of fairness and explainability (XAI) in
artificial intelligence, oriented to solve a sophisticated business problems.
For fairness, the authors discuss the bias-inducing specifics, as well as
relevant mitigation methods, concluding with a set of recipes for introducing
fairness in data-driven organizations. Additionally, for XAI, the authors audit
specific algorithms paired with demonstrational business use-cases, discuss a
plethora of techniques of explanations quality quantification and provide an
overview of future research avenues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chomiak_K/0/1/0/all/0/1"&gt;Krzysztof Chomiak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miktus_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Miktus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-parametric Bayesian Additive Regression Trees. (arXiv:2108.07636v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.07636</id>
        <link href="http://arxiv.org/abs/2108.07636"/>
        <updated>2021-08-18T01:55:00.088Z</updated>
        <summary type="html"><![CDATA[We propose a new semi-parametric model based on Bayesian Additive Regression
Trees (BART). In our approach, the response variable is approximated by a
linear predictor and a BART model, where the first component is responsible for
estimating the main effects and BART accounts for the non-specified
interactions and non-linearities. The novelty in our approach lies in the way
we change tree generation moves in BART to deal with confounding between the
parametric and non-parametric components when they have covariates in common.
Through synthetic and real-world examples, we demonstrate that the performance
of the new semi-parametric BART is competitive when compared to regression
models and other tree-based methods. The implementation of the proposed method
is available at https://github.com/ebprado/SP-BART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Prado_E/0/1/0/all/0/1"&gt;Estev&amp;#xe3;o B. Prado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Parnell_A/0/1/0/all/0/1"&gt;Andrew C. Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McJames_N/0/1/0/all/0/1"&gt;Nathan McJames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+OShea_A/0/1/0/all/0/1"&gt;Ann O&amp;#x27;Shea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moral_R/0/1/0/all/0/1"&gt;Rafael A. Moral&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07790</id>
        <link href="http://arxiv.org/abs/2108.07790"/>
        <updated>2021-08-18T01:55:00.067Z</updated>
        <summary type="html"><![CDATA[Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1"&gt;Helen Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1"&gt;Cooper Raterink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o G.M. Ara&amp;#xfa;jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1"&gt;Ivan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Carol Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1"&gt;Adrien Morisot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1"&gt;Nicholas Frosst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGNet: Weighing Black Holes with Deep Learning. (arXiv:2108.07749v1 [astro-ph.GA])]]></title>
        <id>http://arxiv.org/abs/2108.07749</id>
        <link href="http://arxiv.org/abs/2108.07749"/>
        <updated>2021-08-18T01:55:00.057Z</updated>
        <summary type="html"><![CDATA[Supermassive black holes (SMBHs) are ubiquitously found at the centers of
most massive galaxies. Measuring SMBH mass is important for understanding the
origin and evolution of SMBHs. However, traditional methods require
spectroscopic data which is expensive to gather. We present an algorithm that
weighs SMBHs using quasar light time series, circumventing the need for
expensive spectra. We train, validate, and test neural networks that directly
learn from the Sloan Digital Sky Survey (SDSS) Stripe 82 light curves for a
sample of $38,939$ spectroscopically confirmed quasars to map out the nonlinear
encoding between SMBH mass and multi-color optical light curves. We find a
1$\sigma$ scatter of 0.37 dex between the predicted SMBH mass and the fiducial
virial mass estimate based on SDSS single-epoch spectra, which is comparable to
the systematic uncertainty in the virial mass estimate. Our results have direct
implications for more efficient applications with future observations from the
Vera C. Rubin Observatory. Our code, \textsf{AGNet}, is publicly available at

{\color{red} \url{https://github.com/snehjp2/AGNet}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1"&gt;Joshua Yao-Yu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pandya_S/0/1/0/all/0/1"&gt;Sneh Pandya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pratap_D/0/1/0/all/0/1"&gt;Devanshi Pratap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kind_M/0/1/0/all/0/1"&gt;Matias Carrasco Kind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kindratenko_V/0/1/0/all/0/1"&gt;Volodymyr Kindratenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPAN: Subgraph Prediction Attention Network for Dynamic Graphs. (arXiv:2108.07776v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.07776</id>
        <link href="http://arxiv.org/abs/2108.07776"/>
        <updated>2021-08-18T01:55:00.045Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel model for predicting subgraphs in dynamic graphs,
an extension of traditional link prediction. This proposed end-to-end model
learns a mapping from the subgraph structures in the current snapshot to the
subgraph structures in the next snapshot directly, i.e., edge existence among
multiple nodes in the subgraph. A new mechanism named cross-attention with a
twin-tower module is designed to integrate node attribute information and
topology information collaboratively for learning subgraph evolution. We
compare our model with several state-of-the-art methods for subgraph prediction
and subgraph pattern prediction in multiple real-world homogeneous and
heterogeneous dynamic graphs, respectively. Experimental results demonstrate
that our model outperforms other models in these two tasks, with a gain
increase from 5.02% to 10.88%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuanchang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yubo Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hai Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Nonparametric Inference via Deep Neural Network. (arXiv:1902.01687v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.01687</id>
        <link href="http://arxiv.org/abs/1902.01687"/>
        <updated>2021-08-18T01:55:00.038Z</updated>
        <summary type="html"><![CDATA[Deep neural network is a state-of-art method in modern science and
technology. Much statistical literature have been devoted to understanding its
performance in nonparametric estimation, whereas the results are suboptimal due
to a redundant logarithmic sacrifice. In this paper, we show that such
log-factors are not necessary. We derive upper bounds for the $L^2$ minimax
risk in nonparametric estimation. Sufficient conditions on network
architectures are provided such that the upper bounds become optimal (without
log-sacrifice). Our proof relies on an explicitly constructed network estimator
based on tensor product B-splines. We also derive asymptotic distributions for
the constructed network and a relating hypothesis testing procedure. The
testing procedure is further proven as minimax optimal under suitable network
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boukai_B/0/1/0/all/0/1"&gt;Ben Boukai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1"&gt;Zuofeng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KCNet: An Insect-Inspired Single-Hidden-Layer Neural Network with Randomized Binary Weights for Prediction and Classification Tasks. (arXiv:2108.07554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07554</id>
        <link href="http://arxiv.org/abs/2108.07554"/>
        <updated>2021-08-18T01:55:00.029Z</updated>
        <summary type="html"><![CDATA[Fruit flies are established model systems for studying olfactory learning as
they will readily learn to associate odors with both electric shock or sugar
rewards. The mechanisms of the insect brain apparently responsible for odor
learning form a relatively shallow neuronal architecture. Olfactory inputs are
received by the antennal lobe (AL) of the brain, which produces an encoding of
each odor mixture across ~50 sub-units known as glomeruli. Each of these
glomeruli then project its component of this feature vector to several of ~2000
so-called Kenyon Cells (KCs) in a region of the brain known as the mushroom
body (MB). Fly responses to odors are generated by small downstream neuropils
that decode the higher-order representation from the MB. Research has shown
that there is no recognizable pattern in the glomeruli--KC connections (and
thus the particular higher-order representations); they are akin to
fingerprints~-- even isogenic flies have different projections. Leveraging
insights from this architecture, we propose KCNet, a single-hidden-layer neural
network that contains sparse, randomized, binary weights between the input
layer and the hidden layer and analytically learned weights between the hidden
layer and the output layer. Furthermore, we also propose a dynamic optimization
algorithm that enables the KCNet to increase performance beyond its structural
limits by searching a more efficient set of inputs. For odorant-perception
tasks that predict perceptual properties of an odorant, we show that KCNet
outperforms existing data-driven approaches, such as XGBoost. For
image-classification tasks, KCNet achieves reasonable performance on benchmark
datasets (MNIST, Fashion-MNIST, and EMNIST) without any data-augmentation
methods or convolutional layers and shows particularly fast running time. Thus,
neural networks inspired by the insect brain can be both economical and perform
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Jinyung Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1"&gt;Theodore P. Pavlic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Program Synthesis with Large Language Models. (arXiv:2108.07732v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2108.07732</id>
        <link href="http://arxiv.org/abs/2108.07732"/>
        <updated>2021-08-18T01:55:00.022Z</updated>
        <summary type="html"><![CDATA[This paper explores the limits of the current generation of large language
models for program synthesis in general purpose programming languages. We
evaluate a collection of such models (with between 244M and 137B parameters) on
two new benchmarks, MBPP and MathQA-Python, in both the few-shot and
fine-tuning regimes. Our benchmarks are designed to measure the ability of
these models to synthesize short Python programs from natural language
descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974
programming tasks, designed to be solvable by entry-level programmers. The
MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914
problems that evaluate the ability of the models to synthesize code from more
complex text. On both datasets, we find that synthesis performance scales
log-linearly with model size. Our largest models, even without finetuning on a
code dataset, can synthesize solutions to 59.6 percent of the problems from
MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a
held-out portion of the dataset improves performance by about 10 percentage
points across most model sizes. On the MathQA-Python dataset, the largest
fine-tuned model achieves 83.8 percent accuracy. Going further, we study the
model's ability to engage in dialog about code, incorporating human feedback to
improve its solutions. We find that natural language feedback from a human
halves the error rate compared to the model's initial prediction. Additionally,
we conduct an error analysis to shed light on where these models fall short and
what types of programs are most difficult to generate. Finally, we explore the
semantic grounding of these models by fine-tuning them to predict the results
of program execution. We find that even our best models are generally unable to
predict the output of a program given a specific input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1"&gt;Jacob Austin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Odena_A/0/1/0/all/0/1"&gt;Augustus Odena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1"&gt;Maxwell Nye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1"&gt;Maarten Bosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1"&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1"&gt;David Dohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_E/0/1/0/all/0/1"&gt;Ellen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Carrie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1"&gt;Michael Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting State Augmentation methods for Reinforcement Learning with Stochastic Delays. (arXiv:2108.07555v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07555</id>
        <link href="http://arxiv.org/abs/2108.07555"/>
        <updated>2021-08-18T01:54:59.999Z</updated>
        <summary type="html"><![CDATA[Several real-world scenarios, such as remote control and sensing, are
comprised of action and observation delays. The presence of delays degrades the
performance of reinforcement learning (RL) algorithms, often to such an extent
that algorithms fail to learn anything substantial. This paper formally
describes the notion of Markov Decision Processes (MDPs) with stochastic delays
and shows that delayed MDPs can be transformed into equivalent standard MDPs
(without delays) with significantly simplified cost structure. We employ this
equivalence to derive a model-free Delay-Resolved RL framework and show that
even a simple RL algorithm built upon this framework achieves near-optimal
rewards in environments with stochastic delays in actions and observations. The
delay-resolved deep Q-network (DRDQN) algorithm is bench-marked on a variety of
environments comprising of multi-step and stochastic delays and results in
better performance, both in terms of achieving near-optimal rewards and
minimizing the computational overhead thereof, with respect to the currently
established algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1"&gt;Somjit Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baranwal_M/0/1/0/all/0/1"&gt;Mayank Baranwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1"&gt;Harshad Khadilkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Placement of Public Electric Vehicle Charging Stations Using Deep Reinforcement Learning. (arXiv:2108.07772v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.07772</id>
        <link href="http://arxiv.org/abs/2108.07772"/>
        <updated>2021-08-18T01:54:59.990Z</updated>
        <summary type="html"><![CDATA[The placement of charging stations in areas with developing charging
infrastructure is a critical component of the future success of electric
vehicles (EVs). In Albany County in New York, the expected rise in the EV
population requires additional charging stations to maintain a sufficient level
of efficiency across the charging infrastructure. A novel application of
Reinforcement Learning (RL) is able to find optimal locations for new charging
stations given the predicted charging demand and current charging locations.
The most important factors that influence charging demand prediction include
the conterminous traffic density, EV registrations, and proximity to certain
types of public buildings. The proposed RL framework can be refined and applied
to cities across the world to optimize charging station placement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Petratos_A/0/1/0/all/0/1"&gt;Aidan Petratos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ting_A/0/1/0/all/0/1"&gt;Allen Ting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Padmanabhan_S/0/1/0/all/0/1"&gt;Shankar Padmanabhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kristina Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hageman_D/0/1/0/all/0/1"&gt;Dylan Hageman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pisel_J/0/1/0/all/0/1"&gt;Jesse R. Pisel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pyrcz_M/0/1/0/all/0/1"&gt;Michael J. Pyrcz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct domain adaptation through reciprocal linear transformations. (arXiv:2108.07600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07600</id>
        <link href="http://arxiv.org/abs/2108.07600"/>
        <updated>2021-08-18T01:54:59.979Z</updated>
        <summary type="html"><![CDATA[We propose a direct domain adaptation (DDA) approach to enrich the training
of supervised neural networks on synthetic data by features from real-world
data. The process involves a series of linear operations on the input features
to the NN model, whether they are from the source or target domains, as
follows: 1) A cross-correlation of the input data (i.e. images) with a randomly
picked sample pixel (or pixels) of all images from that domain or the mean of
all randomly picked sample pixel (or pixels) of all images. 2) The convolution
of the resulting data with the mean of the autocorrelated input images from the
other domain. In the training stage, as expected, the input images are from the
source domain, and the mean of auto-correlated images are evaluated from the
target domain. In the inference/application stage, the input images are from
the target domain, and the mean of auto-correlated images are evaluated from
the source domain. The proposed method only manipulates the data from the
source and target domains and does not explicitly interfere with the training
workflow and network architecture. An application that includes training a
convolutional neural network on the MNIST dataset and testing the network on
the MNIST-M dataset achieves a 70% accuracy on the test data. A principal
component analysis (PCA), as well as t-SNE, show that the input features from
the source and target domains, after the proposed direct transformations, share
similar properties along with the principal components as compared to the
original MNIST and MNIST-M input features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alkhalifah_T/0/1/0/all/0/1"&gt;Tariq Alkhalifah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ovcharenko_O/0/1/0/all/0/1"&gt;Oleg Ovcharenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O-HAS: Optical Hardware Accelerator Search for Boosting Both Acceleration Performance and Development Speed. (arXiv:2108.07538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07538</id>
        <link href="http://arxiv.org/abs/2108.07538"/>
        <updated>2021-08-18T01:54:59.970Z</updated>
        <summary type="html"><![CDATA[The recent breakthroughs and prohibitive complexities of Deep Neural Networks
(DNNs) have excited extensive interest in domain-specific DNN accelerators,
among which optical DNN accelerators are particularly promising thanks to their
unprecedented potential of achieving superior performance-per-watt. However,
the development of optical DNN accelerators is much slower than that of
electrical DNN accelerators. One key challenge is that while many techniques
have been developed to facilitate the development of electrical DNN
accelerators, techniques that support or expedite optical DNN accelerator
design remain much less explored, limiting both the achievable performance and
the innovation development of optical DNN accelerators. To this end, we develop
the first-of-its-kind framework dubbed O-HAS, which for the first time
demonstrates automated Optical Hardware Accelerator Search for boosting both
the acceleration efficiency and development speed of optical DNN accelerators.
Specifically, our O-HAS consists of two integrated enablers: (1) an O-Cost
Predictor, which can accurately yet efficiently predict an optical
accelerator's energy and latency based on the DNN model parameters and the
optical accelerator design; and (2) an O-Search Engine, which can automatically
explore the large design space of optical DNN accelerators and identify the
optimal accelerators (i.e., the micro-architectures and
algorithm-to-accelerator mapping methods) in order to maximize the target
acceleration efficiency. Extensive experiments and ablation studies
consistently validate the effectiveness of both our O-Cost Predictor and
O-Search Engine as well as the excellent efficiency of O-HAS generated optical
accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengquan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongzhi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yonggan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingyan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Students performance with Artificial Neural Network using Demographic Traits. (arXiv:2108.07717v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07717</id>
        <link href="http://arxiv.org/abs/2108.07717"/>
        <updated>2021-08-18T01:54:59.962Z</updated>
        <summary type="html"><![CDATA[Many researchers have studied student academic performance in supervised and
unsupervised learning using numerous data mining techniques. Neural networks
often need a greater collection of observations to achieve enough predictive
ability. Due to the increase in the rate of poor graduates, it is necessary to
design a system that helps to reduce this menace as well as reduce the
incidence of students having to repeat due to poor performance or having to
drop out of school altogether in the middle of the pursuit of their career. It
is therefore necessary to study each one as well as their advantages and
disadvantages, so as to determine which is more efficient in and in what case
one should be preferred over the other. The study aims to develop a system to
predict student performance with Artificial Neutral Network using the student
demographic traits so as to assist the university in selecting candidates
(students) with a high prediction of success for admission using previous
academic records of students granted admissions which will eventually lead to
quality graduates of the institution. The model was developed based on certain
selected variables as the input. It achieved an accuracy of over 92.3 percent,
showing Artificial Neural Network potential effectiveness as a predictive tool
and a selection criterion for candidates seeking admission to a university.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kehinde_A/0/1/0/all/0/1"&gt;Adeniyi Jide Kehinde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeniyi_A/0/1/0/all/0/1"&gt;Abidemi Emmanuel Adeniyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogundokun_R/0/1/0/all/0/1"&gt;Roseline Oluwaseun Ogundokun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Himanshu Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1"&gt;Sanjay Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental cluster validity index-guided online learning for performance and robustness to presentation order. (arXiv:2108.07743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07743</id>
        <link href="http://arxiv.org/abs/2108.07743"/>
        <updated>2021-08-18T01:54:59.936Z</updated>
        <summary type="html"><![CDATA[In streaming data applications incoming samples are processed and discarded,
therefore, intelligent decision-making is crucial for the performance of
lifelong learning systems. In addition, the order in which samples arrive may
heavily affect the performance of online (and offline) incremental learners.
The recently introduced incremental cluster validity indices (iCVIs) provide
valuable aid in addressing such class of problems. Their primary use-case has
been cluster quality monitoring; nonetheless, they have been very recently
integrated in a streaming clustering method to assist the clustering task
itself. In this context, the work presented here introduces the first adaptive
resonance theory (ART)-based model that uses iCVIs for unsupervised and
semi-supervised online learning. Moreover, it shows for the first time how to
use iCVIs to regulate ART vigilance via an iCVI-based match tracking mechanism.
The model achieves improved accuracy and robustness to ordering effects by
integrating an online iCVI framework as module B of a topological adaptive
resonance theory predictive mapping (TopoARTMAP) -- thereby being named
iCVI-TopoARTMAP -- and by employing iCVI-driven post-processing heuristics at
the end of each learning step. The online iCVI framework provides assignments
of input samples to clusters at each iteration in accordance to any of several
iCVIs. The iCVI-TopoARTMAP maintains useful properties shared by ARTMAP models,
such as stability, immunity to catastrophic forgetting, and the many-to-one
mapping capability via the map field module. The performance (unsupervised and
semi-supervised) and robustness to presentation order (unsupervised) of
iCVI-TopoARTMAP were evaluated via experiments with a synthetic data set and
deep embeddings of a real-world face image data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1"&gt;Leonardo Enzo Brito da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rayapati_N/0/1/0/all/0/1"&gt;Nagasharath Rayapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1"&gt;Donald C. Wunsch II&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Panoramic Learning with A Standardized Machine Learning Formalism. (arXiv:2108.07783v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07783</id>
        <link href="http://arxiv.org/abs/2108.07783"/>
        <updated>2021-08-18T01:54:59.905Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) is about computational methods that enable machines to
learn concepts from experiences. In handling a wide variety of experiences
ranging from data instances, knowledge, constraints, to rewards, adversaries,
and lifelong interplay in an ever-growing spectrum of tasks, contemporary ML/AI
research has resulted in a multitude of learning paradigms and methodologies.
Despite the continual progresses on all different fronts, the disparate
narrowly-focused methods also make standardized, composable, and reusable
development of learning solutions difficult, and make it costly if possible to
build AI agents that panoramically learn from all types of experiences. This
paper presents a standardized ML formalism, in particular a standard equation
of the learning objective, that offers a unifying understanding of diverse ML
algorithms, making them special cases due to different choices of modeling
components. The framework also provides guidance for mechanic design of new ML
solutions, and serves as a promising vehicle towards panoramic learning with
all experiences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Laws for Deep Learning. (arXiv:2108.07686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07686</id>
        <link href="http://arxiv.org/abs/2108.07686"/>
        <updated>2021-08-18T01:54:59.898Z</updated>
        <summary type="html"><![CDATA[Running faster will only get you so far -- it is generally advisable to first
understand where the roads lead, then get a car ...

The renaissance of machine learning (ML) and deep learning (DL) over the last
decade is accompanied by an unscalable computational cost, limiting its
advancement and weighing on the field in practice. In this thesis we take a
systematic approach to address the algorithmic and methodological limitations
at the root of these costs. We first demonstrate that DL training and pruning
are predictable and governed by scaling laws -- for state of the art models and
tasks, spanning image classification and language modeling, as well as for
state of the art model compression via iterative pruning. Predictability, via
the establishment of these scaling laws, provides the path for principled
design and trade-off reasoning, currently largely lacking in the field. We then
continue to analyze the sources of the scaling laws, offering an
approximation-theoretic view and showing through the exploration of a noiseless
realizable case that DL is in fact dominated by error sources very far from the
lower error limit. We conclude by building on the gained theoretical
understanding of the scaling laws' origins. We present a conjectural path to
eliminate one of the current dominant error sources -- through a data bandwidth
limiting hypothesis and the introduction of Nyquist learners -- which can, in
principle, reach the generalization error lower limit (e.g. 0 in the noiseless
case), at finite dataset size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Jonathan S. Rosenfeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coalesced Multi-Output Tsetlin Machines with Clause Sharing. (arXiv:2108.07594v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.07594</id>
        <link href="http://arxiv.org/abs/2108.07594"/>
        <updated>2021-08-18T01:54:59.890Z</updated>
        <summary type="html"><![CDATA[Using finite-state machines to learn patterns, Tsetlin machines (TMs) have
obtained competitive accuracy and learning speed across several benchmarks,
with frugal memory- and energy footprint. A TM represents patterns as
conjunctive clauses in propositional logic (AND-rules), each clause voting for
or against a particular output. While efficient for single-output problems, one
needs a separate TM per output for multi-output problems. Employing multiple
TMs hinders pattern reuse because each TM then operates in a silo. In this
paper, we introduce clause sharing, merging multiple TMs into a single one.
Each clause is related to each output by using a weight. A positive weight
makes the clause vote for output $1$, while a negative weight makes the clause
vote for output $0$. The clauses thus coalesce to produce multiple outputs. The
resulting coalesced Tsetlin Machine (CoTM) simultaneously learns both the
weights and the composition of each clause by employing interacting Stochastic
Searching on the Line (SSL) and Tsetlin Automata (TA) teams. Our empirical
results on MNIST, Fashion-MNIST, and Kuzushiji-MNIST show that CoTM obtains
significantly higher accuracy than TM on $50$- to $1$K-clause configurations,
indicating an ability to repurpose clauses. E.g., accuracy goes from $71.99$%
to $89.66$% on Fashion-MNIST when employing $50$ clauses per class (22 Kb
memory). While TM and CoTM accuracy is similar when using more than $1$K
clauses per class, CoTM reaches peak accuracy $3\times$ faster on MNIST with
$8$K clauses. We further investigate robustness towards imbalanced training
data. Our evaluations on imbalanced versions of IMDb- and CIFAR10 data show
that CoTM is robust towards high degrees of class imbalance. Being able to
share clauses, we believe CoTM will enable new TM application domains that
involve multiple outputs, such as learning language models and auto-encoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Glimsdal_S/0/1/0/all/0/1"&gt;Sondre Glimsdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1"&gt;Ole-Christoffer Granmo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demonstrating REACT: a Real-time Educational AI-powered Classroom Tool. (arXiv:2108.07693v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07693</id>
        <link href="http://arxiv.org/abs/2108.07693"/>
        <updated>2021-08-18T01:54:59.883Z</updated>
        <summary type="html"><![CDATA[We present a demonstration of REACT, a new Real-time Educational AI-powered
Classroom Tool that employs EDM techniques for supporting the decision-making
process of educators. REACT is a data-driven tool with a user-friendly
graphical interface. It analyzes students' performance data and provides
context-based alerts as well as recommendations to educators for course
planning. Furthermore, it incorporates model-agnostic explanations for bringing
explainability and interpretability in the process of decision making. This
paper demonstrates a use case scenario of our proposed tool using a real-world
dataset and presents the design of its architecture and user interface. This
demonstration focuses on the agglomerative clustering of students based on
their performance (i.e., incorrect responses and hints used) during an in-class
activity. This formation of clusters of students with similar strengths and
weaknesses may help educators to improve their course planning by identifying
at-risk students, forming study groups, or encouraging tutoring between
students of different strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1"&gt;Ajay Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gkountouna_O/0/1/0/all/0/1"&gt;Olga Gkountouna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating smooth and sparse neural receptive fields with a flexible spline basis. (arXiv:2108.07537v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07537</id>
        <link href="http://arxiv.org/abs/2108.07537"/>
        <updated>2021-08-18T01:54:59.861Z</updated>
        <summary type="html"><![CDATA[Spatio-temporal receptive field (STRF) models are frequently used to
approximate the computation implemented by a sensory neuron. Typically, such
STRFs are assumed to be smooth and sparse. Current state-of-the-art approaches
for estimating STRFs based on empirical Bayes are often not computationally
efficient in high-dimensional settings, as encountered in sensory neuroscience.
Here we pursued an alternative approach and encode prior knowledge for
estimation of STRFs by choosing a set of basis functions with the desired
properties: natural cubic splines. Our method is computationally efficient and
can be easily applied to a wide range of existing models. We compared the
performance of spline-based methods to non-spline ones on simulated and
experimental data, showing that spline-based methods consistently outperform
the non-spline versions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ran_Y/0/1/0/all/0/1"&gt;Yanli Ran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oesterle_J/0/1/0/all/0/1"&gt;Jonathan Oesterle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Euler_T/0/1/0/all/0/1"&gt;Thomas Euler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1"&gt;Philipp Berens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Powerful is Graph Convolution for Recommendation?. (arXiv:2108.07567v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07567</id>
        <link href="http://arxiv.org/abs/2108.07567"/>
        <updated>2021-08-18T01:54:59.852Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) have recently enabled a popular class of
algorithms for collaborative filtering (CF). Nevertheless, the theoretical
underpinnings of their empirical successes remain elusive. In this paper, we
endeavor to obtain a better understanding of GCN-based CF methods via the lens
of graph signal processing. By identifying the critical role of smoothness, a
key concept in graph signal processing, we develop a unified graph
convolution-based framework for CF. We prove that many existing CF methods are
special cases of this framework, including the neighborhood-based methods,
low-rank matrix factorization, linear auto-encoders, and LightGCN,
corresponding to different low-pass filters. Based on our framework, we then
present a simple and computationally efficient CF baseline, which we shall
refer to as Graph Filter based Collaborative Filtering (GF-CF). Given an
implicit feedback matrix, GF-CF can be obtained in a closed form instead of
expensive training with back-propagation. Experiments will show that GF-CF
achieves competitive or better performance against deep learning-based methods
on three well-known datasets, notably with a $70\%$ performance gain over
LightGCN on the Amazon-book dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yifei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongji Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1"&gt;Caihua Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1"&gt;Khaled B. Letaief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCCAD: Graph Contrastive Coding for Anomaly Detection. (arXiv:2108.07516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07516</id>
        <link href="http://arxiv.org/abs/2108.07516"/>
        <updated>2021-08-18T01:54:59.845Z</updated>
        <summary type="html"><![CDATA[Graph-based anomaly detection has been widely used for detecting malicious
activities in real-world applications. Existing attempts to address this
problem have thus far focused on structural feature engineering or learning in
the binary classification regime. In this work, we propose to leverage graph
contrastive coding and present the supervised GCCAD model for contrasting
abnormal nodes with normal ones in terms of their distances to the global
context (e.g., the average of all nodes). To handle scenarios with scarce
labels, we further enable GCCAD as a self-supervised framework by designing a
graph corrupting strategy for generating synthetic node labels. To achieve the
contrastive objective, we design a graph neural network encoder that can infer
and further remove suspicious links during message passing, as well as learn
the global context of the input graph. We conduct extensive experiments on four
public datasets, demonstrating that 1) GCCAD significantly and consistently
outperforms various advanced baselines and 2) its self-supervised version
without fine-tuning can achieve comparable performance with its fully
supervised version.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaokang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jian Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaibo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1"&gt;Evgeny Kharlamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07493</id>
        <link href="http://arxiv.org/abs/2108.07493"/>
        <updated>2021-08-18T01:54:59.838Z</updated>
        <summary type="html"><![CDATA[It's challenging to customize transducer-based automatic speech recognition
(ASR) system with context information which is dynamic and unavailable during
model training. In this work, we introduce a light-weight contextual spelling
correction model to correct context-related recognition errors in
transducer-based ASR systems. We incorporate the context information into the
spelling correction model with a shared context encoder and use a filtering
algorithm to handle large-size context lists. Experiments show that the model
improves baseline ASR model performance with about 50% relative word error rate
reduction, which also significantly outperforms the baseline method such as
contextual LM biasing. The model also shows excellent performance for
out-of-vocabulary terms not seen during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RRLFSOR: An Efficient Self-Supervised Learning Strategy of Graph Convolutional Networks. (arXiv:2108.07481v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07481</id>
        <link href="http://arxiv.org/abs/2108.07481"/>
        <updated>2021-08-18T01:54:59.830Z</updated>
        <summary type="html"><![CDATA[To further improve the performance and the self-learning ability of GCNs, in
this paper, we propose an efficient self-supervised learning strategy of GCNs,
named randomly removed links with a fixed step at one region (RRLFSOR). In
addition, we also propose another self-supervised learning strategy of GCNs,
named randomly removing links with a fixed step at some blocks (RRLFSSB), to
solve the problem that adjacent nodes have no selected step. Experiments on
transductive link prediction tasks show that our strategies outperform the
baseline models consistently by up to 21.34% in terms of accuracy on three
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Feng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+V_A/0/1/0/all/0/1"&gt;Ajith Kumar V&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guanci Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qikui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiyun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Ansi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makwana_D/0/1/0/all/0/1"&gt;Dhruv Makwana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Generalization for Randomized Coordinate Descent. (arXiv:2108.07414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07414</id>
        <link href="http://arxiv.org/abs/2108.07414"/>
        <updated>2021-08-18T01:54:59.803Z</updated>
        <summary type="html"><![CDATA[Randomized coordinate descent (RCD) is a popular optimization algorithm with
wide applications in solving various machine learning problems, which motivates
a lot of theoretical analysis on its convergence behavior. As a comparison,
there is no work studying how the models trained by RCD would generalize to
test examples. In this paper, we initialize the generalization analysis of RCD
by leveraging the powerful tool of algorithmic stability. We establish argument
stability bounds of RCD for both convex and strongly convex objectives, from
which we develop optimal generalization bounds by showing how to early-stop the
algorithm to tradeoff the estimation and optimization. Our analysis shows that
RCD enjoys better stability as compared to stochastic gradient descent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yunwen Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in Sequential Recommendation. (arXiv:2108.07505v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07505</id>
        <link href="http://arxiv.org/abs/2108.07505"/>
        <updated>2021-08-18T01:54:59.797Z</updated>
        <summary type="html"><![CDATA[Successful sequential recommendation systems rely on accurately capturing the
user's short-term and long-term interest. Although Transformer-based models
achieved state-of-the-art performance in the sequential recommendation task,
they generally require quadratic memory and time complexity to the sequence
length, making it difficult to extract the long-term interest of users. On the
other hand, Multi-Layer Perceptrons (MLP)-based models, renowned for their
linear memory and time complexity, have recently shown competitive results
compared to Transformer in various tasks. Given the availability of a massive
amount of the user's behavior history, the linear memory and time complexity of
MLP-based models make them a promising alternative to explore in the sequential
recommendation task. To this end, we adopted MLP-based models in sequential
recommendation but consistently observed that MLP-based methods obtain lower
performance than those of Transformer despite their computational benefits.
From experiments, we observed that introducing explicit high-order interactions
to MLP layers mitigates such performance gap. In response, we propose the
Multi-Order Interaction (MOI) layer, which is capable of expressing an
arbitrary order of interactions within the inputs while maintaining the memory
and time complexity of the MLP layer. By replacing the MLP layer with the MOI
layer, our model was able to achieve comparable performance with
Transformer-based models while retaining the MLP-based models' computational
benefits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hojoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Dongyoon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sunghwan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Uncertainty in Learning to Defer Algorithms for Safe Computer-Aided Diagnosis. (arXiv:2108.07392v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07392</id>
        <link href="http://arxiv.org/abs/2108.07392"/>
        <updated>2021-08-18T01:54:59.784Z</updated>
        <summary type="html"><![CDATA[In this study we propose the Learning to Defer with Uncertainty (LDU)
algorithm, an approach which considers the model's predictive uncertainty when
identifying the patient group to be evaluated by human experts. Our aim is to
ensure patient safety when ML models are deployed in healthcare settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jessie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallego_B/0/1/0/all/0/1"&gt;Blanca Gallego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbieri_S/0/1/0/all/0/1"&gt;Sebastiano Barbieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neonatal Bowel Sound Detection Using Convolutional Neural Network and Laplace Hidden Semi-Markov Model. (arXiv:2108.07467v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.07467</id>
        <link href="http://arxiv.org/abs/2108.07467"/>
        <updated>2021-08-18T01:54:59.742Z</updated>
        <summary type="html"><![CDATA[Abdominal auscultation is a convenient, safe and inexpensive method to assess
bowel conditions, which is essential in neonatal care. It helps early detection
of neonatal bowel dysfunctions and allows timely intervention. This paper
presents a neonatal bowel sound detection method to assist the auscultation.
Specifically, a Convolutional Neural Network (CNN) is proposed to classify
peristalsis and non-peristalsis sounds. The classification is then optimized
using a Laplace Hidden Semi-Markov Model (HSMM). The proposed method is
validated on abdominal sounds from 49 newborn infants admitted to our tertiary
Neonatal Intensive Care Unit (NICU). The results show that the method can
effectively detect bowel sounds with accuracy and area under curve (AUC) score
being 89.81% and 83.96% respectively, outperforming 13 baseline methods.
Furthermore, the proposed Laplace HSMM refinement strategy is proven capable to
enhance other bowel sound detection models. The outcomes of this work have the
potential to facilitate future telehealth applications for neonatal care. The
source code of our work can be found at:
https://bitbucket.org/chirudeakin/neonatal-bowel-sound-classification/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1"&gt;Chiranjibi Sitaula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jinyuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyadarshi_A/0/1/0/all/0/1"&gt;Archana Priyadarshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tracy_M/0/1/0/all/0/1"&gt;Mark Tracy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kavehei_O/0/1/0/all/0/1"&gt;Omid Kavehei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinder_M/0/1/0/all/0/1"&gt;Murray Hinder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Withana_A/0/1/0/all/0/1"&gt;Anusha Withana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEwan_A/0/1/0/all/0/1"&gt;Alistair McEwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzbanrad_F/0/1/0/all/0/1"&gt;Faezeh Marzbanrad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Correlated Data: Taming the Tail for Age-Optimal Industrial IoT. (arXiv:2108.07504v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07504</id>
        <link href="http://arxiv.org/abs/2108.07504"/>
        <updated>2021-08-18T01:54:59.732Z</updated>
        <summary type="html"><![CDATA[While information delivery in industrial Internet of things demands
reliability and latency guarantees, the freshness of the controller's available
information, measured by the age of information (AoI), is paramount for
high-performing industrial automation. The problem in this work is cast as a
sensor's transmit power minimization subject to the peak-AoI requirement and a
probabilistic constraint on queuing latency. We further characterize the tail
behavior of the latency by a generalized Pareto distribution (GPD) for solving
the power allocation problem through Lyapunov optimization. As each sensor
utilizes its own data to locally train the GPD model, we incorporate federated
learning and propose a local-model selection approach which accounts for
correlation among the sensor's training data. Numerical results show the
tradeoff between the transmit power, peak AoI, and delay's tail distribution.
Furthermore, we verify the superiority of the proposed correlation-aware
approach for selecting the local models in federated learning over an existing
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chen-Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diagnosis of Acute Myeloid Leukaemia Using Machine Learning. (arXiv:2108.07396v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07396</id>
        <link href="http://arxiv.org/abs/2108.07396"/>
        <updated>2021-08-18T01:54:59.695Z</updated>
        <summary type="html"><![CDATA[We train a machine learning model on a dataset of 2177 individuals using as
features 26 probe sets and their age in order to classify if someone has acute
myeloid leukaemia or is healthy. The dataset is multicentric and consists of
data from 27 organisations, 25 cities, 15 countries and 4 continents. The
accuracy or our model is 99.94\% and its F1-score 0.9996. To the best of our
knowledge the performance of our model is the best one in the literature, as
regards the prediction of AML using similar or not data. Moreover, there has
not been any bibliographic reference associated with acute myeloid leukaemia
for the 26 probe sets we used as features in our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angelakis_A/0/1/0/all/0/1"&gt;A. Angelakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulioti_I/0/1/0/all/0/1"&gt;I. Soulioti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Compute Approximate Nash Equilibrium for Normal-form Games. (arXiv:2108.07472v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2108.07472</id>
        <link href="http://arxiv.org/abs/2108.07472"/>
        <updated>2021-08-18T01:54:59.657Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a general meta learning approach to computing
approximate Nash equilibrium for finite $n$-player normal-form games. Unlike
existing solutions that approximate or learn a Nash equilibrium from scratch
for each of the games, our meta solver directly constructs a mapping from a
game utility matrix to a joint strategy profile. The mapping is parameterized
and learned in a self-supervised fashion by a proposed Nash equilibrium
approximation metric without ground truth data informing any Nash equilibrium.
As such, it can immediately predict the joint strategy profile that
approximates a Nash equilibrium for any unseen new game under the same game
distribution. Moreover, the meta-solver can be further fine-tuned and adaptive
to a new game if iteration updates are allowed. We theoretically prove that our
meta-solver is not affected by the non-smoothness of exact Nash equilibrium
solutions, and derive a sample complexity bound to demonstrate its
generalization ability across normal-form games. Experimental results
demonstrate its substantial approximation power against other strong baselines
in both adaptive and non-adaptive cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Zhijian Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yali Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiaotie Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From the Greene--Wu Convolution to Gradient Estimation over Riemannian Manifolds. (arXiv:2108.07406v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07406</id>
        <link href="http://arxiv.org/abs/2108.07406"/>
        <updated>2021-08-18T01:54:59.577Z</updated>
        <summary type="html"><![CDATA[Over a complete Riemannian manifold of finite dimension, Greene and Wu
introduced a convolution, known as Greene-Wu (GW) convolution. In this paper,
we introduce a reformulation of the GW convolution. Using our reformulation,
many properties of the GW convolution can be easily derived, including a new
formula for how the curvature of the space would affect the curvature of the
function through the GW convolution. Also enabled by our new reformulation, an
improved method for gradient estimation over Riemannian manifolds is
introduced. Theoretically, our gradient estimation method improves the order of
estimation error from $O \left( \left( n + 3 \right)^{3/2} \right)$ to $O
\left( n^{3/2} \right)$, where $n$ is the dimension of the manifold.
Empirically, our method outperforms the best existing method for gradient
estimation over Riemannian manifolds, as evidenced by thorough experimental
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Didong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Secure and Practical Machine Learning via Secret Sharing and Random Permutation. (arXiv:2108.07463v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07463</id>
        <link href="http://arxiv.org/abs/2108.07463"/>
        <updated>2021-08-18T01:54:59.550Z</updated>
        <summary type="html"><![CDATA[With the increasing demands for privacy protection, privacy-preserving
machine learning has been drawing much attention in both academia and industry.
However, most existing methods have their limitations in practical
applications. On the one hand, although most cryptographic methods are provable
secure, they bring heavy computation and communication. On the other hand, the
security of many relatively efficient private methods (e.g., federated learning
and split learning) is being questioned, since they are non-provable secure.
Inspired by previous work on privacy-preserving machine learning, we build a
privacy-preserving machine learning framework by combining random permutation
and arithmetic secret sharing via our compute-after-permutation technique.
Since our method reduces the cost for element-wise function computation, it is
more efficient than existing cryptographic methods. Moreover, by adopting
distance correlation as a metric for privacy leakage, we demonstrate that our
method is more secure than previous non-provable secure methods. Overall, our
proposal achieves a good balance between security and efficiency. Experimental
results show that our method not only is up to 6x faster and reduces up to 85%
network traffic compared with state-of-the-art cryptographic methods, but also
leaks less privacy during the training process compared with non-provable
secure methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Fei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaochao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaolin Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07435</id>
        <link href="http://arxiv.org/abs/2108.07435"/>
        <updated>2021-08-18T01:54:59.531Z</updated>
        <summary type="html"><![CDATA[Protein is linked to almost every life process. Therefore, analyzing the
biological structure and property of protein sequences is critical to the
exploration of life, as well as disease detection and drug discovery.
Traditional protein analysis methods tend to be labor-intensive and
time-consuming. The emergence of deep learning models makes modeling data
patterns in large quantities of data possible. Interdisciplinary researchers
have begun to leverage deep learning methods to model large biological
datasets, e.g. using long short-term memory and convolutional neural network
for protein sequence classification. After millions of years of evolution,
evolutionary information is encoded in protein sequences. Inspired by the
similarity between natural language and protein sequences, we use large-scale
language models to model evolutionary-scale protein sequences, encoding protein
biology information in representation. Significant improvements are observed in
both token-level and sequence-level tasks, demonstrating that our large-scale
model can accurately capture evolution information from pretraining on
evolutionary-scale individual sequences. Our code and model are available at
https://github.com/THUDM/ProteinLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yijia Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Chang-Yu Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Efficient Factorization Machines via Binarizing both Data and Model Coefficients. (arXiv:2108.07421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07421</id>
        <link href="http://arxiv.org/abs/2108.07421"/>
        <updated>2021-08-18T01:54:59.491Z</updated>
        <summary type="html"><![CDATA[Factorization Machines (FM), a general predictor that can efficiently model
feature interactions in linear time, was primarily proposed for collaborative
recommendation and have been broadly used for regression, classification and
ranking tasks. Subspace Encoding Factorization Machine (SEFM) has been proposed
recently to overcome the expressiveness limitation of Factorization Machines
(FM) by applying explicit nonlinear feature mapping for both individual
features and feature interactions through one-hot encoding to each input
feature. Despite the effectiveness of SEFM, it increases the memory cost of FM
by $b$ times, where $b$ is the number of bins when applying one-hot encoding on
each input feature. To reduce the memory cost of SEFM, we propose a new method
called Binarized FM which constraints the model parameters to be binary values
(i.e., 1 or $-1$). Then each parameter value can be efficiently stored in one
bit. Our proposed method can significantly reduce the memory cost of SEFM
model. In addition, we propose a new algorithm to effectively and efficiently
learn proposed FM with binary constraints using Straight Through Estimator
(STE) with Adaptive Gradient Descent (Adagrad). Finally, we evaluate the
performance of our proposed method on eight different classification datasets.
Our experimental results have demonstrated that our proposed method achieves
comparable accuracy with SEFM but with much less memory cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1"&gt;Yu Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1"&gt;Liang Lan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Convolutional Neural Networks. (arXiv:2108.07387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.07387</id>
        <link href="http://arxiv.org/abs/2108.07387"/>
        <updated>2021-08-18T01:54:59.481Z</updated>
        <summary type="html"><![CDATA[We propose contextual convolution (CoConv) for visual recognition. CoConv is
a direct replacement of the standard convolution, which is the core component
of convolutional neural networks. CoConv is implicitly equipped with the
capability of incorporating contextual information while maintaining a similar
number of parameters and computational cost compared to the standard
convolution. CoConv is inspired by neuroscience studies indicating that (i)
neurons, even from the primary visual cortex (V1 area), are involved in
detection of contextual cues and that (ii) the activity of a visual neuron can
be influenced by the stimuli placed entirely outside of its theoretical
receptive field. On the one hand, we integrate CoConv in the widely-used
residual networks and show improved recognition performance over baselines on
the core tasks and benchmarks for visual recognition, namely image
classification on the ImageNet data set and object detection on the MS COCO
data set. On the other hand, we introduce CoConv in the generator of a
state-of-the-art Generative Adversarial Network, showing improved generative
results on CIFAR-10 and CelebA. Our code is available at
https://github.com/iduta/coconv.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duta_I/0/1/0/all/0/1"&gt;Ionut Cosmin Duta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1"&gt;Mariana Iuliana Georgescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Biased Subgroups in Ranking and Classification. (arXiv:2108.07450v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07450</id>
        <link href="http://arxiv.org/abs/2108.07450"/>
        <updated>2021-08-18T01:54:59.474Z</updated>
        <summary type="html"><![CDATA[When analyzing the behavior of machine learning algorithms, it is important
to identify specific data subgroups for which the considered algorithm shows
different performance with respect to the entire dataset. The intervention of
domain experts is normally required to identify relevant attributes that define
these subgroups.

We introduce the notion of divergence to measure this performance difference
and we exploit it in the context of (i) classification models and (ii) ranking
applications to automatically detect data subgroups showing a significant
deviation in their behavior. Furthermore, we quantify the contribution of all
attributes in the data subgroup to the divergent behavior by means of Shapley
values, thus allowing the identification of the most impacting attributes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1"&gt;Eliana Pastor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfaro_L/0/1/0/all/0/1"&gt;Luca de Alfaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1"&gt;Elena Baralis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification. (arXiv:2108.07464v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07464</id>
        <link href="http://arxiv.org/abs/2108.07464"/>
        <updated>2021-08-18T01:54:59.466Z</updated>
        <summary type="html"><![CDATA[Data labeling in supervised learning is considered an expensive and
infeasible tool in some conditions. The self-supervised learning method is
proposed to tackle the learning effectiveness with fewer labeled data, however,
there is a lack of confidence in the size of labeled data needed to achieve
adequate results. This study aims to draw a baseline on the proportion of the
labeled data that models can appreciate to yield competent accuracy when
compared to training with additional labels. The study implements the
kaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the
self-supervised learning task by implementing random rotations augmentation on
the original datasets. To reveal the true effectiveness of the pretext process
in self-supervised learning, the original dataset is divided into smaller
batches, and learning is repeated on each batch with and without the pretext
pre-training. Results show that the pretext process in the self-supervised
learning improves the accuracy around 15% in the downstream classification task
when compared to the plain supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+AlQuabeh_H/0/1/0/all/0/1"&gt;Hilal AlQuabeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bawazeer_A/0/1/0/all/0/1"&gt;Ameera Bawazeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhashmi_A/0/1/0/all/0/1"&gt;Abdulateef Alhashmi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregation Delayed Federated Learning. (arXiv:2108.07433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07433</id>
        <link href="http://arxiv.org/abs/2108.07433"/>
        <updated>2021-08-18T01:54:59.457Z</updated>
        <summary type="html"><![CDATA[Federated learning is a distributed machine learning paradigm where multiple
data owners (clients) collaboratively train one machine learning model while
keeping data on their own devices. The heterogeneity of client datasets is one
of the most important challenges of federated learning algorithms. Studies have
found performance reduction with standard federated algorithms, such as FedAvg,
on non-IID data. Many existing works on handling non-IID data adopt the same
aggregation framework as FedAvg and focus on improving model updates either on
the server side or on clients. In this work, we tackle this challenge in a
different view by introducing redistribution rounds that delay the aggregation.
We perform experiments on multiple tasks and show that the proposed framework
significantly improves the performance on non-IID data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Ye Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1"&gt;Diego Klabjan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Common Waveforms Including a Watchdog for Unknown Signals. (arXiv:2108.07339v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.07339</id>
        <link href="http://arxiv.org/abs/2108.07339"/>
        <updated>2021-08-18T01:54:59.435Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the use of a deep multi-layer perceptron model
architecture to classify received signal samples as coming from one of four
common waveforms, Single Carrier (SC), Single-Carrier Frequency Division
Multiple Access (SC-FDMA), Orthogonal Frequency Division Multiplexing (OFDM),
and Linear Frequency Modulation (LFM), used in communication and radar
networks. Synchronization of the signals is not needed as we assume there is an
unknown and uncompensated time and frequency offset. An autoencoder with a deep
CNN architecture is also examined to create a new fifth classification category
of an unknown waveform type. This is accomplished by calculating a minimum and
maximum threshold values from the root mean square error (RMSE) of the radar
and communication waveforms. The classifier and autoencoder work together to
monitor a spectrum area to identify the common waveforms inside the area of
operation along with detecting unknown waveforms. Results from testing showed
the classifier had 100\% classification rate above 0 dB with accuracy of 83.2\%
and 94.7\% at -10 dB and -5 dB, respectively, with signal impairments present.
Results for the anomaly detector showed 85.3\% accuracy at 0 dB with 100\% at
SNR greater than 0 dB with signal impairments present when using a high-value
Fast Fourier Transform (FFT) size. Accurate detection rates decline as
additional noise is introduced to the signals, with 78.1\% at -5 dB and 56.5\%
at -10 dB. However, these low rates seen can be potentially mitigated by using
even higher FFT sizes also shown in our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fredieu_C/0/1/0/all/0/1"&gt;C. Tanner Fredieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bui_J/0/1/0/all/0/1"&gt;Justin Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Martone_A/0/1/0/all/0/1"&gt;Anthony Martone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marks_R/0/1/0/all/0/1"&gt;Robert J. Marks II&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baylis_C/0/1/0/all/0/1"&gt;Charles Baylis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buehrer_R/0/1/0/all/0/1"&gt;R. Michael Buehrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Deep Learning Approach for Epileptic Seizure Prediction. (arXiv:2108.07453v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.07453</id>
        <link href="http://arxiv.org/abs/2108.07453"/>
        <updated>2021-08-18T01:54:59.425Z</updated>
        <summary type="html"><![CDATA[An accurate seizure prediction system enables early warnings before seizure
onset of epileptic patients. It is extremely important for drug-refractory
patients. Conventional seizure prediction works usually rely on features
extracted from Electroencephalography (EEG) recordings and classification
algorithms such as regression or support vector machine (SVM) to locate the
short time before seizure onset. However, such methods cannot achieve
high-accuracy prediction due to information loss of the hand-crafted features
and the limited classification ability of regression and SVM algorithms. We
propose an end-to-end deep learning solution using a convolutional neural
network (CNN) in this paper. One and two dimensional kernels are adopted in the
early- and late-stage convolution and max-pooling layers, respectively. The
proposed CNN model is evaluated on Kaggle intracranial and CHB-MIT scalp EEG
datasets. Overall sensitivity, false prediction rate, and area under receiver
operating characteristic curve reaches 93.5%, 0.063/h, 0.981 and 98.8%,
0.074/h, 0.988 on two datasets respectively. Comparison with state-of-the-art
works indicates that the proposed model achieves exceeding prediction
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yankun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shiqi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hemmings Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sawan_M/0/1/0/all/0/1"&gt;Mohamad Sawan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing. (arXiv:2108.07386v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07386</id>
        <link href="http://arxiv.org/abs/2108.07386"/>
        <updated>2021-08-18T01:54:59.416Z</updated>
        <summary type="html"><![CDATA[Computerized adaptive testing (CAT) refers to a form of tests that are
personalized to every student/test taker. CAT methods adaptively select the
next most informative question/item for each student given their responses to
previous questions, effectively reducing test length. Existing CAT methods use
item response theory (IRT) models to relate student ability to their responses
to questions and static question selection algorithms designed to reduce the
ability estimation error as quickly as possible; therefore, these algorithms
cannot improve by learning from large-scale student response data. In this
paper, we propose BOBCAT, a Bilevel Optimization-Based framework for CAT to
directly learn a data-driven question selection algorithm from training data.
BOBCAT is agnostic to the underlying student response model and is
computationally efficient during the adaptive testing process. Through
extensive experiments on five real-world student response datasets, we show
that BOBCAT outperforms existing CAT methods (sometimes significantly) at
reducing test length.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Aritra Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1"&gt;Andrew Lan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoGram and Admissible Machine Learning. (arXiv:2108.07380v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.07380</id>
        <link href="http://arxiv.org/abs/2108.07380"/>
        <updated>2021-08-18T01:54:59.410Z</updated>
        <summary type="html"><![CDATA[We have entered a new era of machine learning (ML), where the most accurate
algorithm with superior predictive power may not even be deployable, unless it
is admissible under the regulatory constraints. This has led to great interest
in developing fair, transparent and trustworthy ML methods. The purpose of this
article is to introduce a new information-theoretic learning framework
(admissible machine learning) and algorithmic risk-management tools (InfoGram,
L-features, ALFA-testing) that can guide an analyst to redesign off-the-shelf
ML methods to be regulatory compliant, while maintaining good prediction
accuracy. We have illustrated our approach using several real-data examples
from financial sectors, biomedical research, marketing campaigns, and the
criminal justice system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mukhopadhyay_S/0/1/0/all/0/1"&gt;Subhadeep Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FARF: A Fair and Adaptive Random Forests Classifier. (arXiv:2108.07403v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07403</id>
        <link href="http://arxiv.org/abs/2108.07403"/>
        <updated>2021-08-18T01:54:59.402Z</updated>
        <summary type="html"><![CDATA[As Artificial Intelligence (AI) is used in more applications, the need to
consider and mitigate biases from the learned models has followed. Most works
in developing fair learning algorithms focus on the offline setting. However,
in many real-world applications data comes in an online fashion and needs to be
processed on the fly. Moreover, in practical application, there is a trade-off
between accuracy and fairness that needs to be accounted for, but current
methods often have multiple hyperparameters with non-trivial interaction to
achieve fairness. In this paper, we propose a flexible ensemble algorithm for
fair decision-making in the more challenging context of evolving online
settings. This algorithm, called FARF (Fair and Adaptive Random Forests), is
based on using online component classifiers and updating them according to the
current distribution, that also accounts for fairness and a single
hyperparameters that alters fairness-accuracy balance. Experiments on
real-world discriminated data streams demonstrate the utility of FARF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1"&gt;Albert Bifet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1"&gt;Jeremy C. Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1"&gt;Wolfgang Nejdl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Cluster via Same-Cluster Queries. (arXiv:2108.07383v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07383</id>
        <link href="http://arxiv.org/abs/2108.07383"/>
        <updated>2021-08-18T01:54:59.380Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning to cluster data points using an oracle which
can answer same-cluster queries. Different from previous approaches, we do not
assume that the total number of clusters is known at the beginning and do not
require that the true clusters are consistent with a predefined objective
function such as the K-means. These relaxations are critical from the practical
perspective and, meanwhile, make the problem more challenging. We propose two
algorithms with provable theoretical guarantees and verify their effectiveness
via an extensive set of experiments on both synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Classification Using Group-Level Labels. (arXiv:2108.07330v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07330</id>
        <link href="http://arxiv.org/abs/2108.07330"/>
        <updated>2021-08-18T01:54:59.372Z</updated>
        <summary type="html"><![CDATA[In many applications, finding adequate labeled data to train predictive
models is a major challenge. In this work, we propose methods to use
group-level binary labels as weak supervision to train instance-level binary
classification models. Aggregate labels are common in several domains where
annotating on a group-level might be cheaper or might be the only way to
provide annotated data without infringing on privacy. We model group-level
labels as Class Conditional Noisy (CCN) labels for individual instances and use
the noisy labels to regularize predictions of the model trained on the
strongly-labeled instances. Our experiments on real-world application of land
cover mapping shows the utility of the proposed method in leveraging
group-level labels, both in the presence and absence of class imbalance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1"&gt;Guruprasad Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Rahul Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizing Pareto-Optimal Interpretations for Black-Box Models. (arXiv:2108.07307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07307</id>
        <link href="http://arxiv.org/abs/2108.07307"/>
        <updated>2021-08-18T01:54:59.364Z</updated>
        <summary type="html"><![CDATA[We present a new multi-objective optimization approach for synthesizing
interpretations that "explain" the behavior of black-box machine learning
models. Constructing human-understandable interpretations for black-box models
often requires balancing conflicting objectives. A simple interpretation may be
easier to understand for humans while being less precise in its predictions
vis-a-vis a complex interpretation. Existing methods for synthesizing
interpretations use a single objective function and are often optimized for a
single class of interpretations. In contrast, we provide a more general and
multi-objective synthesis framework that allows users to choose (1) the class
of syntactic templates from which an interpretation should be synthesized, and
(2) quantitative measures on both the correctness and explainability of an
interpretation. For a given black-box, our approach yields a set of
Pareto-optimal interpretations with respect to the correctness and
explainability measures. We show that the underlying multi-objective
optimization problem can be solved via a reduction to quantitative constraint
solving, such as weighted maximum satisfiability. To demonstrate the benefits
of our approach, we have applied it to synthesize interpretations for black-box
neural-network classifiers. Our experiments show that there often exists a rich
and varied set of choices for interpretations that are missed by existing
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Torfah_H/0/1/0/all/0/1"&gt;Hazem Torfah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shetal Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Supratik Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akshay_S/0/1/0/all/0/1"&gt;S. Akshay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1"&gt;Sanjit A. Seshia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07344</id>
        <link href="http://arxiv.org/abs/2108.07344"/>
        <updated>2021-08-18T01:54:59.357Z</updated>
        <summary type="html"><![CDATA[The recent success of distributed word representations has led to an
increased interest in analyzing the properties of their spatial distribution.
Current metrics suggest that contextualized word embedding models do not
uniformly utilize all dimensions when embedding tokens in vector space. Here we
argue that existing metrics are fragile and tend to obfuscate the true spatial
distribution of point clouds. To ameliorate this issue, we propose IsoScore: a
novel metric which quantifies the degree to which a point cloud uniformly
utilizes the ambient vector space. We demonstrate that IsoScore has several
desirable properties such as mean invariance and direct correspondence to the
number of dimensions used, which are properties that existing scores do not
possess. Furthermore, IsoScore is conceptually intuitive and computationally
efficient, making it well suited for analyzing the distribution of point clouds
in arbitrary vector spaces, not necessarily limited to those of word embeddings
alone. Additionally, we use IsoScore to demonstrate that a number of recent
conclusions in the NLP literature that have been derived using brittle metrics
of spatial distribution, such as average cosine similarity, may be incomplete
or altogether inaccurate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1"&gt;William Rudman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1"&gt;Nate Gillman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1"&gt;Taylor Rayne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tuning is Fine in Federated Learning. (arXiv:2108.07313v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07313</id>
        <link href="http://arxiv.org/abs/2108.07313"/>
        <updated>2021-08-18T01:54:59.348Z</updated>
        <summary type="html"><![CDATA[We study the performance of federated learning algorithms and their variants
in an asymptotic framework. Our starting point is the formulation of federated
learning as a multi-criterion objective, where the goal is to minimize each
client's loss using information from all of the clients. We propose a linear
regression model, where, for a given client, we theoretically compare the
performance of various algorithms in the high-dimensional asymptotic limit.
This asymptotic multi-criterion approach naturally models the high-dimensional,
many-device nature of federated learning and suggests that personalization is
central to federated learning. Our theory suggests that Fine-tuned Federated
Averaging (FTFA), i.e., Federated Averaging followed by local training, and the
ridge regularized variant Ridge-tuned Federated Averaging (RTFA) are
competitive with more sophisticated meta-learning and proximal-regularized
approaches. In addition to being conceptually simpler, FTFA and RTFA are
computationally more efficient than its competitors. We corroborate our
theoretical claims with extensive experiments on federated versions of the
EMNIST, CIFAR-100, Shakespeare, and Stack Overflow datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Gary Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_K/0/1/0/all/0/1"&gt;Karan Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1"&gt;John Duchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06423</id>
        <link href="http://arxiv.org/abs/2101.06423"/>
        <updated>2021-08-18T01:54:59.339Z</updated>
        <summary type="html"><![CDATA[Neural text matching models have been widely used in community question
answering, information retrieval, and dialogue. However, these models designed
for short texts cannot well address the long-form text matching problem,
because there are many contexts in long-form texts can not be directly aligned
with each other, and it is difficult for existing models to capture the key
matching signals from such noisy data. Besides, these models are
computationally expensive for simply use all textual data indiscriminately. To
tackle the effectiveness and efficiency problem, we propose a novel
hierarchical noise filtering model, namely Match-Ignition. The main idea is to
plug the well-known PageRank algorithm into the Transformer, to identify and
filter both sentence and word level noisy information in the matching process.
Noisy sentences are usually easy to detect because previous work has shown that
their similarity can be explicitly evaluated by the word overlapping, so we
directly use PageRank to filter such information based on a sentence similarity
graph. Unlike sentences, words rely on their contexts to express concrete
meanings, so we propose to jointly learn the filtering and matching process, to
well capture the critical word-level matching signals. Specifically, a word
graph is first built based on the attention scores in each self-attention block
of Transformer, and key words are then selected by applying PageRank on this
graph. In this way, noisy words will be filtered out layer by layer in the
matching process. Experimental results show that Match-Ignition outperforms
both SOTA short text matching models and recent long-form text matching models.
We also conduct detailed analysis to show that Match-Ignition efficiently
captures important sentences and words, to facilitate the long-form text
matching process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1"&gt;Liang Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic optimization under time drift: iterate averaging, step decay, and high probability guarantees. (arXiv:2108.07356v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.07356</id>
        <link href="http://arxiv.org/abs/2108.07356"/>
        <updated>2021-08-18T01:54:59.316Z</updated>
        <summary type="html"><![CDATA[We consider the problem of minimizing a convex function that is evolving in
time according to unknown and possibly stochastic dynamics. Such problems
abound in the machine learning and signal processing literature, under the
names of concept drift and stochastic tracking. We provide novel non-asymptotic
convergence guarantees for stochastic algorithms with iterate averaging,
focusing on bounds valid both in expectation and with high probability.
Notably, we show that the tracking efficiency of the proximal stochastic
gradient method depends only logarithmically on the initialization quality,
when equipped with a step-decay schedule. The results moreover naturally extend
to settings where the dynamics depend jointly on time and on the decision
variable itself, as in the performative prediction framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cutler_J/0/1/0/all/0/1"&gt;Joshua Cutler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1"&gt;Dmitriy Drusvyatskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterotic String Model Building with Monad Bundles and Reinforcement Learning. (arXiv:2108.07316v1 [hep-th])]]></title>
        <id>http://arxiv.org/abs/2108.07316</id>
        <link href="http://arxiv.org/abs/2108.07316"/>
        <updated>2021-08-18T01:54:59.307Z</updated>
        <summary type="html"><![CDATA[We use reinforcement learning as a means of constructing string
compactifications with prescribed properties. Specifically, we study heterotic
SO(10) GUT models on Calabi-Yau three-folds with monad bundles, in search of
phenomenologically promising examples. Due to the vast number of bundles and
the sparseness of viable choices, methods based on systematic scanning are not
suitable for this class of models. By focusing on two specific manifolds with
Picard numbers two and three, we show that reinforcement learning can be used
successfully to explore monad bundles. Training can be accomplished with
minimal computing resources and leads to highly efficient policy networks. They
produce phenomenologically promising states for nearly 100% of episodes and
within a small number of steps. In this way, hundreds of new candidate standard
models are found.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Constantin_A/0/1/0/all/0/1"&gt;Andrei Constantin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Harvey_T/0/1/0/all/0/1"&gt;Thomas R. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Lukas_A/0/1/0/all/0/1"&gt;Andre Lukas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.02605</id>
        <link href="http://arxiv.org/abs/2108.02605"/>
        <updated>2021-08-18T01:54:59.299Z</updated>
        <summary type="html"><![CDATA[This report presents the results of the EENLP project, done as a part of EEML
2021 summer school.

It presents a broad index of NLP resources for Eastern European languages,
which, we hope, could be helpful for the NLP community; several new
hand-crafted cross-lingual datasets focused on Eastern European languages, and
a sketch evaluation of cross-lingual transfer learning abilities of several
modern multilingual Transformer-based models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1"&gt;Alex Malkhasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1"&gt;Andrey Manoshin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1"&gt;George Dima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1"&gt;R&amp;#xe9;ka Cserh&amp;#xe1;ti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;Md.Sadek Hossain Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1"&gt;Matt S&amp;#xe1;rdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Chemistry 101: Learning to Reason about Social and Moral Norms. (arXiv:2011.00620v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00620</id>
        <link href="http://arxiv.org/abs/2011.00620"/>
        <updated>2021-08-18T01:54:59.278Z</updated>
        <summary type="html"><![CDATA[Social norms -- the unspoken commonsense rules about acceptable social
behavior -- are crucial in understanding the underlying causes and intents of
people's actions in narratives. For example, underlying an action such as
"wanting to call cops on my neighbors" are social norms that inform our
conduct, such as "It is expected that you report crimes."

We present Social Chemistry, a new conceptual formalism to study people's
everyday social norms and moral judgments over a rich spectrum of real life
situations described in natural language. We introduce Social-Chem-101, a
large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run
a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further
broken down with 12 different dimensions of people's judgments, including
social judgments of good and bad, moral foundations, expected cultural
pressure, and assumed legality, which together amount to over 4.5 million
annotations of categorical labels and free-text descriptions.

Comprehensive empirical results based on state-of-the-art neural models
demonstrate that computational modeling of social norms is a promising research
direction. Our model framework, Neural Norm Transformer, learns and generalizes
Social-Chem-101 to successfully reason about previously unseen situations,
generating relevant (and potentially novel) attribute-aware social
rules-of-thumb.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1"&gt;Maxwell Forbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jena D. Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1"&gt;Vered Shwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1"&gt;Maarten Sap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.07253</id>
        <link href="http://arxiv.org/abs/2108.07253"/>
        <updated>2021-08-18T01:54:59.270Z</updated>
        <summary type="html"><![CDATA[We present a task and benchmark dataset for person-centric visual grounding,
the problem of linking between people named in a caption and people pictured in
an image. In contrast to prior work in visual grounding, which is predominantly
object-based, our new task masks out the names of people in captions in order
to encourage methods trained on such image-caption pairs to focus on contextual
cues (such as rich interactions between multiple people), rather than learning
associations between names and appearances. To facilitate this task, we
introduce a new dataset, Who's Waldo, mined automatically from image-caption
data on Wikimedia Commons. We propose a Transformer-based method that
outperforms several strong baselines on this task, and are releasing our data
to the research community to spur work on contextual models that consider both
vision and language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Claire Yuqing Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"&gt;Apoorv Khandelwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1"&gt;Yoav Artzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1"&gt;Noah Snavely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1"&gt;Hadar Averbuch-Elor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the factors driving the opioid epidemic using machine learning. (arXiv:2108.07301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07301</id>
        <link href="http://arxiv.org/abs/2108.07301"/>
        <updated>2021-08-18T01:54:59.255Z</updated>
        <summary type="html"><![CDATA[In recent years, the US has experienced an opioid epidemic with an
unprecedented number of drugs overdose deaths. Research finds such overdose
deaths are linked to neighborhood-level traits, thus providing opportunity to
identify effective interventions. Typically, techniques such as Ordinary Least
Squares (OLS) or Maximum Likelihood Estimation (MLE) are used to document
neighborhood-level factors significant in explaining such adverse outcomes.
These techniques are, however, less equipped to ascertain non-linear
relationships between confounding factors. Hence, in this study we apply
machine learning based techniques to identify opioid risks of neighborhoods in
Delaware and explore the correlation of these factors using Shapley Additive
explanations (SHAP). We discovered that the factors related to neighborhoods
environment, followed by education and then crime, were highly correlated with
higher opioid risk. We also explored the change in these correlations over the
years to understand the changing dynamics of the epidemic. Furthermore, we
discovered that, as the epidemic has shifted from legal (i.e., prescription
opioids) to illegal (e.g.,heroin and fentanyl) drugs in recent years, the
correlation of environment, crime and health related variables with the opioid
risk has increased significantly while the correlation of economic and
socio-demographic variables has decreased. The correlation of education related
factors has been higher from the start and has increased slightly in recent
years suggesting a need for increased awareness about the opioid epidemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gavali_S/0/1/0/all/0/1"&gt;Sachin Gavali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cowart_J/0/1/0/all/0/1"&gt;Julie Cowart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shanshan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cathy Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1"&gt;Tammy Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01377</id>
        <link href="http://arxiv.org/abs/2008.01377"/>
        <updated>2021-08-18T01:54:59.244Z</updated>
        <summary type="html"><![CDATA[Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a
key requirement for both linguistic research and subsequent automated natural
language processing (NLP) tasks. This problem is commonly tackled using machine
learning methods, i.e., by training a POS tagger on a sufficiently large corpus
of labeled data. While the problem of POS tagging can essentially be considered
as solved for modern languages, historical corpora turn out to be much more
difficult, especially due to the lack of native speakers and sparsity of
training data. Moreover, most texts have no sentences as we know them today,
nor a common orthography. These irregularities render the task of automated POS
tagging more difficult and error-prone. Under these circumstances, instead of
forcing the POS tagger to predict and commit to a single tag, it should be
enabled to express its uncertainty. In this paper, we consider POS tagging
within the framework of set-valued prediction, which allows the POS tagger to
express its uncertainty via predicting a set of candidate POS tags instead of
guessing a single one. The goal is to guarantee a high confidence that the
correct POS tag is included while keeping the number of candidates small. In
our experimental study, we find that extending state-of-the-art POS taggers to
set-valued prediction yields more precise and robust taggings, especially for
unknown words, i.e., words not occurring in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1"&gt;Stefan Heid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1"&gt;Marcel Wever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linguistic Resources for Bhojpuri, Magahi and Maithili: Statistics about them, their Similarity Estimates, and Baselines for Three Applications. (arXiv:2004.13945v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13945</id>
        <link href="http://arxiv.org/abs/2004.13945"/>
        <updated>2021-08-18T01:54:59.234Z</updated>
        <summary type="html"><![CDATA[Corpus preparation for low-resource languages and for development of human
language technology to analyze or computationally process them is a laborious
task, primarily due to the unavailability of expert linguists who are native
speakers of these languages and also due to the time and resources required.
Bhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in
the north-eastern parts), are low-resource languages belonging to the
Indo-Aryan (or Indic) family. They are closely related to Hindi, which is a
relatively high-resource language, which is why we compare with Hindi. We
collected corpora for these three languages from various sources and cleaned
them to the extent possible, without changing the data in them. The text
belongs to different domains and genres. We calculated some basic statistical
measures for these corpora at character, word, syllable, and morpheme levels.
These corpora were also annotated with parts-of-speech (POS) and chunk tags.
The basic statistical measures were both absolute and relative and were
exptected to indicate of linguistic properties such as morphological, lexical,
phonological, and syntactic complexities (or richness). The results were
compared with a standard Hindi corpus. For most of the measures, we tried to
the corpus size the same across the languages to avoid the effect of corpus
size, but in some cases it turned out that using the full corpus was better,
even if sizes were very different. Although the results are not very clear, we
try to draw some conclusions about the languages and the corpora. For POS
tagging and chunking, the BIS tagset was used to manually annotate the data.
The POS tagged data sizes are 16067, 14669 and 12310 sentences, respectively,
for Bhojpuri, Magahi and Maithili. The sizes for chunking are 9695 and 1954
sentences for Bhojpuri and Maithili, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mundotiya_R/0/1/0/all/0/1"&gt;Rajesh Kumar Mundotiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Manish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapur_R/0/1/0/all/0/1"&gt;Rahul Kapur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Swasti Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Anil Kumar Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MigrationsKB: A Knowledge Base of Public Attitudes towards Migrations and their Driving Factors. (arXiv:2108.07593v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07593</id>
        <link href="http://arxiv.org/abs/2108.07593"/>
        <updated>2021-08-18T01:54:59.141Z</updated>
        <summary type="html"><![CDATA[With the increasing trend in the topic of migration in Europe, the public is
now more engaged in expressing their opinions through various platforms such as
Twitter. Understanding the online discourses is therefore essential to capture
the public opinion. The goal of this study is the analysis of social media
platform to quantify public attitudes towards migrations and the identification
of different factors causing these attitudes. The tweets spanning from 2013 to
Jul-2021 in the European countries which are hosts to immigrants are collected,
pre-processed, and filtered using advanced topic modeling technique. BERT-based
entity linking and sentiment analysis, and attention-based hate speech
detection are performed to annotate the curated tweets. Moreover, the external
databases are used to identify the potential social and economic factors
causing negative attitudes of the people about migration. To further promote
research in the interdisciplinary fields of social science and computer
science, the outcomes are integrated into a Knowledge Base (KB), i.e.,
MigrationsKB which significantly extends the existing models to take into
account the public attitudes towards migrations and the economic indicators.
This KB is made public using FAIR principles, which can be queried through
SPARQL endpoint. Data dumps are made available on Zenodo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1"&gt;Harald Sack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mehwish Alam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. (arXiv:2007.15207v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15207</id>
        <link href="http://arxiv.org/abs/2007.15207"/>
        <updated>2021-08-18T01:54:59.131Z</updated>
        <summary type="html"><![CDATA[Progress in cross-lingual modeling depends on challenging, realistic, and
diverse evaluation sets. We introduce Multilingual Knowledge Questions and
Answers (MKQA), an open-domain question answering evaluation set comprising 10k
question-answer pairs aligned across 26 typologically diverse languages (260k
question-answer pairs in total). Answers are based on a heavily curated,
language-independent data representation, making results comparable across
languages and independent of language-specific passages. With 26 languages,
this dataset supplies the widest range of languages to-date for evaluating
question answering. We benchmark a variety of state-of-the-art methods and
baselines for generative and extractive question answering, trained on Natural
Questions, in zero shot and translation settings. Results indicate this dataset
is challenging even in English, but especially in low-resource languages]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1"&gt;Shayne Longpre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daiber_J/0/1/0/all/0/1"&gt;Joachim Daiber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Game Interface to Study Semantic Grounding in Text-Based Models. (arXiv:2108.07708v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07708</id>
        <link href="http://arxiv.org/abs/2108.07708"/>
        <updated>2021-08-18T01:54:59.118Z</updated>
        <summary type="html"><![CDATA[Can language models learn grounded representations from text distribution
alone? This question is both central and recurrent in natural language
processing; authors generally agree that grounding requires more than textual
distribution. We propose to experimentally test this claim: if any two words
have different meanings and yet cannot be distinguished from distribution
alone, then grounding is out of the reach of text-based models. To that end, we
present early work on an online game for the collection of human judgments on
the distributional similarity of word pairs in five languages. We further
report early results of our data collection campaign.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1"&gt;Timothee Mickus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_M/0/1/0/all/0/1"&gt;Mathieu Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1"&gt;Denis Paperno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06132</id>
        <link href="http://arxiv.org/abs/2106.06132"/>
        <updated>2021-08-18T01:54:58.990Z</updated>
        <summary type="html"><![CDATA[Answering questions about why characters perform certain actions is central
to understanding and reasoning about narratives. Despite recent progress in QA,
it is not clear if existing models have the ability to answer "why" questions
that may require commonsense knowledge external to the input narrative. In this
work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more
than 30k questions and free-form answers concerning why characters in short
narratives perform the actions described. For a third of this dataset, the
answers are not present within the narrative. Given the limitations of
automated evaluation for this task, we also present a systematized human
evaluation interface for this dataset. Our evaluation of state-of-the-art
models show that they are far below human performance on answering such
questions. They are especially worse on questions whose answers are external to
the narrative, thus providing a challenge for future QA and narrative
understanding research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1"&gt;Yash Kumar Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1"&gt;Nathanael Chambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weak Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07638</id>
        <link href="http://arxiv.org/abs/2108.07638"/>
        <updated>2021-08-18T01:54:58.969Z</updated>
        <summary type="html"><![CDATA[Affective Computing is the study of how computers can recognize, interpret
and simulate human affects. Sentiment Analysis is a common task in NLP related
to this topic, but it focuses only on emotion valence (positive, negative,
neutral). An emerging approach in NLP is Emotion Recognition, which relies on
fined-grained classification. This research describes an approach to create a
lexical-based weak supervised corpus for fine-grained emotion in Portuguese. We
evaluate our dataset by fine-tuning a transformer-based language model (BERT)
and validating it on a Golden Standard annotated validation set. Our results
(F1-score= .64) suggest lexical-based weak supervision as an appropriate
strategy for initial work in low resources environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cortiz_D/0/1/0/all/0/1"&gt;Diogo Cortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jefferson O. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calegari_N/0/1/0/all/0/1"&gt;Newton Calegari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Ana Lu&amp;#xed;sa Freitas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1"&gt;Ana Ang&amp;#xe9;lica Soares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botelho_C/0/1/0/all/0/1"&gt;Carolina Botelho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rego_G/0/1/0/all/0/1"&gt;Gabriel Gaudencio R&amp;#xea;go&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampaio_W/0/1/0/all/0/1"&gt;Waldir Sampaio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boggio_P/0/1/0/all/0/1"&gt;Paulo Sergio Boggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07344</id>
        <link href="http://arxiv.org/abs/2108.07344"/>
        <updated>2021-08-18T01:54:58.946Z</updated>
        <summary type="html"><![CDATA[The recent success of distributed word representations has led to an
increased interest in analyzing the properties of their spatial distribution.
Current metrics suggest that contextualized word embedding models do not
uniformly utilize all dimensions when embedding tokens in vector space. Here we
argue that existing metrics are fragile and tend to obfuscate the true spatial
distribution of point clouds. To ameliorate this issue, we propose IsoScore: a
novel metric which quantifies the degree to which a point cloud uniformly
utilizes the ambient vector space. We demonstrate that IsoScore has several
desirable properties such as mean invariance and direct correspondence to the
number of dimensions used, which are properties that existing scores do not
possess. Furthermore, IsoScore is conceptually intuitive and computationally
efficient, making it well suited for analyzing the distribution of point clouds
in arbitrary vector spaces, not necessarily limited to those of word embeddings
alone. Additionally, we use IsoScore to demonstrate that a number of recent
conclusions in the NLP literature that have been derived using brittle metrics
of spatial distribution, such as average cosine similarity, may be incomplete
or altogether inaccurate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1"&gt;William Rudman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1"&gt;Nate Gillman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1"&gt;Taylor Rayne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards. (arXiv:2108.07374v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2108.07374</id>
        <link href="http://arxiv.org/abs/2108.07374"/>
        <updated>2021-08-18T01:54:58.938Z</updated>
        <summary type="html"><![CDATA[Developing documentation guidelines and easy-to-use templates for datasets
and models is a challenging task, especially given the variety of backgrounds,
skills, and incentives of the people involved in the building of natural
language processing (NLP) tools. Nevertheless, the adoption of standard
documentation practices across the field of NLP promotes more accessible and
detailed descriptions of NLP datasets and models, while supporting researchers
and developers in reflecting on their work. To help with the standardization of
documentation, we present two case studies of efforts that aim to develop
reusable documentation templates -- the HuggingFace data card, a general
purpose card for datasets in NLP, and the GEM benchmark data and model cards
with a focus on natural language generation. We describe our process for
developing these templates, including the identification of relevant
stakeholder groups, the definition of a set of guiding principles, the use of
existing templates as our foundation, and iterative revisions based on
feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1"&gt;Angelina McMillan-Major&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1"&gt;Salomey Osei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1"&gt;Juan Diego Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1"&gt;Pawan Sasanka Ammanamanchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1"&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1"&gt;Yacine Jernite&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07571</id>
        <link href="http://arxiv.org/abs/2108.07571"/>
        <updated>2021-08-18T01:54:58.918Z</updated>
        <summary type="html"><![CDATA[Citation recommendation is intended to assist researchers in the process of
searching for relevant papers to cite by recommending appropriate citations for
a given input text. Existing test collections for this task are noisy and
unreliable since they are built automatically from parsed PDF papers. In this
paper, we present our ongoing effort at creating a publicly available, manually
annotated test collection for citation recommendation. We also conduct a series
of experiments to evaluate the effectiveness of content-based baseline models
on the test collection, providing results for future work to improve upon. Our
test collection and code to replicate experiments are available at
https://github.com/boudinfl/acm-cr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1"&gt;Florian Boudin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Expert. (arXiv:2108.07535v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07535</id>
        <link href="http://arxiv.org/abs/2108.07535"/>
        <updated>2021-08-18T01:54:58.884Z</updated>
        <summary type="html"><![CDATA[Many generation tasks follow a one-to-many mapping relationship: each input
could be associated with multiple outputs. Existing methods like Conditional
Variational AutoEncoder(CVAE) employ a latent variable to model this
one-to-many relationship. However, this high-dimensional and dense latent
variable lacks explainability and usually leads to poor and uncontrollable
generations. In this paper, we innovatively introduce the linguistic concept of
pattern to decompose the one-to-many mapping into multiple one-to-one mappings
and further propose a model named Sparse Pattern Mixture of Experts(SPMoE).
Each one-to-one mapping is associated with a conditional generation pattern and
is modeled with an expert in SPMoE. To ensure each language pattern can be
exclusively handled with an expert model for better explainability and
diversity, a sparse mechanism is employed to coordinate all the expert models
in SPMoE. We assess the performance of our SPMoE on the paraphrase generation
task and the experiment results prove that SPMoE can achieve a good balance in
terms of quality, pattern-level diversity, and corpus-level diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shaobo Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1"&gt;Xintong Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhongzhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haiqing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07493</id>
        <link href="http://arxiv.org/abs/2108.07493"/>
        <updated>2021-08-18T01:54:58.876Z</updated>
        <summary type="html"><![CDATA[It's challenging to customize transducer-based automatic speech recognition
(ASR) system with context information which is dynamic and unavailable during
model training. In this work, we introduce a light-weight contextual spelling
correction model to correct context-related recognition errors in
transducer-based ASR systems. We incorporate the context information into the
spelling correction model with a shared context encoder and use a filtering
algorithm to handle large-size context lists. Experiments show that the model
improves baseline ASR model performance with about 50% relative word error rate
reduction, which also significantly outperforms the baseline method such as
contextual LM biasing. The model also shows excellent performance for
out-of-vocabulary terms not seen during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Relation Linking for Question Answering over Knowledge Bases. (arXiv:2108.07337v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07337</id>
        <link href="http://arxiv.org/abs/2108.07337"/>
        <updated>2021-08-18T01:54:58.866Z</updated>
        <summary type="html"><![CDATA[Relation linking is essential to enable question answering over knowledge
bases. Although there are various efforts to improve relation linking
performance, the current state-of-the-art methods do not achieve optimal
results, therefore, negatively impacting the overall end-to-end question
answering performance. In this work, we propose a novel approach for relation
linking framing it as a generative problem facilitating the use of pre-trained
sequence-to-sequence models. We extend such sequence-to-sequence models with
the idea of infusing structured data from the target knowledge base, primarily
to enable these models to handle the nuances of the knowledge base. Moreover,
we train the model with the aim to generate a structured output consisting of a
list of argument-relation pairs, enabling a knowledge validation step. We
compared our method against the existing relation linking systems on four
different datasets derived from DBpedia and Wikidata. Our method reports large
improvements over the state-of-the-art while using a much simpler model that
can be easily adapted to different knowledge bases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1"&gt;Gaetano Rossiello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1"&gt;Nandana Mihindukulasooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1"&gt;Ibrahim Abdelaziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1"&gt;Mihaela Bornea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1"&gt;Tahira Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1"&gt;Pavan Kapanipathi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12085</id>
        <link href="http://arxiv.org/abs/2105.12085"/>
        <updated>2021-08-18T01:54:58.836Z</updated>
        <summary type="html"><![CDATA[Long-range and short-range temporal modeling are two complementary and
crucial aspects of video recognition. Most of the state-of-the-arts focus on
short-range spatio-temporal modeling and then average multiple snippet-level
predictions to yield the final video-level prediction. Thus, their video-level
prediction does not consider spatio-temporal features of how video evolves
along the temporal dimension. In this paper, we introduce a novel Dynamic
Segment Aggregation (DSA) module to capture relationship among snippets. To be
more specific, we attempt to generate a dynamic kernel for a convolutional
operation to aggregate long-range temporal information among adjacent snippets
adaptively. The DSA module is an efficient plug-and-play module and can be
combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform
powerful long-range modeling with minimal overhead. The final video
architecture, coined as DSANet. We conduct extensive experiments on several
video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,
Something-Something V1 and ActivityNet) to show its superiority. Our proposed
DSA module is shown to benefit various video recognition models significantly.
For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is
improved from 74.9% to 78.2% on Kinetics-400. Codes are available at
https://github.com/whwu95/DSANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Mingde Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zichao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yifeng Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07627</id>
        <link href="http://arxiv.org/abs/2108.07627"/>
        <updated>2021-08-18T01:54:58.811Z</updated>
        <summary type="html"><![CDATA[An independent ethical assessment of an artificial intelligence system is an
impartial examination of the system's development, deployment, and use in
alignment with ethical values. System-level qualitative frameworks that
describe high-level requirements and component-level quantitative metrics that
measure individual ethical dimensions have been developed over the past few
years. However, there exists a gap between the two, which hinders the execution
of independent ethical assessments in practice. This study bridges this gap and
designs a holistic independent ethical assessment process for a text
classification model with a special focus on the task of hate speech detection.
The assessment is further augmented with protected attributes mining and
counterfactual-based analysis to enhance bias assessment. It covers assessments
of technical performance, data bias, embedding bias, classification bias, and
interpretability. The proposed process is demonstrated through an assessment of
a deep hate speech detection model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amitoj Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1"&gt;Amin Rasekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1"&gt;Ilana Golbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anand Rao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Multi-scale Convolution for Dialect Identification. (arXiv:2108.07787v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07787</id>
        <link href="http://arxiv.org/abs/2108.07787"/>
        <updated>2021-08-18T01:54:58.802Z</updated>
        <summary type="html"><![CDATA[Time Delay Neural Networks (TDNN)-based methods are widely used in dialect
identification. However, in previous work with TDNN application, subtle variant
is being neglected in different feature scales. To address this issue, we
propose a new architecture, named dynamic multi-scale convolution, which
consists of dynamic kernel convolution, local multi-scale learning, and global
multi-scale pooling. Dynamic kernel convolution captures features between
short-term and long-term context adaptively. Local multi-scale learning, which
represents multi-scale features at a granular level, is able to increase the
range of receptive fields for convolution operation. Besides, global
multi-scale pooling is applied to aggregate features from different bottleneck
layers in order to collect information from multiple aspects. The proposed
architecture significantly outperforms state-of-the-art system on the
AP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020,
with the best average cost performance (Cavg) of 0.067 and the best equal error
rate (EER) of 6.52%. Compared with the known best results, our method achieves
9% of Cavg and 45% of EER relative improvement, respectively. Furthermore, the
parameters of proposed model are 91% fewer than the best known model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1"&gt;Tianlong Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1"&gt;Shouyi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1"&gt;Wang Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dandan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jinwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Huiyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaorui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not All Linearizations Are Equally Data-Hungry in Sequence Labeling Parsing. (arXiv:2108.07556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07556</id>
        <link href="http://arxiv.org/abs/2108.07556"/>
        <updated>2021-08-18T01:54:58.795Z</updated>
        <summary type="html"><![CDATA[Different linearizations have been proposed to cast dependency parsing as
sequence labeling and solve the task as: (i) a head selection problem, (ii)
finding a representation of the token arcs as bracket strings, or (iii)
associating partial transition sequences of a transition-based parser to words.
Yet, there is little understanding about how these linearizations behave in
low-resource setups. Here, we first study their data efficiency, simulating
data-restricted setups from a diverse set of rich-resource treebanks. Second,
we test whether such differences manifest in truly low-resource setups. The
results show that head selection encodings are more data-efficient and perform
better in an ideal (gold) framework, but that such advantage greatly vanishes
in favour of bracketing formats when the running setup resembles a real-world
low-resource configuration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1"&gt;Alberto Mu&amp;#xf1;oz-Ortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strzyz_M/0/1/0/all/0/1"&gt;Michalina Strzyz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1"&gt;David Vilares&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining speakers of multiple languages to improve quality of neural voices. (arXiv:2108.07737v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07737</id>
        <link href="http://arxiv.org/abs/2108.07737"/>
        <updated>2021-08-18T01:54:58.787Z</updated>
        <summary type="html"><![CDATA[In this work, we explore multiple architectures and training procedures for
developing a multi-speaker and multi-lingual neural TTS system with the goals
of a) improving the quality when the available data in the target language is
limited and b) enabling cross-lingual synthesis. We report results from a large
experiment using 30 speakers in 8 different languages across 15 different
locales. The system is trained on the same amount of data per speaker. Compared
to a single-speaker model, when the suggested system is fine tuned to a
speaker, it produces significantly better quality in most of the cases while it
only uses less than $40\%$ of the speaker's data used to build the
single-speaker model. In cross-lingual synthesis, on average, the generated
quality is within $80\%$ of native single-speaker models, in terms of Mean
Opinion Score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Latorre_J/0/1/0/all/0/1"&gt;Javier Latorre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bailleul_C/0/1/0/all/0/1"&gt;Charlotte Bailleul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morrill_T/0/1/0/all/0/1"&gt;Tuuli Morrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conkie_A/0/1/0/all/0/1"&gt;Alistair Conkie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_Y/0/1/0/all/0/1"&gt;Yannis Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation Guidelines for the Turku Paraphrase Corpus. (arXiv:2108.07499v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07499</id>
        <link href="http://arxiv.org/abs/2108.07499"/>
        <updated>2021-08-18T01:54:58.776Z</updated>
        <summary type="html"><![CDATA[This document describes the annotation guidelines used to construct the Turku
Paraphrase Corpus. These guidelines were developed together with the corpus
annotation, revising and extending the guidelines regularly during the
annotation work. Our paraphrase annotation scheme uses the base scale 1-4,
where labels 1 and 2 are used for negative candidates (not paraphrases), while
labels 3 and 4 are paraphrases at least in the given context if not everywhere.
In addition to base labeling, the scheme is enriched with additional
subcategories (flags) for categorizing different types of paraphrases inside
the two positive labels, making the annotation scheme suitable for more
fine-grained paraphrase categorization. The annotation scheme is used to
annotate over 100,000 Finnish paraphrase pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1"&gt;Jenna Kanerva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1"&gt;Filip Ginter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1"&gt;Li-Hsin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastas_I/0/1/0/all/0/1"&gt;Iiro Rastas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skantsi_V/0/1/0/all/0/1"&gt;Valtteri Skantsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilpelainen_J/0/1/0/all/0/1"&gt;Jemina Kilpel&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kupari_H/0/1/0/all/0/1"&gt;Hanna-Mari Kupari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piirto_A/0/1/0/all/0/1"&gt;Aurora Piirto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saarni_J/0/1/0/all/0/1"&gt;Jenna Saarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sevon_M/0/1/0/all/0/1"&gt;Maija Sev&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarkka_O/0/1/0/all/0/1"&gt;Otto Tarkka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An NLP approach to quantify dynamic salience of predefined topics in a text corpus. (arXiv:2108.07345v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07345</id>
        <link href="http://arxiv.org/abs/2108.07345"/>
        <updated>2021-08-18T01:54:58.736Z</updated>
        <summary type="html"><![CDATA[The proliferation of news media available online simultaneously presents a
valuable resource and significant challenge to analysts aiming to profile and
understand social and cultural trends in a geographic location of interest.
While an abundance of news reports documenting significant events, trends, and
responses provides a more democratized picture of the social characteristics of
a location, making sense of an entire corpus to extract significant trends is a
steep challenge for any one analyst or team. Here, we present an approach using
natural language processing techniques that seeks to quantify how a set of
pre-defined topics of interest change over time across a large corpus of text.
We found that, given a predefined topic, we can identify and rank sets of
terms, or n-grams, that map to those topics and have usage patterns that
deviate from a normal baseline. Emergence, disappearance, or significant
variations in n-gram usage present a ground-up picture of a topic's dynamic
salience within a corpus of interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bock_A/0/1/0/all/0/1"&gt;A. Bock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palladino_A/0/1/0/all/0/1"&gt;A. Palladino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_Heisters_S/0/1/0/all/0/1"&gt;S. Smith-Heisters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boardman_I/0/1/0/all/0/1"&gt;I. Boardman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pellegrini_E/0/1/0/all/0/1"&gt;E. Pellegrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bienenstock_E/0/1/0/all/0/1"&gt;E.J. Bienenstock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valenti_A/0/1/0/all/0/1"&gt;A. Valenti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07435</id>
        <link href="http://arxiv.org/abs/2108.07435"/>
        <updated>2021-08-18T01:54:58.726Z</updated>
        <summary type="html"><![CDATA[Protein is linked to almost every life process. Therefore, analyzing the
biological structure and property of protein sequences is critical to the
exploration of life, as well as disease detection and drug discovery.
Traditional protein analysis methods tend to be labor-intensive and
time-consuming. The emergence of deep learning models makes modeling data
patterns in large quantities of data possible. Interdisciplinary researchers
have begun to leverage deep learning methods to model large biological
datasets, e.g. using long short-term memory and convolutional neural network
for protein sequence classification. After millions of years of evolution,
evolutionary information is encoded in protein sequences. Inspired by the
similarity between natural language and protein sequences, we use large-scale
language models to model evolutionary-scale protein sequences, encoding protein
biology information in representation. Significant improvements are observed in
both token-level and sequence-level tasks, demonstrating that our large-scale
model can accurately capture evolution information from pretraining on
evolutionary-scale individual sequences. Our code and model are available at
https://github.com/THUDM/ProteinLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yijia Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Chang-Yu Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Incorrectness Logic and Kleene Algebra With Top and Tests. (arXiv:2108.07707v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2108.07707</id>
        <link href="http://arxiv.org/abs/2108.07707"/>
        <updated>2021-08-18T01:54:58.717Z</updated>
        <summary type="html"><![CDATA[Kleene algebra with tests (KAT) is a foundational equational framework for
reasoning about programs, which has found applications in program
transformations, networking and compiler optimizations, among many other areas.
In his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,
showing that one can reason about the (partial) correctness of while programs
by means of the equational theory of KAT.

In this work, we investigate the support that KAT provides for reasoning
about \emph{incorrectness}, instead, as embodied by Ohearn's recently proposed
incorrectness logic. We show that KAT cannot directly express incorrectness
logic. The main reason for this limitation can be traced to the fact that KAT
cannot express explicitly the notion of codomain, which is essential to express
incorrectness triples. To address this issue, we study Kleene algebra with Top
and Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT
is powerful enough to express a codomain operation, to express incorrectness
triples, and to prove all the rules of incorrectness logic sound. This shows
that one can reason about the incorrectness of while-like programs by means of
the equational theory of TopKAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amorim_A/0/1/0/all/0/1"&gt;Arthur Azevedo de Amorim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06423</id>
        <link href="http://arxiv.org/abs/2101.06423"/>
        <updated>2021-08-18T01:54:58.691Z</updated>
        <summary type="html"><![CDATA[Neural text matching models have been widely used in community question
answering, information retrieval, and dialogue. However, these models designed
for short texts cannot well address the long-form text matching problem,
because there are many contexts in long-form texts can not be directly aligned
with each other, and it is difficult for existing models to capture the key
matching signals from such noisy data. Besides, these models are
computationally expensive for simply use all textual data indiscriminately. To
tackle the effectiveness and efficiency problem, we propose a novel
hierarchical noise filtering model, namely Match-Ignition. The main idea is to
plug the well-known PageRank algorithm into the Transformer, to identify and
filter both sentence and word level noisy information in the matching process.
Noisy sentences are usually easy to detect because previous work has shown that
their similarity can be explicitly evaluated by the word overlapping, so we
directly use PageRank to filter such information based on a sentence similarity
graph. Unlike sentences, words rely on their contexts to express concrete
meanings, so we propose to jointly learn the filtering and matching process, to
well capture the critical word-level matching signals. Specifically, a word
graph is first built based on the attention scores in each self-attention block
of Transformer, and key words are then selected by applying PageRank on this
graph. In this way, noisy words will be filtered out layer by layer in the
matching process. Experimental results show that Match-Ignition outperforms
both SOTA short text matching models and recent long-form text matching models.
We also conduct detailed analysis to show that Match-Ignition efficiently
captures important sentences and words, to facilitate the long-form text
matching process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1"&gt;Liang Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07790</id>
        <link href="http://arxiv.org/abs/2108.07790"/>
        <updated>2021-08-18T01:54:58.657Z</updated>
        <summary type="html"><![CDATA[Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1"&gt;Helen Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1"&gt;Cooper Raterink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o G.M. Ara&amp;#xfa;jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1"&gt;Ivan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Carol Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1"&gt;Adrien Morisot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1"&gt;Nicholas Frosst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Capsule Aggregation for Unaligned Multimodal Sequences. (arXiv:2108.07543v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07543</id>
        <link href="http://arxiv.org/abs/2108.07543"/>
        <updated>2021-08-18T01:54:58.644Z</updated>
        <summary type="html"><![CDATA[Humans express their opinions and emotions through multiple modalities which
mainly consist of textual, acoustic and visual modalities. Prior works on
multimodal sentiment analysis mostly apply Recurrent Neural Network (RNN) to
model aligned multimodal sequences. However, it is unpractical to align
multimodal sequences due to different sample rates for different modalities.
Moreover, RNN is prone to the issues of gradient vanishing or exploding and it
has limited capacity of learning long-range dependency which is the major
obstacle to model unaligned multimodal sequences. In this paper, we introduce
Graph Capsule Aggregation (GraphCAGE) to model unaligned multimodal sequences
with graph-based neural model and Capsule Network. By converting sequence data
into graph, the previously mentioned problems of RNN are avoided. In addition,
the aggregation capability of Capsule Network and the graph-based structure
enable our model to be interpretable and better solve the problem of long-range
dependency. Experimental results suggest that GraphCAGE achieves
state-of-the-art performance on two benchmark datasets with representations
refined by Capsule Network and interpretation provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianfeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1"&gt;Sijie Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haifeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion. (arXiv:2107.01175v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01175</id>
        <link href="http://arxiv.org/abs/2107.01175"/>
        <updated>2021-08-18T01:54:58.621Z</updated>
        <summary type="html"><![CDATA[We propose an audio-visual spatial-temporal deep neural network with: (1) a
visual block containing a pretrained 2D-CNN followed by a temporal
convolutional network (TCN); (2) an aural block containing several parallel
TCNs; and (3) a leader-follower attentive fusion block combining the
audio-visual information. The TCN with large history coverage enables our model
to exploit spatial-temporal information within a much larger window length
(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36
or 48). The fusion block emphasizes the visual modality while exploits the
noisy aural modality using the inter-modality attention mechanism. To make full
use of the data and alleviate over-fitting, cross-validation is carried out on
the training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. On the test (validation)
set of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence
and 0.492 (0.649) for arousal, which significantly outperforms the baseline
method with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for
valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.07789</id>
        <link href="http://arxiv.org/abs/2108.07789"/>
        <updated>2021-08-18T01:54:58.598Z</updated>
        <summary type="html"><![CDATA[Language models (LMs) pre-trained on massive amounts of text, in particular
bidirectional encoder representations from Transformers (BERT), generative
pre-training (GPT), and GPT-2, have become a key technology for many natural
language processing tasks. In this paper, we present results using fine-tuned
GPT, GPT-2, and their combination for automatic speech recognition (ASR).
Unlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct
product of the output probabilities is no longer a valid language prior
probability. A conversion method is proposed to compute the correct language
prior probability based on bidirectional LM outputs in a mathematically exact
way. Experimental results on the widely used AMI and Switchboard ASR tasks
showed that the combination of the fine-tuned GPT and GPT-2 outperformed the
combination of three neural LMs with different architectures trained from
scratch on the in-domain text by up to a 12% relative word error rate reduction
(WERR). Furthermore, the proposed conversion for language prior probabilities
enables BERT to receive an extra 3% relative WERR, and the combination of BERT,
GPT and GPT-2 results in further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xianrui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Self-Attentive Neural Networks for Click-Through Rate Prediction. (arXiv:2101.03654v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03654</id>
        <link href="http://arxiv.org/abs/2101.03654"/>
        <updated>2021-08-18T01:54:58.574Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate (CTR) prediction, whose aim is to predict the probability
of whether a user will click on an item, is an essential task for many online
applications. Due to the nature of data sparsity and high dimensionality of CTR
prediction, a key to making effective prediction is to model high-order feature
interaction. An efficient way to do this is to perform inner product of feature
embeddings with self-attentive neural networks. To better model complex feature
interaction, in this paper we propose a novel DisentanglEd Self-atTentIve
NEtwork (DESTINE) framework for CTR prediction that explicitly decouples the
computation of unary feature importance from pairwise interaction.
Specifically, the unary term models the general importance of one feature on
all other features, whereas the pairwise interaction term contributes to
learning the pure impact for each feature pair. We conduct extensive
experiments using two real-world benchmark datasets. The results show that
DESTINE not only maintains computational efficiency but achieves consistent
improvements over state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yichen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Feng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01377</id>
        <link href="http://arxiv.org/abs/2008.01377"/>
        <updated>2021-08-18T01:54:58.386Z</updated>
        <summary type="html"><![CDATA[Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a
key requirement for both linguistic research and subsequent automated natural
language processing (NLP) tasks. This problem is commonly tackled using machine
learning methods, i.e., by training a POS tagger on a sufficiently large corpus
of labeled data. While the problem of POS tagging can essentially be considered
as solved for modern languages, historical corpora turn out to be much more
difficult, especially due to the lack of native speakers and sparsity of
training data. Moreover, most texts have no sentences as we know them today,
nor a common orthography. These irregularities render the task of automated POS
tagging more difficult and error-prone. Under these circumstances, instead of
forcing the POS tagger to predict and commit to a single tag, it should be
enabled to express its uncertainty. In this paper, we consider POS tagging
within the framework of set-valued prediction, which allows the POS tagger to
express its uncertainty via predicting a set of candidate POS tags instead of
guessing a single one. The goal is to guarantee a high confidence that the
correct POS tag is included while keeping the number of candidates small. In
our experimental study, we find that extending state-of-the-art POS taggers to
set-valued prediction yields more precise and robust taggings, especially for
unknown words, i.e., words not occurring in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1"&gt;Stefan Heid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1"&gt;Marcel Wever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Analysis on the News to Improve Mental Health. (arXiv:2108.07706v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07706</id>
        <link href="http://arxiv.org/abs/2108.07706"/>
        <updated>2021-08-18T01:54:58.353Z</updated>
        <summary type="html"><![CDATA[The popularization of the internet created a revitalized digital media. With
monetization driven by clicks, journalists have reprioritized their content for
the highly competitive atmosphere of online news. The resulting negativity bias
is harmful and can lead to anxiety and mood disturbance. We utilized a pipeline
of 4 sentiment analysis models trained on various datasets - using Sequential,
LSTM, BERT, and SVM models. When combined, the application, a mobile app,
solely displays uplifting and inspiring stories for users to read. Results have
been successful - 1,300 users rate the app at 4.9 stars, and 85% report
improved mental health by using it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Saurav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayant_R/0/1/0/all/0/1"&gt;Rushil Jayant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charagulla_N/0/1/0/all/0/1"&gt;Nihaar Charagulla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in Sequential Recommendation. (arXiv:2108.07505v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07505</id>
        <link href="http://arxiv.org/abs/2108.07505"/>
        <updated>2021-08-18T01:54:58.337Z</updated>
        <summary type="html"><![CDATA[Successful sequential recommendation systems rely on accurately capturing the
user's short-term and long-term interest. Although Transformer-based models
achieved state-of-the-art performance in the sequential recommendation task,
they generally require quadratic memory and time complexity to the sequence
length, making it difficult to extract the long-term interest of users. On the
other hand, Multi-Layer Perceptrons (MLP)-based models, renowned for their
linear memory and time complexity, have recently shown competitive results
compared to Transformer in various tasks. Given the availability of a massive
amount of the user's behavior history, the linear memory and time complexity of
MLP-based models make them a promising alternative to explore in the sequential
recommendation task. To this end, we adopted MLP-based models in sequential
recommendation but consistently observed that MLP-based methods obtain lower
performance than those of Transformer despite their computational benefits.
From experiments, we observed that introducing explicit high-order interactions
to MLP layers mitigates such performance gap. In response, we propose the
Multi-Order Interaction (MOI) layer, which is capable of expressing an
arbitrary order of interactions within the inputs while maintaining the memory
and time complexity of the MLP layer. By replacing the MLP layer with the MOI
layer, our model was able to achieve comparable performance with
Transformer-based models while retaining the MLP-based models' computational
benefits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hojoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Dongyoon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sunghwan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Powerful is Graph Convolution for Recommendation?. (arXiv:2108.07567v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07567</id>
        <link href="http://arxiv.org/abs/2108.07567"/>
        <updated>2021-08-18T01:54:58.324Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) have recently enabled a popular class of
algorithms for collaborative filtering (CF). Nevertheless, the theoretical
underpinnings of their empirical successes remain elusive. In this paper, we
endeavor to obtain a better understanding of GCN-based CF methods via the lens
of graph signal processing. By identifying the critical role of smoothness, a
key concept in graph signal processing, we develop a unified graph
convolution-based framework for CF. We prove that many existing CF methods are
special cases of this framework, including the neighborhood-based methods,
low-rank matrix factorization, linear auto-encoders, and LightGCN,
corresponding to different low-pass filters. Based on our framework, we then
present a simple and computationally efficient CF baseline, which we shall
refer to as Graph Filter based Collaborative Filtering (GF-CF). Given an
implicit feedback matrix, GF-CF can be obtained in a closed form instead of
expensive training with back-propagation. Experiments will show that GF-CF
achieves competitive or better performance against deep learning-based methods
on three well-known datasets, notably with a $70\%$ performance gain over
LightGCN on the Amazon-book dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yifei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongji Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1"&gt;Caihua Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1"&gt;Khaled B. Letaief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Product Search Meets Collaborative Filtering: A Hierarchical Heterogeneous Graph Neural Network Approach. (arXiv:2108.07574v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07574</id>
        <link href="http://arxiv.org/abs/2108.07574"/>
        <updated>2021-08-18T01:54:58.296Z</updated>
        <summary type="html"><![CDATA[Personalization lies at the core of boosting the product search system
performance. Prior studies mainly resorted to the semantic matching between
textual queries and user/product related documents, leaving the user
collaborative behaviors untapped. In fact, the collaborative filtering signals
between users intuitively offer a complementary information for the semantic
matching. To close the gap between collaborative filtering and product search,
we propose a Hierarchical Heterogeneous Graph Neural Network (HHGNN) approach
in this paper. Specifically, we organize HHGNN with a hierarchical graph
structure according to the three edge types. The sequence edge accounts for the
syntax formulation from word nodes to sentence nodes; the composition edge
aggregates the semantic features to the user and product nodes; and the
interaction edge on the top performs graph convolutional operation between user
and product nodes. At last, we integrate the higher-order neighboring
collaborative features and the semantic features for better representation
learning. We conduct extensive experiments on six Amazon review datasets. The
results show that our proposed method can outperform the state-of-the-art
baselines with a large margin. In addition, we empirically prove that
collaborative filtering and semantic matching are complementary to each other
in product search performance enhancement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xiangkun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yangyang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyong Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Biased Subgroups in Ranking and Classification. (arXiv:2108.07450v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.07450</id>
        <link href="http://arxiv.org/abs/2108.07450"/>
        <updated>2021-08-18T01:54:58.270Z</updated>
        <summary type="html"><![CDATA[When analyzing the behavior of machine learning algorithms, it is important
to identify specific data subgroups for which the considered algorithm shows
different performance with respect to the entire dataset. The intervention of
domain experts is normally required to identify relevant attributes that define
these subgroups.

We introduce the notion of divergence to measure this performance difference
and we exploit it in the context of (i) classification models and (ii) ranking
applications to automatically detect data subgroups showing a significant
deviation in their behavior. Furthermore, we quantify the contribution of all
attributes in the data subgroup to the divergent behavior by means of Shapley
values, thus allowing the identification of the most impacting attributes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1"&gt;Eliana Pastor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfaro_L/0/1/0/all/0/1"&gt;Luca de Alfaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1"&gt;Elena Baralis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.07571</id>
        <link href="http://arxiv.org/abs/2108.07571"/>
        <updated>2021-08-18T01:54:58.253Z</updated>
        <summary type="html"><![CDATA[Citation recommendation is intended to assist researchers in the process of
searching for relevant papers to cite by recommending appropriate citations for
a given input text. Existing test collections for this task are noisy and
unreliable since they are built automatically from parsed PDF papers. In this
paper, we present our ongoing effort at creating a publicly available, manually
annotated test collection for citation recommendation. We also conduct a series
of experiments to evaluate the effectiveness of content-based baseline models
on the test collection, providing results for future work to improve upon. Our
test collection and code to replicate experiments are available at
https://github.com/boudinfl/acm-cr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1"&gt;Florian Boudin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.07627</id>
        <link href="http://arxiv.org/abs/2108.07627"/>
        <updated>2021-08-18T01:54:58.209Z</updated>
        <summary type="html"><![CDATA[An independent ethical assessment of an artificial intelligence system is an
impartial examination of the system's development, deployment, and use in
alignment with ethical values. System-level qualitative frameworks that
describe high-level requirements and component-level quantitative metrics that
measure individual ethical dimensions have been developed over the past few
years. However, there exists a gap between the two, which hinders the execution
of independent ethical assessments in practice. This study bridges this gap and
designs a holistic independent ethical assessment process for a text
classification model with a special focus on the task of hate speech detection.
The assessment is further augmented with protected attributes mining and
counterfactual-based analysis to enhance bias assessment. It covers assessments
of technical performance, data bias, embedding bias, classification bias, and
interpretability. The proposed process is demonstrated through an assessment of
a deep hate speech detection model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amitoj Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1"&gt;Amin Rasekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1"&gt;Ilana Golbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anand Rao&lt;/a&gt;</name>
        </author>
    </entry>
</feed>